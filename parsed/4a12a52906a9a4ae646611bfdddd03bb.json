{
"text": "Adaptive Dependency-aware Prompt Optimization Framework for\n                                Multi-Step LLM Pipeline\n\n\n                Minjun Zhao  Xinyu Zhang   Shuai Zhang  Deyang Li   Ruifeng Shi\n                                      Huawei Poisson Lab\n          {zhaominjun1, zhangxinyu35, zhangshuai117, lideyang2, shiruifeng}@huawei.com\n\n\n\n\n                          Abstract\n\n\n                  Multi-step LLM pipelines invoke large lan-\n                guage models multiple times in a structured se-\n                 quence and can effectively solve complex tasks,2025\n                  but their performance heavily depends on the\n                prompts used at each step. Jointly optimizing\n                  these prompts is difficult due to missing step-Dec\n                    level supervision and inter-step dependencies.\n                   Existing end-to-end prompt optimization meth-31\n                ods struggle under these conditions and often\n                   yield suboptimal or unstable updates. We pro-\n                 pose ADOPT, an Adaptive Dependency-aware\n                Prompt Optimization framework for multi-step\n           LLM pipelines. ADOPT explicitly models\n                   the dependency between each LLM step and[cs.CL]\n                   the final task outcome, enabling precise text-\n                  gradient estimation analogous to computing\n                   analytical derivatives.   It decouples textual        Figure 1: An example of the optimization problem we\n                   gradient estimation from gradient updates, re-        explore for a multi-step pipeline.\n                 ducing multi-prompt optimization to flexible\n                  single-prompt optimization steps, and employs\n                  a Shapley-based mechanism to adaptively allo-                                       LLM systems and improve robustness and success\n                   cate optimization resources. Experiments on\n                                                                        rates when tackling intricate real-world tasks.\n                  real-world datasets and diverse pipeline struc-\n                   tures show that ADOPT is effective and ro-          Nevertheless, current multi-step LLM pipelines\n                    bust, consistently outperforming state-of-the-          still face several challenges, particularly in opti-\n                      art prompt optimization baselines.                mizing multiple prompts. Since the prompts used\n                                                                         at each step directly shape intermediate outputs\n          1  Introduction                             and error propagation throughout the pipeline, ef-arXiv:2512.24933v1                                                                      fective prompt optimization is crucial for improv-\n          Background and Motivation. Recent years have    ing the end-to-end success rate of such systems.\n             witnessed large language models (LLMs) achieving   On the one hand, manual prompt optimization\n            remarkable performance across a broad spectrum     is highly heuristic, labor-intensive, and difficult\n             of tasks (Yang et al., 2024a; Bubeck et al., 2023).   to scale, practitioners often need to iteratively re-\n           However, despite the continuous advancement of    fine prompts across many steps, and modifying\n             individual LLMs, they still exhibit notable limita-   the prompt for one step may inadvertently disturb\n              tions when solving complex, multi-faceted prob-   the behavior of others, leading to a \"robbing Pe-\n             lems. To address these limitations, recent research    ter to pay Paul\" effect where local improvements\n            has explored multi-step LLM pipelines (Ma et al.,   do not necessarily yield better end-to-end perfor-\n            2025; Lin et al., 2025), where a task is decomposed   mance. On the other hand, existing automated\n              into multiple steps and executed through programs   prompt optimization methods (Pryzant et al., 2023;\n           composed of coordinated LLM calls. Such multi-   Prasad et al., 2023; Guo et al., 2025; Fernando\n              step approaches extend the capability boundaries of    et al., 2023) are primarily designed for optimiz-\n\n\n                                                    1\n\ning prompts in single-step LLM calls. In multi-   where an error originates, a process that often intro-\nstep LLM pipelines, only end-to-end supervision   duces interpretation bias or severely weakens the\nis available, and intermediate outputs are further    signal. The mechanism resembles a team receiv-\nprocessed before producing the final result, making    ing general suggestions for improvement, where\nit difficult to obtain labels for individual LLM calls   each member naturally identifies the part relevant\nwithin the pipeline.                                   to their own responsibilities. (2) Second, ADOPT\n                                                     separates the generation of these adjustment direc-\nLimitations of Existing Studies. Recently, sev-\n                                                        tions from the optimization procedure. Each LLM\neral data-driven, end-to-end multi-prompt opti-\n                                                       step can independently apply an appropriate single-\nmization methods have been proposed to address\n                                            prompt optimizer, and the system then searches\nthe aforementioned challenges, such as Dspy-\n                                                        for an effective combination of these local updates\nMIPRO (Opsahl-Ong et al., 2024), TextGrad (Yük-\n                                                           at the pipeline level. This design makes ADOPT\nsekgönül et al., 2025), Trace (Cheng et al., 2024),\n                                                         flexible and compatible with a broad range of op-\nand GEPA (Agrawal et  al., 2025).  These ap-\n                                                     timization strategies. (3) Third, ADOPT allocates\nproaches have demonstrated certain capabilities\n                                                    optimization effort according to the estimated con-\nin enabling the automatic optimization of multi-\n                                                          tribution of each step. Using a Shapley-based mea-\nstep LLM pipelines. However, they also exhibit no-\n                                                         sure, LLM steps that provide greater improvement\ntable limitations. First, some methods (Opsahl-Ong\n                                                      receive more optimization resources in subsequent\net al., 2024) fail to effectively utilize feedback from\n                                                             iterations. Through dependency-guided adjustment\nerroneous cases, resulting in unclear optimization\n                                                         signals, flexible local optimization, and principled\ndirections and inefficient exploration of the prompt\n                                                   resource allocation, ADOPT offers a stable and ef-\nsearch space.  Second, although methods using\n                                                         fective solution for optimizing multiple prompts in\nbackpropagation textual gradients (Yüksekgönül\n                                            complex multi-step LLM pipelines.\net al., 2025; Cheng et al., 2024) can propagate feed-\nback signals across steps, they rely on LLMs to   Contributions. Our main contributions are as fol-\nperform the backward reasoning, which introduces    lows.  (1) We introduce ADOPT, a dependency-\nmodel-induced interpretations that often distort or   aware prompt optimization framework that derives\nattenuate the gradients. Not only that, these meth-    step-level adjustment directions in multi-step LLM\nods struggle to determine whether an error stems    pipelines without relying on LLM-based backward\nfrom the current step or from upstream steps, lead-   reasoning. (2) We decouple adjustment-direction\ning to unstable and unreliable optimization sugges-   estimation from prompt optimization, enabling flex-\ntions. Besides, these methods are difficult to apply    ible use of single-prompt optimizers and effective\nto pipelines containing looping structures, which   combination of local updates.  (3) We design a\nare commonly used in complex tasks. As a result,   Shapley-based strategy that allocates optimization\nthe improvements achieved by existing end-to-end    resources according to each step’s contribution. (4)\nmulti-prompt optimization methods are limited.     Experiments on real-world datasets and diverse\n                                                      pipeline structures show that ADOPT consistently\nOur Solutions. Motivated by these limitations, we                                              improves end-to-end performance over existing\npropose ADOPT, an Adaptive Dependency-aware                                               methods.\nprompt OPTimization framework for multi-step\nLLM pipelines. ADOPT consists of three com-   2  Preliminaries\nponents. (1) First, it analyzes execution traces to\nidentify how each LLM step in the pipeline influ-   2.1  Multi-Step LLM Pipelines\nences the final outcome. Based on these depen-  We formalize a multi-step LLM system as\ndencies, ADOPT examines erroneous cases from\na global perspective and distributes the high-level            Φ = (C, M, Π),               (1)\nimprovement signals to individual steps, producing\na step-level adjustment direction that plays a role   where C denotes all executable code in the sys-\nsimilar to a partial derivative with respect to each   tem other than the LLMs themselves.  This in-\nprompt. This process is analogous to computing an    cludes the control-flow logic, tool calls, data prepro-\nanalytic gradient based on the functional relation-   cessing and postprocessing routines, conditional\nship between the parameters and the final output. It    branches, loops, retries, and any additional oper-\navoids relying on LLMs to reason backward about    ations that orchestrate the overall pipeline. The\n\n\n                                         2\n\nFigure 2: Dependency-aware Textual Gradient Estimation.\n\n\nset M = {M1, . . . , Mm} collects the LLM param-   2.2  Problem Formulation\neters of each LLM step used in the system, and                                          The multi-step LLM system Φ takes an input x and\nΠ = {p1, . . . , pm} denotes the natural language                                                 produces a final output Φ(x; Π), where the prompts\nprompts associated with these LLM steps. Each                             Π = {p1, . . . , pm} serve as the learnable parame-\nLLM step is determined by a pair (Mi, pi).                                                            ters and all other system components remain fixed.\n                                                    Intermediate results generated during execution are\n  Given an input x, the system executes C, which    not supervised, and only the end-to-end output is\nin turn triggers LLM steps (Mi, pi) whenever their    available for evaluation.\ncorresponding operations are reached. The execu-     Given a training set Dtrain = {(xj, yj)} and a\ntion may follow arbitrary program logic, including    task metric T(·, ·) that evaluates the final output\nsequential execution, branching, iterative refine-    relative to the ground-truth label, the goal is to opti-\nment, or tool-augmented steps. After completing   mize the prompts so that the overall system perfor-\nall operations specified by C, the system produces   mance is maximized on the training set. Formally,\na final output, which we denote by Φ(x; C, M, Π).  we define the optimization problem as\n\n                                   P ∗= arg max X   T Φ(x; P), y  ,  (2)\n  For notational simplicity, since C and M are            P∈Sm\nfixed in our setting, we write Φ(x; Π) when only                         (x,y)∈Dtrain\nthe prompts are treated as learnable parameters.   where S denotes the space of natural language\nThis formulation remains general and can naturally    strings. This problem is challenging because the\nextend to settings in which additional components    search space Sm is extremely large, supervision is\nof the system, C and M, are also subject to opti-    available only at the end-to-end level, and the exe-\nmization.                                            cution of Φ is non-differentiable, making it difficult\n\n\n                                         3\n\nto determine which LLM steps are responsible for   and bad cases according to the task metric M. We\nerrors and how they should be adjusted.               refer to the end-to-end inputs and outputs, together\n                                                with the inputs and outputs of each LLM step dur-\n3  Methodology                                ing execution, as the trace of a case. ADOPT re-\n                                                         tains the traces of both good and bad cases for use\nIn this section, we present ADOPT, an adaptive                                                        in the subsequent optimization procedure.\ndependency-aware prompt optimization framework\n                                              Analyzing Final Output Dependency on Step. Tofor multi-step LLM pipelines. ADOPT aims to\n                                                   simulate the effect of computing analytical partialimprove end-to-end performance by systematically\n                                                          derivatives, ADOPT must first determine how eachdetermining how each LLM step should adjust its\n                               LLM step functionally influences the final pipelineprompt based on the dependencies present in the\n                                                       output. ADOPT begins with a direct analysis stage,pipeline’s execution.\n                                                      in which an optimizer E1 examines the workflow  The framework operates in three stages. First,\n                                              code together with the prompts of all steps to inferADOPT analyzes execution traces to derive step-\n                                                    the overall task and the intended role of each stepspecific adjustment directions, which approximate\n                                                   within the pipeline.analytical partial derivatives with respect to individ-\n                                                   Building  on  this  structural  understanding,ual prompts and do not rely on backward reasoning\n                                ADOPT then performs a data-driven analysis us-by LLMs.  Second,  it decouples the estimation\n                                                   ing the traces collected from good cases. For eachof these adjustment directions from the optimiza-\n                               LLM step, optimizer E2 is provided with the direct-tion procedure, allowing each step to employ an\n                                                     analysis results, the step’s input–output pairs, andappropriate single-prompt optimizer while the sys-\n                                                      the corresponding end-to-end traces. The optimizertem searches for an effective combination of local\n                                                  analyzes how variations in the step’s output affectupdates at the pipeline level. Third, ADOPT allo-\n                                                     the final result, yielding a final output dependencycates optimization effort dynamically using Shap-\n                                          on that step. This dependency characterizes theley value–based attribution, focusing computation\n                                                        step’s functional contribution to successful execu-on the steps that contribute most to performance\n                                                        tion and its influence on the end-to-end behavior.improvement.\n  Together, these components enable stable, tar-   Computing Global Textual Gradient. For each\ngeted, and scalable optimization of prompts in com-   bad case, ADOPT first uses optimizer E3 to iden-\nplex multi-step LLM pipelines. We use LLM-based    tify the metric-defined discrepancy L between the\noptimizers to optimize multi-step LLM pipelines.   pipeline output and the ground truth.  This dis-\nTo avoid ambiguity, we will hereafter refer to LLM-   crepancy, referred to as textual loss or feedback\nbased optimizers simply as “optimizers.”             in our setting, explicitly enumerates the errors re-\n                                                  sponsible for the score reduction and provides a\n3.1  Dependency-aware Textual Gradient         fine-grained and interpretable description of the\n     Estimation                               mismatch, serving as the natural-language coun-\n                                                         terpart of an error signal.  In essence, the opti-A central challenge in optimizing multi-step LLM\n                                                 mization objective is to minimize this textual losspipelines is determining how end-to-end supervi-\n                               L = M(F(x; p1, . . . , pm)), that is, to eliminatesion should be decomposed into step-level update\n                                                     the discrepancy as much as possible.directions. ADOPT first analyzes how each step in-\n                                  ADOPT then invokes optimizer E4 to transformfluences the final output and derives the final output\n                                                           this textual loss into a global textual gradient. E4dependency on every step. Based on the feedback\n                                               performs a diagnostic analysis that explains whyproduced by the optimizers, which describes the\n                                                each discrepancy arises, producing a global descrip-discrepancy in textual form, ADOPT then gener-\n                                                      tion of the failure mode of the final output. Thisates a global optimization direction. Finally, using\n                                                      yields a distilled correction direction expressed atthe inferred dependencies, ADOPT decomposes\n                                                    the level of the final output itself. The resultingthis global direction into step-level update direc-\n                                                    global textual gradient functions as an analogue oftions.\n                                                     the derivative of L with respect to the textual loss,  During each training iteration, ADOPT runs the\n                                                          that is, gglobal = ∇textOutputL.pipeline Φ(x; p1, . . . , pm), where pi is the prompt\nof step i, on a minibatch from the training set and   Computing Local Textual Gradient. Given the\npartitions the resulting examples into good cases    global textual gradient and the final output depen-\n\n\n                                         4\n\ndencies inferred for each step, ADOPT uses opti-   modular and extensible, allowing different prompt\nmizer E5 to compute a local textual gradient for ev-   optimizers to be incorporated as interchangeable\nery node. For each step, E5 receives the global tex-   components.\ntual gradient, the step’s final output dependency, the      In this work, we consider two representative opti-\nstep-level input–output pair, and the final output,    mizers. The first is an instruction optimizer that up-\nand then produces a step-specific natural-language    dates only the instruction component of the prompt.\noptimization direction that constitutes the local tex-   The second jointly optimizes the instruction and au-\ntual gradient. Formally, this direction serves as the    tomatically selects representative examples to be in-\ntextual analogue of an analytical partial derivative,   cluded as in-context learning demonstrations. Each\nglocali  = ∇textpi L.                                round of step-level optimization produces multi-\n                                                    ple candidate prompts for each LLM step, which\n3.2  Prompt Optimization for Pipeline\n                                                     are subsequently evaluated and coordinated at the\nAfter deriving step-level local textual gradients,    pipeline level.\nthe remaining challenge is to convert these textual\n                                            Global Prompt Selection. The step-level prompt\noptimization directions into executable prompt up-\n                                                  optimization procedure produces multiple candi-\ndates while achieving effective coordination across\n                                                   date prompts for each LLM step, which gives rise\na multi-step LLM pipeline. Pipeline-level prompt\n                                                      to a pipeline-level selection problem. Let Pi =\noptimization requires not only that each step in-\n                                                      {p(1)i   , . . . , p(ni)i  } denote the candidate prompt setdependently updates its prompt according to its\n                                                 generated for step i. The objective is to select a\nlocal signal, but also that the resulting prompts\n                                            prompt configuration (p1, . . . , pm) ∈P1 × · · · ×interact coherently in an end-to-end manner. To\n                               Pm that maximizes the end-to-end performance ofaddress this, ADOPT adopts a prompt optimiza-\n                                                      the pipeline. Since the effect of a prompt at one step\ntion strategy that combines step-level prompt op-\n                                     may depend on the prompts chosen at other steps,\ntimization guided by local textual gradients with\n                                                           this problem cannot be decomposed into indepen-\npipeline-level prompt selection based on a search\n                                                 dent per-step decisions and instead constitutes a\nalgorithm.\n                                                   combinatorial optimization problem over the joint\nDecoupled Step-level Prompt Optimization. For                                             prompt space.\neach bad case, ADOPT uses the local textual gradi-                                   ADOPT addresses this challenge by formulating\nent of step i to generate a revised step output that                                                       pipeline-level prompt selection as a global search\nrepresents the desired behavior of this step under                                              problem and solving it with search-based optimiza-\nthe given input. This revised output specifies how                                                      tion methods. In this work, Bayesian Optimiza-\nthe step’s output should change in order to reduce                                                        tion is employed to efficiently explore the space of\nthe end-to-end loss and correct the final pipeline                                            prompt configurations under a limited evaluation\nerror. Formally, for step i with input xi, the revised                                                   budget.\noutput ˆyi is generated by optimizer E6 guided by\nthe local textual gradient.                                                    3.3  Shapley-based Resource Allocation\n  Across multiple bad cases, ADOPT aggregates\n                                           Prompt updates in multi-step LLM pipelines typi-pairs of step-level inputs and their corresponding\n                                                       cally yield uneven returns across steps. After sev-revised step outputs, forming a step-specific dataset\n                                                           eral optimization rounds, certain steps may become\n                                                     saturated and contribute little additional improve-          Di = {(x(k)i   , ˆy(k)i  )}Kk=1.           (3)\n                                               ment, while other steps remain critical to the end-\nThis dataset serves as supervision for updating the    to-end metric. Treating all steps equally, therefore,\nprompt pi of LLM step i using a single-prompt   wastes optimization budget. ADOPT addresses\noptimization procedure.                                 this issue by allocating step-level optimization re-\n   Crucially, ADOPT decouples the generation of    sources according to each step’s estimated con-\noptimization directions from the choice of the    tribution to end-to-end performance, and adopts\nprompt optimization algorithm.  For each LLM    Shapley-style attribution as a principled measure\nstep, any existing single-prompt optimizer can be    of contribution.\napplied to Di to update pi, without modifying the      Before optimization begins, each step has the\ndependency analysis or textual gradient compu-   same optimization resources, i.e., how many can-\ntation stages. This design makes the framework    didates can be generated during step-level opti-\n\n\n                                         5\n\nmization. At the end of each round, the global   most strongly influence end-to-end performance\nprompt selector evaluates a set of prompt config-   while maintaining stable computational cost across\nurations and records their end-to-end scores un-   rounds.\nder the task metric M. For each step i, we con-\nsider two prompt states, a weak prompt pweaki    rep-   4  Experiments\nresenting the prompt used in the previous round,\n                                                    4.1  Experimental Setup\nand a strong prompt pstrongi     representing the best-\nperforming prompt selected from the current can-  We compare ADOPT with four methods: Chain-\ndidate set Pi. Among all evaluated configurations,   of-Thought (CoT) (Wei et al., 2022), no COT,\nADOPT considers only those in which each step   MIPRO (Opsahl-Ong et al., 2024), GEPA (Agrawal\nadopts either its weak or strong prompt. These    et al., 2025). For CoT, we add \"let’s think step by\nconfigurations can thus be represented by a coali-   step\" to the prompt of each agent in the tested pro-\ntion S ⊆{1, . . . , m}, where i ∈S indicates that   grams.\nstep i uses pstrongi    and i /∈S uses pweaki      . Let P(S)    We implement two multi-step pipelines to thor-\ndenote the corresponding prompt assignment, and   oughly evaluate the effectiveness of our proposed\ndefine the value function                 ADOPT and baselines.  And we use two real-\n                                              world datasets, i.e., HotPotQA (Yang et al., 2018),\n          v(S) = M Φ(x; P(S))  .         (4)   HoVer (Jiang et al., 2020), as benchmarks for dif-\n                                                        ferent multi-step pipelines separately.\nThe Shapley value for step i measures its average\n                                               Throughout the experiments, every large lan-\nmarginal contribution across all coalitions,\n                                              guage model used in the multi-step pipeline and the\n  ϕi = X  w(S) v(S ∪{i}) −v(S)  ,  (5)   LLM-based optimizers is Qwen2.5-72B-Instruct.\n        S⊆[m]\\{i}                       We used it with the temperature set to 0, top p to 1.\n\n                 |S|!(m−|S|−1)!                        4.2  Main Results\nwhere w(S) =      m!         . Computing {ϕi} ex-\nactly requires evaluating v(S) for an exponential    Table 1 reports the overall performance of ADOPT\nnumber of coalitions, which is prohibitively expen-   and baseline methods on two real-world multi-\nsive because each evaluation corresponds to run-   step benchmarks. ADOPT consistently achieves\nning the full pipeline. ADOPT therefore estimates    the strongest performance across all evaluated\nstep contributions using Kernel SHAP, which ap-    datasets, demonstrating its effectiveness for opti-\nproximates Shapley values from a limited number   mizing multi-step LLM pipelines under end-to-end\nof observed coalitions. Concretely, ADOPT applies    supervision. Compared with prompting-based base-\nKernel SHAP to the prompt configurations that    lines such as No-CoT and CoT, ADOPT yields sub-\nhave already been evaluated during the pipeline-    stantial performance gains, indicating that merely\nlevel search. Each evaluated configuration pro-   encouraging step-by-step reasoning is still insuffi-\nvides a binary indicator vector z ∈{0, 1}m for its    cient to address errors arising from complex interac-\ncoalition membership and an observed score v(z).    tions among multiple pipeline steps. ADOPT also\nKernel SHAP then fits a weighted linear model   outperforms existing multi-prompt optimization\nover these samples and uses the fitted coefficients   methods, including MIPRO and GEPA. MIPRO\nas efficient approximations of Shapley-style contri-   generates candidate prompts only from correctly\nbutions.                                          solved examples and selects an optimal combi-\n  The resulting contribution estimates are used    nation among them, without iteratively refining\nonly for resource allocation in subsequent rounds.   prompts based on erroneous cases, while GEPA in-\nSteps with larger estimated contributions are as-   corporates bad cases but directly optimizes prompts\nsigned larger candidate budgets, while steps with    using only final feedback, analogous to the textual\nsmaller contributions receive fewer candidates.    loss in our framework, without explicitly reasoning\nThis reallocation preserves the overall evaluation   about how individual steps should be adjusted. In\nbudget. The number of pipeline-level prompt con-    contrast, ADOPT leverages textual loss to derive\nfigurations evaluated per round is kept fixed, and    global textual gradient and uses dependencies to de-\nno additional pipeline executions are introduced   compose end-to-end supervision into step-specific\nfor contribution estimation. In this way, ADOPT    optimization signals, i.e., local textual gradients,\nconcentrates optimization effort on the steps that    enabling more precise and coordinated step-level\n\n\n                                         6\n\nTable 1: Overall performance of ADOPT and baselines on different real-world datasets.\n\n\n         Dataset    No-COT  COT  MIPRO  GEPA  ADOPT-Instruct  ADOPT-Joint\n\n       HotPotQA     0.52     0.58    0.62     0.63         0.67            0.68\n       HoVer        0.55     0.58    0.62     0.63         0.69            0.71\n\n\n                                                     Table 2: Comparison of different resource allocation\n                                                    methods.\n\n\n                                                      Allocation  Average  Random  Shapley\n\n                                                            Iterations      6.6        6.7        3.7\n\n\n\n                                                    4.4  Ablation Studies\n\n                                                    Effect of Step-level Prompt Optimizers. ADOPT\n                                                decouples dependency-aware textual gradient es-\n                                                   timation from the choice of the step-level prompt\n                                                  optimization algorithm, allowing different single-\n         Figure 3: Convergence of ADOPT           prompt optimizers to be applied without modifying\n                                                    the upstream analysis. To assess the effect of this\n                                                    design, we instantiate ADOPT with two represen-\n                                                          tative optimizers at the step level: an instruction-\noptimization across the pipeline, which leads to\n                                                only optimizer that updates the natural-language\nconsistently better end-to-end performance. Taken\n                                                       instruction in each prompt, and a joint instruction-\ntogether, the results indicate that ADOPT provides\n                                               and-example optimizer that additionally selects rep-\na more effective mechanism for improving end-to-\n                                                       resentative in-context learning examples based on\nend performance in multi-step LLM systems than\n                                                    the same step-level supervision constructed from\nexisting methods.\n                                                    revised step outputs. Under identical local tex-\n                                                         tual gradients and step-level training data, both\n                                                    optimizers consistently improve end-to-end perfor-\n4.3  Convergence Analysis                   mance over the baselines, while the joint optimizer\n                                                  achieves further gains, indicating that enriching\n                                                     step prompts with selected examples can betterTo examine the convergence behavior of ADOPT,\n                                                    capture step-specific behaviors when accurate localwe track the end-to-end accuracy over succes-\n                                                    supervision is available. These results demonstratesive optimization iterations.  As shown in Fig-\n                                                          that ADOPT’s performance is not tied to a particu-ure 3, ADOPT demonstrates steady performance\n                                                              lar prompt optimizer and that its decoupled formu-improvements from the initial iteration, with ac-\n                                                         lation enables the framework to benefit from morecuracy increasing rapidly in the early stages and\n                                                    expressive optimization strategies as they becomecontinuing to improve over subsequent iterations.\n                                                         available.While minor fluctuations are observed in later it-\nerations, the overall trend remains stable and con-   Effect of resource allocation module. Table 2\nvergent, and the performance does not degrade as    evaluates the effect of different resource alloca-\noptimization proceeds. This behavior indicates that    tion strategies by measuring the average number\nthe step-level local textual gradients provide consis-   of optimization iterations required to reach a pre-\ntent and informative optimization signals, enabling    defined target performance. Uniform allocation\niterative prompt updates to accumulate construc-   and random allocation exhibit nearly identical con-\ntively at the pipeline level. The results suggest that   vergence behavior, indicating that naive redistribu-\nADOPT supports stable convergence in multi-step    tion of optimization resources does not meaning-\nLLM pipelines despite the discrete and interdepen-    fully improve efficiency. In contrast, the Shapley-\ndent nature of prompt optimization.                based resource allocation significantly reduces the\n\n\n                                         7\n\nrequired number of iterations, converging in 3.7 it-   global coordination. Experiments demonstrate that\nerations on average compared to over 6.6 iterations  ADOPT improves end-to-end performance.\nfor the other strategies. This result demonstrates\nthat contribution-aware allocation effectively con-\n                                          Referencescentrates optimization effort on the most impactful\nsteps, substantially accelerating convergence with-   Lakshya A Agrawal, Shangyin Tan, Dilara Soylu,\nout increasing the overall optimization budget.        Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Ar-\n                                                   nav Singhvi, Herumb Shandilya, Michael J Ryan,\n                                          Meng Jiang, and 1 others. 2025.  Gepa:  Reflec-\n5  Related Work                                                                    tive prompt evolution can outperform reinforcement\n                                                               learning. arXiv preprint arXiv:2507.19457.\nPrompt Optimization for Single-Step Tasks\n                                                      Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\nSingle-step prompt optimization typically refines a\n                                                           dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nstatic prompt. APE (Zhou et al., 2022) uses Monte        ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nCarlo search with LLM-generated candidates; Evo-     Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nPrompt (Guo et al., 2025) evolves prompts via      and Yi Zhang. 2023. Sparks of artificial general in-\n                                                                telligence: Early experiments with gpt-4. Preprint,LLM-driven mutation and crossover. OPRO (Yang\n                                                        arXiv:2303.12712.\net al., 2023) iteratively proposes variants based on\nperformance feedback. ProTeGi (Pryzant et al.,   Ching-An Cheng, Allen Nie, and Adith Swaminathan.\n2023) applies LLM-generated critiques as textual      2024. Trace is the next autodiff: Generative optimiza-\n                                                              tion with rich feedback, execution traces, and llms.\ngradients to guide beam search. AMPO (Yang                                                          In Advances in Neural Information Processing Sys-\net al., 2024b) explores multiple prompt trajectories      tems 38: Annual Conference on Neural Information\nin parallel to enhance robustness. While effective      Processing Systems 2024, NeurIPS 2024, Vancouver,\nfor static tasks, these methods do not exploit multi-     BC, Canada, December 10 - 15, 2024.\nstep pipeline structures.                                                      Chrisantha  Fernando,  Dylan  Banarse,  Henryk\nPrompt Optimization in Multi-Step Pipelines        Michalewski, Simon Osindero, and Tim Rock-\n                                                                täschel. 2023.   Promptbreeder:  Self-referential\nRecent advances extend prompt optimization to\n                                                       self-improvement via prompt evolution.  Preprint,\nmulti-step settings. MIPRO (Opsahl-Ong et al.,      arXiv:2309.16797.\n2024) jointly optimizes module instructions and\n                                                Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitaoexamples using a surrogate model and meta-\n                                                     Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\noptimization to assign inter-module credit. How-                                                                     jiu Yang. 2025. Evoprompt: Connecting llms with\never,   it assumes a  fixed  task decomposition,      evolutionary algorithms yields powerful prompt opti-\nlimiting  its  ability  to  target  true  bottlenecks.      mizers. Preprint, arXiv:2309.08532.\nTextGrad (Yuksekgonul et al., 2024) introduces\n                                                 Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles\ngradient-like textual feedback propagated across                                                    Dognin, Maneesh Singh, and Mohit Bansal. 2020.\ncomponents for global optimization, but relies on a      Hover: A dataset for many-hop fact extraction and\nstatic pipeline and cannot reassign responsibilities      claim verification. arXiv preprint arXiv:2011.03088.\nor handle loops. Trace (Cheng et al., 2024) lever-                                                  Xuanrui Lin, Chao Jia, Junhui Ji, Hui Han, and Us-\nages execution traces for structured feedback and     man Naseem. 2025. Ask, acquire, understand: A\nexplicit inter-module dependency modeling, yet      multimodal agent-based framework for social abuse\nalso assumes a fixed, acyclic workflow, restricting       detection in memes. In Proceedings of the ACM on\n                                           Web Conference 2025, WWW 2025, Sydney, NSW,\nits applicability to dynamic or iterative pipelines.\n                                                             Australia, 28 April 2025- 2 May 2025, pages 4734–\n                                                       4744. ACM.\n6  Conclusion\n                                                          Jiatong Ma, Linmei Hu, Rang Li, and Wenbo Fu. 2025.\nWe propose ADOPT, a dependency-aware prompt      Local: Logical and causal fact-checking with llm-\n                                                     based multi-agents. In Proceedings of the ACM onoptimization framework  for  multi-step LLM\n                                           Web Conference 2025, WWW 2025, Sydney, NSW,\npipelines that decomposes end-to-end supervision                                                             Australia, 28 April 2025- 2 May 2025, pages 1614–\ninto coherent step-level optimization signals via      1625. ACM.\ntextual gradients. By decoupling local prompt op-\n                                                         Krista Opsahl-Ong, Michael  J. Ryan, Josh Purtell,\ntimization from pipeline-level prompt selection,                                                  David Broman, Christopher Potts, Matei Zaharia,\nADOPT enables modular integration of existing      and Omar Khattab. 2024. Optimizing instructions\nprompt optimizers while maintaining effective      and demonstrations for multi-stage language model\n\n\n                                         8\n\nprograms. In Proceedings of the 2024 Conference on      prompt engineers.  In The Eleventh International\n  Empirical Methods in Natural Language Processing,      Conference on Learning Representations.\n  EMNLP 2024, Miami, FL, USA, November 12-16,\n  2024, pages 9340–9366. Association for Computa-\n   tional Linguistics.\n\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\n   Bansal. 2023. Grips: Gradient-free, edit-based in-\n   struction search for prompting large language models.\n   Preprint, arXiv:2203.07281.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\n  guang Zhu, and Michael Zeng. 2023.  Automatic\n  prompt optimization with\" gradient descent\" and\n  beam search. arXiv preprint arXiv:2305.03495.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n  Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\n  and Denny Zhou. 2022. Chain-of-thought prompting\n   elicits reasoning in large language models. In Ad-\n  vances in Neural Information Processing Systems 35:\n  Annual Conference on Neural Information Process-\n   ing Systems 2022, NeurIPS 2022, New Orleans, LA,\n  USA, November 28 - December 9, 2022.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\n  Quoc V Le, Denny Zhou, and Xinyun Chen. 2023.\n  Large language models as optimizers. arXiv preprint\n  arXiv:2309.03409.\n\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-\n   tian Han, Qizhang Feng, Haoming Jiang, Shaochen\n  Zhong, Bing Yin, and Xia Hu. 2024a. Harnessing the\n  power of llms in practice: A survey on chatgpt and\n  beyond. ACM Transactions on Knowledge Discovery\n  from Data, 18(6):1–32.\n\nSheng Yang, Yurong Wu, Yan Gao, Zineng Zhou,\n  Bin Benjamin Zhu, Xiaodi Sun, Jian-Guang Lou,\n  Zhiming Ding, Anbang Hu, Yuan Fang, Yunsong Li,\n  Junyan Chen, and Linjun Yang. 2024b. Ampo: Auto-\n   matic multi-branched prompt optimization. Preprint,\n  arXiv:2410.08696.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\n  William Cohen, Ruslan Salakhutdinov, and Christo-\n  pher D Manning. 2018.  Hotpotqa: A dataset for\n   diverse, explainable multi-hop question answering.\n   In Proceedings of the 2018 conference on empiri-\n   cal methods in natural language processing, pages\n  2369–2380.\n\nMert Yuksekgonul, Federico Bianchi, Joseph Boen,\n  Sheng Liu, Zhi Huang, Carlos Guestrin, and James\n  Zou. 2024. Textgrad: Automatic\" differentiation\" via\n   text. arXiv preprint arXiv:2406.07496.\n\nMert Yüksekgönül, Federico Bianchi, Joseph Boen,\n  Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin,\n  and James Zou. 2025. Optimizing generative AI by\n  backpropagating language model feedback. Nature,\n  639(8055):609–616.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\n  Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  Ba. 2022. Large language models are human-level\n\n\n                                         9",
"headers": [
"arXiv:2512.24933v1  [cs.CL]  31 Dec 2025",
"Adaptive Dependency-aware Prompt Optimization Framework for",
"Multi-Step LLM Pipeline"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2512.24933v1.pdf"
}