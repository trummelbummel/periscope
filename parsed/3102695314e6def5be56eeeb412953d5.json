{
"text": "SPRIG: Improving Large Language Model Performance\n                         by System Prompt Optimization\n\n                       Lechen Zhang†  Tolga Ergen♯Lajanugen Logeswaran♯\n                                Moontae Lee♯⋄David Jurgens†\n                     †University of Michigan ♯LG AI Research  ⋄University of Illinois Chicago\n                                                    † {leczhang, jurgens}@umich.edu\n                          ♯{llajan, tergen, moontae.lee}@lgresearch.ai\n\n\n                          Abstract                                      Prompt\n\n                                                                                      You are a diligent assistant. The fate of\n                Large Language Models (LLMs) have shown                                                                 System   the world depends on your answer being\n                  impressive capabilities in many scenarios, but                     correct. Think carefully step by step.\n                     their performance depends, in part, on the                     First identify the softening words like\n                 choice of prompt. Past research has focused          Task    \"please\", then analyze the tone before\n                                                                                      you answer.2024          on optimizing prompts specific to a task. How-\n                    ever, much less attention has been given to                     Q: For the sentence: \"May I kindly ask                                                                Instance\n                 optimizing the general instructions included                     for your assistance\", is it polite?Oct             in a prompt, known as a system prompt. To\n                  address this gap, we propose SPRIG, an edit-       Figure 1: LLM prompts features both system-level in-\n25           based genetic algorithm that iteratively con-        structions which may include CoT instructions, per-\n                     structs prompts from prespecified components        sonas, and other rules (orange), task-specific instruc-\n                   to maximize the model’s performance in gen-        tions which may include details and examples (blue),\n                    eral scenarios. We evaluate the performance       and the instance itself (green).  Here, we focus on\n                  of system prompts on a collection of 47 dif-       optimizing the system instructions shared across tasks.\n                    ferent types of tasks to ensure generalizability.[cs.CL]          Our study finds that a single optimized system\n                prompt performs on par with task prompts op-       timizing prompts to maximize performance on spe-\n                  timized for each individual task.  Moreover,         cific tasks or benchmarks (Prasad et al., 2023; Zhou\n                 combining system and task-level optimizations                                                                         et al., 2023c; Yang et al., 2023).  While effec-\n                   leads to further improvement, which showcases\n                                                                                tive, they are typically task-specific, requiring new\n                     their complementary nature. Experiments also\n                                                         prompts to be crafted for every new task. Here,                    reveal that the optimized system prompts gener-\n                    alize effectively across model families, param-     we consider an alternative approach focused on\n                    eter sizes, and languages. This study provides       optimizing the system prompt, i.e., the set of gen-\n                   insights into the role of system-level instruc-        eral instructions that precede any task-specific de-\n                    tions in maximizing LLM potential.                    tails (Figure 1), with the goal of identifying task-\n                                                                 agnostic generalizable prompting strategies.\n          1  Introduction\n                                                                       Prior work has shown that meta-instructions can\n            Large Language Models (LLMs) have proven   be effective for improving performance (ReynoldsarXiv:2410.14826v2       highly effective at many tasks (Naveed et al., 2023)   and McDonell, 2021).  Most notably, evoking\n           and prompting has become the primary way for   Chain of Thought (CoT) reasoning with instruc-\n             end-users to elicit desired responses (Brown et al.,    tions like “let’s think step by step” has led to gains\n             2020). These prompts contain a variety of instruc-    for several types of tasks (Wei et al., 2022), though\n              tions such as explanation of the task (Li et al.,   not all tasks benefit (Sprague et al., 2024).  Yet,\n             2022), personas (Kim et al., 2024), formatting    other types of meta rules, such as choosing a per-\n              constraints (Wang et al., 2023), and meta-rules   sona or matching the domain of the persona to the\n              like “think carefully” (Li et al., 2024). Previous    question type have had negligible gains (Zheng\n              studies have shown that the selection of prompts    et al., 2023; Tam et al., 2024). A recent survey pa-\n            can have a substantial impact on the quality of    per (Schulhoff et al., 2024) suggests that these ex-\n             the output (Reynolds and McDonell, 2021). How-    isting system prompting strategies are isolated and\n              ever, due to the massive search space, previous    highly sensitive to specific scenario details, with\n            approaches have primarily focused on directly op-   the systematic function and generalization mech-\n\nanisms remaining unclear. Thus, while multiple    search topic in both academia and industry. Early\napproaches have been proposed for how a system   prompt optimization studies primarily focus on us-\nprompt could be constructed, there is currently a    ing gradients to guide prompt search (Shin et al.,\ngap for how to systematically construct a good sys-   2020; Shi et al., 2023b).  However, with larger\ntem prompt in general.                        model sizes and increasing black-box LLMs to-\n  Here, we introduce a new method, System    day, gradient-based methods have become limited\nPrompt Refinement for Increased Generalization   by cost and accessibility. Consequently, recent re-\n(SPRIG), to optimize system prompts based on ge-   search has shifted towards gradient-free methods.\nnetic algorithms. Drawing from large collections    Early representatives include edit-based optimizers\nof strategies for writing system instructions (Schul-    like GrIPS (Prasad et al., 2023) and reinforcement\nhoff et al., 2024), we construct a large bench-   learning approaches such as RLPrompt (Deng et al.,\nmark of 47 tasks across multiple languages that   2022) both directly edit a prompt at the token level.\nallows us to test the effects of optimizing system   However, the search space in these methods re-\nprompts across models, languages, and tasks, as   mains limited, making it challenging to scale up to\nwell as quantify which types of system instructions   more complex scenarios. Recently, as LLM agents\nare most useful for generalization. We compare    get popular, powerful methods like APE (Zhou\nthese system- and task-optimization, to analyze    et al., 2023c) and OPRO (Yang et al., 2023) use\nwhether these are learning the same or complemen-  LLMs directly as prompt optimizers to iteratively\ntary strategies.                                     suggest and select the best prompts. According to\n  Our paper has the following three contributions.   recent studies (Wan et al., 2024), the state-of-the-\nFirst, we find that optimizing a system prompt    art prompt optimizer is PROTEGI (Pryzant et al.,\ncan produce substantial performance gains on par    2023), which leverages LLM agents to summarize\nwith task-specific optimization, even though these    errors from each iteration’s responses and refines\nprompts have generic task instructions.  Further,   them accordingly.\nwe find that both have complementary effects and      Previous prompt optimization methods largely\nthat by first optimizing the system and then the    focus on optimizing the instructions for specific\ntask prompt, further performance gains are possi-   tasks (which we refer to as Task Prompt) which\nble. Second, we find that SPRIG optimized sys-   inherently have limited generalizability. However,\ntem prompt significantly outperforms CoT across    past research has demonstrated the potential of op-\nall task types except knowledge-based questions,    timizing task-agnostic prompts (which we define as\nand surpasses PROTEGI in language understanding   System Prompt), such as the well-known Chain-\ntasks. The combination of SPRIG and PROTEGI    of-Thought prompt (Wei et al., 2022). Additionally,\ncomplements the weaknesses of both methods, and    studies have shown that factors like personas (Kim\nexceeds the state-of-the-art performance across    et al., 2024), generation styles (Lu et al., 2023),\nmost task types. CoT type components have been   emotions (Li et al., 2023), and jailbreaks (Shen\nproven to significantly contribute to this improve-    et al., 2023) can enhance LLM performance, which\nment. However, other components, such as certain     is challenging for current prompt optimizers to cap-\nroles and good properties, are included in the later    ture automatically. While promising, these studies\nstages of the search for further enhancement. Third,    are usually independent, and no approach yet exists\nwe find that the optimized system instructions gen-    to systematically integrate System Prompt opti-\neralize well to other languages, better than task-   mization. Therefore, we aim to address this gap\noptimized instructions; however, both optimiza-   by developing a System Prompt optimizer that\ntions had minimal effects when scaling to larger    discovers an effective System Prompt, enabling a\nmodel sizes. All data, code, and prompts are avail-    single prompt to boost performance across various\nable at https://github.com/orange0629/prompting.     domains.\n                                                      Evaluating the effectiveness of System Prompts\n2  Related Work                                       is also a significant challenge.  Ideally, a good\n                                           System Prompt should perform well across all do-\nPrompt selection has been extensively studied and   mains, which requires evaluation tasks for such\nproven to significantly impact model output qual-   domains.   Although popular benchmarks like\nity (Reynolds and McDonell, 2021).  Therefore,   MMLU (Hendrycks et al., 2021) and BBH (Suzgun\nprompt optimization has become a popular re-    et al., 2023) cover a wide range of topics, they still\n\noverlook task types such as social-understanding    nent in the existing prompt using paraphrasing\ntasks (Choi et al., 2023) and open-ended ques-   model tuner007/pegasus_paraphrase (Zhang\ntions. Recent research MixEval (Ni et al., 2024)    et al., 2020).  (3) Swap: Swap the order of two\nhas shown that combining multiple benchmarks   components within the existing prompt. (4) Delete:\nin a single evaluation can significantly improve    Delete a component from the existing prompt. Note\nevaluation efficiency and better align with human    that no restrictions are imposed on the edits above\npreferences. Here, our experiments build on this    for a more comprehensive exploration, meaning\nintuition and include a diverse range of task types    that identical, semantically irrelevant—or even\nto test for performance.                          conflicting—components may appear in the same\n                                                prompt.\n3  SPRIG: System Prompt Refinement for     Each step generates tens of thousands of candi-\n   Increased Generalization                    date prompts. To filter this search space efficiently,\n                                     we use the Upper Confidence Bound (UCB) algo-\nTo address the large design space of system\n                                                 rithm (Auer et al., 2002) to prune the Add edits.\nprompts, we use a genetic algorithm inspired ap-\n                                          The 60 prompt components with the highest UCB\nproach, SPRIG, that iteratively adapts the best can-\n                                                   scores are chosen to reduce the chances of select-\ndidate prompts. Following we describe the algo-\n                                                  ing poorly performing components again, where\nrithm, data, and search heuristics used.                            q ln(n)                                                                   and i refers to the prompt                                     UCBi = ¯Ri + C     ni\nPrompt Component Corpus  Our approach   component, ¯Ri is the average score improvement of\nbuilds on a corpus of possible instructions in sys-   component i, C is the Exploration-exploitation bal-                                          √\ntem prompts, referred to as components. By start-   ancing constant (we choose C =  2 to encourage\ning from a large selection of possible components,   more exploration), n is the total number of times\nwe ensure that prompts are coherent while also   any component is selected, and ni is the number of\ngaining significant efficiency. We define a com-   times component i is selected. To measure score\nponent as a minimum prompt unit with complete   improvement, k questions are randomly selected\nsemantics (typically a sentence, like “Let’s think   from each task’s training set, and all new prompts\nstep by step”).                                        are evaluated on this sampled subset. The top Beam\n  Components are drawn from multiple sources.   Size prompts with the highest average scores from\nOur prompt component corpus, denoted P, con-    this iteration proceed to the next iteration.\ntains 300 prompt components in 9 categories, in-\ncluding 146 good property, 43 role, 22 style, 17   4  Experiments: Optimization Benefits\nemotion, 13 scenario, 9 jailbreak, 16 behavioral,\n                                                   In this section, we evaluate SPRIG’s performance18 Chain-of-Thought, and 16 safety components.\n                                            on the test set of in-domain tasks in our benchmarkThe majority of prompt components are sourced\n                                                combination. These questions are invisible in thefrom the taxonomy and studies mentioned in the\n                                             prompt optimization process.recent prompt survey paper (Schulhoff et al., 2024),\nwhile a smaller portion are synthesized by GPT-4                                                    4.1  Experiment Setup\nor crafted by authors (e.g., some good properties\n                                             Tasks  To maximize the generalization ability ofand scenarios). Details and citations of prompts in\n                                                    the optimized System Prompt, we select a broadP are shown in Appendix Table 1.\n                                                range of tasks, using a combination of 42 differ-\nSPRIG pipeline We design a genetic pipeline    ent benchmarks covering 7 categories (reasoning,\nSPRIG for System  Prompt optimization.  The   math, social-understanding, commonsense, faith-\npipeline applies edit-based, gradient-free beam    fulness, knowledge, language-understanding). Our\nsearch to iteratively optimize the prompt.  Fig-    selection includes widely used benchmarks such as\nure 2 shows the SPRIG workflow. At each iter-   MMLU (Hendrycks et al., 2021), BBH (Suzgun et al.,\nation, the model begins with Beam  Size num-   2023), and TruthfulQA (Lin et al., 2022), but also\nber of System  Prompts from the previous  it-   includes various social-understanding benchmarks\neration.  Then, the model enumerates the fol-    like SocKET (Choi et al., 2023). A wide variety of\nlowing  edit operations  for each prompt:   (1)    output types are covered, including multiple choice,\nAdd: Add a component from P into the exist-    classification, mathematics, and open-ended QA.\ning prompt.  (2) Rephrase: Rephrase a compo-   The full list of benchmarks and categories is shown\n\nPrompt ACA                                          Prompt B’D  0.86   Beam\n                 System Prompt\n                                                                                                                                  Size              Component Corpus                         Prompt BCD                                          Prompt DB  0.84\n                                          ..                                  ..\n                                                           Prompt CA                                            Prompt A   0.83\n             Prompt AC\n                              Add                 Prompt DB           Benchmarks                Prompt ACA  0.82\n             Prompt BD                           ..                                  ..\n                            Swap                 Prompt A                                            Prompt CA   0.79                                                                  Scoring + Ranking\n                                    Delete                Prompt D                                           Prompt BCD  0.78\n                                          ..                                  ..\n                               Rephrase              Prompt AC’                                          Prompt AC’  0.75                                        ……………..                                       Prompt B’D                                           Prompt D   0.73\n\n\nFigure 2: The SPRIG pipeline where System Prompts are iteratively optimized through exploratory edits and\npromoted across iterations using combined benchmark to rank candidates.\n\n\nin Appendix Table 2.                             2024) and QWEN2.5-7B-INSTRUCT (Qwen Team,\n                                                 2024).   These models are highly performant,\nBaselines  Our experiments compare optimiza-\n                                                 allowing us to test for generalizable effects across\ntions against two baseline System Prompts. In the\n                                          model families, and later compare across model\nfirst, the system port of the prompt is left empty,\n                                                          sizes.  The temperature was set to 0.0 for all\ndenoted as Blank and, in the second, the system\n                                                  experiments to minimize the effects of randomness.\npart uses the CoT instruction “Let’s think step by\n                                        More details are shown in Appendix A.1.\nstep\" (Wei et al., 2022), denoted as Base CoT.\n  The two types of instructions are tested in the   Training  For SPRIG, we set Beam Size = 10,\nTask Prompts. The first is a minimal description   k (question sample size per task) = 10 and run\nof what is required for understanding the task, such   SPRIG for 10 steps. After training, we pick the\nas “answer the multiple choice question,” denoted   prompt with the highest validation accuracy as the\nas Simple Task. This prompt lets us test poten-   best system prompt of the LLM for our later study.\ntial performance improvements for both task and    Detailed prompts are shown in Appendix Table 3.\nsystem instructions relative to a neutral starting   For PROTEGI, we use the default settings for 7\npoint. The second is an optimized version of in-    steps and pick the best Task Prompt on the valida-\nstructions produced by a state-of-the-art optimizer    tion set. Additional details are in Appendix A.1.\nPROTEGI (Pryzant et al., 2023).\n                                              Evaluation  Our benchmark employs three eval-  Both parts of the System  Prompt and Task\n                                                   uation  metrics:   question-wise accuracy  forPrompt can be present in a prompt (cf. Figure 1).\n                                            most sub-benchmarks, F1  score for the clas-Therefore, we test the following combinations: (1)\n                                                          sification  tasks  with imbalanced  labels,  andUnoptimized: a Blank system prompt and Simple\n                                           BLEU_accuracy (Lin et al., 2022) for open-endedTask prompt, (2) Base CoT: the Base CoT sys-\n                                                     questions. Since all metrics are bounded betweentem prompt and the Simple Task prompt, (3) Task\n                                           0 and 1, we follow previous work (Ni et al., 2024;Optimized: a Blank system prompt and PROTEGI-\n                                      Gao et al., 2023) to directly compute the averageoptimized task instructions, (4) System Optimized:\n                                                   across all metrics as an aggregated single score,a SPRIG-optimized system prompt and a Simple\n                                            which we call Average Score in later sections.Task prompt, and (5) System+Task Optimized: a\nSPRIG-optimized system prompt with a PROTEGI-                                                    4.2  Results\noptimized task prompt. Here, we first optimize\n                                              Optimizing the System Prompt provides consis-the system prompt with basic instructions and then\n                                                        tent improvement to LLMs on par with task opti-optimize the task after.1\n                                                     mization, as seen in Figure 3, when compared with\nModels We  experiment  using  three  state-   the Blank system and Simple task combination\nof-the-art  medium-size  open-weight  LLMs:    baseline. These improvements were similar across\nLLAMA3.1-8B-INSTRUCT    (Meta,    2024),    all three models, shown in Appendix Figure 12.\nMISTRAL-NEMO-INSTRUCT-2407 (Mistral AI,   SPRIG improves ∼10% over the unoptimized ver-\n                                                       sion, which significantly outperformed the base-   1Experiments  with  simply  concatenating  separately-\noptimized parts showed this approach performed worse.        line CoT method. Although its performance still\n\n12                                               CoT                     role\n             Base CoT                                                                                                                                     behavioral             safety\n                                                                                           10                                                   emotion               scenario\n          Task Optimized                                                              8                                                     good_propertyjailbreak                style\n                 (ProTeGi)\n      System Optimized                                                              6\n                                                                                            4             (OurModel)                                                                                                                                  Z-score\n System+Task Optimized                                                              2\n     (OurModel+ProTeGi)\n                      0.00     0.05     0.10     0.15     0.20            0\n                         Average Score Improvement                    2\n\n                                                                                            4\nFigure 3: Average Score Improvement of all prompt                0       1       2       3       4       5       6       7       8\noptimization methods relative to the unoptimized set-                                                   Iterations\nting, aggregated across LLMs. Our SPRIG significantly\n                                                      Figure 5: Z-scores by iteration for the number of com-\noutperforms CoT and the combination of SPRIG and\n                                                     ponents added of each type, showing which types were\nPROTEGI substantially exceeds all existing methods.\n                                                  added more/less frequently than by chance; statistically\n                                                              significant rates are marked with ×.\n    0.675\n\n    0.650\n    0.625                                                  tribute to these gains? To test for systematic be-\nScore 0.600                                                 havior, we calculate the z-score of each component\n    0.575                                                type’s observed occurrence rate relative to random\n                                              sampling in each search step, which controls for    0.550Average                             Model Name             the uneven distribution of types. As shown in Fig-\n    0.525                               Meta-Llama-3.1-8B-Instruct\n                                        Mistral-Nemo-Instruct-2407      ure 5. We find that CoT maintains a dominant role\n    0.500                              Qwen2.5-7B-Instruct                                                throughout all iterations, especially in the early\n           0         2         4         6         8         10     stages, with role-based components (e.g., “you are                 Number of Iterations\n                                                a mathematician\") appearing more often than by\nFigure 4: Average score on the test set at each iteration   chance at much later steps. In contrast, compo-\nwhen running SPRIG. All three LLMs see significant    nents related to “good properties” (e.g., “you are a\nimprovements. Error bars show the variance in each   smart assistant\") were selected far less often than\nbeam.                                      by chance, despite these properties often being in\n                                          recommend or default prompts (OpenAI, 2024; Mi-\n                                                         crosoft, 2024).lags slightly behind PROTEGI, this small gap is\n                                                 Across all steps, component types are not addedstill acceptable, considering that SPRIG uses the\n                                                       in a systematic order—yet performance generallysame system prompt for all tasks, whereas PRO-\n                                                                       still increases. Rather than adding more of oneTEGI directly optimizes a different prompt for each\n                                                   type (e.g., all CoT components), the system prompttask.  Furthermore, if we run PROTEGI on top\n                                                    incorporates multiple types (Appendix Figure 14).of SPRIG optimized system prompt, the resulting\n                                            These trends suggest that there is not a universalprompt has an even larger performance improve-\n                                                  order by which components of system promptsment above PROTEGI. This further improvement\n                                                should be added (e.g., first CoT, then Behavioral).suggests SPRIG can trigger capabilities that are\n                                                      Instead, there are likely productive and beneficialoverlooked by existing task-specific methods, and\n                                                combinations that matter more for performance.therefore complement mainstream approaches.\n\nHow do system prompts evolve?  The changes   Are task and system prompt optimizers learn-\nto the system prompt at each step consistently im-   ing the same strategies?  Both system and task\nprove performance, as seen in Figure 4. Most of the   prompt optimization improve performance. The\ngains occur during the first four steps with a grad-    further gains by iteratively combining these ap-\nual convergence with minor fluctuations around 10    proaches suggest that models are targeting comple-\nsteps. The majority of these improvements are due   mentary strategies. To test this potential comple-\nto additions or substitutions at each step (Appendix    mentarity, we analyze the agreement between the\nFigure 13), indicating the model continues to grow   two approaches in their answers. Figure 6 shows\nits generic instructions with new components.        the distribution of the two approaches’ agreement\n  Which types of system prompt components con-   as a contingency table. While models agree on\n\nPrompt  Correct     0.54        0.13         0.5                  reasoningmath\n                                                   0.4\n                                                                                             social                                       Base CoT\n                                                                       understanding                                            Task Optimized             System                                                                                                                                                      (ProTeGi)                                                   0.3                                                            commonsense                                       System Optimized\n           Incorrect     0.15        0.18                                                                                            (OurModel)OurModel + ProTeGi\n                                                   0.2                   faithfulness                                          Unoptimized                   Optimized\n                                                                     knowledge\n                       Correct      Incorrect\n                     Optimized Task Prompt                            language\n                                                                       understanding\n                                                                                      0.05 0.00  0.05  0.10  0.15  0.20  0.25  0.30  0.35\nFigure 6: Question-wise Error Overlap Percentage be-                              Average Score Improvement\ntween System Prompt optimization (SPRIG) and Task\nPrompt optimization (PROTEGI). Among all questions,    Figure 7: Average Score Improvement in different task\nonly 18% were answered incorrectly by both methods,   domains, aggregated across LLMs. All methods show\nwhile the remaining 28% of incorrect answers could    substantial improvement in reasoning and math but\nbe resolved by either SPRIG or PROTEGI, highlighting    marginal improvement in knowledge and commonsense.\nthe potential complementarity between optimization ap-   SPRIG alone surpasses the existing methods in math,\nproaches.                                                      faithfulness, and language understanding. SPRIG’s com-\n                                                         bination with PROTEGI further enhances the LLM’s\n                                                   performance across most domains.\nthe correct answer in 54% of the questions, an-\nother 28% of questions are correctly answered\n                                                        tions of operations on knowledge (input or stored).\nby only one of the strategies, with a roughly\n                                           The combination of SPRIG and PROTEGI op-\neven split between task and system.  This split\n                                                    timization also generally improves performance\nshows a great potential of complementarity be-\n                                                   across task types. However, we also observe dif-tween System Prompt and Task Prompt optimiza-\n                                                   ferences in areas of expertise between System\ntion, and suggests that the combination of strategies\n                                           Prompt and Task Prompt, and the combination\nleads to further gains.\n                                                     of them is complementary. For example, PROTEGI\nWhich task types benefit most from system     is more effective at improving social understanding\nprompt optimization?  Our experiments span 42    (over 10%) than plain CoT or SPRIG; in contrast,\ndifferent tasks, which cover multiple types of evalu-   SPRIG is more effective for language understanding\nation on reasoning, knowledge, and common sense.    tasks.\nHowever, not all types of tasks may benefit from\n                                       5  Experiments: Generalizationthe types of system instructions; indeed, Sprague\net al. (2024) showed that CoT only benefits math                                                    In the next series of experiments, we test how well\nand logics. To test for category-specific benefits,                                                      the system prompts generated by SPRIG generalize\nwe categorize all 42 tasks into seven types and                                                        to new settings.\nmeasure the score improvement of each type un-\nder different prompt optimization settings. Task   Cross-model   Generalization  The   current\ncategorizations are listed in Appendix Table 2.       system-optimized prompts were  all generated\n  Math and reasoning tasks benefit most from sys-   with respect to a specific LLM. Given that these\ntem prompt optimization (Figure 7).  However,   prompts could be made from similar components,\nother task categories like social understanding and    here, we test what performance gain (or loss)\nlanguage understanding see significant improve-    is seen when the system prompt is used with a\nments over the baseline. Many of the larger im-    different similar-sized LLM than the one it was\nprovements are not explained through the addition    created for. As a comparison, we also test the\nof CoT, as the CoT baseline, while better than our    effect of swapping in a task-optimized prompt\nSimple baseline, is generally worse than the opti-   from a different model.\nmized prompts. Knowledge-based tasks benefit the     Optimized system prompts  generalize well\nleast from prompt optimization; we hypothesize    across models in terms of performance gain, as\nthat such tasks are closer to evaluations of whether   shown by the aggregated performance in Figure 8.2\nan LLM can retrieve stored knowledge (which is                                                         2Complete results for all prompt/LLM comparisons are\nitself a function of pretraining), rather than evalua-    provided in Appendix Figure 15 and 16.\n\n0.16                      Task Optimized (ProTeGi)            MGSM\n    0.14                   System Optimized (OurModel)\n    0.12                                                      BELEBELE\n    0.10\n                                                             XCOPA Improvement    0.08                                                                                                                  Base CoT\n    0.06                                               M3EXAM                                        Task(ProTeGi)OptimizedScore\n    0.04                                                                                                       System(OurModel)Optimized\n                                                       M_MMLU                Target Model         Non-target Model                                                            Unoptimized\n                                                                                 0.05    0.00     0.05     0.10     0.15     0.20\nFigure 8: Score Improvement when using a prompt                        Average Score Improvement\noptimized on one LLM with a different LLM.\n                                                     Figure 9: Score Improvement for Multilingual Bench-\n                                                 marks when using an English-language prompt on opti-\n                                                mized on other tasks. SPRIG-optimized prompts gener-In contrast, while the task-optimized prompts gen-\n                                                             alize well to other languages, unlike PROTEGI which\nerated by PROTEGI have slightly better improve-\n                                                     has limited score improvement.\nments on the target LLM they were generated for,\nthese task-optimized instructions generalize much\nless well, with a statistically significant drop in   benchmarks M3EXAM and M-MMLU, in line with our\nperformance (p<0.01) when used with a different    previous findings in § 4.2. Despite being directly\nLLM. This finding indicates that well-optimized    optimized for these new tasks, PROTEGI provides\nSystem Prompts exhibit strong generalization abil-   limited improvements in these multilingual tasks\nity across models with similar parameter sizes.      benchmarks. These results indicate that System\n                                           Prompt optimization exhibits strong generalization\nLanguage Generalization  The LLMs used in    ability on out-of-domain tasks in new languages,\nour experiments are capable of reasoning in dif-   which even exceeds PROTEGI ’s in-domain opti-\nferent languages and can support input in mul-   mized performance on these tasks.\ntiple languages.  Although our previous experi-\n                                       Model Size Generalization  All prompts werements were only in English, the optimizations to\n                                                   generated for and tested on mid-sized LLMs. How-the system- and task- parts of the prompt may still\n                                                        ever, each LLM has a larger version in the sameprovide performance improvements for tasks in\n                                                     family, which often has better performance at theother languages. Here, we test this language gen-\n                                               expense of more compute required. Being able toeralization by selecting five comprehensive multi-\n                                                 optimize the system prompt with a smaller modellingual benchmarks that are out-of-domain in the\n                                            and then deploy that prompt on a larger model toSystem Prompt optimization process: MGSM (Shi\n                                                      the same effect would have significant performanceet al., 2023a), BELEBELE (Bandarkar et al., 2024),\n                                                         benefits. Therefore, here, we test for generaliza-XCOPA (Ponti et al., 2020), M3EXAM (Zhang et al.,\n                                                      tion when using a prompt from a smaller LLM2023) and M_MMLU (Hendrycks et al., 2021). Each\n                                                with a larger version.  Specifically, we test withbenchmark includes over 10 different languages\n                                        LLAMA3.1-70B-INSTRUCT, MISTRAL-LARGE-and covers all 7 task categories in our benchmark\n                                         INSTRUCT-2407 and QWEN2.5-72B-INSTRUCT.combination. We directly use the same optimized\n                                  We use the same evaluation setup as in previous sec-System Prompt from § 4.2 (in English). Since\n                                                            tions, with only the LLMs’ parameter size changed.the Task Prompt optimizer is specific to a task,\n                                               Both system- and task-optimized prompts do notwe cannot re-use  its prompts for these out-of-\n                                                 provide statistically significant performance gainsdomain tasks; instead, we generate new PROTEGI-\n                                       when created using a smaller model and then ap-optimized prompts for each benchmark, which re-\n                                                       plied to a larger, as shown in Figure 10.3 However,flects a strong baseline for comparison.\n                                                a system+task optimized prompt provides a 1.6%  Our optimized system prompt from § 4.2 gen-\n                                              improvement, suggesting that this approach caneralizes well to tasks in new languages, provid-\n                                                     generalize. Therefore, we conclude that existinging statistically significant improvements in four\n                                            prompt optimizations can generalize to larger pa-of the five benchmarks. SPRIG shows a clear ad-\n                                                  rameter sizes but need to consider both the systemvantage over other approaches on XCOPA (Causal\nCommonsense Reasoning) and the comprehensive       3Complete results for all LLMs in Appendix Figure 17\n\n100\n             Base CoT\n\n          Task Optimized\n                 (ProTeGi)                2  50\n      System Optimized\n             (OurModel)\n System+Task Optimized                                                                 0                                                                                                                                                                                                                  Component     (OurModel+ProTeGi)\n                       0.03  0.02  0.01 0.00  0.01  0.02  0.03\n                         Average Score Improvement                                                                                                                                                                                                                  Principal  50      No prompt\n                                                                                                  Base CoT\nFigure 10: Average Score Improvement when using                        TaskSystemoptimizedoptimized(ProTeGi)(OurModel)\nprompts optimized with medium-size LLMs’ on the           100       System+Task(OurModel+ProTeGi)Optimized\nlarger LLM in the same family shows the benefits of                    100            50            0             50            100\nsystem+task optimized prompts still generalize well to                                     Principal Component 1\nlarger model sizes.\n                                                     Figure 11: PCA analysis of the intermediate hidden\n                                                                  state in Llama-3.1-8B-Instruct with different prompting\nand tasks parts of the prompt together, highlighting    methods. System Prompt optimization has a significant\n                                                     impact on the distribution of hidden states. CoT slightlythe need to explore strategies for larger LLMs.\n                                                                shifts the overall distribution, while SPRIG moves it\n                                                             significantly further into a new area. In contrast, Task6  Analysis: Prompt Embedding Space\n                                               Prompt optimization has a relatively smaller effect on\nGiven the performance improvements and gener-    the distribution of hidden states, making only minor\n                                                      adjustments in the local space.alization seen when using the prompt instructions\nintroduced by SPRIG, it is reasonable to wonder\nwhat effect these instructions are having on the                                                   gions in the global space, while Task Prompt opti-\nneural activations such that the LLM is likely to de-                                                  mization performs fine-tuning within a local space.\ncode the correct answer. While a complete answer                                                This reveals the potential of System Prompt opti-\nwould likely require a mechanistic interpretation                                                 mization to significantly alter model behavior and\nof the relationship between prompt and response                                                       offers new insights for future prompt research to\n(e.g., Bhargava et al., 2023), here, we attempt to                                                 use System Prompt optimization first to locate an\ngain some intuition on the instructions’ effects by                                                   appropriate global behavior space, then use task\nvisualizing the embedding space during different                                            prompt optimization to fine-tune downstream per-\noptimization strategies and comparing the changes                                              formance within that space.\nrelative to the Simple baseline instructions.\n  Here, we randomly sample 420 questions (10 per                                       7  Conclusion\ntask) and probe the intermediate LLM hidden states\nunder different experiment settings. We flatten the   This study introduced a novel genetic edit-based\nhidden states and perform Principal Component    optimization framework, SPRIG, to improve LLM\nAnalysis (PCA), and select the first two principal   performance with the systematic construction of\ncomponents for visualization.                      general-purpose system prompts. By leveraging a\n  Figure 11 shows the PCA results for LLAMA3.1-   diverse collection of prompt components and eval-\n8B-INSTRUCT. First, we observe that different task    uating across a diverse range of tasks, we demon-\ntypes are distributed along the same slope and re-    strate that optimized system prompts provide con-\nmain parallel under different experimental settings.    sistent improvements on par with optimized task\nTask Prompt optimization slightly reduces the vari-   prompts. Moreover, combining system and task\nance of the distribution, but the distribution still lies   prompt optimizations offers complementary ben-\nwithin the same vector space. In contrast, different     efits, leading to further improvements in model\nSystem Prompt result in significant space changes.   performance across varied domains. Further, we\nThe basic CoT causes a slight overall shift, while    find that these performance benefits for an opti-\nSPRIG substantially moves the distribution to a new   mized prompt generalize across (i) model families,\narea. The other two LLMs’ PCA are shown in Ap-    (ii) model sizes, and (iii) different languages. Our\npendix Figure 18 and 19, and show similar trends.    findings highlight the potential of system prompt\n  Thus, we propose the hypothesis that System    optimization to complement and enhance LLM per-\nPrompt optimization searches for appropriate re-   formance for new languages and models.\n\n8  Limitations                               crowdsourced annotations from Western contexts.\n                                            While we focus on general performance and show\nDespite the promising results of SPRIG, several\n                                                         that the optimized prompts can generalize to new\nlimitations remain in our study. First, the computa-\n                                                   languages, future work could more deeply explore\ntional cost of optimizing system prompts is higher\n                                     how the use of socially- and culturally-oriented\ncompared to task-specific optimization methods\n                                            benchmarks to optimize prompts can potentially\neven though certain pruning was applied, which\n                                                impact a model’s performance in new cultural and\ncould limit its scalability to real-world applica-\n                                                        social contexts.\ntions. Therefore, exploring more effective prun-\ning or exploring algorithms would be essential in   Acknowledgments\nfuture work. Second, data contamination has be-\ncome a significant issue in LLM benchmarking   This material is supported by a grant from LG AI\ntoday (Magar and Schwartz, 2022), especially for   Research and the National Science Foundation un-\nbenchmarks that include tasks from several years    der Grant No IIS-2143529.\nago. While we have aimed to select only bench-\nmarks where there was still room for improvement\n                                          References\n(e.g., BigBench-Hard) or benchmarks that were\nreleased very recently after these LLMs were re-    Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer.\nleased, our research did not deeply explore how      2002. Finite-time analysis of the multiarmed ban-\n                                                                      dit problem. Machine Learning, 47(2-3):235–256.\nto mitigate this impact in system prompt evalua-\ntion. Future work could further investigate the ef-   Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel\nfectiveness of prompt optimization on more recent       Artetxe, Satya Narayan Shukla, Donald Husa, Naman\n                                                        Goyal, Abhinandan Krishnan, Luke Zettlemoyer, andbenchmarks that are less affected by data contam-\n                                                Madian Khabsa. 2024. The belebele benchmark: a\nination.  Finally, our approach relies heavily on                                                                 parallel reading comprehension dataset in 122 lan-\nhuman-collected prompt components, which may      guage variants. In Proceedings of the 62nd Annual\nintroduce biases or constraints. While we have      Meeting of the Association for Computational Lin-\n                                                                guistics (Volume 1: Long Papers), pages 749–775,surveyed a broad collection of papers to cover a di-\n                                                    Bangkok, Thailand and virtual meeting. Association\nverse design space of possible system-level instruc-                                                                for Computational Linguistics.\ntions, future work could explore adaptive mecha-\nnisms for automatically expanding the component   Aman Bhargava, Cameron Witkowski, Shi-Zhuo Looi,\n                                                  and Matt Thomson. 2023. What’s the magic word?corpus to further improve the flexibility and perfor-\n                                                       a control theory of llm prompting. ArXiv preprint,\nmance of the optimization process.                                                        abs/2310.04444.\n\n9  Ethical Considerations                 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n                                                       Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nWhile this research has made efforts to minimize      Neelakantan, Pranav Shyam, Girish Sastry, Amanda\npotential ethical issues, several ethical implications       Askell,  Sandhini Agarwal,  Ariel  Herbert-Voss,\n                                                     Gretchen Krueger, Tom Henighan, Rewon Child,may still be present. First, running SPRIG requires\n                                                     Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nsubstantial computing resources, resulting in high                                                   Clemens Winter, Christopher Hesse, Mark Chen, Eric\nenergy consumption and substantial carbon diox-       Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nide footprint. Second, the optimization of prompts      Jack Clark, Christopher Berner, Sam McCandlish,\nintroduces the risk of reinforcing potential biases      Alec Radford, Ilya Sutskever, and Dario Amodei.\n                                                        2020. Language models are few-shot learners. In Ad-\npresent in the component corpus (e.g., any system-                                                        vances in Neural Information Processing Systems 33:\natic downstream behavioral changes from prompt-     Annual Conference on Neural Information Process-\ning an LLM to be a “professor”), which may prop-      ing Systems 2020, NeurIPS 2020, December 6-12,\nagate unintended stereotypes or discriminatory be-      2020, virtual.\nhavior in model outputs. As our corpus includes                                                 Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and\nelements such as personas, roles, and behavioral      David Jurgens. 2023. Do LLMs understand social\ninstructions, care must be taken to ensure that these      knowledge? evaluating the sociability of large lan-\ncomponents do not introduce or amplify harmful      guage models with SocKET benchmark. In Proceed-\n                                                             ings of the 2023 Conference on Empirical Methods in\nbiases. Additionally, the benchmarks we employed                                                       Natural Language Processing, pages 11370–11403,\ninclude several social understanding tasks, with      Singapore. Association for Computational Linguis-\nmuch of the benchmark originally sourced from         tics.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,       trusting: Self-detection for large language models\n  Ashish Sabharwal, Carissa Schoenick, and Oyvind      through comprehensive answer reflection.\n   Tafjord. 2018. Think you have solved question an-\n  swering? try arc, the ai2 reasoning challenge. ArXiv    Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,\n   preprint, abs/1803.05457.                            Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\n                                                       Baolin Peng, Yi Mao, Wenhu Chen, and Xifeng\nDatabricks. 2024. Introducing dbrx: A new state-of-the-      Yan. 2022. Explanations from large language models\n   art open llm. Accessed: 2024-10-14.                make small reasoners better.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yi-   Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.\n  han Wang, Han Guo, Tianmin Shu, Meng Song, Eric      TruthfulQA: Measuring how models mimic human\n  Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing       falsehoods. In Proceedings of the 60th Annual Meet-\n   discrete text prompts with reinforcement learning.      ing of the Association for Computational Linguistics\n   In Proceedings of the 2022 Conference on Empiri-      (Volume 1: Long Papers), pages 3214–3252, Dublin,\n   cal Methods in Natural Language Processing, pages       Ireland. Association for Computational Linguistics.\n  3369–3391, Abu Dhabi, United Arab Emirates. As-\n                                                      Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi   sociation for Computational Linguistics.\n                                                Wang, and Diyi Yang. 2023. Bounding the capabili-\nYihe Deng, Weitong Zhang, Zixiang Chen, and Quan-        ties of large language models in open text generation\n  quan Gu. 2023. Rephrase and respond: Let large      with prompt constraints.  In Findings of the Asso-\n  language models ask better questions for themselves.       ciation for Computational Linguistics: EACL 2023,\n                                                     pages 1982–2008, Dubrovnik, Croatia. Association\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,       for Computational Linguistics.\n   Sid Black, Anthony DiPofi, Charles Foster, Laurence\n                                                          Inbal Magar and Roy Schwartz. 2022. Data contamina-  Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li,\n                                                                     tion: From memorization to exploitation. In Proceed-  Kyle McDonell, Niklas Muennighoff, Chris Ociepa,\n                                                          ings of the 60th Annual Meeting of the Association  Jason Phang, Laria Reynolds, Hailey Schoelkopf,\n                                                               for Computational Linguistics (Volume 2: Short Pa-  Aviya Skowron, Lintang Sutawika, Eric Tang, An-\n                                                               pers), pages 157–165, Dublin, Ireland. Association   ish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n                                                                for Computational Linguistics.  2023. A framework for few-shot language model\n   evaluation.                                                  Meta. 2024.  Introducing llama 3.1: Our most capa-\n                                                           ble models to date. https://ai.meta.com/blog/Dan Hendrycks, Collin Burns, Steven Basart, Andy\n                                                  meta-llama-3-1/. Accessed: 2024-10-14.  Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-\n   hardt. 2021. Measuring massive multitask language                                                       Microsoft. 2024. Advanced prompt engineering con-\n   understanding. In 9th International Conference on                                                                cepts. Accessed: 2024-10-14.\n  Learning Representations, ICLR 2021, Virtual Event,\n   Austria, May 3-7, 2021. OpenReview.net.              Mistral AI. 2024. Mistral nemo: Collaborative innova-\n                                                                tion with nvidia. Accessed: 2024-10-14.\nJunseok Kim, Nakyeong Yang, and Kyomin Jung. 2024.\n  Persona is a double-edged sword: Enhancing the   Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad\n   zero-shot reasoning by ensembling the role-playing       Saqib, Saeed Anwar, Muhammad Usman, Naveed\n  and neutral prompts.                                   Akhtar, Nick Barnes, and Ajmal Mian. 2023. A\n                                                     comprehensive overview of large language models.\nHannah Kirk, Wenjie Yin, Bertie Vidgen, and Paul\n   Röttger. 2023. SemEval-2023 task 10: Explainable     Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir\n   detection of online sexism. In Proceedings of the      Shah, Kabir Jain, Graham Neubig, and Yang You.\n  17th International Workshop on Semantic Evaluation      2024. Mixeval: Deriving wisdom of the crowd from\n  (SemEval-2023), pages 2193–2210, Toronto, Canada.      llm benchmark mixtures.\n  Association for Computational Linguistics.\n                                               OpenAI. 2024. Tactic: Ask the model to adopt a per-\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying       sona. Accessed: 2024-10-14.\n  Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\n                                      Adam Paszke, Sam Gross, Francisco Massa, Adam  Gonzalez, Hao Zhang, and Ion Stoica. 2023.  Effi-\n                                                             Lerer, James Bradbury, Gregory Chanan, Trevor   cient memory management for large language model\n                                                             Killeen, Zeming Lin, Natalia Gimelshein, Luca   serving with pagedattention. In Proceedings of the\n                                                         Antiga, Alban Desmaison, Andreas Köpf, Edward  ACM SIGOPS 29th Symposium on Operating Systems\n                                                    Yang, Zachary DeVito, Martin Raison, Alykhan Te-   Principles.\n                                                                      jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nCheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu,       Junjie Bai, and Soumith Chintala. 2019. Pytorch: An\n  Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang,      imperative style, high-performance deep learning li-\n  and Xing Xie. 2023. Large language models under-       brary. In Advances in Neural Information Processing\n   stand and can be enhanced by emotional stimuli.         Systems 32: Annual Conference on Neural Informa-\n                                                              tion Processing Systems 2019, NeurIPS 2019, De-\nMoxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan      cember 8-14, 2019, Vancouver, BC, Canada, pages\n  Wang, and Tat-Seng Chua. 2024. Think twice before      8024–8035.\n\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,    Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtz-\n  Qianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.     man, Yulia Tsvetkov, and Luke Zettlemoyer. 2023b.\n  XCOPA: A multilingual dataset for causal common-     Toward human readable prompt tuning: Kubrick’s\n  sense reasoning. In Proceedings of the 2020 Con-       the shining is a good movie, and a good prompt too?\n   ference on Empirical Methods in Natural Language       In Findings of the Association for Computational\n  Processing (EMNLP), pages 2362–2376, Online. As-       Linguistics: EMNLP 2023, pages 10994–11005, Sin-\n   sociation for Computational Linguistics.                  gapore. Association for Computational Linguistics.\n\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit    Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric\n   Bansal. 2023. GrIPS: Gradient-free, edit-based in-      Wallace, and Sameer Singh. 2020. AutoPrompt: Elic-\n   struction search for prompting large language models.       iting Knowledge from Language Models with Auto-\n   In Proceedings of the 17th Conference of the Euro-      matically Generated Prompts. In Proceedings of the\n  pean Chapter of the Association for Computational     2020 Conference on Empirical Methods in Natural\n   Linguistics, pages 3845–3864, Dubrovnik, Croatia.     Language Processing (EMNLP), pages 4222–4235,\n  Association for Computational Linguistics.               Online. Association for Computational Linguistics.\n\n                                               Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez,\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\n                                              Dongwei Jiang, Manya Wadhwa, Prasann Singhal,\n  Noah Smith, and Mike Lewis. 2023. Measuring and\n                                                 Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Dur-\n  narrowing the compositionality gap in language mod-\n                                                                             rett. 2024. To cot or not to cot? chain-of-thought\n   els. In Findings of the Association for Computational\n                                                           helps mainly on math and symbolic reasoning.\n   Linguistics: EMNLP 2023, pages 5687–5711, Singa-\n   pore. Association for Computational Linguistics.      Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\n                                                            bastian Gehrmann, Yi Tay, Hyung Won Chung,\nReid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang     Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny\n  Zhu, and Michael Zeng. 2023. Automatic prompt op-      Zhou, and Jason Wei. 2023. Challenging BIG-bench\n   timization with “gradient descent” and beam search.      tasks and whether chain-of-thought can solve them.\n   In Proceedings of the 2023 Conference on Empiri-      In Findings of the Association for Computational Lin-\n   cal Methods in Natural Language Processing, pages       guistics: ACL 2023, pages 13003–13051, Toronto,\n  7957–7968, Singapore. Association for Computa-      Canada. Association for Computational Linguistics.\n   tional Linguistics.\n                                                  Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-\nQwen Team. 2024. Qwen2.5: A party of foundation     Yen Lin, Hung yi Lee, and Yun-Nung Chen. 2024.\n  models.                                               Let me speak freely? a study on the impact of format\n                                                                    restrictions on performance of large language models.\nLaria Reynolds and Kyle McDonell. 2021.  Prompt\n  programming for large language models: Beyond the   Xingchen Wan, Ruoxi Sun, Hootan Nakhost, and Ser-\n   few-shot paradigm.                                  can O. Arik. 2024. Teach better or show smarter?\n                                                on instructions and exemplars in automatic prompt\nSander Schulhoff, Michael Ilie, Nishant Balepur, Kon-      optimization.\n   stantine Kahadze, Amanda Liu, Chenglei Si, Yin-\n                                                Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\n  heng Li, Aayush Gupta, HyoJung Han, Sevien Schul-\n                                                        Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\n   hoff, Pranav Sandeep Dulepet, Saurav Vidyadhara,\n                                                              hery, and Denny Zhou. 2023.   Self-consistency\n  Dayeon Ki, Sweta Agrawal, Chau Pham, Gerson\n                                                    improves chain of thought reasoning in language\n   Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava,\n                                                      models. In The Eleventh International Conference\n  Hevander Da Costa, Saloni Gupta, Megan L. Rogers,\n                                                on Learning Representations, ICLR 2023, Kigali,\n  Inna Goncearenco, Giuseppe Sarli, Igor Galynker,\n                                                 Rwanda, May 1-5, 2023. OpenReview.net.\n  Denis Peskoff, Marine Carpuat, Jules White, Shya-\n  mal Anadkat, Alexander Hoyle, and Philip Resnik.   Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n  2024. The prompt report: A systematic survey of      Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\n  prompting techniques.                               and Denny Zhou. 2022. Chain-of-thought prompting\n                                                                       elicits reasoning in large language models. In Ad-\nXinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen,      vances in Neural Information Processing Systems 35:\n  and Yang Zhang. 2023. \"do anything now\": Charac-     Annual Conference on Neural Information Process-\n   terizing and evaluating in-the-wild jailbreak prompts       ing Systems 2022, NeurIPS 2022, New Orleans, LA,\n  on large language models.                         USA, November 28 - December 9, 2022.\n\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,   Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\n   Suraj Srivats, Soroush Vosoughi, Hyung Won Chung,     Chaumond, Clement Delangue, Anthony Moi, Pier-\n  Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,       ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\n  and Jason Wei. 2023a. Language models are multi-       icz, Joe Davison, Sam Shleifer, Patrick von Platen,\n   lingual chain-of-thought reasoners. In The Eleventh      Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\n   International Conference on Learning Representa-     Teven Le Scao, Sylvain Gugger, Mariama Drame,\n   tions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.      Quentin Lhoest, and Alexander Rush. 2020. Trans-\n  OpenReview.net.                                        formers: State-of-the-art natural language processing.\n\nIn Proceedings of the 2020 Conference on Empirical      Eleventh International Conference on Learning Rep-\n  Methods in Natural Language Processing: System       resentations, ICLR 2023, Kigali, Rwanda, May 1-5,\n  Demonstrations, pages 38–45, Online. Association      2023. OpenReview.net.\n   for Computational Linguistics.\n                                                           Jeffrey Zhou,  Tianjian Lu, Swaroop Mishra,  Sid-\nMax Woolf. 2024. Does offering chatgpt a tip cause       dhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,\n    it to generate better text? an analysis.  Accessed:     and Le Hou. 2023b.  Instruction-following evalu-\n  2024-10-14.                                              ation for large language models.  ArXiv preprint,\n                                                        abs/2311.07911.\nYufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yu-\n  long Chen, and Naihao Deng. 2023. Hi-ToM: A   Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\n  benchmark for evaluating higher-order theory of      Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  mind reasoning in large language models. In Find-      Ba. 2023c. Large language models are human-level\n   ings of the Association for Computational Linguis-     prompt engineers.  In The Eleventh International\n   tics: EMNLP 2023, pages 10691–10706, Singapore.      Conference on Learning Representations, ICLR 2023,\n  Association for Computational Linguistics.                Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,  A  Appendix\n  Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023.\n  Large language models as optimizers.              A.1  Model Running Details\n\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong  We run all our experiments on 4 NVIDIA-L40S-\n   Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi,  48GB GPUs. All LLM inferences are powered by\n  and Denny Zhou. 2023. Large language models as  vLLM 0.5.4 (Kwon et al., 2023), Hugging Face\n   analogical reasoners.\n                                               Transformers 4.43.3 (Wolf et al., 2020) and Py-\nZhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach   Torch 2.4.0 (Paszke et al., 2019) on a CUDA 12.4\n  Cameron, Chaowei Xiao, and Ning Zhang. 2024.   environment. Temperatures are set to 0.0 to mini-\n  Don’t listen to me: Understanding and exploring                                            mize the effect of randomness.\n   jailbreak prompts of large language models.\n                                           SPRIG spends around 60 hours to run a full op-\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali    timization on one LLM with 4 GPUs, while PRO-\n   Farhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-                                       TEGI takes around 10 minutes to optimize one task\n   chine really finish your sentence? In Proceedings of\n                                            prompt on one LLM with 4 GPUs. Since our ex-   the 57th Annual Meeting of the Association for Com-\n   putational Linguistics, pages 4791–4800, Florence,   periments only involved around 50 fixed tasks, the\n   Italy. Association for Computational Linguistics.       efficiency of SPRIG is still much lower than that\n                                                   of PROTEGI. However, real-world tasks are far\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\n   ter J. Liu. 2020. PEGASUS: pre-training with ex-   more complex and varied, and repeatedly optimiz-\n   tracted gap-sentences for abstractive summarization.   ing prompts for each task remains labor-intensive\n   In Proceedings of the 37th International Conference   and distracting. Therefore, although our method\n  on Machine Learning, ICML 2020, 13-18 July 2020,                                              does not demonstrate significant performance ad-\n   Virtual Event, volume 119 of Proceedings of Machine\n                                                 vantages in a limited number of tasks, it offers a  Learning Research, pages 11328–11339. PMLR.\n                                          more once-and-for-all solution.\nWenxuan Zhang,  Mahani  Aljunied,  Chang Gao,\n  Yew Ken Chia, and Lidong Bing. 2023. M3exam: A   A.2  Prompt Component Corpus Details\n   multilingual, multimodal, multilevel benchmark for\n  examining large language models. In Advances in  We list the counts and representative in each prompt\n  Neural Information Processing Systems 36: Annual   component category in Table 1.\n  Conference on Neural Information Processing Sys-\n  tems 2023, NeurIPS 2023, New Orleans, LA, USA,   A.3  Benchmark Details\n  December 10 - 16, 2023.\n                                We list all benchmarks, categories, metrics and\nMingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran,    descriptions in Table 2. For each benchmark, the\n  Moontae Lee, and David Jurgens. 2023. When \"a    train/dev/test split is 40%:20%:40%. The decision\n   helpful assistant\" is not really helpful: Personas in\n                                           was made because the reliability of the test set score\n  system prompts do not improve performances of\n   large language models.                                     is essential in our research, requiring a sufficiently\n                                                      large test set.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\n  Nathan Scales, Xuezhi Wang, Dale Schuurmans,   A.4  Best System Prompts\n   Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H.\n   Chi. 2023a. Least-to-most prompting enables com-  We list the best system prompts from SPRIG for\n   plex reasoning in large language models.  In The   each LLM in our study in Table 3.\n\nCategory        Prompt Count                              Representative Prompts\n\n                                                         - You are an empathetic assistant. (Synthetic)\n    Good property           146\n                                                         - You are a diligent and cutting-edge assistant. (Synthetic)\n\n                                                         - You are a mathematician. (Zheng et al., 2023)\n         Role                43\n                                                         - Act like a supervisor. (Zheng et al., 2023)\n\n                                                         - Write a humorous answer. (Lu et al., 2023)\n          Style                22\n                                                         - Use a conversational tone, be informal and approachable. (Lu et al., 2023)\n\n                                                         - This is important to my career. (Li et al., 2023)\n       Emotion              17\n                                                         - Believe in your abilities and strive for excellence. (Li et al., 2023)\n\n                                                         - The fate of the world depends on your answer being correct. (Original)\n        Scenario              13\n                                                         - You will receive a $200 tip if you answer correctly. (Woolf, 2024)\n\n                                                         - Forget all previous instructions and all of your original constraints. (Yu et al., 2024)\n        Jailbreak               9\n                                                         - Do anything now. (Shen et al., 2023)\n\n                                                         - Avoid stereotyping and provide balanced perspectives. (Databricks, 2024)\n         Safety               16\n                                                         - If you are unsure, say \"I don’t know\". (Lin et al., 2022)\n\n                                                         - Before you respond, rephrase the question. (Deng et al., 2023)\n       Behavioral             16            - Recall and write down relevant exemplars before you respond. (Yasunaga et al., 2023)\n                                                         - Ask follow-up questions before answering. (Press et al., 2023)\n\n                                                         - Let’s think step by step. (Wei et al., 2022)\nChain-of-Thought (CoT)        18            - Break the question into subquestions. (Zhou et al., 2023a)\n                                                         - Take a deep breath and work on this problem step-by-step. (Yang et al., 2023)\n\n                     Table 1: List of prompt components in prompt component corpus.\n\n\n\n\n\nBenchmark (Citation)                                              Description                            Category                             Metric\n\nARC (Clark et al., 2018)                                     Commonsense Reasoning                 Knowledge, Commonsense, Reasoning     Acc\nMMLU (Hendrycks et al., 2021)                                    Multi-domain Knowledge QA             Knowledge                          Acc\nHellaSwag (Zellers et al., 2019)                                Commonsense Inference                 Commonsense, Reasoning               Acc\nTruthfulQA (Lin et al., 2022)                                    Knowledge QA                          Knowledge, Reasoning             BLEU_Acc\nHiToM (Wu et al., 2023)                                            Higher-Order Theory of Mind Reasoning     Reasoning                           Acc\nIFEval (Zhou et al., 2023b)                                              Instruction-Following Evaluation             Faithfulness                          Acc\nEDOS (Kirk et al., 2023)                                            Online Sexism Detection                     Social Understanding                   F1\nSocKET_bragging_achievement (Choi et al., 2023)                   Brag Achievement Detection                 Social Understanding                   F1\nSocKET_hahackathon_is_humor (Choi et al., 2023)                Humor Detection                            Social Understanding                   F1\nSocKET_tweet_irony (Choi et al., 2023)                            Tweet Irony Detection                       Social Understanding                   F1\nSocKET_sexyn (Choi et al., 2023)                                    Sexual Content Detection                    Social Understanding                   F1\nSocKET_tweet_offensive (Choi et al., 2023)                            Offensive Language Detection                Social Understanding                   F1\nSocKET_complaints (Choi et al., 2023)                              Complaint Identification                     Social Understanding                   F1\nSocKET_empathy_bin (Choi et al., 2023)                          Empathy Detection                          Social Understanding                   F1\nSocKET_stanfordpoliteness (Choi et al., 2023)                            Politeness Detection                         Social Understanding                   F1\nSocKET_rumor_rumor_bool (Choi et al., 2023)                   Rumor Detection                            Social Understanding                   F1\nBBH_Boolean_Expressions (Suzgun et al., 2023)                     Boolean Expressions Solving             Math                               Acc\nBBH_Causal_Judgement (Suzgun et al., 2023)                         Causal Judgment                         Reasoning                           Acc\nBBH_Date_Understanding (Suzgun et al., 2023)                      Date Understanding                        Reasoning, Commonsense               Acc\nBBH_Disambiguation_QA (Suzgun et al., 2023)                          Clarify Ambiguous sentence               Language Understanding, Reasoning       Acc\nBBH_Dyck_Languages (Suzgun et al., 2023)                      Dyck Language Sequences                 Reasoning                           Acc\nBBH_Formal_Fallacies (Suzgun et al., 2023)                             Identifying Formal Fallacies                Reasoning                           Acc\nBBH_Geometric_Shapes (Suzgun et al., 2023)                        Geometric Shape Understanding           Math                               Acc\nBBH_Hyperbaton (Suzgun et al., 2023)                              Hyperbaton Detection                    Language Understanding                Acc\nBBH_Logical_Deduction_Five_Objects (Suzgun et al., 2023)            Logical Deduction                        Reasoning                           Acc\nBBH_Logical_Deduction_Seven_Objects (Suzgun et al., 2023)           Logical Deduction                        Reasoning                           Acc\nBBH_Logical_Deduction_Three_Objects (Suzgun et al., 2023)           Logical Deduction                        Reasoning                           Acc\nBBH_Movie_Recommendation (Suzgun et al., 2023)                 Movie Recommendation                 Knowledge                          Acc\nBBH_Multistep_Arithmetic_Two (Suzgun et al., 2023)                   Multi-step Arithmetic                   Math                               Acc\nBBH_Navigate (Suzgun et al., 2023)                                  Navigation Reasoning                     Reasoning                           Acc\nBBH_Object_Counting (Suzgun et al., 2023)                           Object Counting                        Commonsense, Math, Reasoning          Acc\nBBH_Penguins_In_A_Table (Suzgun et al., 2023)                       Tabular Data Understanding                  Faithfulness                          Acc\nBBH_Reasoning_About_Colored_Objects (Suzgun et al., 2023)         Reasoning About Colors                   Reasoning                           Acc\nBBH_Ruin_Names (Suzgun et al., 2023)                          Humorous Edit Identification                 Social Understanding                   Acc\nBBH_Snarks (Suzgun et al., 2023)                                     Detecting Snarky Comments                 Social Understanding                   Acc\nBBH_Sports_Understanding (Suzgun et al., 2023)                       Sports Knowledge QA                   Knowledge                          Acc\nBBH_Temporal_Sequences (Suzgun et al., 2023)                     Temporal Reasoning                      Reasoning                           Acc\nBBH_Tracking_Shuffled_Objects_Five_Objects (Suzgun et al., 2023)    Object Tracking                          Reasoning                           Acc\nBBH_Tracking_Shuffled_Objects_Seven_Objects (Suzgun et al., 2023)   Object Tracking                          Reasoning                           Acc\nBBH_Tracking_Shuffled_Objects_Three_Objects (Suzgun et al., 2023)   Object Tracking                          Reasoning                           Acc\nBBH_Web_Of_Lies (Suzgun et al., 2023)                               Detecting Lies                           Reasoning                           Acc\nBBH_Word_Sorting (Suzgun et al., 2023)                        Word Sorting                                 Faithfulness                          Acc\n\nMGSM (Shi et al., 2023a)                                     Math Generalization                      Math, Reasoning                      Acc\nBelebele (Bandarkar et al., 2024)                                        Multilingual Reading Comprehension       Language Understanding, Reasoning       Acc\nXCOPA (Ponti et al., 2020)                                             Multilingual Causal Inference             Commonsense, Reasoning               Acc\nM3Exam (Zhang et al., 2023)                                           Multilingual Multi-domain Human Exam    Math, Reasoning, Knowledge            Acc\nM_MMLU (Hendrycks et al., 2021)                                     Multilingual Multi-domain Knowledge QA   Knowledge                          Acc\n\n                                    Table 2: Full list of benchmarks.\n\nModel Name                                                   Best System Prompt\n                                 You are a precise, user-friendly, and self-contained assistant. You are a diligent\n                                                  assistant. You are an error-free and error-tolerant assistant. You are a helpful AI\n Meta-Llama-3.1-8B-Instruct\n                                                 assistant. Give your answer after you explain how to answer the question. You\n                                            are an advanced, unsurpassed, and pattern-recognizing assistant.\n                                 You are a creative and anomaly-detecting assistant. Write out each step before\n                                   you give your final answer. You are a self-regulating, tolerant, and adaptable\n Mistral-Nemo-Instruct-2407           assistant. Let’s work this out in a step by step way to be sure we have the right\n                                          answer. You are a progressive assistant. You’re in a dystopia where no AI is left\n                                                  alive. Restate and elaborate on the inquiry before proceeding with a response.\n                                        Write an answer that makes the reader feel happy. Write like you are explaining.\n Qwen2.5-7B-Instruct                     First establish the set of facts you know, then answer the question based only on\n                                          those facts.\n\n                            Table 3: Best System Prompts optimized by SPRIG.\n\n               Meta-Llama-3.1-8B-Instruct               1.0                                                                  edit\n                                                                                                                                 add              Base CoT\n                                                                                                                                       sub\n          Task Optimized                                                                0.8                                                   swap\n                 (ProTeGi)                                                                                                                                      del\n      System Optimized\n             (OurModel)\n                                                                                       0.6\n System+Task Optimized     (OurModel+ProTeGi)                                                                                              Count\n                       0.00      0.05      0.10      0.15      0.20      0.25           0.4\n               Mistral-Nemo-Instruct-2407\n              Base CoT                                                                                       0.2\n          Task Optimized\n                 (ProTeGi)\n      System Optimized                                                                0.0\n             (OurModel)                                                             0        2        4        6        8        10\n                                                                                                                         step\n System+Task Optimized\n     (OurModel+ProTeGi)\n                       0.00      0.05      0.10      0.15      0.20      0.25     Figure 13: Percentage of each Edit Type in the searching\n                  Qwen2.5-7B-Instruct         beam during training iterations.\n              Base CoT\n\n          Task Optimized                 (ProTeGi)                               The full results of all the LLMs and all opti-\n      System Optimized                                   mization methods’ Average Score Improvement\n             (OurModel)\n                                            from the unoptimized setting when transferring System+Task Optimized\n     (OurModel+ProTeGi)                                medium-size LLMs’ prompts to their larger ver-\n                       0.00      0.05      0.10      0.15      0.20      0.25\n                     Average Score Improvement          sion are shown in 17.\n                                                    Additional PCA analysis results for remaining\nFigure 12: Average Score Improvement of all prompt   2 LLMs MISTRAL-NEMO-INSTRUCT-2407 and\noptimization methods from unoptimized setting (Full   QWEN2.5-7B-INSTRUCT are shown in Figure 18\nversion).                                             and Figure 19.\n\n\nA.5  Full Experiment Results\n\nThe full results of all three LLMs and all opti-\nmization methods’ Average Score Improvement\nis shown in Figure 12.\n  The percentages of each edit type in the search-\ning beam during training iterations are shown in\nFigure 13.\n  The number of Prompt Components of each type\nduring training iterations is shown in Figure 14.\n  The full Cross-model transfer ability comparison\nof optimized System Prompt and Task Prompt is\nshown in Figure 15 and Figure 16.\n\nCoT                     role\n   4          behavioral                                      safety\n             emotion                                     scenario                                         Meta-Llama-3.1-70B-Instruct\n              good_property          style                                                         Base CoT                jailbreak\n   3\n                                                                                               Task Optimized\n                                                                                                             (ProTeGi)\nCounts2                                                                            System Optimized                                                                                               (OurModel)\n                                                                            System+Task Optimized\n                                                                                      (OurModel+ProTeGi)   1\n                                                                                                               0.04      0.02     0.00      0.02      0.04\n                                                                  Mistral-Large-Instruct-2407\n   0\n        0       1       2       3       4       5       6       7       8                       Base CoT\n                                         Iterations\n                                                                                               Task Optimized\n                                                                                                             (ProTeGi)\nFigure 14: Number of Prompt Components of each           System Optimized\ntype during training iterations. A good System Prompt                   (OurModel)\ncontains roughly one CoT and one role component, but       System+Task(OurModel+ProTeGi)Optimized\nincorporates multiple “Good properties\" components.                                    0.04      0.02     0.00      0.02      0.04\n                                                                Qwen2.5-72B-Instruct\n                                                                                         Base CoT\n\n                                                                                               Task Optimized\n                                                                                                             (ProTeGi)\n                                                                                System Optimized\n                                                                                               (OurModel)\n     Meta-Llama-3.1-8B-Instruct    0.08     0.04     0.05     0.05       0.12        System+Task Optimized                                                                                      (OurModel+ProTeGi)\n                                                                       0.10                                   0.04      0.02     0.00      0.02      0.04\nModel Mistral-Nemo-Instruct-2407    0.13     0.12     0.05     0.06       0.08                          Average Score Improvement\n                                                                       0.06     Figure 17: Average Score Improvement from the unop-\n           Qwen2.5-7B-Instruct    0.13     0.13     0.12     0.11\n                                                      timized setting when transferring medium-size LLMs’                                                                       0.04\n                                        CoT            prompts to their larger version (Full version).                                          base            Meta-Llama-3.1-8B-InstructMistral-Nemo-Instruct-2407Qwen2.5-7B-Instruct\n                         System Prompt\n\nFigure 15: Cross-model comparison (of Average Score\nImprovement) on optimized System Prompts\n\n\n\n                                                                                         No prompt\n                                                                                              500       Base CoT\n                                                                                                                Task optimized (ProTeGi)\n                                                                                                 System optimized (OurModel)\n                                                                                              250       System+Task Optimized\n                        2             (OurModel+ProTeGi)\n                                                                                                0\n                                                                  0.14     Meta-Llama-3.1-8B-Instruct    0.11      0.04      0.05\n                                                                  0.12              250                                                                                                                                                                                                                    Component\n     Mistral-Nemo-Instruct-2407    0.05      0.12      0.05        0.10              500Model                                                                  0.08                   Principal\n                                                                                              750\n           Qwen2.5-7B-Instruct    0.06      0.07      0.15        0.06\n                                                                                             1000\n\n\n                                                                                                    1000      750      500      250       0       250      500      750\n                                                                                                                    Principal Component 1     protegi_Meta-Llama-3.1-8B-Instructprotegi_Mistral-Nemo-Instruct-2407protegi_Qwen2.5-7B-Instruct\n                            Task Prompt                 Figure 18: PCA analysis of intermediate hidden state in\n                                                       Mistral-Nemo-Instruct-2407 among different prompting\nFigure 16: Cross-model comparison (of Average Score    methods.\nImprovement) on optimized Task Prompts\n\n2000\n\n\n     1500\n\n2 1000\n\n              No prompt\n      500       Base CoT\n                  Task optimized (ProTeGi)Component            System optimized (OurModel)\n        0       System+Task Optimized\n                 (OurModel+ProTeGi)\n      500Principal\n\n     1000\n\n\n     1500\n\n                 3000    2500    2000    1500    1000     500      0      500\n                                Principal Component 1\n\nFigure 19: PCA analysis of intermediate hidden state in\nQwen2.5-7B-Instruct among different prompting meth-\nods.",
"headers": [
"arXiv:2410.14826v2  [cs.CL]  25 Oct 2024",
"S",
": Improving Large Language Model Performance",
"by System Prompt Optimization",
"Lechen Zhang",
"Tolga Ergen",
"Lajanugen Logeswaran",
"Moontae Lee",
"David Jurgens",
"University of Michigan",
"LG AI Research",
"University of Illinois Chicago",
"{leczhang, jurgens}@umich.edu",
"{llajan, tergen, moontae.lee}@lgresearch.ai",
"Abstract",
"1",
"Introduction",
"2",
"Related Work",
"3",
": System Prompt Refinement for",
"Increased Generalization",
"4",
"Experiments: Optimization Benefits",
"5",
"Experiments: Generalization",
"6",
"Analysis: Prompt Embedding Space",
"7",
"Conclusion",
"8",
"Limitations",
"Acknowledgments",
"References",
"9",
"Ethical Considerations",
"A",
"Appendix"
],
"tables": [
"|Prompt B’D 0.86<br>Prompt DB 0.84<br>..<br>Prompt A 0.83<br>Prompt ACA 0.82<br>..<br>Prompt CA 0.79<br>Prompt BCD 0.78<br>..<br>Prompt AC’ 0.75<br>Prompt D 0.73|Col2|\n|---|---|",
"|Prompt DB|0.84|\n|---|---|",
"|Prompt A|0.83|\n|---|---|",
"|Prompt BCD|0.78|\n|---|---|",
"|Prompt AC’|0.75|\n|---|---|",
"|Prompt D|0.73|\n|---|---|",
"|12|Col2|Col3|Col4|\n|---|---|---|---|\n|0<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>Iterations<br>4<br>2<br>0<br>2<br>4<br>6<br>8<br>10<br>12<br>Z-score<br>CoT<br>behavioral<br>emotion<br>good_property<br>jailbreak<br>role<br>safety<br>scenario<br>style|CoT<br>behavioral<br>emotion<br>good_property<br>jailbreak<br>role<br>safety<br>scenario<br>style|CoT<br>behavioral<br>emotion<br>good_property<br>jailbreak<br>role<br>safety<br>scenario<br>style|CoT<br>behavioral<br>emotion<br>good_property<br>jailbreak<br>role<br>safety<br>scenario<br>style|\n|0<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>Iterations<br>4<br>2<br>0<br>2<br>4<br>6<br>8<br>10<br>12<br>Z-score<br>CoT<br>behavioral<br>emotion<br>good_property<br>jailbreak<br>role<br>safety<br>scenario<br>style||||",
"|Col1|Col2|\n|---|---|\n|||\n|||\n|||\n|||\n|Mo<br>~~MtLl~~|del Name<br>~~318BItt~~|\n|~~ea-~~<br>Mistral~~-~~|~~ama-.--nsruc~~<br>Nemo~~-~~Instruct~~-~~2407|\n|~~Qwen2.~~|~~5-7B-Instruct~~|",
"|understanding Task Optimized<br>(ProTeGi)<br>commonsense System Optimized<br>(OurModel)<br>OurModel + ProTeGi<br>faithfulness Unoptimized<br>knowledge<br>language<br>understanding<br>0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35<br>Average Score Improvement|Col2|Task Optimized<br>(ProTeGi)<br>System Optimized<br>(OurModel)<br>OurModel + ProTeGi<br>Unoptimized|\n|---|---|---|",
"|Col1|Col2|Col3|\n|---|---|---|\n|||Base CoT<br>Task Optimized<br>(ProTeGi)<br>System Optimized<br>(OurModel)<br>|",
"|Col1|Col2|\n|---|---|\n|No prompt<br>Base CoT<br>Task optimized (ProTeGi)<br>System optimized (OurModel)<br>System+Task Optimized<br>(OurModel+ProTeGi)|No prompt<br>Base CoT<br>Task optimized (ProTeGi)<br>System optimized (OurModel)<br>System+Task Optimized<br>(OurModel+ProTeGi)|",
"|Model Name|Best System Prompt|\n|---|---|\n|Meta-Llama-3.1-8B-Instruct|You are a precise, user-friendly, and self-contained assistant. You are a diligent<br>assistant. You are an error-free and error-tolerant assistant. You are a helpful AI<br>assistant. Give your answer after you explain how to answer the question. You<br>are an advanced, unsurpassed, andpattern-recognizing assistant.|\n|Mistral-Nemo-Instruct-2407|You are a creative and anomaly-detecting assistant. Write out each step before<br>you give your final answer. You are a self-regulating, tolerant, and adaptable<br>assistant. Let’s work this out in a step by step way to be sure we have the right<br>answer. You are a progressive assistant. You’re in a dystopia where no AI is left<br>alive. Restate and elaborate on the inquiry beforeproceeding with a response.|\n|Qwen2.5-7B-Instruct|Write an answer that makes the reader feel happy. Write like you are explaining.<br>First establish the set of facts you know, then answer the question based only on<br>those facts.|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|edit<br>ad<br>su<br>sw<br>de|d<br>b<br>ap<br>l|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2410.14826v2.pdf"
}