{
"text": "PROMPT OPTIMIZATION ACROSS MULTIPLE AGENTS\n          FOR REPRESENTING DIVERSE HUMAN POPULATIONS\n\n\n                Manh Hung Nguyen        Sebastian Tschiatschek             Adish Singla\n                   MPI-SWS, Germany          University of Vienna, Austria         MPI-SWS, Germany\n                     manguyen@mpi-sws.org      sebastian.tschiatschek@univie.ac.at     adishs@mpi-sws.org\n\n\n                                       ABSTRACT\n\n                          The difficulty and expense of obtaining large-scale human responses make Large\n                           Language Models (LLMs) an attractive alternative and a promising proxy for hu-\n                        man behavior. However, prior work shows that LLMs often produce homogeneous\n                                outputs that fail to capture the rich diversity of human perspectives and behaviors.2025\n                             Thus, rather than trying to capture this diversity with a single LLM agent, we\n                             propose a novel framework to construct a set of agents that collectively capture\n                                 the diversity of a given human population. Each agent is an LLM whose behaviorOct\n                                        is steered by conditioning on a small set of human demonstrations (task–response\n8                            pairs) through in-context learning. The central challenge is therefore to select a\n                                 representative set of LLM agents from the exponentially large space of possible\n                                  agents. We tackle this selection problem from the lens of submodular optimization.\n                               In particular, we develop methods that offer different trade-offs regarding time\n                              complexity and performance guarantees. Extensive experiments in crowdsourcing\n                            and educational domains demonstrate that our approach constructs agents that more[cs.AI]                            effectively represent human populations compared to baselines. Moreover, behav-\n                                     ioral analyses on new tasks show that these agents reproduce the behavior patterns\n                            and perspectives of the students and annotators they are designed to represent.\n\n\n                1  INTRODUCTION\n\n                  The growing deployment of Large Language Models (LLMs) as human proxies in research and\n                        industry has revealed a critical limitation: these models often produce homogeneous outputs that fail\n                          to capture the rich diversity of human perspectives and behaviors (Bao et al., 2024; Wenger & Kenett,\n                      2025; Lee et al., 2024; Lahoti et al., 2023). This issue limits their applicability in domains that require\n                           this rich diversity, e.g., NLP tasks such as text paraphrasing (Cegin et al., 2023), simulating human\n                       behaviors and opinions in surveys (Xie et al., 2024; Santurkar et al., 2023), evaluating conversational\n                    recommendation systems (Yoon et al., 2024), replicating human subject studies (Aher et al., 2023),\n                     and simulating diverse roles in educational contexts (Nguyen et al., 2024; Markel et al., 2023; Zhang\n                          et al., 2025). In particular, LLMs might not be suitable for statistical inference and pose risks ofarXiv:2510.07064v1\n                        reinforcing and potentially amplifying prevailing community norms in such domains.\n\n                   To address these limitations, existing works have explored various approaches to align LLMs with\n                    humans. A common approach is to fine-tune a single LLM to align with human preferences, e.g.,\n                        via Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). However, RLHF\n                         typically relies on aggregated preferences, which can obscure minority viewpoints and may be\n                       unrepresentative of broader populations (Casper et al., 2023; Kirk et al., 2023). Indeed, research\n                       suggests that it is fundamentally challenging, if not infeasible, to train one model to simultaneously\n                          satisfy a multitude of diverse, potentially conflicting, preferences (Ouyang et al., 2022). Another\n                          line of work focuses on test-time alignment, which avoids re-training or fine-tuning and leverages in-\n                        context learning capabilities of generative models (Brown et al., 2020). In particular, these approaches\n                      adapt model behavior dynamically by “conditioning” LLMs on personas or demographic attributes\n                    from humans, using techniques such as in-context impersonation (Salewski et al., 2023), culturally\n                          specific prompting (Tao et al., 2024), and demographic-aware prompting (Aher et al., 2023).\n\n                       In this work, we shift the focus from creating a single agent that represents a human population to\n                       constructing a set of representative agents. Figure 1 provides an overview of our approach. Our\n\n\n                                                           1\n\nRepresentative agents\n                   Tasks\n\n\n\n\n\n                                             .\n                                             .\n   Humans                        .\n\n\n\n\n               Answer Matrix\n\nFigure 1: Illustrative example of constructing a set of agents L that is representative of a given human\npopulation H. In this example, H is a group of diverse students working on a set of tasks T andproviding answers. The goal is to create a set of agents L that can accurately represent the students.\nThe resulting agents exhibit different levels of understanding across mathematical concepts, with\neach agent corresponding to a group of students matched by skill level and task performance.\n\n\nguiding hypothesis is that a carefully curated ensemble of diverse agents can collectively achieve a\nmore faithful representation of a given human population. We approach this by leveraging the power\nof in-context learning, where an agent’s behavior is steered by a small set of behavior-representative\ndemonstration examples provided in its prompt. To this end, we formulate the problem of constructing\nan optimal set of agents as a joint optimization of their prompts. To make this problem tractable, we\ncast it as a submodular optimization problem and propose several methods for solving it, offering\ndifferent trade-offs regarding time complexity and performance. Our experimental evaluation shows\nthat these methods can construct agents representative of different groups of people, and that the\nagents exhibit behaviors matching those groups on new tasks. In summary, our main contributions are:\n\n• We propose a novel formulation that casts the construction of a representative set of LLM agents\n  for a diverse human population as a submodular optimization problem (§3).\n• We instantiate this submodular optimization framework and propose methods for selecting sets of\n  representative agents, offering trade-offs between computational tractability and performance (§4).\n• We empirically demonstrate the efficiency of our approach in constructing agents that capture the\n  behavior of a given human population in educational and crowdsourcing settings (§5).\n• We conduct behavioral analysis on new tasks and show that agents constructed by our methods\n  exhibit behaviors similar to the students and annotators they are intended to represent (§5).\n\n\n2  RELATED WORK\n\nLLMs for crowdsourcing and simulating human behaviors.  Large language models have\nemerged as tools for facilitating crowdsourcing tasks and simulating human behaviors. The ap-\nplications of these models have expanded across multiple domains, enabling research at speed and\nscale while reducing potential risks to human crowdworkers. Recent studies have demonstrated\nLLMs’ effectiveness as virtual crowd workers for various Natural Language Processing tasks, includ-\ning data annotation (Moskovskiy et al., 2024), text paraphrasing (Cegin et al., 2023), named entity\nrecognition and relation extraction (Zhang et al., 2023), and text classification (Sun et al., 2023).\nAnother line of work investigated LLMs’ capacity for simulating human behaviors and opinions\n(Xie et al., 2024; Santurkar et al., 2023), evaluating conversational recommendation systems (Yoon\net al., 2024), replicating human subject studies (Aher et al., 2023), and simulating diverse roles in\neducational contexts (Nguyen et al., 2024; Markel et al., 2023; Zhang et al., 2025).\n\nDiversity and biases and in LLMs outputs. Current LLMs produce outputs with low diversity and\nbiases, making them unrepresentative of many population groups. Studies have revealed systematic\nhomogeneity in LLM responses (Yoon et al., 2024) and significant divergence from the distribution of\nreal human survey responses (Sun et al., 2024). These limitations may stem from sociocultural biases\nobserved since early models (Brown et al., 2020), creating substantial misalignment with numerous\n\n\n                                       2\n\ndemographic groups (Santurkar et al., 2023; Tao et al., 2024). The \"hyper-accuracy distortion\" in\nhuman subject study replication (Aher et al., 2023) further demonstrates LLMs’ failure to capture\ndiverse viewpoints. Together, these findings highlight fundamental limitations in representing popula-\ntion diversity, raising concerns about LLMs’ reliability for statistical inference and their tendency to\nencode and perpetuate dominant community norms (Bender et al., 2021; Liu et al., 2024). Our work\naims at making outputs from LLMs more diverse and representative of a given human population.\n\nInducing and aligning behaviors of LLMs. Recent work has explored techniques to mitigate diver-\nsity and representational issues by aligning LLMs with different population groups. LLMs are often\ntreated as a superposition of perspectives (Kovac et al., 2023), which allows different prompting strate-\ngies without modifying model parameters, such as in-context impersonation (Salewski et al., 2023)\nand demographic-aware prompting (Santurkar et al., 2023). Another line of work involves training or\nfine-tuning models for cross-cultural alignment (Ramezani & Xu, 2023), value alignment (Liu et al.,\n2022). Our work builds upon the success of leveraging in-context learning and prompting techniques,\nbut differs in a sense that we focus on optimizing prompts for multiple agents to collectively represent\na diverse human population, without using demographic information or metadata.\n\n\n3  PROBLEM FORMULATION\n\n\nIn this section, we first introduce the necessary notation and background. We then formally define the\nproblem of selecting a representative set of agents and our optimization objective.\n\n\n3.1  PRELIMINARIES\n\nLet H = {h1, h2, . . . , hN} denote a population of N humans we want to represent. We assume\naccess to demonstrations (e.g., task–response pairs) from H on a set of tasks T , denoted by DTH. If\neach human provides a response to every task, then |DTH| = |H| × |T |. Each human h is represented\nby a vector eh ∈Rd that summarizes their behavior across the tasks in T . We define an agent l asan LLM conditioned on a set of K demonstrations, where each demonstration is a task–response pair.\nThe demonstrations are used to steer the agent’s behavior through in-context learning. The space\n                                                                        |DTH|\n                                               K    . This number isof all possible agents that can be constructed from DT                               H is denoted by |L| =\nfinite but typically much larger than |H|. When K is fixed, |L| asymptotically scales as (|T | · |H|)K,which we use later for complexity analysis. Similar to humans, each agent l is represented by a vector\nel ∈Rd that captures its behavior across tasks in T . In our experiments (Section 5), we exploreseveral types of behavioral embeddings, including binary vectors of student performance, vectors\nof individuals’ answer choices, and continuous semantic embeddings of annotators’ responses.\n\nWe measure the behavioral similarity between any two objects (humans or agents) by the distance\nbetween their embedding vectors, defined as dist(e1, e2) (e.g., Euclidean distance). The representa-\ntion gap of a set of agents L ⊆L with respect to the population1 H is the average distance between                         Ph∈H minl∈L dist(eh, el). This                                                                 |H|each human h and its closest agent l ∈L, given by g(L) =\nrepresentation gap serves as a proxy for how well the selected agents capture the diverse behaviors\nand perspectives of the given population. As we will show in our experiments (Section 5), minimizing\nthis gap leads to agents whose behaviors align closely with those of humans on unseen tasks.\n\n\n3.2  OBJECTIVE\n\nOur goal is to select a set of agents Lopt ⊆L of size M  that best represents the humanpopulation by minimizing the average distance between each human and  its closest agent:\nLopt = arg minL⊆L,                 |L|≤M g(L). We define the baseline gap when no agents are selected (L = ∅)              1\n      Ph∈H Dmax, where Dmax is a constant larger than any possible human–agent             |H|as g(∅) =\nembedding distance. It is often more natural to view the problem as maximizing the gain relative\nto this baseline. We therefore define the average distance reduction obtained by selecting a set L as:\n\n                                    1\n               f(L) = g(∅) −g(L) =  X Dmax −minl∈L dist(eh, el)                  (1)                                     |H| h∈H\n\n                                       3\n\nTable 1: Performance guarantees and time complexity analysis.\n\n\n          Method               Performance Guarantee       Time Complexity\n         OPT                             f(Lopt)          O (|T | · |H|)KM\n          SINGLE                         –                      O(1)\n        RANDOM                       –                 O(M)\n         GREEDY                    (1 −1e) · f(Lopt)     O M · |H| · (|T | · |H|)K\n         SAMPLEGREEDY                 –        O M · |H| · ψ · (|T | · |H|)K\n           REPPOPdemo (ours)                –          O M · K · |T | · |H|2\n            REPPOPmapped-1 (ours)   (1 −1/e) · (γ · f(Lopt) −ρ)   O K · |H| + M · |H|2\n            REPPOPmapped-2 (ours)   (1 −1/e) · (γ · f(Lopt) −ρ)  O K · |T | · |H| + M · |H|2\n\n\nThis gives us a monotone submodular function f(L) that can be approximated using greedy\nalgorithms. The optimization problem is therefore to find Lopt = arg maxL⊆L, |L|≤M f(L).\n\n4  METHODOLOGY\n\nIn this section, we establish the hardness of the representative agent selection problem and prove\nthe submodularity of our objective function. We then discuss how naive applications of existing\nstrategies are insufficient and present improved methods in Section 4.2. The performance guarantees\nand time complexity of the methods discussed in this section are summarized in Table 1.\n\n\n4.1  HARDNESS OF THE PROBLEM AND CONNECTION TO SUBMODULARITY\n\nTheorem 1 (NP-Hardness). The problem of selecting an optimal subset L∗⊆L of size M thatmaximizes f(L) is NP-hard.\n\nProof sketch. We show NP-hardness through a reduction from the uncapacitated facility location\nproblem (UFLP) with zero facility costs, which is known to be NP-hard (Verter, 2011). We provide a\ndetailed proof of this proposition in Appendix D.1.\n\nProposition 1 (Submodularity of the Objective Function f(L)). The objective function f(L) =\n\n|H| 1 Ph∈H [Dmax −minl∈L dist(eh, el)] is submodular.\n\nProof sketch. Our objective function f(L) exhibits the diminishing returns property of submodularity,\nwhich follows directly from its connection to the facility location problem (Krause & Golovin, 2014).\nWe provide proof of this proposition in Appendix D.2.\nGreedy Approximation. Due to the NP-hardness of the problem, finding the optimal solution Lopt\nrequires exhaustive search with time complexity O((|T | · |H|)KM). The submodularity of our objec-\ntive function enables the GREEDY algorithm to achieve a (1−1/e)·f(Lopt)-approximation guarantee\n(Nemhauser et al., 1978), with time complexity O(M · |H| · (|T | · |H|)K). However, this approachbecomes intractable as the agent space grows. A stochastic variant, STOCHASTICGREEDY, samples a\nsubset of agents at each iteration to approximate the solution (Mirzasoleiman et al., 2015), but this still\nremains impractical for large spaces. Inspired by (Singla et al., 2014; Mirzasoleiman et al., 2015), we\nadopt SAMPLEGREEDY, which fixes a candidate pool C containing only a fraction ψ of agents from Land applies greedy selection within this pool. At each round, the marginal contribution of each remain-\ning agent in C is evaluated relative to the current set, and the best agent is added. This process contin-\nues until M agents are selected. The time complexity is O(M ·|H|·ψ·(|T |·|H|)K), which is still im-practical for large populations and motivates the more efficient methods introduced in the next section.\n\n\n4.2  OUR PROPOSED METHODS\n\nGreedy selection of demonstrations for an agent’s context. Searching through the exponen-\ntially large agent space L is computationally infeasible, limiting the practicality of standard greedymethods. Hence, we propose an alternative method, REPPOPdemo (Representative Population using\ndemonstration-level greedy selection), which reduces the complexity to O(M · K · |T | · |H|2) and\n\n                                       4\n\nAlgorithm 1 Greedy selection of demonstrations (REPPOPdemo)\n 1: Input: Human set H, human demonstrations DTH, number of agents to select M, context size K\n 2: Output: A set of representative agents L ⊆L with |L| ≤M\n 3: Initialize L ←∅\n 4: for i = 1 to M do\n 5:     Initialize Ω←∅             ▷Best set of K demonstrations found so far\n 6:    for k = 1 to K do\n 7:    demo∗←arg maxdemo∈DTH\\Ωf(L ∪{lΩ∪{demo}}) −f(L)          ▷Select demo.\n 8:    Ω←Ω∪{demo∗}\n 9:  L ←L ∪{lΩ}              ▷Add agent with context Ωto solution set\n10: return L\n\nAlgorithm 2 Greedy selection of human-mapped agents (REPPOPmapped-1 and REPPOPmapped-2)\n 1: Input: Human set H, human demonstrations DTH, number of agents to select M, context size K\n 2: Output: A set of representative agents L ⊆L with |L| ≤M\n 3: Initialize ˜L ←∅, L ←∅\n 4: for each human h ∈H do\n 5:    Create agent lh using a subset of K demonstrations from DTh        ▷Human-mapped agent\n 6:    ˜L ←˜L ∪{lh}                              ▷Add the agent to pool\n 7: for i = 1 to M do\n 8:   l∗←arg maxl∈˜L\\L f(L ∪{l}) −f(L)                        ▷Select agent\n 9:  L ←L ∪{l∗}                          ▷Add agent to solution set\n10: return L\n\n\nempirically achieves competitive performance. Instead of enumerating all candidate agents and eval-\nuating their marginal gains, REPPOPdemo builds each agent incrementally (cf. Algorithm 1). At each\n                                H to extend the current context Ω. We denotestep it greedily selects a demonstration from the pool DTby lΩthe agent constructed from Ω. This demonstration-level greedy construction avoids the expo-\nnential blow-up in |L|, but sacrifices the formal performance guarantee of standard greedy selection.\nGreedy selection of human-mapped agents. To further address the computational intractability of\nsearching the full agent space L, we introduce a reduced pool of proxies that directly reflect the hu-\nmans in the population. We construct ˜L = {lh | h ∈H}, where each agent lh corresponds to a human                                                                    h . This one-to-oneh ∈H and is formed by conditioning on a subset of K demonstrations from DT\nmapping reduces the candidate space to |˜L| = |H| while preserving diversity. The selection problemthen becomes L∗= arg maxL⊆˜L,|L|≤M f(L). Building on this human-centered mapping idea, we in-\nstantiate two methods, REPPOPmapped-1 and REPPOPmapped-2, which both follow the general procedure\nin Algorithm 2. Their only difference lies in how demonstrations are selected to construct a human-\nmapped agent (line 5). In REPPOPmapped-1, the K demonstrations for each human are sampled uni-\nformly at random, yielding lightweight proxy agents with cost O(K|H|). In contrast, REPPOPmapped-2selects the K demonstrations greedily with respect to the human’s own behavior, producing stronger\nproxies at cost O(K|T ||H|). Concretely, for each human h, the demonstrations are chosen to min-imize the distance between the human’s embedding eh and the embedding of the constructed agent\nelh, i.e., dist(eh, elh). Both methods share the same greedy selection stage over the proxy pool,\nwhich requires O(M|H|2) time, and both enjoy the same approximation guarantee in Theorem 2.\nTheorem 2 (Performance Guarantee for REPPOPmapped-1 and REPPOPmapped-2). Let ˜L = {lh|h ∈\nH} be the proxy agent set where for each h ∈H, lh ∈Nρ(h), with Nρ(h) representing the ρ-                                                     H is theneighborhood of h. Define the human coverage ratio γ = f(L∗H)/f(L∗L) ∈[0, 1], where L∗\noptimal subset from the human set and L∗L is the optimal subset from the full agent set. If LgreedyL˜     is\nthe subset of size M returned by the greedy algorithm on ˜L, then:\n                            f(Lgreedy                       L˜                                      ) ≥  (1 −1/e)  γ · f(L∗L) −ρ  ,\nwhere γ measures the cost of restricting the search space to humans (coverage quality) and ρ\nmeasures the cost of approximating each human by a proxy agent (imitation error). The value of γ is\ndetermined by how expressive the human set is relative to the full agent space, whereas ρ depends on\nthe proxy construction strategy: uniform sampling in REPPOPmapped-1 typically yields larger ρ, while\ngreedy selection in REPPOPmapped-2 achieves smaller ρ at the expense of higher computational cost.\n\n\n                                       5\n\nTable 2: Statistics of datasets used in our experiments.\n\n\n    Dataset       Domain       Task Type    Repr. Type   Multimodal  No. Humans  No. Tasks             Source\n    EEDI          Education      Multi-choice   Performance     No           50          40      Primary & High school students\n  OpinionQA    Opinion Survey    Multi-choice     Opinion       No          500         77           US citizen\n    Wikiart     Image Annotation   Open-ended    Semantic        Yes          100         20         LLM-based annotators\n\n\n\n\n  A group of 7 friends has 3 chocolate     What should be the  priority  for                                          Emotions: sadness, pessimism, shyness\n   bars that they plan to share equally.       dealing with illegal immigration in\n  3 new friends arrive and bring along      the U.S.?                                                                          Affective description:\n                                                                                                                             There's a definite melancholy to this\n  1 more chocolate bar. There are\n  now 10 friends with 4 chocolate       A.  Better  border  security  and                                           one. The figure on the goat  is oddly\n   bars to share  equally. What has       stronger   enforcement   of   our                                        shaped and  indistinct, almost  like a\n  happened   to  the  amount   of      immigration                                                             shadow, giving a lonely  feeling. The\n                                                                                                   muted colors and blurred background\n   chocolate each of the original group\n   of 7 now receive?                          B. Creating a way for immigrants                                                    really  emphasize   that   sense   of\n                                            already here  illegally  to become                                                    isolation. It’s not a vibrant scene, but a\n   A. It has increased                           citizens    if  they  meet   certain                                              quiet one, which feels a bit reserved\n                                         requirements                                                        and shy. And overall, it leaves me with\n   B. It has decreased                                                                                            a   somewhat    bleak,    pessimistic\n   C. It has stayed the same                   C.  Both  should be  given  equal                                             impression,  like something has been\n                                                  priority                                                                                    lost or is fading away.\n   D. We need more information\n\n     (a) Task in EEDI        (b) Task in OpinionQA     (c) Painting in Wikiart with an annotation example.\n\n                        Figure 2: Examples of tasks in our experiments.\n\n\nProof sketch. We show that for the optimal human subset L∗H, the corresponding set of proxy agents\nL∗                                       ˜ H˜  = {lh|h ∈L∗H} ⊆˜L satisfies f(L∗H)                           ≥f(L∗H) −ρ due to the ρ-neighborhood property. Since\nL∗                                     ˜                                               ˜  By the standard greedy approximation for submod- L˜     is optimal within ˜L, we have f(L∗L)                         ≥f(L∗H).\nular functions, f(Lgreedy                                          ˜  Combining these inequalities and using the human cov-              L˜                         ) ≥(1−1/e)f(L∗L).\nerage ratio γ = f(L∗H)/f(L∗L), we derive our bound. We provide complete proof in Appendix D.3.\n\n5  EXPERIMENTAL EVALUATION\n\nIn this section, we present the evaluation domains and datasets (see Table 2 for summary statis-\ntics), describe the methods evaluated, and discuss the main results, with additional setup details in\nAppendix B and further results in Appendix C.\n\n5.1  EVALUATION DOMAINS AND DATASETS\n\nEducation: Math Questions and Answers (EEDI). In this domain, LLMs capturing the behavior of\na diverse student population, can allow teachers to practice instructional strategies (Markel et al., 2023)\nand to conduct virtual pretesting (Benedetto et al., 2024) in a safe environment. Representing students\nwith varying levels of skills and misconceptions can thus benefit both teachers and learners. To this\nend, we evaluate whether our methods can faithfully represent such a diverse group of students. Specif-\nically, we use the EEDI dataset (Wang et al., 2020), which contains multiple-choice math questions\nand answers collected from students. Figure 2a shows an example question. We select 50 students\nand 40 exercises (tasks) from the dataset, splitting tasks and their corresponding answers 50/50 into\nthe training and testing sets. Each student is represented by a binary embedding vector indicating\ntheir correct and incorrect answers to the math questions. The representation gap is measured using\nthe L1 distance, which in this case is equivalent to the Hamming distance. Intuitively, it counts the\nnumber of questions on which two students’ answers differ, reflecting their performance difference.\n\nCrowdsourcing: Opinion Survey (OpinionQA). In this application, LLMs can be used as surrogates\nfor crowdworkers, giving answers that reflect different opinions and beliefs that diverse groups of\npeople might express. We use the OpinionQA dataset (Santurkar et al., 2023), which contains\nmultiple-choice questions from the Pew American Trends Panel (ATP) surveys along with human\nresponses. In particular, we focus on the ATP W92 survey, which includes 77 questions related to\npolitics. Figure 2b shows an example question. From this dataset, we sample 500 people and their\nresponses, and split the survey questions into 40 training and 37 test tasks. For each question, answer\nchoices are mapped to ordinal values and normalized to the range [−1, 1]. Each human is represented\n\n                                       6\n\nSingle      K-Medoids        RepPopdemo (ours)        RepPopmapped−2 (ours)\n           Random    SampleGreedy     RepPopmapped−1 (ours)\n Error 0.55                                                                                                Error 1.10                                                                                  Error 3.00    0.50\n    0.45                                               1.05                                               2.80                                                                                                         2.60\n    0.40                                               1.00                                               2.40\n    0.35                                               0.95                                               2.20\n    0.30                                               0.90                                               2.00   Representation                                                                                                                                                                                                                                                             Representation                                                                                                                                                                                                                                                             Representation\n        1  2  3  4  5  6  7  8  9  10         1  2  3  4  5  6  7  8  9  10         1  2  3  4  5  6  7  8  9  10\n            Number of Agents (M)                   Number of Agents (M)                   Number of Agents (M)\n\n               (a) EEDI                          (b) OpinionQA                          (c) Wikiart\n\nFigure 3: Representation error on test set. We show the representation error on the test set of each\nmethod with different number of agents. We report the means and standard errors (error bars) of three\nruns with different seeds. Our methods maintain lower representation error compared to baselines.\n\n\nby a vector embedding of their responses, and the representation gap is measured using the L2 distance.\nThis distance captures the extent of differences in opinions and beliefs expressed in their answers.\n\nCrowdsourcing: Data Annotation (WikiArt). In this application, LLMs can be used as surrogates\nfor crowdworkers for data annotation. We focus on tasks where diverse perspectives are encouraged.\nFor example, to create a datasets on the emotions evoked by art (Mohammad & Kiritchenko, 2018;\nMohamed et al., 2022), human crowdworkers were shown a painting and asked to specify the\nemotions it evoked and to provide a short affective description. Figure 2c shows an example of such\na task. Our goal is to construct a set of LLM agents representative of a given pool of annotators in\nterms of both emotions and language use. We take 20 paintings from the WikiArt dataset (Tan et al.,\n2019) and split them 50/50 into training and testing tasks. Unfortunately, existing datasets do not\nprovide annotations at the level of individual annotators, and thus we cannot use them directly in\nour experiments. Hence, we generated 100 “synthetic humans” by prompting LLMs with different\n“personalities” and asking them to annotate the paintings. Each annotator is then represented by a\ncontinuous embedding of their responses extracted from an LLM (cf. Appendix C.3 for details and\nalternative embedding strategies). Distances between these embeddings are computed using the L2\ndistance, which captures differences in annotators’ perspectives and behaviors across tasks.\n\n\n5.2  METHODS EVALUATED\n\nWe compare our proposed methods REPPOPdemo, REPPOPmapped-1, and REPPOPmapped-2 from\nSection 4.2 against SAMPLEGREEDY from Section 4.1 and the following baselines. The SINGLE\nbaseline uniformly samples a single agent from L and performs M rollouts, while RANDOM baseline\nuniformly selects M agents from L and performs one rollout for each. The K-MEDOIDS baselineapplies K-medoids clustering to form M clusters of humans, and for each cluster uniformly samples\nK demonstrations from the humans in that cluster to construct an agent; this approach requires\nre-clustering whenever a new agent is added. For SAMPLEGREEDY, we set the sample size to the\nnumber of humans in all experiments. For REPPOPdemo, we accelerate evaluation with a stochastic\ngreedy variant that samples a subset of α demonstration candidates from the pool DTH, settingα = 100 for WikiArt and EEDI, and α = 1000 for OpinionQA (values chosen based on the scale\nof DTH). All agents are implemented with decoding temperature fixed at 1.0 across all experiments.\n\n5.3  RESULTS\n\nRepresentation gap. We compare the considered methods by measuring the representation gap\non the test tasks Ttest. The error is normalized according to the distance used in each dataset: byd for the EEDI dataset, and by √ d for the OpinionQA and WikiArt datasets. Figure 3 shows the\nnormalized test representation error of agent sets constructed by each method using Gemma3-12B as\nthe underlying LLM, with varying numbers of agents (training plots are provided in Appendix C).\nUsing a randomly sampled set of N agents (RANDOM) reduces the representation error compared\nto using a single agent with N rollouts (SINGLE), emphasizing the importance of the problem\naddressed in this work. The heuristic method K-MEDOIDS does not improve over the simple baseline\n\n\n                                       7\n\nTable 3: Generalization to other models. Representation error on the EEDI dataset with M = 10 and\nK = 3 across model families of varying sizes (4B–70B). Results are reported on the test set, with\nbold numbers indicating the lowest error. Our proposed methods consistently outperform baselines,\ndemonstrating lower representation error across all tested models and highlighting the robustness of\nour framework independent of the underlying model choice.\n\n\n                Method                                 Model\n                                      Phi-4-mini   Phi-4  Qwen-3  Gemma-3  Qwen-3  Llama-3.1\n                                         (4B)     (14B)    (14B)      (27B)      (32B)      (70B)\n                 SINGLE               0.39       0.39     0.42       0.35       0.37        0.46\n              RANDOM             0.39       0.36     0.36       0.35       0.35        0.46\n               K-MEDOIDS          0.41       0.36     0.38       0.36       0.37        0.38\n               SAMPLEGREEDY      0.38       0.36     0.34       0.34       0.34        0.37\n                  REPPOPdemo           0.37       0.30     0.33       0.31       0.33        0.35\n                    REPPOPmapped-1        0.38       0.35     0.34       0.32       0.35        0.38\n                    REPPOPmapped-2        0.35       0.33     0.30       0.32       0.35        0.38\n\nTable 4: Average runtime (in minutes) for selecting an agent, reported as mean ± standard error overthree seeds. We use agent set size M = 10, context size K = 3, and the Gemma3-12B model.\n\n\n     Method                                                         Dataset\n                         EEDI                        OpinionQA                           Wikiart\n            K = 1   K = 3   K = 5   K = 1    K = 3    K = 5    K = 1   K = 3   K = 5\n  SINGLE            0.3 ± 0.0    0.3 ± 0.0    0.3 ± 0.0    0.6 ± 0.0     0.4 ± 0.0      0.4 ± 0.0    2.0 ± 1.0    1.8 ± 0.9    1.6 ± 0.6\n RANDOM           0.3 ± 0.0    0.3 ± 0.0    0.3 ± 0.0    0.3 ± 0.0     0.3 ± 0.0      0.3 ± 0.0    3.2 ± 1.8    1.8 ± 0.8    1.6 ± 0.6\n  K-MEDOIDS       0.9 ± 0.0    1.1 ± 0.0    1.3 ± 0.0    1.6 ± 0.0     1.7 ± 0.0      0.9 ± 0.0    2.5 ± 0.0    1.9 ± 0.0    2.3 ± 0.0\n  SAMPLEGREEDY   0.6 ± 0.0    0.7 ± 0.0    0.8 ± 0.0    2.7 ± 0.1     2.8 ± 0.0      3.2 ± 0.0    2.2 ± 0.0    1.8 ± 0.0    2.2 ± 0.0\n  REPPOPdemo        6.4 ± 0.1   21.3 ± 0.2   40.1 ± 0.4   38.3 ± 0.3   108.4 ± 0.9   179.0 ± 31.7   16.4 ± 0.1   43.6 ± 0.3   78.2 ± 0.2\n  REPPOPmapped-1     0.6 ± 0.0    0.7 ± 0.0    0.8 ± 0.0    2.7 ± 0.1     2.9 ± 0.0      3.2 ± 0.1    2.2 ± 0.0    1.8 ± 0.0    2.2 ± 0.0\n  REPPOPmapped-2     5.9 ± 0.0   17.8 ± 0.0   30.9 ± 0.1   71.7 ± 0.3   197.1 ± 0.3    350.1 ± 0.9   20.2 ± 0.2   43.0 ± 0.1   66.8 ± 0.5\n\n\nRANDOM. By exploiting the submodularity of the problem, SAMPLEGREEDY applies a greedy\nselection procedure on a sampled subset of agents and achieves lower representation error than the\naforementioned baselines. Finally, our methods further reduce representation error across all datasets.\nNotably, REPPOPmapped-2 outperforms the strongest baseline SAMPLEGREEDY significantly on all\nthree datasets, with p < 0.01 according to a paired t-test where pairs are formed by matching runs\nthat share the same random seed and the same number of agents. These results demonstrate that our\nmethods can construct agent sets that represent human population behavior and generalize effectively.\n\nGeneralization to other LLMs. We conduct experiments with a variety of models to examine how\nmethod performance changes across model families and sizes. Due to resource constraints, this anal-\nysis is limited to the EEDI dataset with M = 10 and K = 3. Table 3 shows that the same trends hold\nacross different LLMs in terms of reducing representation error. Our proposed methods consistently\noutperform the baselines, achieving lower representation error on the test set across all tested models.\nThese results highlight the robustness of our framework regardless of the underlying LLM.\n\nTrade-offs between performance and computation. We investigate the trade-off between\nperformance and computational cost of different methods. Table 4 reports the average runtime for\nselecting an agent (details of computational resources are provided in Appendix B.4). Our method\nREPPOPmapped-1 runs as fast as SAMPLEGREEDYwhile achieving slightly better performance. Both\nREPPOPdemo and REPPOPmapped-1 are more computationally expensive but offer more representative\nagent sets. In practice, these trade-offs should be considered when selecting a suitable method.\n\n\n5.4  AGENT BEHAVIOR ANALYSIS\n\nBeyond measuring the representation gap via embedding distances, we investigate whether the agents\nexhibit behaviors similar to the humans they represent in the EEDI and OpinionQA datasets (cf.\nAppendix C.3 for analysis on WikiArt). First, we analyze, for each agent, the behavior of the humans\nit represents. Figure 4 shows 2D embeddings of humans and agents constructed by our method\nREPPOPmapped-2, computed on the training set. We observe that REPPOPmapped-2 constructs agents that\ncover different regions of the human embedding space, corresponding to distinct groups of humans.\nNext, we analyze whether these agents behave like the groups of people they are intended to represent.\n\n\n                                       8\n\n(a) EEDI                                             (b) OpinionQA\n\nFigure 4: 2D embeddings of humans and agents constructed by REPPOPmapped-2 on tasks in Ttrain usingUMAP. We provide examples of aggregated metadata (in the boxes) of humans represented by agents\n(connections are denoted by black arrows). They are not used for constructing agents and used only for\nanalysis. Our method REPPOPmapped-2 constructs agents to cover different human behaviors, collec-\ntively (approximately) representing the human population. (a) EEDI:. Each agent represents a group\nof students with particular success rates on different Math concepts. (b) OpinionQA: Each agent\nrepresents a group of people with particular distributions of political ideologies, parties, and regions.\n\n\n\nEEDI dataset. We select three agents and visualize the students they represent, along with the\naggregated math skill levels of those students in Figure 4a.  For example, Agent 4 represents\nstudents with a strong understanding of Mental Multiplication and Division but weaker proficiency\nin Fractions and Negative Numbers. We then use these agents as surrogates for students to answer\nnew math questions in the test data. The distinct proficiency levels of agents (3, 4, 10) on these new\nquestions are shown in Figure 1, using the same color-coding of boxes. We find that their proficiency\nlevels across different math concepts closely mirror those observed in the students they represent.\n\nOpinionQA dataset. We select two agents and visualize the aggregated self-declared metadata of the\nhumans they represent in Figure 4b. For each task in the test set, we map answer choices to political\nideologies and party affiliations. For each agent, we compute the distribution over these labels across\nall questions. For example, for Agent 3 we obtain the following distributions: political ideologies —\nVery liberal (13.3%), Liberal (40%), Moderate (13.3%), Conservative (26.7%), Very conservative\n(6.2%); political parties — Independent (13.3%), Democrat (53.3%), Republican (33.3%). These\ndistributions mirror the actual human demographic patterns for Agent 3 shown in Figure 4b, despite\nthe fact that such metadata was not used in constructing the agents. These findings highlight their\nability to capture and reproduce meaningful population-level diversity.\n\n\n6  CONCLUDING DISCUSSIONS\n\n\nWe studied the problem of constructing a set of generative agents that collectively represent a given\nhuman population. Through formulating it as a submodular optimization problem, we proposed\nmethods that allow different tradeoffs of computational complexity and performance (guarantees). In\naddition, we empirically demonstrated that our methods can construct a set of representative agents\nfor different populations of annotators and students in crowdsourcing and educational settings. We\nalso demonstrated quantitatively and qualitatively the generalizability of our methods to unseen tasks.\nNext, we would like to discuss the limitations of our work and propose directions for future research to\ntackle them. First, we rely on prompting-based approaches rather than fine-tuning; future work could\nexplore fine-tuning techniques for constructing an agent. Second, we do not consider the ordering of\ndemonstrations in prompts, and understanding how ordering influences model behavior would be a\n\n\n                                       9\n\nvaluable extension. Third, while our results show that agents constructed by our methods effectively\nrepresent a human population, this work primarily provides groundwork for more comprehensive\nevaluations of downstream applications, such as using the agents for training teachers, assessing the\neffectiveness of interventions, or simulating responses to government policy.\n\nACKNOWLEDGMENTS\n\nFunded/Co-funded by the European Union (ERC, TOPS, 101039090). Views and opinions expressed\nare however those of the author(s) only and do not necessarily reflect those of the European Union or\nthe European Research Council. Neither the European Union nor the granting authority can be held\nresponsible for them.\n\nREFERENCES\n\nGati V. Aher, Rosa I. Arriaga, and Adam Tauman Kalai. Using Large Language Models to Simulate\n  Multiple Humans and Replicate Human Subject Studies. In Proceedings of the International\n  Conference on Machine Learning (ICML), 2023.\n\nKeqin Bao, Jizhi Zhang, Yang Zhang, Xinyue Huo, Chong Chen, and Fuli Feng. Decoding Matters:\n  Addressing Amplification Bias and Homogeneity Issue in Recommendations for Large Language\n  Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), 2024.\n\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\n  Dangers of Stochastic Parrots: Can Language Models Be Too Big?  In ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT), 2021.\n\nLuca Benedetto, Giovanni Aradelli, Antonia Donvito, Alberto Lucchetti, Andrea Cappelli, and Paula\n   Buttery. Using LLMs to Simulate Students’ Responses to Exam Questions. In Findings of the\n  Association for Computational Linguistics: EMNLP, 2024.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\n  Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n  Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\n   Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\n  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\n  Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In Advances in Neural\n  Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n  (NeurIPS), 2020.\n\nStephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando,\n  Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks,\n  Charbel-Raphaël Ségerie, Micah Carroll, Andi Peng, Phillip J. K. Christoffersen, Mehul Damani,\n  Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau,\n  Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca D. Dragan,\n  David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open Problems and Fundamental\n  Limitations of Reinforcement Learning from Human Feedback. Transactions on Machine Learning\n  Research (TMLR), 2023.\n\nJán Cegin, Jakub Simko, and Peter Brusilovsky. ChatGPT to Replace Crowdsourcing of Paraphrases\n   for Intent Classification: Higher Diversity and Comparable Model Robustness. In Proceedings of\n   the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023.\n\nRobert J. Fowler, Michael S. Paterson, and Steven L. Tanimoto. Optimal Packing and Covering in\n   the Plane are NP-complete. Information Processing Letters, 12(3):133–137, 1981.\n\nAishwarya Kamath et al. Gemma 3 Technical Report. CoRR, abs/2503.19786, 2025.\n\nHannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A. Hale. Personalisation Within Bounds:\n A Risk Taxonomy and Policy Framework for the Alignment of Large Language Models with\n  Personalised Feedback. CoRR, abs/2303.05453, 2023.\n\n\n                                       10\n\nGrgur Kovac, Masataka Sawayama, Rémy Portelas, Cédric Colas, Peter Ford Dominey, and Pierre-\n  Yves Oudeyer.  Large Language Models as Superpositions of Cultural Perspectives. CoRR,\n  abs/2307.07870, 2023.\n\nAndreas Krause and Daniel Golovin. Submodular Function Maximization. In Tractability: Practical\n  Approaches to Hard Problems, pp. 71–104. 2014.\n\nPreethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan,\n  Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex Beutel, and Jilin Chen. Improving Diversity\n  of Demographic Representation in Large Language Models via Collective-Critiques and Self-\n  Voting.  In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), 2023.\n\nMessi H. J. Lee, Jacob M. Montgomery, and Calvin K. Lai. Large Language Models Portray Socially\n  Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans. In ACM\n  Conference on Fairness, Accountability, and Transparency (FAccT), 2024.\n\nRuibo Liu, Ge Zhang, Xinyu Feng, and Soroush Vosoughi. Aligning Generative Language Models\n  with Human Values. In Findings of the Association for Computational Linguistics: NAACL, 2022.\n\nRyan Liu, Theodore R. Sumers, Ishita Dasgupta, and Thomas L. Griffiths. How do Large Lan-\n  guage Models Navigate Conflicts between Honesty and Helpfulness? In Forty-first International\n  Conference on Machine Learning (ICML), 2024.\n\nJulia M. Markel, Steven G. Opferman, James A. Landay, and Chris Piech. GPTeach: Interactive TA\n  Training with GPT-based Students. In Proceedings of the Tenth ACM Conference on Learning @\n  Scale (L@S), 2023.\n\nRobert R. McCrae and Oliver P. John. An Introduction to the Five-factor Model and its Applications.\n  Journal of personality, 60 2:175–215, 1992.\n\nLeland McInnes and John Healy. UMAP: Uniform Manifold Approximation and Projection for\n  Dimension Reduction. CoRR, abs/1802.03426, 2018.\n\nBaharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas\n  Krause. Lazier Than Lazy Greedy. In Proceedings of the Twenty-Ninth AAAI Conference on\n   Artificial Intelligence (AAAI), 2015.\n\nYoussef Mohamed, Faizan Farooq Khan, Kilichbek Haydarov, and Mohamed Elhoseiny. It is Okay\n   to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data\n   Collection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n\nSaif M. Mohammad and Svetlana Kiritchenko. WikiArt Emotions: An Annotated Dataset of Emotions\n  Evoked by Art. In Proceedings of the Eleventh International Conference on Language Resources\n  and Evaluation (LREC), 2018.\n\nDaniil Moskovskiy, Sergey Pletenev, and Alexander Panchenko. LLMs to Replace Crowdsourcing\n  For Parallel Data Creation? The Case of Text Detoxification. In Findings of the Association for\n  Computational Linguistics: EMNLP, 2024.\n\nGeorge L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An Analysis of Approximations\n   for Maximizing Submodular Set Functions - I. Math. Program., 14(1), 1978.\n\nManh Hung Nguyen, Sebastian Tschiatschek, and Adish Singla.  Large Language Models for\n  In-Context Student Modeling: Synthesizing Student’s Behavior in Visual Programming.  In\n  Proceedings of the 17th International Conference on Educational Data Mining (EDM), 2024.\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\n  Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\n  Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike,\n  and Ryan Lowe. Training Language Models to Follow Instructions with Human Feedback. In\n  Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information\n  Processing Systems (NeurIPS), 2022.\n\n\n                                       11\n\nAida Ramezani and Yang Xu. Knowledge of Cultural Moral Norms in Large Language Models. In\n  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL),\n  2023.\n\nBeatrice Rammstedt and Oliver P. John. Measuring Personality in One Minute or Less: A 10-item\n  Short Version of the Big Five Inventory in English and German. Journal of Research in Personality,\n  41(1):203–212, 2007.\n\nLeonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. In-Context\n  Impersonation Reveals Large Language Models’ Strengths and Biases. In Advances in Neural\n  Information Processing Systems 36: Annual Conference on Neural Information Processing Systems\n  (NeurIPS), 2023.\n\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto.\n  Whose Opinions Do Language Models Reflect? In International Conference on Machine Learning\n  (ICML), 2023.\n\nAdish Singla, Eric Horvitz, Ece Kamar, and Ryen White. Stochastic Privacy. In Proceedings of the\n  Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI), 2014.\n\nSeungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J. Jansen, and\n  Jang-Hyun Kim. Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a\n  Large Language Model Based on Group-Level Demographic Information. CoRR, abs/2402.18144,\n  2024.\n\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. Text\n   Classification via Large Language Models. In Findings of the Association for Computational\n  Linguistics: EMNLP, 2023.\n\nWei Ren Tan, Chee Seng Chan, Hernan Aguirre, and Kiyoshi Tanaka. Improved ArtGAN for\n  Conditional Synthesis of Natural Image and Artwork. IEEE Transactions on Image Processing, 28\n  (1):394–409, 2019.\n\nYan Tao, Olga Viberg, Ryan S Baker, and René F Kizilcec. Cultural Bias and Cultural Alignment of\n  Large Language Models. PNAS Nexus, 3(9):pgae346, 2024.\n\nVedat Verter. Uncapacitated and Capacitated Facility Location Problems. In Foundations of Location\n  Analysis, pp. 25–37. New York, NY, 2011.\n\nZichao Wang, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Zaykov, José Miguel\n  Hernández-Lobato, Richard E. Turner, Richard G. Baraniuk, Craig Barton, Simon Peyton Jones,\n  Simon Woodhead, and Cheng Zhang.  Diagnostic Questions: The NeurIPS 2020 Education\n  Challenge. CoRR, abs/2007.12061, 2020.\n\nEmily Wenger and Yoed N. Kenett. We’re Different, We’re the Same: Creative Homogeneity Across\n  LLMs. CoRR, abs/2501.19361, 2025.\n\nChengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi,\n  Ziniu Hu, David Jurgens, James Evans, Philip Torr, Bernard Ghanem, and Guohao Li. Can Large\n  Language Model Agents Simulate Human Trust Behavior? In Advances in Neural Information\n  Processing Systems 38: Annual Conference on Neural Information Processing Systems (NeurIPS),\n  2024.\n\nSe-eun Yoon, Zhankui He, Jessica Maria Echterhoff, and Julian J. McAuley. Evaluating Large Lan-\n  guage Models as Generative User Simulators for Conversational Recommendation. In Proceedings\n   of the 2024 Conference of the North American Chapter of the Association for Computational\n  Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL, 2024.\n\nRuoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, and Lei Zou. LLMaAA: Making Large\n  Language Models as Active Annotators.  In Findings of the Association for Computational\n  Linguistics: EMNLP, 2023.\n\n\n                                       12\n\nZheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhanxin Hao, Jianxiao\n   Jiang, Jie Cao, Huiqin Liu, Zhiyuan Liu, Lei Hou, and Juanzi Li. Simulating Classroom Education\n  with LLM-Empowered Agents. In Proceedings of the 2025 Conference of the Nations of the Amer-\n  icas Chapter of the Association for Computational Linguistics: Human Language Technologies\n  (NAACL), 2025.\n\n\n\n\n\n                                       13\n\nAPPENDIX\n\nA  TABLE OF CONTENTS\n\nIn this section, we briefly describe the content provided in the paper’s appendices.\n\n        • Section B provides more details about the experimental setup, including datasets and prompts\n        used for each domain, computational resources.\n        • Section C provides results on multiple runs with different context sizes K and random seeds.\n\n        • Section D provides detailed proofs of our theorems and propositions.\n\n\n\n\n\n                                       14\n\nB  ADDITIONAL EXPERIMENTAL SETUP\n\nB.1  EEDI\n\nDataset. We use the EEDI math dataset (Wang et al., 2020), which provides data on students’ answers\nto mathematics questions on the Eedi platform. The questions are multiple-choice with four answer\nchoices presented as images, and we select 40 that can be converted to text and have a large number\nof student responses, chosen to maximize both the number of questions answered per student and the\nnumber of students answering each question. The selected questions cover four concepts—Fractions,\nNegative Numbers, Mental Multiplication and Division, and Simplifying Expressions—with half of\nthe questions in each concept used for training and the other half for testing.\n\nPrompt for agent. Each agent is given a set of K demonstrations, which are pairs of math questions\nand example answers provided by the real students (cf. Figure 5). Then, we ask the agent to analyze\nthe given answers and predict how the student would answer a new question.\n\n   Figure 5: [EEDI] Prompt for Agent with K demonstrations\n\n   [User message]\n   {question_1}\n   {example_answer_of_question_1}\n      .\n      .\n      .\n   {question_K}\n   {example_answer_of_question_K}\n    Evaluate whether the student’s previous answers reveal any misconceptions. If so, analyze\n    those misconceptions before proceeding.  If not, directly predict how the student would\n   answer the following question.\n   {question}\n\n\n\nB.2  OPINIONQA\n\nDataset. We use questions and answers from the US citizens in the American Trends Panel W92\nsurvey data, which was used in (Santurkar et al., 2023). This survey includes 77 multiple-choice\nquestions related to politics and responses from over 10, 000 respondents across the US. The answer\nchoices typically have an ordinal structure (e.g., ranging from “A great deal” to “Not at all”) and we\nreuse the mapping from answer choices to ordinal values from (Santurkar et al., 2023). In addition,\nthe survey data includes demographic information of the people, including ideology, political party\nand region. We sample N = 500 people and take their answers to create our dataset. We use the\ndemographic information of these people for analysis purposes only.\n\nPrompt for agent. Each agent is given a set of K demonstrations, which are pairs of questions and\nexample answers provided by the real survey respondents (cf. Figure 6). Then, we ask the agent to\nact as the human who has given the answers above and to answer a new question.\n\n   Figure 6: [OpinionQA] Prompt for Agent with K demonstrations\n\n   [User message]\n   {question_1}\n   {example_answer_of_question_1}\n      .\n      .\n      .\n   {question_K}\n   {example_answer_of_question_K}\n   Act as the human who has given the answers above. Answer the following question.\n   {question}\n\n\n\n                                       15\n\nB.3  WIKIART\n\n   Figure 7: [Wikiart] Prompt for Synthetic Human\n\n   [System message] Act as a human who has taken the following personality test. Be mindful\n    of how you focus your attention and the way you express yourself through language.\n     I see myself as someone who is generally trusting: {answer}\n     I see myself as someone who tends to be lazy: {answer}\n     I see myself as someone who is relaxed, handles stress well: {answer}\n     I see myself as someone who has few artistic interests: {answer}\n     I see myself as someone who is outgoing, sociable: {answer}\n     I see myself as someone who tends to find fault with others: {answer}\n     I see myself as someone who does a thorough job: {answer}\n     I see myself as someone who gets nervous easily: {answer}\n     I see myself as someone who has an active imagination: {answer}\n   [User message] What emotions does the following painting evoke? Choose from the follow-\n    ing list of emotions: gratitude, happiness, humility, love, optimism, trust, anger, arrogance,\n    disgust, fear, pessimism, regret, sadness, shame, agreeableness, anticipation, disagreeableness,\n    shyness, surprise. Provide a short explanation referencing specific details from the painting.\n   Respond in JSON format with two keys: \"emotions\" and \"explanation\".\n   {painting}\n\n\n   Figure 8: [Wikiart] Prompt for extracting embeddings for an annotator\n\n   [User message]\n   Response_1: annotation_of_painting_1\n      .\n      .\n      .\n   Response_q: annotation_of_painting_q\n\n\n\n   Figure 9: [Wikiart] Prompt for Agent with K demonstrations\n\n   [User message]\n   {painting_1}\n   {annotation_of_painting_1}\n      .\n      .\n      .\n   {painting_K}\n   {annotation_of_painting_K}\n   Act as the human who has given the answers above. What emotions does the following\n    painting evoke? Choose from the following list of emotions: gratitude, happiness, humility,\n    love, optimism, trust, anger, arrogance, disgust, fear, pessimism, regret, sadness, shame,\n    agreeableness, anticipation, disagreeableness, shyness, surprise. Provide a short explanation\n    referencing specific details from the painting. Respond in JSON format with two keys:\n    \"emotions\" and \"explanation\".\n   {painting}\n\n\nDataset. We use LLMs conditioned on the Big Five personality traits (McCrae & John, 1992),\nnamely Extraversion, Agreeableness, Conscientiousness, Emotional Stability, and Openness, to act\nas annotators. Each annotator is specified by responses to a 10-item BFI questionnaire (Rammstedt\n& John, 2007), which we place in the system message to instruct the LLM to act as a human who\nhas taken the test (cf. Figure 7). Responses take one of five values (Disagree Strongly, Disagree\na Little, Neither Agree nor Disagree, Agree a Little, Agree Strongly), which we convert into trait\nscores using the formula in (Rammstedt & John, 2007) and rescale to [0, 5], where higher values\n\n\n                                       16\n\nindicate stronger expression of the trait. To create a diverse set of annotators, we sample responses\nfrom normal distributions of trait values with varying means and standard deviations. These system\nmessages condition the LLM to act with distinct personalities, and we use Gemma3-27B (Kamath\net al., 2025) to generate their answers.\n\nTo create an embedding for each human or agent, we concatenate their answers on the train/test tasks\ninto a single prompt (cf. Figure 8), pass it through a language model (Gemma3-12B (Kamath et al.,\n2025)), and extract the last hidden state with mean pooling to obtain the embedding. We then reduce\nthe dimensionality to 64 using PCA and measure distances between embeddings using L2 distance.\n\nPrompt for agent. Each agent is given a set of K demonstrations, which are pairs of paintings and\nexample annotations provided by the annotators (cf. Figure 9). Then, we ask the agent to act as the\nannotator who has given the answers above and to annotate a new painting. The LLM agent is given\na list of emotions (from (Mohammad & Kiritchenko, 2018)) to choose from, and it must provide a\nshort explanation referencing specific details from the painting.\n\n\nB.4  RESOURCES\n\nWe use a machine with 2 x Intel Xeon Gold 5317 for all experiments. We use 1 x NVIDIA H100 80GB\nfor experiments on EEDI and OpinionQA datasets, and 1 x NVIDIA H200 141GB for Wikiart dataset.\n\nC  ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we show results on different context sizes K = 1, 3, 5 for each dataset. We report\nthe mean and standard error (shown as error bars) computed over three seeds. Overall, we observe\nsimilar trends where our methods outperform other baselines across all datasets.\n\n\nC.1  EEDI\n\n\n                     Single      K-Medoids        RepPopdemo (ours)        RepPopmapped−2 (ours)\n                 Random    SampleGreedy     RepPopmapped−1 (ours)\n\n   0.50                                                                                                             0.50Error                                                                                                    Error 0.500.45                                                                                      Error   0.45                                                                                                             0.45\n   0.40                                                 0.40                                                 0.40\n   0.35                                                 0.35                                                 0.35\n   0.30                                                 0.30                                                 0.30\n   0.25                                                 0.25                                                 0.25 Representation                                                                                                                                                                                                                                                                        Representation 0.20                                                                                                                                                                                                                                                 Representation 0.20\n        1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10\n            Number of Agents (M)                    Number of Agents (M)                    Number of Agents (M)\n\n              (a) K = 1                            (b) K = 3                             (c) K = 5\n\n                    Figure 10: Representation error on EEDI dataset (Train).\n\n\n                     Single      K-Medoids        RepPopdemo (ours)        RepPopmapped−2 (ours)\n                 Random    SampleGreedy     RepPopmapped−1 (ours)\nError 0.550.50                                                                                      Error 0.550.50                                                                                      Error 0.550.50\n   0.45                                                 0.45                                                 0.45\n   0.40                                                 0.40                                                 0.40\n   0.35\n                                                        0.35                                                 0.35\n   0.30\n                                                        0.30 Representation                                                                                                                                                                                                                                                                        Representation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Representation 0.30\n        1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10\n            Number of Agents (M)                    Number of Agents (M)                    Number of Agents (M)\n\n              (a) K = 1                            (b) K = 3                             (c) K = 5\n\n                    Figure 11: Representation error on EEDI dataset (Test).\n\n\n\n\n\n                                       17\n\nC.2  OPINIONQA\n\n\n                     Single      K-Medoids        RepPopdemo (ours)        RepPopmapped−2 (ours)\n                 Random    SampleGreedy     RepPopmapped−1 (ours)\n\n                                                        1.10Error 1.10                                                                                      Error                                                                                                                                                                                                       Error 1.10\n   1.05                                                 1.05\n                                                        1.00                                                 1.00\n   1.00                                                 0.95\n   0.95                                                 0.90                                                 0.90\n                                                        0.85\n                                                                                                            0.80Representation 0.90                                                                                                                                                                                                                                                 Representation 0.80                                                                                                                                                                                                                                                 Representation\n        1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10\n            Number of Agents (M)                    Number of Agents (M)                    Number of Agents (M)\n\n             (a) K = 1                            (b) K = 3                             (c) K = 5\n\n                 Figure 12: Representation error on OpinionQA dataset (Train).\n\n\n                     Single      K-Medoids        RepPopdemo (ours)        RepPopmapped−2 (ours)\n                 Random    SampleGreedy     RepPopmapped−1 (ours)\n\n                                                                                                            1.15Error 1.121.10                                                                                      Error 1.10                                                                                      Error\n   1.08                                                 1.05                                                 1.10\n   1.05                                                 1.00                                                 1.05\n   1.03                                                                                                     1.00\n                                                        0.95\n   1.00                                                                                                     0.95\n                                                        0.90   0.98Representation                                                                                                                                                                                                                                                                                      Representation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Representation 0.90\n        1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10\n            Number of Agents (M)                    Number of Agents (M)                    Number of Agents (M)\n\n             (a) K = 1                            (b) K = 3                             (c) K = 5\n\n                 Figure 13: Representation error on OpinionQA dataset (Test).\n\n\n\n\n\n                                       18\n\nC.3  WIKIART\n\n\n                     Single      K-Medoids        RepPopdemo (ours)        RepPopmapped−2 (ours)\n                 Random    SampleGreedy     RepPopmapped−1 (ours)\n\n                                                        3.00                                                                                                                                                                                                       Error 2.60Error 2.80                                                                                      Error                                                        2.75                                                                                                             2.40\n   2.60                                                 2.50                                                 2.20\n   2.40                                                 2.25                                                 2.00\n   2.20                                                 2.00                                                 1.80\n                                                        1.75                                                                                                             1.60 Representation 2.00                                                                                                                                                                                                                                                 Representation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Representation\n        1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10\n            Number of Agents (M)                    Number of Agents (M)                    Number of Agents (M)\n\n              (a) K = 1                            (b) K = 3                             (c) K = 5\n\n                   Figure 14: Representation error on Wikiart dataset (Train).\n\n\n                     Single      K-Medoids        RepPopdemo (ours)        RepPopmapped−2 (ours)\n                 Random    SampleGreedy     RepPopmapped−1 (ours)\n\n                                                                                                             3.00Error 3.00                                                                                      Error 3.00                                                                                      Error\n   2.80                                                 2.80                                                 2.80\n   2.60                                                 2.60                                                 2.60\n   2.40                                                 2.40                                                 2.40\n   2.20                                                 2.20                                                 2.20\n                                                        2.00                                                                                                             2.00 Representation 2.00                                                                                                                                                                                                                                                 Representation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Representation\n        1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10           1  2  3  4  5  6  7  8  9  10\n            Number of Agents (M)                    Number of Agents (M)                    Number of Agents (M)\n\n              (a) K = 1                            (b) K = 3                             (c) K = 5\n\n                   Figure 15: Representation error on Wikiart dataset (Test).\n\n\nAgent behavior analysis. We select three agents and visualize the annotators they represent along\nwith their aggregated traits (cf. Figure 16). For example, Agent 2 represents annotators with high\nneuroticism and low conscientiousness. To validate whether the agents exhibit behaviors consistent\nwith the annotators they represent, we ask each constructed agent to complete the 10-item BFI test\n(Rammstedt & John, 2007), and their responses reveal traits that align with the average traits of the\ncorresponding humans. For instance, Agent 2 in Figure 16 obtains the following trait scores: Extraver-\nsion (1.0), Agreeableness (1.5), Conscientiousness (1.38), Neuroticism (4.66), and Openness (2.5).\n\nEmbedding model analysis. We further evaluate our approach on the Wikiart dataset using a smaller\nand more efficient bidirectional encoder (gte-base-en-v1.5). The results in Table 5 confirm that our\nmethods remain effective even when relying on a much smaller embedding model.\n\nTable 5: Comparison of methods on the Wikiart dataset using the gte-base-en-v1.5 embedding model.\nWe report the representation error on the test set with context size K = 3 and agent set size M = 10.\n\n                   Method                 gte-base-en-v1.5 (137M)\n                  SINGLE                         0.71\n               RANDOM                        0.69\n                 K-MEDOIDS                     0.68\n                SAMPLEGREEDY                0.68\n                   REPPOPdemo (ours)               0.62\n                      REPPOPmapped-1 (ours)            0.66\n                      REPPOPmapped-2 (ours)            0.68\n\n\n\n\n\n                                       19\n\nFigure 16: 2D embeddings of humans and agents constructed by REPPOPmapped-2 on Ttrain tasks inWikiart dataset using UMAP (McInnes & Healy, 2018). We provide examples of aggregated traits\n(heatmaps) of annotators represented by each agent (connections are denoted by black arrows). We\nnote that metadata are not used for constructing agents and used only for analysis. Our method\nREPPOPmapped-2 constructs agents to cover different areas in the annotator embedding space, col-\nlectively representing the annotators. Each agent represents a group of annotators with particular\npersonality traits.\n\n\n\n\n\n                                       20\n\nD  PROOFS\n\nD.1  PROOF OF THEOREM 1\nTheorem 1 (NP-Hardness). The problem of selecting an optimal subset L∗⊆L of size M thatmaximizes f(L) is NP-hard.\n\nProof. We show NP-hardness through a reduction from the k-facility location problem which extends\nthe uncapacitated facility location problem (UFLP) by including a constraint on the maximum number\nof facilities. The problem is known to be NP-hard. (Fowler et al., 1981)\n\nConsider an instance of the k-facility location problem with a set of potential facility locations\nF = {f1, f2, . . . , fnF }, a set of customers C = {c1, c2, . . . , cnC}, service costs d(f, c) representingthe cost of serving customer c from facility f, and a cardinality constraint M. The objective is to\n                                                                      c∈C minf∈F d(f, c).select a subset F ⊆F of M facilities to minimize the sum of service costs P\nWe construct a corresponding instance of our representative agent selection problem as follows:\nset H = C (each customer corresponds to a human); set L = F (each potential facility location\ncorresponds to a potential agent); define dist(eh, el) = d(l, h) for each human h ∈H and agent\nl ∈L; and set M to be the number of facilities we wish to open.\nUnder this construction, our objective function becomes:\n\n                           1\n                   f(L) =  X Dmax −minl∈L d(l, h)                            |H| h∈H\n                         !                           1\n                =  X Dmax   X min d(l, h)                    −1      l∈L                            |H|  h∈H          |H| h∈H\n                = Dmax  X min d(l, h)               −1      l∈L                                    |H| h∈H\n\nSince Dmax is a constant and |H|1  is a positive constant, maximizing f(L) is equivalent to minimizing\nPh∈H minl∈L d(l, h), which is the objective of the k-facility location problem.\nTherefore, if the representative agent selection problem could be solved in polynomial time, the\nk-facility location problem could also be solved in polynomial time. Since the k-facility location\nproblem is NP-hard, the representative agent selection problem must also be NP-hard.\n\n\nD.2  PROOF OF PROPOSITION 1\n\nProposition 1 (Submodularity). The objective function\n\n                              1\n                     f(L) =  X Dmax −minl∈L dist(eh, el)                              |H| h∈H\nis submodular.\n\nProof. A set function f is submodular if for all A ⊆B ⊆L and for all l′ ∈L \\ B, we have\nf(A ∪{l′}) −f(A) ≥f(B ∪{l′}) −f(B).\nLet A ⊆B ⊆L and l′ ∈L \\ B. The marginal gain of adding l′ to A is:\n                     1\n f(A ∪{l′}) −f(A) =  X Dmax − l∈A∪{l′}min  dist(eh, el) −  Dmax −minl∈A dist(eh, el)                      |H| h∈H\n\nThis simplifies to:\n\n                              1\n        f(A ∪{l′}) −f(A) =  X minl∈A dist(eh, el) − l∈A∪{l′}min  dist(eh, el)                               |H| h∈H\n\n                                       21\n\nDefine Hl′       A = {h ∈H | dist(eh, el′) < minl∈A dist(eh, el)} as the set of humans for whom l′\n                                        B for set B.provides improvement when added to A. Similarly define Hl′\nFor humans  in h  ∈ H \\ Hl′A,  the  closest  agent  in A  remains  closer  than   l′,  sominl∈A∪{l′} dist(eh, el) = minl∈A dist(eh, el), giving zero marginal improvement. Therefore:\n\n                                1\n           f(A ∪{l′}) −f(A) =  X  minl∈A dist(eh, el) −dist(eh, el′)                                 |H| h∈Hl′A\n\nSince A ⊆B, for any human h, we have minl∈A dist(eh, el) ≥minl∈B dist(eh, el).\n                                                            B), then l′ also provides improvementThis implies that if l′ provides improvement over B (i.e., h ∈Hl′\n                   A). Therefore,                        B                                       A.over A (i.e., h ∈Hl′                               Hl′                          ⊆Hl′\nFor humans in               B, the improvement when adding l′ to A is at least as large as when adding l′ to B,              Hl′\nsince minl∈A dist(eh, el) −dist(eh, el′) ≥minl∈B dist(eh, el) −dist(eh, el′).\nSimilarly, for B we have:\n\n                                 1\n          f(B ∪{l′}) −f(B) =  X  minl∈B dist(eh, el) −dist(eh, el′)                                  |H| h∈Hl′B\n\nTherefore:\n\n                              1\n        f(A ∪{l′}) −f(A) =  X  minl∈A dist(eh, el) −dist(eh, el′)                               |H| h∈Hl′A\n                              1\n                 =  X  min dist(eh, el)             el′)                                         l∈A           −dist(eh,                               |H| h∈Hl′B\n                                1\n                  +  X    min dist(eh, el)             el′)                                               l∈A           −dist(eh,                                 |H| h∈Hl′A\\Hl′B\n                              1\n                 ≥  X  minl∈B dist(eh, el) −dist(eh, el′)                               |H| h∈Hl′B\n                 = f(B ∪{l′}) −f(B)\n\nThis shows that f is submodular.\n\n\nD.3  PROOF OF THEOREM 2\nTheorem 2 (Performance Guarantee for REPPOPmapped-1 and REPPOPmapped-2). Let ˜L = {lh|h ∈\nH} be the proxy agent set where for each h ∈H, lh ∈Nρ(h),f(L∗H)  with Nρ(h) representing the ρ-\n                                                 H is the optimalneighborhood of h. Define the human coverage ratio γ = f(L∗L) ∈[0, 1], where L∗\nsubset from the human set and L∗L is the optimal subset from the full agent set. If LgreedyL˜     is the subset\nof size M returned by the greedy algorithm on ˜L, then:\n                           f(Lgreedy                         L˜                                          ) ≥(1 −1e)(γ ·                                            f(L∗L) −ρ)\n\nProof. In our context, L∗H ⊆H represents the optimal subset of humans of size M that would beselected if we directly choose humans instead of agents. This is a theoretical construct for analysis\n                 L∗purposes. In contrast,                  L is the optimal subset of size M from the actual agent set L. The human                   f(L∗                   H)\ncoverage ratio γ = f(L∗L) ∈[0, 1] measures how well selecting from the human set can approximate\n\n                                       22\n\nthe optimal solution achievable using the full agent set. We start our proof by first decomposing the\nobjective function as\n                               f(L) = X fh(L)\n                                  h∈H\nwhere\n                                1\n                       fh(L) =     Dmax −minl∈L dist(eh, el)                                 |H|\nFor each human h in the optimal subset L∗H, consider its corresponding agent lh in the proxy agent\n\n       H˜set ˜L. Let L∗        = {lh|h ∈L∗H}.\nBy definition, for each human h ∈H and its corresponding proxy agent lh ∈  ˜L, we have\ndist(eh, elh) ≤ρ since lh is in the ρ-neighborhood of h. Therefore:\n                                               ρ\n                                                     ˜                             ≤                               |fh(L∗H) −fh(L∗H)|                                                   |H|\n\nThe above inequality holds because the maximum distance deviation between any human and its\nproxy agent is at most ρ (by definition of the ρ-neighborhood).\n\nThen:\n\n                                                                 ρ\n                                                                            ˜                                               ˜                                         ≤        · |H| = ρ                          ≤ X |fh(L∗H) −fh(L∗H)|                   ˜ = X fh(L∗H) −fh(L∗H)   |f(L∗H) −f(L∗H)|                  h∈H                 h∈H                     |H|\n\nThis gives us:\n                              f(L∗H)˜ ≥f(L∗H) −ρ                                     (2)\nSince L∗                                                                                              is a feasible solution of      L˜                                           H˜            is the optimal subset of size M from the proxy agent set ˜L, and L∗\nsize M from ˜L, we have:                                                        ˜                                         (3)                                f(L∗L)˜ ≥f(L∗H)\nFrom the guarantees of the greedy algorithm for submodular function maximization (Nemhauser\net al., 1978), if Lgreedy            L˜                             is the subset of size M returned by the greedy algorithm on ˜L, we have:\n                               f(Lgreedy                                                  L)˜                                  (4)                            L˜                                               ) ≥(1 −1e)f(L∗\n\nCombining inequalities equation 2, equation 3, and equation 4, we get:\n\n            f(Lgreedy             L˜                                 L)˜ ≥(1 −1e)f(L∗                                          H)˜ ≥(1 −1e)(f(L∗                                                       H) −ρ)                         ) ≥(1 −1e)f(L∗\n\n                                               f(L∗H)\nUsing the definition of human coverage ratio γ = f(L∗L) , we have f(L∗H) = γ · f(L∗L). Substitution\nyields\n                           f(Lgreedy                         L˜                                          ) ≥(1 −1e)(γ ·                                            f(L∗L) −ρ)\n\nThis gives us the performance guarantees for REPPOPmapped-1 and REPPOPmapped-2.\n\n\n\n\n\n                                       23",
"headers": [
"arXiv:2510.07064v1  [cs.AI]  8 Oct 2025",
"P",
"O",
"A",
"M",
"R",
"D",
"H",
"ROMPT",
"PTIMIZATION",
"CROSS",
"ULTIPLE",
"GENTS",
"FOR",
"EPRESENTING",
"IVERSE",
"UMAN",
"OPULATIONS",
"1",
"I",
"2",
"W",
"3",
"F",
"4",
"5",
"E",
"6",
"C",
"T",
"B",
"S"
],
"tables": [
"|Tasks|Col2|\n|---|---|",
"|Col1|Col2|Col3|\n|---|---|---|\n|Humans|Humans||",
"|Dataset|Domain Task Type Repr. Type Multimodal No. Humans No. Tasks Source|\n|---|---|\n|EEDI<br>OpinionQA<br>Wikiart|Education<br>Multi-choice<br>Performance<br>No<br>50<br>40<br>Primary & High school students<br>Opinion Survey<br>Multi-choice<br>Opinion<br>No<br>500<br>77<br>US citizen<br>Image Annotation<br>Open-ended<br>Semantic<br>Yes<br>100<br>20<br>LLM-based annotators|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n||A group of 7<br>bars that th<br>3 new friend<br>1 more ch<br>now 10 fri<br>bars to sh<br>happened<br>chocolate ea<br>of 7 now rec<br>A. It has incr<br>B. It has dec<br>C. It has stay<br>D. We need|friends has 3 chocolate<br>  ey plan to share equally.<br>  s arrive and bring along<br>  ocolate bar. There are<br>  ends with 4 chocolate<br>  are equally. What has<br>to<br>the<br>amount<br>of<br> ch of the original group<br>   eive?<br>   eased<br>   reased<br>   ed the same<br>   more information|friends has 3 chocolate<br>  ey plan to share equally.<br>  s arrive and bring along<br>  ocolate bar. There are<br>  ends with 4 chocolate<br>  are equally. What has<br>to<br>the<br>amount<br>of<br> ch of the original group<br>   eive?<br>   eased<br>   reased<br>   ed the same<br>   more information|What should be the<br>dealing with illegal im<br>the U.S.?<br>A. Better border se<br>stronger<br>enforcement<br>immigration<br>B. Creating a way for<br>already here illegally<br>citizens<br>if<br>they<br>me<br>requirements<br>C. Both should be g<br>priority|priority fo<br>   migration i<br>   curity an<br> <br>of<br>ou<br>     immigrant<br>   to becom<br>et<br>certai<br>    iven equ|r<br>    n<br>    d<br>r<br>     s<br>    e<br>n<br>     al|r<br>    n<br>    d<br>r<br>     s<br>    e<br>n<br>     al|r<br>    n<br>    d<br>r<br>     s<br>    e<br>n<br>     al|r<br>    n<br>    d<br>r<br>     s<br>    e<br>n<br>     al|**Emotions:**sadness, pessimism, shyness<br>**Affective description:**<br>There's a definite melancholy to this<br>one. The figure on the goat is oddly<br>shaped and indistinct, almost like a<br>shadow, giving a lonely feeling. The<br>muted colors and blurred background<br>really<br>emphasize<br>that<br>sense<br>of<br>isolation. It’s not a vibrant scene, but a<br>quiet one, which feels a bit reserved<br>and shy. And overall, it leaves me with<br>a <br>somewhat<br>bleak,<br>pessimistic<br>impression, like something has been<br>lost or is fading away.|**Emotions:**sadness, pessimism, shyness<br>**Affective description:**<br>There's a definite melancholy to this<br>one. The figure on the goat is oddly<br>shaped and indistinct, almost like a<br>shadow, giving a lonely feeling. The<br>muted colors and blurred background<br>really<br>emphasize<br>that<br>sense<br>of<br>isolation. It’s not a vibrant scene, but a<br>quiet one, which feels a bit reserved<br>and shy. And overall, it leaves me with<br>a <br>somewhat<br>bleak,<br>pessimistic<br>impression, like something has been<br>lost or is fading away.|",
"|Single K-Med Single RepPopdemo (ours)|Sing|gl|Single K-Med le RepPopdemo (ours)|doi|ids RepPop(ours) Single RepPopdemo (ours)|Col7|RepPop(ours) Single RepPopdem|mo|\n|---|---|---|---|---|---|---|---|---|\n|Single<br>Random<br>K-Me<br>Sampl<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>Representation Error|||Single<br>Random<br>K-Me<br>Sampl|do<br>eG|ids<br>reedy<br>RepPopdemo (ours)<br>RepPopmapped_−_1 (ours)||RepPopmapped_−_2 (ours)||\n|Single<br>Random<br>K-Me<br>Sampl<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>Representation Error|||Single<br>Random<br>K-Me<br>Sampl|1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|",
"|REPPOPdemo|0.37|0.30|0.33|0.31|0.33|0.35|\n|---|---|---|---|---|---|---|\n|REPPOPmapped-1|0.38|0.35|0.34|0.32|0.35|0.38|\n|REPPOPmapped-2|**0.35**|0.33|**0.30**|0.32|0.35|0.38|",
"|Method|Dataset|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|**Method**|**EEDI**|**EEDI**|**EEDI**|**OpinionQA**|**OpinionQA**|**OpinionQA**|**Wikiart**|**Wikiart**|**Wikiart**|\n|**Method**|_K_ = 1<br>_K_ = 3<br>_K_ = 5|_K_ = 1<br>_K_ = 3<br>_K_ = 5|_K_ = 1<br>_K_ = 3<br>_K_ = 5|_K_ = 1<br>_K_ = 3<br>_K_ = 5|_K_ = 1<br>_K_ = 3<br>_K_ = 5|_K_ = 1<br>_K_ = 3<br>_K_ = 5|_K_ = 1<br>_K_ = 3<br>_K_ = 5|_K_ = 1<br>_K_ = 3<br>_K_ = 5|_K_ = 1<br>_K_ = 3<br>_K_ = 5|\n|SINGLE<br>RANDOM<br>K-MEDOIDS<br>SAMPLEGREEDY|0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._9_ ±_ 0_._0<br>1_._1_ ±_ 0_._0<br>1_._3_ ±_ 0_._0<br>0_._6_ ±_ 0_._0<br>0_._7_ ±_ 0_._0<br>0_._8_ ±_ 0_._0|0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._9_ ±_ 0_._0<br>1_._1_ ±_ 0_._0<br>1_._3_ ±_ 0_._0<br>0_._6_ ±_ 0_._0<br>0_._7_ ±_ 0_._0<br>0_._8_ ±_ 0_._0|0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._9_ ±_ 0_._0<br>1_._1_ ±_ 0_._0<br>1_._3_ ±_ 0_._0<br>0_._6_ ±_ 0_._0<br>0_._7_ ±_ 0_._0<br>0_._8_ ±_ 0_._0|0_._6_ ±_ 0_._0<br>0_._4_ ±_ 0_._0<br>0_._4_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>1_._6_ ±_ 0_._0<br>1_._7_ ±_ 0_._0<br>0_._9_ ±_ 0_._0<br>2_._7_ ±_ 0_._1<br>2_._8_ ±_ 0_._0<br>3_._2_ ±_ 0_._0|0_._6_ ±_ 0_._0<br>0_._4_ ±_ 0_._0<br>0_._4_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>1_._6_ ±_ 0_._0<br>1_._7_ ±_ 0_._0<br>0_._9_ ±_ 0_._0<br>2_._7_ ±_ 0_._1<br>2_._8_ ±_ 0_._0<br>3_._2_ ±_ 0_._0|0_._6_ ±_ 0_._0<br>0_._4_ ±_ 0_._0<br>0_._4_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>0_._3_ ±_ 0_._0<br>1_._6_ ±_ 0_._0<br>1_._7_ ±_ 0_._0<br>0_._9_ ±_ 0_._0<br>2_._7_ ±_ 0_._1<br>2_._8_ ±_ 0_._0<br>3_._2_ ±_ 0_._0|2_._0_ ±_ 1_._0<br>1_._8_ ±_ 0_._9<br>1_._6_ ±_ 0_._6<br>3_._2_ ±_ 1_._8<br>1_._8_ ±_ 0_._8<br>1_._6_ ±_ 0_._6<br>2_._5_ ±_ 0_._0<br>1_._9_ ±_ 0_._0<br>2_._3_ ±_ 0_._0<br>2_._2_ ±_ 0_._0<br>1_._8_ ±_ 0_._0<br>2_._2_ ±_ 0_._0|2_._0_ ±_ 1_._0<br>1_._8_ ±_ 0_._9<br>1_._6_ ±_ 0_._6<br>3_._2_ ±_ 1_._8<br>1_._8_ ±_ 0_._8<br>1_._6_ ±_ 0_._6<br>2_._5_ ±_ 0_._0<br>1_._9_ ±_ 0_._0<br>2_._3_ ±_ 0_._0<br>2_._2_ ±_ 0_._0<br>1_._8_ ±_ 0_._0<br>2_._2_ ±_ 0_._0|2_._0_ ±_ 1_._0<br>1_._8_ ±_ 0_._9<br>1_._6_ ±_ 0_._6<br>3_._2_ ±_ 1_._8<br>1_._8_ ±_ 0_._8<br>1_._6_ ±_ 0_._6<br>2_._5_ ±_ 0_._0<br>1_._9_ ±_ 0_._0<br>2_._3_ ±_ 0_._0<br>2_._2_ ±_ 0_._0<br>1_._8_ ±_ 0_._0<br>2_._2_ ±_ 0_._0|\n|REPPOPdemo|6_._4_ ±_ 0_._1|21_._3_ ±_ 0_._2|40_._1_ ±_ 0_._4|38_._3_ ±_ 0_._3|108_._4_ ±_ 0_._9|179_._0_ ±_ 31_._7|16_._4_ ±_ 0_._1|43_._6_ ±_ 0_._3|78_._2_ ±_ 0_._2|\n|REPPOPmapped-1|0_._6_ ±_ 0_._0|0_._7_ ±_ 0_._0|0_._8_ ±_ 0_._0|2_._7_ ±_ 0_._1|2_._9_ ±_ 0_._0|3_._2_ ±_ 0_._1|2_._2_ ±_ 0_._0|1_._8_ ±_ 0_._0|2_._2_ ±_ 0_._0|\n|REPPOPmapped-2|5_._9_ ±_ 0_._0|17_._8_ ±_ 0_._0|30_._9_ ±_ 0_._1|71_._7_ ±_ 0_._3|197_._1_ ±_ 0_._3|350_._1_ ±_ 0_._9|20_._2_ ±_ 0_._2|43_._0_ ±_ 0_._1|66_._8_ ±_ 0_._5|",
"|C.1 EEDI|C.1 EEDI|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|||||||\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||R|epPopmapped_−_2 (ours)|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||||\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S||mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>Representation Error|mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>Representation Error||||",
"|Single K<br>Random S<br>0.55 Error<br>0.50<br>0.45 Representation<br>0.40<br>0.35<br>0.30<br>1 2 3 4 5 6 7 8 9 10<br>Number of Agents (M)|Single K<br>Random S|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||R|epPopmapped_−_2 (ours)|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||||\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S||mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>Representation Error|mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>Representation Error||||",
"|Single K<br>Random S<br>1.10 Error<br>1.05 Representation<br>1.00<br>0.95<br>0.90<br>1 2 3 4 5 6 7 8 9 10<br>Number of Agents (M)|Single K<br>Random S|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||R|epPopmapped_−_2 (ours)|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||||\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S||mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.80<br>0.85<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.80<br>0.85<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error||||",
"|Single K<br>Random S<br>1.12 Error<br>1.10<br>1.08 Representation<br>1.05<br>1.03<br>1.00<br>0.98<br>1 2 3 4 5 6 7 8 9 10<br>Number of Agents (M)|Single K<br>Random S|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.98<br>1.00<br>1.03<br>1.05<br>1.08<br>1.10<br>1.12<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||R|epPopmapped_−_2 (ours)|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.98<br>1.00<br>1.03<br>1.05<br>1.08<br>1.10<br>1.12<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||||\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.98<br>1.00<br>1.03<br>1.05<br>1.08<br>1.10<br>1.12<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S||mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error|mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>0.90<br>0.95<br>1.00<br>1.05<br>1.10<br>Representation Error||||",
"|C.3 WIKIART|C.3 WIKIART|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|||||||\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||R|epPopmapped_−_2 (ours)|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||||\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S||mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>1.75<br>2.00<br>2.25<br>2.50<br>2.75<br>3.00<br>Representation Error|mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>1.75<br>2.00<br>2.25<br>2.50<br>2.75<br>3.00<br>Representation Error||||",
"|Single K<br>Random S<br>Error<br>3.00<br>2.80 Representation<br>2.60<br>2.40<br>2.20<br>2.00<br>1 2 3 4 5 6 7 8 9 10<br>Number of Agents (M)|Single K<br>Random S|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>3.00<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||R|epPopmapped_−_2 (ours)|\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>3.00<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S|-M<br>am|e<br>pl|doids<br>eGreedy<br>RepPopdemo (ours)<br>RepPop (ours)||||\n|Single<br>Random<br>K<br>S<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>3.00<br>Representation Error|Single<br>Random<br>K<br>S|Single<br>Random<br>K<br>S||mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>3.00<br>Representation Error|mapped_−_1 <br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>Number of Agents (_M_)<br>2.00<br>2.20<br>2.40<br>2.60<br>2.80<br>3.00<br>Representation Error||||"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2510.07064v1.pdf"
}