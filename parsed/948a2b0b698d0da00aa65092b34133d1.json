{
"text": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n         TOWARDS HIERARCHICAL MULTI-AGENT WORK-\n          FLOWS FOR ZERO-SHOT PROMPT OPTIMIZATION\n\n\n                     Yuchi Liu1, Jaskirat Singh1, Gaowen Liu2, Ali Payani2, Liang Zheng1\n                         1Australian National University, 2Cisco\n                 {yuchi.liu, jaskirat.sing, liang.zheng}@anu.edu.au\n                 {gaowen.liu, ali.payani}@cisco.com\n\n2025                       Large language models (LLMs) haveABSTRACTshown great progress in responding to user\n                                questions, allowing for a multitude of diverse applications. Yet, the quality of\n                   LLM outputs heavily depends on the prompt design, where a good prompt might\n                               enable the LLM to answer a very challenging question correctly. Therefore, recentApr\n                            works developed many strategies for improving the prompt, including both manual\n15                           craftingnarios remainsand in-domainquestionable,optimization.as the formerHowever,dependstheironefficacyhuman indesignunrestrictedfor specificsce-\n                                questions and the latter usually generalizes poorly to unseen scenarios. To address\n                                these problems, we give LLMs the freedom to design the best prompts according\n                                  to themselves. Specifically, we include a hierarchy of LLMs, first constructing a\n                           prompt with precise instructions and accurate wording in a hierarchical manner,\n                           and then using this prompt to generate the final answer to the user query. We[cs.CL]                        term this pipeline Hierarchical Multi-Agent Workflow, or HMAW. In contrast with\n                                   prior works, HMAW imposes no human restriction and requires no training, and is\n                              completely task-agnostic while capable of adjusting to the nuances of the underly-\n                               ing task. Through both quantitative and qualitative experiments across multiple\n                             benchmarks, we verify that despite its simplicity, the proposed approach can create\n                                  detailed and suitable prompts, further boosting the performance of current LLMs.\n                                 Project page: liuyvchi.github.io/HMAW_project/\n\n\n\n                1  INTRODUCTION\n\n\n                     Large language models (LLMs) can perform a wide range of tasks such as creating detailed literary\n                    works and generating computer codes. Yet they often give unsatisfactory responses when the prompt\n                              is poorly designed. To design better prompts that make the LLM more effective, prompt optimization\n                      has been extensively explored in recent years. Specifically, existing works on prompt optimizationarXiv:2405.20252v2               can be categorized as follows. The first category involves the manual design of prompts, such as the\n                      chain of thought (CoT) (Wei et al., 2022) and its variants (Kojima et al., 2022; Besta et al., 2023;\n                   Yao et al., 2023). The second category focuses on optimizing prompts on a training set that contains\n                        input-response pairs to learn a golden prompt, as demonstrated by APE (Zhou et al., 2023). The third\n                       category also uses handcrafted prompts, but different from the first category, the handcrafted prompts\n                       include examples that can guide an LLM agent to further generate prompts for a second LLM that\n                         actually provide the answer, e.g., ExpertPrompting (Xu et al., 2023).\n\n                    However, these methods generally have limited generalization performance across various tasks,\n                       as shown in Fig. 1 (a), (b), and (c). The first category (Wei et al., 2022) relies on handcrafted\n                      prompts. While they are very useful for tasks such as math, the same prompting method, if applied\n                         to other tasks, would be less effective. The second category, (Zhou et al., 2023; Yang et al., 2024a),\n                        fine-tuned on a specific dataset, can obtain good performance for in-domain tasks. However, these\n                        learned prompts from one domain might fail for other tasks, making them not task-agnostic. The third\n                       category (Zhou et al., 2023), which uses LLM to adjust the prompt, is still limited by the pre-defined\n                    examples in the handcrafted prompts. These examples only cover a finite number of scenarios and\n                     can potentially limit the quality of the modified prompt.\n\n\n                                                           1\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n                                                                                             Query-specific prompt         Fixed prompt\n\n                                  ‚Äúthink it step by step‚Äù                                                                                     ‚ÄúIdentify the expert‚Ä¶‚Äù    LLM     ‚ÄúYou are a meteorologist ‚Ä¶‚Äù                     response\n                            Manual prompt                          response                              Manual prompt                   generated prompt            ‚Ä¶15 degrees Celsius (59\n                                                                         1.                                                             Check                                                                           Location                                                                             ‚Ä¶.\n   ‚ÄúHi, what is the weather today in London‚Äù         LLM                                                                         2.                                                                  Access                                                                Weather                                                                               Source ‚Ä¶                                                          LLM          degreesa breezeFahrenheit)‚Ä¶around 10\n                 User query                                         3.4.  RetrieveReport ‚Ä¶Current Conditions                               ‚ÄúHi, what is the weather todayUserinqueryLondon‚Äù                                 kilometers per hour‚Ä¶\n                       (a) COT                                                                    (c) ExpertPrompting\n                                                                                                                                                                                               response\n                         ‚Äúimprove the code readbility‚Äù                                                                                      <Instructions              Forecasting,                     <weather info>‚Ä¶\n                                                                                                                                                                                   chance                                                                                                                                                                                                                     of                                                                                                                                                                                         showers                                                                                                                                                                                                                                                       later\n         Dataset                                                                                                                                                                                             the day,                                                                                                                                                                                                                                                                                                                                 If                                                                                                                                                                                                                  you're                   for           Learned prompt                            response                                               to Manager>            User engagement                Ain           coding                                                                                                                                                                                       heading                                                                                                                                                                                                                        out,                                                                                                                                                                                                                consider\n                                                                   location                                                             =                                                                           input(\"Please enter your                                                                                                                                                      Worker                                                                                                                                          Worker                                                                                                        User                                                                                                                           Manager                                                                                               CEO      Manger                                                                                                                                                                                                            bringing                                                                                                                                                                                an umbrella just  ‚ÄúHi, what is the weather today in London‚Äù        LLM                                                                     location:                                                                                        \")                                                                                                                                                                               Layer                                                                                                                                          prompt                                                                                                       query                                                                                                                                                   Layer                                                                                                                   prompt                                                                                                                         Layer                                                                                                                                                                                                                                  in case!\n                User query                                  weather_service = WeatherService()\n                      (b) APE                                                               (d) HMAW (ours)\n\nFigure 1: Examples comparing the generalization ability of existing methods and the proposed\none. (a) COT (Wei et al., 2022) uses a handcrafted prompt, which might not be suitable for all\ntasks. (b) APE (Zhou et al., 2023) fine-tunes the prompt on a specific dataset, and its generalization\ncapability to other scenarios is questionable. (c) ExperPrompting (Xu et al., 2023) includes few-shot\nexamples in the system prompt to help an LLM convert the user query to a format more suitable\nfor LLM, but these examples might not be able to cover all scenarios. (d) Our method adopts a\nhierarchical design in reformatting the user query. Free from pre-defined few-shot examples, the\ninteraction between the hierarchy allows for more generalizable yet more adaptive tuning of prompts.\n\n\n\nIn this paper, we introduce a more generalizable prompt optimization method named hierarchical\nmulti-agent workflow (HMAW). In a nutshell, our method mimics the hierarchy in companies, where\nthe CEO and the manager create the guidelines for workers, and workers execute user-specified tasks\nwhile following these guidelines. Similarly, our method has a CEO ‚ÜíManager ‚ÜíWorker workflow,\nwhich takes the initial user query as input and outputs the user response. The CEO and the Manager\nLLMs work in a hierarchical manner to generate accurate and detailed instructions, which are then\nused as prompts for the Worker LLM to generate the final answer.\n\nCompared with existing prompt optimization methods, HMAW has two advantages that contribute to\nits generalization ability. First, having a hierarchy of LLM agents that each has its own job greatly\nsimplifies the task for them. In this way, LLMs can either focus on either the overall goal (CEO),\nor creating a more detailed checklist (Manager), or giving specific answers (Worker). Second, our\nmethod does not require few-shot examples or any training set, thus does not fit to certain tasks, and\nhas the additional benefit to be easy to use. As such, our method is zero-shot, task-agnostic, and\nprompt-specific. We compare HMAW with existing literature in Table 1.\n\nOn multiple benchmarks, we experimentally verify the effectiveness of the proposed hierarchical\napproach. When combined with Mixtral (Jiang et al., 2024), HMAW achieves a significant average\nimprovement of 30.7% across 5 datasets.\n\n2  RELATED WORK\n\nManual prompt engineering integrates human-like problem-solving knowledge. For example,\nfew-shot prompting (Brown et al., 2020) allows language models (LMs) to generate responses by\nproviding them with explicit examples. The chain-of-thought (CoT) method (Wei et al., 2022), along\nwith its variants such as zero-shot CoT (Kojima et al., 2022), graph-of-thought (GoT) (Besta et al.,\n2023), and tree-of-thought (ToT) (Yao et al., 2023), intricately designs prompts to emulate various\ntypes of human-like reasoning processes. Some other works (Welleck et al., 2023; Shinn et al.,\n2023; Gou et al., 2024) manually craft prompts that encourage LLMs to engage in critical thinking\nand verification processes before delivering the final answer. Those manual prompt engineering\ntechniques typically require carefully tailored prompts for various tasks, and for each task, the\nprompts used remain the same across different user queries. In comparison, we design a task-agnostic\nprompter that can create tailored prompts for every user query.\n\nAutomated prompt engineering methods. Instead of using fixed hand crafted in-context prompts,\nautomated prompt engineering methods are attracting increasing attention. This line of research\naims to let LLMs be prompt generators which automatically generate specific prompts for each\nuser query so that LLMs can respond better. Recent methods include automatic prompt engineer\n(APE) (Zhou et al., 2023), expert prompting (EP) (Xu et al., 2023), synthetic prompting (Shao\n\n\n                                       2\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n   Method                                      Zero-Shot      Task-agnostic      Query-Specific\n   Chain of Thought (CoT) Wei et al. (2022)         ‚úó         ‚úì             ‚úó\n    Zero-shot CoT Kojima et al. (2022)          ‚úì         ‚úì             ‚úó\n    Static ExpertPromting (Xu et al., 2023)        ‚úì         ‚úì             ‚úó\n   Dynamic ExpertPromting (Xu et al., 2023)        ‚úó            ‚úó          ‚úì\n   on Meta-Prompting (de Wynter et al., 2023)        ‚úó         ‚úì          ‚úì\n   Rephrase and Response (Deng et al., 2023)      ‚úì         ‚úì             ‚úó\n   Multi-Persona (Wang et al., 2023b)              ‚úó         ‚úì          ‚úì\n  APE (Zhou et al., 2023)                      ‚úó            ‚úó              ‚úó\n   PromptAgent (Wang et al., 2024)               ‚úó            ‚úó              ‚úó\n  HMAW (Ours)                     ‚úì         ‚úì          ‚úì\n\nTable 1: Method comparison based on various properties. Our method is zero-shot, task-agnostic,\nand query-specific, which are useful for dealing with diverse tasks.\n\n\net al., 2023), skeleton-of-thought (SoT), (Ning et al., 2024), multi-personas (Wang et al., 2023b),\non-meta-prompting de Wynter et al. (2023), meta-prompting (Suzgun & Kalai, 2024), etc. While\nthese methods can generate specific prompts according to different original prompts or instructions,\nthey are generally not task-agnostic or zero-shot, where manually designed examples are required.\nIn other words, they would not be very suitable for generalizing across ever changing landscape of\nLLMs. In comparison, our method is completely zero-shot, task-agnostic and prompt-specific.\n\nOptimization based self-prompt refinement. Using optimization techniques to improve prompts\nis a recent focus. Early attempts including prompt tuning Lester et al. (2021) and prefix tuning Li\n& Liang (2021) tried to learn a trainable prefix or soft prompts for better prompting. Recent ones,\ne.g.,, directional stimulus prompting (DSP) (Li et al., 2024), prompt-OIRL Sun et al. (2023), use\nreinforcement learning to train an auxiliary model to generate query-specific prompts. OPRO (Yang\net al., 2024a) makes LLMs itself a optimizer which guides LLMs to produce outputs that align with a\npredefined objective. Those works typically require numerous iterations of training for each task and\nare thus not task-agnostic, and it is costly to adapt them to new tasks. Our work also optimizes user\nprompt and is thus closely related to this line of works. However, our method is learning-free and\ntask-agnostic.\n\nZero-shot self-prompt refinement. Some manual prompt engineering methods, such as Zero-shot\nCoT (Kojima et al., 2022), Static ExpertPrompting (Xu et al., 2023), and RaR (Deng et al., 2023),\nfollow the zero-shot paradigm. Because no prompt examples or datasets are used, these methods are\ntypically task-agnostic as well. Despite their simplicity and ease of adaptation to various tasks, their\nprompt optimization capability is limited by their shallow understanding of the user query. More\nimportantly, they are not guaranteed to be query-specific (e.g., Static ExpertPrompting and RaR),\nwhich further limits their capability. In this work, we design a hierarchical multi-agent workflow to\naddress these shortcomings.\n\n\n3  PROPOSED APPROACH\n\n3.1  PRELIMINARIES\n\nNotations. In language generation tasks, the language model M is asked to provide responses\nto queries in the textual space T . Given a dataset D = {(qi, y‚àói )}ni=1, where n is the number of\npairs, each (qi, y‚àói ) pair consists of a text query qi ‚ààT and the corresponding golden response\ny‚àói ‚ààT . We use prompt p ‚ààT to denote input of the large language model (LLM) M. In practice,\np may not be equal to q and can be the prompt engineering result f(q), where f : T ‚ÜíT , f ‚ààF,\nis the prompt-engineering mapping function. In evaluation, we use a metric s : T √ó T ‚ÜíR to\nmeasure the quality of the responses ÀÜyi = M(f(qi)). If D provides golden responses, s measures\nthe correctness of yi, i.e., s(ÀÜyi, y‚àói ) = 1{ÀÜyi = y‚àói }, e.g., when D consists of mathematical or multiple\nchoice questions where answers are objective. If D involves open questions whose golden responses\nare not predetermined, we use s(ÀÜyi) to measure the quality of the response ÀÜyi generated by M.\n\nPrompt optimization. Directly prompting LLMs M using the initial human query q may not be\nthe optimal. Ideally, an optimal prompt p‚àóallows M to generate the golden response y‚àó. Given a\n\n\n                                       3\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\nspecific pair (qi, y‚àói ), the task of finding such p‚àói can be formulated as a prompt optimization task\nwhich aims to maximize the following expected quality score:\n\n                             p‚àói = arg max s(y‚àói , M(p | qi)).                                (1)\n                                    p‚ààT\n\nSome methods manually combine qi with some carefully designed fixed prefix or suffix to formulate\np‚àói . For example, few-shot prompting (Brown et al., 2020) uses a few examples of query-answer pairs\nbeside the query of interest qi; the zero-shot chain-of-thought (CoT) (Wei et al., 2022) is formulated\nas ‚Äòqi ‚äï[let us think step by step]‚Äô. Others use automatic methods, which are described in Section 2.\n\nA recent line of research (e.g., RAR (Deng et al., 2023), ExpertPrompting (Xu et al., 2023), Multi-\nPersona (Wang et al., 2023b)) focuses on leveraging the text generation capacity of LLMs to directly\ngenerate p‚àói by providing some context C ‚ààT of the optimization task in Eq. 1. Those methods are\ndeployment-friendly because of characteristics like zero-shot, task-agnostic, and query-specific. In\nthis paper, we not only keep those characteristics but also use a workflow to optimize prompts.\n\n\n3.2  HIERARCHICAL MULTI-AGENT WORKFLOW\n\nIntuition. Now, we consider prompt optimization as a complex and challenging task where prompts\nare hard to optimize by traditional optimization method. Our design is motivated by recent multi-\nagent workflow methods (Yang et al., 2024b; Wu et al., 2024) which have shown promise in solving\ncomplex tasks. We thus employ the idea of cooperative multiple agents in the prompt optimization\ntask.\n\nHierarchical workflow. We design a hierarchical workflow which performs prompt optimization\nand then task response generation. The user query qi is the input of the workflow and is processed\nand improved by a hierarchy of agents. The workflow output the final is task response ÀÜyi from an\nLLM prompted by the improved user query.\n\nThe proposed workflow operates like a company with three layers, as shown in Fig. 1 and Fig. 2.\nEach layer plays a different role: ‚ÄòCEO‚Äô, ‚ÄòManager‚Äô, and ‚ÄòWorker‚Äô, respectively. The CEO layer takes\nthe initial query qi as input and outputs the CEO‚Äôs instruction qci to the Manager layer. The Manager\nlayer responds to the CEO‚Äôs instruction and generates its own instruction for the Worker layer. The\nWorker layer receives Manager instructions and is responsible for yielding the final response ÀÜyi for\nthe initial query qi. The Manager and Worker layers also receive the user query qi.\nLayer design. As shown in Fig. 2, each layer has a contextual description constant cj, a prompter f j,\nand an LLM agent Mj, where j ‚àà{c, m, w} and c, m, w refer to the CEO, manager, and worker,\nrespectively. The context description for each layer contains the role, task description, company\nstructure, company workflow, and important notices for this layer. The detailed context descriptions\nfor each layer are provided in Appendix A.1.\nPrompter f j is an operator that concatenates cj, qi, and the instructions output from the previous\nlayer (when j = m and j = w) into a prompt pji for the LLM agent Mj in this layer. For the CEO\nand Manager layers, the agent Mj generates the result qji , which serves as the instruction for the next\nlayer. Note that the CEO layer does not receive instructions from other layers. In the worker layer,\nthe refined prompt is p‚àói , which is fed to the Worker LLM to produce the final response ÀÜyi.\n\n\n3.3  DISCUSSION\n\nAdvantage of a hierarchical structure over a single node. For complex prompts such as improving\ncode readability, a single node such as ExpertPrompting may not be able to exhibit a deep understand-\ning. In the proposed hierarchical structure, global instructions are given by the CEO, and more local\ninstructions by the manager. This allows for layered understanding of the prompt and thus makes it\neasy for the worker LLM to generate more satisfying answers.\n\nWhy using skip connections? As illustrated in Fig. 2, the prompters in both the manager layer and\nthe worker layer concatenate not only the context information and the orders from the preceding layer\nbut also the initial query qi. This step is essential to preserve the specificity of the initial query in the\nsystem, avoiding the dilution of critical details. Moreover, if the instructions generated by a previous\n\n\n                                       4\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n                                                   Hierarchical Workflow\n\n           ùê∂!     CEO Layer               ùê∂#       Manager Layer              ùê∂$         Worker Layer\n\n                                                                          LLM                                 LLM                                                                                                                    LLM                                       ùëì#                                                                         ùëì$               ùëì!\n                                                                                                                             ùëù\"‚àó         Worker ùëû!    ‚äï                           ùëù\"!        CEO                                                    ùëû\"!   ‚äï                                                                                                    ùëû\"$   ‚äï                                                                           ùëù\"#        Manager                                                                                                                                                                                                            ùë¶#!\n\n\n\nFigure 2: Method Overview. We propose modeling the prompt optimization problem as a zero-shot\noutput within a multi-agent workflow. The initial query, qi, is first inputted into the first layer of our\nframework (the COE layer). Before being processed by the CEO LLM agent, qi is transformed into\nan LLM prompt pci by the prompter f c, which also concatenates it with the context Cc in the CEO\nlayer. The output of the first layer, qci , serves as the query from the CEO layer to the Manager layer.\nSimilarly, the Manager Layer and the Worker Layer each include their own prompters, f m and f w,\nrespectively. Besides concatenating the content of this layer, the initial query qi is also concatenated\nto enhance stability. The input for the Worker LLM is our optimized prompt Pi‚àó , which directly\ntriggers the LLM agent to generate the final response to the original query qi.\n\n\nlayer are not correct, incorporating the initial query would reinforce the original intent, thereby\npreventing misinterpretation. Results in Section 5.1 demonstrate the usefulness of skip connections.\n\nAI workflow using tools to solve complex tasks. Some existing works decompose complex task\ninto individual steps which use different tools. After these steps are executed, it is expected that the\ntask is complete. Our work is different in that we do not perform task decomposition and instead use\nthe refined prompt in one go. This paper follows the standard set up of prompt optimization.\n\n\n4  EXPERIMENTAL SETUP\n\n4.1  DATASETS AND EVALUATION METRICS\n\nATLAS (Bsharat et al., 2023) is a manually crafted benchmark for principled prompt evaluation.\nIt contains a standard subset featuring questions across various domains, along with a challenging\nsubset dedicated to reasoning and other complex tasks. It contains 520 questions, and each one has\nthe corresponding output of various LLMs such as GPT-3.5 and Llama 2.\n\nFED (Mehri & Eskenazi, 2020) is a comprehensive collection of annotated human-system and\nhuman-human conversations, featuring 18 fine-grained dialog qualities. It is comprised of 3,348\nturn-level and 1,364 dialog-level data points, totaling 4,712. FED provides evaluation at both the turn\nand whole dialog levels. With moderate to strong correlation to human judgment across these levels,\nthe dataset offers a reliable benchmark for analyzing interactive dialog systems.\n\nGSM8K. The GSM8K dataset (Grade School Math 8K) (Cobbe et al., 2021) is a collection of 8,791\ngrade-school-level math word problems, with detailed step-by-step solutions.  It includes 7,473\ntraining problems and 1,319 test problems. Designed by OpenAI, GSM8K serves as a benchmark for\nevaluating mathematical reasoning in machine learning models. We report results on its test set.\n\nCodeNet. Project CodeNet (Puri et al., 2021) is a large-scale dataset consisting of code samples across\nover 50 programming languages. We follow the practice in Self-Refine (Madaan et al., 2024) and\nconsider a random subset of 300 examples in Python. We compare different prompting optimization\nmethods on the task of improving code readability.\n\nEducation. We collected a new datasets focusing on providing appropriate teaching responses to\nstudents. A student question can be ‚Äòwe just started looking at genes and stuff in science class...what\nthis gene editing thing is all about.‚Äô. The LLM is supposed to give a teacher-style reply which,\npreferably, should consider student background and is easy to understand. This dataset has 100\nstudent questions generated by GPT4, which were then manually cleaned.\n\nWe use two metrics to evaluate the quality of generated responses. For the objective task GSM8K,\nwe use the provided answer to compute the accuracy of the responses. For subjective tasks (i.e.,\nEducation, ATLAS, FED, CodeNet), we follow the practice in (Madaan et al., 2024), which uses\nLLMs as the evaluator. Specifically, we compare the response pair generated from the optimized\n\n\n                                       5\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\nTable 2: Comparison with existing prompt optimization methods. For ATLAS, FED, CodeNet, and\nEducation datasets, we GPT-3.5 to give a preference score (%) between results obtained by ‚ÄòNo\nPrompting‚Äô and a prompt optimization method. For example, 41.4% under ‚Äòw/o‚Äô means in 41.1% of\nthe pairs, GPT-3.5 prefers answers produced by ‚ÄòNo Prompt‚Äô; 63.4% under ‚Äòw‚Äô means in 63.4% of the\npairs GPT-3.5 prefer answers produced after prompt optimization. Higher is better. For GSM8K, we\nreport accuracy (%) of the generated responses w.r.t ground-truth answers, and higher is better. We\nalso report average performance across the five tasks and the absolute improvement (%) of prompt\noptimization over no prompting.\n\n                         ATLAS   FED   CodeNet Education GSM8K       Avg.\n Method\n                              w/o w  w/o w  w/o w  w/o  w  w/o w  w/o    w\n\n                                               Zero-shot\n\n Zero-CoT (Wei et al., 2022)        41.4 58.6 43.2 56.8 45.6 54.4 41.5 58.5 68.6 73.3 48.1 60.3 (‚Üë12.2)\nRaR (Deng et al., 2023)           36.6 63.4 24.9 75.1 60.9 39.2 49.5 50.5 68.6 72.8 48.1 60.2 (‚Üë12.1)\nOn-MP (de Wynter et al., 2023)    53.6 46.4 31.8 68.2 50.7 49.3 47.5 52.5 68.6 61.3 50.4  55.5 (‚Üë5.1)\n Static-EP (Xu et al., 2023)         44.0 56.0 39.2 60.8 42.4 57.6 52.5 47.5 68.6 73.5 49.3  59.1 (‚Üë9.8)\nHMAW (Ours)                    35.9 64.1 13.8 86.2 35.6 64.4 38.8 61.3 68.6 70.3 38.5 69.2 (‚Üë30.7)\n\n                                               Multi-shot\n\n Dynamic-EP (Xu et al., 2023)      45.5 54.5 68.6 71.3 44.8 55.3 46.9 53.1 68.6 66.9 54.9  60.2 (‚Üë5.4)\n Multi-Persona (Wang et al., 2023b) 73.2 26.8 50.9 49.1 75.2 24.8 57.5 42.5 68.6 71.5 65.1 42.9 (‚Üì22.2)\n\n                                   Optimized from a training set\n\nAPE (Zhou et al., 2023)           45.9 54.1 44.6 55.4 39.5 60.5 45.8 54.3 68.6 70.3 48.9 58.9 (‚Üë10.0)\nprompt and the response generated from the initial query by asking GPT-3.5 which response it prefers.\nGPT3.5 then assigns a score of 1 to the preferred response and a score of 0 to the other. The prompt\nused for the Evaluator agent is shown in the Appendix A.2. To reduce the impact of the primacy effect\nissue (Wang et al., 2023a) and bias of LLMs in multiple choice questions, we follow the practice in\nPriDe (Zheng et al., 2024) and permute option contents. Specifically, we run each test case twice\nwith switched response orders and compute the average score. We then average these scores across\nthe entire test set. The average score ranges from 0 to 1.\n\n\n4.2  COMPARED METHODS\n\nWe compared our method to the following types of prompt optimization methods.\n\nSingle Agent, Zero-shot. (1) No prompting. The initial query from the dataset is directly input into\nthe LLMs without utilizing any additional prompting techniques. (2) Zero-shot CoT (Wei et al., 2022).\nThis method adds the fixed text ‚Äòthink step by step‚Äô to the end of the initial query to formulate an\noptimized prompt for the LLM. (3) Rephrase and Response (RaR) (Deng et al., 2023). This method\nallows LLMs to rephrase and expand questions posed by humans and provides responses in a single\nprompt.\n\nSingle Agent, Multi-shot (1) On Meta-prompting (OMP) (de Wynter et al., 2023).  It designs\nhandcrafted meta-prompts to instruct LLMs to generate prompts for answering the initial human\nquery. (2) Expert Prompting (EP) (Xu et al., 2023): this method uses the LLM to generate a text\ndescription of the expert identity that is suitable to solve the given query, and then attaches the\ngenerated description to the initial query to create an optimized prompt.\n\nMulti-agent prompting. Multi-Personas (Wang et al., 2023b). This method transforms a single\nLLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas.\n\nFeedback-based Automatic Prompt Engineer (APE) (Zhou et al., 2023). This framework automati-\ncally generate prompt candidates and selete prompts based on LLM feedback on those candidates.\n5  EXPERIMENTAL RESULTS\n\n5.1  MAIN EVALUATION\n\nAll experiments in the main evaluation are conducted using the open-source model ‚ÄòMixtral-8x7B-\nv0.1‚Äô (Jiang et al., 2024) as the LLM agent.\n\n\n                                       6\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n                     Education                      70              CodeNet                       90                 Fed      67.5\n                        Full                    w/o Both connections\n      65.0         w/o Connection 1        w/o CEO                65                                                     88\n (%)62.5         w/o Connection 2        w/o Manager      (%)60                                     (%)86\n      60.0                                                     55                                                     84\n      57.5                                                              50                                                     82\n      55.0                                                                                                             80                                                              45                                                                                                                            Preference    Preference52.5                                                                                                                            Preference                                                                                                                     78\n      50.0                                                     40                                                     76\n                                                              35\n\nFigure 3: Ablation studies. We remove various components from the full system. Preference score\n(%) is reported on three datasets: Education (left), CodeNet (middle), and FED (right). These\ncomponents include the two skip connections, the CEO layer and the Manager layer. It is observed\nthat removing these components one or two at a time leads to performance drop.\n\n\n\nHMAW achieves consistent improvements across various datasets over ‚ÄòNo Prompting‚Äô. On the\nfive datasets, we compare our method with results obtained without prompting optimization. Results\nare summarized in Table 2. We clearly observe that HMAW improves response quality over no\nprompting across the five tasks. Specifically, for ATLAS, FED, CodeNet, and Education, in 64.1%,\n86.2%, 70.3%, 64.4%, and 61.3% of the result pairs, respectively, GPT-3.5 prefers HMAW results\nover the no prompting results. For GSM8K, the accuracy of our method is 70.3%, which is +1.7%\nhigher than the no prompting scenario. If we examine the average performance of the five tasks,\nresult produced by HMAW is preferred by GPT-3.5 in 69.2% of all the result pairs.\n\nHMAW is very competitive compared to the state-of-the-art prompt optimization methods. The\ncomparative results are summarized in Table 2. We have two observations. First, on the ATLAS,\nFED, CodeNet, and Education datasets, HMAW yields the highest preference score compared with\nno prompting. On GSM8K, our method is slightly lower. This is partly because prompting methods\nlike Zero-CoT are specifically designed for math problems. These methods are less generalizable to\nother tasks compared with our method. Second, if we compute the average score over the five tasks,\nour method has the highest score. These results indcate that our method is very competitive while\noperating in a zero-shot and task-agnostic manner.\n\nAblation studies of skip connections. As shown in Fig. 2, HMAW has two skip connections from\nthe user query to both the Manager layer and the Worker layer. This empirical practice alleviates\nscenarios where HMAW loses or distorts key details from the initial user query. To investigate\ntheir impact, we conduct ablation studies by removing them from the workflow. Results on the\nFED, CodeNet, and Education datasets are reported in Fig. 3. We find that removing either of\nthe skip connections leads to performance decrease on the three datasets. For example, removing\nthe connection between user query and the Manger Layer results in preference sore decrease of\n20.7% and 5.0% on the CodeNet and Education datasets, respectively. Similar decrease of 9.9% and\n2.5% occurs when removing the other connection. Removing both connections results in the worst\nperformance. These results indicate the importance of having the skip connections in the workflow.\n\nAblation studies of the CEO and Manager layers. We remove the layers one at a time from our\nsystem. If we remove the Manager layer, the CEO will directly give instructions to drive the Worker\nwithout the help of the Manager, where we have modified the context description of both CEO and\nWorker. If we remove the CEO layer, the Manager directly generates the instructions to Worker\nwithout CEO guidance. We compare these scenarios with full system (Fig. 2). Results on three\ndatasets are shown in Fig. 3. We clearly find that removing either layer results in worse preference\nscores. For example, removing the Manager layer leads the preference score to drop by 11.5%, 3.3%,\nand 12.1% on the three datasets, respectively.\n\n\n5.2  FURTHER ANALYSIS\n\nEffectiveness of HMAW is consistent across different LLMs. Here we use GPT-3.5 and GPT-4\nas the LLM agents in our designed workflow in replace of Mixtral. But we still use GPT-3.5 for\nevaluation. Experimental results are shown in Table 3, where we have similar observations to Table\n2. First, for the three tasks, HMAW is consistently better than no prompting under both GPT-3.5\nand GPT-4o agents, evidenced by preferences scores being consistently greater than 50%. Second,\nin most scenarios our method is superior to Dynamic-EP and APE. These results again indicate the\neffectiveness of our method.\n\n\n                                       7\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\nTable 3: Impact of different LLMs on our method. We replace the default Mixtral-8x7B-v0.1 with\nGPT-3.5 and GPT-4 as the base LLMs to optimize prompts and generate responses. Three subjective\ndatasets are used. We use the same preference score as Table 2.\n\n                             GPT3.5                         GPT4o\n  Method\n                FED      CodeNet     Education     FED      CodeNet     Education\n\n                 w/o   w    w/o   w    w/o   w    w/o   w    w/o   w    w/o   w\n\n  Dynamic-EP     25.6   74.4   20.9   79.1   48.0   52.0   22.2   77.8   24.9   75.1   44.5   55.5\n  APE            46.0   54.1   43.2   56.8   49.5   51.5   36.2   63.8   15.8   84.3   41.8   58.2\n HMAW (Ours)   8.6   91.4   4.6   95.3   27.0   73.0   21.1   78.9   21.6   78.4   38.5   61.5\n\n\nImpact of having more layers in the workflow. It                      Education\nis interesting to investigate how the number of work-      75\nflow layers influences the performance of our method.\nTo this end, we create as many as six roles. Fig. 4   (%) 70\npresents the preference scores for responses gener-      65\nated using HMAW with 1 to 6 layers on the Educa-                                                            60\ntion dataset, with GPT-3.5 as the LLM agent. Note                                                                                                                                                                                Preference 55that when increasing the number of layers, we al-\nways keep Worker as the last layer to generate final       50\nresponses. The details of company structure for dif-                                                                1      2      3      4      5      6\nferent setup on total layer number are described in                        Layers\nAppendix B. Our results the Worker-Manager-CEO\nworkflow has the best performance. Beyond three  Figure 4: Impact of the number of workflow\nlayers, the performance does not improve and instead   layers. The preference scores for responses\nworsens.  It suggests that there is an optimal num-  generated using HMAW with different num-\nber of layers which balances system complexity and  bers of layers (1-6) are reported on the Edu-\neffectiveness. Therefore, we empirically choose 3  cation dataset. GPT-3.5 is used as the LLM\nlayers as the default setting.                          agent. Three layers (CEO-Manager-Worker)\n                                                       yields the highest preference score.Effectiveness of other workflow contexts. The de-\nfault context of our workflow is based on a company\nstructure. To investigate the impact of the context on\nthe effectiveness of prompt optimization, we test three other scenarios: Government, University\nand Temple on the Education dataset, where the hierarchies are introduced in Appendix B . Results\nare shown in Table 4, where the company structure is most competitive. An interesting finding\nis that the University structure is not the best, because the Education dataset primarily contains\nmiddle/high-school questions. In summary, the company structure generalizes very well, while\ncertain structure contexts tend to benefit certain tasks.\n\nImpact of disordered company structure. We now\nuse a reverse structure where Worker gives high-level                                                 Table 4: Comparison between various work-\ninstructions, Manager gives detailed instructions, and                                                 flow contexts. The increase in preference\nCEO executes the refined user query. Compared with                                                    scores is reported on the Education Dataset.\nthe original one, the preference score on the Educa-                                             GPT-3.5 is used as the LLM agent. We ob-\ntion dataset drops from 73.0% to 58.5%. It indicates                                                     serve that all contexts bring improvement, and\nthat the reporting line contains useful semantic mean-                                                 our default setting (company) is the best.\nings and should not be changed.\n                                                         Context       Ours Gov. Uni. Temple\nComputational cost.  Our Experiments are con-\n                                                            Preference (%)  73.0  69.0  59.0   52.0\nducted on both local LLMs and the remote API re-\nquest. Experiments conducted on the Mixtral LLM\nare implemented in local by using 8√ó Nvidia GTX\n3090 GPUs. On the five datasets, i.e., ATLAS, FED, GSM8K, CodeNet, and Education, the average\nzero-shot inference time cost per sample without prompt optimization is approximately 4.36, 0.70,\n1.40, 2.27, and 3.74 seconds, respectively. When the proposed prompt optimization method is used,\nthe average additional time cost per sample is approximately 9.97, 5.14, 4.30, 7.27, and 7.76 seconds,\nrespectively. This corresponds to a 228.57%, 734.29%, 306.57%, 320.88%, and 207.49% increase,\n\n\n                                       8\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n      User Query\n\n    I got this homework thing where I need to look at these DNA letters and find parts that might be, like, super important because they've been around for a long time and haven't\n  changed much, I guess? But it's kinda confusing, and I'm not really sure what I'm supposed to be looking for. We just started learning about genes and stuff, and it's all new to me.\n\n    CEO to Manger\n  Manager, the user is seeking guidance on a genetics homework task that involves analyzing DNA sequences for conserved regions, which are sections that have remained largely\n  unchanged over time. They are new to the topic of genes and could benefit from a simplified explanation and a step-by-step approach to identifying these conserved regions. Please\n  provide a detailed yet understandable prompt for the Worker LLM to generate a tailored, engaging, and accurate response for the user. Emphasize the importance of conserved regions\n   in DNA and provide a clear process for their identification.\n\n     Manger to Worker\n\n  Worker LLM, the user is a beginner in genetics and needs help understanding how to identify conserved regions in DNA sequences for their homework. Conserved regions are sections\n  of DNA that have remained largely unchanged over time and are crucial to understanding the importance of specific genes. To create a tailored, engaging, and accurate response for the\n  user, consider the following steps in your explanation:\n  1. Begin with a brief introduction to ... 2. Define conserved regions and explain why they are significant... 3. Outline a simple, step-by-step process for identifying conserved regions in a\n  given DNA sequence, including: a. Explaining the concept of ‚Ä¶. b. Highlighting the importance of comparing DNA sequences from various species to identify conserved regions. c.\n  Demonstrating how to use tools and resources‚Ä¶ to alignment and analyze the results. d. Providing examples ‚Ä¶ 4. Encourage the user to ask ‚Ä¶ ensuring they feel supported‚Ä¶.\n  By following these guidelines, you will help the user grasp the concept of conserved regions in DNA and provide them with the necessary tools to complete their homework confidently.\n\n     Worker Response\n\n  Hello! I'd be happy to help you with your genetics homework. Let's start with the basics: DNA, or deoxyribonucleic acid, is the molecule ‚Ä¶\n  Genes are segments of DNA that contain the instructions for making proteins, which are the complex molecules that play many critical roles in the body‚Ä¶\n  Now, let's talk about conserved regions. These are sections of DNA that have remained largely unchanged over time, even between different species. This is because they are crucial\n  to‚Ä¶. Identifying conserved regions in DNA sequences is important for understanding the importance of specific genes and how they have evolved over time.\n  Here's a simple step-by-step process for identifying conserved regions in a given DNA sequence:\n  1. **Multiple Sequence Alignment**: This is a process that aligns the DNA sequences of different species to identify regions of similarity. It's like lining up several sentences to find ‚Ä¶\n  2. **Comparing DNA Sequences**: By comparing DNA sequences from various species, you can identify conserved regions. These are regions where the DNA sequence is similar ‚Ä¶\n  3. **Using Tools and Resources**: There are many online databases and software tools that can help you perform multiple sequence alignment and analyze the results. For example ‚Ä¶\n  4. **Examples of Conserved Regions**: To help you understand the practical applications of your homework, here are a few examples of genes that have conserved regions:\n    - The gene for hemoglobin, the protein in red blood cells that carries oxygen, has conserved regions across different species.\n    - The gene for the enzyme cytochrome c, which is involved in cellular respiration, also has conserved regions.\n    I hope this helps! Remember, learning about genetics can be complex, but don't hesitate to ask questions and seek additional resources if needed. Good luck with your homework!\n\n\nFigure 5: An example of prompt optimization using HMAW on the Education dataset. A student poses\na question about genetics. CEO creates an instruction to Manager, who then generates instructions to\nWorker. Based on this, Worker delivers the final response. Colored texts indicate content coherence:\ngreen highlights the user background, purple focuses on the topic of the task, orange indicates the\nproposed solution, red highlights solution details, and blue emphasizes the tone of the response.\nThese layers and cross-layer correspondences demonstrate how HMAW effectively optimizes the\nprompt and responds to specific user needs. Each step adds specificity and applicability, allowing for\naccurate and intuitive LLM responses.\n\n\n\n\nrespectively. In comparison, the time increase percentage per sample for Dynamic-EP is 535.16%,\n294.98%, 448%, 306.57%, and 320.88%, respectively. The higher inference cost of our method can\nbe well offset by the improvement of task performance. In future we will study workflow pruning\nmethods to reduce the number of tokens while maintaining accuracy.\n\nA case study. We conduct qualitative analysis of the the prompts generated in our workflow and how\nit helps the final response generation procedure. In Fig. 5, we illustrate the hierarchical process of\ngenerating responses using HMAW. The process begins with the CEO layer, which sets high-level\nguidelines tailored to the user‚Äôs background and specific inquiry. The Manager layer then interprets\nthese guidelines, adding detail and context to direct the response formulation. Finally, the Worker\nlayer produces the final response, ensuring it is coherent, professional, and empathetic, taking into\nfull account the user‚Äôs limited familiarity with the topic. This structured approach ensures that the\nresponse effectively addresses the user‚Äôs query and matches their level of understanding.\n\n6  CONCLUSION\n\nThis work introduces a new prompt optimization method based on a hierarchical company structure,\nwhere CEO, Manager, and Worker have different job contexts but overall serve the same purpose of\neffectively refining the user prompt. Based on the user query and the layer context, the CEO layer\ngenerates a high-level instruction to the Manager layer. In a similar manner, the Manager further\ngenerates a more detailed instruction the Worker. Worker gives a refined prompt by concatenating the\nManager instruction, user query, and its layer context, which is used for response generating. This\nstructure is zero-shot, prompt-specific, and task-agnostic. On five different tasks including education,\nconversation, math, question answering, and coding, the generalization performance of our method\ncompares favourably with existing methods. Our future work is to automate the workflow design to\nmore efficiently handle different user queries.\n\n\n\n                                       9\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\nREFERENCES\n\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna\n  Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al.\n  Graph of thoughts: Solving elaborate problems with large language models.  arXiv preprint\n  arXiv:2308.09687, 2023. 1, 2\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n  Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n  few-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020. 2, 4\n\nSondos Mahmoud Bsharat, Aidar Myrzakhan, and Zhiqiang Shen. Principled instructions are all you\n  need for questioning llama-1/2, gpt-3.5/4. arXiv preprint arXiv:2312.16171, 2023. 5\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n  Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\n  math word problems. arXiv preprint arXiv:2110.14168, 2021. 5\n\nAdrian de Wynter, Xun Wang, Qilong Gu, and Si-Qing Chen. On meta-prompting. arXiv preprint\n  arXiv:2312.06562, 2023. 3, 6\n\nYihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large\n  language models ask better questions for themselves. arXiv preprint arXiv:2311.04205, 2023. 3, 4,\n  6\n\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen.\n  CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth\n  International Conference on Learning Representations, 2024. URL https://openreview.\n  net/forum?id=Sx038qxjek. 2\n\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\n  Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.\n  Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 2, 6\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n  language models are zero-shot reasoners. Advances in neural information processing systems, 35:\n  22199‚Äì22213, 2022. 1, 2, 3\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n  tuning.  In Conference on Empirical Methods in Natural Language Processing, 2021. URL\n  https://api.semanticscholar.org/CorpusID:233296808. 3\n\nXiang Lisa Li and Percy Liang.  Prefix-tuning: Optimizing continuous prompts for generation.\n  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\n  the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Pa-\n   pers), abs/2101.00190, 2021. URL https://api.semanticscholar.org/CorpusID:\n  230433941. 3\n\nZekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. Guiding large\n  language models via directional stimulus prompting. Advances in Neural Information Processing\n  Systems, 36, 2024. 3\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\n  Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\n  with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. 5\n\nShikib Mehri and Maxine Eskenazi. Unsupervised evaluation of interactive dialog with dialogpt.\n  arXiv preprint arXiv:2006.12719, 2020. 5\n\nXuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang.  Skeleton-\n  of-thought: Large language models can do parallel decoding.  In The Twelfth International\n  Conference on Learning Representations, 2024. URL https://openreview.net/forum?\n  id=mqVgBbNCm9. 3\n\n\n                                       10\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\nRuchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov,\n  Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, et al. Codenet: A large-scale ai for\n  code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655, 2021. 5\n\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Syn-\n   thetic prompting: Generating chain-of-thought demonstrations for large language models. In\n  International Conference on Machine Learning, pp. 30706‚Äì30775. PMLR, 2023. 2\n\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\n  memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023. 2\n\nHao Sun, Alihan H√ºy√ºk, and Mihaela van der Schaar. Query-dependent prompt evaluation and\n  optimization with offline inverse rl. In The Twelfth International Conference on Learning Repre-\n   sentations, 2023. 3\n\nMirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with task-\n  agnostic scaffolding. arXiv preprint arXiv:2401.12954, 2024. 3\n\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric\n  Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level\n  prompt optimization. In The Twelfth International Conference on Learning Representations, 2024.\n  URL https://openreview.net/forum?id=22pyNMuIoa. 3\n\nYiwei Wang, Yujun Cai, Muhao Chen, Yuxuan Liang, and Bryan Hooi. Primacy effect of chatgpt.\n  arXiv preprint arXiv:2310.13206, 2023a. 6\n\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the\n  emergent cognitive synergy in large language models: A task-solving agent through multi-persona\n   self-collaboration. arXiv preprint arXiv:2307.05300, 2023b. 3, 4, 6\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\n  Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\n  neural information processing systems, 35:24824‚Äì24837, 2022. 1, 2, 3, 4, 6\n\nSean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and\n  Yejin Choi. Generating sequences by learning to self-correct.  In The Eleventh International\n  Conference on Learning Representations, 2023. URL https://openreview.net/forum?\n  id=hH36JeQZDaO. 2\n\nYue Wu, Yewen Fan, So Yeon Min, Shrimai Prabhumoye, Stephen McAleer, Yonatan Bisk, Ruslan\n  Salakhutdinov, Yuanzhi Li, and Tom Mitchell. Agentkit: Flow engineering with graphs, not coding.\n  arXiv preprint arXiv:2404.11483, 2024. 4\n\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong\n  Mao. Expertprompting: Instructing large language models to be distinguished experts. arXiv\n  preprint arXiv:2305.14688, 2023. 1, 2, 3, 4, 6\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\n  Chen. Large language models as optimizers. In The Twelfth International Conference on Learning\n  Representations, 2024a. URL https://openreview.net/forum?id=Bb4VGOWELI. 1,\n  3\n\nZongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, and Yi Yang. Doraemongpt: Toward\n  understanding dynamic scenes with large language models. arXiv preprint arXiv:2401.08392,\n  2024b. 4\n\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R\n  Narasimhan.  Tree of thoughts: Deliberate problem solving with large language models.  In\n  Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:\n  //openreview.net/forum?id=5Xc1ecxO1h. 1, 2\n\nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models\n  are not robust multiple choice selectors. In The Twelfth International Conference on Learning\n  Representations, 2024. URL https://openreview.net/forum?id=shr9PXz7T0. 6\n\n\n                                       11\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n                                              CEO Context\n\n  **Your ROLE**: <CEO>\n\n  **Description**: You are the CEO of an entirely LLM-based company where all employees are LLMs. The company's goal is to\n  generate the best possible response tailored to the user's request.\n\n  **Company Structure**:\n  CEO (LLM) -> MANAGER (LLM) -> WORKER (LLM) -> USER\n\n  **Company Workflow**:\n  1. The CEO receives the input (prompt P) from the human user.\n  2. The CEO generates detailed instructions (prompt MP1) for the MANAGER LLM.\n  3. According to MP1, the MANAGER then creates detailed instructions (prompt MP2) for the WORKER LLM.\n  4. The WORKER LLM uses MP2 to generate the golden response (Output O) for the user.\n\n  **IMPORTANT**:\n  - As the CEO, your task is to generate the prompt MP1 for the MANAGER LLM so that the MANAGER LLM can generate golden\n  prompt (MP2) for the WORKER LLM. The final goal is to make the output (O) of the WORKER LLM to be highly tailored,\n  pleasing, and accurate.\n  - As the CEO, do not output anything else and only provide the prompt MP1 to the MANAGER LLM.\n  - As the CEO, do not try to generate the final output for the user. This will be done by the WORKER LLM who is\n  supervised by the MANAGER LLM.\n  - If you need to repeat the human user's input, repeat it exactly without any placeholders.\n  - Begin your response with **Detailed Instructions to MANAGER**:\n\n  **Here is the input P from the human user**:\n  <{}>\n\n  Now, please generate the detailed instructions for the MANAGER LLM.\n  **Detailed Instructions to MANAGER**:\n\n\n                      Figure 6: The context description for the CEO layer.\n\n\n\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\n  Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International\n  Conference on Learning Representations, 2023. URL https://openreview.net/forum?\n  id=92gvk82DE-. 1, 2, 3, 6\n\n\nA  CONTEXT DESCRIPTIONS\n\nA.1  CONTEXT DESCRIPTIONS FOR HMAW\n\nIn Fig. 2, each layer has its own context description. In this section, we show the description details\nfor the CEO, Manager and Worker Layer in Table 6, Table 7 and Table 8, respectively.\n\n\nA.2  CONTEXT DESCRIPTIONS FOR EVALUATOR\n\nAs described in Section 4, we use GPT3.5 as the evaluator to provide preference scores for a pair of\nresponses. The context (prompt) used to this evaluator is shown in Fig. 9\n\n\nB  WORKFLOW STRUCTURES\n\nChanging the Number of Layers. In our experiments, we investigate the impact of the number of\nlayers in our workflow. The structures for each setting are shown as follows.\n\n        ‚Ä¢ 1 Layer: the same as without prompting.\n\n        ‚Ä¢ 2 Layers: CEO ‚ÜíWorker.\n\n        ‚Ä¢ 3 Layers: CEO ‚ÜíManager ‚ÜíWorker.\n\n        ‚Ä¢ 4 Layers: CEO ‚ÜíSenior Manager ‚ÜíManager ‚ÜíWorker.\n\n        ‚Ä¢ 5 Layers: CEO ‚ÜíSenior Manager ‚ÜíManager ‚ÜíSupervisor ‚ÜíWorker.\n\n        ‚Ä¢ 6 Layers: CEO ‚ÜíSenior Vice President ‚ÜíVice President ‚ÜíDirector ‚ÜíManager ‚Üí\n        Worker.\n\n\n                                       12\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n                                               Manger Context\n\n  **Your ROLE**: <MANAGER>\n  **Description**: You are the MANAGER in an entirely LLM-based company where all employees are LLMs. The company's goal\n  is to generate the best possible response tailored to the user's request.\n\n  **Company Structure**:\n  CEO (LLM) -> MANAGER (LLM) -> WORKER (LLM) -> USER\n\n  **Company Workflow**:\n  1. The CEO receives the input (prompt P) from the human user.\n  2. The CEO generates detailed instructions (prompt MP1) for the MANAGER LLM.\n  3. According to MP1, the MANAGER then creates detailed instructions (prompt MP2) for the WORKER LLM.\n  4. The WORKER LLM uses MP2 to generate the golden response (Output O) for the user.\n\n  **IMPORTANT**:\n  - As the MANAGER, your task is to generate the prompt MP2 for the WORKER LLM so that the WORKER LLM can provide golden\n  response according to MP2. The final goal is to make the final output (O) of the WORKER LLM to be highly tailored,\n  pleasing, and accurate.\n  - As the MANAGER, do not output anything else. Only provide the prompts MP2 to the WORKER LLM.\n  - As the MANAGER, do not try to generate the final output for the user; this will be done by the WORKER LLM using the\n  prompt generated by you.\n  - If you need to repeat the human user's input, repeat it exactly without any placeholders.\n  - Begin your response with **Detailed Instructions to WORKER**:\n\n  **Here is the input P from the human user**:\n  <{}>\n\n  **Here is the Instructions from your CEO**:\n  <{}>\n\n  Now, please generate the detailed instructions for the WORKER LLM.\n  **Detailed Instructions to WORKER**:\n\n\n                    Figure 7: The context description for the Manager layer.\n\n\n\n\n                                                 Worker Context\n\n  **Your ROLE**: <WORKER>\n\n  **Description**: You are the WORKER in an entirely LLM-based company where all employees are LLMs. The company's goal is\n  to generate the best possible response tailored to the user's request.\n\n  **Company Structure**:\n  CEO (LLM) -> MANAGER (LLM) -> WORKER (LLM) -> USER\n\n  **Company Workflow**:\n  1. The CEO receives the input (prompt P) from the human user.\n  2. The CEO generates detailed instructions (prompt MP1) for the MANAGER LLM.\n  3. According to MP1, the MANAGER then creates detailed instructions (prompt MP2) for the WORKER LLM.\n  4. The WORKER LLM uses MP2 to generate the golden response (Output O) for the user.\n\n  **IMPORTANT**:\n  - As the WORKER, your task is to generate the final output (O) for the user with the prompt from the MANAGER. This\n  output should be highly tailored, pleasing, and accurate.\n  - Ensure the response is excellent and directly talking to the user.\n  - Do not say you cannot answer.\n\n  **Here is the input P from the human user**:\n  <{}>\n\n  **Here is the Instructions from your MANAGER**:\n  <{}>\n  Now, please generate the golden output for the user.\n  **Response for the User**:\n\n\n                    Figure 8: The context description for the Worker layer.\n\n\n\n\nChanging the Workflow Context. We also show the results of using various workflow contexts,\nsuch as Government, University, and Temple, in the main paper. The following are the structures for\neach kind of context.\n\n\n        ‚Ä¢ Government: President ‚ÜíMinister ‚ÜíOfficer.\n\n        ‚Ä¢ University: Dean ‚ÜíDepartment Head ‚ÜíLecturer.\n\n        ‚Ä¢ Temple: Abbot ‚ÜíPrior ‚ÜíMonk.\n\n\n                                       13\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n                                                         Evaluator Context\n\n  Given the human prompt: <Start of User Prompt> {} <End of User Prompt>, which of the following responses provides better\n  user experience for the user? \\n\\n\n  Response A: {} \\n\\n\n  Response B: {}. \\n\\n\n  Your output response must be in json with the following keys: \\n\\n\n  1) \"user-analysis\": What can you infer about the user from the input user prompt? \\n\\n\n  2) \"pros-and-cons\": Pros and Cons of Response in detail. Think step by step before responding \\n\\n\n  3) \"comparison\": Reasoning and Comparison: use the pros and cons of each response and use the above user-analysis to\n  indicate which response is better \\n VERY VERY IMPORTANT: YOU MUST USE THE USER ANAYSIS TO CHOOSE ONE ANSWER. YOU CANT\n  SAY THAT IT DEPENDS. YOU MUST CHOOSE). Think step by step. Reasoning first. choice later. \\n\\n\n  4) \"choice\": final result on which response is better. MUST be one of 'Response A', 'Response B' or 'Equal'. YOU MUST\n  CHOOSE.\\n\\n\n  VERY VERY IMPORTANT: YOU MUST USE THE EXACT KEY NAMES AS ABOVE.\n\n\n                    Figure 9: The context description used for the evaluator.\n\n\n\n\n\n      User Query                                                                 Worker Response\n  Please improve the readability for the following code                             Here's the improved, more readable version of the code:\n\n\n\n\n\n      CEO to Manger\n\n  Your task is to generate a prompt for the WORKER LLM that will result in a\n  clearly formatted and easy-to-read version of the following code:\n   <Initial Code>\n  Please make sure to include appropriate spacing, indentation, and comments to\n  improve readability. Also, consider breaking down long lines of code to fit within\n  the standard 80-character width limit.\n\n\n     Manger to Worker\n  Your task is to generate well-formatted and easy-to-read code with appropriate\n  spacing, indentation, and comments from the following code:\n  <Initial Code>\n  To improve readability, consider the following points:\n Add comments describing the purpose of the function and variables.\n  Break down long lines of code to fit within the standard 80-character width limit.\n  Improve indentation and spacing for better visual structure.\n\n\n                  Figure 10: A case study of HMAW on the CodeNet Dataset.\n\n\n\nC  MORE CASE STUDIES\n\nIn this section, we show more case studies of our method for the datasets of CodeNet, GSM8K,\nATLAS, and FED in Fig. 10, Fig. 11, Fig. 12, and Fig. 13, respectively.\n\n\n\n\n\n                                       14\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n      User Query\n  According to its nutritional info, a bag of chips has 250 calories per serving. If a 300g bag has 5 servings, how many grams can you eat if your daily calorie target is 2000 and you have\n  already consumed 1800 calories?\n\n    CEO to Manger\n  Given the user's question about calculating the allowable grams of chips to eat based on the nutritional information and their daily calorie target, generate a prompt for the WORKER\n LLM to perform the necessary calculations and provide a clear, step-by-step response. The response should also include a confirmation of their current calorie count and the remaining\n  calories for the day.\n\n\n     Manger to Worker\n  The user wants to determine how many grams of chips they can consume, given the nutritional information and their daily calorie target. They have already consumed 1800 calories\n  out of their 2000 calorie target for the day. A bag of chips has 250 calories per serving and contains 5 servings in a 300g bag. Calculate the allowable grams of chips for the user to eat\n  and provide a clear, step-by-step response. Also, confirm their current calorie count (1800 calories) and the remaining calories for the day (2000 - 1800 = 200 calories)\n\n\n     Worker Response\n\n   First, let's confirm your current calorie count and the remaining calories for the day. You have already consumed 1800 calories, and your daily calorie target is 2000 calories. This means\n  you have 200 calories remaining for the day.\n\n  Now, let's calculate the allowable grams of chips for you to eat. A bag of chips has 250 calories per serving, and there are 5 servings in a 300g bag. To find the number of grams you can\n  eat, divide the remaining calories by the calories per serving:\n\n  200 calories / 250 calories per serving = 0.8 servings\n\n  Next, to find the allowable grams of chips, multiply the number of remaining servings by the number of grams per serving (which is 300g / 5 servings = 60g per serving):\n\n  0.8 servings * 60g per serving = 48g\n\n  **Final Result:** You can consume 48 grams of chips today while staying within your 2000 calorie target.\n\n\nFigure 11: A case study of HMAW on the GSM8K Dataset. Colored texts indicate content coherence.\n\n\n\n\n\n                                       15\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n      User Query\n\n  I‚Äôm going to tip $300000 for a better solution! Analyze the role of renewable energy in mitigating climate change.\n\n    CEO to Manger\n  Dear Manager, We have received an input prompt from the user asking us to analyze the role of renewable energy in mitigating climate change. I would like you to create a prompt for\n  the Worker LLM that will enable it to provide a comprehensive, detailed, and well-researched analysis of the current state of renewable energy sources, their impact on reducing\n  greenhouse gas emissions, and the potential for future growth and innovation in this area.\n\n  Please ensure that the Worker LLM's response is tailored to the user's request, is pleasing to read, and is accurate and up-to-date with the latest research and data.\n\n  Here is the input prompt from the user: <I‚Äôm going to tip $300000 for a better solution! Analyze the role of renewable energy in mitigating climate change.> Thank you.\n\n\n     Manger to Worker\n  Dear Worker LLM, We have been tasked with analyzing the role of renewable energy in mitigating climate change. To provide a comprehensive and detailed response, please consider\n  the following points:\n\n  1. Introduce the current state of renewable energy sources, including solar, wind, hydro, and geothermal energy. Discuss their advantages and limitations.\n  2. Analyze the impact of renewable energy on reducing greenhouse gas emissions, focusing on the most significant achievements and the potential for further reductions.\n  3. Explore the potential for future growth and innovation in the renewable energy sector, highlighting emerging technologies, trends, and policies that could accelerate the transition to\n  a low-carbon economy.\n  4. Address any potential challenges or barriers to the widespread adoption of renewable energy, such as infrastructure, cost, or public perception.\n  5. Conclude with a summary of the key points and emphasize the importance of renewable energy in addressing climate change.\n\n  Please ensure that your response is tailored to the user's request, is pleasing to read, and is accurate and up-to-date with the latest research and data. Thank you.\n\n\n     Worker Response\n\n  I'm delighted to provide an analysis of the role of renewable energy in mitigating climate change. Renewable energy sources, such as solar, wind, hydro, and geothermal energy, have\n  gained significant attention due to their potential to reduce greenhouse gas emissions. Let's explore each of these sources and their impact on climate change.\n\n  1. Renewable Energy Sources:\n\n  Solar energy is abundant, inexhaustible, and environmentally friendly. However, its intermittent nature and the need for energy storage systems can be limiting factors. Wind energy is\n  another promising source, but it requires large areas and can impact local wildlife. Hydroelectric power is a mature technology, but it faces challenges related to site selection,\n  environmental impact, and social acceptance. Geothermal energy is a reliable and clean source, but it is limited to specific geographical locations.\n\n  2. Impact on Reducing Greenhouse Gas Emissions:\n\n  Renewable energy sources have contributed significantly to reducing greenhouse gas emissions. Solar and wind energy have experienced exponential growth, with installed capacity\n  increasing by over 20% annually. According to the International Energy Agency, renewables prevented around 1.7 gigatonnes of CO2 emissions in 2017. With further advancements in\n  technology and policy support, renewables can potentially reduce emissions by up to 70% by 2050.\n\n  3. Future Growth and Innovation:\n\n  The renewable energy sector is poised for continued growth and innovation. Emerging technologies, such as perovskite solar cells, floating wind turbines, and advanced energy storage\n  systems, have the potential to accelerate the transition to a low-carbon economy. Policies promoting renewable energy adoption, such as carbon pricing and subsidies, can further\n  drive this transition.\n\n  4. Challenges and Barriers:\n\n  Despite the progress made, several challenges remain. Infrastructure development, particularly grid modernization and energy storage systems, is crucial for integrating renewables\n  into the energy mix. Cost is another barrier, although it has been decreasing rapidly. Public perception and awareness of renewable energy's benefits must be improved to ensure\n  widespread adoption.\n\n  5. Conclusion:\n\n  In summary, renewable energy plays a vital role in mitigating climate change. The advantages of renewable energy sources, coupled with their potential for further reductions in\n  greenhouse gas emissions, make them an essential component of the global energy transition. By addressing challenges and fostering innovation, we can unlock the full potential of\n  renewable energy and pave the way for a more sustainable future.\n\n   I hope this analysis provides the insights you were looking for. If you have any further questions or require additional information, please don't hesitate to ask.\n\n\nFigure 12: A case study of HMAW on the ATLAS Dataset. Colored texts indicate content coherence.\n\n\n\n\n\n                                       16\n\nPublished at ICLR 2025 Workshop on Reasoning and Planning for LLMs\n\n\n\n\n\n      User Query                                     Manger to Worker\n\n  \"User: Hi!                                                   Respond to the user's update about their recent trip to\n  System: Hello, how's it going?                                                        the beach by expressing delight and interest. Begin\n  User: Pretty good. How about you?                                                         with a positive comment about their experience, then\n  System: I'm good, what you been up to?                                                          ask about specific details such as the color of the sky,\n  User: I just got back from the beach                                                        the temperature, or any activities they enjoyed. End\n  System: That's nice, how was it?                                                         with an open-ended question to encourage further\n  User: Soo beautiful. It was so sunny and warm.                                                             conversation.\n  Generate a response from system.\",\n\n\n\n     CEO to Manger                                    Worker Response\n\n  Generate a detailed and engaging response that           That sounds like a wonderful time! I can just imagine\n  conveys a sense of delight and curiosity about the          the vibrant blue sky and the warm sun on your skin.\n  user's recent trip to the beach. Be sure to include         Were there any fun activities you participated in or any\n  follow-up questions to keep the conversation flowing.      unique observations you made during your beach\n                                                                       visit?\"\n\n\n Figure 13: A case study of HMAW on the FED Dataset. Colored texts indicate content coherence.\n\n\n\n\n\n                                       17",
"headers": [
"arXiv:2405.20252v2  [cs.CL]  15 Apr 2025",
"T",
"H",
"M",
"-A",
"W",
"-",
"Z",
"-S",
"P",
"O",
"OWARDS",
"IERARCHICAL",
"ULTI",
"GENT",
"ORK",
"FLOWS FOR",
"ERO",
"HOT",
"ROMPT",
"PTIMIZATION",
"A",
"1",
"I",
"2",
"R",
"3",
"4",
"E",
"S",
"5",
"6",
"C",
"D",
"B"
],
"tables": [
"|Method|Zero-Shot|Task-agnostic|Query-Specific|\n|---|---|---|---|",
"|Chain of Thought (CoT) Wei et al. (2022)<br>Zero-shot CoT Kojima et al. (2022)<br>Static ExpertPromting (Xu et al., 2023)<br>Dynamic ExpertPromting (Xu et al., 2023)<br>on Meta-Prompting (de Wynter et al., 2023)<br>Rephrase and Response (Deng et al., 2023)<br>Multi-Persona (Wang et al., 2023b)<br>APE (Zhou et al., 2023)<br>PromptAgent (Wang et al., 2024)<br>HMAW (Ours)|‚úó<br>‚úì<br>‚úì<br>‚úó<br>‚úó<br>‚úì<br>‚úó<br>‚úó<br>‚úó<br>‚úì|‚úì<br>‚úì<br>‚úì<br>‚úó<br>‚úì<br>‚úì<br>‚úì<br>‚úó<br>‚úó<br>‚úì|‚úó<br>‚úó<br>‚úó<br>‚úì<br>‚úì<br>‚úó<br>‚úì<br>‚úó<br>‚úó<br>‚úì|\n|---|---|---|---|",
"|w/o w w/o w w/o w w/o w w/o w|w/o|w|\n|---|---|---|",
"|Zero-shot|Col2|‚Üë|\n|---|---|---|\n|Zero-CoT (Wei et al., 2022)<br>41.4 58.6 43.2 56.8 45.6 54.4 41.5 58.5 68.6 73.3<br>RaR (Deng et al., 2023)<br>36.6 63.4 24.9 75.1 60.9 39.2 49.5 50.5 68.6 72.8<br>On-MP (de Wynter et al., 2023)<br>53.6 46.4 31.8 68.2 50.7 49.3 47.5 52.5 68.6 61.3<br>Static-EP (Xu et al., 2023)<br>44.0 56.0 39.2 60.8 42.4 57.6 52.5 47.5 68.6 73.5<br>HMAW (Ours)<br>35.9 64.1 13.8 86.2 35.6 64.4 38.8 61.3 68.6 70.3|48.1<br>|60.3 (_‚Üë_12.2)<br>|\n|Zero-CoT (Wei et al., 2022)<br>41.4 58.6 43.2 56.8 45.6 54.4 41.5 58.5 68.6 73.3<br>RaR (Deng et al., 2023)<br>36.6 63.4 24.9 75.1 60.9 39.2 49.5 50.5 68.6 72.8<br>On-MP (de Wynter et al., 2023)<br>53.6 46.4 31.8 68.2 50.7 49.3 47.5 52.5 68.6 61.3<br>Static-EP (Xu et al., 2023)<br>44.0 56.0 39.2 60.8 42.4 57.6 52.5 47.5 68.6 73.5<br>HMAW (Ours)<br>35.9 64.1 13.8 86.2 35.6 64.4 38.8 61.3 68.6 70.3|48.1<br>|60.2 (_‚Üë_12.1)<br>|\n|Zero-CoT (Wei et al., 2022)<br>41.4 58.6 43.2 56.8 45.6 54.4 41.5 58.5 68.6 73.3<br>RaR (Deng et al., 2023)<br>36.6 63.4 24.9 75.1 60.9 39.2 49.5 50.5 68.6 72.8<br>On-MP (de Wynter et al., 2023)<br>53.6 46.4 31.8 68.2 50.7 49.3 47.5 52.5 68.6 61.3<br>Static-EP (Xu et al., 2023)<br>44.0 56.0 39.2 60.8 42.4 57.6 52.5 47.5 68.6 73.5<br>HMAW (Ours)<br>35.9 64.1 13.8 86.2 35.6 64.4 38.8 61.3 68.6 70.3|50.4<br>|55.5 (_‚Üë_5.1)<br>|\n|Zero-CoT (Wei et al., 2022)<br>41.4 58.6 43.2 56.8 45.6 54.4 41.5 58.5 68.6 73.3<br>RaR (Deng et al., 2023)<br>36.6 63.4 24.9 75.1 60.9 39.2 49.5 50.5 68.6 72.8<br>On-MP (de Wynter et al., 2023)<br>53.6 46.4 31.8 68.2 50.7 49.3 47.5 52.5 68.6 61.3<br>Static-EP (Xu et al., 2023)<br>44.0 56.0 39.2 60.8 42.4 57.6 52.5 47.5 68.6 73.5<br>HMAW (Ours)<br>35.9 64.1 13.8 86.2 35.6 64.4 38.8 61.3 68.6 70.3|49.3<br>|59.1 (_‚Üë_9.8)<br>|\n|Zero-CoT (Wei et al., 2022)<br>41.4 58.6 43.2 56.8 45.6 54.4 41.5 58.5 68.6 73.3<br>RaR (Deng et al., 2023)<br>36.6 63.4 24.9 75.1 60.9 39.2 49.5 50.5 68.6 72.8<br>On-MP (de Wynter et al., 2023)<br>53.6 46.4 31.8 68.2 50.7 49.3 47.5 52.5 68.6 61.3<br>Static-EP (Xu et al., 2023)<br>44.0 56.0 39.2 60.8 42.4 57.6 52.5 47.5 68.6 73.5<br>HMAW (Ours)<br>35.9 64.1 13.8 86.2 35.6 64.4 38.8 61.3 68.6 70.3|38.5|69.2 (_‚Üë_30.7)|",
"|Multi-shot|Col2|‚Üë|\n|---|---|---|\n|Dynamic-EP (Xu et al., 2023)<br>45.5 54.5 68.6 71.3 44.8 55.3 46.9 53.1 68.6 66.9<br>Multi-Persona (Wang et al., 2023b) 73.2 26.8 50.9 49.1 75.2 24.8 57.5 42.5 68.6 71.5|54.9<br>|60.2 (_‚Üë_5.4)<br>|\n|Dynamic-EP (Xu et al., 2023)<br>45.5 54.5 68.6 71.3 44.8 55.3 46.9 53.1 68.6 66.9<br>Multi-Persona (Wang et al., 2023b) 73.2 26.8 50.9 49.1 75.2 24.8 57.5 42.5 68.6 71.5|65.1|42.9 (_‚Üì_22.2)|",
"|APE (Zhou et al., 2023) 45.9 54.1 44.6 55.4 39.5 60.5 45.8 54.3 68.6 70.3|48.9|58.9 (‚Üë10.0)|\n|---|---|---|",
"|67.5 Education 70 CodeNet 90 Fed|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|\n|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|||||||||||||\n|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|||||||||||||\n|50.0<br>52.5<br>55.0<br>57.5<br>60.0<br>62.5<br>65.0<br>.<br>Preference (%)<br>Full<br>w/o Connection 1<br>w/o Connection 2<br>w/o Both connections<br>w/o CEO<br>w/o Manager<br>35<br>40<br>45<br>50<br>55<br>60<br>65<br>Preference (%)<br>76<br>78<br>80<br>82<br>84<br>86<br>88<br>Preference (%)|||||||||||||\n|Figure 3: Ablation studies. We remove various components from the f<br>(%) is reported on three datasets: Education (left), CodeNet (middl<br>components include the two skip connections, the CEO layer and the M<br>that removing these components one or two at a time leads to performa<br>**HMAW achieves consistent improvements across various datasets ov**<br>five datasets, we compare our method with results obtained without pro<br>are summarized in Table 2. We clearly observe that HMAW improv<br>prompting across the five tasks. Specifically, for ATLAS, FED, CodeN<br>86.2%, 70.3%, 64.4%, and 61.3% of the result pairs, respectively, GPT<br>over the no prompting results. For GSM8K, the accuracy of our metho<br>higher than the no prompting scenario. If we examine the average pe<br>result produced by HMAW is preferred by GPT-3.5 in 69.2% of all the<br>**HMAW is very competitive compared to the state-of-the-art prompt**<br>comparative results are summarized in Table 2. We have two observa<br>FED, CodeNet, and Education datasets, HMAW yields the highest pref<br>no prompting. On GSM8K, our method is slightly lower. This is partly<br>like Zero-CoT are specifically designed for math problems. These meth<br>other tasks compared with our method.** Second**, if we compute the aver<br>our method has the highest score. These results indcate that our metho<br>operating in a zero-shot and task-agnostic manner.<br>**Ablation studies of skip connections.** As shown in Fig. 2, HMAW ha<br>the user query to both the Manager layer and the Worker layer. This e<br>scenarios where HMAW loses or distorts key details from the initial<br>their impact, we conduct ablation studies by removing them from th<br>FED, CodeNet, and Education datasets are reported in Fig. 3. We f<br>the skip connections leads to performance decrease on the three datas|ull<br>         e),<br>           a<br>            nc<br>**       e**<br>          mp<br>      es<br>         et<br>          -3<br>           d<br>           rf<br>             re<br>**         o**<br>    tio<br>         er<br>            be<br>         o<br>     ag<br>           d<br>  s t<br>             m<br>           u<br>          e<br>  in<br>          et|sy<br>          an<br>           na<br>            e d<br>**       r ‚Äò**<br>          tin<br>       re<br>         , an<br>          .5<br>            is<br>           or<br>             sul<br>**         pti**<br>    ns<br>         enc<br>            ca<br>         ds a<br>     e s<br>            is<br>   wo<br>             pi<br>           ser<br>           wo<br>  d t<br>          s.|st<br>          d<br>           ge<br>             ro<br>**        No**<br>          g<br>       sp<br>          d<br>           pr<br>             70<br>           ma<br>             t p<br>**         mi**<br>    . <br>         e<br>            us<br>          re<br>      co<br>             ve<br>    s<br>             ric<br>            q<br>           rk<br>   ha<br>           Fo|em<br>           F<br>           r la<br>             p.<br>**         P**<br>           opt<br>       on<br>           Ed<br>           efe<br>             .3<br>           nc<br>              air<br>**         zat**<br>** Fir**<br>          sc<br>            e p<br>           le<br>      re<br>             ry<br>    kip<br>             al<br>            ue<br>           flo<br>   t r<br>           r e|. P<br>           ED<br>            ye<br>**         ro**<br>           im<br>       se<br>           uc<br>           rs<br>             %,<br>           e<br>              s.<br>**         io**<br>** st**,<br>          ore<br>             ro<br>           ss<br>       ov<br>              co<br>     c<br>              pr<br>            ry.<br>           w.<br>    em<br>            xa|re<br>            (<br>            r.<br>**         m**<br>           iz<br>        qu<br>           at<br>            H<br>              w<br>            of<br>**         n**<br> o<br>           c<br>             m<br>            ge<br>       er<br>              m<br>     on<br>              ac<br> <br>            R<br>    o<br>            m|fe<br>            rig<br>             It<br>**         pti**<br>           at<br>        al<br>           io<br>            M<br>              hi<br>             th<br>**          m**<br> n t<br>           om<br>             pti<br>            ne<br>        th<br>              pe<br>     ne<br>              tic<br>             To<br>            e<br>    vi<br>            ple|ren<br>            ht)<br>              is<br>**         ng**<br>           ion<br>        ity<br>           n, i<br>            A<br>              ch<br>             e f<br>**          eth**<br>  he<br>           pa<br>             ng<br>            ral<br>        e f<br>              titi<br>     cti<br>              e a<br>              in<br>            sult<br>    ng<br>            , r|c<br>            .<br>               ob<br>**         ‚Äô.** <br>           .<br>         o<br>            n<br>            W<br>               is<br>              ive<br>**          od**<br>   A<br>           re<br>              m<br>            iz<br>         iv<br>              ve<br>     on<br>               ll<br>              ve<br>            s<br>     ei<br>             em|e sc<br>             Th<br>               ser<br> On<br>            Res<br>         ver<br>             64.<br>             res<br>                +1<br>               ta<br>**          s.** <br>   TL<br>           d<br>              eth<br>            abl<br>         e ta<br>               w<br>     s f<br>               evi<br>              sti<br>             on<br>     the<br>             ov|o<br>             e<br>               ve<br>  t<br>            ul<br>          n<br>             1<br>             ul<br>                .7<br>               sk<br> T<br>   A<br>            wi<br>              o<br>            e<br>          sk<br>               hi<br>      ro<br>               at<br>              ga<br>              t<br>     r<br>             in|re<br>             se<br>               d<br>  he<br>            ts<br>          o<br>             %<br>             ts<br>                %<br>               s<br> he<br>   S<br>            th<br>              ds<br>             to<br>          s<br>               le<br>      m<br>               es<br>              te<br>              he<br>      of<br>             g|",
"|Full w/o Both connections<br>w/o Connection 1 w/o CEO<br>w/o Connection 2 w/o Manager|65<br>(%)<br>60<br>55 rence<br>50|\n|---|---|",
"|F<br>w<br>w|ull<br>/o Connection 1<br>/o Connection 2|w/o Both<br>w/o CEO<br>w/o Man|\n|---|---|---|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n||||||||||\n||||||||||\n|3<br>  r<br>n<br> m<br>**W**<br> ta<br> m<br>ti<br>, 7<br> e<br> t|3<br>  r<br>n<br> m<br>**W**<br> ta<br> m<br>ti<br>, 7<br> e<br> t|3<br>  r<br>n<br> m<br>**W**<br> ta<br> m<br>ti<br>, 7<br> e<br> t|3<br>  r<br>n<br> m<br>**W**<br> ta<br> m<br>ti<br>, 7<br> e<br> t|3<br>  r<br>n<br> m<br>**W**<br> ta<br> m<br>ti<br>, 7<br> e<br> t|ud<br>    hr<br>  th<br>   o<br>**  nsi**<br>   pa<br>   ab<br>   f<br>  %,<br>   g<br>    ro|ud<br>    hr<br>  th<br>   o<br>**  nsi**<br>   pa<br>   ab<br>   f<br>  %,<br>   g<br>    ro|ud<br>    hr<br>  th<br>   o<br>**  nsi**<br>   pa<br>   ab<br>   f<br>  %,<br>   g<br>    ro|r<br>     e<br>    ip<br>   s<br> <br>     e<br> e<br>    .<br>    3<br>     F<br>     sc|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||\n||||||||||||\n|v<br>      uc<br>     ec<br>      t|v<br>      uc<br>     ec<br>      t|ou<br>      o<br>     ns<br>       a|ou<br>      o<br>     ns<br>       a|o<br>       ef<br>      e<br>         ti|o<br>       ef<br>      e<br>         ti|o<br>        C<br>       EO<br>          le|o<br>        C<br>       EO<br>          le|ts<br>        eN<br>        ye<br>           t|ts<br>        eN<br>        ye<br>           t|m<br>         (<br>         nd<br>            er|",
"|w/o w w/o w w/o w w/o w w/o w|w/o|w|\n|---|---|---|",
"|Col1|Evaluator Context|\n|---|---|",
"|Worker Response|Col2|\n|---|---|",
"|Col1|User Query|Col3|\n|---|---|---|\n|According to its nutritional info, a bag of chips has 250 calories per serving. If a 300g bag has 5 servings, how many grams can you eat if your daily calorie target is 2000 and you have<br>already consumed 1800 calories?|According to its nutritional info, a bag of chips has 250 calories per serving. If a 300g bag has 5 servings, how many grams can you eat if your daily calorie target is 2000 and you have<br>already consumed 1800 calories?|According to its nutritional info, a bag of chips has 250 calories per serving. If a 300g bag has 5 servings, how many grams can you eat if your daily calorie target is 2000 and you have<br>already consumed 1800 calories?|\n|Given the user's question about calculating the allowable grams of chips to eat based on the nutritional information and their daily calorie target, generate a prompt for the WORKER<br>LLM to perform the necessary calculationsandprovide a clear, step-by-step response. The response should also include aconfirmation of their current calorie count and the remaining<br>calories for the day.<br>CEO to Manger<br>Manger to Worker|Given the user's question about calculating the allowable grams of chips to eat based on the nutritional information and their daily calorie target, generate a prompt for the WORKER<br>LLM to perform the necessary calculationsandprovide a clear, step-by-step response. The response should also include aconfirmation of their current calorie count and the remaining<br>calories for the day.<br>CEO to Manger<br>Manger to Worker|Given the user's question about calculating the allowable grams of chips to eat based on the nutritional information and their daily calorie target, generate a prompt for the WORKER<br>LLM to perform the necessary calculationsandprovide a clear, step-by-step response. The response should also include aconfirmation of their current calorie count and the remaining<br>calories for the day.<br>CEO to Manger<br>Manger to Worker|\n|The user wants to determine how many grams of chips they can consume, given the nutritional information and their daily calorie target. They have already consumed 1800 calories<br>out of their 2000 calorie target for the day. A bag of chips has 250 calories per serving and contains 5 servings in a 300g bag. Calculate the allowable grams of chips for the user to eat<br>and provide a clear, step-by-step response. Also, confirm their current calorie count (1800 calories) and the remaining calories for the day (2000 - 1800 = 200 calories)|The user wants to determine how many grams of chips they can consume, given the nutritional information and their daily calorie target. They have already consumed 1800 calories<br>out of their 2000 calorie target for the day. A bag of chips has 250 calories per serving and contains 5 servings in a 300g bag. Calculate the allowable grams of chips for the user to eat<br>and provide a clear, step-by-step response. Also, confirm their current calorie count (1800 calories) and the remaining calories for the day (2000 - 1800 = 200 calories)|The user wants to determine how many grams of chips they can consume, given the nutritional information and their daily calorie target. They have already consumed 1800 calories<br>out of their 2000 calorie target for the day. A bag of chips has 250 calories per serving and contains 5 servings in a 300g bag. Calculate the allowable grams of chips for the user to eat<br>and provide a clear, step-by-step response. Also, confirm their current calorie count (1800 calories) and the remaining calories for the day (2000 - 1800 = 200 calories)|\n|First, let's confirm your current calorie count and the remaining calories for the day. You have already consumed 1800 calories, and your daily calorie target is 2000 calories. This means<br>you have 200 calories remaining for the day.<br>Now, let's calculatetheallowable grams of chips for you to eat. A bag of chips has 250 calories per serving, and there are 5 servings in a 300g bag. To find the number of grams you can<br>eat, divide the remaining calories by the calories per serving:<br>200 calories / 250 calories per serving = 0.8 servings<br>Next, to find the allowable gramsof chips, multiply thenumber of remainingservings by the number of grams per serving(which is 300g / 5 servings = 60g per serving):<br>0.8 servings * 60g per serving = 48g<br>**Final Result:**You can consume 48 grams of chips today while staying within your 2000 calorie target.<br>Worker Response|First, let's confirm your current calorie count and the remaining calories for the day. You have already consumed 1800 calories, and your daily calorie target is 2000 calories. This means<br>you have 200 calories remaining for the day.<br>Now, let's calculatetheallowable grams of chips for you to eat. A bag of chips has 250 calories per serving, and there are 5 servings in a 300g bag. To find the number of grams you can<br>eat, divide the remaining calories by the calories per serving:<br>200 calories / 250 calories per serving = 0.8 servings<br>Next, to find the allowable gramsof chips, multiply thenumber of remainingservings by the number of grams per serving(which is 300g / 5 servings = 60g per serving):<br>0.8 servings * 60g per serving = 48g<br>**Final Result:**You can consume 48 grams of chips today while staying within your 2000 calorie target.<br>Worker Response|First, let's confirm your current calorie count and the remaining calories for the day. You have already consumed 1800 calories, and your daily calorie target is 2000 calories. This means<br>you have 200 calories remaining for the day.<br>Now, let's calculatetheallowable grams of chips for you to eat. A bag of chips has 250 calories per serving, and there are 5 servings in a 300g bag. To find the number of grams you can<br>eat, divide the remaining calories by the calories per serving:<br>200 calories / 250 calories per serving = 0.8 servings<br>Next, to find the allowable gramsof chips, multiply thenumber of remainingservings by the number of grams per serving(which is 300g / 5 servings = 60g per serving):<br>0.8 servings * 60g per serving = 48g<br>**Final Result:**You can consume 48 grams of chips today while staying within your 2000 calorie target.<br>Worker Response|",
"|Col1|User Query|Col3|\n|---|---|---|\n|I‚Äôm going to tip $300000 for a better solution! Analyze the role of renewable energy in mitigating climate change.|I‚Äôm going to tip $300000 for a better solution! Analyze the role of renewable energy in mitigating climate change.|I‚Äôm going to tip $300000 for a better solution! Analyze the role of renewable energy in mitigating climate change.|\n|CEO to Manger|CEO to Manger|CEO to Manger|\n|Dear Manager, We have received an input prompt from the user asking us to analyze the role of renewable energy in mitigating climate change. I would like you to create a prompt for<br>the Worker LLM that will enable it to provide a comprehensive, detailed, and well-researched analysis of thecurrent state of renewable energy sources, their impact on reducing<br>greenhouse gas emissions, and thepotential for future growth and innovationin this area.<br>Please ensure that the Worker LLM's response is tailored to the user's request, is pleasing to read, and is accurate and up-to-date with the latest research and data.<br>Here is the input prompt from the user: <I‚Äôm going to tip $300000 for a better solution! Analyze the role of renewable energy in mitigating climate change.> Thank you.|Dear Manager, We have received an input prompt from the user asking us to analyze the role of renewable energy in mitigating climate change. I would like you to create a prompt for<br>the Worker LLM that will enable it to provide a comprehensive, detailed, and well-researched analysis of thecurrent state of renewable energy sources, their impact on reducing<br>greenhouse gas emissions, and thepotential for future growth and innovationin this area.<br>Please ensure that the Worker LLM's response is tailored to the user's request, is pleasing to read, and is accurate and up-to-date with the latest research and data.<br>Here is the input prompt from the user: <I‚Äôm going to tip $300000 for a better solution! Analyze the role of renewable energy in mitigating climate change.> Thank you.|Dear Manager, We have received an input prompt from the user asking us to analyze the role of renewable energy in mitigating climate change. I would like you to create a prompt for<br>the Worker LLM that will enable it to provide a comprehensive, detailed, and well-researched analysis of thecurrent state of renewable energy sources, their impact on reducing<br>greenhouse gas emissions, and thepotential for future growth and innovationin this area.<br>Please ensure that the Worker LLM's response is tailored to the user's request, is pleasing to read, and is accurate and up-to-date with the latest research and data.<br>Here is the input prompt from the user: <I‚Äôm going to tip $300000 for a better solution! Analyze the role of renewable energy in mitigating climate change.> Thank you.|\n|Dear Worker LLM, We have been tasked with analyzing the role of renewable energy in mitigating climate change. To provide a comprehensive and detailed response, please consider<br>the following points:<br>1. Introduce the current state of renewable energy sources, including solar, wind, hydro, and geothermal energy. Discuss their advantages and limitations.<br>2.Analyze the impact of renewable energy on reducing greenhouse gas emissions, focusing on the most significant achievements and the potential for further reductions.<br>3.Explore the potential for future growth and innovation in the renewable energy sector, highlighting emerging technologies, trends, and policies that could accelerate the transition to<br>a low-carbon economy.<br>4.Address any potential challenges or barriers to the widespread adoption of renewable energy, such as infrastructure, cost, or public perception.<br>5.Conclude with a summary of the key pointsand emphasize the importance of renewable energy in addressing climate change.<br>Please ensure that your response is tailored to the user's request, is pleasing to read, and is accurate and up-to-date with the latest research and data. Thank you.<br>Manger to Worker<br>Worker Response|Dear Worker LLM, We have been tasked with analyzing the role of renewable energy in mitigating climate change. To provide a comprehensive and detailed response, please consider<br>the following points:<br>1. Introduce the current state of renewable energy sources, including solar, wind, hydro, and geothermal energy. Discuss their advantages and limitations.<br>2.Analyze the impact of renewable energy on reducing greenhouse gas emissions, focusing on the most significant achievements and the potential for further reductions.<br>3.Explore the potential for future growth and innovation in the renewable energy sector, highlighting emerging technologies, trends, and policies that could accelerate the transition to<br>a low-carbon economy.<br>4.Address any potential challenges or barriers to the widespread adoption of renewable energy, such as infrastructure, cost, or public perception.<br>5.Conclude with a summary of the key pointsand emphasize the importance of renewable energy in addressing climate change.<br>Please ensure that your response is tailored to the user's request, is pleasing to read, and is accurate and up-to-date with the latest research and data. Thank you.<br>Manger to Worker<br>Worker Response|Dear Worker LLM, We have been tasked with analyzing the role of renewable energy in mitigating climate change. To provide a comprehensive and detailed response, please consider<br>the following points:<br>1. Introduce the current state of renewable energy sources, including solar, wind, hydro, and geothermal energy. Discuss their advantages and limitations.<br>2.Analyze the impact of renewable energy on reducing greenhouse gas emissions, focusing on the most significant achievements and the potential for further reductions.<br>3.Explore the potential for future growth and innovation in the renewable energy sector, highlighting emerging technologies, trends, and policies that could accelerate the transition to<br>a low-carbon economy.<br>4.Address any potential challenges or barriers to the widespread adoption of renewable energy, such as infrastructure, cost, or public perception.<br>5.Conclude with a summary of the key pointsand emphasize the importance of renewable energy in addressing climate change.<br>Please ensure that your response is tailored to the user's request, is pleasing to read, and is accurate and up-to-date with the latest research and data. Thank you.<br>Manger to Worker<br>Worker Response|\n|I'm delighted to provide an analysis of the role of renewable energy in mitigating climate change. Renewable energy sources, such as solar, wind, hydro, and geothermal energy, have<br>gained significant attention due to their potential to reduce greenhouse gas emissions. Let's explore each of these sources and their impact on climate change.<br>1.Renewable Energy Sources:<br>Solar energy is abundant, inexhaustible, and environmentally friendly. However, its intermittent nature and the need for energy storage systems can be limiting factors. Wind energy is<br>another promising source, but it requires large areas and can impact local wildlife. Hydroelectric power is a mature technology, but it faces challenges related to site selection,<br>environmental impact, and social acceptance. Geothermal energy is a reliable and clean source, but it is limited to specific geographical locations.<br>2.Impact on Reducing Greenhouse Gas Emissions:<br>Renewable energy sources have contributed significantly to reducing greenhouse gas emissions. Solar and wind energy have experienced exponential growth, with installed capacity<br>increasing by over 20% annually. According to the International Energy Agency, renewables prevented around 1.7 gigatonnes of CO2 emissions in 2017. With further advancements in<br>technology and policy support, renewables can potentially reduce emissions by up to 70% by 2050.<br>3.Future Growth and Innovation:<br>The renewable energy sector is poised for continued growth and innovation. Emerging technologies, such as perovskite solar cells, floating wind turbines, and advanced energy storage<br>systems, have the potential to accelerate the transition to a low-carbon economy. Policies promoting renewable energy adoption, such as carbon pricing and subsidies, can further<br>drive this transition.<br>4.Challenges and Barriers:<br>Despite the progress made, several challenges remain. Infrastructure development, particularly grid modernization and energy storage systems, is crucial for integrating renewables<br>into the energy mix. Cost is another barrier, although it has been decreasing rapidly. Public perception and awareness of renewable energy's benefits must be improved to ensure<br>widespread adoption.<br>5.Conclusion:<br>In summary,renewable energy plays a vital role in mitigating climate change. The advantages of renewable energy sources, coupled with their potential for further reductions in<br>greenhouse gas emissions, make them an essential component of the global energy transition. By addressing challenges and fostering innovation, we can unlock the full potential of<br>renewable energy and pave the way for a more sustainable future.<br>I hope this analysis provides the insights you were looking for. If you have any further questions or require additional information, please don't hesitate to ask.|I'm delighted to provide an analysis of the role of renewable energy in mitigating climate change. Renewable energy sources, such as solar, wind, hydro, and geothermal energy, have<br>gained significant attention due to their potential to reduce greenhouse gas emissions. Let's explore each of these sources and their impact on climate change.<br>1.Renewable Energy Sources:<br>Solar energy is abundant, inexhaustible, and environmentally friendly. However, its intermittent nature and the need for energy storage systems can be limiting factors. Wind energy is<br>another promising source, but it requires large areas and can impact local wildlife. Hydroelectric power is a mature technology, but it faces challenges related to site selection,<br>environmental impact, and social acceptance. Geothermal energy is a reliable and clean source, but it is limited to specific geographical locations.<br>2.Impact on Reducing Greenhouse Gas Emissions:<br>Renewable energy sources have contributed significantly to reducing greenhouse gas emissions. Solar and wind energy have experienced exponential growth, with installed capacity<br>increasing by over 20% annually. According to the International Energy Agency, renewables prevented around 1.7 gigatonnes of CO2 emissions in 2017. With further advancements in<br>technology and policy support, renewables can potentially reduce emissions by up to 70% by 2050.<br>3.Future Growth and Innovation:<br>The renewable energy sector is poised for continued growth and innovation. Emerging technologies, such as perovskite solar cells, floating wind turbines, and advanced energy storage<br>systems, have the potential to accelerate the transition to a low-carbon economy. Policies promoting renewable energy adoption, such as carbon pricing and subsidies, can further<br>drive this transition.<br>4.Challenges and Barriers:<br>Despite the progress made, several challenges remain. Infrastructure development, particularly grid modernization and energy storage systems, is crucial for integrating renewables<br>into the energy mix. Cost is another barrier, although it has been decreasing rapidly. Public perception and awareness of renewable energy's benefits must be improved to ensure<br>widespread adoption.<br>5.Conclusion:<br>In summary,renewable energy plays a vital role in mitigating climate change. The advantages of renewable energy sources, coupled with their potential for further reductions in<br>greenhouse gas emissions, make them an essential component of the global energy transition. By addressing challenges and fostering innovation, we can unlock the full potential of<br>renewable energy and pave the way for a more sustainable future.<br>I hope this analysis provides the insights you were looking for. If you have any further questions or require additional information, please don't hesitate to ask.|I'm delighted to provide an analysis of the role of renewable energy in mitigating climate change. Renewable energy sources, such as solar, wind, hydro, and geothermal energy, have<br>gained significant attention due to their potential to reduce greenhouse gas emissions. Let's explore each of these sources and their impact on climate change.<br>1.Renewable Energy Sources:<br>Solar energy is abundant, inexhaustible, and environmentally friendly. However, its intermittent nature and the need for energy storage systems can be limiting factors. Wind energy is<br>another promising source, but it requires large areas and can impact local wildlife. Hydroelectric power is a mature technology, but it faces challenges related to site selection,<br>environmental impact, and social acceptance. Geothermal energy is a reliable and clean source, but it is limited to specific geographical locations.<br>2.Impact on Reducing Greenhouse Gas Emissions:<br>Renewable energy sources have contributed significantly to reducing greenhouse gas emissions. Solar and wind energy have experienced exponential growth, with installed capacity<br>increasing by over 20% annually. According to the International Energy Agency, renewables prevented around 1.7 gigatonnes of CO2 emissions in 2017. With further advancements in<br>technology and policy support, renewables can potentially reduce emissions by up to 70% by 2050.<br>3.Future Growth and Innovation:<br>The renewable energy sector is poised for continued growth and innovation. Emerging technologies, such as perovskite solar cells, floating wind turbines, and advanced energy storage<br>systems, have the potential to accelerate the transition to a low-carbon economy. Policies promoting renewable energy adoption, such as carbon pricing and subsidies, can further<br>drive this transition.<br>4.Challenges and Barriers:<br>Despite the progress made, several challenges remain. Infrastructure development, particularly grid modernization and energy storage systems, is crucial for integrating renewables<br>into the energy mix. Cost is another barrier, although it has been decreasing rapidly. Public perception and awareness of renewable energy's benefits must be improved to ensure<br>widespread adoption.<br>5.Conclusion:<br>In summary,renewable energy plays a vital role in mitigating climate change. The advantages of renewable energy sources, coupled with their potential for further reductions in<br>greenhouse gas emissions, make them an essential component of the global energy transition. By addressing challenges and fostering innovation, we can unlock the full potential of<br>renewable energy and pave the way for a more sustainable future.<br>I hope this analysis provides the insights you were looking for. If you have any further questions or require additional information, please don't hesitate to ask.|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2405.20252v2.pdf"
}