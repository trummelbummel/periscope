{
"text": "TAPO: Task-Referenced Adaptation for Prompt\n                      Optimization\n\n                   Wenxin Luo*, Weirui Wang*, Xiaopeng Li*, Weibo Zhou, Pengyue Jia, Xiangyu Zhao†\n                                                City University of Hong Kong\n                      {wenxinluo5-c, wrwang8-c, xiaopli2-c}@my.cityu.edu.hk, xianzhao@cityu.edu.hk\n\n\n          Abstract—Prompt engineering can significantly improve the   improve language proficiency, their effectiveness in other es-\n         performance of large language models (LLMs), with automated   sential language tasks, such as communication and reasoning,\n        prompt optimization (APO) gaining significant attention due to                                                                                           is diminished.\n          the time-consuming and laborious nature of manual prompt de-\n                                                             To address the limitations listed above, we propose Task-           sign. However, much of the existing work in APO overlooks task-\n           specific characteristics, resulting in prompts that lack domain   Referenced Adaptation for Prompt Optimization (TAPO), a2025   specificity and are not well-suited for task-specific optimization.   task-aware framework that dynamically selects task-related\n         In this paper, we introduce TAPO, a multitask-aware prompt   metrics and automates a task-adaptive prompt evaluation and\n         optimization framework composed of three key modules. First,                                                                        generation process to facilitate prompt evolution. The frame-\n        a task-aware metric selection module  is proposed to enhanceFeb                                                      work comprises three key components. The module Dynamic           task-specific prompt generation capabilities. Second, we present a\n26   multi-metricsmultiple perspectives.evaluationThird,moduleanto jointlyevolution-basedevaluate promptsoptimizationfrom  MetricaccordingSelectionto differentenablestasksthe LLMand assignsto chooseweightsrelevantbasedmetricson\n        framework is introduced for automatic prompt refinement, which   their priority, establishing task-adapted evaluation metrics for\n         improves adaptability across various tasks. Extensive experiments                                                                          the subsequent stage Task-Aware Prompt Evaluation. In\n        on six datasets demonstrate the effectiveness of our approach, and\n                                                                          the Evolution-Based Prompt Optimization module, we use        our code is publicly available1.\n           Index Terms—Prompt Engineering, Automated Prompt Opti-   a systematic selection mechanism to  iteratively select and\n          mization, Large Language Models, Multi-Task Learning         mutate high-performing prompts, continuously refining them[cs.CL]                                                                       for improved task-specific performance. In summary, the major\n                                        I. INTRODUCTION                        contributions of this paper are listed as follows.\n          Prompt engineering plays a critical role in improving the     • We propose Task-Referenced Adaptation for Prompt Op-\n        performance of large language models (LLMs) [1]. However,        timization (TAPO), an innovative approach that dynami-\n        manually constructing prompts is both time-consuming and        cally generates task-specific strategies to enhance multi-\n          labor-intensive. Thus, automated prompt optimization [2] has        task performance and promote generalization across di-\n        been introduced as a more systematic and efficient approach.        verse tasks.\n      Among current approaches, models such as TEMPERA [3]     • A novel task-aware metrics selection and a prompt evalu-\n         leverage reinforcement learning  to dynamically adapt and        ation module are developed to guide LLMs in generating\n         optimize prompts. Bayesian  optimization  techniques  offer         results that better align with task requirements.\n         a probabilistic framework for prompt refinement, while in-     • Extensive experiments conducted on six public datasets\n         context learning integrates examples directly into the prompts,        validate the significance of TAPO’s model components\n         as seen in models such as Voke-k [4] and Auto-CoT [5]. These       and its versatility across diverse tasks.\n        methods illustrate the growing trend toward more sophisticated\n        prompt engineering strategies.                                                                  II. METHODOLOGYarXiv:2501.06689v3\n          However, these methods face two primary limitations. First,                                                                           In  this  section, we  introduce  the TAPO framework, a\n         current prompt evaluation techniques predominantly rely on a                                                             dynamic self-improvement method that enhances task-specific\n          single metric, which hinders a comprehensive assessment. For                                                                  performance by optimizing prompts through the  selection\n        example, PromptBreeder [6] and APE [2] employ a single                                                               and weighting of evaluation metrics based on the unique\n          similarity metric for fitness measurement, limiting their ability                                                                               characteristics of each task.\n          to logically improve tasks related to planning. Second, the\n         lack of diverse metrics reduces their versatility, limiting their   A. Framework Overview\n          adaptability to multi-task optimization. For example, GATE [7]\n                                                           TAPO’s core innovation  lies in  its multi-objective  opti-         optimizes prompts for role-playing tasks, but exhibits limited\n                                                                         mization, which balances criteria like accuracy, fluency, and          scalability to a broader range of tasks. Similarly, while certain\n                                                                                   diversity. As shown in Figure 1, TAPO integrates LLMs into        prompt  strategies focused on machine  translation  [8] can\n                                                                key components, including task identification, metric selection,\n            *Equal contribution.   †Corresponding author.                     and prompt optimization. Dynamically adapts to various tasks\n             1https://github.com/Applied-Machine-Learning-Lab/TAPO             by  selecting appropriate evaluation metrics and  iteratively\n\nEvolution-Based Prompt Optimization\n                    Dynamic Metric Selection\n                                                                                                                                       Small\n                                                                                                                                                       Variations\n                                            Input                                                                      Candidate                          Prompt           Generated\n                                                                                                 +                        Mutation           Prompts        An Example    Given the explicitly stated premises, deductions are valid or invalid.                               Prompts\n          Task Types     Maths, Language Understanding, Logic Reasoning ...\n\n       Evaluation Metrics  Similarity, Diversity, Complexity ...\n                                                                                                                                           Evaluate        Select Candidates\n\n                                                                                 Task-Aware Prompt Evaluation\n                                                      Output\n                                                                                                                                       Perplexity\n                          I’m happy to assist! I have reviewed the example provided in the task dataset, which\n                                                                                                                                                           Evaluation\n                       belongs                                    to                                   the Logic                                       Reasoning                                                        task                                                                type, because...\n                                                                                                                                                  Similarity     Fusion       Metrics      Score       Scored                    Based on                                           this,                                                                        it is reasonable                                                            to                                                       consider                                                                            SimilaritySimilarity and ComplexityComplexity as the\n                         evaluation metrics.                                                                               Complexity                                       Prompts\n\n                                                                                                                                                                                                                                                          ...\n\n\n\nFig. 1. The framework of TAPO. For Dynamic Metric Selection, We provide a task dataset example for the LLM to select metrics and assign weights based\non priority, creating task-specific evaluation metrics for Task-Aware Prompt Evaluation. We employ a tournament selection algorithm for Evolution-Based\nPrompt Optimization to select and mutate the better-performing prompts, adding task-adapted prompts to the candidates.\n\n\nrefining prompts through an adaptive feedback loop, thus   TAPO integrates similarity, diversity, perplexity, and com-\nimproving task-specific performance.                            plexity as metrics to balance accuracy, creativity, and fluency.\n  The process begins with task classification, where the LLM   Adjusts the weight of each metric based on task requirements,\nidentifies the type of task. TAPO then selects relevant metrics,   prioritizing similarity for precision tasks and enhancing diver-\nsuch as similarity and complexity, to guide prompt design and   sity and perplexity for creative tasks.\nevaluation. High-performing prompts are iteratively refined                                                      D. Evolution-Based Prompt Optimization\nthrough mutation and selection, ensuring continuous improve-\n                                                                    Traditional prompt optimization methods often stagnate inment. This adaptive process makes TAPO flexible and effective\n                                                                    local optima, limiting  their  ability to explore better  alter-in diverse tasks.\n                                                                     natives. TAPO addresses this limitation by refining prompts\nB. Dynamic Metric Selection                                through evolutionary strategies, leveraging mutation, and se-\n  Different  tasks  require  different evaluation  criteria, and   lection for continuous improvement.\nfixed metrics often fail to capture nuanced demands such as     During initialization, TAPO generates prompts by integrat-\nprecision, creativity, or logical consistency. TAPO optimizes   ing random thinking  styles with the problem  description,\nprompts by dynamically selecting and weighting task-specific  which  are subsequently processed by  the LLM.  In  self-\nevaluation metrics. The process begins with task classification,   evolution processes, small variations, such as ”breaking the\nwhere the LLM-driven module identifies the task type (e.g.,   task into steps”, are selected from a predefined strategy library\nreasoning, language, real-world problem) and selects relevant   to combine with candidate prompts. These combinations are\nmetrics. For factual tasks, similarity ensures accuracy, while   then processed through mutation operators to generate evolved\ncreative tasks emphasize diversity to avoid repetition. Metrics   prompts. During each iteration, the performance evaluation is\nsuch  as complexity  assess  fluency, while both  perplexity   conducted using the multi-metric scoring function mentioned\nand logical consistency are crucial for advanced reasoning,   above. TAPO applies tournament selection to filter candidates,\ndialogue, and decision-support systems. This approach enables   ensuring improved task-specific  results. This  iterative pro-\nTAPO to adapt to various tasks, ensuring optimal performance   cess continues for multiple cycles, dynamically refining the\nacross multiple dimensions.                                prompts until they achieve the desired performance or reach\n                                                           a predefined iteration limit, ensuring continuous optimization.\nC. Task-Aware Prompt Evaluation\n                                                                                                     III. EXPERIMENT\n  To evaluate and adapt prompts for various tasks, we propose\n                                                                  In this section, we present experimental settings and outlinea prompt evaluation module with two components: metric\n                                                                 the design of our experiments to address the following researchfusion and dynamic weight adjustment. After selecting the\n                                                                 questions: RQ1: How does our model perform comparedevaluation metrics, TAPO combines them into a final scoring\n                                                                    to state-of-the-art approaches? RQ2: How effectively doesfunction to comprehensively assess the performance of the\n                                                           our model adapt to  different types of tasks? RQ3: Doestask. The scoring function is defined as:\n                                                           our framework maintain consistent performance across open-\n                        n\n              S(P) = X wi · Mi(P)                 (1)   source LLMs? RQ4: What is the impact of individual com-\n                                                           ponents on overall performance?\n                          i=1\nwhere P is the optimized prompt, wi is the weight of the i-th   A. Experiment Settings\nmetric, Mi(P) denotes the score for the i-th metric, and S(P)     Datasets. To evaluate our method, we use a range of\nrepresents the overall score for n metrics.                       datasets focused on mathematical reasoning and multi-task\n\nTABLE I\n  PERFORMANCE COMPARISON ON DIFFERENT DATASETS WITH GPT-3.5-TURBO AND GPT-4O (SIMILARITY SCORES).\n\n\n                                GPT-3.5-turbo                               GPT-4o\n   Dataset\n              COT    APE    PE2    PB    TAPO    COT    APE    PE2     PB     TAPO\n\n  BBH(23 tasks)    66.68    68.10    63.57    68.17    69.28*     74.18     74.83    77.05    79.90     80.51*\n  GSM8K          83.70*    81.99    78.63    82.45    83.40     85.49     79.79    83.37    88.61*     88.40\n  AddSub           58.61    57.04    68.10    82.11    88.15*    100.00*    97.92    95.78    97.62     96.32\n   MultiArith        69.00    86.36    83.56    85.26    89.26*    100.00    100.00    97.92   100.00     100.00\n  SingleEQ         61.91    63.14    78.92    82.11    89.06*     96.12     96.78    89.63    96.74     97.83*\n  SVAMP          94.38*    93.10    91.02    90.44    92.72     93.23     95.71    94.81    99.72    100.00*\n\n   “*” indicates significance level test p < 0.05, the suboptimal results are underlined.\n\n\nproblem solving, including AddSub [9], MultiArith [10], and  GPT-3.5-turbo, respectively, demonstrating a clear advan-\nSingleEQ [11] for arithmetic reasoning, as well as SVAMP   tage over static methods like CoT [15] and APE [2]. For multi-\n[12] and GSM8K [13] for multi-step problem solving. Fur-   step reasoning tasks such as GSM8K, TAPO reaches 88.40%\nthermore, we incorporate BIG-Bench Hard (BBH) [14], a  on GPT-4o, just shy of the best score at 88.61%. In BBH,\ndataset comprising 23 diverse and challenging tasks, including  TAPO achieves 80.51% in GPT-4o, slightly exceeding the\nlogical reasoning and common sense understanding, to ensure   next best method at 79.90%.\na comprehensive evaluation.                                  Although TAPO  does  not  always  achieve  the  highest\n  Baselines. We compare TAPO against the following base-   score, such  as  in SVAMP where  it  reaches 92.72% on\nline methods: (a) Zero-Shot CoT [15], which generates rea-  GPT-3.5-turbo compared to 94.38%, it consistently ranks\nsoning steps in a zero-shot manner; (b) APE [2], a method that  among the best methods, showing strong adaptability in var-\ninitializes multiple prompt candidates from a base prompt and   ious tasks. These results, summarized in Table I, underscore\nselects the best one based on development set performance;  TAPO’s effectiveness in optimizing task-specific prompts for\n(c) PE2 [16], a two-step prompting approach that iteratively   a wide range of language and reasoning tasks.\ngenerates and refines candidate prompts through evaluation;\n                                                          C. Task-Specific Prompt Performance (RQ2)\nand (d) PromptBreeder (PB) [6], a self-referential optimization\nframework that refines prompts by leveraging redescriptions to                        TABLE II\nimprove downstream task performance.                   EFFECT OF DIFFERENT PROMPT OPTIMIZATION METHODS.\n  Experiment Details. In our experiments, we utilize the\nfollowing language models: GPT-3.5-turbo-0125 [17],      Method     Math Reasoning Prompt\nGPT-4o-2024-08-06 [18], and Llama3-8B-Instruct     Zero-shot CoT    Let’s think step by step.\n[19]. The first two models are accessed through the OpenAI       APE        Solve arithmetic word problems.\nAPI, while Llama3  is deployed using the NVIDIA API.                       Subtract 2 from 8 to find how many kittens\n                                                                                  Joan has now.\nTo ensure consistency between tasks, the temperature is set       PB                                                                              Answer: 8 - 2 = 6\nto  0.1. The evaluation of generated  text employs metrics                     Advice: Correct subtraction. Well done!\nfor  similarity,  fluency,  diversity, and  complexity.  Similar-                    Break the problem into smaller parts. Identify\nity  is assessed using cosine  similarity, calculated via the      TAPO       key elements, use diagrams or rephrase, and\nall-MiniLM-L6-v2 [20] model, to measure the semantic                    remove unnecessary information.\nalignment between the generated and reference  texts. Flu-      Method      Translation Error Detection Prompt\nency is evaluated through perplexity scores derived from the     Zero-shot CoT    Let’s think step by step.\ngpt2-large [21] model, where lower values indicate more                        Identify translation errors based on specific cat-\ncoherent and grammatically accurate outputs. Diversity is used       APE         egories like Named Entities, Numerical Values,\n                                                                              and Modifiers.to quantify lexical variety by calculating the proportion of\n                                                                     Become a fearless error detective with a magni-\nunique n-grams, with higher scores reflecting reduced repeti-                                                     PB         fying glass, finding quirky translation missteps\ntion. Complexity is assessed by analyzing text length, syntactic                         in a whimsical German fairytale.\nstructures, and logical reasoning steps.                                          Improve translation error detection by creating\n                                                                                     a structured framework to categorize errors,\nB. Overall Performance (RQ1)                            TAPO                                                                                        focusing on common issues, and using iterative\n  TAPO consistently outperforms baseline methods by dy-                         testing with feedback to refine strategies.\nnamically selecting and weighting task-specific metrics. In\narithmetic reasoning tasks such as AddSub and MultiArith,    TAPO’s ability to tailor prompts for specific tasks signif-\nTAPO achieves similarity scores of 88.15% and 89.26% in   icantly enhances performance across different domains, as\n\nillustrated in Table II. For math reasoning tasks like AddSub,   a performance decline across  all datasets, highlighting the\nTAPO emphasizes reasoning and computational steps, offering   importance of refined prompt generation of TAPO. Similarly,\na customized approach  that outperforms  general methods   in the w/o MS variant, using a single-metric method instead\nlike zero-shot CoT, which uses the generic prompt \"Let’s   of the multi-metric approach led to a significant performance\nthink step by step\". For translation tasks, TAPO ex-   drop, particularly in datasets such as SigleEQ and MultiArith,\ncels by using a systematic framework for error categoriza-   highlighting the crucial  role of multi-metric evaluation  in\ntion and iterative feedback, in contrast to the less structured   improving task-specific results.\nmethods of APE or PromptBreeder. This task-specific design\n                                                                                 IV. RELATED WORK\nensures that TAPO consistently delivers superior results by\nadapting the prompts to the unique requirements of each task,    The optimization of prompts has become central to the\nimproving both clarity and performance in mathematical and   application of LLMs in areas such as agents [22], [23], RAG\ntranslation error detection tasks.                                   [24], [25], IR [26], [27], RecSys [28]–[35], etc., and  it has\n                                                                  recently gained significant attention due to the time-consuming\nD. Open-Source LLM Performance (RQ3)                                                       and labor-intensive process of manual prompt tuning. Sev-\n                                                                      eral approaches have been proposed, including leveraging\n                                                           feedback from LLMs and iterative improvements [2], rein-\n                                                           forcement learning-based methods [36], Bayesian optimization\n                                                              techniques [37], in-context learning [16], and evolutionary\n                                                              algorithms [6], among others. However, current prompt eval-\n                                                               uation methods rely on uniform metrics that lack adaptability\n                                                       and  fail  to optimize  for  different  task-specific  objectives.\n                                                               In  this paper, we propose a task-aware automatic prompt\n                                                               optimization framework that dynamically selects evaluation\n                                                               metrics based on task-specific feedback, thereby improving the\n         Fig. 2.  Performance Comparison with Llama3-8B-Instruct.\n                                                                    generalizability of prompt optimization across multiple tasks.\n\n  When evaluating open-source large language models, such                                                                               V. CONCLUSION\nas Llama3-8B-Instruct, TAPO consistently outperforms\n                                          We propose a novel framework, TAPO, to improve prompt\nbaseline methods, including CoT and PE2, even on tasks\n                                                               generation and enhance the adaptability of LLMs to diverse\nthat require precise formatting and multi-step reasoning. As\n                                                                      tasks. TAPO leverages a comprehensive set of evaluation met-\nillustrated in Figure 2, while Llama3-8B-Instruct strug-\n                                                                              rics, dynamically adjusting them based on task requirements,\ngles to maintain the correct output formats on datasets such\n                                                           while an adaptive feedback loop iteratively refines the prompts\nas MultiArith and AddSub, TAPO enables  the model  to\n                                                                    to ensure continuous improvement. Extensive experiments on\nachieve  significantly  better  results. In tasks  like GSM8K,\n                                                               various datasets demonstrate that TAPO consistently outper-\nwhich emphasize multi-step reasoning, TAPO further improves\n                                                         forms existing methods, offering superior performance and\nthe LLM’s performance, narrowing the gap with state-of-the-\n                                                                         versatility across different models and task types, ranging from\nart models. On average, TAPO improved similarity scores in\n                                                                 arithmetic reasoning to multistep problem solving, creative\nmath reasoning tasks by 10.2% compared to CoT and 6.2%\n                                                                 generation, and logical reasoning challenges.\ncompared to PE2. These results demonstrate that TAPO’s op-\ntimization enhances performance even in open-source LLMs,               ACKNOWLEDGMENT\nmaking it effective across different model architectures.                                                              This research was  partially supported by Research Im-\nE. Ablation Study (RQ4)                                       pact  Fund  (No.R1015-23),  Collaborative  Research  Fund\n                                                       (No.C1043-24GF), APRC  - CityU New  Research  Initia-\n                      TABLE III                                tives  (No.9610565,  Start-up  Grant  for New  Faculty  of\n         ABLATION STUDY OF TAPO WITH GPT-3.5-TURBO            CityU),  CityU  - HKIDS  Early  Career  Research  Grant\n                     (SIMILARITY SCORES).                      (No.9360163), Hong Kong ITC Innovation and Technology\n                                                   Fund Midstream Research Programme for Universities Project\n  Method      AddSub   SingleEQ  SVAMP   MultiArith  GSM8K\n                                                         (No.ITS/034/22MS), Hong Kong Environmental and Conser-\n  TAPO (Full)   88.15%    89.06%    92.72%    89.26%    83.40%     vation Fund (No. 88/2022), and SIRG - CityU Strategic Inter-  w/o PO        87.24%    82.75%    85.38%     83.04%     81.82%\n  w/o MS       82.60%    75.91%    80.14%     78.58%     82.41%     disciplinary Research Grant (No.7020046), Huawei (Huawei\n                                                             Innovation Research Program), Tencent (CCF-Tencent Open\n                                                         Fund, Tencent Rhino-Bird Focused Research Program), Ant\n  We conducted an ablation study to evaluate the key compo-\n                                                    Group  (CCF-Ant  Research  Fund,  Ant  Group  Research\nnents of TAPO, as shown in Table III, focusing on removing\n                                                            Fund), Alibaba (CCF-Alimama Tech Kangaroo Fund No.\nprompt optimization (PO) and multi-metric scoring (MS). In\n                                                          2024002), CCF-BaiChuan-Ebtech Foundation Model Fund,\nthe w/o PO variant, we replaced the optimization of task-\n                                                       and Kuaishou.\nspecific prompts with a generic approach, which resulted in\n\nREFERENCES                                 [24]  P. Jia, D. Xu, X. Li, Z. Du, X. Li, X. Zhao, Y. Wang, Y. Wang, H. Guo,\n                                                                         and R. Tang, “Bridging relevance and reasoning: Rationale distillation\n [1] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,          in retrieval-augmented generation,” arXiv preprint arXiv:2412.08519,\n       J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv        2024.\n      preprint arXiv:2303.18223, 2023.                                         [25]  P.  Jia, Y. Liu, X. Li, Y. Wang, Y. Du, X. Han, X. Wei, S. Wang,\n [2] Y. Zhou, A.  I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and        D. Yin, and X. Zhao, “G3: an effective and adaptive framework for\n       J. Ba, “Large language models are human-level prompt engineers,” arXiv        worldwide geolocalization using large multi-modality models,” arXiv\n      preprint arXiv:2211.01910, 2022.                                                preprint arXiv:2405.14702, 2024.\n [3]  T. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E. Gonzalez, “Tem-    [26] X. Li, X. Li, H. Zhang, Z. Du, P. Jia, Y. Wang, X. Zhao, H. Guo,\n      pera: Test-time prompting via reinforcement learning,” arXiv preprint        and R. Tang, “Syneg: Llm-driven synthetic hard-negatives for dense\n     arXiv:2211.11890, 2022.                                                                retrieval,” arXiv preprint arXiv:2412.17250, 2024.\n                                                                               [27]  P. Jia, Y. Liu, X. Zhao, X. Li, C. Hao, S. Wang, and D. Yin, “Mill: [4] H. Su,  J. Kasai, C. H. Wu, W. Shi, T. Wang,  J. Xin, R. Zhang,\n                                                                         Mutual verification with large language models for zero-shot query    M. Ostendorf, L. Zettlemoyer, N. A. Smith et al., “Selective annota-\n                                                                                   expansion,” arXiv preprint arXiv:2310.19056, 2023.      tion makes language models better few-shot learners,” arXiv preprint\n                                                                               [28] Q. Liu, X. Wu, X. Zhao, Y. Zhu, D. Xu, F. Tian, and Y. Zheng, “When     arXiv:2209.01975, 2022.\n                                                             moe meets llms: Parameter efficient fine-tuning for multi-task medical [5] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought\n                                                                                           applications,” in Proceedings of the 47th International ACM SIGIR     prompting in large language models,” arXiv preprint arXiv:2210.03493,\n                                                                             Conference on Research and Development in Information Retrieval,     2022.\n                                                                              2024, pp. 1104–1114.\n [6] C.  Fernando,  D.  Banarse,  H.  Michalewski,  S.  Osindero,  and\n                                                                               [29] Q. Liu, X. Wu, Y. Wang, Z. Zhang, F. Tian, Y. Zheng, and X. Zhao,\n      T. Rockt¨aschel, “Promptbreeder: Self-referential self-improvement via\n                                                                                  “Llm-esr: Large language models enhancement for long-tailed sequential\n     prompt evolution,” arXiv preprint arXiv:2309.16797, 2023.\n                                                                             recommendation,” in The Thirty-eighth Annual Conference on Neural\n [7] B. Z. Li, A. Tamkin, N. Goodman, and J. Andreas, “Eliciting human\n                                                                                 Information Processing Systems, 2024.\n     preferences with language models,” arXiv preprint arXiv:2310.11589,\n                                                                               [30] X. Li, F. Yan, X. Zhao, Y. Wang, B. Chen, H. Guo, and R. Tang, “Hamur:\n     2023.\n                                                                       Hyper adapter for multi-domain recommendation,” in Proceedings of\n [8]  S. E. S. Nooshin Pourkamali, “Machine translation with large language                                                                                       the 32nd ACM International Conference on Information and Knowledge\n     models: Prompt engineering for persian, english, and russian directions,”                                                                       Management, 2023, pp. 1268–1277.\n     arXiv preprint arXiv:2401.08429, 2024.                                                                               [31]  J. Gao, B. Chen, M. Zhu, X. Zhao, X.  Li, Y. Wang, Y. Wang,\n [9] M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman, “Learning to                                                                         H. Guo, and R. Tang, “Hierrec: Scenario-aware hierarchical modeling\n     solve arithmetic word problems with verb categorization,” in Proceed-                                                                                           for multi-scenario recommendations,” in Proceedings of the 33rd ACM\n     ings of the 2014 Conference on Empirical Methods in Natural Language                                                                                       International Conference on Information and Knowledge Management,\n     Processing (EMNLP), 2014, pp. 523–533.                                                                              2024, pp. 653–662.\n[10]  S. Roy and D. Roth, “Solving general arithmetic word problems,” arXiv    [32]  P.  Jia, Y. Wang, S. Lin, X.  Li, X. Zhao, H. Guo, and R. Tang,\n      preprint arXiv:1608.01413, 2016.                                        “D3: A methodological exploration of domain division, modeling, and\n[11] R. Koncel-Kedziorski, H. Hajishirzi, A. Sabharwal, O. Etzioni, and S. D.         balance in multi-domain recommendations,” in Proceedings of the AAAI\n     Ang, “Parsing algebraic word problems into equations,” Transactions        Conference on Artificial Intelligence, vol. 38, no. 8, 2024, pp. 8553–\n      of the Association for Computational Linguistics, vol. 3, pp. 585–597,        8561.\n     2015.                                                                    [33] X. Li, J. Gao, P. Jia, Y. Wang, W. Wang, Y. Wang, Y. Wang, H. Guo,\n[12] A. Patel, S. Bhattamishra, and N. Goyal, “Are nlp models really able to        and R. Tang, “Scenario-wise rec: A multi-scenario recommendation\n     solve simple math word problems?” arXiv preprint arXiv:2103.07191,        benchmark,” arXiv preprint arXiv:2412.17374, 2024.\n     2021.                                                                    [34] Q.  Liu, X. Zhao, Y. Wang, Y. Wang, Z. Zhang, Y. Sun, X.  Li,\n[13] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,       M. Wang,  P.  Jia, C. Chen et  al., “Large language model enhanced\n    M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers        recommender systems: Taxonomy, trend, application and future,” arXiv\n      to solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.         preprint arXiv:2412.13432, 2024.\n[14] M. Suzgun, N. Scales, S. Gehrmann et  al., “Challenging big-bench    [35] Z. Zhang, S. Liu, Z. Liu, R. Zhong, Q. Cai, X. Zhao, C. Zhang, Q. Liu,\n      tasks and whether chain-of-thought can solve them,” arXiv preprint        and P. Jiang, “Llm-powered user simulator for recommender system,”\n     arXiv:2210.09261, 2022.                                                   arXiv preprint arXiv:2412.16984, 2024.\n[15]  J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,    [36] M. Deng,  J. Wang, and Hsieh, “Rlprompt: Optimizing discrete text\n     D. Zhou et al., “Chain-of-thought prompting elicits reasoning in large        prompts with reinforcement learning,” arXiv preprint arXiv:2205.12548,\n     language models,” Advances in neural information processing systems,        2022.\n      vol. 35, pp. 24 824–24 837, 2022.                                         [37]  T.  Liu, N.  Astorga, N.  Seedat, and M. van  der  Schaar, “Large\n[16] Q. Ye, M. Axmed, R. Pryzant, and F. Khani, “Prompt engineering a        language models to enhance bayesian optimization,” arXiv preprint\n     prompt engineer,” arXiv preprint arXiv:2311.05661, 2023.                    arXiv:2402.03921, 2024.\n[17] OpenAI, “Gpt-3.5 turbo model documentation,” https://platform.openai.\n     com/docs/models/gpt-3-5#gpt-3-5-turbo, 2023.\n[18] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark,\n     A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., “Gpt-4o system\n      card,” arXiv preprint arXiv:2410.21276, 2024.\n[19] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\n     A. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of\n     models,” arXiv preprint arXiv:2407.21783, 2024.\n[20] N. Reimers, “Sentence-bert: Sentence embeddings using siamese bert-\n     networks,” arXiv preprint arXiv:1908.10084, 2019.\n[21] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n     “Language models are unsupervised multitask learners,” OpenAI blog,\n      vol. 1, no. 8, p. 9, 2019.\n[22] Q. Cai, X. Zhao, L. Pan, X. Xin, J. Huang, W. Zhang, L. Zhao, D. Yin,\n     and G. H. Yang, “Agentir: 1st workshop on agent-based information\n      retrieval,” in Proceedings of the 47th International ACM SIGIR Confer-\n     ence on Research and Development in Information Retrieval, 2024, pp.\n     3025–3028.\n[23] X.  Li, L. Su,  P.  Jia, X. Zhao,  S. Cheng,  J. Wang, and D. Yin,\n     “Agent4ranking: Semantic robust ranking via personalized query rewrit-\n     ing using multi-agent llm,” arXiv preprint arXiv:2312.15450, 2023.",
"headers": [
"TAPO: Task-Referenced Adaptation for Prompt",
"Optimization",
"arXiv:2501.06689v3  [cs.CL]  26 Feb 2025"
],
"tables": [
"|TAPO|Break the problem into smaller parts. Identify<br>key elements, use diagrams or rephrase, and<br>remove unnecessary information.|\n|---|---|",
"|TAPO|Improve translation error detection by creating<br>a structured framework to categorize errors,<br>focusing on common issues, and using iterative<br>testing with feedback to refine strategies.|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2501.06689v3.pdf"
}