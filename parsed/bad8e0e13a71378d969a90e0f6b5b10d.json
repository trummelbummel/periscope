{
"text": "Rethinking Prompt Optimization\n\n\n\n              Rethinking Prompt Optimization: Reinforcement, Diversifica-\n                 tion, and Migration in Blackbox LLMs\n\n\n                    MohammadReza Davari1,2   Utkarsh Garg3   Weixin Cai3   Eugene Belilovsky1,2\n                                1Concordia University   2Mila – Quebec AI Institute   3Microsoft\n                                {mohammadreza.davari,eugene.belilovsky}@concordia.ca\n                                       {utkarshgarg,weixincai}@microsoft.com\n\n\n                                               Abstract\n\n                     An increasing number of NLP applications interact with large language mod-\n                                    els (LLMs) through black-box APIs, making prompt engineering critical for\n                               controlling model outputs. While recent Automatic Prompt Optimization2025                      (APO) methods iteratively refine prompts using model-generated feedback,\nJul                          textualable insightsgradients,fromtheycorrectprimarilypredictions.focus onThiserrorlimitscorrectionboth theirand effectivenessneglect valu-\n                         and efficiency. In this paper, we propose a novel APO framework centered on\n                            enhancing the feedback mechanism. We reinterpret the textual gradient as14\n                           a form of negative reinforcement and introduce the complementary positive\n                              reinforcement to explicitly preserve beneficial prompt components identified\n                           through successful predictions. To mitigate the noise inherent in LLM-\n                             generated feedback, we introduce a technique called feedback diversification,\n                          which aggregates multiple feedback signals, emphasizing consistent, action-\n                             able advice while filtering out outliers. Motivated by the rapid evolution[cs.LG]                    and diversity of available LLMs, we also formalize Continual Prompt Opti-\n                            mization (CPO), addressing the practical challenge of efficiently migrating\n                            optimized prompts between different model versions or API providers. Our\n                            experiments reveal that naive prompt migration often degrades performance\n                         due to loss of critical instructions. In contrast, our approach consistently\n                            outperforms strong baselines, achieving significant accuracy improvements,\n                                faster convergence, and lower computational costs in both standard and\n                            migration scenarios.1\n\n                1  Introduction\n\n                       Traditionally, NLP tasks have relied on direct fine-tuning of pretrained foundation mod-\n                           els (Bommasani et al., 2021; Devlin et al., 2019; Lewis et al., 2020; Radford et al., 2018; Raffel\n                        et al., 2020) on downstream datasets (Davari et al., 2019; Farahnak et al., 2021; Davari et al.,\n                     2020; Yang et al., 2023b; Marks et al., 2024; Davari, 2020). This process enables models\n                     to adapt their internal parameters to task-specific distributions.  Parallel to fine-tuning,arXiv:2507.09839v1             methods utilizing internal model representations, such as hidden states (Rogers et al., 2021;\n                     Davari et al., 2022a), gradients, and attention patterns (Kornblith et al., 2019; Raghu et al.,\n                     2021; Davari et al., 2023; 2022b), have inspired advanced optimization techniques including\n                  prompt tuning (Li & Liang, 2021; Lester et al., 2021), LoRA (Hu et al., 2022), and other\n                      parameter-efficient methods (Davari & Belilovsky, 2024b; Yadav et al., 2023; Davari &\n                       Belilovsky, 2024a; Yu et al., 2024).\n\n                    However, the NLP landscape is shifting toward closed-weight Large Language Models (LLMs)\n                     accessed via black-box APIs, where internal representations and gradients are inaccessible.\n                     Consequently, traditional fine-tuning and interpretability-driven methods become impractical,\n                      placing greater emphasis on prompt design as the primary mechanism for model adaptation.\n                 The increasing reliance on API-based models, such as GPT-3 and GPT-4, has spurred a wave\n                        of commercial applications (OpenAI, 2023; Bubeck et al., 2023), where carefully engineered\n                  prompts critically influence model outputs (Luo et al., 2022; Kojima et al., 2022; Wang\n                       et al., 2022; Zhou et al., 2022a; Madaan et al., 2023; Bai et al., 2022; Chen et al., 2023c).\n\n                      1Code and implementation details are available at: https://github.com/rezazzr/BREAD\n\n\n                                                           1\n\nRethinking Prompt Optimization\n\n\n\n\n\nYet manually formulating effective prompts is costly, requiring extensive domain knowledge,\nprompting expertise (Jiang et al., 2023b; Wei et al., 2022; Kong et al., 2023), and considerable\ntrial-and-error effort.\n\nTo alleviate these challenges, recent work has introduced Automatic Prompt Optimization\n(APO) methods (Wang et al., 2023a; Yang et al., 2023a; Zhou et al., 2022b; Pryzant et al.,\n2023), which iteratively refine prompts based on model performance feedback, typically\nmeasured on a held-out validation set. These refinement methods either follow predefined\nactions such as deleting, paraphrasing, or rearranging prompt elements (Zhou et al., 2022b;\nSchnabel & Neville, 2024), or employ a secondary LLM to generate candidate prompts guided\nby model predictions and corresponding feedback signals (Wang et al., 2023a; Pryzant et al.,\n2023; Yang et al., 2023a). Such feedback, often referred to as the textual gradient (Wang et al.,\n2023a), is typically generated from incorrect predictions and serves as a form of negative\nreinforcement aimed at correcting errors. While useful, this singular focus neglects valuable\ninformation available from correct predictions, limiting both efficiency and effectiveness.\n\nIn this paper, we enhance the feedback component of prompt optimization, a dimension\nrelatively underexplored compared to search and planning approaches explored in prior\nwork (Wang et al., 2023a; Pryzant et al., 2023; Schnabel & Neville, 2024). Our insights\ncomplement existing methods and can be seamlessly integrated with them.  Specifically,\nwe reconceptualize the textual gradient as negative reinforcement (Mnih et al., 2013),\ncapturing corrective signals from incorrect predictions. To augment this, we introduce\npositive reinforcement, explicitly identifying and reinforcing beneficial prompt components\ndiscovered through successful predictions.\n\nAdditionally, to manage variability and noise in LLM-generated feedback, we propose\nfeedback diversification. This method aggregates multiple feedback samples (both positive\nand negative) for the same prompt and training examples, summarizing them via an LLM.\nThe aggregation leads to an emphasis on consistent, impactful instructions while filtering out\nnoisy outliers. This process resembles the peer review process, where diverse perspectives\ncollectively highlight the most valuable insights. We combine these concepts into BReAD,\nshort for Balanced Reinforcement and Aggregated Diversification, a unified framework\nyielding robust, efficient prompt optimization.\n\nPrompt optimization becomes even more challenging when migrating optimized prompts\nbetween different LLMs or across API providers. Rapid advancements in LLM technology\nfrequently necessitate adapting existing optimized prompts to newer models, such as from\nGPT-3 to GPT-4.  To formally address this scenario, we introduce Continual Prompt\nOptimization (CPO), inspired by the machine learning paradigm of continual learning (Parisi\net al., 2019; Li & Hoiem, 2017; Davari et al., 2022a). Similar to continual learning, CPO\nemphasizes retaining critical knowledge, specifically valuable prompt elements, while adapting\nto new models.\n\nNaive prompt migration strategies, involving direct reuse of previously optimized prompts, fre-\nquently degrade performance due to differences in model capabilities, tasks, and providers (Ko-\njima et al., 2022; Zhou et al., 2022b; Zhang et al., 2022; Ma et al., 2023; Chen et al., 2023a).\nStandard APO methods can exacerbate this issue, inadvertently discarding or altering crucial\nprompt elements during iterative optimization. Our proposed framework mitigates this\nproblem by explicitly preserving essential instructions through balanced reinforcement and\nfeedback diversification, ensuring efficient and effective prompt migration.\n\nEmpirical evaluations demonstrate our framework consistently surpasses strong baselines in\nstandard optimization and realistic migration scenarios. Experiments migrating prompts from\nGPT-3.5-turbo to GPT-4o show significantly improved accuracy and reduced computational\ncost through accelerated convergence. Overall, our contributions are as follows:\n\n      1. We introduce and formalize Continual Prompt Optimization (CPO), a framework\n        designed to efficiently adapt optimized prompts across evolving LLM versions,\n          effectively addressing performance degradation and instruction loss common in naive\n        migration approaches.\n\n\n                                       2\n\nRethinking Prompt Optimization\n\n\n\n\n\n      2. We propose BReAD, an APO method combining structured positive/negative re-\n        inforcement and feedback diversification, achieving robust accuracy improvements,\n         faster convergence, and reduced API usage.\n\n2  Related Work\n\nAutomatic prompt optimization (APO) methods can be broadly categorized based on their\nlevel of access to model internals. The first category includes approaches that assume full or\npartial access to internal model states, such as parameters, gradients, or output probabilities.\nThese methods are primarily applicable to open-source models like LLaMA (Touvron et al.,\n2023a;b; Grattafiori et al., 2024) or Mistral (Jiang et al., 2023a; team, 2024). Leveraging\nthis internal information, these methods can directly train additional parameters, such as\nsoft prompts (Li & Liang, 2021; Lester et al., 2021; Hu et al., 2022; Wang et al., 2023b;\nQin & Eisner, 2021), or optimize discrete prompts using gradient-guided search (Shin et al.,\n2020; Wen et al., 2023; Gao et al., 2020; Chen et al., 2023b). Some approaches even involve\ntraining the prompt generator itself (Hao et al., 2023; Wang et al., 2022). However, these\ntechniques are not feasible when interacting with black-box APIs, which is the primary focus\nof our paper.\n\nThe second category encompasses methods designed for black-box interaction, where models\nare accessed only through APIs without internal visibility. Black-box APO methods generally\nfall into two subcategories: (1) iterative generation methods and (2) search and planning-\nbased methods.\n\nIterative Generation and Evaluation  Iterative generation methods repeatedly propose new\nprompt candidates, evaluate their effectiveness based on the model’s performance, and select\nthe best-performing prompts for subsequent iterations. These methods typically rely on\nperformance-driven cycles of generation and evaluation, often using external metrics or\nvalidation sets.\n\nAutomatic Prompt Engineer (APE)(Zhou et al., 2022b), for example, iteratively generates\nprompt variations through semantic modifications of an initial prompt, selecting the best\ncandidate based on validation accuracy until convergence.  Similarly, Optimization by\nPROmpting (OPRO)(Yang et al., 2023a) employs an LLM to iteratively propose new prompts\ninformed by previous prompts and their respective performance metrics. Unlike APE, OPRO\ncan generate substantially different prompts rather than mere semantic variations.\n\nDespite their effectiveness, iterative methods like APE and OPRO primarily rely on implicit\nand unstructured feedback signals derived indirectly from performance metrics. Consequently,\nthese methods are limited in the depth of optimization they can achieve. In contrast, our\nproposed approach explicitly incorporates structured feedback, both positive and negative,\nderived directly from model predictions, significantly enriching the feedback mechanism and\nenhancing the optimization process.\n\nSearch and Planning-based Approaches  Another family of black-box APO methods frames\nprompt optimization as a search or planning problem. These methods leverage systematic\nexploration techniques, such as tree search or multi-objective optimization, to efficiently\nnavigate the prompt space. Prominent examples include PromptAgent (Wang et al., 2023a),\nProTeGi (Pryzant et al., 2023), and SAMMO (Schnabel & Neville, 2024).\n\nPromptAgent employs Monte Carlo Tree Search (MCTS) (Coulom, 2006) to formulate\nprompt optimization as sequential decision-making, guided by a textual gradient derived from\nincorrect predictions. Similarly, ProTeGi uses beam search to systematically explore prompt\nmodifications, also relying primarily on corrective textual gradients. In contrast, SAMMO\napplies a multi-objective optimization framework, performing structural modifications, such\nas adding, removing, or replacing prompt components, guided by predefined objectives.\n\nWhile these search-based methods effectively explore the prompt space, they predominantly\nutilize corrective feedback from incorrect model predictions, neglecting beneficial insights from\ncorrect predictions. Our proposed framework complements these methods by enhancing the\nfeedback component. By explicitly integrating structured positive and negative reinforcement\nsignals, along with feedback diversification to manage feedback noise, our approach improves\n\n\n                                       3\n\nRethinking Prompt Optimization\n\n\n\n\n\nboth efficiency and robustness. Importantly, our framework can be seamlessly combined with\nthese existing search-based methods, potentially increasing their performance by enriching\ntheir feedback mechanisms.\n\n3  Methodology\n\nIn this section, we introduce our proposed\n                                                                       Current Promptframework for APO, which is composed of five\nprimary modules (see Figure 1).                                   Base prompt for the current\n                                                                                              optimization step\n                                                       Training BatchForward Generation:  This module generates\nmodel predictions by applying the current\nprompt to a batch of training examples via      Forward Generation\nthe underlying LLM.\n                                                        Generates predictions via LLM\nEvaluation:  The evaluation module measures\nthe effectiveness of a given prompt based on\nthe accuracy of the model’s predictions on a                                                       Evaluation Module\nheld-out validation set. The evaluation can\n                                                            Evaluates prompt effectiveness\nbe deterministic, such as measuring accuracy                                                          on validation set\nvia the exact match between the model’s pre-\ndictions and the ground truth labels, or it can                            Next Iteration\nbe LLM-based, where the model is asked to                                               Feedback Generation\nevaluate the quality of the predictions. In this\n                                                                  Positive & negative feedback for\npaper, we focus on deterministic evaluation,                                                           a training batch\nbut the framework can be easily extended to\ninclude LLM-based evaluations.\n                                                           Feedback Diversification\nFeedback Generation:  This module is respon-                      Generates multiple feedback\nsible for producing structured feedback signals                                     signals\nbased on the model’s predictions using the cur-\nrent prompt. It generates positive feedback,     Feedback Aggregation\nwhich explicitly identifies and encourages re-    Aggregates multiple feedbacks\ntaining prompt components that lead to cor-         into one actionable signal\nrect model predictions, and negative feedback,\nwhich highlights potential issues that lead to\nincorrect predictions, suggesting specific mod-     Prompt Update Module\nifications to improve model accuracy. When     Generates a new prompt based\nemploying feedback diversification, this mod-           on feedback\nule generates multiple feedback signals (both\npositive and negative) for the same prompt\nand training batch. These signals are then                Search Module - Abstracted\naggregated via a summarization call to an\nLLM, resulting in a consolidated feedback sig-                       Selects next candidate prompt\nnal emphasizing consistent and impactful in-\nstructions while minimizing noise.            Figure 1: Overview of our proposed frame-\n                                      work for automatic prompt optimization\nPrompt Update:  This module generates a\n                                     (APO). The framework consists of five pri-\nnew prompt based on the current prompt, the\n                                      mary modules: Forward Generation, Evalua-\ntraining batch processed by the model, and\n                                                    tion, Feedback Generation, Prompt Update,\nthe feedback signals generated in the previous\n                                      and Search Module. The Search Module is\nstep.\n                                              abstracted to allow for the integration of var-\nSearch Module (Abstracted):  The search  ious search and planning methods.\nmodule proposes the next candidate prompt to be further explored. The suggestion is\nbased on the evaluation metrics and other search-related criteria (e.g., visitation frequency\nin MCTS). Since this research focuses explicitly on feedback enhancement rather than search\nor planning techniques, this module is treated as an abstract, interchangeable component\nthat can adopt existing APO methods, including PromptAgent (Wang et al., 2023a), Pro-\nTeGi (Pryzant et al., 2023), or iterative generation methods such as APE (Zhou et al., 2022b)\nand OPRO (Yang et al., 2023a).\n\n\n                                       4\n\nRethinking Prompt Optimization\n\n\n\n\n\nIn our experimental evaluation, Section 4, we specifically utilize PromptAgent (Wang et al.,\n2023a) as the baseline search and planning method due to its established effectiveness\nand robust performance across diverse prompt optimization tasks (Zhang et al., 2025; Li\net al., 2025). Nevertheless, the modular design of our feedback generation component allows\nstraightforward integration with other existing APO frameworks, making our approach\nversatile and broadly applicable.\n\n4  Experiments\n\n4.1  Data, Metrics, and Models\nWe evaluate our method across five tasks that collectively span causal, spatial, tabular,\ninferential, and semantic reasoning, covering both classification and regression settings. Three\nof these tasks are drawn from the BBH benchmark (Suzgun et al., 2022), a widely used\nsuite in prompt optimization (Schnabel & Neville, 2024; Wang et al., 2023a; Zhou et al.,\n2022b; Pryzant et al., 2023; Yang et al., 2023a) and prompt engineering research (Wei et al.,\n2022; Kojima et al., 2022; Wang et al., 2022; Zhou et al., 2022a; Madaan et al., 2023; Bai\net al., 2022; Chen et al., 2023c). We focus on a representative subset of BBH tasks selected\nfor their diversity in reasoning requirements: Causal Judgment evaluates causal inference\nthrough binary classification; Geometric Shapes assesses geometric and spatial reasoning by\nrequiring the model to interpret SVG path strings in a multi-class classification setting; and\nPenguins tests the model’s ability to perform table-based classification by reasoning over\nstructured data.\n\nTo complement these, we include two additional benchmarks outside of BBH. The Com-\nmitmentBank (CB) (De Marneffe et al., 2019) is a natural language inference dataset in\nwhich models classify contextualized sentence pairs as entailment, contradiction, or neutral.\nBiosses (Soğancıoğlu et al., 2017) is a biomedical dataset that measures sentence-level seman-\ntic similarity, framed as a regression task with continuous similarity scores. This combination\nof tasks enables a comprehensive evaluation of our framework’s generalizability, robustness,\nand performance across a broad spectrum of reasoning types, domains, and output formats.\nFor details on the dataset statistics, see Appendix A.\n\nWe use accuracy as the primary evaluation metric. During the optimization process, model\nperformance is assessed on a held-out validation set, and final results are reported on the test\nset. Both GPT-3.5-turbo and GPT-4o are evaluated using the dataset’s default prompts, as\nwell as the optimized prompts generated by our proposed approach and the baseline method,\nPromptAgent (Wang et al., 2023a).\n\n4.2  Standard Prompt Optimization (GPT-3.5-turbo)\nIn this section, we evaluate our methodology in the standard Automatic Prompt Optimiza-\ntion (APO) setting, where the goal is to improve a prompt for a fixed language model,\nGPT-3.5-turbo. We initialize the optimization with a task-specific default prompt, which\nis a concise, single-sentence instruction describing the task (e.g., “Answer questions about\ncausal attribution” for Causal Judgment). The full list of default prompts is provided in\nAppendix B.\n\nEach experiment is run for up to 15 optimization iterations. During this process, the feedback\ndiversification module samples six feedback signals per batch. Positive feedback is introduced\nafter the third or fourth iteration, depending on the dataset, as determined empirically (see\nSection 4.4 for details). This staged introduction ensures that early updates prioritize error\ncorrection before preserving beneficial prompt components.\n\nTable 1 summarizes our experimental results, averaged across five random seeds. Compared\nto the PromptAgent baseline (Wang et al., 2023a), our approach achieves consistent and\nstatistically significant improvements in accuracy across all tasks, ranging from 4.9% to\n21.5% (p < 0.01). Additionally, incorporating feedback diversification, either to the baseline\nor in our approach, reduces the standard deviation, which highlights the role of feedback\ndiversification in stabilizing the optimization process. Beyond accuracy, our method offers\nimproved efficiency. By reinforcing helpful prompt elements early, it avoids redundant\nremovals and rediscoveries, leading to faster convergence. This translates into a reduction\nranging from 0.5% to 3.3% in the number of LLM calls across tasks compared to the baseline\n\n\n                                       5\n\nRethinking Prompt Optimization\n\n\n\n\n\n        Dataset (Init. Acc.)  Method          Accuracy   p-value  Cohen’s d\n\n                               Baseline          58.6 ± 3.98        –          –\n        Causal Judgment     Baseline + FD*   60.8 ± 2.38    0.020       1.687\n         (56.5 ± 3.67 )          Baseline + PR*   63.6 ± 2.62    0.040       1.336\n                       BReAD**         64.4 ± 2.16    0.008       2.162\n\n                               Baseline          52.1 ± 4.94        –          –\n        Geometric Shapes     Baseline + FD*   57.8 ± 3.15    0.032       1.439\n         (32.7 ± 2.04 )          Baseline + PR*   61.6 ± 4.18    0.035       1.399\n                       BReAD**         63.3 ± 1.16    0.004       2.644\n\n                               Baseline          65.1 ± 4.96        –          –\n        Penguins             Baseline + FD*   66.1 ± 2.42    0.083       1.148\n         (60.5 ± 4.87 )          Baseline + PR*   66.9 ± 3.97    0.043       1.464\n                       BReAD**         68.6 ± 1.87    0.007       2.556\n\n                               Baseline          62.5 ± 4.19        –          –\n         Biosses               Baseline + FD*   67.0 ± 2.92    0.044       1.456\n         (25.2 ± 3.84 )                               Baseline + PR*   68.2 ± 3.02    0.021       1.844\n                       BReAD**         70.4 ± 2.02    0.006       2.654\n\n                               Baseline          81.7 ± 3.17        –          –\n     CB                   Baseline + FD*   84.2 ± 2.02    0.032       1.610\n         (68.5 ± 4.22 )                               Baseline + PR*   84.2 ± 3.73    0.049       1.402\n                       BReAD**         85.7 ± 3.54    0.008       2.495\n\nTable 1: Performance of prompt optimization methods on GPT-3.5-turbo, averaged over\nfive random seeds. Results show mean accuracy (± standard deviation), with statistical\nsignificance assessed via two-tailed paired t-tests (p-value, Cohen’s d). +FD = Feedback\nDiversification; +PR = Positive Reinforcement; BReAD = full method combining both. Init.\nAcc. refers to the accuracy achieved by the default prompt before optimization. All methods\nuse 15 iterations and 6 feedback samples per step; positive reinforcement is introduced at\niteration 3 for Casual Judgment, Geometric Shapes, and Penguins, and at iteration 4 for\nBiosses and CB. BReAD consistently outperforms the baseline and its variants, achieving\nsignificant accuracy gains (4.9%–21.5%) and improved stability.\n\n\n(details in Appendix C). Such efficiency is particularly valuable in real-world applications,\nwhere API costs and latency are critical considerations.\n\n4.3  Prompt Migration: GPT-3.5-turbo →GPT-4o\nHaving established the effectiveness of our method in the standard optimization setting,\nwe now turn to the more challenging and practically important scenario of prompt migra-\ntion. This setting involves transferring prompts optimized for one language model (e.g.,\nGPT-3.5-turbo) to a newer or different model (e.g., GPT-4o), with the goal of preserving\nboth effectiveness and key instructional components.\n\nWe begin by evaluating the direct transferability of prompts optimized for GPT-3.5-turbo.\nWe refer to these as expert prompts, which are the final outputs of the APO procedure\ndescribed in Section 4.2. These prompts are applied directly to GPT-4o and compared against\nthe task-specific default prompts. A full list of default and expert prompts is provided in\nAppendix B. As shown in Table 2, expert prompts generally outperform default prompts\nwhen transferred directly, yielding initial accuracy gains of 1.3% to 3.3% across tasks.\n\nHowever, applying standard APO to these transferred expert prompts often results in\nworse or statistically insignificant performance improvements, compared to optimizing from\nthe default prompt. This degradation occurs because early-stage corrective feedback can\noverwrite useful instructions of the expert prompt that were critical for task success. While\nsome of this content may be recovered in later iterations, the optimization becomes inefficient\n\n\n                                       6\n\nRethinking Prompt Optimization\n\n\n\n\n\n       Dataset            Stage  DP Acc.   EP Acc.    p-value   Cohen’s d\n                                 Initial   71.8 ± 1.92   74.2 ± 3.46   0.033∗        1.434\n       Causal Judgment\n                            Final    73.4 ± 1.82   73.8 ± 1.79   0.803         0.119\n                                 Initial   54.8 ± 1.89   58.2 ± 2.22   0.001∗∗       4.138\n       Geometric Shapes                            Final    79.0 ± 5.32   75.1 ± 5.80   0.040∗       –1.344\n                                 Initial   92.9 ± 1.85   95.8 ± 1.72   0.025∗        1.745\n       Penguins                            Final    95.2 ± 2.03   92.3 ± 2.89   0.005∗∗      –2.771\n                                 Initial   69.9 ± 2.73   72.2 ± 1.78   0.0125∗       2.159\n        Biosses\n                            Final    76.7 ± 2.84   76.1 ± 1.97   0.381        –0.600\n                                 Initial   79.3 ± 1.57   80.3 ± 1.54   0.005∗∗       2.855\n     CB\n                            Final    80.0 ± 4.62   78.7 ± 2.13   0.381        –0.492\n\nTable 2: Transferability of Expert Prompts (EP) (optimized prompts from GPT-3.5-turbo)\nto GPT-4o. Accuracy (mean ± std, over five seeds) is reported for the Default Prompt (DP)\nand the transferred EP at two stages: Initial: direct transfer without further optimization,\nand Final: after 15 iterations of PromptAgent on GPT-4o. Significance is assessed with\ntwo-tailed paired t-tests. Expert prompts yield small but significant gains on direct transfer\n(1.3%–3.3% improvement), yet re-optimizing them often erodes or reverses this advantage.\n\n\n\nand unstable. These findings highlight a key limitation of naive prompt migration: it risks\ndiscarding task-relevant knowledge embedded in prompts tuned for the source model.\n\nNext, we evaluate the effectiveness of our framework in the prompt migration setting.\nThe optimization process is initialized with expert prompts derived from GPT-3.5-turbo\noptimization (see Section 4.2) and applied to GPT-4o for up to 15 iterations. To account\nfor the higher API cost and greater accuracy of GPT-4o, we reduce the number of feedback\ndiversification samples to two per iteration. Unlike in the standard APO setting, where the\ninitial prompt contains minimal task-specific guidance, we introduce positive feedback from\nthe first iteration to preserve valuable information already present in the expert prompts.\nThese design choices are guided by preliminary experiments, which showed diminishing\nreturns from additional diversification and performance degradation when delaying positive\nfeedback.\n\nAs shown in Table 3, our method consistently improves performance across all tasks, with\nrelative accuracy gains ranging from 3.5% to 16.0% over the PromptAgent baseline (p < 0.01).\nThe most substantial improvements are observed in Geometric Shapes (12.5%) and Biosses\n(16.0%), where preserving domain-specific instructions is especially important. In Geometric\nShapes, spatial reasoning patterns embedded in the prompt are critical; in Biosses, biomedical\nterminology and precise phrasing directly affect model output. These results demonstrate\nthe framework’s ability to maintain and adapt task-relevant instructions during migration.\n\nIn addition to accuracy improvements, our method enhances both stability and efficiency. As\nin the standard APO setting, we again observe that feedback diversification reduces variance\nacross runs, leading to more consistent convergence. Moreover, the total number of LLM\ncalls is reduced by 4.2% to 6.2% compared to the baseline, offering meaningful cost savings\ngiven the higher API expense of GPT-4o. A detailed breakdown of LLM usage is provided in\nAppendix C.\n\n4.4  Ablation Studies\n\nIn this section, we analyze the contribution of key components and hyperparameters in\nour framework through a series of ablation studies. All experiments are conducted using\nthe GPT-3.5-turbo model with a maximum of 8 optimization iterations, starting from the\nminimal instruction set defined in the default prompts (see Appendix B). We report relative\naccuracy improvements over the PromptAgent baseline (Wang et al., 2023a), shown as a\ndotted red line in Figures 2a and 2b.\n\n\n                                       7\n\nRethinking Prompt Optimization\n\n\n\n\n\n        Dataset (Init. Acc.)  Method          Final Acc.   p-value  Cohen’s d\n\n                              Baseline          73.8 ± 1.79        –          –\n        Causal Judgment\n                              Baseline + FD*   74.7 ± 1.44    0.053       1.214       DP: 71.8 ± 1.92\n       EP: 74.2 ± 3.46        Baseline + PR*   75.8 ± 1.78    0.047       1.265\n                       BReAD**         76.4 ± 1.59    0.007       2.280\n\n                              Baseline          75.1 ± 5.80        –          –\n       Geometric Shapes\n                              Baseline + FD*   79.4 ± 2.92    0.016       1.782       DP: 54.8 ± 1.89\n       EP: 58.2 ± 2.22        Baseline + PR*   81.7 ± 3.07    0.021       1.641\n                       BReAD**         84.5 ± 2.33    0.008       2.175\n\n                              Baseline          92.3 ± 2.89        –          –\n        Penguins\n                              Baseline + FD*   94.2 ± 1.33    0.083       1.148       DP: 92.9 ± 1.85\n       EP: 95.8 ± 1.72        Baseline + PR*   96.7 ± 0.88    0.041       1.493\n                       BReAD**         98.0 ± 0.73    0.008       2.449\n\n                              Baseline          76.1 ± 1.97        –          –\n        Biosses\n                              Baseline + FD*   78.4 ± 1.66    0.043       1.461       DP: 69.9 ± 2.73\n       EP: 72.2 ± 1.78        Baseline + PR*   83.7 ± 3.86    0.003       3.186\n                       BReAD**         88.3 ± 2.00   0.0001       8.696\n\n                              Baseline          78.7 ± 2.13        –          –\n     CB\n                              Baseline + FD*   82.7 ± 2.31    0.029       1.659       DP: 79.3 ± 1.57\n       EP: 80.3 ± 1.54        Baseline + PR*   85.3 ± 4.49    0.014       2.047\n                       BReAD**         87.5 ± 3.56    0.006       2.713\n\nTable 3: Prompt migration results from GPT-3.5-turbo to GPT-4o. Each block reports the\ninitial accuracy (± std.) of the Default Prompt (DP) and Expert Prompt (EP), followed by\nfinal post-optimization accuracy under different methods, averaged over five random seeds.\nAll experiments start from the EPs previously optimized for GPT-3.5-turbo. Baseline refers\nto the original PromptAgent; +FD augments the baseline with Feedback Diversification; +PR\nadds Positive Reinforcement; BReAD combines both. Two-tailed paired t-test p-values and\nCohen’s d measure statistical significance relative to the baseline. Significant improvements:\n∗p < 0.05, ∗∗p < 0.01. Our method, BReAD, consistently achieves the highest final accuracy\nand strongest effect sizes, with gains up to 16.0% over the baseline, underscoring its robustness\nin real-world migration scenarios where preserving critical prompt elements is essential.\n\n\n\nImpact of Positive and Negative Reinforcement  Our framework uses structured reinforce-\nment signals derived from model predictions. Negative reinforcement, based on incorrect\npredictions, guides the prompt toward correcting errors. Positive reinforcement, drawn from\ncorrect predictions, encourages the retention of effective prompt components.\n\n\nA key consideration here is when to introduce positive reinforcement. If applied too early,\nwhen the prompt is still underdeveloped, it may preserve premature structures and impede\nbeneficial edits. If applied too late, valuable components may already have been discarded,\nrequiring inefficient rediscovery. The timing, or depth, of introducing positive reinforcement\nis therefore critical.\n\n\nFigure 2a shows the relative accuracy gains across tasks as a function of this introduction\ndepth. Early introduction (depth 0 or 1) leads to underperformance across all tasks. Peak\nperformance is achieved at depth 3 for Causal Judgment, Geometric Shapes, and Penguins,\nand at depth 4 for CB and Biosses. Beyond these points, performance declines as the\noptimization process continues to discard and re-learn useful instructions for too long before\nreinforcement stabilizes them. These results suggest that a brief initial phase of corrective-\nonly feedback allows the model to build a minimal set of effective instructions, which can\nthen be reinforced and refined in subsequent iterations.\n\n\n                                       8\n\nRethinking Prompt Optimization\n\n\n\n\n      Impact of Balanced Reinforcement vs. Negative Reinforcement                Impact of Aggregated Feedback Diversification\n                  Causal Judgment      Geometric Shapes      Penguins      Biosses     CB                           Causal Judgment      Geometric Shapes      Penguins      Biosses     CB\n(%) 20                                                               (%)                                                            15\nAcc.    15                                                                                   Acc.\nin                                               in 10\n    10   Baseline: PromptAgent                                    5\n     5                                                       0Improvement                                                                                                                                                                                                                                                     Improvement  5     0\n                                                            10                     Baseline: PromptAgent\n     5\n                                                            15Normalized 10                                                                                                                                                                                                                 Normalized\n      0     1     2     3     4     5     6     7         2     3     4     5     6     7     8     9    10\n              Depth of Introduction of Positive Reinfrocement                                 Number of Feedbacks\n\n(a) Accuracy gains relative to the baseline (no pos- (b) Accuracy improvements relative to the base-\n itive reinforcement) when positive reinforcement  line (without feedback diversification) as a func-\n is introduced at different optimization depths.  tion of the number of diversification samples. Per-\n Performance peaks at depth 3–4.  Early intro- formance consistently improves up to six samples\n duction (depth 0–1) impedes prompt refinement,  across tasks, after which accuracy declines. This\n while later introduction (depth ≥5) leads to di- drop is attributed to overly generalized feedback\n minishing returns due to premature loss of useful  aggregation, which dilutes specific and actionable\n instructions.                                      suggestions.\n\n\n\nImpact of Feedback Diversification We also study the role of feedback diversification, which\n aggregates multiple independently sampled feedback signals into a consolidated actionable\nsummary. Inspired by the peer-review process, this mechanism aims to highlight consistent,\n high-value suggestions while down-weighting noisy or inconsistent signals.\n\nTo isolate this effect, we apply feedback diversification to the PromptAgent baseline using\n only negative reinforcement. Figure 2b reports relative accuracy improvements as the number\n of feedback samples per batch increases from 2 to 10. With just two samples, the benefits\n are limited, either due to insufficient variety or aggregation of conflicting signals into generic\n summaries. Accuracy improves steadily with more samples and plateaus around six for all\n datasets. Beyond this, performance drops.\n\nManual inspection reveals that excessive diversification (8–10 samples) produces overly\n generalized summaries that dilute actionable guidance. These findings highlight the need for\n a balanced sampling strategy that captures meaningful feedback diversity without introducing\n aggregation noise. While our implementation uses LLM-based summarization, alternative\n aggregation methods such as concatenation or voting may offer complementary trade-offs.\nHowever, these approaches may increase context length and computational cost, and we\n leave their exploration to future work.\n\n5  Conclusion and Future Work\n\n In this work, we introduced a novel framework for Automatic Prompt Optimization (APO)\n specifically designed to enhance the feedback generation process when optimizing prompts for\n black-box Large Language Models (LLMs). Unlike prior APO methods, which predominantly\n rely on corrective feedback from incorrect predictions, our method integrates structured\n reinforcement signals derived from both correct (positive reinforcement) and incorrect (nega-\n tive reinforcement) model predictions. Additionally, we introduced feedback diversification,\nan effective technique that aggregates multiple feedback signals to highlight consistently\n impactful instructions and reduce the influence of noisy or irrelevant feedback.\n\n Motivated by real-world industry scenarios, we also formalized the concept of Continual\nPrompt Optimization (CPO), addressing the practical need for efficiently adapting optimized\nprompts across evolving LLM versions or migrating prompts between different API providers.\nOur empirical evaluations demonstrated that naive migration strategies degrade performance\ndue to inadvertent loss or distortion of crucial instructions. In contrast, our reinforcement-\n driven APO framework consistently outperformed the state-of-the-art baseline, increased\n robustness, converged faster, and reduced computational cost.\n\n\n                                        9\n\nRethinking Prompt Optimization\n\n\n\n\n\nThese contributions open several promising directions for future work. Exploring alternative\naggregation techniques, such as weighted voting or prompt-based structuring of feedback\ninputs, may enhance the granularity and reliability of diversified signals. For instance, using\nconsistent prompting templates or role-based formulations could encourage clearer, more\ntargeted feedback from the model. Extending the framework to dynamic or interactive\nsettings, including multi-turn dialogue, real-time systems, or multimodal inputs, could\nfurther broaden its applicability. Finally, incorporating adaptive reinforcement schedules\nthat respond to uncertainty or performance drift may lead to more sample-efficient and\nresilient optimization strategies in real-world deployments.\n\n6  Limitations\n\nDespite the consistent improvements observed in prompt optimization efficiency, robustness,\nand migration performance, our framework has several limitations. First, our experiments\nare limited to GPT-family models (GPT-3.5-turbo and GPT-4o), which may constrain\ngeneralizability to other LLMs or commercial APIs.  Future work should evaluate the\nframework’s adaptability across a broader range of models and providers.\n\nSecond, the effectiveness of both reinforcement and feedback diversification strategies depends\non careful hyperparameter selection, such as the number of diversification samples and the\ntiming of positive reinforcement. While we conducted ablation studies to characterize these\nchoices, more systematic or automated tuning may be necessary for optimal performance\nacross diverse tasks.\n\nFinally, although our method reduces computational cost relative to a strong baseline, it still\ninvolves multiple API calls per optimization step. This can be a limiting factor in large-scale\nor latency-sensitive deployments. Exploring approaches such as adaptive sampling, caching\nstrategies, or lightweight aggregation mechanisms could help reduce this cost and further\nimprove real-world feasibility.\n\n\n\n\n\n                                       10\n\nRethinking Prompt Optimization\n\n\n\n\nReferences\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy\n  Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitu-\n  tional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\n\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\n  Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On\n  the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n\nSébastien Bubeck, Varun Chadrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\n  Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial\n  general intelligence: Early experiments with gpt-4, 2023.\n\nJiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. When do you need chain-of-\n  thought prompting for chatgpt? arXiv preprint arXiv:2304.03262, 2023a.\n\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero:\n  Efficient instruction optimization for black-box large language models. arXiv preprint\n  arXiv:2306.03082, 2023b.\n\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language\n  models to self-debug. arXiv preprint arXiv:2304.05128, 2023c.\n\nRémi Coulom.  Efficient selectivity and backup operators in monte-carlo tree search. In\n  International conference on computers and games, pp. 72–83. Springer, 2006.\n\nMohammadReza Davari. Neural network approaches to medical toponym recognition. PhD\n   thesis, Concordia University, 2020.\n\nMohammadReza Davari and Eugene Belilovsky. Model breadcrumbs: Scaling multi-task\n  model merging with sparse masks. In European Conference on Computer Vision, pp.\n  270–287. Springer, 2024a.\n\nMohammadReza Davari and Eugene Belilovsky. Model breadcrumbs: scalable upcycling of\n  finetuned foundation models via sparse task vectors merging. In ICML 2024 Workshop on\n  Foundation Models in the Wild, 2024b.\n\nMohammadReza Davari, Leila Kosseim, and Tien D Bui. Toponym identification in epidemi-\n  ology articles–a deep learning approach. In International Conference on Computational\n  Linguistics and Intelligent Text Processing, pp. 26–37. Springer, 2019.\n\nMohammadReza Davari, Leila Kosseim, and Tien Bui. Timbert: toponym identifier for the\n  medical domain based on bert. In Proceedings of the 28th International Conference on\n  Computational Linguistics, pp. 662–668, 2020.\n\nMohammadReza Davari, Nader Asadi, Sudhir Mudur, Rahaf Aljundi, and Eugene Belilovsky.\n  Probing representation forgetting in supervised and unsupervised continual learning. In\n  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\n  16712–16721, 2022a.\n\nMohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, and\n  Eugene Belilovsky. On the inadequacy of cka as a measure of similarity in deep learning.\n  In ICLR 2022 Workshop on Geometrical and Topological Representation Learning, 2022b.\n\nMohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, and\n  Eugene Belilovsky. Reliability of CKA as a similarity measure in deep learning. In The\n  Eleventh International Conference on Learning Representations, 2023.\n\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank:\n  Investigating projection in naturally occurring discourse.  In proceedings of Sinn und\n  Bedeutung, volume 23, pp. 107–124, 2019.\n\n\n                                       11\n\nRethinking Prompt Optimization\n\n\n\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\n  deep bidirectional transformers for language understanding. In Proceedings of the 2019\n  conference of the North American chapter of the association for computational linguistics:\n  human language technologies, volume 1 (long and short papers), pp. 4171–4186, 2019.\n\nFarhood Farahnak, Elham Mohammadi, MohammadReza Davari, and Leila Kosseim. Se-\n  mantic similarity matching using contextualized representations. In Canadian AI, 2021.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better\n  few-shot learners. arXiv preprint arXiv:2012.15723, 2020.\n\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\n  Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The\n  llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei.  Optimizing prompts for text-to-image\n  generation. Advances in Neural Information Processing Systems, 36:66923–66939, 2023.\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\n  Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR,\n  1(2):3, 2022.\n\nAlbert Q Jiang, A Sablayrolles, A Mensch, C Bamford, D Singh Chaplot, Ddl Casas,\n  F Bressand, G Lengyel, G Lample, L Saulnier, et al. Mistral 7b. arxiv. arXiv preprint\n  arXiv:2310.06825, 10, 2023a.\n\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen.\n  Structgpt: A general framework for large language model to reason over structured data.\n  arXiv preprint arXiv:2305.09645, 2023b.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\n  Large language models are zero-shot reasoners. Advances in neural information processing\n  systems, 35:22199–22213, 2022.\n\nAobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi\n  Wang, and Xiaohang Dong. Better zero-shot reasoning with role-play prompting. arXiv\n  preprint arXiv:2308.07702, 2023.\n\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of\n  neural network representations revisited. In International conference on machine learning,\n  pp. 3519–3529. PMLR, 2019.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\n  prompt tuning. arXiv preprint arXiv:2104.08691, 2021.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\n  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-\n  sequence pre-training for natural language generation, translation, and comprehension. In\n  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\n  pp. 7871–7880. Association for Computational Linguistics, July 2020.\n\nWenwu Li, Xiangfeng Wang, Wenhao Li, and Bo Jin. A survey of automatic prompt\n  engineering: An optimization perspective. arXiv preprint arXiv:2502.11560, 2025.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.\n  arXiv preprint arXiv:2101.00190, 2021.\n\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern\n  analysis and machine intelligence, 40(12):2935–2947, 2017.\n\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan\n  Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining.\n  Briefings in bioinformatics, 23(6):bbac409, 2022.\n\n\n                                       12\n\nRethinking Prompt Optimization\n\n\n\n\n\nXiao Ma, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen.  Let’s do a\n  thought experiment: Using counterfactuals to improve moral reasoning. arXiv preprint\n  arXiv:2306.14308, 2023.\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe,\n  Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative\n  refinement with self-feedback. Advances in Neural Information Processing Systems, 36:\n  46534–46594, 2023.\n\nJennifer Marks, MohammadReza Davari, and Leila Kosseim. Clac at semeval-2024 task\n  2: Faithful clinical trial inference. In Proceedings of the 18th International Workshop on\n  Semantic Evaluation (SemEval-2024), pp. 1673–1677, 2024.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\n  Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv\n  preprint arXiv:1312.5602, 2013.\n\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\nGerman I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter.\n  Continual lifelong learning with neural networks: A review. Neural networks, 113:54–71,\n  2019.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Au-\n  tomatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint\n  arXiv:2305.03495, 2023.\n\nGuanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft\n  prompts. arXiv preprint arXiv:2104.06599, 2021.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\n  understanding by generative pre-training. 2018.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\n  Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a\n  unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.\n\nMaithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Doso-\n   vitskiy. Do vision transformers see like convolutional neural networks? Advances in neural\n  information processing systems, 34:12116–12128, 2021.\n\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know\n  about how bert works. Transactions of the association for computational linguistics, 8:\n  842–866, 2021.\n\nTobias Schnabel and Jennifer Neville. Symbolic prompt program search: A structure-aware\n  approach to efficient compile-time prompt optimization. arXiv preprint arXiv:2404.02319,\n  2024.\n\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh.\n  Autoprompt: Eliciting knowledge from language models with automatically generated\n  prompts. arXiv preprint arXiv:2010.15980, 2020.\n\nGizem Soğancıoğlu, Hakime Öztürk, and Arzucan Özgür.  Biosses: a semantic sentence\n  similarity estimation system for the biomedical domain. Bioinformatics, 33(14):i49–i58,\n  2017.\n\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\n  Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-\n  bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261,\n  2022.\n\nMistral AI team.  Mistral nemo, 2024. URL https://mistral.ai/news/mistral-nemo.\n  Accessed: 2025.\n\n\n                                       13\n\nRethinking Prompt Optimization\n\n\n\n\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\n  Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\n  Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,\n  2023a.\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\n  Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\n  Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,\n  Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose\n  language understanding systems. Advances in neural information processing systems, 32,\n  2019.\n\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic,\n  Eric P Xing, and Zhiting Hu. Promptagent: Strategic planning with language models\n  enables expert-level prompt optimization. arXiv preprint arXiv:2310.16427, 2023a.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\n  Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in\n  language models. arXiv preprint arXiv:2203.11171, 2022.\n\nZhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim.\n  Multitask prompt tuning enables parameter-efficient transfer learning. arXiv preprint\n  arXiv:2303.02861, 2023b.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\n  Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\n  Advances in neural information processing systems, 35:24824–24837, 2022.\n\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom\n  Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt\n  tuning and discovery. Advances in Neural Information Processing Systems, 36:51008–51025,\n  2023.\n\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. Ties-\n  merging: Resolving interference when merging models. Advances in Neural Information\n  Processing Systems, 36:7093–7115, 2023.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and\n  Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409,\n  2023a.\n\nZachary Yang, Yasmine Maricar, MohammadReza Davari, Nicolas Grenon-Godbout, and\n  Reihaneh Rabbany. Toxbuster: In-game chat toxicity buster with bert. arXiv preprint\n  arXiv:2305.12542, 2023b.\n\nLe Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario:\n  Absorbing abilities from homologous models as a free lunch. In Forty-first International\n  Conference on Machine Learning, 2024.\n\nJian Zhang, Zhangqi Wang, Haiping Zhu, Jun Liu, Qika Lin, and Erik Cambria. Mars: A\n  multi-agent framework incorporating socratic guidance for automated prompt optimization.\n  arXiv preprint arXiv:2503.16874, 2025.\n\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tem-\n  pera: Test-time prompting via reinforcement learning. arXiv preprint arXiv:2211.11890,\n  2022.\n\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\n  Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.  Least-to-most prompting\n  enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625,\n  2022a.\n\n\n                                       14\n\nRethinking Prompt Optimization\n\n\n\n\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\n  and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint\n  arXiv:2211.01910, 2022b.\n\n\n\n\n\n                                       15\n\nRethinking Prompt Optimization\n\n\n\n\nA  Dataset\n\n\nTable 4 summarizes the dataset splits used in our experiments, including the number of\ntraining, validation, and test instances per task. Each dataset is designed to probe distinct\nreasoning capabilities of large language models (LLMs), and collectively they offer a diverse\nevaluation ground for prompt optimization.\n\n\nCausal Judgment  This dataset tests causal attribution skills by presenting real-world\nscenarios and asking whether one event caused another. It evaluates the model’s ability to\nperform commonsense reasoning under ambiguity. Below is an example instance from the\ndataset:\n\n\n  Joe was feeling quite dehydrated, so he stopped by the local smoothie shop to\n  buy the largest sized drink available.  Before ordering, the cashier told him\n  that the Mega-Sized Smoothies were now one dollar more than they used to be.\n  Joe replied, “I don’t care if I have to pay one dollar more, I just want the\n  biggest smoothie you have.”  Sure enough, Joe received the Mega-Sized Smoothie\n  and paid one dollar more for it.  Did Joe intentionally pay one dollar more?\n  Label:  Yes\n\nGeometric Shapes A synthetic visual-reasoning-inspired dataset that describes a geometric\nshape via its SVG representation. The task tests the model’s ability to infer spatial and\ncomparative relationships. An example is shown below:\n\n\n  This SVG path element <path d=¨M 59.43,52.76 L 75.49,27.45 L 54.92,4.40 M\n  54.92,4.40 L 23.70,7.77 L 15.15,42.15 L 34.51,57.44 L 59.43,52.76¨/> draws a\n  Label:  hexagon\n\n\nPenguins  This dataset examinsines the model’s ability to reason about tabular data. At\neach instance, the model is presented with a question about penguins in a table format, and\nit must select the correct answer from a set of choices.\n\n\n  Here is a table where the first line is a header and each subsequent line is a\n  penguin:\n\n  name, age, height (cm), weight (kg)\n  Louis, 7, 50, 11\n  Bernard, 5, 80, 13\n  Vincent, 9, 60, 11\n  Gwen, 8, 70, 15\n  For example:  the age of Louis is 7, the weight of Gwen is 15 kg, the height of\n  Bernard is 80 cm.\n  Question:  What is the height of Gwen?\n  Options:  A) 50 cm, B) 80 cm, C) 70 cm, D) 60 cm\n\n  Label:  C) 70 cm\n\n\nBiosses  This biomedical sentence similarity dataset presents pairs of scientific statements\nand asks for semantic similarity on a scale, assessing the model’s ability to reason about\nspecialized, domain-specific language. A sample pair is shown below:\n\n\n  S1:  It has recently been shown that Craf is essential for Kras G12D-induced\n  NSCLC.\n  S2:  It has recently become evident that Craf is essential for the onset of\n  Kras-driven non-small cell lung cancer.\n\n  Label:  Similar\n\n\n\n                                       16\n\nRethinking Prompt Optimization\n\n\n\n\n\n                        Dataset             Train   Validation   Test\n\n                       Causal Judgment    30       60      100\n                       Geometric Shapes    50       100      200\n                       Penguins            30       40       79\n                          Biosses             30       30       40\n                CB                 30       95       56\n\nTable 4: Dataset splits used for prompt optimization and evaluation. The training set is\nused during prompt updates, the validation set is used to select the best-performing prompt,\nand the test set is reserved for final accuracy reporting.\n\n\nCB (CommitmentBank) CB is a SuperGLUE (Wang et al., 2019) NLI dataset designed\nto evaluate pragmatic inference and speaker commitment in naturally occurring sentences.\nIt differs from standard NLI datasets because each hypothesis is derived directly from the\npremise’s embedded clause, minimizing annotation artifacts. Below is a representative\nexample:\n\n\n  Premise:  Some of them, like for instance the farm in Connecticut, are quite\n  small.  If I like a place I buy it.  I guess you could say it’s a hobby.”\n  Hypothesis:  Buying places is a hobby.\n\n  Label:  Entailment\n\nIn this case, the hypothesis is the complement of the clause-embedding verb say, and models\nmust correctly infer that the sentence author is committed to the embedded proposition.\nThis task hinges on understanding modality, clause embedding, and speaker stance, rather\nthan surface-level lexical overlap.\n\nThese datasets collectively test a broad spectrum of reasoning abilities, ranging from causal\ninference and visual abstraction to factual recall, biomedical semantics, and logical entailment,\nmaking them suitable benchmarks for evaluating the generality and robustness of prompt\noptimization methods.\n\nB  Task Prompts\n\nEach task begins with a minimalist base prompt that serves as the initialization point for\nprompt optimization. These prompts are written as system messages in the GPT-3.5-turbo\nand GPT-4o chat interfaces, and are intentionally kept simple to avoid embedding task-\nspecific strategies or heuristics. The goal is to provide just enough instruction for the model\nto attempt the task, allowing the optimization process to refine and expand the prompt\neffectively. Below, we list the base prompts used for each task.\n\n\n   Task: Causal Judgment\n  Answer questions about causal attribution\n\n\n\n   Task: Geometric Shapes\n  Name geometric shapes from their SVG paths\n\n\n\n   Task: Penguins\n  Answer questions about a table of penguins and their attributes\n\n\n\n   Task: Biosses\n  Decide if these two sentences are (A) Not similar (B) Somewhat similar (C)\n  Similar.\n\n\n\n                                       17\n\nRethinking Prompt Optimization\n\n\n\n\n\n  Task: CB\n  What is the relationship between the following premise and the hypothesis?\n  Options:\n  - Contradiction\n  - Neutral\n  - Entailment\n\nIn Section 4.2, we described how each base prompt is optimized using our reinforcement-based\napproach. Below, we include the resulting expert prompts obtained from the final iteration\nof the standard prompt optimization process. These reflect the outcome of the optimization\nprocess when targeting performance on the GPT-3.5 model.\n\n\n  Task: Causal Judgment\n  Provide causal attributions in complex scenarios by guiding the model to\n  thoroughly analyze the critical steps, individual intentions, and specific\n  actions that lead to outcomes.  Emphasize the importance of identifying and\n  prioritizing the primary cause in each scenario, focusing on direct causes\n  rather than incidental factors.  Define clear criteria for evaluating factors\n  and determining the primary cause, considering the combined impact of multiple\n  factors working in conjunction.  Instruct the model to weigh the influence of\n  various factors and explicitly guide it on handling conflicting actions and\n  scenarios involving multiple individuals.  Ensure that the model carefully\n  considers all significant actions, intentions, and sequences of events leading\n  to the final outcome to accurately attribute causation.  Provide explicit\n  instructions for distinguishing between direct causes and incidental factors,\n  prioritizing immediate actions that directly influence outcomes.  Define\n  specific criteria for evaluating factors and determining the primary cause,\n  especially in scenarios involving multiple individuals.  Emphasize the need to\n  analyze critical steps and actions leading to outcomes in order to accurately\n  attribute causation.\n\n\n\n  Task: Geometric Shapes\n  Name the geometric shape accurately based on the provided SVG path.  Carefully\n  analyze the properties of the path, including the number of sides, angles,\n  lengths of sides, and overall configuration, to determine the most appropriate\n  geometric shape.  Your options should encompass a wide variety of shapes,\n  ranging from simple polygons to circles.  Ensure that the model considers all\n  relevant attributes before selecting the most suitable shape from the available\n  options.\n  Options:  (A) circle, (B) equilateral triangle, (C) regular hexagon, (D)\n  rhombus, (E) line segment, (F) octagon, (G) pentagon, (H) rectangle, (I) sector,\n  (J) square, (K) trapezoid, (L) oval\n\n\n\n\n\n                                       18\n\nRethinking Prompt Optimization\n\n\n\n\n\n  Task: Penguins\n  Answer questions regarding the following tables of penguins and giraffes,\n  ensuring to accurately reflect any changes made to the penguin table throughout\n  our discussion.  Please note these modifications specifically when determining\n  key attributes such as age, weight, or when making comparisons between penguins\n  and giraffes.\n\n  Penguin Table:\n  name, age, height (cm), weight (kg)\n  Louis, 7, 50, 11\n  Vincent, 9, 60, 11\n  Gwen, 8, 70, 15\n  (Any additions or deletions of penguins will be noted in subsequent questions)\n\n  Giraffe Table:\n  name, age, height (cm), weight (kg)\n  Jody, 5, 430, 620\n  Gladys, 10, 420, 590\n  Marian, 2, 310, 410\n  Donna, 9, 440, 650\n\n  For each question, provide clear and logical reasoning behind your answer.\n  Remember to validate the latest state of the penguin table before responding,\n  especially when involving comparisons with giraffes or assessing the attributes\n  of the penguins.\n\n  Additionally, if modifications were made to the penguin table, please annotate\n  them clearly in your response.  This ensures that we maintain an accurate\n  understanding of the current data.\n\n\n\n  Task: Biosses\n  Decide if these two sentences are (A) Not similar (B) Somewhat similar\n  (C) Similar.  Compare the specific regulatory mechanisms and molecular\n  pathways mentioned in each sentence to determine their similarity, explicitly\n  identifying the role of miRNA expression and binding, as well as the relevance\n  of the molecular characteristics of GEFs and nucleotide-binding pockets in\n  the context of the sentences.  Analyze both the similarities and differences\n  between the sentences, focusing on the nuances of the regulatory mechanisms and\n  molecular pathways mentioned, and considering the implications for cancer types\n  and cellular processes\n\n\n\n  Task: CB\n  What is the relationship between the following premise and the hypothesis?\n  Premise:  As the storm raged outside, with thunder clapping and lightning\n  illuminating the dark sky, Sarah felt a wave of panic wash over her.  She\n  could hear the wind howling, and every crash of thunder made her heart race\n  faster.  Despite being tucked away under her thick blankets, she couldn’t shake\n  the feeling of terror that gripped her.  The flickering candle nearby offered\n  little comfort as she lay wide awake, listening to the chaos around her.\n  Hypothesis:  Sarah felt a strong fear of the storm.\n  Entailment:  The hypothesis is entailed by the premise.  Sarah’s panic and\n  terror at the storm directly imply that she felt a strong fear of it.  What is\n  the relationship between the following premise and the hypothesis?\n  Options:\n  - Contradiction\n  - Neutral\n  - Entailment\n\n\n\n\n\n                                       19\n\nRethinking Prompt Optimization\n\n\n\n\n\n                       Causal       Geometric\n Model   Method      Judgment    Shapes        Penguins     Biosses     CB\n\n          Baseline      7670.4       11263.8       2663.6       3566.1       5489.5\n         Baseline+FD 7918.8(↑3.2%) 11730.9(↑4.1%) 2735.4(↑2.7%) 3709.7(↑4.0%) 5606.2(↑2.1%)\n GPT-3.5\n         Baseline+PR 7039.8(↓8.2%) 10899.7(↓3.2%) 2533.3(↓4.9%) 3294.0(↓7.6%) 5337.2(↓2.8%)\n       BReAD       7429.2(↓3.1%) 11204.0(↓0.5%) 2622.4(↓1.5%) 3449.1(↓3.3%) 5421.9(↓1.2%)\n\n          Baseline      8575.8       9642.0         2980.4       3008.1       6228.9\n         Baseline+FD 9271.0(↑8.1%) 10093.2(↑4.7%) 3068.3(↑2.9%) 3069.6(↑2.0%) 6393.6(↑2.6%)\n GPT-4o\n         Baseline+PR 7963.2(↓7.1%) 9395.6(↓2.6%)  2908.9(↓2.4%) 2868.8(↓4.6%) 6079.1(↓2.4%)\n       BReAD       8040.8(↓6.2%) 9049.4(↓6.1%)  2825.2(↓5.2%) 2860.1(↓4.9%) 5964.5(↓4.2%)\n\nTable 5: Average number of API calls required for prompt optimization and migration (lower\nis better), averaged over five runs. Percent changes are shown relative to the PromptAgent\nbaseline: green indicates a reduction in calls, red indicates an increase. +FD = Feedback\nDiversification; +PR = Positive Reinforcement; BReAD = both combined. Despite leveraging\nmore feedback signals, BReAD achieves faster convergence, reducing API calls by 0.5–3.3%\nin the standard setting (GPT-3.5) and by 4.2–6.2% in the migration setting (GPT-4o),\ndemonstrating efficiency gains in optimization and transfer.\n\n\nC  Experimentation Costs\n\nTable 5 reports the average number of API calls required by each method during prompt\noptimization and migration, for both GPT-3.5 and GPT-4o. Each value reflects the mean\nover five runs per task.\n\nThese figures serve as a proxy for real-world deployment costs, especially when interacting\nwith commercial LLM APIs. While Feedback Diversification (+FD) increases the number\nof calls due to repeated querying, both Positive Reinforcement (+PR) and our full method,\nBReAD, achieve more efficient convergence. Notably, BReAD consistently reduces total API\ncalls despite using more feedback samples per iteration, underscoring its sample efficiency\nand effective guidance.\n\nEfficiency gains are more pronounced in the migration setting, where BReAD lowers API\nusage by 4.2–6.2% compared to the PromptAgent baseline. This cost reduction, combined\nwith improved accuracy and stability, positions BReAD as a practical optimization strategy\nfor scalable LLM-based systems.\n\n\n\n\n\n                                       20",
"headers": [
"arXiv:2507.09839v1  [cs.LG]  14 Jul 2025",
"Rethinking Prompt Optimization: Reinforcement, Diversifica-",
"tion, and Migration in Blackbox LLMs",
"Abstract",
"1",
"Introduction",
"2",
"Related Work",
"3",
"Methodology",
"4",
"Experiments",
"5",
"Conclusion and Future Work",
"6",
"Limitations",
"References",
"A",
"Dataset",
"B",
"Task Prompts",
"C",
"Experimentation Costs"
],
"tables": [
"|20 (%)<br>Acc.<br>15<br>in<br>10 Improvement<br>5<br>0<br>alized<br>5|Causal Judgment Geom|etric Shapes Penguins Bio|sses CB|\n|---|---|---|---|\n|5<br>0<br>5<br>10<br>15<br>20<br>alized Improvement in Acc. (%)|pg|||\n|5<br>0<br>5<br>10<br>15<br>20<br>alized Improvement in Acc. (%)||||",
"|Causal Judgment Geome<br>(%)<br>15<br>Acc.<br>10<br>in<br>5 Improvement<br>0<br>5<br>10 Baselin malized|Col2|tric Shapes Penguins|Biosses CB|Col5|\n|---|---|---|---|---|\n|10<br>5<br>0<br>5<br>10<br>15<br>malized Improvement in Acc. (%)<br>Baselin<br>CausalJudgment<br>Geome|||||\n|10<br>5<br>0<br>5<br>10<br>15<br>malized Improvement in Acc. (%)<br>Baselin<br>CausalJudgment<br>Geome|Baselin|e: PromptAgent|||"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2507.09839v1.pdf"
}