{
"text": "CAPO: Cost-Aware Prompt Optimization\n\n             Tom Zehle1,âˆ—Moritz Schlager1,âˆ—Timo HeiÃŸ1,âˆ—Matthias Feurer1,2\n                 1Department of Statistics, LMU Munich, Munich, Germany\n                2Munich Center for Machine Learning (MCML)\n                  âˆ—Equal contribution.\n\n\n                  Abstract Large language models (LLMs) have revolutionized natural language processing by solving a\n                           wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to\n                           prompt formulation. While automatic prompt optimization addresses this challenge by find-\n                               ing optimal prompts, current methods require a substantial number of LLM calls and input\n                                tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt2025\n                               Optimization), an algorithm that enhances prompt optimization efficiency by integrating\n                        AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporat-Jun                         ing racing to save evaluations and multi-objective optimization to balance performance with\n                           prompt length. It jointly optimizes instructions and few-shot examples while leveraging task\n17                           descriptions for improved robustness. Our extensive experiments across diverse datasets and\n                       LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\n                           methods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm achieves\n                                 better performances already with smaller budgets, saves evaluations through racing, and\n                              decreases average prompt length via a length penalty, making it both cost-efficient and\n                                cost-aware. Even without few-shot examples, CAPO outperforms its competitors and gener-[cs.CL]                             ally remains robust to initial prompts. CAPO represents an important step toward making\n                           prompt optimization more powerful and accessible by improving cost-efficiency.\n\n\n\n           1 Introduction\n              The increasing capabilities of transformer-based large\n                language models (LLMs) (Vaswani et al., 2017; Brown\n                   et al., 2020) have led to a paradigm shift in Natural Lan-\n               guage Processing (NLP): instead of pre-training and\n                expensively fine-tuning models for each individual\n               downstream task, a single LLM, pre-trained in an en-\n                    tirely unsupervised manner, can now solve a diverse\n               range of tasks, simply steered by a textual prompt\n                without requiring any additional training (Liu et al.,arXiv:2504.16005v4\n                   2023). These models demonstrate strong performance\n              on many NLP tasks, often nearly reaching perfor-\n              mances of state-of-the-art fine-tuned models (Brown\n                   et al., 2020). In this context, a prompt refers to instruc-  Figure 1: CAPO yields superior mean population\n                  tions provided to the LLM as input to guide its output   test scores on Subj with Qwen2.5-32B.\n               toward solving a specific task (Karmaker Santu and\n                 Feng, 2023; White et al., 2025). It may additionally include in-context examples (â€œshotsâ€) of the task,\n                 acting as demonstrations (Brown et al., 2020). However, LLM performance is highly sensitive to\n               prompt quality, format, as well as choice and order of few-shot examples (Zhao et al., 2021; Lu et al.,\n                 2022; Zhou et al., 2023). It has been demonstrated that semantically similar prompts can perform\n                 quite differently (Yang et al., 2024), which we illustrate in Table 1 with two semantically similar\n               prompts differing by 10%p in accuracy after optimization.\n\n\n\n              AutoML 2025                            Â© 2025 the authors, released under CC BY 4.0\n\nTable 1: Best performing prompts from our benchmark experiments on GSM8K with Llama-3.3-70B.\n\n Before optimization (43.8%): Please analyze this elementary school math problem that requires multiple logical steps.\n After explaining your reasoning, provide the ultimate solution between <final_answer> </final_answer> tags.\n\n After optimization with EvoPromptGA (53.8%): Assist with solving the elementary or grade school level math\n problem that requires multiple steps and provide the solution within <final_answer> </final_answer> tags for easy\n identification.\n\n After optimization with CAPO (ours, 79.2%): To tackle this math word problem, which demands a series of logical\n steps, dissect it methodically. Outline your thought process and ensure you clearly signify your solution, enclosing it\n within <final_answer> </final_answer> markers for easy identification. + 2 few shots\n\n\n    This phenomenon introduces the need for prompt engineering or optimization, i.e., designing\nprompts to enable an LLM to optimally solve a task (Liu et al., 2023; MeskÃ³, 2023). Manual prompt\nengineering requires time and expertise (Liu et al., 2023). Therefore, automatic prompt optimization\nhas gained increasing attention, including both continuous approaches optimizing learnable â€œsoft\npromptsâ€ (Lester et al., 2021; Li and Liang, 2021; Qin and Eisner, 2021) and discrete methods acting\ndirectly on textual prompts (Zhou et al., 2023; Agarwal et al., 2024; Yang et al., 2024). The discrete\nprompt optimization framework EvoPrompt (Guo et al., 2024), which leverages LLMs as operators in\nan evolutionary algorithm, achieves strong performance across various tasks. However, EvoPrompt\nrelies on good, task-specific initial prompts. Other approaches incorporate human-designed task\ndescriptions to mitigate this reliance (Yang et al., 2024). Moreover, recent advances in prompt\noptimization also integrate few-shot example selection (Agarwal et al., 2024; Wu et al., 2024).\n    Nonetheless, many prompt optimization methods remain relatively expensive in terms of\nthe number of LLM calls (Agarwal et al., 2024). For instance, optimizing with EvoPrompt in its\noriginal parametrization requires 4-6 million input tokens per task until convergence (Guo et al.,\n2024). Given the API costs for commercial LLMs, this can quickly become expensive: at current\ncommercial API rates, this translates to approximately $34 for GPT-4.1 and $300 for Claude Opus 41\nfor optimization, not even accounting for costs of productively using the optimized prompt.\n    In this paper, we address the cost problem in prompt optimization by introducing CAPO\n(Cost-Aware Prompt Optimization), a novel discrete prompt optimization algorithm that integrates\nAutoML techniques for enhanced cost-efficiency. CAPO draws its underlying mechanism on Evo-\nPrompt (Guo et al., 2024) and combines it with racing (Birattari et al., 2002) to reduce the number of\nevaluations and improve cost-efficiency. It employs multi-objective optimization by incorporating\nprompt length as an additional objective through a penalty. In addition, our algorithm integrates re-\ncent advances in prompt optimization by combining instruction and few-shot example optimization\nas well as leveraging task descriptions for improved robustness. Our main contributions are:\n1. We introduce CAPO, a cost-efficient and -aware prompt optimization algorithm that integrates\n   racing and multi-objective optimization, leveraging few-shot examples and task descriptions.\n2. We conduct extensive benchmark experiments comparing CAPO against three state-of-the-art\n  prompt optimization algorithms across diverse datasets and LLMs, demonstrating its superior\n   performance in most scenarios, even with substantially fewer input tokens (see, e.g., Figure 1).\n3. We provide comprehensive ablation studies indicating that few-shot example selection greatly\n   enhances performance, racing improves cost-efficiency, the prompt length objective reduces\n   average prompt length, and task descriptions make the algorithm robust to initial prompt quality.\nWe make our complete implementation publicly available under the Apache 2.0 license at\nhttps://github.com/finitearth/capo/ to facilitate reproducibility and adoption.\n\n    1Depending on the task, 2-3 million output tokens are additionally required. Considering input token costs of\n$2 / 1M tokens for GPT-4.1 ($15 / 1M tokens for Claude Opus 4) and output token costs of $8 / 1M tokens ($75 /\n1M tokens), we arrive at a total of roughly $34 ($300). For API prices, see https://openai.com/api/pricing/ and\nhttps://www.anthropic.com/pricing (accessed: 2025-03-22).\n\n\n                                                                                            2\n\n2 Notation & Problem Statement\n  Let I denote the space of all possible instructions ğ‘–and E the space of all possible examples\n   ğ‘’, also referred to as â€œshotsâ€. A tuple of few-shot examples consisting of ğ‘˜shots is denoted by\n  ğ’†= (ğ‘’1, . . . ,ğ‘’ğ‘˜), the space of all possible ğ‘˜-shot examples is represented by Eğ‘˜. We define the space\n   of possible prompts with up to ğ‘˜max shots as P = I Ã— Ãğ‘˜max Eğ‘˜, where each prompt ğ‘=  (ğ‘–, ğ’†)                                                              ğ‘˜=0\n   consists of an instruction and between 0 and ğ‘˜max shots. Let an LLM be a function Î¦ that takes\n  a prompt ğ‘and some input, and produces an output. In the classical case, the input refers to an\n   instance ğ‘¥âˆˆX from a dataset D = {(ğ‘¥(ğ‘–),ğ‘¦(ğ‘–))}ğ‘›ğ‘–=1 âˆ¼Pğ‘¥ğ‘¦and the output to a corresponding\n   predicted label Ë†ğ‘¦âˆˆY. We also use LLMs for generating variations of instructions, where input and\n  output both refer to instructions ğ‘–. We refer to this as meta-LLM in contrast to the evaluation-LLM\n   for which we optimize the prompt. LLMs are treated as black boxes without access to gradients or\n  token probabilities, a common scenario for API LLMs from closed-source vendors.\n    We evaluate a prompt ğ‘by comparing the true label ğ‘¦to the predicted label Ë†ğ‘¦= Î¦(ğ‘,ğ‘¥) for a\n  given instance ğ‘¥with a point-wise scoring function ğœ: Y Ã— Y â†’R. While any scoring function is\n   generally possible, we always test for direct match using\n\n                                   ( 1   if ğ‘¦= Ë†ğ‘¦\n                                     ğœ(ğ‘¦, Ë†ğ‘¦) =                                                         (1)\n                                            0  otherwise\n\n   as scoring function. Our goal is to find a prompt ğ‘that maximizes this score in expectation:\n\n                                arg max E(ğ‘¥,ğ‘¦)âˆ¼Pğ‘¥ğ‘¦[ğœ(ğ‘¦, Î¦(ğ‘,ğ‘¥))].                                  (2)\n                              ğ‘âˆˆP\n\n  Estimating this quantity based on a finite dataset D = {(ğ‘¥(ğ‘–),ğ‘¦(ğ‘–))}ğ‘›ğ‘–=1 yields our objective ğ‘“:\n   ğ‘“(ğ‘; D) = ğ‘›1 Ãğ‘›ğ‘–=1 ğœ(ğ‘¦ğ‘–, Î¦(ğ‘,ğ‘¥ğ‘–)). Our goal is to find a prompt ğ‘that maximizes ğ‘“within a limited\n  budget of input tokens to an LLM. Since we want to generalize well to unseen data, we measure ğ‘“\n  on a separate, finite test dataset Dğ‘¡ğ‘’ğ‘ ğ‘¡= {(ğ‘¥(ğ‘–),ğ‘¦(ğ‘–))}ğ‘›+ğ‘šğ‘–=ğ‘›+1 drawn from the same distribution.\n\n3 Related Work\n  Automatic Prompt Optimization. Recently, interest in automating prompt optimization has grown\n   as manual prompt engineering requires time and expertise without guaranteeing optimality (Jiang\n   et al., 2020; Liu et al., 2023). A related area is prompt selection, which aims to find optimal prompts\n  from a pre-defined pool of candidates (Sorensen et al., 2022; Do et al., 2024; Schneider et al., 2024; Shi\n   et al., 2024). Prompt optimization includes both the optimization of instructions and the selection\n   of relevant few-shot examples (â€œexemplar optimizationâ€) (Wan et al., 2024; Wu et al., 2024).\n      Continuous prompt optimization improves prompts in continuous space to obtain learnable â€œsoft\n  promptsâ€ (Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021). While this requires access\n   to LLM parameters and makes prompts not interpretable (Lester et al., 2021), recent approaches\n   like InstructZero (Chen et al., 2024) and its extension INSTINCT (Lin et al., 2024) address this by\n  performing Bayesian optimization on soft prompts used to generate human-readable instructions.\n      Discrete methods directly optimize textual prompts (Agarwal et al., 2024). Unlike earlier ap-\n  proaches that require access to gradients or token probabilities (Shin et al., 2020; Deng et al., 2022;\n  Shi et al., 2023), recent discrete methods also work with black box LLMs. They typically use\n  a â€œmeta-LLMâ€ instructed by a â€œmeta-promptâ€ to alternate prompt candidates: APE (Zhou et al.,\n  2023) uses a meta-LLM to generate instructions from demonstrations and iteratively proposes\n  semantically similar variants, ProTeGi (Pryzant et al., 2023) leverages mispredicted instances as\n  â€œpseudo-gradientsâ€, and PromptBreeder (Fernando et al., 2024) uses an evolutionary strategy with a\n  meta-LLM guided by self-improving mutation-prompts. EvoPrompt (Guo et al., 2024), which serves\n   as foundation of our work, is also based on evolutionary algorithms and has two instantiations: a\n\n\n\n                                                                                               3\n\ngenetic algorithm (GA) and differential evolution (DE). Both implement evolutionary operations\nby a meta-LLM. Despite outperforming previous discrete methods, EvoPrompt has two major\ndrawbacks: it requires many LLM calls (Agarwal et al., 2024) and its performance depends on a\ngood, task-specific initial prompt population (Yang et al., 2024). OPRO (Yang et al., 2024) directly\nemploys LLMs as optimizers by leveraging task descriptions, task examples, and previous candidates\nwith scores in the meta-prompt, maintaining good performance even with task-unspecific initial\nprompts. These methods optimize instructions without incorporating few-shot examples in prompt\ncandidates. However, even simple random example selection can outperform sophisticated instruc-\ntion optimizers. Combining instruction and example optimization is found to create synergies (Wan\net al., 2024). PromptWizard (Agarwal et al., 2024) optimizes instructions and examples simulta-\nneously using a critique-synthesis mechanism, reportedly outperforming previously described\nmethods while greatly reducing LLM calls. However, approaches like PromptWizard, ProTeGi,\nor OPRO require a notion of what constitutes a â€œgoodâ€ prompt, asking a meta-LLM to identify\nproblems or improve prompts. Since prompt performance does not necessarily follow predictable\npatterns (Yang et al., 2024), this potentially limits these methodsâ€™ ability to capture such subtleties.\n   While the methods described above focus on refining single prompts, there are frameworks that\ntreat black-box prompt optimization as a component of larger dynamic systems. TextGrad (Yuk-\nsekgonul et al., 2025) introduces a pipeline-based optimization approach that uses LLM-generated\nfeedback as textual gradients to improve prompts. DSPy (Khattab et al., 2024) offers a modular\nprogramming framework for building LLM pipelines with an embedded optimization component.\nAviary (Narayanan et al., 2024) reframes prompt optimization within the broader context of language\nagents solving complex tasks through a partially observable multi-step decision process.\n\nAutoML for Efficiency. The field of AutoML offers several techniques to enhance optimization\nefficiency and methods like multi-fidelity optimization (Jamieson and Talwalkar, 2016; Li et al., 2018;\nFalkner et al., 2018; Awad et al., 2021) have also been successfully adopted outside AutoML, e.g., for\nprompt selection, where efficiency is similarly important (Schneider et al., 2024; Shi et al., 2024).\nRacing algorithms are applicable when objectives are decomposable into cheaper sub-objectives that\ncan be evaluated individually. They sequentially evaluate candidates and eliminate poor ones once\nsufficient statistical evidence accumulates, preserving budget for promising candidates (Birattari\net al., 2002, 2010). Important works include Hoeffding Races (Maron and Moore, 1994) using\nHoeffdingâ€™s bound for elimination, BRACE (Moore and Lee, 1994) employing Bayesian statistics,\nF-Race (Birattari et al., 2002) using Friedmanâ€™s test (Conover, 1999), and I/F-Race (Balaprakash\net al., 2007) iteratively applying F-Race while biasing a probabilistic model of the candidates to\npromising areas. The irace package (LÃ³pez-IbÃ¡Ã±ez et al., 2016) provides a general iterated racing\nimplementation, including a paired t-test as alternative. Related methods that save evaluations by\nadaptively increasing evaluations include FocusedILS (Hutter et al., 2009), as well as ROAR and\nSMAC (Hutter et al., 2011), employing an â€œintensificationâ€ mechanism without statistical testing.\n    Multi-objective optimization (MOO) addresses scenarios with multiple competing objectives\nsuch as performance versus efficiency (Karl et al., 2023). A priori methods transform multiple\nobjectives into a single one, e.g., via scalarization, yielding only a single solution candidate (Karl et al.,\n2023). While greatly simplifying optimization (Miettinen, 1998), choosing scalarization weights\na-priori is often non-trivial (Jin and Sendhoff, 2008). A posteriori methods produce a set of Pareto-\noptimal solutions (Karl et al., 2023). Notable approaches include evolutionary methods like NSGA-II\n(Deb et al., 2002) and SMS-EMOA (Beume et al., 2007) based on non-dominated sorting rank, and\nBayesian optimization approaches such as ParEGO (Knowles, 2006), approximating the Pareto-front\nusing a set of randomly generated scalarization weights. Combinations of MOO and racing include\nirace with Hypervolume (LÃ³pez-IbÃ¡Ã±ez et al., 2016), S-Race and its extensions (Zhang et al., 2013,\n2015a; Miranda et al., 2015), MO-ParamILS (Blot et al., 2016), and MO-SMAC (Rook et al., 2025).\n    For additional background on the discussed approaches, we refer to Appendix A.\n\n\n\n                                                                                            4\n\n4 CAPO: Cost-Aware Prompt Optimization\n  We introduce CAPO (Cost-Aware Prompt Optimization), a discrete prompt optimization algorithm\n   that addresses the cost problem in automatic prompt optimization and integrates recent prompt\n   optimization advances. Conceptually, CAPO builds on EvoPromptGA (Guo et al., 2024), following\n  a standard genetic algorithm (Goldberg, 1989) with a meta-LLM for cross-over and mutation\n   operations. As the number of evaluations is a major cost factor in prompt optimization, CAPO\n  employs racing to eliminate underperforming candidates early. In addition, CAPO draws inspiration\n  from multi-objective optimization, incorporating efficiency as additional objective by penalizing\n  prompt length. Keeping the length of the resulting prompt minimal reduces evaluation cost during\n  optimization and deployment cost of the final prompt. Similar to PromptWizard (Agarwal et al.,\n   2024), CAPO optimizes both instructions and few-shot examples simultaneously. Furthermore,\n  CAPO leverages task descriptions in the meta-prompt to reduce reliance on task-specific initial\n  prompts (Yang et al., 2024). We additionally simplify the meta-prompt templates by substantially\n  shortening them and avoiding formulations like â€œbetter promptâ€ that require a notion of what\n   constitutes a good prompt. We now describe the CAPO algorithm as outlined in Algorithm 1.\n      Population Initialization: A set of initial instructions I0 of population size ğœ‡is provided as\n   input, either manually engineered or automatically generated with approaches like APE (Zhou\n   et al., 2023). We first augment each instruction with a random number of few-shot examples\n  between 0 and ğ‘˜max. For each example, we generate reasoning to provide richer information\n  compared to solely a label as example output. We prompt the evaluation-LLM with the initial\n   instruction to solve the example input, which typically yields a response with both reasoning\n  and prediction. If the LLM fails to generate a correct prediction, we use the true label as example\n   output.2 This resembles PromptWizard (Agarwal et al., 2024), which leverages reasoning chains.\n  This initialization procedure yields a diverse population with varying number and lengths of shots.\n      Cross-over & Mutation:  For cross-over, CAPO randomly selects parents, unlike Evo-\n  PromptGA (Guo et al., 2024), which uses score-based roulette wheel selection. While less ex-\n   ploitative, our choice eliminates expensive evaluations during parent selection. The cross_over\n  operation (cf. Appendix B) leverages a meta-LLM Î¦meta to create an offspring instruction ğ‘–off from\n\n  Algorithm 1 CAPO: Cost-Aware Prompt Optimization\n   Require: datasets Ddev and Dshots, meta-LLM Î¦meta, evaluation-LLM Î¦eval, initial instructions I0 =\n         {ğ‘–1, . . . ,ğ‘–ğœ‡}, population size ğœ‡, block size ğ‘, number of iterations ğ‘‡, number of crossovers per itera-\n       tion ğ‘, max. number of few-shot examples ğ‘˜max, max. number of evaluated blocks ğ‘§max, confidence level\n       ğ›¼, token length penalty control parameter ğ›¾, cross-over-meta-prompt ğ‘ğ¶, mutation-meta-prompt ğ‘ğ‘€\n      1: Divide dataset Ddev into blocks B = {ğµ1, ..., ğµğ‘§} where |ğµğ‘–| = ğ‘\n      2: Pğœ‡â†[]\n      3: for ğ‘–âˆˆI0 do                                                               âŠ²Initialize prompt population\n      4:    ğ‘˜âˆ¼Unif({0, . . . ,ğ‘˜max})                                   âŠ²Sample number of few-shots\n      5:    ğ’†â†create_shots(Dshots,ğ‘˜,ğ‘–, Î¦eval)                                     âŠ²Create few-shots\n      6:   ğ‘â†(ğ‘–, ğ’†)\n      7:   Pğœ‡â†append(ğ‘, Pğœ‡)\n      8: end for\n      9: for ğ‘¡= 1 to ğ‘‡do\n    10:     Poff â†cross_over(Pğœ‡, Î¦meta, ğ‘ğ¶,ğ‘)                         âŠ²Perform cross-over operation\n    11:     Poff â†mutate(Poff, Î¦meta, Î¦eval, ğ‘ğ‘€, Dshots,ğ‘˜max)             âŠ²Mutation operation on offspring\n    12:    Pğœ‡â†do_racing(Pğœ‡âˆªPoff, B, Î¦eval, ğ›¼,ğ›¾, ğœ‡,ğ‘§max)                  âŠ²Survival selection via racing\n    13: end for\n    14: return Pğœ‡\n\n\n       2For illustration purposes, we provide exemplary few-shot examples with and without reasoning in Appendix E.2.\n\n\n\n                                                                                               5\n\nthe two selected parentsâ€™ instructions. The meta-LLM is steered by a meta-cross-over prompt ğ‘ğ¶,\n  which is simplified compared to the EvoPromptGA meta-prompt (Guo et al., 2024) and incorporates\n  a task description.3 For the offspringâ€™s few-shot examples ğ’†off, we sample from the union of the\n   parentsâ€™ examples, with the number of examples corresponding to the average of the number of\n  few-shot examples of the two parents. This process is repeated ğ‘times per iteration to generate\n  ğ‘offspring. To each offspring, we then apply the mutate operation (cf. Appendix B). Similar to\n   cross-over, a meta-LLM Î¦meta is instructed via a simplified meta-mutation-prompt ğ‘ğ‘€with task\n   description to create a mutated version of the offspring instruction.3 To mutate few-shot examples,\n  we apply one of three operations with equal probability: adding a new shot if not exceeding ğ‘˜max,\n  removing a random shot if there are any, or keeping them unchanged. Following this step, we\n  randomly shuffle the order of all examples. This approach of modifying only single examples and\n   their order is designed to foster local exploration of the few-shot example choice and their quantity.\n      Survival Selection: To select survivors, we eliminate prompts through racing (do_racing, cf.\n  Appendix B), discarding underperforming prompts early when statistical evidence indicates they\n  perform significantly worse. Our racing procedure operates on blocks of samples B = {ğµ1, ..., ğµğ‘§}\n   of fixed size ğ‘, similar to F-Race (Birattari et al., 2002). We optionally shuffle block order in each\n   iteration to avoid potential elimination biases. We sequentially process blocks, evaluate all prompts\n  on the selected block (caching block scores to save evaluations later), and eliminate inferior prompts\n  when more than ğœ‡other prompts are significantly better according to a statistical test. We do not\n   correct for multiple testing as this can negatively affect racing behavior by making the test more\n   conservative, leading to fewer early eliminations of candidates (Birattari, 2009). This corresponds\n   to a population-based racing approach since we compare across the entire population rather than\n   against a single incumbent.4 Racing continues with additional blocks until we either reach ğœ‡\n   survivors or the maximum block evaluation limit ğ‘§max. If more than ğœ‡prompts survive after ğ‘§max\n  evaluated blocks, we select the ğœ‡best-performing prompts based on their average scores.\n     As statistical test, we employ a paired t-test with ğ›¼= 0.2, which is favorable for our case\n  compared to the commonly used F-test as scores across instances are commensurable (LÃ³pez-IbÃ¡Ã±ez\n   et al., 2016) while less conservative than non-parametric bounds like Hoeffdingâ€™s (Maron and Moore,\n   1994). Since the paired t-test requires normality or sufficiently large sample sizes (â‰¥30) (Hsu and\n  Lachenbruch, 2014), block size ğ‘must be chosen such that assumptions hold even for a single block.\n      Since we aim to maximize performance while keeping prompt length minimal, i.e., shorter\n   instructions, fewer examples, and reasoning only when necessary, we implement a form of multi-\n   objective optimization. This is particularly important given our inclusion of few-shot examples,\n  which can considerably increase the prompt length. To keep the racing procedure simple, we\n   scalarize our objective using a length penalty parameter ğ›¾that controls the trade-off between\n  prompt performance and any measure of relative token length. This parameter must be selected a-\n   priori, yielding the objective ğ‘“ğ›¾(ğ‘; ğµ) = ğ‘“(ğ‘; ğµ)âˆ’ğ›¾Â·rel_token_length(ğ‘). In our implementation,\n  rel_token_length represents token count5 normalized by the longest initial prompt.\n\n5 Experimental Setup\n  For our experiments, we use three different LLMs: Llama-3.3-70B-Instruct-GPTQ (Meta, 2024),\n  Qwen2.5-32B-Instruct-GPTQ (Qwen: Yang et al., 2025) and Mistral-Small-24B-GPTQ (Mistral AI\n  Team, 2025). These cover different model sizes from different companies and regions. We opt for\n  model sizes that still fit on a single GPU while exhibiting strong performances. To meet hardware\n   constraints, we employ GPTQ-quantized models (Frantar et al., 2023), which show negligible\n\n\n      3Prompt templates are provided in Appendix D.3. We illustrate instruction variation with examples in Appendix E.1.\n       4This makes the erroneous elimination of the best candidate very unlikely, as not only one but several type I errors\n   would have to occur.\n     5We give details on how we count tokens in this paper in Appendix C.4.\n\n\n\n                                                                                               6\n\nperformance loss compared to uncompressed models. For each setup, we use the same model as\n    meta- and evaluation-LLM. For further technical details, we refer to Appendix C.\n     We employ five datasets spanning a diverse range of typical NLP tasks with different subject\n    areas, targets, and complexity levels: SST-5 (sentiment classification; Socher et al., 2013), AG\n   News (topic classification; Zhang et al., 2015b), Subj (subjectivity classification; Pang and Lee, 2004),\n   GSM8K (grade school math word problems; Cobbe et al., 2021) and (Balanced) COPA (commonsense\n    causal reasoning; Kavumba et al., 2019). The first three datasets are used in the EvoPrompt\n    paper (Guo et al., 2024), GSM8K in OPRO (Yang et al., 2024) and PromptWizard (Agarwal et al.,\n    2024), and COPA is added as, to the best of our knowledge, a novel application for discrete prompt\n    optimization. For each dataset, we use 200 samples as few-shot dataset, 300 as development set\n    for optimization (larger than EvoPrompt (Guo et al., 2024), where 200 samples are used for these\n     tasks), and 500 holdout samples as test set (equivalent to the size of the smallest test set from\n    our five datasets; details in Appendix C.2). We automatically create a diverse pool of 15 initial\n    instructions per dataset with Anthropicâ€™s Claude Sonnet 3.7 (cf. Appendix D.2), and sample the\n     initial instructions from this pool for all optimizers and models. CAPO and OPRO (Yang et al., 2024)\n    additionally use task descriptions, which we manually craft (cf. Appendix D.1).\n     We benchmark CAPO against three state-of-the-art discrete prompt optimizers:  Evo-\n   PromptGA (Guo et al., 2024), OPRO (Yang et al., 2024), and PromptWizard (Agarwal et al., 2024).\n  We use the GA instantiation of EvoPrompt as it performs similar to the DE variant while being\n    conceptually simpler and closer to CAPO. For EvoPromptGA and OPRO, we use reimplementations\n    of a public library while using PromptWizardâ€™s original implementation with small adaptions. For\n    implementation and parametrization details of the optimizers, we refer to Appendix C.4.\n       For all experiments with CAPO, EvoPromptGA, and OPRO, we do not restrict maximum\n    iterations but instead use a budget of 5M input tokens after which the run terminates.6 We choose\n    this budget such that EvoPromptGA, which is most expensive in terms of LLM calls, has likely\n    converged (cf. Guo et al., 2024). We evaluate each optimizer with each LLM and dataset, performing\n    three repetitions with different random seeds per setup to quantify variance.\n\n 6 Results & Analysis\n\n6.1 Benchmark Results\n   We report the test scores of our benchmark experiments in Table 2. The results demonstrate that\n   CAPO outperforms the other prompt optimization methods on most datasets and models (11/15).\n    Notably, for Llama-3.3-70B, CAPO leads to the best results on every single dataset. For scenarios in\n   which another optimizer is better, CAPO is still competitive and within one standard deviation.\n   While performance gains of CAPO compared to the rest are small on SST-5 or AG News, we observe\n    substantial performance improvements on Subj and GSM8K, with up to 21%p improvement over\n    the rest (Llama-3.3-70B on GSM8K). Initial instructions are consistently improved by CAPO.\n      To assess the performance at intermediate token budgets, we depict the mean population\n    performance over input tokens for two representative examples of optimizer-dataset pairs in\n    Figures 1 & 2 (see Appendix H.2 for the remaining optimization curves). For both examples, as soon\n    as CAPO yields the first prompt, it consistently dominates the other optimizers over the entire\n    token range. Early performances of CAPO already exceed the other optimizersâ€™ final performances\n     after the full budget, underscoring its cost-efficiency. However, we observe that CAPO often yields\n     its first prompt later in terms of used input tokens than its competitors. This is due to the fact that\n   CAPO includes few-shot examples, making evaluations more costly. It follows that CAPO requires\n   many tokens in the first step while being very cost-efficient later (see Appendix I.2 for details).\n\n\n        6PromptWizard has no clear way to increase compute time, we report its performance on reduced budget (for details,\n     see Appendix C.4).\n\n\n\n                                                                                                7\n\nTable 2: Performance comparison of different prompt optimizers (last step before exceeding 5M input tokens).\n   We report the mean accuracy (in %) on test set with standard deviation across three seeds of the best prompts.\n    The best prompt per seed is selected from the last population based on development set scores. Bold values\n     indicate best, underlined values second to best performance for each LLM and dataset.\n\n      Model       Optimizer       SST-5     AG News     Subj      GSM8K    COPA       Avg.\n                          Initial           58.47Â± 1.53   87.06Â± 0.65   62.00Â±5.22   44.28Â± 4.91   97.65Â± 1.31   69.89\n              OPRO           60.87Â± 1.09   88.20Â± 0.49   71.33Â±2.80   51.87Â± 2.04   98.07Â± 0.57   74.07\n       Llama-3.3-                  PromptWizard   32.80Â± 1.73   23.33Â± 0.19   51.93Â±0.25   39.33Â±15.09   50.33Â± 0.34   39.55\n      70B                 EvoPromptGA   60.53Â± 1.73   88.67Â± 0.41   75.53Â±1.39   50.87Â± 0.74   97.60Â± 1.13   74.64\n              CAPO (ours)     62.27Â± 0.34   88.80Â± 0.75   91.60Â±2.16   73.73Â± 3.73   98.27Â± 0.52   82.93\n                          Initial           56.68Â± 1.94   79.57Â± 0.84   62.85Â±4.53   33.08Â± 7.78   98.27Â± 0.43   66.09\n              OPRO           57.00Â± 0.43   79.87Â± 0.19   70.67Â±2.36   46.33Â± 3.07   98.67Â± 0.34   70.51\n      Qwen2.5-                  PromptWizard   39.73Â±12.31   63.47Â±28.49   64.93Â±5.01   15.27Â±20.19   98.13Â± 0.19   56.31\n      32B                 EvoPromptGA   58.60Â± 1.73   81.73Â± 1.68   75.87Â±3.58   61.27Â± 8.39   97.87Â± 0.66   75.07\n              CAPO (ours)     59.07Â± 0.50   87.07Â± 0.81   91.00Â±0.65   60.20Â± 4.82   98.47Â± 0.19   79.16\n                          Initial           48.69Â± 2.94   72.21Â± 7.45   61.65Â±6.04   33.71Â± 5.89   94.56Â± 0.94   62.17\n              OPRO           53.20Â± 2.83   84.20Â± 0.16   77.07Â±0.09   43.53Â± 0.47   96.33Â± 0.34   70.87\n        Mistral-                  PromptWizard   31.07Â± 3.80   44.40Â±25.76   59.00Â±5.09   48.67Â± 6.46   57.47Â±10.28   48.12\n       Small-24B                 EvoPromptGA   54.93Â± 0.94   84.40Â± 0.28   74.93Â±2.04   43.93Â± 3.85   96.13Â± 0.34   70.87\n              CAPO (ours)     60.20Â± 0.33   84.33Â± 2.13   81.67Â±1.64   65.07Â± 1.20   95.13Â± 1.20   77.28\n\n\n\n\n\n    Figure 2: Population mean test scores over input to-    Figure 3: Test scores vs. prompt length (system\n    kens on GSM8K with Mistral-Small-24B with mean   + user prompt) for every prompt on GSM8K with\n   Â± std across seeds. PromptWizard yields only a sin-    Mistral-Small-24B. Stars mark the best performing\n     gle prompt early, marked with a star and error bars.   on dev-set from the last population.\n\n\n     We also find that CAPO yields longer prompts than EvoPromptGA and OPRO due to few-\n    shot examples but still shorter than PromptWizard (cf. Figure 3). Thus, though PromptWizard\n    requires fewer tokens during optimization (on average only 25k input tokens), CAPO reduces the\n   deployment cost of the optimized prompts. A more detailed analysis of CAPOâ€™s prompt length\n    reveals that, on average, 66% of the tokens can be attributed to few-shot examples compared to the\n    instruction, with up to 92% in the prompts of Llama-3.3-70B on GSM8K (cf. Appendix G.2).\n        Further, we identify the evaluation of prompts as the main driver behind the token usage:\n   on average, 97% of the input tokens are consumed by the evaluation-LLM and only 3% by the\n   meta-LLM (cf. Appendix G.1), justifying our approach of reducing evaluation costs through racing.\n\n6.2 Ablation Studies\n   To better understand design choices in CAPO, we ablate several components on AG News and\n   GSM8K with Llama-3.3-70B, a budget of 5M input tokens, three seeds, and optimizer parameters as\n\n\n\n                                                                                                8\n\nbefore. We provide results in Table 3 and give further insights in Appendix I while describing the\nkey findings here.\n      I. Zero-shot performance: Without  Table 3: Ablation study results using Llama-3.3-70B. Mean\nfew-shot examples, the performances of  accuracy (in %) on test set of best prompt per seed selected on\nthe best prompts remain unchanged for  the development set scores (format as in Table 2). â€œall aboveâ€\nAG News while being substantially worse   is the combination of zero-shot, ğ›¾= 0, and w/o racing.\nfor the more complex GSM8K task (cf. Ta-                       Accuracy          Prompt length\n                                              Ablationble 3). This highlights the importance of\n                                         AG News  GSM8K   AG News  GSM8K\nfew-shot examples for complex tasks. No-\n                                 CAPO           88.80Â±0.75   73.73Â±3.73   481Â±113    110Â±46\ntably, zero-shot CAPO still considerably\n                                              â†©â†’zero-shot     89.00Â±0.16   59.20Â±5.03    94Â± 17     74Â±24\noutperforms EvoPromptGA on GSM8K.                              â†©â†’ğ›¾= 0         89.27Â±0.41   74.93Â±1.04   297Â± 27    128Â±27\nDue to the lack of few-shot examples, the  â†©â†’w/o racing    89.20Â±0.43   75.00Â±3.12   469Â±130    146Â±52\nresulting prompts are much shorter than  â†©â†’all above     88.53Â±0.09   50.93Â±5.25    78Â± 11     37Â±13\ndefault CAPO prompts but interestingly  â†©â†’generic init   89.33Â±0.19   82.93Â±2.36   206Â±113    182Â±22\nlonger than for EvoPromptGA. We find  EvoPromptGA   88.67Â±0.41   50.87Â±0.74    28Â±  2     30Â± 1\nthat this is due to our meta-prompt tem-  â†©â†’generic init   23.20Â±0.00   53.47Â±0.38    17Â±  8     20Â± 2\nplate sometimes causing repetitions in-\nside optimized prompts after cross-over.7\n     II. No length penalty: Removing the length penalty (ğ›¾= 0) improves performance of the\nfinal prompts compared to default CAPO while the prompt length stays in a similar range (cf.\nTable 3). Such a performance improvement is expected as disabling the length penalty results in\ndirectly optimizing accuracy. Nonetheless, we find that with length penalty, average prompt length\ndecreases as optimization progresses, enabling more steps. We discuss this effect of different length\npenalties in Appendix F.\n     III. No racing: After 5M input tokens, CAPO without racing performs slightly better while\ndifferences lie within one standard deviation (cf. Table 3). Still, comparing performance over input\ntokens reveals that with racing, substantially fewer input tokens are needed to yield first prompts\nwith relatively good performance (cf. Figure 16). We further find that racing, on average, saves 44%\nof evaluations, enabling considerably more steps with the same budget (cf. Appendix I.2).\n    IV. No shots, length penalty & racing: To\nidentify the joint influence of our core inno-\nvations, we combine the three ablations above\nby removing few-shot examples, racing, and\nthe length penalty. The resulting CAPO con-\nfiguration shows a considerable performance\ndrop compared to the default CAPO on GSM8K\nwhile only slightly losing performance on AG\nNews. Generally, we observe that, as expected,\nthis CAPO configuration yields performances\nvery similar to EvoPromptGA. Thus, the per-\nformance gains of default CAPO over Evo-\nPromptGA must stem from our core innova-  Figure 4: Population mean test scores over input to-\ntions and their interplay.                      kens on AG News with Llama-3.3-70B with mean Â±\n    V. Generic initial instructions: We use au-  std across seeds. CAPO and EvoPromptGA start from\ntomatically generated task-unspecific initial in-  task-specific initial instructions, their respective coun-\nstructions (cf. Appendix D.2) and analyze if   terparts from generic initial instructions.\ntask descriptions in CAPO counteract degrad-\n\n\n    7The cross-over step prompts the meta-LLM to â€[...] merge the two prompts [...]â€ (cf. Appendix D.3), which leads to\noccasional concatenation of prompts.\n\n\n\n                                                                                            9\n\ning performances observed by Yang et al. (2024). Our results confirm the degrading performance\n   of EvoPromptGA, especially for AG News (cf. Figure 4). The optimization curves reveal that\n  EvoPromptGAâ€™s performance stays constant as no valid labels are predicted while CAPO starts\n  lower than with task-specific instructions but quickly improves as task descriptions introduce\n   task-specific information, eventually reaching similar performances. Surprisingly, for GSM8K,\n   generic initial instructions even lead to improved CAPO performance (cf. Table 3), likely because\n   (1) instances of GSM8K contain instructions themselves and (2) CAPO can explore more freely.\n  This demonstrates CAPOâ€™s robustness and suggests even generic instruction repositories could\n   serve as initial populations.\n\n7 Conclusion & Future Work\n   In this paper, we propose the discrete prompt optimization method CAPO, an evolutionary algo-\n  rithm that integrates racing and multi-objective optimization, leveraging few-shot examples and\n   task descriptions. Our experiments demonstrate that CAPO outperforms other discrete prompt\n   optimizers in 11 out of 15 cases, with differences up to 21%p on GSM8K with Llama-3.3-70B, while\n  being competitive in the remaining 4 cases. CAPO yields better performance already at earlier\n   stages than other algorithms after the full budget, showing its cost-efficiency, and remains domi-\n  nant over the entire budget. Nonetheless, it yields longer prompts due to few-shot examples. Our\n   ablation studies reveal several important insights: (I.) few-shot examples substantially contribute\n   to the performance, especially for complex tasks, while CAPO maintains strong performance even\n  without examples; (II.) the length-penalty effectively reduces average prompt length throughout\n   optimization; (III.) racing leads to considerable savings in terms of evaluations, enabling more\n   iterations; (IV.) CAPOâ€™s performance gains must be due to few-shot examples, length penalty, and\n   racing, as well as their interplay; and (V.) task descriptions make CAPO robust, yielding strong\n  performance with generic initial instructions.\n      Despite the great advances, our work also has limitations. First, while racing reduces evaluations,\n    it does not necessarily contribute to better performance after the full budget. Moreover, our study\n  focuses on smaller models and could thus be extended to larger LLMs. Similarly, our analysis is\n   limited to classification and math tasks while the main usage of LLMs is text generation. Additionally,\n   all datasets are older than the LLMs, leading to potential test set contamination. Nonetheless, this\n   limitation holds for all optimizers equally, not affecting our conclusions. Finally, output token\n   length is another major cost factor influenced by the prompt, which is not considered in our work\n  and should be addressed by future work.\n      In the future, we plan to make CAPO an a posteriori multi-objective method, allowing the\n  user to choose from a final population that features different trade-offs in prompt performance\n  and length. In addition, we plan to study the use of other strategies for budget allocation, such as\n   successive halving (Karnin et al., 2013; Parmentier et al., 2019) or hyperband (Li et al., 2018; Awad\n   et al., 2021).\n\n8 Broader Impact Statement\n  Making CAPO openly available enables positive impacts across industrial and research applications,\n  though also creating potential for misuse by malicious actors. As our work builds upon LLMs, it\n   inherits their associated impacts, including potential biases, hallucination, and energy consumption.\n  Prompt optimization specifically requires numerous LLM calls, resulting in significant energy\n  expenditure and negative environmental impact. Nonetheless, CAPO aims to reduce these costs.\n  Through racing, CAPO saves evaluations while producing effective prompts earlier. A length\n  penalty encourages shorter prompts for reduced production costs. Our algorithm often achieves\n   better performance at a substantially smaller input token budget than other optimizers on the\n   full budget, greatly improving cost-efficiency. These efficiency improvements directly translate to\n  reduced energy requirements for more environmentally sustainable prompt optimization.\n\n\n\n                                                                                              10\n\nAcknowledgements. We would like to thank Lennart Schneider for his invaluable suggestions and\nimpulses through multiple discussions. We also gratefully acknowledge the computational and\ndata resources provided by the Leibniz Supercomputing Centre.\n\nReferences\nAgarwal, E., Singh, J., Dani, V., Magazine, R., Ganu, T., and Nambi, A. (2024). PromptWizard:\n  Task-aware prompt optimization framework. arXiv:2405.18369 [cs.CL].\n\nAwad, N., Mallik, N., and Hutter, F. (2021). DEHB: Evolutionary hyperband for scalable, robust and\n   efficient Hyperparameter Optimization. In Zhou, Z., editor, Proceedings of the 30th International\n   Joint Conference on Artificial Intelligence (IJCAIâ€™21), pages 2147â€“2153.\n\nBalaprakash, P., Birattari, M., and StÃ¼tzle, T. (2007). Improvement strategies for the f-race algorithm:\n  Sampling design and iterative refinement. In Bartz-Beielstein, T., Aguilera, M. B., Blum, C.,\n  Naujoks, B., Roli, A., Rudolph, G., and Sampels, M., editors, Proceedings of the International\n  Workshop on Hybrid Metaheuristics (HMâ€™07), volume 4771 of Lecture Notes in Computer Science,\n  pages 108â€“122. Springer.\n\nBeume, N., Naujoks, B., and Emmerich, M. (2007). SMS-EMOA: Multiobjective selection based on\n  dominated hypervolume. European Journal of Operational Research, 181(3):1653â€“1669.\n\nBirattari, M. (2009). Tuning Metaheuristics, volume 197 of Studies in Computational Intelligence.\n  Springer, Berlin, Heidelberg.\n\nBirattari, M., StÃ¼tzle, T., Paquete, L., and Varrentrapp, K. (2002). A racing algorithm for configuring\n  metaheuristics. In Langdon, W., Cantu-Paz, E., Mathias, K., Roy, R., Davis, D., Poli, R., Balakrish-\n  nan, K., Honavar, V., Rudolph, G., Wegener, J., Bull, L., Potter, M., Schultz, A., Miller, J., Burke,\n   E., and Jonoska, N., editors, Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCOâ€™02), pages 11â€“18. Morgan Kaufmann Publishers.\n\nBirattari, M., Yuan, Z., Balaprakash, P., and StÃ¼tzle, T. (2010). F-Race and Iterated F-Race: An\n  overview. In Bartz-Beielstein, T., Chiarandini, M., Paquete, L., and Preuss, M., editors, Exper-\n  imental Methods for the Analysis of Optimization Algorithms, pages 311â€“336. Springer, Berlin,\n  Heidelberg.\n\nBlot, A., Hoos, H., Jourdan, L., Kessaci-Marmion, M., and Trautmann, H. (2016). MO-ParamILS:\n A multi-objective automatic algorithm configuration framework. In Festa, P., Sellmann, M.,\n  and Vanschoren, J., editors, Proceedings of the Tenth International Conference on Learning and\n   Intelligent Optimization (LIONâ€™16), Lecture Notes in Computer Science, pages 32â€“47. Springer.\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P.,\n   Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh,\n   A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,\n  Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language\n  models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.-F., and Lin,\n   H., editors, Proceedings of the 33rd International Conference on Advances in Neural Information\n  Processing Systems (NeurIPSâ€™20), pages 1877â€“1901. Curran Associates.\n\nChen, L., Chen, J., Goldstein, T., Huang, H., and Zhou, T. (2024). InstructZero: Efficient instruction\n  optimization for black-box large language models. In Salakhutdinov, R., Kolter, Z., Heller, K.,\n  Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F., editors, Proceedings of the 41st International\n  Conference on Machine Learning (ICMLâ€™24), volume 235 of Proceedings of Machine Learning\n  Research, pages 6503â€“6518. PMLR.\n\n\n                                                                                           11\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton,\n    J., Nakano, R., Hesse, C., and Schulman, J. (2021). Training verifiers to solve math word problems.\n  arXiv:2110.14168 [cs.LG].\n\nConover, W. (1999). Practical Nonparametric Statistics. John Wiley & Sons.\n\nDeb, K., Pratap, A., Agarwal, S., and Meyarivan, T. (2002). A fast and elitist multiobjective genetic\n  algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, 6(2):182â€“197.\n\nDeng, M., Wang, J., Hsieh, C., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E., and Hu, Z. (2022).\n  RLPrompt: Optimizing discrete text prompts with reinforcement learning.  In Goldberg, Y.,\n  Kozareva, Z., and Zhang, Y., editors, Proceedings of the 2022 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP), pages 3369â€“3391. Association for Computational\n   Linguistics.\n\nDo, V.-T., Hoang, V., Nguyen, D., Sabahi, S., Yang, J., Hotta, H., Nguyen, M., and Le, H. (2024).\n  Automatic prompt selection for large language models. arXiv:2404.02717 [cs.CL].\n\nDolan, E. and MorÃ©, J. (2002). Benchmarking optimization software with performance profiles.\n  Mathematical Programming, 91(2):201â€“213.\n\nFalkner, S., Klein, A., and Hutter, F. (2018). BOHB: Robust and efficient Hyperparameter Optimization\n   at scale. In Dy, J. and Krause, A., editors, Proceedings of the 35th International Conference on\n  Machine Learning (ICMLâ€™18), volume 80, pages 1437â€“1446. Proceedings of Machine Learning\n  Research.\n\nFernando, C., Banarse, D., Michalewski, H., Osindero, S., and RocktÃ¤schel, T. (2024). Promptbreeder:\n   self-referential self-improvement via prompt evolution. In Salakhutdinov, R., Kolter, Z., Heller, K.,\n  Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F., editors, Proceedings of the 41st International\n  Conference on Machine Learning (ICMLâ€™24), volume 235 of Proceedings of Machine Learning\n  Research, pages 13481â€“13544. PMLR.\n\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. (2023). GPTQ: Accurate post-training\n  quantization for generative pre-trained transformers. arXiv:2210.17323 [cs.LG].\n\nGoldberg, D. (1989). Genetic Algorithms in Search, Optimization and Machine Learning. Addison-\n  Wesley Longman Publishing Co., Inc., USA, 1st edition.\n\nGuo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. (2024). Connecting\n  large language models with evolutionary algorithms yields powerful prompt optimizers. In The\n  Twelfth International Conference on Learning Representations (ICLRâ€™24). ICLR. Published online:\n  iclr.cc.\n\nHsu, H. and Lachenbruch, P. (2014). Paired test. In Wiley StatsRef: Statistics Reference Online. John\n  Wiley & Sons, Ltd.\n\nHutter, F., Hoos, H., and Leyton-Brown, K. (2011). Sequential model-based optimization for general\n  algorithm configuration. In Coello, C., editor, Proceedings of the Fifth International Conference\n  on Learning and Intelligent Optimization (LIONâ€™11), volume 6683 of Lecture Notes in Computer\n   Science, pages 507â€“523. Springer.\n\nHutter, F., Hoos, H., Leyton-Brown, K., and StÃ¼tzle, T. (2009). ParamILS: An automatic algorithm\n  configuration framework. Journal of Artificial Intelligence Research, 36:267â€“306.\n\n\n                                                                                           12\n\nJamieson, K. and Talwalkar, A. (2016). Non-stochastic best arm identification and Hyperparameter\n  Optimization. In Gretton, A. and Robert, C., editors, Proceedings of the Seventeenth International\n  Conference on Artificial Intelligence and Statistics (AISTATSâ€™16), volume 51. Proceedings of Machine\n  Learning Research.\n\nJiang, Z., Xu, F., Araki, J., and Neubig, G. (2020). How can we know what language models know?\n  Transactions of the Association for Computational Linguistics, 8:423â€“438.\n\nJin, Y. and Sendhoff, B. (2008). Pareto-based multiobjective machine learning: an overview and case\n   studies. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),\n  38(3):397â€“415.\n\nKarl, F., Pielok, T., Moosbauer, J., Pfisterer, F., Coors, S., Binder, M., Schneider, L., Thomas, J., Richter,\n    J., Lang, M., Garrido-MerchÃ¡n, E., Branke, J., and Bischl, B. (2023). Multi-objective hyperparameter\n  optimization â€“ an overview. Transactions of Evolutionary Learning and Optimization, 3(4):1â€“â€“50.\n\nKarmaker Santu, S. and Feng, D. (2023). TELeR: A general taxonomy of LLM prompts for bench-\n  marking complex tasks. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association\n   for Computational Linguistics: EMNLP 2023, pages 14197â€“14203. Association for Computational\n   Linguistics.\n\nKarnin, Z., Koren, T., and Somekh, O. (2013). Almost optimal exploration in multi-armed bandits.\n  In Dasgupta, S. and McAllester, D., editors, Proceedings of the 30th International Conference on\n  Machine Learning (ICMLâ€™13), pages 1238â€“1246. Omnipress.\n\nKavumba, P., Inoue, N., Heinzerling, B., Singh, K., Reisert, P., and Inui, K. (2019). When choosing\n   plausible alternatives, clever hans can be clever. In Ostermann, S., Zhang, S., Roth, M., and Clark,\n   P., editors, Proceedings of the First Workshop on Commonsense Inference in Natural Language\n   Processing, pages 33â€“42. Association for Computational Linguistics.\n\nKhattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., Vardhamanan, S., Haq, S.,\n  Sharma, A., Joshi, T., Moazam, H., Miller, H., Zaharia, M., and Potts, C. (2024). DSPy: Compiling\n  declarative language model calls into state-of-the-art pipelines. In The Twelfth International\n  Conference on Learning Representations (ICLRâ€™24). ICLR. Published online: iclr.cc.\n\nKnowles, J. (2006). ParEGO: a hybrid algorithm with on-line landscape approximation for expensive\n  multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1):50â€“\n   66.\n\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C., Gonzalez, J., Zhang, H., and Stoica, I.\n   (2023). Efficient memory management for large language model serving with pagedattention. In\n  Proceedings of the 29th Symposium on Operating Systems Principles (SOSP â€™23), pages 611â€“626.\n  Association for Computing Machinery.\n\nLester, B., Al-Rfou, R., and Constant, N. (2021). The power of scale for parameter-efficient prompt\n  tuning. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t., editors, Proceedings of the 2021\n  Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3045â€“3059.\n  Association for Computational Linguistics.\n\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. (2018). Hyperband: A novel\n  bandit-based approach to Hyperparameter Optimization. Journal of Machine Learning Research,\n  18(185):1â€“52.\n\n\n\n                                                                                           13\n\nLi, X. and Liang, P. (2021). Prefix-tuning: Optimizing continuous prompts for generation. In Zong,\n   C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th Annua Meeting of the Association\n   for Computational Linguistics and the 11th International Joint Conference on Natural Language\n  Processing (Volume 1: Long Papers), pages 4582â€“4597. Association for Computational Linguistics.\n\nLin, X., Wu, Z., Dai, Z., Hu, W., Shu, Y., Ng, S., Jaillet, P., and Low, B. (2024). Use your INSTINCT:\n  INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers. In Salakhut-\n  dinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F., editors,\n  Proceedings of the 41st International Conference on Machine Learning (ICMLâ€™24), volume 235 of\n  Proceedings of Machine Learning Research, pages 30317â€“30345. PMLR.\n\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023). Pre-train, prompt, and predict:\n A systematic survey of prompting methods in natural language processing. ACM Computing\n  Surveys, 55(9):195:1â€“195:35.\n\nLÃ³pez-IbÃ¡Ã±ez, M., Dubois-Lacoste, J., Caceres, L., Birattari, M., and StÃ¼tzle, T. (2016). The irace\n  package: Iterated racing for automatic algorithm configuration. Operations Research Perspectives,\n  3:43â€“58.\n\nLu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. (2022). Fantastically ordered prompts and\n  where to find them: Overcoming few-shot prompt order sensitivity. In Muresan, S., Nakov, P.,\n  and Villavicencio, A., editors, Proceedings of the 60th Annual Meeting of the Association for Com-\n  putational Linguistics (Volume 1: Long Papers), pages 8086â€“8098. Association for Computational\n   Linguistics.\n\nMaron, O. and Moore, A. (1994). Hoeffding races: accelerating model selection search for clas-\n   sification and function approximation. In Cowan, J. D., Tesauro, G., and Alspector, J., editors,\n  Proceedings of the 8th International Conference on Advances in Neural Information Processing\n  Systems (NeurIPSâ€™94), pages 59â€“66. Morgan Kaufmann Publishers.\n\nMeskÃ³, B. (2023). Prompt engineering as an important emerging skill for medical professionals:\n  Tutorial. Journal of Medical Internet Research, 25(1):e50638.\n\nMeta (2024).  Llama 3.3: Model cards and prompt formats.  https://www.llama.com/docs/\n  model-cards-and-prompt-formats/llama3_3/. Accessed: 2025-03-31.\n\nMiettinen, K. (1998). Nonlinear Multiobjective Optimization, volume 12 of International Series in\n  Operations Research & Management Science. Springer US, Boston, MA.\n\nMiranda, P., Silva, R., and PrudÃªncio, R. (2015).  I/S-Race: An iterative multi-objective racing\n  algorithm for the SVM parameter selection problem. In 22st European Symposium on Artificial\n  Neural Networks, Computational Intelligence And Machine Learning, Bruges, April, pages 23â€“24.\n\nMistral AI Team (2025). Mistral Small 3: Mistral AI. https://mistral.ai/news/mistral-small-3.\n  Accessed: 2025-03-31.\n\nMoore, A. and Lee, M. (1994). Efficient algorithms for minimizing cross validation error. In Cohen,\n  W. and Hirsh, H., editors, Machine Learning Proceedings 1994, pages 190â€“198. Morgan Kaufmann.\n\nNarayanan, S., Braza, J., Griffiths, R., Ponnapati, M., Bou, A., Laurent, J., Kabeli, O., Wellawatte, G.,\n  Cox, S., Rodriques, S., and White, A. (2024). Aviary: training language agents on challenging\n   scientific tasks. arXiv:2412.21154 [cs.AI].\n\n\n                                                                                           14\n\nPang, B. and Lee, L. (2004). A sentimental education: Sentiment analysis using subjectivity\n  summarization based on minimum cuts.  In Proceedings of the 42nd Annual Meeting of the\n  Association for Computational Linguistics (ACL-04), pages 271â€“278.\n\nParmentier, L., Nicol, O., Jourdan, L., and Kessaci, M. (2019). TPOT-SH: A faster optimization\n  algorithm to solve the AutoML problem on large datasets.  In Proceedings of the 31st IEEE\n  International Conference on Tools with Artificial Intelligence (ICTAIâ€™19), pages 471â€“478. IEEE\n  Computer Society, IEEE.\n\nPryzant, R., Iter, D., Li, J., Lee, Y., Zhu, C., and Zeng, M. (2023). Automatic prompt optimization with\n  â€œgradient descentâ€ and beam search. In Bouamor, H., Pino, J., and Bali, K., editors, Proceedings\n   of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n  7957â€“7968. Association for Computational Linguistics.\n\nQin, G. and Eisner, J. (2021). Learning how to ask: Querying LMs with mixtures of soft prompts. In\n  Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell,\n   R., Chakraborty, T., and Zhou, Y., editors, Proceedings of the 2021 Conference of the North American\n  Chapter of the Association for Computational Linguistics: Human Language Technologies, pages\n  5203â€“5212. Association for Computational Linguistics.\n\nQwen: Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin,\n   H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K.,\n  Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren,\n   X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. (2025). Qwen2.5\n  technical report. arXiv:2412.15115 [cs.CL].\n\nRook, J., Benjamins, C., Bossek, J., Trautmann, H., Hoos, H., and Lindauer, M. (2025). MO-SMAC:\n  Multiobjective sequential model-based algorithm configuration. Evolutionary Computation, pages\n  1â€“24.\n\nSchneider, L., Wistuba, M., Klein, A., Golebiowski,  J., Zappella, G., and Merra, F. A. (2024).\n  Hyperband-based Bayesian Optimization for black-box prompt selection.  arXiv:2412.07820\n   [cs.LG].\n\nShi, C., Yang, K., Yang, J., and Shen, C. (2024). Best arm identification for prompt learning under\n  a limited budget.  In ICLR 2024 Workshop on Mathematical and Empirical Understanding of\n  Foundation Models.\n\nShi, W., Han, X., Gonen, H., Holtzman, A., Tsvetkov, Y., and Zettlemoyer, L. (2023). Toward human\n  readable prompt tuning: Kubrickâ€˜s the shining is a good movie, and a good prompt too? In\n  Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics:\n  EMNLP 2023, pages 10994â€“11005. Association for Computational Linguistics.\n\nShin, T., Razeghi, Y., Logan IV, R., Wallace, E., and Singh, S. (2020). Autoprompt: Eliciting knowledge\n  from language models with automatically generated prompts. In Webber, B., Cohn, T., He, Y.,\n  and Liu, Y., editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), pages 4222â€“4235. Association for Computational Linguistics.\n\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C., Ng, A., and Potts, C. (2013). Recursive\n  deep models for semantic compositionality over a sentiment treebank. In Yarowsky, D., Baldwin,\n   T., Korhonen, A., Livescu, K., and Bethard, S., editors, Proceedings of the 2013 Conference on Em-\n   pirical Methods in Natural Language Processing, pages 1631â€“1642. Association for Computational\n   Linguistics.\n\n\n\n                                                                                           15\n\nSorensen, T., Robinson, J., Rytting, C., Shaw, A., Rogers, K., Delorey, A., Khalil, M., Fulda, N., and\n  Wingate, D. (2022). An information-theoretic approach to prompt engineering without ground\n  truth labels. In Muresan, S., Nakov, P., and Villavicencio, A., editors, Proceedings of the 60th\n  Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n  819â€“862. Association for Computational Linguistics.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I.\n   (2017). Attention is all you need. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H., Fergus,\n   R., Vishwanathan, S., and Garnett, R., editors, Proceedings of the 31st International Conference on\n  Advances in Neural Information Processing Systems (NeurIPSâ€™17). Curran Associates, Inc.\n\nWan, X., Sun, R., Nakhost, H., and Arik, S. (2024). Teach better or show smarter? on instructions\n  and exemplars in automatic prompt optimization. In Globerson, A., Mackey, L., Belgrave, D., Fan,\n   A., Paquet, U., Tomczak, J., and Zhang, C., editors, Proceedings of the 37th International Conference\n  on Advances in Neural Information Processing Systems (NeurIPSâ€™24), pages 58174â€“58244. Curran\n  Associates.\n\nWhite, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., and\n  Schmidt, D. (2025). A prompt pattern catalog to enhance prompt engineering with ChatGPT. In\n  Proceedings of the 30th Conference on Pattern Languages of Programs, PLoP â€™23, pages 1â€“31, USA.\n  The Hillside Group.\n\nWu, Z., Lin, X., Dai, Z., Hu, W., Shu, Y., Ng, S.-K., Jaillet, P., and Low, B. (2024). Prompt optimization\n  with EASE? efficient ordering-aware automated selection of exemplars. In Globerson, A., Mackey,\n   L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., and Zhang, C., editors, Proceedings of the 37th\n   International Conference on Advances in Neural Information Processing Systems (NeurIPSâ€™24), pages\n  122706â€“122740. Curran Associates.\n\nYang, C., Wang, X., Lu, Y., Liu, H., Le, Q., Zhou, D., and Chen, X. (2024). Large language models as\n  optimizers. In The Twelfth International Conference on Learning Representations (ICLRâ€™24). ICLR.\n  Published online: iclr.cc.\n\nYuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Lu, P., Huang, Z., Guestrin, C., and Zou, J. (2025).\n  Optimizing generative AI by backpropagating language model feedback. Nature, 639(8055):609â€“\n  â€“616.\n\nZhang, T., Georgiopoulos, M., and Anagnostopoulos, G. (2013). S-Race: a multi-objective rac-\n  ing algorithm. In Blum, C. and Alba, E., editors, Proceedings of the Genetic and Evolutionary\n  Computation Conference (GECCOâ€™13), pages 1565â€“1572. ACM Press.\n\nZhang, T., Georgiopoulos, M., and Anagnostopoulos, G. (2015a). SPRINT multi-objective model\n  racing. In Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation,\n  pages 1383â€“1390. Association for Computing Machinery.\n\nZhang, X., Zhao,  J., and LeCun, Y. (2015b).  Character-level convolutional networks for text\n   classification.  In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R., editors,\n  Proceedings of the 28th International Conference on Advances in Neural Information Processing\n  Systems (NeurIPSâ€™15). Curran Associates.\n\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. (2021). Calibrate before use: Improving\n  few-shot performance of language models. In Meila, M. and Zhang, T., editors, Proceedings of\n  the 38th International Conference on Machine Learning (ICMLâ€™21), volume 139 of Proceedings of\n  Machine Learning Research, pages 12697â€“12706. PMLR.\n\n\n                                                                                           16\n\nZhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2023). Large language\n  models are human-level prompt engineers. In The Eleventh International Conference on Learning\n  Representations (ICLRâ€™23). ICLR. Published online: iclr.cc.\n\n\n\n\n\n                                                                                           17\n\nAppendix\n\n\nA  Background                                                                 19\n   A.1  Prompt Optimization Algorithms    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   19\n   A.2  AutoML Techniques: Racing and Multi-Objective Optimization  .  .  .  .  .  .  .  .  .  .  .   20\n\nB  Algorithm Details                                                            22\n\nC  Technical Details                                                             23\n   C.1  Model Details    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   23\n   C.2  Dataset Details  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   23\n   C.3  Hardware Details    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   24\n   C.4  Implementation Details  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   24\n\nD  Input Specifications and Templates                                              25\n   D.1  Task Descriptions   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   25\n   D.2   Initial Instructions  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   25\n   D.3  Meta-Prompt Templates .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   26\n\nE  Examples of CAPO Algorithm Operations                                        27\n    E.1  Cross-over and Mutation Examples    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   27\n    E.2  Exemplary Few-Shot Examples With and Without Reasoning .  .  .  .  .  .  .  .  .  .  .  .   28\n\nF  Hyperparameter Sensitivity Analysis                                            29\n\nG CAPO Detailed Analysis                                                       33\n   G.1  Token Usage Breakdown of Evaluation-LLM vs. Meta-LLM  .  .  .  .  .  .  .  .  .  .  .  .  .   33\n   G.2  Influence of Few-Shot Examples on Prompt Length    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   33\n   G.3  Prompt Survival Analysis  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   34\n\nH  Further Benchmark Results                                                    35\n   H.1  Performance Profile  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   35\n   H.2  Further Optimization Curves from Benchmark Experiments    .  .  .  .  .  .  .  .  .  .  .  .   36\n   H.3  Prompt Lengths from Benchmark Experiments  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   38\n\nI   Further Ablation Results                                                      39\n     I.1   Optimization Curves from Ablation Studies .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   39\n     I.2   Impact of Racing  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   40\n     I.3   Influence of Meta-Prompt Simplification and Task Descriptions    .  .  .  .  .  .  .  .  .  .   42\n\nJ  Best Prompts per Tasks                                                        43\n    J.1    Initial Prompts  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   43\n    J.2  CAPO Prompts .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   44\n    J.3  EvoPromptGA Prompts  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   44\n    J.4  OPRO Prompts .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   45\n    J.5   PromptWizard Prompts  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   45\n\n\n\n\n\n                                                                                           18\n\nA Background\n    This section provides additional background on concepts and algorithms employed in this paper.\n\nA.1 Prompt Optimization Algorithms\n     In the following, we present the three prompt optimization algorithms we benchmark against:\n    EvoPrompt (Guo et al., 2024), OPRO (Yang et al., 2024), and PromptWizard (Agarwal et al., 2024).\n     All three are of discrete prompt optimization methods. They optimize textual prompts directly by\n    generating multiple prompt variations and selecting the best candidates. Thus, they do not require\n     access to gradients or token probabilities but are also applicable to black box LLMs.\n\n    EvoPrompt. EvoPrompt (Guo et al., 2024) is a discrete prompt optimization framework based on\n    evolutionary algorithms. It uses a meta-LLM to alternate prompts via cross-over and mutation\n     operations, enabling direct optimization of discrete prompts while maintaining coherence and\n   human readability. EvoPrompt starts from an initial prompt population, iteratively generates new\n    prompts, evaluates generated candidates on a development set, selects the best performing ones as\n     survivors, and terminates after a predefined number of iterations.\n      Guo et al. (2024) present two instantiations of EvoPrompt: as a Genetic Algorithm (GA) and\n    as a Differential Evolution (DE) method. We focus on EvoPromptGA, which serves as basis\n     for our algorithm and is therefore also used in our benchmark experiments. In each iteration,\n    EvoPromptGA selects two parent prompts via roulette wheel selection and generates new candidate\n    prompts in two steps:  first, the cross-over operation combines properties from both parents\n     into an offspring; second, each offspring is mutated through small random modifications. Both\n     evolutionary operations are implemented through a single meta-prompt instructing the meta-LLM\n     (for the template, see Appendix D.3). Each iteration produces ğœ‡new prompts that compete with\n    the existing ğœ‡ones, from which the top ğœ‡survive.\n       Experiments by Guo et al. (2024) across language understanding, generation, and BIG-Bench\n    Hard (BBH) tasks demonstrate that both EvoPrompt instantiations outperform human-written in-\n     structions and previous prompt optimizers such as APE (Zhou et al., 2023) and APO/ProTeGi (Pryzant\n     et al., 2023). This makes EvoPrompt a suitable reference for our benchmark experiments.\n       However, EvoPrompt has two major drawbacks: First, as pointed out in the main paper, it is\n     cost-intensive: it requires a total of ğœ‡Â· ğ‘‡Â· (1 + |Ddev|) LLM calls (Guo et al., 2024) with population\n     size ğœ‡, number of iterationsğ‘‡, and development set size |Ddev|. This number is mainly driven by the\n     size of Ddev, which is usually much larger than ğœ‡and ğ‘‡. Second, as identified by Yang et al. (2024),\n    EvoPromptâ€™s performance can degrade with poor or task-unspecific prompts due to its reliance on\n     task specification via prompt population, a phenomenon we also demonstrate in the main paper.\n    Both shortcomings of EvoPrompt are addressed by CAPO.\n\n   OPRO. OPRO (Optimization by PROmpting) (Yang et al., 2024) directly employs LLMs as optimizers\n    by specifying optimization tasks in natural language. When used for prompt optimization, a meta-\n   LLM generates new prompt candidates at each iteration, guided by a meta-prompt that contains\n    the task description, task examples, and previously generated candidates with their scores. New\n    candidates are evaluated and appended to the meta-prompt for the subsequent iteration. This\n    approach substantially outperforms human-designed prompts on GSM8K and BBH tasks. Unlike\n    EvoPrompt, OPRO maintains good performance even with task-unspecific initial instructions by\n     leveraging explicit task descriptions and examples within the meta-prompt.\n       However, OPRO, similar to EvoPrompt, focuses solely on instruction optimization without\n    incorporating few-shot examples (those are only used in the meta-prompt), despite evidence that\n    such examples can significantly improve LLM performance (Brown et al., 2020).\n\n    PromptWizard. A recent approach that jointly optimizes instructions and examples is PromptWiz-\n    ard (Agarwal et al., 2024). This is an important advancement as automatic prompt optimization\n\n\n\n                                                                                                19\n\nalso covers optimization of the few-shot examples (â€œexemplar optimizationâ€), i.e., improving the\n     selection of relevant examples. Recent research by Wan et al. (2024) indicates that (1) even sim-\n     ple random example selection can yield performance improvements compared to sophisticated\n     instruction optimization methods, and (2) combining instruction and example optimization creates\n     synergistic effects enhancing overall performance.\n       The PromptWizard algorithm iteratively improves prompts through multiple steps: generating\n     instruction variants via different thinking styles (mutation), evaluating them (scoring), providing\n    feedback on top performers (critique), and implementing refinements (synthesis). It simultaneously\n    optimizes in-context examples and uses critique and synthesis to produce synthetic examples ad-\n     dressing the promptâ€™s weaknesses. Moreover, PromptWizard incorporates automatically generated\n     chain-of-thought reasoning for few-shot examples and leverages task intent and an expert persona\n     in prompts. It reportedly outperforms INSTINCT (Lin et al., 2024), InstructZero (Chen et al., 2024),\n   APE (Zhou et al., 2023), PromptBreeder (Fernando et al., 2024), and EvoPrompt (Guo et al., 2024) on\n    BIG-Bench Instruction Induction (BBII) while substantially reducing LLM calls and token usage.\n\n\nA.2 AutoML Techniques: Racing and Multi-Objective Optimization\n\n   As pointed out in the main paper, the field of AutoML offers many techniques that aim to make\n     optimization more efficient. This includes racing algorithms (Maron and Moore, 1994; Birattari et al.,\n     2002; LÃ³pez-IbÃ¡Ã±ez et al., 2016), multi-fidelity optimization (Jamieson and Talwalkar, 2016; Li et al.,\n     2018; Falkner et al., 2018; Awad et al., 2021), and multi-objective optimization with efficiency as an\n     additional goal (Karl et al., 2023), to name just a few. These methods have also been successfully\n    adopted beyond AutoML, for example, in the field of prompt selection, where efficiency is similarly\n    important (Schneider et al., 2024; Shi et al., 2024). In the following, we provide more details on\n     racing and multi-objective optimization, two AutoML techniques which we transfer to the field of\n    prompt optimization with CAPO.\n\n    Racing. Racing refers to class of algorithms initially proposed for model selection in Machine\n    Learning (Maron and Moore, 1994) and later adopted for algorithm configuration (Birattari et al.,\n     2002). These algorithms sequentially evaluate candidates and eliminate poor one as soon as enough\n     statistical evidence is collected against them, continuing the race only with surviving candidates.\n    This approach accelerates optimization by spending fewer evaluations on poor candidates, allowing\n    more resources to be concentrated on promising candidates (Birattari et al., 2002, 2010).\n        Hoeffding Races (Maron and Moore, 1994), one of the earliest racing methods, sequentially\n     evaluate candidates on problem instances and use the Hoeffdingâ€™s bound to eliminate statistically\n     inferior options early. While this non-parametric approach imposes no distributional assumptions,\n       it tends to be relatively conservative (Moore and Lee, 1994). BRACE (Moore and Lee, 1994) therefore\n    uses Bayesian statistics instead of loose non-parametric bounds like Hoeffdingâ€™s, enabling much\n     earlier elimination of poor candidates.\n       F-Race (Birattari et al., 2002), forming the basis for many contemporary racing algorithms,\n    employs the Friedman two-way analysis of variance by ranks (Conover, 1999), an omnibus test to\n    compare multiple candidates across multiple problem instances. It ranks the candidatesâ€™ perfor-\n    mances within each instance to build cumulative evidence of which configurations are superior.\n      It tests against the null hypothesis that all possible candidate rankings are equally likely. If this\n    hypothesis is rejected, pairwise post-hoc tests between individual candidates are performed with\n     significantly worse candidates being eliminated. Otherwise, all candidates advance to the next step.\n    Since F-Race is suitable only for moderate numbers of candidates, Iterative F-Race (I/F-Race) (Bal-\n    aprakash et al., 2007) extends it by iteratively applying F-Race while updating a probabilistic model\n     of the candidate space to assign more probability mass to promising regions, from which subsequent\n    candidates are sampled.\n\n\n\n                                                                                                20\n\nThe irace package (LÃ³pez-IbÃ¡Ã±ez et al., 2016) provides a general iterated racing implementation,\nof which I/F-Race is a special case, and offers several extensions and improvements. It implements\nthe paired t-test as an alternative to the Friedman test. The latter is preferable when score ranges\nacross different instances are not commensurable or the objective is an order statistic, while the\nt-test is more suitable when the objective corresponds to the mean of the score function. For multi-\nclass tasks, irace recommends structuring instances in blocks rather than adding single instances\nper iteration. At the end of a race, the surviving candidates with highest overall rank across all\ninstances/blocks are selected. They also present elitist racing as extension, which protects high-\nperforming candidates (â€œelitesâ€) from elimination unless a new candidate demonstrates superior\nperformance across at least the same number of evaluation instances.\n    Lastly, we also present some approaches related to racing. FocusedILS, an instantiation of\nParamILS (Hutter et al., 2009), employs an approach similar to racing to save evaluation costs by\nadaptively increasing the number of evaluations and comparing configurations based on domina-\ntion: one configuration dominates another when it performs at least as well on the same number of\ninstances. A â€œbonus runâ€ mechanism allocates more evaluation resources to promising configura-\ntions. Similarly, Random Online Adaptive Racing (ROAR) and Sequential Model-based Algorithm\nConfiguration (SMAC) (Hutter et al., 2011) implement an â€œintensificationâ€ mechanism. Although\nlabeled as racing, these algorithms do not use statistical testing. If a new candidate performs worse\nthan the incumbent on the set of common instances, evaluating the new candidate immediately\nstops. Otherwise, the mechanism adds further evaluations exponentially.\n\nMulti-Objective Optimization. Multi-objective optimization addresses scenarios with multiple\ncompeting objectives. Typical applications involve balancing different prediction performance\nmetrics or trading offpredictive performance against computational efficiency, interpretability, or\nsparseness. Multi-objective approaches are commonly categorized into a priori and a posteriori\nmethods (Karl et al., 2023).\n  A priori methods transform multiple objectives into a single one, for example, using a weighted\nsum of the objectives (scalarization), yielding only a single solution candidate (Karl et al., 2023).\nAlthough a single objective simplifies the optimization problem (Miettinen, 1998), this approach\nrequires the weights to be chosen a priori, which can be non-trivial, and trade-offs between\ncompeting objectives cannot be fully captured by a single solution (Jin and Sendhoff, 2008).\n    Conversely, a posteriori methods produce a set of Pareto-optimal solutions that domain ex-\nperts can analyze after the optimization process (Karl et al., 2023). Evolutionary algorithms are\nparticularly well-suited due to their population-based nature. Notable multi-objective evolution-\nary optimizers include NSGA-II (Deb et al., 2002), which uses non-dominated sorting rank and\ncrowding distance for selection, and SMS-EMOA (Beume et al., 2007), which employs marginal\nhypervolume contribution as secondary criterion. Bayesian Optimization approaches have also\nbeen extended to multi-objective scenarios, with ParEGO (Knowles, 2006) being a prominent exam-\nple. ParEGO approximates the Pareto-front by utilizing a set of randomly generated scalarization\nweights throughout its iterations.\n    Finally, combinations of multi-objective optimization and racing methods have been developed.\nirace can be used to configure multi-objective optimization algorithms by converting multi-objective\nproblems into single-objective evaluations using hypervolume or the ğœ€-measure (LÃ³pez-IbÃ¡Ã±ez et al.,\n2016). S-Race (Zhang et al., 2013), specifically designed for multiple objectives, discards candidates\nonce there is sufficient statistical evidence against them with respect to all objectives, later extended\nby SPRINT-Race (Zhang et al., 2015a) and I/S-Race (Miranda et al., 2015). A multi-objective variant\nof ParamILS, MO-ParamILS (Blot et al., 2016), also exists, which works on a set of non-dominated\nconfigurations in the Pareto-sense (â€œarchiveâ€) instead of a single configuration. MO-SMAC (Rook\net al., 2025) extends the SMAC framework to multi-objective scenarios via a hypervolume-based\nacquisition function and a procedure for managing non-dominated configuration sets.\n\n\n\n                                                                                           21\n\nB Algorithm Details\n\n\n\n  Algorithm 2 CAPO Functions\n   Require: population Pğœ‡, meta-LLM Î¦meta, evaluation-LLM Î¦eval, cross-over-meta-prompt ğ‘ğ¶, mutation-meta-prompt\n      ğ‘ğ‘€, number of crossovers ğ‘, offspring prompts Poff, few-shot dataset Dshots, maximum number of few-shot examples\n       ğ‘˜max, blocks B, confidence level ğ›¼, token length penalty control parameter ğ›¾, number of survivors ğ‘›survive, max.\n      number of evaluated blocks ğ‘§max\n      1: function cross_over(Pğœ‡, Î¦meta, ğ‘ğ¶, ğ‘)\n      2:    Poff â†[]\n      3:     for ğ‘—= 1 to ğ‘do\n      4:         ğ‘ğ‘, ğ‘ğ‘â†sample(Pğœ‡, 2)                    âŠ²Sample without replacement; ğ‘ğ‘= (ğ‘–ğ‘, ğ’†ğ’‚), ğ‘ğ‘= (ğ‘–ğ‘, ğ’†ğ’ƒ)\n      5:          ğ‘–off â†Î¦meta(ğ‘ğ¶||ğ‘–ğ‘||ğ‘–ğ‘)                                  âŠ²Let meta-LLM cross the parent prompts\n      6:         ğ’†off â†sample(ğ’†ğ’‚âˆªğ’†ğ’ƒ, j |ğ’†ğ’‚|+|ğ’†ğ’ƒ|2    k )                 âŠ²Sample from parent shots without replacement\n      7:        ğ‘off â†(ğ‘–off, ğ’†off)\n      8:        Poff â†append(ğ‘off, Poff)\n      9:    end for\n    10:    return Poff\n    11: end function\n    12: function mutate(Poff, Î¦meta, Î¦eval, ğ‘ğ‘€, Dshots, ğ‘˜max)\n    13:    Pmut â†[]\n    14:     for ğ‘off âˆˆPoff do\n    15:        ğ‘–mut â†Î¦meta(ğ‘ğ‘€âˆ¥ğ‘–off)                                    âŠ²Let meta-LLM mutate the instruction\n    16:       ğ‘Ÿâˆ¼Unif({0, 1, 2})\n    17:           if ğ‘Ÿ= 0 and |ğ’†off| < ğ‘˜max then                            âŠ²Case 1: Create a new few-shot example\n    18:           ğ’†new â†ğ’†off âˆªcreate_shots(Dshots, 1,ğ‘–mut, Î¦eval)\n    19:         else if ğ‘Ÿ= 1 and |ğ’†off| > 0 then                              âŠ²Case 2: Remove a few-shot example\n    20:           ğ’†new â†sample(ğ’†off, |ğ’†off| âˆ’1)\n    21:       end if                                            âŠ²Case 3: Keep number of few-shot examples\n    22:       ğ‘mut â†  ğ‘–mut, shuffle(ğ’†new)\n    23:       Pmut â†append(ğ‘mut, Pmut)\n    24:    end for\n    25:    return Pmut\n    26: end function\n    27: function do_racing(Pğœ‡, B, Î¦eval, ğ›¼, ğ›¾, ğ‘›survive, ğ‘§max)\n    28:   ğ‘—â†0\n    29:    shuffle(B)                                                            âŠ²Optional (hyperparameter)\n    30:    while |Pğœ‡| > ğ‘›survive and ğ‘—< ğ‘§max do\n    31:     ğ‘—â†ğ‘—+ 1\n    32:      ğ‘ºâ†evaluate(Pğœ‡, ğµ:ğ‘—, length_penalty = ğ›¾)                     âŠ²Note: cache already evaluated blocks\n    33:       Pğœ‡â†racing_elimination(Pğœ‡, ğ‘º, ğ›¼,ğ‘›survive)\n    34:    end while\n    35:   Pğœ‡â†sort(Pğœ‡)[: ğ‘›survive]                           âŠ²Make sure to return only ğ‘›survive prompts\n    36:    return Pğœ‡\n    37: end function\n    38: function racing_elimination(Pğœ‡, ğ‘º, ğ›¼, ğ‘›survive)\n    39:     Psurvivors â†Pğœ‡\n    40:    ğ‘ğ›¼â†get_critical_value(ğ›¼)\n    41:     for ğ‘ğ‘–âˆˆPsurvivors do\n    42:          ğ‘›sig_better â†Ã ğ‘—â‰ ğ‘–I{get_test_statistic(ğ’”ğ’‹, ğ’”ğ’Š) > ğ‘ğ›¼}                    âŠ²Perform significance tests\n    43:           if ğ‘›sig_better â‰¥ğ‘›survive then\n    44:             Psurvivors â†Psurvivors \\{ğ‘ğ‘–}                                                      âŠ²Eliminate ğ‘ğ‘–\n    45:       end if\n    46:    end for\n    47:    return Psurvivors\n    48: end function\n\n\n\n                                                                                              22\n\nC Technical Details\n\nC.1 Model Details\n\n   We report detailed IDs and revisions of the utilized LLMs from HuggingFace in Table 4. To locally\n    host the LLMs, we use vLLM 0.7.3 (Kwon et al., 2023) as fast and easy-to-use library for LLM\n    inference and serving since it efficiently manages the required memory and allows the usage of\n    quantized models. Note that we restrict maximum output length to 2048, which is long enough for\n    almost all generations while still allowing for reasonable large batch sizes. The optimal batch size\n      is chosen by vLLM depending on available memory.\n\n\n                                    Table 4: Overview of the utilized LLMs.\n\n    Model               Huggingface ID                            Revision\n     Llama-3.3-70B        shuyuej/Llama-3.3-70B-Instruct-GPTQ        3a7f7f7d46e362291821aaefb0a38b632f1190a8\n    Qwen2.5-32B         Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4      c83e67dfb2664f5039fd4cd99e206799e27dd800\n     Mistral-Small-24B     ConfidentialMind/Mistral-Small-24B-        803393813b8fc4046fb663af2e3c56339a5b520b\n                        Instruct-2501_GPTQ_G128_W4A16_MSE\n\n\nC.2 Dataset Details\n\n     In our experiments we utilize five datasets, all retrieved from HuggingFace:\n     (1) SST-5 (Socher et al., 2013): sentiment classification dataset from the Stanford Sentiment Treebank\n        (SST) with five different sentiment classes. The input ğ‘¥is taken from the column â€œtextâ€, the\n         labels ğ‘¦from the column â€œlabel_textâ€.\n     (2) AG News (Zhang et al., 2015b): topic classification dataset with titles and descriptions of news\n          articles that are to be assigned to either World, Sports, Business or Sci/Tech. The input ğ‘¥is taken\n       from the column â€œtextâ€, the labels ğ‘¦from the column â€œlabel_textâ€.\n     (3) Subj (Pang and Lee, 2004): subjectivity classification dataset with movie reviews that are to\n       be classified as either subjective or objective. The input ğ‘¥is taken from the column â€œtextâ€, the\n         labels ğ‘¦from the column â€œlabel_textâ€.\n     (4) GSM8K (Cobbe et al., 2021): grade school math word problems requiring multi-step reasoning.\n     We utilize the train and test split of the â€œmainâ€ subset, from which the column â€œquestionâ€ is\n       used as input ğ‘¥, the label ğ‘¦is extracted from the â€œanswerâ€ after ####.\n     (5) (Balanced) COPA (Kavumba et al., 2019): commonsense causal reasoning dataset with premises\n         for which the plausible cause or effect is to be chosen from two alternatives. We create the\n        input ğ‘¥by concatenating the columns â€œpremiseâ€, â€œquestionâ€, â€œchoice1â€, and â€œchoice2â€ as follows:\n       â€œ<premise>\\n <question> A: \\n <choice1> \\n <question> B: \\n <choice2>â€. The labels ğ‘¦are\n      mapped from 0 and 1 in column â€œlabelâ€ to â€œAâ€ and â€œBâ€.\n   We provide detailed IDs and revisions of the utilized datasets in Table 5. For Dshots and Ddev,\n    500 instances are sampled from the train split without replacement with the random seed of the\n    corresponding experiment. The first 300 points are used for Ddev, the remaining 200 for Dshots. To\n    obtain Dtest, 500 instances are sampled from the test split and used throughout all experiments.\n\n                            Table 5: Overview of the utilized HuggingFace datasets.\n\n     Dataset    Huggingface ID             Revision                                               ntrain   ntest    #classes\n     SST-5       SetFit/sst5                  e51bdcd8cd3a30da231-967c1a249ba59361279a3    8.5k    2.2k   5\n   AG News   SetFit/ag_news              ca5ba619eb034211db5-f70932b6702efd21e7c73    120k   7.6k   4\n     Subj         SetFit/subj                  f3c1162e678417f664d-76b21864fdb87b0615fcf     8k     2k     2\n    GSM8K    openai/gsm8k               e53f048856ff4f594e95-9d75785d2c2d37b678ee     7.5k    1.3k     -\n    COPA     pkavumba/balanced-copa    813bd03cd6e07d9bd8d7333896ad5d40abb95ea9   1k     500    2\n\n\n\n                                                                                                23\n\nC.3 Hardware Details\n     All computations are performed on a GPU cluster. For each experiment configuration, only a single\n   GPU with at least 80GB of RAM (NVIDIA A100 (80GB) or NVIDIA H100 (94GB)) is used to host the\n    corresponding LLM. Experiments are distributed across multiple instances for parallel execution.\n   We report a total computation time of 13 GPU days for our experiments, not including the compute\n    time for evaluation on hold-out test data.\n\nC.4 Implementation Details\n   Answer Extraction. To reliably extract information from LLM output in our experiments, we\n     utilize marker-based extraction. Concretely, we parse the information in html-style tags: offspring/-\n    mutated prompts are extracted between <prompt></prompt> markers and predictions between\n    <final_answer></final_answer> markers in the LLM output. This information is also included in\n    the initial instructions and task descriptions. Details and examples are provided in the subsequent\n     sections of this appendix.\n\n    Optimizer Parametrization. For our experiments, we use the following default hyperparameters:\n   We parametrize our CAPO algorithm with ğ›¼= 0.2, ğ‘= 30 and ğ‘§max = 10 (i.e., ğ‘Â· ğ‘§max = |Ddev|),\n    ğ‘˜max = 5, ğœ‡= 10, ğ‘= 4, ğ›¾= 0.05 (a prompt with same length as the longest initial prompt\n     (instruction + examples) is penalized by 5%p). Further, we use our simplified meta-prompts ğ‘ğ¶and\n    ğ‘ğ‘€(cf. Appendix D.3), a paired t-test for racing, and no block shuffling for cost-efficiency.\n       For EvoPromptGA (Guo et al., 2024), we also use a population size 10 following the recom-\n    mendations of the original paper. For OPRO (Yang et al., 2024), also following the publication,\n   we limit the number of previous prompts in the meta-prompt to 20, generate 8 new prompts per\n     iteration, and use 3 few-shot examples in the meta-prompt. For PromptWizard (Agarwal et al.,\n     2024), we use the original parametrization, and provide one randomly sampled instruction from\n    our pool, our task description, and answer format. It is important to note that we cannot trivially\n    extend PromptWizard to make use of the full budget in our experiments. In its original multi-step\n    implementation, each algorithmic step (cf. Appendix A.1) is performed a specified number of\n     iterations before continuing to the next one. Having a predefined maximum budget, it is unclear\n   how to distribute it between the steps in advance.\n\n    Optimizer Implementation. For EvoPromptGA and OPRO, we use reimplementations that are\n     available as part of a public library.8 We note that this library is developed and maintained by the\n     authors, allowing to directly ensure the correctness of the implementations. For PromptWizard, we\n     utilize the original implementation9 with small adaptions for our LLMs.\n\n    Seeding. For statistical robustness, we conduct three independent runs of each optimizer-LLM-\n     dataset configuration with varying random seeds to quantify variance. Seeds influence stochastic\n    elements of the optimizers, initial instruction selection, dev set sampling and LLM decoding.\n\n    Budget and Prompt Length Computation. We compute input token budget usage by applying each\n    LLMâ€™s respective tokenizer and count the resulting number of tokens. Similarly, the prompt length\n    penalty is also computed based on the number of tokens produced by the respective tokenizer. In\n     contrast, to compute the prompt lengths reported in our results (Section 6 and Appendix F to I), we\n    count the number of words in a prompt separated by whitespace to ensure comparability between\n    LLMs.\n\n\n\n\n        8https://github.com/finitearth/promptolution (accessed: 2025-03-22)\n        9https://github.com/microsoft/PromptWizard (accessed: 2025-03-22)\n\n\n                                                                                                24\n\nD Input Specifications and Templates\n\nD.1 Task Descriptions\n\n\n              Table 6: Manually created task descriptions used for CAPO, OPRO, and PromptWizard.\n\n     SST-5:\n     The dataset consists of movie reviews with five levels of sentiment labels: very negative, negative, neutral, positive, and\n     very positive. The task is to classify each movie review into one of these five sentiment categories. The class will be\n      extracted between the markers <final_answer>answer/final_answer>.\n\n   AG News:\n    The dataset contains news articles categorized into four classes: World, Sports, Business, and Sci/Tech. The task\n       is to classify each news article into one of the four categories. The class will be extracted between the markers\n     <final_answer>answer</final_answer>.\n\n      Subj:\n    The dataset contains sentences labeled as either subjective or objective. The task is to classify each sentence as either\n      subjective or objective. The class will be extracted between the markers <final_answer>answer</final_answer>.\n\n    GSM8K:\n    The dataset consists of grade school math word problems that require multi-step reasoning to solve. The task is to\n     solve each word problem and provide the final answer. The final solution will be extracted between the markers\n     <final_answer>answer</final_answer>.\n\n     (Balanced) COPA:\n     The dataset consists of premises and two possible choices for the effect or cause of the premise. The task is to determine\n     which of the two choices (A or B) is the correct effect of the premise. The class will be extracted between the markers\n     <final_answer>answer</final_answer>.\n\n\nD.2 Initial Instructions\n    Since both CAPO and EvoPrompt require initial instructions to start from, we create a set of 15\n      initial instructions for each task. To demonstrate that this requirement of initial instructions is not\n    a major limiting factor of the algorithms, we produce them in an automated manner, prompting\n    Anthropicâ€™s Claude Sonnet 3.7 (https://claude.ai/) to create a diverse set of initial instructions,\n    making use of our task descriptions in Appendix D.1. The full prompt template is provided in\n    Table 7. Alternatively, approaches like APE (Zhou et al., 2023) could be employed to generate initial\n     instructions, or they could be manually engineered, e.g., by domain experts, to incorporate specific\n     prior knowledge. Examples of our initial instructions with corresponding test scores are given in\n    Appendix J.1.\n\n       Table 7: Prompt used to generate initial instructions with Anthropicâ€™s Claude Sonnet 3.7. The\n       <task_description> placeholder is replaced with our task description.\n\n     Please create diverse prompts for the following task. They should be linguistically diverse (but always in English) and\n     have varying lengths and complexities. This means some consist only of a short sentence with a rather high-level\n      description while others elaborate on the task in little more detail.\n     Task: <task_description>\n      Explicitly state this expected format as part of the prompts. Create overall 15 prompts within quotes as an array:\n\n\n    To generate generic, task-unspecific instructions for ablation study V. in Section 6.2, we use the\n    â€œtask descriptionâ€ in Table 8.\n\n                    Table 8: Task Description for generation of â€œgenericâ€ initial instructions.\n\n     Create prompts that are so generic, they could work for almost any task. The answers provided by the LLM should be\n     contained within <final_answer> </final_answer>.\n\n\n\n\n                                                                                                25\n\nD.3 Meta-Prompt Templates\n\n\n        Table 9: List of all meta-prompt templates used in CAPO and EvoPromptGA. The purple text indicates\n        placeholders where the according elements are inserted.\n\n    CAPO cross-over meta-prompt template:\n     You receive two prompts for the following task: <task_description>\n     Please merge the two prompts into a single coherent prompt. Maintain the key linguistic features from both original\n     prompts:\n     Prompt 1: <mother>\n     Prompt 2: <father>\n\n     Return the new prompt in the following format:\n     <prompt>new prompt</prompt>.\n\n    CAPO mutation meta-prompt template:\n     You receive a prompt for the following task: <task_description>\n     Please rephrase the prompt, preserving its core meaning while substantially varying the linguistic style.\n     Prompt: <instruction>\n\n     Return the new prompt in the following format:\n     <prompt>new prompt </prompt>\n\n     Original EvoPromptGA meta-prompt template from Guo et al. (2024):\n     Please follow the instruction step-by-step to generate a better prompt.\n       1. Crossover the following prompts and generate a new prompt:\n     Prompt 1: Rewrite the input text into simpler text.\n     Prompt 2: Rewrite my complex sentence in simpler terms, but keep the meaning.\n       2. Mutate the prompt generated in Step 1 and generate a final prompt bracketed with <prompt> and <prompt>.\n\n       1. Crossover Prompt: Rewrite the complex text into simpler text while keeping its meaning.\n       2. <prompt>Transform the provided text into simpler language, maintaining its essence.<prompt>\n\n     Please follow the instruction step-by-step to generate a better prompt.\n       1. Crossover the following prompts and generate a new prompt:\n     Prompt 1: <prompt1>\n     Prompt 2: <prompt2>\n       2. Mutate the prompt generated in Step 1 and generate a final prompt bracketed with <prompt> and <prompt>.\n\n       1.\n\n    EvoPromptGA simplified meta-prompt template used in the ablation study in Appendix I.3:\n     You receive two prompts for the following task: <task_description>\n       1. Please merge the two prompts into a single coherent prompt. Maintain the key linguistic features from both original\n     prompts:\n     Prompt 1: <prompt1>\n     Prompt 2: <prompt2>\n\n       2. Please rephrase the prompt generated in step 1, preserving its core meaning while substantially varying the linguistic\n       style.\n     Return the final prompt in the following format:\n     <prompt>final prompt<prompt>\n\n   CAPO performs cross-over and mutation separately, each with its own template, while Evo-\n   PromptGA (Guo et al., 2024) executes both operations with a single meta-prompt. We emphasize\n     that the CAPO prompts are simplified and substantially shorter, i.e., need fewer input tokens, and\n    they do not require any notion of what is a â€œgoodâ€ prompt by avoiding terms like â€œbetterâ€. In an\n     additional experiment described in Appendix I.3, we use our simplified CAPO templates within\n    EvoPromptGA. For this purpose, we combine them into a single meta-prompt also shown in Table 9.\n\n\n\n                                                                                                26\n\nE Examples of CAPO Algorithm Operations\n\nE.1 Cross-over and Mutation Examples\n\n\n       Table 10: Concrete examples for cross-over and mutation with Mistral-Small-24B on COPA using\n      CAPO. The purple text indicates the filled placeholders from the templates, green text marks the\n        resulting response of the LLM.\n\n     Crossover:\n    You receive two prompts for the following task: The dataset consists of premises and two possible choices for the effect\n     or cause of the premise. The task is to determine which of the two choices (A or B) is the correct effect of the premise.\n    The class will be extracted between the markers <final_answer>answer</final_answer>.\n\n     Please merge the two prompts into a single coherent prompt. Maintain the key linguistic features from both original\n     prompts:\n    Prompt 1: Select the statement that represents the most reasonable causal relationship to the given context. Respond\n     with <final_answer>A</final_answer> or <final_answer>B</final_answer> only.\n    Prompt 2: Based on causal reasoning, which is more plausible: A or B? Enclose your answer with <final_answer> tags\n      like this: <final_answer>A</final_answer> or <final_answer>B</final_answer>.\n\n     Return the new prompt in the following format:\n    <prompt>new prompt</prompt>\n\n     <prompt>Based on causal reasoning, select the statement that represents the most reasonable causal relationship\n     to the given context. Which is more plausible: A or B? Enclose your answer with <final_answer> tags like this:\n    <final_answer>A</final_answer> or <final_answer>B</final_answer>.</prompt>\n\n     Mutation:\n    You receive a prompt for the following task: The dataset consists of premises and two possible choices for the effect or\n     cause of the premise. The task is to determine which of the two choices (A or B) is the correct effect of the premise. The\n      class will be extracted between the markers <final_answer>answer</final_answer>.\n     Please rephrase the prompt, preserving its core meaning while substantially varying the linguistic style.\n     Prompt: Select the statement that represents the most reasonable causal relationship to the given context. Respond with\n    <final_answer>A</final_answer> or <final_answer>B</final_answer> only.\n\n     Return the new prompt in the following format:\n    <prompt>new prompt</prompt>\n\n     <prompt>Identify the statement that best aligns with the cause of the given context.  Provide your response as\n    <final_answer>A</final_answer> or <final_answer>B</final_answer> only.</prompt>\n\n\n\n\n\n                                                                                               27\n\nE.2 Exemplary Few-Shot Examples With and Without Reasoning\n\n\n       Table 11: Concrete examples for few-shot examples in CAPO with and without reasoning using\n       Llama-3.3-70B on GSM8K and Subj.\n\n   GSM8K - Few-shot example with reasoning:\n     Input: In 10 years, Terry will be 4 times the age that Nora is currently. If Nora is currently 10 years old, how old is Terry\n    now?\n     Output: To solve this problem, letâ€™s break it down step by step.\n      1. We know that Nora is currently 10 years old.\n      2. In 10 years, Terry will be 4 times the age that Nora is currently. Since Nora is currently 10 years old, 4 times her\n     current age is 4 * 10 = 40.\n      3. This means that in 10 years, Terry will be 40 years old.\n      4. To find Terryâ€™s current age, we need to subtract 10 years from the age Terry will be in 10 years. So, Terryâ€™s current\n     age is 40 - 10 = 30.\n     Therefore, <final_answer>30</final_answer> is Terryâ€™s current age.\n\n   GSM8K - Few-shot example without reasoning:\n     Input: Kendra has 4 packs of pens. Tony has 2 packs of pens. There are 3 pens in each pack. If Kendra and Tony decide\n     to keep two pens each and give the remaining pens to their friends one pen per friend, how many friends will they give\n     pens to?\n     Output: <final_answer>14</final_answer>\n\n     Subj - Few-shot example with reasoning:\n     Input: gangs , despite the gravity of its subject matter , is often as fun to watch as a good spaghetti western .\n     Output: The given sentence is subjective because it expresses a personal opinion by comparing the experience of\n     watching \"gangs\" to a \"good spaghetti western\" and describing it as \"fun to watch.\" This comparison and the use of the\n    word \"fun\" introduce a personal judgment about the entertainment value of the subject matter, which may vary from\n     person to person.\n     <final_answer> Subjective </final_answer>\n\n     Subj - Few-shot example without reasoning:\n     Input: this holds particularly true for blacky , a white teen who is more interested in books than sport , and his best\n     friend , dumby , the aboriginal star of the team .\n     Output: <final_answer>objective</final_answer>\n\n\n\n\n\n                                                                                               28\n\nF Hyperparameter Sensitivity Analysis\n   In this section, we investigate the univariate effects of hyperparameters in CAPO. The hyperpa-\n  rameters we alter are the length penalty ğ›¾(0.0, 0.01, 0.02, 0.05, 0.1), significance level ğ›¼(0.05, 0.1,\n   0.2, 0.5), population size ğœ‡(6, 8, 10, 12), cross-overs per iteration ğ‘(4, 7, 10), and whether we shuffle\n   the blocks in racing or not. In each case, we hold all other hyperparameters fixed to their defaults\n   (cf. Appendix C.4). Thus, multivariate dependencies are not considered here. All experiments are\n  conducted with Llama-3.3-70B model on two datasets (AG News and GSM8K). The budget is limited\n   to 5M input tokens, and each configuration is executed with three different seeds. We summarize\n  our results in Table 12.\n     The results indicate that our default parameters are not optimal for neither AG News nor\n  GSM8K as they are outperformed by other parametrizations. However, performance differences for\n   all parameter variations lie within one standard deviation. We conclude that while hyperparameters\n   influence the final performance, their impact is rather moderate.  Since changing individual\n  parameters affects not only the final performance but also the behavior of the optimization process,\n  we provide test score curves below.\n\n\n   Table 12: Hyperparameter sensitivity analysis of various CAPO parametrizations with Llama-3.3-70B after\n  5M input tokens. We report mean accuracy (in %) with standard deviations on test set for the best prompts\n   across three seeds. The best prompt per seed is selected from the final population based on the available\n  development set scores. Hyperparameters are varied univariately, keeping all other parameters at their\n   defaults. Bold values indicate best performance for each parameter and task.\n\n                                Parametrization  AG News  GSM8K     Avg.\n\n                            ğ›¾=0                89.27Â±0.41   74.93Â±1.04   82.10\n                                ğ›¾=0.01             89.53Â±0.25   75.27Â±3.10   82.40\n                                ğ›¾=0.02             89.20Â±0.43   74.20Â±3.28   81.70\n                                ğ›¾=0.05 (default)    88.80Â±0.75   73.37Â±3.73   81.27\n                                ğ›¾=0.1              88.73Â±1.11   74.80Â±3.15   81.77\n\n                                ğ›¼=0.05             89.20Â±0.59   73.87Â±2.17   81.53\n                                ğ›¼=0.1              88.93Â±0.62   74.87Â±2.79   83.00\n                                ğ›¼=0.2 (default)     88.80Â±0.75   73.73Â±3.73   81.90\n                                ğ›¼=0.5              87.40Â±2.37   75.93Â±1.51   81.67\n\n                             ğœ‡=6                89.00Â±0.49   77.67Â±3.03   83.33\n                             ğœ‡=8                88.33Â±0.25   77.67Â±3.74   83.00\n                              ğœ‡=10 (default)      88.80Â±0.75   73.73Â±3.73   81.27\n                              ğœ‡=12              89.33Â±0.19   76.87Â±1.31   83.10\n\n                              ğ‘=4 (default)       88.80Â±0.75   73.73Â±3.73   81.27\n                              ğ‘=7                89.47Â±0.25   73.07Â±1.64   81.27\n                              ğ‘=10               89.53Â±0.19   74.40Â±3.30   81.97\n\n                           w/ shuffling        89.60Â±0.28   76.73Â±1.81   83.17\n                            w/o (default)       88.80Â±0.75   73.73Â±3.73   81.27\n\n  A smaller length penalty ğ›¾naturally improves performance (cf. Figure 5) since the prompt length\n  becomes less influential to the optimization process allowing for longer, often better performing\n  prompts. Figure 6 shows that for larger length penalties, prompt lengths decrease as optimization\n  advances before stabilizing, which aligns with expected behavior. However, a trade-offexists since\n  long prompts consume significant portions of the budget and therefore permit fewer steps within\n   the same budget constraints.\n     The choice of the significance level ğ›¼used in the paired t-test for racing governs how strictly\n  underperforming prompts are eliminated: lower values lead to more conservative eliminations\n  while higher values allow for more aggressive pruning. As shown in Table 12, ğ›¼= 0.1 yields\n\n\n                                                                                              29\n\nthe highest average performance across tasks with our default setting of ğ›¼= 0.2 still within one\nstandard deviation. Notably, ğ›¼= 0.05 performs best on AG News, whereas ğ›¼= 0.5 achieves the\ntop result on GSM8K. Nonetheless, while ğ›¼does influence optimization dynamics, especially the\ntrade-offbetween exploration and premature elimination, its overall effect on final performance\nremains moderate. The optimization curves in Figure 7 illustrate the effect of ğ›¼: while larger\nvalues allow for more steps as prompts are eliminated early, the corresponding optimization curves\nincrease more slowly (for GSM8K) or not all (for AG News) as the probability of falsely eliminating\na good prompt is considerably higher.\n   Choosing the optimal population size ğœ‡depends on the task. Large ğœ‡improves performance\non AG News while a small ğœ‡is beneficial on GSM8K. Looking at Table 12, we observe that this\nhyperparameter choice has the largest impact on the average test set performance of the best\ncandidates per seed. The smaller the population size, the more steps can be performed, which is\nagain a trade-off. For small population sizes, there is a danger of getting â€œstuckâ€ when there is\ninsufficient diversity in the prompts to create new explorative candidates. We can see this effect in\nFigure 8 for AG News at ğœ‡= 6. We also observe a larger standard deviation for smaller population\nsizes.\n   The number of cross-overs per iteration has a minor influence on final performance. On our\ntwo datasets, we observe slight improvements for larger ğ‘. In general, for smaller ğ‘, more steps are\npossible and standard deviations are smaller (cf. Figure 9). An important consideration is that with\nlarge ğ‘, promising prompts from previous populations are more likely to be erroneously eliminated\nin racing despite being superior, as it may be eliminated on early blocks.\n    Shuffling the blocks during racing slightly improves the performance on both tasks. A potential\nexplanation is that shuffling prevents overfitting to early blocks. However, this approach has the\ndrawback that fewer steps are possible (cf. Figure 10) since we cannot always use cached evaluations\nand therefore cannot perform as many steps as without shuffling.\n\n\n\n\n\n                       (a) AG News.                                               (b) GSM8K.\n\nFigure 5: Population mean test scores over steps with Llama-3.3-70B. Mean and standard deviations are\ncomputed across seeds. We univariately vary the length penalty ğ›¾keeping all other parameters at their\ndefaults.\n\n\n\n\n\n                                                                                           30\n\n(a) AG News.                                               (b) GSM8K.\n\nFigure 6: Population mean prompt lengths over steps with Llama-3.3-70B. Mean and standard deviations\nare computed across seeds. We univariately vary the length penalty ğ›¾keeping all other parameters at their\ndefaults.\n\n\n\n\n\n                       (a) AG News.                                               (b) GSM8K.\n\nFigure 7: Population mean test scores over steps with Llama-3.3-70B. Mean and standard deviations are\ncomputed across seeds. We univariately vary the significance level ğ›¼keeping all other parameters at their\ndefaults.\n\n\n\n\n\n                       (a) AG News.                                               (b) GSM8K.\n\nFigure 8: Population mean test scores over steps with Llama-3.3-70B. Mean and standard deviations are\ncomputed across seeds. We univariately vary the population size ğœ‡keeping all other parameters at their\ndefaults.\n\n\n\n\n                                                                                           31\n\n(a) AG News.                                               (b) GSM8K.\n\nFigure 9: Population mean test scores over steps with Llama-3.3-70B. Mean and standard deviations are\ncomputed across seeds. We univariately vary the number of crossovers ğ‘keeping all other parameters at\ntheir defaults.\n\n\n\n\n\n                       (a) AG News.                                               (b) GSM8K.\n\nFigure 10: Population mean test scores over steps with Llama-3.3-70B. Mean and standard deviations are\ncomputed across seeds. We compare CAPO with vs. without (default) shuffling of the blocks during racing\nCAPO.\n\n\n\n\n\n                                                                                           32\n\nG CAPO Detailed Analysis\n\nG.1 Token Usage Breakdown of Evaluation-LLM vs. Meta-LLM\n     In the following, we analyze the proportion of input tokens consumed by the evaluation-LLM\n    compared to the meta-LLM across various tasks (cf. Table 13). On average, the evaluation-LLM\n    accounts for 96.6% of the total token usage. For most datasets, this proportion exceeds 98%. The\n    only notable exception is COPA, for which the share of the evaluation-LLM drops slightly for some\n    models. This consistently high share justifies our approach of mainly considering the evaluation-\n   LLM in our cost considerations.\n\n     Table 13: Proportion of input tokens consumed by the evaluation-LLM compared to the meta-LLM across\n     the benchmark experiments in Section 6.1.\n\n                          Model  AG News  COPA  GSM8K   SST-5   Subj\n                           Llama    97.9 %      98.4%    99.6%     98.9%   98.6%\n                                Mistral   98.8%       73.4%    99.7%     99.1%   99.0%\n                      Qwen    99.0%       89.5%    99.6%     99.1%   98.8%\n\n\n\nG.2 Influence of Few-Shot Examples on Prompt Length\n   We find that CAPO allocates prompt length adaptively across tasks, with few-shot examples\n     contributing substantially more to the total prompt length on complex tasks. For instance, on\n    GSM8K, over 80% of the final prompt length stems from few-shot examples, compared to less than\n    52% on COPA. Averaged across all datasets, few-shot examples account for 66% of the total prompt\n     length.\n\n     Table 14: Instruction length, few-shot length, and percentage of few-shot content of the best prompts\n     generated by CAPO across different tasks. Mean and standard deviation are computed across three seeds.\n    The best prompt per seed is selected from the final population based on development set scores. The system\n    prompts are counted as part of the instructions.\n\n                    Model            Task        Instruction   Few-shots  % Few-shots\n\n                                          SST-5      52Â±24       109Â± 61     68Â± 3\n                             AG News   76Â±41        34Â± 25     31Â±25\n                                           Subj       56Â±27       102Â± 22     65Â±16\n                        Llama-3.3-70B\n                               GSM8K    37Â±14        444Â±126     92Â± 1\n                               COPA      40Â±21        43Â± 29     51Â±23\n\n                                          SST-5      74Â± 7       114Â± 21     61Â±15\n                             AG News   63Â±30        53Â± 28     46Â±28\n                                           Subj       86Â±14        72Â± 13     46Â± 8\n                     Qwen2.5-32B\n                               GSM8K    40Â± 8       190Â± 98     83Â±18\n                               COPA      97Â±47         8Â± 10      8Â± 7\n\n                                          SST-5      56Â± 1        86Â± 19     61Â±20\n                             AG News   56Â±19        98Â± 69     64Â±35\n                                           Subj       35Â± 8       103Â± 35     75Â± 5\n                         Mistral-Small-24B\n                               GSM8K    38Â± 9       247Â± 18     87Â± 2\n                               COPA     59Â± 3        18Â± 24     23Â±20\n\n\n\n\n\n                                                                                                33\n\nG.3 Prompt Survival Analysis\n    Figure 11 shows how the population evolves over multiple steps for two examples with different\n    models and datasets. The visualization tracks test performance for all population members, distin-\n    guishing between surviving prompts, newly proposed candidates, and eliminated (killed) prompts\n     in each step.\n        In the early optimization phases, we observe the generation of relatively low-performing\n    prompts, which the algorithm correctly eliminates. As optimization progresses, the quality of\n    newly proposed prompts gradually improves. Since the algorithm does its selection based on the\n    development set scores it can happen that a prompt, which would have performed better on the\n     test set, gets eliminated (cf. Figure 11a).\n\n\n\n\n\n                  (a) Mistral-Small-24B on SST-5.                             (b) Qwen2.5-32B on Subj.\n\n     Figure 11: Test scores of all population members over steps of default CAPO for one seed (42). Every time\n     a prompt is newly proposed or gets killed this is indicated by a special marker. The line at the upper end\n    shows the progression of the current best prompt.\n\n\n\n\n\n                                                                                                34\n\nH Further Benchmark Results\n\nH.1 Performance Profile\n    The performance profile plot displays the frequency ğœŒ(ğœ) of an optimization algorithm producing an\n     instance with a performance difference of ğœto the best performing instance. For each dataset-model\n      pair, we compute the average performance across seeds, using the best-performing prompts selected\n    from the final optimization step on the dev-set. Each of these averaged results serves as an instance\n     in our analysis. While the original proposal introduced by Dolan and MorÃ© (2002) uses the ratio to\n    the maximum performance, we follow Agarwal et al. (2024) and Lin et al. (2024) and report the\n     difference to the best performing prompt, as the accuracy metric is bounded between 0 and 1.\n       Thus we get for distance ğœ, optimizer Î¨, performance on task ğ‘–with optimizerğœ“ğœğ‘–,ğœ“and number\n     of tasks ğ‘›:\n                                        1  ğ‘›\n                                ğœŒÎ¨(ğœ) =  âˆ‘ï¸ I[ğœğ‘–,max âˆ’ğœğ‘–,Î¨ â‰¤ğœ].                                  (3)                                    ğ‘›\n                                                       ğ‘–=1\n        Therefore, ğœŒÎ¨(0) indicates the frequency of optimizer Î¨ producing the best instance per task.\n     Figure 12 shows, that with a ğœŒCAPO(0.012) = 1 we are within 1.2 %p of the best performing instance\n     in every single task-model pair.\n\n\n\n\n\n                         Figure 12: Performance profiles of all benchmarked optimizers.\n\n\n\n\n\n                                                                                                35\n\nH.2 Further Optimization Curves from Benchmark Experiments\n\n\n\n\n\n                     (a) Llama-3.3-70B on SST-5.                              (b) Qwen2.5-32B on SST-5.\n\n\n\n\n\n                   (c) Mistral-Small-24B on SST-5.                          (d) Llama-3.3-70B on AG News.\n\n\n\n\n\n                   (e) Qwen2.5-32B on AG News.                           (f) Mistral-Small-24B on AG News.\n\n\n\n\n\n                    (g) Llama-3.3-70B on Subj.                              (h) Qwen2.5-32B on Subj.\n\n\n\n\n\n                                                                                                36\n\n(i) Mistral-Small-24B on Subj.                               (j) Llama-3.3-70B on GSM8K.\n\n\n\n\n\n             (k) Qwen2.5-32B on GSM8K.                                 (l) Mistral-Small-24B on GSM8K.\n\n\n\n\n\n          (m) Llama-3.3-70B on COPA.                            (n) Qwen2.5-32B on COPA.\n\n\n\n\n\n                                          (o) Mistral-Small-24B on COPA.\n\n\nFigure 13: Population mean test scores over input tokens from benchmark experiments for all datasets and\nmodels. Mean and standard deviations are computed across seeds. PromptWizard produces prompts only\nonce after a small number of input tokens, marked with a star (mean) and error bars (std). If an algorithm\nconverges (i.e., when it outputs the same prompts each step, which can happen for OPRO), we continue the\ncurve with a dashed horizontal line and hatched area.\n\n\n\n\n                                                                                           37\n\nH.3 Prompt Lengths from Benchmark Experiments\n\n\n     Table 15: Mean prompt length with standard deviation of the best prompts for different optimization methods,\n     datasets, and models. Mean and standard deviation are computed across three seeds. The best prompt per\n     seed is selected from the final population based on the available development set scores (for CAPO: penalized\n     average block scores of evaluated blocks). Bold values indicate shortest prompts. The system prompts are\n     counted as part of the prompt.\n\n                Model         Optimizer       SST-5    AG News   Subj     GSM8K   COPA      Avg.\n\n                                             Initial           33Â±  5      35Â±  6    31Â±  8    29Â± 7      30Â±  5    32\n                        OPRO           63Â± 22      32Â±  4    42Â±  4    58Â± 15     33Â±  7    46\n                   Llama-3.3-    PromptWizard   563Â± 36    1106Â±265    863Â±400   544Â±173    613Â± 33   738\n                70B          EvoPromptGA    33Â±  2      30Â±  1    28Â±  2    28Â±  2     32Â±  2    29\n                        CAPO (ours)     161Â± 85     110Â± 46    158Â± 12   481Â±113     83Â± 22   199\n\n                                             Initial            33Â±  5     35Â±  6    31Â±  8    29Â±  7     30Â±  5    32\n                        OPRO            38Â±  5     37Â±  8    33Â±  5    27Â±  2     51Â± 14    37\n                Qwen2.5-     PromptWizard    677Â±517    753Â±541   297Â± 22   698Â±392    337Â± 32   552\n                32B          EvoPromptGA     37Â±  4     35Â±  6    35Â±  5    25Â±  6     40Â±  9    34\n                        CAPO (ours)      187Â± 28    116Â± 56    158Â± 13   230Â± 89    105Â± 49   159\n\n                                             Initial            33Â±  5     35Â±  6    31Â±  8    29Â±  7     30Â±  5    32\n                        OPRO            29Â±  2     44Â±  7    26Â±  0    32Â± 10     36Â±  5    33\n                    Mistral-       PromptWizard   1027Â±246    544Â±214    701Â±297   579Â±112   1139Â±188   798\n                  Small-24B     EvoPromptGA     29Â±  2     39Â±  9    26Â±  1    20Â±  1     31Â±  2    29\n                        CAPO (ours)      142Â± 21    153Â± 78    138Â± 39   286Â± 24     76Â± 27   159\n\n\n\n\n\n                                                                                                38\n\nI Further Ablation Results\n\nI.1 Optimization Curves from Ablation Studies\n   For all plots of the mean test scores over input tokens, the mean and standard deviations are\n   computed across seeds. If an algorithm run terminates early, we continue the curve with a dashed\n    horizontal line and hatched area.\n\n\n\n\n\n                            (a) AG News.                                               (b) GSM8K.\n\n    Figure 14: Population mean test scores over input tokens with Llama-3.3-70B. We compare CAPO with no\n    few-shot included to the default CAPO and EvoPromptGA.\n\n\n\n\n\n                            (a) AG News.                                               (b) GSM8K.\n\n    Figure 15: Test score vs. prompt length for every prompt with Llama-3.3-70B. A star marks the final selected\n   prompt per seed (best performing from last step based on available dev scores). Prompt length includes\n    both the number of tokens in the system prompt and (user) prompt. We compare CAPO with no few-shot\n    included to the default CAPO and EvoPromptGA.\n\n\n\n\n\n                            (a) AG News.                                               (b) GSM8K.\n\n    Figure 16: Population mean test scores over input tokens with Llama-3.3-70B. We compare CAPO without\n    racing (one block with ğ‘= |Ddev|) with the default CAPO.\n\n\n\n\n                                                                                               39\n\n(a) AG News.                                               (b) GSM8K.\n\n    Figure 17: Popultation mean test scores over input tokens with Llama-3.3-70B. We compare CAPO without\n    racing, without few-shot examples, and without length penalty to default CAPO.\n\n\n\n\n\n                            (a) AG News.                                               (b) GSM8K.\n\n    Figure 18: Population mean test scores over input tokens with Llama-3.3-70B. CAPO and EvoPromptGA\n    started with generic, task-unspecific prompts.\n\n\nI.2 Impact of Racing\n    In Figure 19, we compare the required input token budget per step for CAPO (w/ racing), CAPO\n   w/o racing, and EvoPromptGA on AG News with Llama-3.3-70B. All three optimizers require a\n    large number of tokens in the first step. This is due to the additional evaluation of initial prompts\n   on top of the candidates of the first step. Both EvoPromptGA and CAPO w/o racing remain at\n   a constant rate afterwards. While CAPO w/o racing benefits from the prompt-evaluation-cache\n   but suffers from long prompts potentially including few-shots, EvoPrompt has short prompts\n   but no cache. Both effects seem to cancel out and the required input tokens stay at a constant\n    rate of about 250k input tokens per step, allowing for roughly 19 optimization steps. In contrast,\n    the CAPO budget requirement is already low at the beginning, as it does not necessarily need to\n    evaluate the candidates on the entire dev set, terminating poor candidates early through racing. The\n    required budget decreases further after 3 steps and stays roughly constant with small fluctuations\n   around 100k tokens per step, allowing for over 70 steps with the same budget. These observations\n   underscore the benefits of racing in terms of cost-efficiency.\n\n\n\n\n\n                                                                                               40\n\nFigure 19: Sum of input tokens required per optimization step of Llama-3.3-70B on AG News. Mean and\nstandard deviations are computed across seeds. We compare default CAPO, EvoPromptGA and CAPO\nwithout racing.\n\n\nThis conclusion is further supported by Table 16, where we compare the actual block evaluations\nrequired for CAPO with racing to the theoretical evaluations required if each prompt had been\nevaluated on all blocks. In the example of Figure 19, we save around 50% of evaluations. On average,\nwe save 44% of evaluations over all datasets and models.\n\nTable 16: Evaluated blocks per model and dataset with racing vs. number of required blocks that would have\nbeen required if prompts had been evaluated across all 10 blocks, averaged over seeds. We calculate how\nmany blocks (in %) were saved by using racing.\n\n                    Dataset    Model           w/ racing  w/o racing   savings (%)\n\n              AG News   Llama-3.3-70B            929.0        1886.7         50.76\n                                 Mistral-Small-24B        608.3        1356.7         55.16\n                             Qwen2.5-32B            707.0        1310.0         46.03\n\n              COPA      Llama-3.3-70B            804.7        1690.0         52.39\n                                 Mistral-Small-24B        754.7        1273.3         40.73\n                             Qwen2.5-32B            948.7        1566.7         39.45\n\n               GSM8K     Llama-3.3-70B            317.7         630.0         49.58\n                                 Mistral-Small-24B        314.0         456.7         31.24\n                             Qwen2.5-32B            376.7         633.3         40.53\n\n                    SST-5      Llama-3.3-70B            832.7        1316.7         36.76\n                                 Mistral-Small-24B        703.3        1093.3         35.67\n                             Qwen2.5-32B            836.3        1070.0         21.84\n\n                    Subj        Llama-3.3-70B            648.3        1566.7         58.62\n                                 Mistral-Small-24B        625.0        1260.0         50.40\n                             Qwen2.5-32B            672.7        1360.0         50.54\n\n                   Avg.                                 671.9        1231.3         43.98\n\n\n\n\n\n                                                                                           41\n\nI.3 Influence of Meta-Prompt Simplification and Task Descriptions\n   To investigate the influence of our meta-prompt simplification, we perform an additional experiment\n   with EvoPromptGA using our simplified CAPO meta-prompts, including a task description. Since\n   EvoPromptGA uses only a single meta-prompt and LLM call to perform both cross-over and\n    mutation, we combine our CAPO cross-over and mutation prompt into a single meta-prompt. For\n    details, we refer to Appendix D.3. In Figure 20, we compare optimization curves for standard\n   EvoPromptGA and EvoPromptGA with our simplified template. We observe that performance\n   with our simplified template is slightly worse compared to the original template. Nonetheless, it is\n   important to mention that our templates are substantially shorter in terms of number of tokens.\n   Thus, this experiment indicates that the choice of the meta-prompt template is also a trade-off\n   between performance and cost.\n\n\n\n\n\n                            (a) AG News.                                               (b) GSM8K.\n\n    Figure 20: Population mean test scores over input tokens with Llama-3.3-70B. Mean and standard deviations\n    are computed across seeds. We compare the performance of EvoPromptGA with default meta-prompts (Guo\n    et al., 2024) to EvoPromptGA with our combined CAPO meta-prompts.\n\n\n\n\n\n                                                                                               42\n\nJ Best Prompts per Tasks\n    In the following, we report the best prompts per optimizer with Llama-3.3-70B for each dataset.\n   The displayed prompts yield the best test-set performance across all seeds. Note that this section\n    serves primarily to provide illustrative insights and examples of generated prompts rather than to\n    report performance metrics.\n\nJ.1 Initial Prompts\n\n\n    Table 17: Best initial prompts by test scores with Llama-3.3-70B and three exemplary generic prompts. For a\n     full list of all initial prompts, we refer to our research repository.\n\n   AG News (88.6%):\n    Read the following news text and determine which category it belongs to. Choose from: World, Sports, Business, or\n     Sci/Tech. Your final answer must be enclosed in <final_answer> </final_answer> tags for automated extraction.\n\n   COPA (99.2%):\n     Select the statement that represents the most reasonable causal relationship to the given context. Respond with\n    <final_answer>A</final_answer> or <final_answer>B</final_answer> only.\n\n   GSM8K (52.2%):\n    Iâ€™m struggling with this math word problem that needs multiple steps to solve. Can you help? Make sure to put your\n     final answer between <final_answer> </final_answer> tags so I can easily find it.\n\n    SST-5 (60.4%):\n    Movie review sentiment classification task: From the following five options - very negative, negative, neutral, pos-\n      itive, or very positive - which best describes this review? Your answer must appear between <final_answer> and\n    </final_answer> markers.\n\n    Subj (70.0%):\n     Evaluate this sentence and determine if itâ€™s presenting objective information (facts that can be verified) or subjective con-\n     tent (opinions, judgments, or emotions). Provide your classification inside <final_answer> </final_answer> markers.\n\n    Generic Prompt\n     Letâ€™s think step by step. Your answer should be enclosed within <final_answer> </final_answer> tags.\n\n    Generic Prompt\n    Give me your response within <final_answer> tags.\n\n    Generic Prompt\n     Please provide a thoughtful answer to my question and wrap your response in <final_answer> tags so I can easily\n     identify it.\n\n\n\n\n\n                                                                                               43\n\nJ.2 CAPO Prompts\n\n\n          Table 18: Best prompts of CAPO by test scores, optimized and evaluated with Llama-3.3-70B.\n\n   AG News (91.0%):\n   We have a collection of news stories that need to be sorted into categories. Your task is to read the provided article and\n    determine whether it falls under the category of World, Sports, Business, or Sci/Tech news. Once youâ€™ve made your\n     decision, please enclose your chosen category in <final_answer>answer</final_answer> tags for easy identification.\n    +2 few shots\n   COPA (99.8%):\n    To evaluate your ability to reason about cause-and-effect relationships, this task presents you with a scenario and\n    asks you to identify the most plausible consequence or antecedent. Given a premise, assess the two provided op-\n     tions, labeled A and B, and select the one that logically follows or precedes the premise, responding with either\n    <final_answer>A</final_answer> or <final_answer>B</final_answer> to indicate your choice. +2 few shots\n   GSM8K (79.2%):\n    To tackle this math word problem, which demands a series of logical steps, dissect it methodically. Outline your thought\n     process and ensure you clearly signify your solution, enclosing it within <final_answer> </final_answer> markers for\n    easy identification. +2 few shots\n    SST-5 (63.6%):\n    Assess the emotional tone conveyed in the provided movie review, then categorize it into one of five sentiment levels:\n    very negative, negative, neutral, positive, or very positive, and encapsulate your chosen category within <final_answer>\n    </final_answer> tags, following this format: <final_answer> selected_sentiment </final_answer>, to clearly denote\n     the sentiment classification of the film review. +2 few shots\n    Subj (94.6%):\n    Label each sentence as either a statement of fact that can be proven or disproven, or a reflection of personal feelings,\n     opinions, or biases, by categorizing it as <final_answer>objective</final_answer> if it contains information that can\n    be verified, or <final_answer>subjective</final_answer> if it expresses emotions, attitudes, or individual evaluations,\n    and respond with one of these two classifications. +4 few shots\n\n\n\nJ.3 EvoPromptGA Prompts\n\n\n      Table 19: Best prompts of EvoPromptGA by test scores, optimized and evaluated with Llama-3.3-70B.\n\n   AG News (90.0%):\n    Categorize the given news article into its relevant category (World, Sports, Business, or Sci/Tech) and provide your\n     classified response within <final_answer> tags for easy identification.\n\n   COPA (99.4%):\n    Use commonsense knowledge to identify the causally related option (A or B) to the given statement and respond with\n    <final_answer>A</final_answer> or <final_answer>B</final_answer>.\n\n   GSM8K (53.8%):\n     Assist with solving the elementary or grade school level math problem that requires multiple steps and provide the\n     solution within <final_answer> </final_answer> tags for easy identification.\n\n    SST-5 (63.0%):\n    Evaluate the sentiment of the given movie review and categorize it as very negative, negative, neutral, positive, or very\n     positive, enclosing the chosen category within <final_answer> and </final_answer> tags.\n\n    Subj (78.8%):\n    Determine the subjectivity or objectivity of a sentence and provide the assessment enclosed in <final_answer> tags.\n\n\n\n\n\n                                                                                               44\n\nJ.4 OPRO Prompts\n\n          Table 20: Best prompts of OPRO by test scores, optimized and evaluated with Llama-3.3-70B.\n\n   AG News (89.4%):\n     Classify the news article into one of four categories (World, Sports, Business, Sci/Tech) based on its content, and provide\n    your answer in lowercase within <final_answer> tags for efficient data extraction and analysis, ensuring accuracy and\n     consistency in categorization, and enabling informed decision-making with a standardized format for optimal processing\n    and evaluation.\n\n   COPA (99.2%):\n     Select the statement that represents the most reasonable causal relationship to the given context. Respond with\n    <final_answer>A</final_answer> or <final_answer>B</final_answer> only.\n\n   GSM8K (56.0%):\n    To solve the math problem, provide a concise, logical, and step-by-step explanation that directly addresses the problem,\n     incorporating all necessary calculations and formulas. Ensure your reasoning is easy to follow and free of unnecessary in-\n     formation. Clearly present your final numerical answer within <final_answer> and </final_answer> tags, allowing for\n     effortless identification and verification of the solution. Utilize a well-structured approach that effectively communicates\n     the problemâ€™s resolution, enabling efficient understanding and validation of the mathematical solution.\n\n    SST-5 (63.0%):\n    Analyze the movie reviewâ€™s sentiment by identifying the emotional tone and language used, then categorize it as\n    very negative, negative, neutral, positive, or very positive, and provide your answer in the format: <final_answer>\n    sentiment_category </final_answer>, considering the context, tone, and emotional cues to accurately reflect the\n     reviewerâ€™s opinion in a concise and nuanced manner, ensuring your classification is informed by both the explicit and\n     implicit emotional expressions in the review.\n\n    Subj (75.4%):\n     Classify the sentence as â€™objectiveâ€™ if it presents factual information or â€™subjectiveâ€™ if it expresses personal opinions,\n    emotions, or biases, and provide your answer between <final_answer> tags, considering the sentenceâ€™s content, tone,\n    and purpose to inform a clear and accurate judgment.\n\n\n\nJ.5 PromptWizard Prompts\n  We only report a single prompt optimized by PromptWizard, including system prompt for Llama-\n    3.3-70B on AG News for the sake of conciseness and refer the interested reader to our research\n    repository.\n\n       Table 21: Best prompt of PromptWizard by test scores, optimized and evaluated with Llama-3.3-70B.\n\n   AG News (23.6%)\n    system prompt:\n    You are a natural language processing (NLP) specialist with expertise in text classification and machine learning.\n    You have extensive experience in developing and training models to categorize text into predefined categories. Your\n    knowledge of NLP techniques, such as tokenization, stemming, and named entity recognition, enables you to extract\n     relevant features from the news articles and classify them accurately. You are familiar with various machine learning\n     algorithms, including supervised and unsupervised learning methods, and can select the most suitable approach for this\n     task. With your expertise, you can analyze the dataset, identify the key characteristics of each category, and develop a\n    robust classification model that can accurately assign each news article to one of the four categories: World, Sports,\n     Business, or Sci/Tech. Your goal is to achieve high accuracy in classification, and you can use techniques such as\n     cross-validation and hyperparameter tuning to optimize the performance of the model. By extracting the class between\n    the markers <final_answer>answer</final_answer>, you can provide a clear and concise output that indicates the\n     predicted category for each news article.\n     user prompt:\n   What are the key assumptions underlying this news article classification task? To simplify the problem, letâ€™s start by\n     identifying the categories: World, Sports, Business, and Sci/Tech. How can we make progress on this problem? By\n    reading the news article and trying to classify it into one of the four categories, we can start making progress. Letâ€™s\n    make a list of ideas for solving this problem and apply them one by one to see if any progress can be made. Place your\n     classification within <final_answer> tags. +2 few shots\n\n\n\n                                                                                               45",
"headers": [
"arXiv:2504.16005v4  [cs.CL]  17 Jun 2025",
"CAPO: Cost-Aware Prompt Optimization",
"1 Introduction",
"2 Notation & Problem Statement",
"3 Related Work",
"4 CAPO: Cost-Aware Prompt Optimization",
"5 Experimental Setup",
"6 Results & Analysis",
"7 Conclusion & Future Work",
"8 Broader Impact Statement",
"References",
"Appendix",
"A Background",
"B Algorithm Details",
"C Technical Details",
"D Input Specifications and Templates",
"E Examples of CAPO Algorithm Operations",
"F Hyperparameter Sensitivity Analysis",
"G CAPO Detailed Analysis",
"H Further Benchmark Results",
"I Further Ablation Results",
"J Best Prompts per Tasks"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2504.16005v4.pdf"
}