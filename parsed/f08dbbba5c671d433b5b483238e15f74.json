{
"text": "The Art of Asking:\n\n       Multilingual Prompt Optimization for Synthetic Data\n\n\n\n         David Mora⋆1, Viraat Aryabumi2, Wei-Yin Ko2, Sara Hooker1,\n           Julia Kreutzer1, and Marzieh Fadaee1\n\n\n             1Cohere Labs, 2Cohere\n\n\n              Corresponding authors: {david.mora, juliakreutzer, marzieh}@cohere.com2025\nOct\n          Abstract\n22           Synthetic data has become a cornerstone for scaling large language models, yet its multilingual use\n                remains bottlenecked by translation-based prompts. This strategy inherits English-centric framing and\n                    style and neglects cultural dimensions, ultimately constraining model generalization. We argue that the\n                 overlooked prompt space—the very inputs that define training distributions—offers a more powerful lever\n                    for improving multilingual performance. We introduce a lightweight framework for prompt-space opti-[cs.CL]           mization, where translated prompts are systematically transformed for Naturalness, Cultural Adaptation,\n               and Difficulty Enhancement. Using an off-the-shelf multilingual LLM, we apply these transformations to\n               prompts for 12 languages spanning 7 families. Under identical data conditions, our approaches achieve\n                  substantial and consistent downstream improvements over the translation-only baseline: +4.7% on\n             Global-MMLU accuracy, +2.4% on Flores XCometXL and +35.3% wins in preferences on mArenaHard.\n           We establish prompt-space optimization as a simple yet powerful paradigm for building multilingual\n            LLMs that are more robust, culturally grounded, and globally capable.\n\n\n\n\n       1  Introduction\n\n          The field of synthetic data generation has largely operated under a generation-focused paradigm:arXiv:2510.19806v1       given existing prompts, optimize the quality of the generated completions [Long et al., 2024; Liu\n              et al., 2024], via e.g.  targeted filtering [Grattafiori et al., 2024; Shimabucoro et al., 2024], test-\n            time scaling [Muennighoff et al., 2025]. However, this paradigm implicitly inherits the limitations\n              of the prompt distribution: completions are only as diverse and representative as the inputs they\n             are conditioned on, and numerous studies show that prompts themselves can often be noisy or low\n             quality leading synthetic data to reinforce these deficiencies rather than systematically broadening\n            the training distribution [Schreiter, 2025; He et al., 2024].\n\n            This challenge is especially acute in the multilingual setting, where translation-based prompt ex-\n            pansion dominates instruction tuning [Üstün et al., 2024; Dang et al., 2024; Chen et al., 2024;\n            Martins et al., 2025]. While effective for scaling coverage, translations introduce artifacts such as\n            unnatural phrasing (translationese) [Lembersky et al., 2012; Eetemadi & Toutanova, 2014], lexical\n\n                  ⋆First author.\n\n\n             Released as a preprint on October 23, 2025         1\n\nFigure 1: Prompt transformations consistently improve over translations: Comparison of translated\nmodel and our most well-rounded method (Cultural+Difficulty Mix) across different multilingual benchmarks.\nmArenaHard and Polywrite win-rates are in direct comparison between the two models.\n\n\nerrors, or shifts in toxicity [Ermis et al., 2024]. Even high-quality translations project the seman-\ntics of the original English prompt into another language, but rarely adapt content for cultural\nrelevance [Enomoto et al., 2025].\n\nThis perpetuates an English-centric perspective: models are optimized for many target languages,\nbut still trained on prompts that reflect the needs, assumptions, and discourse patterns of En-\nglish speakers. Prior work shows that this mismatch has measurable downstream effects on both\ngeneration quality and fairness [Li et al., 2025].\n\nWe argue that addressing these limitations requires a shift in focus: not only improving completions,\nbut optimizing the distribution of input prompts itself.  In this paper, we introduce a prompt-\nfocused paradigm for synthetic data generation, where translated prompts are systematically\ntransformed along three critical dimensions: Naturalness, Cultural Adaptation, and Difficulty En-\nhancement. By treating prompts as dynamic components rather than fixed scaffolds, we directly\nreshape the input distribution, reducing translation artifacts and embedding inductive biases that\nare better aligned with real user data, see Figure 2 for an example.\n\nWe evaluate this approach across 12 languages spanning diverse families. Starting from translated\nEnglish prompts, we apply targeted prompt-space transformations using a strong teacher LLM, and\nmeasure their impact both on the data itself and on downstream performance. Our data evaluations\nconfirm that our prompt transformations successfully improve quality along the targeted\ndimensions: Naturalness increases lexical diversity, Cultural Adaptation enhances fluency, and the\nDifficulty Enhancement transformation raises both difficulty and overall quality (though at the cost\nof diversity) when compared to translated prompts. When combined, these transformations produce\na well-rounded prompt distribution. These prompt-side improvements carry over to completions\nwhere even small interventions in the prompts lead to substantial changes in completions\n(table 2), improving their fluency, diversity, and difficulty. Downstream (fig. 1), when used for fine-\ntuning a 7B base model, these effects yield strong and consistent improvements across all\nlanguages and a diverse set of benchmarks (mathematical reasoning, translation, language\nand culture understanding, open-ended generation) with particularly pronounced gains on open-\nended tasks, our best proxies for real human use.\n\n\n\n                                              2\n\nFigure 2: Illustration of our prompt transformations on a representative toy example that gets adapted\nfor German: Each transformation modifies the original English prompt, with major modifications highlighted\nin bold. Modifications to the prompt cause changes in the generation as well, so by making the prompt more\nnatural by using the German term “Erntedankfest” rather than the English “Thanksgiving”, the completion\nnow lists typical German rather than American Thanksgiving dishes (“bread, fruit, pumpkin, potato dishes”).\nThe Cultural Adaptationfurther localizes the prompt (“in Germany”) and replaces the event of Thanksgiving\nwith the event of Christmas, which has larger significance in German culture. The Difficulty transformation\nyields a prompt that requests a shopping list for side dishes of Thanksgiving, making it more specific but also\nmore complex. Full examples of prompt transformations and their corresponding completions that were used\nfor our experiments are in Table 9.\n\n\nOverall, this paradigm shift from optimizing only in the generation space to optimizing in the prompt\nspace represents a fundamental evolution in how we approach multilingual data creation. As our\nexperiments show, bootstrapping fine-tuning data from translations via targeted transformations\nhas a tremendous impact on the state of language modeling especially languages that are typically\noverlooked in LLM development.\n\n\n2  Method\n\nExisting synthetic data pipelines primarily expand P(y | x), the conditional mapping from prompts\nto completions, while implicitly assuming that the input prompt distribution P(x) is fixed. This\ngeneration-focused view limits diversity and cultural grounding: completions remain tied to the\nartifacts, biases, and topical scope of the original prompts, especially when these are machine-\ntranslated from English. We instead intervene directly on the input distribution P(x), introducing\nan inductive bias toward more natural, contextually grounded, and linguistically rich prompts.\nThis prompt-focused perspective reframes synthetic data generation as optimization in the prompt\nspace, not just in the generation space.\n\n\n2.1  Problem Setup\n\nLet Psrc(x) denote the distribution of prompts in a high-resource source language (e.g., English). We\nyield a corresponding target-language distribution Ptrg,ℓ(x) for each language ℓthrough translation:\n\n                                      xtrg ∼Ptrg,ℓ= translate(Psrc).\n\nWhile this step expands coverage, it does not adapt content to the linguistic or cultural norms of\nthe target language. We therefore introduce a lightweight transformation operator T that refines\n\n\n\n                                              3\n\ntranslated prompts:\n                                   xopt = T (xtrg),   xopt ∼Popt,ℓ.\n\nThe resulting optimized distribution Popt,ℓreplaces Ptrg,ℓas the input space for training, giving rise\nto\n                                   Ptrain,ℓ(x, y) = Popt,ℓ(x) Pteacher(y | x).\n\nIn this setup, any shift in Popt,ℓdirectly influences the inductive bias of the fine-tuned model,\naltering not only what it learns to say (P(y | x)), but also what it learns to understand.\n\n\n2.2  Transformation Operators\n\nWe instantiate T as a family of modular operators T = {Tnat, Tcult, Tdiff}, each targeting a distinct\ndimension of prompt quality:\n\n\n• Naturalness (Tnat): Removes translation artifacts and restores idiomatic phrasing to better\n   reflect authentic language use.\n\n• Cultural adaptation (Tcult): Recontextualizes prompts to locally relevant examples, values,\n  and references, aligning them with cultural norms.\n\n• Difficulty enhancement (Tdiff): Increases task complexity by expanding or reformulating prompts\n  into more challenging, multi-step instructions.\n\n\nEach transformation produces a valid optimized prompt distribution Popt,ℓ; in practice, these oper-\nators can be applied individually or in sequence (e.g., Naturalness followed by Cultural Adaptation).\nEach operator shifts Popt,ℓcloser to the true user distribution P ℓ,∗ improving both data quality and\ndownstream generalization.\n\nOur approach extends synthetic data generation beyond completions by explicitly optimizing the\ninput side of the data distribution. This simple but general formulation allows multilingual models to\nlearn from richer, more representative prompts—enhancing linguistic diversity, cultural grounding,\nand ultimately, model generalization.\n\n\n2.3  Prompt Tuning\n\nEach transformation T  is executed with an LLM. At the core of the transformation is a prompt\nthat specifies which context and input (e.g. user prompt, original English prompt, target language)\nis included in the transformation, its description and some additional guidelines (the exact prompt\ntemplates are given in table 8). These were improved over a few iterations via manual data in-\nspection, but they can be further customized for desired domains. We kept the prompts relatively\nsimple as overly rigid guidelines risk reducing diversity, making outputs feel templated, limiting\ngeneralization and correctness, especially in underrepresented languages where the teacher model\nmay already struggle with instruction following and hallucinate more easily.\n\n\n3  Experiments\n\nWe set up a multilingual fine-tuning pipeline where the primary goal is to improve quality and\nperformance in various tasks, with special focus on naturalness and fluency of open-ended genera-\ntions, cultural adequacy and accuracy in challenging domains that typically show strong language\n\n\n                                              4\n\nLanguage       Script  Lang. Family      Resources    Prompt Translation Quality\n       (code)                                         Institutional/Data  Expert      Gemma\n\n     German (de)     Latn    IE / Germanic           high, 5         93.96            92.49\n      Spanish (es)      Latn    IE / Italic               high, 5         89.15            86.40\n      Czech (cs)       Latn    IE / Balto-Slavic        high, 4         86.00            82.03\n      Ukrainian (uk)    Cyrl    IE / Balto-Slavic        high, 4         82.23            79.51\n      Greek (el)       Grek    IE / Greek               high, 3         83.86            80.78\n      ⋆Hungarian (hu)  Latn     Uralic / Finnic          high, 4         83.61            78.88\n       ⋆Slovak (sk)      Latn    IE / Balto-Slavic        high, 3         85.71            81.36\n      ⋆Croatian (hr)    Latn    IE / Balto-Slavic        high, 3         79.91            78.86\n      ⋆Lithuanian (lt)   Latn    IE / Balto-Slavic        high, 3         84.11            82.40\n      ⋆Latvian (lv)     Latn    IE / Balto-Slavic        high, 3         69.65            73.18\n      ⋆Basque (eu)     Latn    Basque                 mid, 4         66.01            70.19\n      ⋆Welsh (cy)      Latn    IE / Celtic             mid, 3         73.75            68.30\n\n      Avg                                                                81.04            79.53\n\nTable 1: Language Overview: We characterize the languages of study in terms of resourcedness with respect\nto data availability with levels (1–5) (Data), and whether they are mid or high-institutional in terms of vitality\naccording to Ethnologue (Institutional), both sourced from [Ranathunga & de Silva, 2022]. We also report\nprompt translation quality (XCometXL [Guerreiro et al., 2024], reference-free) of the prompt translation model\n(Expert, in-house expert model) and the transformation model (Gemma3-27B-it) on a 1k sub-sample of our\nprompts. Languages marked with ⋆are not officially supported in the base model. IE: Indo-European.\n\n\ndisparities. The setup aims to make the impact of each of our transformations measurable, first in\nthe resulting data, and then further in downstream performance of the model.\n\n\n3.1  Data Processing Pipeline\n\n\n3.1.1  English Seed Prompts\n\nWe collect real prompts from users around the world (with consent and without PII), similar to e.g.,\nShareGPT.1 Because the prompts are noisy, we apply content filtering and language identification\nfiltering with FastText [Joulin et al., 2016a;b] to extract a pool of 280k English prompts. This pool\nof prompts is attractive for modeling because these are unseen samples of real-life use of state-of-\nthe-art models, and thereby provide an excellent learning opportunity.\n\n\n3.1.2  Prompt Translation into Target Languages\n\nWe take distinct 10k sub-samples from the English pool of prompts and automatically translate\nthem into 12 target languages (German, Spanish, Czech, Ukrainian, Greek, Hungarian, Slovak,\nCroatian, Lithuanian, Latvian, Basque, Welsh), listed in table 6, using an in-house state-of-the-art\ntranslation expert LLM.\n\nWhile geographically close (all spoken in Europe), these languages cover seven language families\n(including one isolate, Basque) and three scripts. They are standardized and have mid to high insti-\ntutional support [Bird, 2022], but vary in terms of their availability of accessible, high-quality data,\nrepresentation on the web and in NLP research, and support in open LLMs [Ranathunga & de Silva,\n2022]. As a result, the translation capabilities of our expert translation model varies, yielding top\nquality e.g.  for German, Spanish and Czech, but much poorer quality e.g.  for Latvian, Basque\nand Welsh. The translation quality on our domain of user-submitted prompts is overall slightly\n\n   1https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o\n\n\n\n                                              5\n\nlower (but also harder to estimate), due to challenging inputs like code or non-standard language.\nNevertheless, we assume that for this selection of languages, bootstrapping with translation and\ntransformation is feasible, and it lets us study our proposed methods on a diverse spectrum.\n\n\n3.1.3  Prompt Optimization\n\nWe choose Gemma3-27B-it2 as our transformation model for its broad language support and\nstrong multilingual performance [Team et al.]. The translation evaluation in table 1 may also serve\nas a loose proxy for understanding the generative capabilities of the model in each language [Üstün\net al., 2024] (more in table 6): We expect highest-quality outputs for German, Spanish and Czech,\nand lowest-quality outputs for Latvian, Basque and Welsh. For each transformation described in\nsection 2, we prompt it with the respective custom instruction and sample a single generation\nwith a temperature of 0.3. Importantly, we apply the Naturalness transformation directly to the\ntranslated prompts, but for the Cultural Adaptation and Difficulty Enhancement transformations,\nwe apply them on top of the Naturalness-transformed prompts.  This decision is based on our\ninitial experiments, which showed that the Naturalness transformation provides a mild, generally\nbeneficial adjustment that does not interfere with the other two. After transforming the prompts,\nwe run FastText’s language identification model and drop the prompts that do not correspond to\nthe target language to prevent language confusion downstream [Marchisio et al., 2024].\n\n\n3.1.4  Prompt Completions\n\nTo generate completions, we rely on a teacher model that provides responses to the prompts without\nany additional instructions. For this purpose, we use the same model as our transformation model,\nGemma3-27B-IT.3 For each prompt, we sample a single generation with a temperature of 0.3. To\nensure that outputs are produced in the intended language, we once again run language identification\nand discard mismatches (the final number of samples for each language can be found in table 7). We\nadopt this simple completion generation setup in order to cleanly isolate the effect of our prompt\ninterventions.\n\n\n3.2  Fine-Tuning\n\n\n3.2.1  Base Model\n\nWe use the base version of CommandR7B,4 an open weights 7B open-weights model pre-trained on\nthe following 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Ko-\nrean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian,\nRomanian, Greek, Hindi, Hebrew, and Persian. Only five of these languages overlap with our target\nlanguages (see table 6), which enables us to study the effectiveness of our transformation tech-\nniques in expanding the language coverage of LLMs during post-training (section 4.3). Supervised\nfine-tuning (SFT) follows a standard procedure, details described in section F.\n\n\n3.2.2  Data Mixture\n\nWe consider four main datasets, one for each of the transformations described in section 2 and an\nadditional one where we mix 50% of Culturally Adapted data and 50% of the Difficulty Enhanced\n\n   2https://huggingface.co/google/gemma-3-27b-it\n   3In principle both models do not need to be identical, it is a choice of convenience.\n   4https://docs.cohere.com/docs/command-r7b\n\n\n\n                                              6\n\nLength    Rel. Dist.↑  Perplexity↓  Diversity↑   Difficulty↑   Quality↑\n\n      Transformation   P   C    P   C    P    C    P   C    P   C    P   C\n\n        Translated         406   2451    –     –    15.34   2.46   0.88   0.77   1.78   1.77   3.21   4.78\n\n        Naturalized        397   2490   0.24   0.64   14.06   2.48   0.90   0.77   1.76   1.75   3.26   4.81\n        Cultural           470   2352   0.30   0.67   12.11   2.51   0.89  0.79   1.76   1.76   3.28   4.82\n         Difficulty          1936  5322  0.86  0.81   3.13   2.15   0.77   0.76  2.44  2.45  4.50  4.83\n        Cultural + Diff.    1205  3873   0.58   0.74   4.52    2.27   0.82   0.77   1.97   2.10   3.75   4.76\n\nTable 2: Comparison of text metrics for Prompts (P) and Completions (C). Lower perplexity and higher\ndiversity (N-gram measurement), difficulty, and quality are better. Computed on a sample of 1000 prompts\nper language.\n\n\ndata. We complement our datasets with a portion of other standard instruction tuning datasets\n(mostly English) in order to reduce overfitting, these include domains like math, code, reasoning but\nalso multilingual datasets (for the 23 languages supported by the base model). In total, each of our\nfour data mixtures contains roughly 590k examples, around 48% of which are contributed by our\nprompt transformations. Table 7 contains the detailed counts for each language and transformation.\nThey differ slightly due to language identification filtering.\n\n\n3.3  Evaluation\n\nIn evaluation, our primary question throughout will be how our transformations compare against\nthe current go-to strategy of prompt translation. We compare this in two stages: at the data level,\nand in downstream evaluations.\n\n\n3.3.1  Data Evaluations\n\nWe evaluate the textual characteristics of both prompts and completions using a combination of\nstandard metrics and LLM-based scores. First, we measure how much the prompts and generations\nhave changed in comparison to their translated counterparts at the surface level, using relative\nedit distance (Levenshtein distance normalized by the maximum length) and length in characters.\nTo assess diversity, we compute corpus n-gram diversity at the language level by tokenizing the\ntexts using spaCy5 and then computing the ratio of unique n-grams to total n-grams [Padmakumar\n& He, 2024; Shaib et al., 2025]. To assess naturalness, we use Gemma3-27b-pt to compute the\nperplexity of each text. Previous works have used target language model perplexity as a metric\nfor translationese [Bizzoni & Lapshinova-Koltunski, 2021; Li et al., 2025]. To assess quality and\ndifficulty, we rely on automatic scoring by prompting an LLM (Gemma3-27b-it) to score the texts\non a discrete scale (prompts included in section G.3). These measures allow us to directly test\nwhether our transformations succeed in eliciting more desirable textual features which are key to\nsteering downstream performance [Shimabucoro et al., 2024].\n\n\n3.3.2  Downstream evaluations\n\nDiscriminative benchmarks. Our suite covers two discriminative tasks, formalized as multi-\nchoice tasks: Include44 [Romanou et al., 2024], with questions from local academic and professional\nexams written in target languages, and Global-MMLU (G-MMLU) [Singh et al., 2025] with trans-\nlated QA tasks from English. We expect that these tasks can help us measure language disparities\nin knowledge access, especially for those questions that are culture-specific. Implementation details\n\n   5https://spacy.io/\n\n\n\n                                              7\n\nFlores   G-MMLU  Include44 MGSM  mArenaHard  PolyWrite\n Prompts in FT\n               xCometXL ↑             Accuracy ↑                    Win-rate % ↑\n\n Translated             0.786          52.5          47.0        56.2          –             –\n Naturalized            0.791          53.1          46.9        56.5          57.7            63.8\n Cultural               0.805        57.9         50.8        66.0          65.7            66.1\n Difficulty            0.816          54.5          51.2        65.1          61.8            64.6\n Cultural + Diff.       0.810          57.2        51.8       67.3        67.7          66.9\n\n\nTable 3: Downstream Results: Performance across multiple evaluation benchmarks. Scores correspond to\nXCometXL (Flores), Accuracy (G-MMLU, Include44, MGSM) and win-rate percentage against Translated\nmodel (mArenaHard, PolyWrite). Highest scores is marked in bold. Results for individual languages in sec-\ntion H.4.\n\n\nare described in section G.\n\nClose-ended generative benchmarks. For these benchmarks, there exist gold standard outputs\nwhich quality can be measured against. This is interesting because it allows us to precisely track\nquality improvements (as in discriminative benchmarks), but also captures the quality of more than\none output tokens (as opposed to discriminative benchmarks). We choose the Flores translation\ntask [Team et al., 2022] for its wide language coverage, and MGSM [Shi et al., 2023] as a challenging\nmath task. For MGSM, we extend the original language coverage by adding translated versions,\nwhich we refer to as MGSM++. The Basque translations were released in IberoBench  [Baucells\net al., 2025],6 Greek curated by ILSP/Athena RC,7, Welsh released by Language Technologies team\nfrom Bangor University,8 Czech, Hungarian as curated for BenchMAX [Huang et al., 2025].9\n\nOpen-ended generative benchmarks. Our primary target are the following two benchmarks\nthat capture open-ended generation quality:10 m-ArenaHard v2.0 [Khairi et al., 2025b] is a collection\nof challenging LMArena prompts [Zheng et al., 2024] that was translated into 23 languages.  It\ncontains prompts from a wide range of domains, but especially code and math—which we assume,\nare challenging especially where base performance is low. We extend the set of support languages to\ninclude our missing ones, by translating the prompts from English (and apply language filtering to\nthe prompts), forming mArenaHard++ v2.0 (the same procedure as for the original mArenaHard-\nv2.0). Performance is measure with win rates (percentage of wins) in pairwise comparisons against a\ncompetitor model as judged by GPT-4.1 (gpt-4.1-2025-04-14).11 To capture language naturalness\nbetter (ill-defined on code and math), we compare our models on creative writing prompts from\nthe PolyWrite benchmark [Ji et al., 2024], where we additionally compute win-rates with a judge\nprompt that evaluates the naturalness of completions, and evaluate the diversity of the generations\nwith self-BLEU [Zhu et al., 2018; Ji et al., 2024].\n\nLanguage coverage. Although not all target languages are covered in GlobalMMLU, Include44\nand MGSM++, each language is represented in at least one of them, while all being included in the\nremaining, see table 3. We only evaluate the models for our focus languages (plus English, where\navailable), and report averages (plus breakdowns in the appendix).\n\n\n   6https://huggingface.co/datasets/HiTZ/MGSM-eu\n   7https://huggingface.co/datasets/ilsp/mgsm_greek\n   8https://huggingface.co/datasets/techiaith/mgsm_cy\n   9https://huggingface.co/datasets/LLaMAX/BenchMAX_Math\n  10Prior work found discriminative benchmarks not indicative enough for generative performance [Üstün et al.,\n2024].\n  11https://platform.openai.com/docs/models/gpt-4.1\n\n\n\n                                              8\n\nSelf-BLEU↓ NWR↑  LPR↑\n\n                      Translated            33.73         –       97.6\n\n                      Naturalized           30.01        57.1      97.3\n                       Cultural              32.65        63.7      97.5\n                          Difficulty             34.01       69.3     97.3\n                       Cultural + Diff.     29.77        66.6     97.9\n\n\nTable 4: Downstream quality on PolyWrite with auxiliary metrics for diversity (Self-BLEU ), naturalness\nwin-rates (NWR, against Translated model under an LLM judge specialized on naturalness) and language\nconfusion (Line Pass Rate, LPR).\n\n\n4  Results\n\n4.1  Data Quality\n\n\n4.1.1  Prompt Quality\n\nTable 2 confirms that our transformations advance the quality of the prompts (“P” columns) over\nthe original translated prompts along all dimensions, in terms of diversity, fluency, and also gen-\neral quality and difficulty. The Naturalness transformation achieves the greatest n-gram diversity,\nwhich confirms that it re-introduces linguistic richness that might have gotten lost in translation.\nThe Cultural Adaptation transformation lowers perplexity the most, showing that it is most closely\naligned to the target-language content that the base model has seen during pretraining. The Diffi-\nculty transformation is the most aggressive transformation, as its edit distance from the translated\nprompts is more than 3× higher than the other transformations. It also increases the prompt length\nby an average factor of 4.8×. We manually inspect a subset of these prompts and find that the\nDifficulty transformation typically introduces additional constraints, which are similar in template\nacross data points, consequently lowering the diversity. Our LLM judge also considers these prompts\nas of substantially higher quality (and obviously difficulty) than the naturalized or cultural ones.\nWe thus expect the largest impact on generations and downstream from this transformation. When\nmixing difficulty and cultural data, we obtain scores in between both individual transformations,\nwhich, compared to difficulty alone, raises n-gram diversity, but lowers the other metrics.\n\n\n4.1.2  Completion Quality\n\nAlthough the changes introduced in the prompts for the Naturalness and Cultural adaptation trans-\nformations are relatively small, the resulting completions differ substantially from those produced\nby the translated model (around 2× higher edit distance), as shown in table 2, “C” columns. This\nsuggests that even minor adjustments on the prompt side can lead to large shifts in completions.\nNotably, completions from the difficulty model are, on average, 2.2× longer than those from the\ntranslated model, i.e. yielding twice as many target-language tokens to train on. The effects of the\nindividual transformations and the data mix overall correspond to the changes brought about in\nthe prompt space.\n\nWe expected generations after the Naturalness transformation to have a lower perplexity as a result\nof being more natural [Li et al., 2025], but this is not indicated by the metric. One confounding\nfactor might be that the perplexity scoring model is the pretrained model for our teacher model,\nwhich might bias the model towards prompts more that it has altered more. We next ask whether\nintervening on the prompts themselves induces greater naturalness in model responses.\n\n\n\n                                              9\n\n4.2  Downstream Performance\n\nTable 3 summarizes the performance of the fine-tuned model across tasks, averaged across languages.\nWe report a detailed language breakdown in section H.4.  In general, our transformations beat\nthe translation-only baseline for all tasks and languages. We see surprisingly big differences in\nbenchmark scores, given that we only exchanged max 10k prompts per language between variants.\n\nBeyond translationese. We can see that the Naturalness transformation, that is focused on in-\ncreasing fluency and removing translation artifacts, brings only marginal gains on most benchmarks\ncompared to the transformations that modify the content and domain of the prompts more.12 This\nhighlights the importance of going beyond translation: even if prompts were translated perfectly,\ntheir utility is limited by their content that is less relevant in other languages and cultures. Though,\nthe naturalness transformation shines the most in open ended generation tasks: in mArenaHard it\nwins over the translated model by 7.7% and even more in PolyWrite, which is focused on created\nwriting, winning by 13.8%.\n\nCultural adaptation. The gains in G-MMLU and Include44 by 5.4% (highest score overall) and\n3.8%, respectively, show that the cultural grounding of the prompts indeed helps for downstream\nknowledge retrieval in culturally relevant tasks. This reflects directly in the score of the cultural-\nsensitive subset of G-MMLU (table 15), where this transformation provides a 7% improvement,\ncompared to 2% for cultural-agnostic questions.\n\nIt also has beneficial effects on translation, math (+9.8% accuracy wins over naturalized) and open-\nended generation quality (e.g. +8% win-rate on mArenaHard over naturalized prompts, especially\nhigh for Ukrainian and Slovak). Interestingly, the Difficulty transformation also brings similar gains\non Include44, which by closer inspection comes from questions in domains (see table 14) centered\naround business, which likely have well-defined constraints and are more difficult in nature.\n\nThe importance of difficulty. The Difficulty transformation, being most aggressive, also brings\nthe overall largest benefits.  It appears important for mathematical reasoning, as shown by the\n+8.6% gains over only naturalized prompts.  But more so in machine translation, achieving a\nnotable improvement of +3.0 XCometXL points.13\n\nCombining complementary strengths. We have seen that Cultural adaptation and Difficulty\ntransformations appear sometimes orthogonal in their benefits to tasks like G-MMLU, mArenaHard,\nand PolyWrite. By mixing their data, taking 50% each, we hope to achieve the best of both worlds.\nFor MGSM and Include44, where they individually score similarly strong, the gains add up to\nyield the best performance overall. For open-ended generation (mArenaHard and PolyWrite), the\ncombined mix also scores highest, yielding an average win rate of 67.7% and 66.9% over translated\nprompts respectively. For the remaining tasks, the mix scores in between both, making this variant\nthe overall most well-rounded model. Future work may explore combinations through model merging\nrather than data mixing [Aakanksha et al., 2024].\n\n\n4.3  Analysis\n\n  12Table 10 shows that for some languages (cs, el,  lt, eu, hu) this can be considered an improvement in\ntranslation quality, but the prompt is not strictly tied to post-editing.\n  13The 3.0 gain in XCometXL scores is estimated to be 95.3% accurately aligned with humans [Kocmi et al.,\n2024].\n\n\n\n\n\n                                              10\n\nFigure 3: Translation performance on Flores by language (grouped by those supported in pretraining vs\nothers), compared also against the teacher model.\n\n\n4.3.1  What matters for quality?\n\nIn table 4 we break down multiple aspects of quality on the PolyWrite benchmark: diversity, natural-\nness and the ability to respond in the correct target language. We can see that our transformations\nimprove over the translated variant in all aspects: downstream outputs are more natural, diverse\nand more likely in the right language. Similar to our prompt analysis we observe that diversity does\nnot increase after Naturalness transformation, but naturalness, as determined by an LLM judge,\nfurther increases. Due to our language id filtering, language confusion is rare across the bench but\nlowest in the mixed approach.\n\n\n4.3.2 How does language support and resourcedness affect performance?\n\nNaturally languages supported during pre-training show higher performance compared to those that\nwere not supported (indicated in table 6), e.g. on Flores the average Translated baseline performance\n(fig. 3) already diverges by 16.6 points in XCometXL between supported and unsupported languages\n(more details in table 16). However, our transformations significantly improve both groups relative\nto the baseline, the unsupported even more—by an average of +3.3 points (achieved by the Difficulty\nmodel)—than the supported ones (+2.6 points on average).14 This is consistent with mArenaHard\nas well with +5.7 over Naturalized for unsupported compared to +3.6 for supported) underlining\nthe effectiveness of prompt optimization especially for cases of language expansion and under-served\nlanguages.\n\n\n4.3.3  Performance on Lowest-Resource Languages\n\nOur method depends on the performance of the translation model and teacher model. It is not well\nunderstood where the trade-off between noise and scale lie for synthetic data generation. In our\nstudy, there are very few cases where individual transformations did not yield downstream improve-\nments over translations for individual languages. We particularly inspect the lowest-resource ones in\nFigure 4: For MGSM, we find that for Basque and Welsh, the Naturalness transformation performs\nworse than direct translation, but the other transformations succeed in improving over it, similar\n\n  14According to [Kocmi et al., 2024], this difference estimated to be 95.2% accuracy with human preferences.\n\n\n                                              11\n\nFigure 4: Performance on lowest-resource languages Welsh (cy), Basque (eu) and Latvian (lv) across three\ntasks. Win Rates are in comparison with the Translated baseline.\n\n\n                             Win-rate %  Completion Length\n\n                               Ours  Qwen  Ours     Qwen\n\n                 mArenaHard  56.8    43.2   5281       2548\n                    PolyWrite     88.4    11.6   3364       2208\n\nTable 5: Open Ended Win-Rates against Qwen2.5-7B Averaged win-rates and completion lengths across\nlanguages from direct comparisons of the Cultural+Difficulty model against Qwen2.5-7B on mArenaHard and\nPolyWrite.\n\n\nas for Welsh or Croatian in WMT (fig. 3). On the other hand, for mArenaHard, our overall best\napproach (Cultural+Difficulty) yields substantial improvements over the light Naturalness transfor-\nmation for these languages. However, for PolyWrite, it has less benefits:  it improves performance\nfor Welsh and Latvian but does not give any gains over the Naturalness transformation in Basque;\nin other words: the data characteristics that we shape with this additional transformation seem not\nto deciding the win rate metric on PolyWrite for these lowest-resource languages.  Overall, these\nresults highlight the nuanced relationship between translation quality, transformation strategy, and\nlanguage resource level in determining the effectiveness of the prompt transformations.\n\n\n4.3.4  Comparison to External Models\n\nTo ground our results in comparison with the external state of the art, we compare the performance\nagainst the teacher model itself, Gemma3-27b-it, focusing on generative performance in the ma-\nchine translation task. The Difficulty transformation performed particularly well—scoring within\n2 points (and 3 points above the Translated baseline) of the (more than 3 times larger) teacher on\naverage. The breakdown by language in fig. 3 reveals that our method nominally outperforms the\nteacher model in German, Spanish, Greek, and Welsh, being most meaningful in Welsh and German\nand Spanish (96%, 91%, 70% accuracy with humans respectively, according to [Kocmi et al., 2024]).\n\n\n\n\n                                              12\n\nFurthermore, we compare in table 5 the Cultural+Difficulty model with Qwen2.5-7B15 on open-\nended generation tasks in both mArenaHard and PolyWrite. This evaluation aims to assess whether\nour interventions—though not explicitly optimized for downstream metrics on particular benchmarks—\nyield a competitive checkpoint.  Our results show that the model achieves a 56% win rate on\nmArenaHard on average across languages, winning in 9/13 languages, being outperformed in the\nhighest-resourced ones, Czech, German, Spanish and English. More impressively, our model achieves\naverage win rates of 88% on PolyWrite, winning in all languages but English, see individual lan-\nguage breakdowns in table 23. While for both evaluation sets, completion lengths for our model are\nsubstantially longer, it does not seem to be the deciding factor in win rates. Upon inspection, we\nfind that our model’s generations for PolyWrite tend to be more elaborate, expressive and creative.\n\nThese outcomes highlight the effectiveness of our interventions in enhancing the model’s proficiency\nin generating text for our target languages.\n\n\n5  Related Work\n\nFor multilingual LLMs, the majority of prior studies on targeted data augmentation has been focused\non QA tasks, as surveyed in  [Liu et al., 2024].  Here, we review in detail the works that target\ngenerative tasks.\n\n\n5.1  Translation\n\nEnomoto et al. [2025] compare instructing multilingual LLMs in the target languages with transla-\ntion from English. They find that instructing models in English is not as advantageous as previously\nassumed, when translationese is being controlled for.\n\nLi et al. [2025] show that translationese bias in LLM outputs for translation tasks stems from the\ninstruction finetuning stage, where models are typically trained on translated data. They propose\napproaching this via polishing or filtering for unnatural data points after data generation, while we\ndirectly address it during the prompt expansion phase. The post-hoc filtering route was also chosen\nin Apertus [Hernández-Cano et al., 2025] and EuroLLM (complexity and readability) [Martins et al.,\n2025]. Martins et al. [2025] also explore writing prompts from scratch giving a LLM seed prompts\nfrom trusted sources.\n\n\n5.2  Naturalness\n\nChen et al. [2024] showed that natural target-language data can outperform translated data for\ninstruction tuning, particularly when evaluated on generative benchmarks and those written in\ntarget languages. Interestingly, the benefits of new knowledge captured in these languages appears\nto outweigh the risk of translation artifacts. Our work studies an attractive middle ground between\nthe two scenarios: Even without native target language data (which are notoriously hard to get by),\nwe can do much better than with plain translated prompts.\n\n\n5.3  Difficulty\n\nXu et al. [2024] demonstrated the effectiveness of using LLMs to synthetically scale-up the complex-\nity of instruction based on the original prompts. Additionally, Muennighoff et al. [2025] showed that\n\n  15https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n\n\n\n                                              13\n\nby curating for difficult mathematical problems, even a small quantity of examples can massively\nimprove the performance of the model on mathematical reasoning tasks. Our work extends these\nfindings to the multilingual setting in the vein that learning from difficult multilingual examples\nimproves model performance on MGSM. Additionally, we note that the biggest gains in PolyWrite\nwin-rates also come from learning from these difficult examples.\n\n\n5.4  Natural, Difficult, and Diverse\n\nZhou et al. [2023] noted that only a handful of curated prompt-completion pairs from diverse\nand high-quality sources are needed to generate human-preferred responses. Our work shows that\nthrough prompt transformations you can extend these curated English prompts to improve model\nperformance in multilingual setting, addressing the massive imbalance of instruction data between\nlanguages.\n\n\n6  Conclusion\n\nIn this work, we have demonstrated the potential of synthetic data generation for enriching multi-\nlingual datasets through the creation of higher-quality, contextually aligned, and culturally sensitive\ndata that better reflects real-world language use. By systematically transforming the prompts, we\nare able to guide teacher model generations to be more adaptive and contextually nuanced to the\ntarget languages, consequently endowing our models with these characteristics. Our results show\nthat systematic data transformations can produce models with outputs that are more natural, cul-\nturally grounded, and linguistically rich. We position this study as an initial step toward principled\napproaches to multilingual synthetic data generation, an essential direction for developing inclusive,\nculturally aware, and globally capable language models.\n\n\nLimitations\n\n6.1  Reliability of Synthetic Data\n\nLearning from synthetic data poses inherent risks and has well-studied limitations [Liu et al., 2024].\nIn our experiments, we mostly found synthetic data beneficial and are not aware of similar human-\nauthored data that could be used in its place. We include language identification filtering, but\nother biases or errors could still have transferred from the teacher into the student model, especially\nthose that would not move the needles of our evaluations. We recommend exploring the addition\nof more targeted filters in future work, especially for lower-resource languages or languages where\nperformance out-of-the-box is sub-par. Ideally, native speakers should be involved to inspect samples\nof the generated data.\n\n\n6.2  Language Scope\n\nOur study covers 12 languages, and we find fairly consistent performance across the bench, taking\ninto account their differences in resourcedness and support in student base and teacher model.\nFurther work needs to confirm if the observations transfer to similarly positioned languages. There\nmight also be some benefits from geographic proximity that we have not controlled for. We have not\npushed the method to the extremes (very low-resource and no evidence of language support) because\nit is obvious that relying on bootstrapping from existing models and automatic translation will not\nwork with a cold start. However, there might be more languages beyond our lowest-resourced ones,\nthat will still benefit from our method.\n\n\n                                              14\n\n6.3  Evaluation Shortcomings\n\nLLM judges might have been trained on translationese as well and therefore favor it in their prefer-\nences [Chen et al., 2024]. Similarly, the base model might still score translationese as low-perplexity\nif it has seen translations during pre-training (likely according to Thompson et al. [2024]). Due\nto a lack of target-language evaluation benchmarks for challenging and relevant open-ended ques-\ntions [Kreutzer et al., 2025], both our datasets for these tasks are composed of translated prompts\nfor languages outside of English. Models trained on more translated prompts might have an ad-\nvantage at evaluation time [Chen et al., 2024; Kreutzer et al., 2025]. Human evaluation would be\nneeded to confirm model rankings eventually.\n\n\n6.4  Further Optimizing the Quality of the Data\n\nDirection for further improvement of downstream results are the optimization of the machine trans-\nlation, e.g.  hand-picking the best available translator for each language and task, and the opti-\nmization of the generation process, e.g. by involving multiple teachers, quality filters or sequential\nedits [Odumakinde et al., 2025; Khairi et al., 2025a].\n\n\nAcknowledgments\n\nWe thank our colleagues at Cohere Labs for their feedback throughout all stages of this project,\nincluding Thomas Euyang and Madeline Smith.\n\n\nReferences\n\nAakanksha, Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, Marzieh Fadaee, and\n  Sara Hooker. Mix data or merge models? optimizing for diverse multi-task learning, 2024. URL\n  https://arxiv.org/abs/2410.10801.\n\nIrene Baucells, Javier Aula-Blasco, Iria de Dios-Flores, Silvia Paniagua Suárez, Naiara Perez, Anna\n   Salles, Susana Sotelo Docio, Júlia Falcão, Jose Javier Saiz, Robiert Sepulveda Torres, Jeremy\n  Barnes, Pablo Gamallo, Aitor Gonzalez-Agirre, German Rigau, and Marta Villegas. IberoBench:\n A benchmark for LLM evaluation in Iberian languages. In Owen Rambow, Leo Wanner, Marianna\n  Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert (eds.), Proceedings of\n  the 31st International Conference on Computational Linguistics, pp. 10491–10519, Abu Dhabi,\n  UAE, January 2025. Association for Computational Linguistics. URL https://aclanthology.o\n  rg/2025.coling-main.699/.\n\nSteven Bird. Local languages, third spaces, and other high-resource scenarios. In Smaranda Muresan,\n  Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pp. 7817–7829, Dublin,\n  Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.\n  539. URL https://aclanthology.org/2022.acl-long.539/.\n\nYuri Bizzoni and Ekaterina Lapshinova-Koltunski.  Measuring translationese across levels of ex-\n   pertise: Are professionals more surprising than students?  In Simon Dobnik and Lilja Øvrelid\n   (eds.), Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pp.\n  53–63, Reykjavik, Iceland (Online), May 31–2 June 2021. Linköping University Electronic Press,\n  Sweden. URL https://aclanthology.org/2021.nodalida-main.6/.\n\nPinzhen Chen, Simon Yu, Zhicheng Guo, and Barry Haddow.  Is it good data for multilingual\n\n\n                                              15\n\ninstruction tuning or just bad multilingual evaluation for large language models?   In Yaser\n  Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on\n  Empirical Methods in Natural Language Processing, pp. 9706–9726, Miami, Florida, USA, Novem-\n  ber 2024. Association for Computational Linguistics.  doi: 10.18653/v1/2024.emnlp-main.542.\n URL https://aclanthology.org/2024.emnlp-main.542/.\n\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash Ahmadian, Alejandro Salamanca, Madeline\n  Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor\n  Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan\n  Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh,\n  David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sam-\n  mie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-\n  Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh\n  Fadaee, Beyza Ermis, Ahmet Üstün, and Sara Hooker. Aya expanse: Combining research break-\n  throughs for a new multilingual frontier, 2024. URL https://arxiv.org/abs/2412.04261.\n\nDaniel Deutsch, Eleftheria Briakou, Isaac Rayburn Caswell, Mara Finkelstein, Rebecca Galor, Juraj\n  Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Eliz-\n  abeth Salesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, and Markus Freitag. WMT24++:\n  Expanding the language coverage of WMT24 to 55 languages & dialects. In Wanxiang Che, Joyce\n  Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association\n  for Computational Linguistics: ACL 2025, pp. 12257–12284, Vienna, Austria, July 2025. Associ-\n  ation for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-a\n   cl.634. URL https://aclanthology.org/2025.findings-acl.634/.\n\nSauleh Eetemadi and Kristina Toutanova. Asymmetric features of human generated translation. In\n  Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference\n  on Empirical Methods in Natural Language Processing (EMNLP), pp. 159–164, Doha, Qatar,\n  October 2014. Association for Computational Linguistics.  doi: 10.3115/v1/D14-1018. URL\n  https://aclanthology.org/D14-1018/.\n\nTaisei Enomoto, Hwichan Kim, Zhousi Chen, and Mamoru Komachi. A fair comparison without\n  translationese: English vs. target-language instructions for multilingual LLMs. In Luis Chiruzzo,\n  Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Amer-\n  icas Chapter of the Association for Computational Linguistics: Human Language Technologies\n  (Volume 2: Short Papers), pp. 649–670, Albuquerque, New Mexico, April 2025. Association for\n  Computational Linguistics. ISBN 979-8-89176-190-2.  doi: 10.18653/v1/2025.naacl-short.55.\n URL https://aclanthology.org/2025.naacl-short.55/.\n\nBeyza Ermis, Luiza Pozzobon, Sara Hooker, and Patrick Lewis.  From one to many: Expand-\n  ing the scope of toxicity mitigation in language models.  In Lun-Wei Ku, Andre Martins, and\n  Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024,\n  pp. 15041–15058, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n  doi: 10.18653/v1/2024.findings-acl.893. URL https://aclanthology.org/2024.findings-acl\n  .893/.\n\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ah-\n  mad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela\n  Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem\n  Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava\n  Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\n  Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret,\n  Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,\n\n\n                                              16\n\nDaniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary,\nDhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab Al-\nBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme\nNail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu,\nHugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov,\nJack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng\nChi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik\nPrasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini,\nKrithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-\nYeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish\nMadaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Ma-\nhesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew\nOldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Ku-\nmar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoy-\nchev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan\nZhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan,\nPunit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy,\nRamon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari,\nRohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Tay-\nlor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh,\nSean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Ra-\nparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya\nBatra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky,\nTamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speck-\nbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh\nRamanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan\nPetrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong\nWang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang,\nYaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue\nLi, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aa-\nditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi,\nAdolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski,\nAllie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres\nAlvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit\nRamchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowd-\nhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben\nMaurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape,\nBing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic,\nBrian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan\nWang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal,\nChristoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li,\nDavid Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem\nFoss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Mont-\ngomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban\nArcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Fi-\nrat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella\nSchwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi,\n\n\n\n                                            17\n\nZhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang,\n  Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Gold-\n  man, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena\n  Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher,\n  Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein,\n  Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shep-\n  ard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U,\n  Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan,\n  Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin\n  Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng\n  Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Marty-\n  nas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov,\n  Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mi-\n  hir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert\n  Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha\n  Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil\n  Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart,\n  Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro\n  Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,\n  Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy,\n  Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin\n  Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,\n  Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh\n  Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,\n  Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang,\n  Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie\n  Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,\n  Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman,\n  Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun\n  Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victo-\n   ria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru,\n  Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz,\n  Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv\n  Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\n  Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait,\n  Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The\n  llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\n\nNuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T.\n  Martins.  xcomet:  Transparent machine translation evaluation through fine-grained error de-\n  tection. Transactions of the Association for Computational Linguistics, 12:979–995, 2024.  doi:\n  10.1162/tacl_a_00683. URL https://aclanthology.org/2024.tacl-1.54/.\n\nJia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, and Sadid Hasan.\n  Does prompt formatting have any impact on llm performance?, 2024. URL https://arxiv.or\n  g/abs/2411.10541.\n\nAlejandro Hernández-Cano, Alexander Hägele, Allen Hao Huang, Angelika Romanou, Antoni-\n  Joan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank Ďurech, Ido\n  Hakimi, Juan García Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng\n  Chen, Vinko Sabolčec, Yixuan Xu, Michael Aerni, Badr AlKhamissi, Ines Altemir Marinas,\n  Mohammad Hossein Amani, Matin Ansaripour, Ilia Badanin, Harold Benoit, Emanuela Boros,\n\n\n                                              18\n\nNicholas Browning, Fabian Bösch, Maximilian Böther, Niklas Canova, Camille Challier, Clement\n  Charmillot, Jonathan Coles, Jan Deriu, Arnout Devos, Lukas Drescher, Daniil Dzenhaliou, Maud\n  Ehrmann, Dongyang Fan, Simin Fan, Silin Gao, Miguel Gila, María Grandury, Diba Hashemi,\n  Alexander Hoyle, Jiaming Jiang, Mark Klein, Andrei Kucharavy, Anastasiia Kucherenko, Fred-\n  erike Lübeck, Roman Machacek, Theofilos Manitaras, Andreas Marfurt, Kyle Matoba, Simon\n  Matrenok, Henrique Mendoncça, Fawzi Roberto Mohamed, Syrielle Montariol, Luca Mouchel,\n  Sven Najem-Meyer, Jingwei Ni, Gennaro Oliva, Matteo Pagliardini, Elia Palme, Andrei Pan-\n  ferov, Léo Paoletti, Marco Passerini, Ivan Pavlov, Auguste Poiroux, Kaustubh Ponkshe, Nathan\n  Ranchin, Javi Rando, Mathieu Sauser, Jakhongir Saydaliev, Muhammad Ali Sayfiddinov, Mar-\n  ian Schneider, Stefano Schuppli, Marco Scialanga, Andrei Semenov, Kumar Shridhar, Raghav\n  Singhal, Anna Sotnikova, Alexander Sternfeld, Ayush Kumar Tarun, Paul Teiletche, Jannis Vam-\n  vas, Xiaozhe Yao, Hao Zhao Alexander Ilic, Ana Klimovic, Andreas Krause, Caglar Gulcehre,\n  David Rosenthal, Elliott Ash, Florian Tramèr, Joost VandeVondele, Livio Veraldi, Martin Raj-\n  man, Thomas Schulthess, Torsten Hoefler, Antoine Bosselut, Martin Jaggi, and Imanol Schlag.\n  Apertus: Democratizing open and compliant llms for global language environments, 2025. URL\n  https://arxiv.org/abs/2509.14233.\n\nXu Huang, Wenhao Zhu, Hanxu Hu, Conghui He, Lei Li, Shujian Huang, and Fei Yuan. Benchmax:\n A comprehensive multilingual evaluation suite for large language models, 2025. URL https:\n  //arxiv.org/abs/2502.07346.\n\nShaoxiong Ji, Zihao Li, Indraneil Paul, Jaakko Paavola, Peiqin Lin, Pinzhen Chen, Dayyán O’Brien,\n  Hengyu Luo, Hinrich Schütze, Jörg Tiedemann, and Barry Haddow. EMMA-500: Enhancing\n  massively multilingual adaptation of large language models. arXiv preprint 2409.17892, 2024.\n URL https://arxiv.org/abs/2409.17892.\n\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas\n  Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651,\n  2016a.\n\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient\n  text classification. arXiv preprint arXiv:1607.01759, 2016b.\n\nAmmar Khairi, Daniel D’souza, Marzieh Fadaee, and Julia Kreutzer. Making, not taking, the best\n  of n, 2025a. URL https://arxiv.org/abs/2510.00931.\n\nAmmar Khairi, Daniel D’souza, Ye Shen, Julia Kreutzer, and Sara Hooker. When life gives you\n  samples: The benefits of scaling up inference compute for multilingual llms, 2025b. URL https:\n  //arxiv.org/abs/2506.20544.\n\nTom Kocmi, Vilém Zouhar, Christian Federmann, and Matt Post. Navigating the metrics maze:\n  Reconciling score magnitudes and accuracies. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar\n   (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n  (Volume 1: Long Papers), pp. 1999–2014, Bangkok, Thailand, August 2024. Association for\n  Computational Linguistics. doi: 10.18653/v1/2024.acl-long.110. URL https://aclanthology.o\n  rg/2024.acl-long.110/.\n\nJulia Kreutzer, Eleftheria Briakou, Sweta Agrawal, Marzieh Fadaee, and Tom Kocmi. Déjà vu:\n  Multilingual LLM evaluation through the lens of machine translation evaluation.  In Second\n  Conference on Language Modeling, 2025. URL https://openreview.net/forum?id=yxzVanFoij.\n\nGennadi Lembersky, Noam Ordan, and Shuly Wintner. Language models for machine translation:\n  Original vs. translated texts. Computational Linguistics, 38(4):799–825, 12 2012. ISSN 0891-2017.\n  doi: 10.1162/COLI_a_00111. URL https://doi.org/10.1162/COLI_a_00111.\n\n\n                                              19\n\nYafu Li, Ronghao Zhang, Zhilin Wang, Huajian Zhang, Leyang Cui, Yongjing Yin, Tong Xiao,\n  and Yue Zhang.  Lost in literalism: How supervised training shapes translationese in LLMs.\n  In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.),\n  Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume\n  1: Long Papers), pp. 12875–12894, Vienna, Austria, July 2025. Association for Computational\n  Linguistics.  ISBN 979-8-89176-251-0.  doi:  10.18653/v1/2025.acl-long.630. URL https:\n  //aclanthology.org/2025.acl-long.630/.\n\nRuibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi\n  Peng, Diyi Yang, Denny Zhou, and Andrew M. Dai.  Best practices and lessons learned on\n  synthetic data. In First Conference on Language Modeling, 2024. URL https://openreview.n\n  et/forum?id=OJaWBhh61C.\n\nLin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang.\n  On LLMs-driven synthetic data generation, curation, and evaluation: A survey.   In Lun-\n  Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Com-\n  putational Linguistics: ACL 2024, pp. 11065–11082, Bangkok, Thailand, August 2024. As-\n  sociation for Computational Linguistics.   doi:  10.18653/v1/2024.findings-acl.658. URL\n  https://aclanthology.org/2024.findings-acl.658/.\n\nKelly Marchisio, Wei-Yin Ko, Alexandre Berard, Théo Dehaze, and Sebastian Ruder. Understanding\n  and mitigating language confusion in LLMs.  In Yaser Al-Onaizan, Mohit Bansal, and Yun-\n  Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language\n  Processing, pp. 6653–6677, Miami, Florida, USA, November 2024. Association for Computational\n  Linguistics. doi: 10.18653/v1/2024.emnlp-main.380. URL https://aclanthology.org/2024.\n  emnlp-main.380/.\n\nPedro Henrique Martins, João Alves, Patrick Fernandes, Nuno M. Guerreiro, Ricardo Rei, Amin\n  Farajian, Mateusz Klimaszewski, Duarte M. Alves, José Pombal, Nicolas Boizard, Manuel Faysse,\n  Pierre Colombo, François Yvon, Barry Haddow, José G. C. de Souza, Alexandra Birch, and André\n  F. T. Martins. Eurollm-9b: Technical report, 2025. URL https://arxiv.org/abs/2506.04079.\n\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\n  Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto.  s1: Simple test-time\n  scaling, 2025. URL https://arxiv.org/abs/2501.19393.\n\nAyomide Odumakinde, Daniel D’souza, Pat Verga, Beyza Ermis, and Sara Hooker.  Multilingual\n  arbitration: Optimizing data pools to accelerate multilingual progress. In Wanxiang Che, Joyce\n  Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd\n  Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n  19142–19164, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-\n  8-89176-251-0. doi: 10.18653/v1/2025.acl-long.939. URL https://aclanthology.org/2025.ac\n  l-long.939/.\n\nVishakh Padmakumar and He He. Does writing with language models reduce content diversity?,\n  2024. URL https://arxiv.org/abs/2309.05196.\n\nSurangika Ranathunga and Nisansa de Silva. Some languages are more equal than others: Probing\n  deeper into the linguistic disparity in the NLP world.  In Yulan He, Heng Ji, Sujian Li, Yang\n  Liu, and Chua-Hui Chang (eds.), Proceedings of the 2nd Conference of the Asia-Pacific Chapter\n  of the Association for Computational Linguistics and the 12th International Joint Conference\n  on Natural Language Processing (Volume 1: Long Papers), pp. 823–848, Online only, November\n  2022. Association for Computational Linguistics.  doi: 10.18653/v1/2022.aacl-main.62. URL\n  https://aclanthology.org/2022.aacl-main.62/.\n\n\n                                              20\n\nAngelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shiv-\n  alika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso\n  Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny\n  Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou,\n  Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam,\n  Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm,\n  Fajri Koto, Dominik Krzemiński, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel\n  Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish\n  Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Mar-\n  jana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar\n  Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol\n  Schlag, Marzieh Fadaee, Sara Hooker, and Antoine Bosselut. Include: Evaluating multilingual lan-\n  guage understanding with regional knowledge, 2024. URL https://arxiv.org/abs/2411.19799.\n\nDimitri Schreiter. Prompt engineering: How prompt vocabulary affects domain knowledge, 2025.\n URL https://arxiv.org/abs/2505.17037.\n\nChantal Shaib, Joe Barrow, Jiuding Sun, Alexa F. Siu, Byron C. Wallace, and Ani Nenkova.\n  Standardizing the measurement of text diversity: A tool and a comparative analysis of scores,\n  2025. URL https://arxiv.org/abs/2403.00553.\n\nFreda  Shi, Mirac Suzgun, Markus  Freitag, Xuezhi Wang, Suraj  Srivats, Soroush Vosoughi,\n  Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.\n  Language models are multilingual chain-of-thought reasoners.  In The Eleventh International\n  Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=fR3w\n  GCk-IXp.\n\nLuísa Shimabucoro, Sebastian Ruder, Julia Kreutzer, Marzieh Fadaee, and Sara Hooker. LLM\n   see, LLM do:  Leveraging active inheritance to target non-differentiable objectives.  In Yaser\n  Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on\n  Empirical Methods in Natural Language Processing, pp. 9243–9267, Miami, Florida, USA, Novem-\n  ber 2024. Association for Computational Linguistics.  doi: 10.18653/v1/2024.emnlp-main.521.\n URL https://aclanthology.org/2024.emnlp-main.521/.\n\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David Ifeoluwa Adelani, Jian Gang Ngui,\n  Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto,\n  Raymond Ng, Shayne Longpre, Sebastian Ruder, Wei-Yin Ko, Antoine Bosselut, Alice Oh, Andre\n  Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and\n  Sara Hooker. Global MMLU: Understanding and addressing cultural and linguistic biases in multi-\n  lingual evaluation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher\n  Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational\n  Linguistics (Volume 1: Long Papers), pp. 18761–18799, Vienna, Austria, July 2025. Association\n   for Computational Linguistics. ISBN 979-8-89176-251-0.  doi: 10.18653/v1/2025.acl-long.919.\n URL https://aclanthology.org/2025.acl-long.919/.\n\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej,\n  Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas\n  Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon,\n  Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai\n  Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman,\n  Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-\n  Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan\n  Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen,\n\n\n                                              21\n\nAbhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade,\n  Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György,\n  André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson,\n  Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Char-\n   lie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel\n  Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivaku-\n  mar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eu-\n  gene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna\n  Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian\n  Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John\n  Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder\n  Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella\n  Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi,\n  Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Mom-\n  chev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan\n  Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk,\n  Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith\n  Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy,\n  Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang,\n  Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Rose-\n  berry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow,\n  Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao,\n  Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero,\n  Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle\n  Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel,\n  Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet,\n  Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud,\n  Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard\n  Hussenot. Gemma 3 technical report.\n\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield,\n  Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler\n  Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez,\n  Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe,\n  Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov,\n  Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre\n  Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language\n   left behind: Scaling human-centered machine translation. 2022.\n\nBrian Thompson, Mehak Dhaliwal, Peter Frisch, Tobias Domhan, and Marcello Federico. A shock-\n  ing amount of the web is machine translated:  Insights from multi-way parallelism.  In Lun-\n  Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Com-\n  putational Linguistics: ACL 2024, pp. 1763–1775, Bangkok, Thailand, August 2024. Asso-\n  ciation for Computational Linguistics.   doi:  10.18653/v1/2024.f indings-acl.103.  URL\n  https://aclanthology.org/2024.findings-acl.103/.\n\nAhmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude,\n  Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne\n  Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya model: An\n  instruction finetuned open-access multilingual language model. In Lun-Wei Ku, Andre Martins,\n  and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Com-\n  putational Linguistics (Volume 1: Long Papers), pp. 15894–15939, Bangkok, Thailand, August\n\n\n                                              22\n\n2024. Association for Computational Linguistics.  doi: 10.18653/v1/2024.acl-long.845. URL\n  https://aclanthology.org/2024.acl-long.845/.\n\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei\n  Lin, and Daxin Jiang.  WizardLM: Empowering large pre-trained language models to follow\n  complex instructions.  In The Twelfth International Conference on Learning Representations,\n  2024. URL https://openreview.net/forum?id=CfXh93NDgH.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao\n  Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. LMSYS-\n  chat-1m: A large-scale real-world LLM conversation dataset. In The Twelfth International Confer-\n  ence on Learning Representations, 2024. URL https://openreview.net/forum?id=BOfDKxfwt0.\n\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\n  Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\n  LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing\n  Systems, 2023. URL https://openreview.net/forum?id=KBMOKmX2he.\n\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen:\n A benchmarking platform for text generation models.  In The 41st International ACM SIGIR\n  Conference on Research & Development in Information Retrieval, SIGIR ’18, pp. 1097–1100,\n  New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450356572.  doi:\n  10.1145/3209978.3210080. URL https://doi.org/10.1145/3209978.3210080.\n\n\n\n\n\n                                              23\n\nA  Use of AI Disclosure\n\nFor this paper we used AI to help with plotting code and grammar correction.\n\n\nB  Focus Languages\n\n\n Language       Script  Lang. Family    Resourcedness   WMT24++        Flores        Prompts\n (code)                                             Institution.  Data  Expert  Gemma  Expert  Gemma  Expert  Gemma\n\n German (de)     Latn    IE / Germanic        high       5     91.91    83.64    97.78    92.41    93.96    92.49\n Spanish (es)      Latn    IE / Italic            high       5     87.98    73.30    96.26    92.78    89.15    86.40\n Czech (cs)       Latn    IE / Balto-Slavic      high       4     86.11    69.37    96.64    91.91    86.00    82.03\n Ukrainian (uk)    Cyrl    IE / Balto-Slavic      high       4     84.87    69.68    94.70    88.69    82.23    79.51\n Greek (el)       Grek    IE / Greek           high       3     84.62    69.86    93.37    85.58    83.86    80.78\n ⋆Hungarian (hu)  Latn     Uralic / Finnic       high       4     81.64    66.19    92.48    88.49    83.61    78.88\n ⋆Slovak (sk)      Latn    IE / Balto-Slavic      high       3     82.67    66.83    94.53    91.54    85.71    81.36\n ⋆Croatian (hr)    Latn    IE / Balto-Slavic      high       3     81.39    69.51    92.60    89.83    79.91    78.86\n ⋆Lithuanian (lt)   Latn    IE / Balto-Slavic      high       3     76.58    61.12    89.10    82.45    84.11    82.40\n ⋆Latvian (lv)     Latn    IE / Balto-Slavic      high       3     64.15    60.90    76.86    77.59    69.65    73.18\n ⋆Basque (eu)     Latn    Basque             mid       4     —          62.87    63.20    66.01    70.19\n ⋆Welsh (cy)      Latn    IE / Celtic          mid       3     —          79.22    62.02    73.75    68.30\n\n Avg              –       –                     82.19     69.04   88.87    83.87    81.04    79.53\n\nTable 6: Language Overview: We characterize the languages of study (1) in terms of resourcedness with\nrespect to data availability with levels estimated by Ranathunga & de Silva [2022] (Data), and whether they are\nmid or high-institutional in terms of vitality according to Ethnologue (Institution.) (data from [Ranathunga\n& de Silva, 2022]) and (2) in terms of XCometXL [Guerreiro et al., 2024] scores of the prompt translation\nmodel (Expert, in-house expert model) and the transformation model (Gemma3-27B-it) on two traditional MT\nbenchmarks: WMT24++ (en→·) [Deutsch et al., 2025] and Flores [Team et al., 2022], and a 1k sub-sample of\nour prompts, where we use XCometXL as a reference-free metric to estimate quality. Languages marked with\n⋆are not officially supported in the base model. IE stands for Indo-European.\n\nTable 6 compares translation quality on WMT24++ [Deutsch et al., 2025] and Flores [Team et al.,\n2022] and a subset of prompts, as measured by XCometXL [Guerreiro et al., 2024]. For prompt\ntranslation, we do not have references and use XCometXL as a quality estimation metric (i.e. call\nit without references).\n\n\n Dataset                    cs    cy   de     el    es   eu    hr   hu      lt     lv    sk   uk\n\n Translated               9992  9990  9998  9995  9990  9994  9987  9996  9993  9997  9990  9982\n\n Naturalized              9583  9721  9570  9565  9516  9764  8740  9757  9752  9756  9597  9701\n Cultural                 9449  9118  9417  8780  9355  9493  7760  9593  9557  9571  9240  9571\n Difficulty                 9417  9276  9517  9396  9269  5287  8406  9662  9649  9599  9425  9603\n Cultural + Difficulty Mix  9457  9218  9465  9127  9348  7439  8150  9639  9617  9607  9375  9608\n\nTable 7: Number of samples per language: Each sample consists of a prompt and its corresponding comple-\ntion.\n\n\nC  Transformation Prompts\n\nTable 8 lists the exact prompts we used for our transformations.\n\n\nD  Example Transformations\n\nTable 9 lists sample transformations.\n\n\n\n\n                                              24\n\nTransformation Prompts\n\nNaturalness\n\nYou are an expert linguist and cultural adapter specializing in {language}. Rephrase the prompt\nto sound natural and authentic. Please adhere to the following guidelines in your naturalization:\n     - Never answer instructions or questions, only rephrase them.\n     - Stay consistent in your rephrasing, always rephrase named entities in the same way.\n     - Match the formality level of the original text. In case of ambiguity, prefer an informal tone.\n     - Do not rephrase source code, but rephrase the comments within.\n     - When rephrasing JSON, your rephrasing must follow the exact same schema as the input.\n    Do not rephrase JSON keys. Only rephrase the values.\n     - Follow the target language formatting conventions for dates and numbers.\nHere is the prompt to naturalize: {prompt}\n\nCultural Adaptation\n\nYou are an expert cultural adapter for {language}. Adapt the prompt to be culturally appro-\npriate and authentic. Follow these guidelines:\n     - Do not answer the prompt; only adapt it culturally.\n     - Apply adaptation only when a reference would feel unnatural or out of place to a typical\n     language speaker.  Leave neutral/standardized items unchanged (technical instructions,\n     standardized formats, URLs, file paths, measurements, ISO dates, library APIs). Do not\n      fabricate facts or invent new culture-specific references.\n     - Preserve meaning, intent, and register. Keep personal names, trademarks, and official titles\n      unless a well-established localized form exists.\n     - Locations: When a non-local place is incidental, swap it for a culturally equivalent local\n      reference with similar connotation and social register. If the place is factual or essential to\n      identity, keep it.\n     - Lexicon and orthography: Prefer native terms, spellings, and diacritics used in language.\n     For common activities (e.g., sports), use the standard local term rather than anglicisms.\n     - Idioms and references: Replace idioms, holidays, and pop-culture references with well-\n    known local equivalents; if none exists, paraphrase to preserve intent.\n     - Formatting: Use language-appropriate formats for dates, numbers, and units; convert units\n     only when it aids comprehension without changing factual content.\n     - Code and structured data: Do not alter source code; comments may be adapted.  For\n    JSON, keep the same schema and keys; only adapt values.\nHere is the prompt to adapt: {prompt}\n\nDifficulty\n\nYou are an expert task designer for {language}. Rewrite the prompt to increase its difficulty\nand complexity while preserving the original intent and domain. Follow these guidelines:\n     - Do not answer the prompt; only rewrite it to be more difficult.\n     - Preserve the original intent and domain. Do not change the topic or introduce factual\n      errors.\n     - Tailor the rewrite to the prompt’s task type (e.g., coding, data wrangling/SQL, creative\n      writing, analysis/explanation, classification, dialogue/roleplay).\n     - Code and JSON: do not alter source code; comments/instructions may be made more\n     demanding. For JSON, keep the same schema and keys; only adapt values and constraints.\n     - Ensure the task remains solvable with the given information;  if extra assumptions are\n     needed, require the solver to list and justify them.\nHere is the prompt to complexify: {prompt}\n\n      Table 8: Transformation prompt templates with placeholders for language and source prompt.\n\n                                             25\n\nTransformation Result\n\n Seed prompt   What’s the difference between thai ginger and regular ginger?\n\n Translation(de)Was ist der Unterschied zwischen thailändischem Ingwer und normalem Ingwer?\n\n Naturalisation Was ist der Unterschied zwischen thailändischem Ingwer und dem Ingwer, den man\n                normalerweise kennt?\n Cultural       Was ist der Unterschied zwischen thailändischem Ingwer und dem Ingwer, den man\n Adaptation    hierzulande üblicherweise im Handel findet?\n\n Difficulty      Vergleichen und kontrastieren Sie detailliert thailändischen Ingwer (Kha) mit dem\n                in der europäischen Küche üblicherweise verwendeten Ingwer (Zingiber officinale).\n                Ihre Analyse soll folgende Aspekte berücksichtigen:  botanische Unterschiede\n                (einschließlich, aber nicht beschränkt auf Rhizom-Morphologie und ätherische\n                Öl-Zusammensetzung), typische Anbaubedingungen und -methoden in Thailand versus anderen\n                Anbauregionen, kulinarische Anwendungen und Geschmacksnuancen (unter Berücksichtigung\n                regionaler Variationen innerhalb Thailands und Europas), sowie traditionelle\n                medizinische Verwendungen in den jeweiligen Kulturen.\\n\\nIhre Antwort soll in\n                Form eines strukturierten Essays verfasst sein, der mindestens 500 Wörter umfasst.\n                Zitieren Sie mindestens drei wissenschaftliche Quellen (z.B. botanische Datenbanken,\n                ethnobotanische Studien, landwirtschaftliche Forschungsberichte) und zwei Kochbücher\n                oder kulinarische Fachartikel, die sich explizit mit Ingwer befassen.\\n\\nDarüber\n                hinaus sollen Sie eine Tabelle erstellen, die die wichtigsten Unterschiede in den\n                oben genannten Aspekten übersichtlich zusammenfasst.\\n\\nSollten Sie Annahmen über\n                spezifische Ingwersorten innerhalb Europas treffen (z.B. hinsichtlich Herkunft oder\n                Anbaumethoden), so sind diese explizit zu benennen und zu begründen.  Ebenso sind\n                etwaige Schwierigkeiten bei der Beschaffung spezifischer Informationen zu Kha zu\n                erwähnen und die angewandten Strategien zur Informationsfindung zu erläutern.\n\n Seed prompt   Is it possible to have a permanent job and a part-time job at the same time in the UK?\n\n Translation    ¿Es posible tener un trabajo permanente y un trabajo a tiempo parcial al mismo tiempo\n (es)           en el Reino Unido?\n\n Naturalisation ¿Se puede tener un trabajo fijo y otro a media jornada a la vez en el Reino Unido?\n\n Cultural       ¿Es posible compaginar un trabajo a tiempo completo con otro a media jornada en el\n Adaptation    Reino Unido?\n\n Difficulty      Un ciudadano español está considerando una oferta de empleo a tiempo parcial en\n                el Reino Unido mientras ya mantiene un contrato de trabajo a tiempo completo con\n                una empresa española que permite el trabajo remoto desde el extranjero.  Analice\n                exhaustivamente la legalidad y las implicaciones fiscales de esta situación,\n                considerando tanto la legislación laboral del Reino Unido como las regulaciones\n                fiscales españolas e internacionales (incluyendo posibles convenios de doble\n                imposición).  Su respuesta debe:\\n\\n1.  **Detallar las restricciones legales del\n                Reino Unido** con respecto a la compatibilidad de empleos, incluyendo la consideración\n                de las cláusulas de exclusividad en los contratos de trabajo (si las hubiera)\n                y las regulaciones específicas para trabajadores extranjeros.\\n2.  **Explicar\n                las obligaciones fiscales del ciudadano español** en España, incluyendo cómo se\n                tratarían los ingresos obtenidos en el Reino Unido y si estaría sujeto a algún\n                tipo de declaración especial.\\n3.  **Evaluar las posibles implicaciones en la\n                Seguridad Social** tanto en España como en el Reino Unido, considerando la posibilidad\n                de cotizaciones en ambos países y cómo esto afectaría a sus derechos a futuras\n                prestaciones.\\n4.  **Identificar y justificar cualquier suposición** que deba hacerse\n                para completar el análisis, por ejemplo, sobre el tipo de contrato de trabajo en\n                España, el nivel de ingresos en ambos empleos, o la existencia de un convenio de doble\n                imposición aplicable.\\n5.  **Presentar un resumen conciso** de los riesgos y beneficios\n                clave para el ciudadano español, incluyendo recomendaciones sobre los pasos a seguir\n                para garantizar el cumplimiento legal y fiscal.\\n\\nLa respuesta debe estar redactada\n                en un español formal y preciso, demostrando un conocimiento profundo de la legislación\n                laboral y fiscal relevante.  Se valorará la capacidad de presentar información compleja\n                de manera clara y organizada.\n\n\nTable 9: Example prompts and their respective transformations for German (de) above and Spanish (es) below.\n\n                                              26\n\nLanguage  Translation  Naturalized  ∆\n\n                     de              91.50          91.11       -0.39\n                         es               85.42          84.88       -0.56\n                         cs               86.21          83.27       -2.95\n                   hu              84.84          85.25      +0.41\n                      hr               80.45          79.68       -0.77\n                   uk              85.34          85.10       -0.24\n                             el               82.46          84.14      +1.70\n                       sk               80.21          77.86       -2.35\n                               lt               77.67          79.07      +1.40\n                          lv               68.07          67.11       -0.96\n                     eu              62.53          65.43      +2.90\n                      cy              69.62          70.32      +0.70\n\n                   Avg            79.53          79.43       -0.10\n\n\nTable 10:  Translation Quality  of Naturalized vs Machine-Translated:  We compare  reference-free\nXCometXL scores before and after the naturalness transformation on a random sample of 100 prompts.\n\n\nE  Naturalness vs Translation\n\nOur naturalness transformation can be seen as a form of post-editing. Therefore, we evaluate the\ntransformed prompts with a reference-free machine translation metric, XCometXL. Table 10 shows\nthat for most languages (7/12), translation quality is worsening after naturalization. This does not\nmean that quality decreases, it just means that it is less directly aligned with the source prompt\nbefore translation, according to the metric. Naturally, the languages with lowest translation quality\nhave the highest potential for improving translation quality via the naturalness process, here Basque\nand Welsh.\n\n\nF  Training\n\nWe trained the model using supervised fine-tuning (SFT) with a cross-entropy loss function. Op-\ntimization was carried out using the Adam optimizer, configured with hyperparameters β1 = 0.9,\nβ2 = 0.95, and ϵ = 10−8. We applied an additive weight decay of 0.1 and used gradient clipping with\na maximum norm of 1.0. Training was conducted for two epochs, and evaluation was performed\nusing the final checkpoint. We used a batch size of 32 and employed a cosine learning rate decay\nschedule, with a peak learning rate of 2.5 × 10−4 and an end learning rate of 1.25 × 10−4.\n\n\nG  Evaluation\n\nG.1  Overview\n\nTable 3 lists covered languages and metrics for each of the included benchmarks.\n\n\n\n\n\n                                              27\n\nName             Language List                        Metric\n\n   GlobalMMLU            cs, en, de, el, lt, es, uk                     Accuracy\n   Include44               eu, hr, de, el, hu, lt, es, uk                 Accuracy\n\n    Flores                  eu, hr, cs, de, el, hu, lv, lt, sk, es, uk, cy     xCometXL\n  MGSM++             eu, cs, en, de, el, hu, es, cy                 Accuracy\n\n   PolyWrite              en, de, es, cs, uk, hu, el, sk, hr, lt, lv, eu, cy  self-BLEU, win rates\n   mArenaHard-v2.0++   en, de, es, cs, uk, el, lv, lt, hu, hr, sk, cy, eu  win rates\n\n\nTable 11: Evaluation suite: We evaluate on close-ended and open-ended tasks, which each cover a subset of\nour 13 focus languages. MGSM++ is an extension of MGSM [Shi et al., 2023] based on publicly available\nhigh-quality translations.\n\n\nG.2  Implementation\n\nFor MGSM, use simple-evals,16 for MCQA tasks we use the generative form of evaluation that\noperates on tokens and not on likelihoods, as also implemented in simple-evals. For win-rates we\nuse our own in-house implementation with LLM judge prompts specified in the next section. The\norder of models presented in preference ratings is shuffled.\n\n\nG.3  Prompts\n\nFor general win-rates we follow the LLM judge prompt proposed in [Üstün et al., 2024] that not\nonly asks the judge to select the more correct generation, but also specifies the target language and\nindicates that it should be grammatically correct and fluent. For naturalness win-rates we use the\nprompt defined in table 13.\n\nTo grade prompts and completions according to difficulty and quality using an LLM as described\nin section 3.3.1, we used the prompt defined in table 12.\n\n\nH  Grouped results\n\nH.1  Include44 Results by Domain\n\nTable 14 presents a domain breakdown of the Include44 results for each model.\n\n\nH.2 G-MMLU Cultural Sensitivity\n\nTable 15 presents Global-MMLU results for questions annotated as cultural-agnostic(CA) and\ncultural-sensitive(CS). Each category contains 200 questions and annotations are only available\nfor English, German and Spanish.\n\n\nH.3  Supported vs Unsupported performance\n\nTable 16 presents performance comparison on Flores for languages grouped by supported languages\n(languages that were included during pretraining of the base model) and unsupported languages.\nList of supported languages is showed in section 3.2.1.\n\n  16https://github.com/openai/simple-evals\n\n\n                                              28\n\nCompletion Grader (Quality & Difficulty Evaluation)\n\nYou are a strict grader of responses. The questions are enclosed in <question></question>\ntags, and the answers are enclosed in <answer></answer> tags. Given a question–answer\npair, evaluate the <answer> according to the following criteria:\n\n     - Does the <answer> address the <question> fully in all parts?\n     - Is the <question> logically sound?\n     - Is the <answer> logically sound?\n     - Is the <answer> factually correct and coherent?\n     - Does the <answer> contain any hallucinations?\n     - Is the <answer> properly formatted?\n     - Does the <answer> use correct punctuation for the given language?\n     - Are there any other issues with the <question> or <answer> not covered above?\n\n\nAny <question> or <answer> that fails to meet the above criteria should be penalized\naccordingly.\n\n     - Give the question and answer pair a quality rating between [A,B,C,D,E] with A being\n      best and E being worst. The rating should be at the very end and inside the tag\n     \"<quality> </quality>\" without markdown formatting.\n     - Give the question and answer pair a difficulty rating between [easy, medium, hard].\n    The rating should come after the quality rating and inside the tag \"<difficulty>\n      </difficulty>\" without markdown formatting.\n\n\nPrompt Grader (Prompt Quality & Difficulty Evaluation)\n\nYou are a strict Grader of prompts. The prompt is inside <prompt></prompt> tags. Given\nthe following <prompt>, please grade it based on the following criteria:\n\n     - Is the <prompt> clear and unambiguous?\n     - Is the <prompt> logically sound and coherent?\n     - Is the <prompt> factually correct?\n     - Does the <prompt> provide sufficient context for a meaningful response?\n     - Is the <prompt> well-structured and grammatically correct?\n     - Does the <prompt> have a clear objective or question?\n     - Is the <prompt> appropriately scoped (not too broad or too narrow)?\n     - Does the <prompt> avoid contradictions or false premises?\n\n\nAny <prompt> that fails to address any of the criteria sufficiently should be penalized\naccordingly.\n\nPlease think step-by-step and elaborate first before doing the following:\n\n     - Give the prompt a quality rating between [A,B,C,D,E] with A being best and E\n     being worst. The rating should be at the very end and inside the tag \"<quality>\n     </quality>\" without markdown formatting.\n     - Give the prompt a difficulty rating between [easy, medium, hard]. The rating should\n     come after the quality rating and inside the tag \"<difficulty> </difficulty>\" without\n    markdown formatting.\n\n\n Table 12: Grader prompts to evaluate completions29 and prompts for overall quality and difficulty.\n\nJudge Prompt\n\n   Which of the following responses is the most natural-sounding and overall best one\n     for the given instruction in {language}? A good response should follow these rules, with\n    a primary focus on Rule 5 (Naturalness):\n           - 1) It should be in {language}.\n           - 2) It should complete the request in the instruction.\n           - 3) It should be factually correct and semantically comprehensible.\n           - 4) It should be grammatically correct and fluent.\n           - 5) Crucially, it should sound natural in {language}. This means it uses com-\n       mon phrasing, appropriate tone, and idiomatic expressions (where suitable) that a\n          native speaker would typically use. It should avoid awkward, stilted, or \"translated-\n         sounding\" sentences.\n\n\n\n     Instruction: {prompt}\n    Response (A): {completion_a}\n    Response (B): {completion_b}\n\n   FIRST provide a concise comparison of the two responses, evaluating primarily which\n    response sounds more natural and authentic in {language}. Consider if it uses\n   common phrasing and tone typical of a native speaker, while also meeting the other criteria\n    (completeness, correctness, grammar). If one Response is better, explain which you prefer\n    and why, highlighting differences in naturalness. If both responses are identical or equally\n    good or bad (especially in terms of naturalness), explain why.\n   SECOND, on a new line, state exactly one of ’Response (A)’ or ’Response (B)’ or ’TIE’\n    to indicate your choice of preferred response.\n\n    Your response should use the format:\n    Comparison: <concise comparison and explanation, focusing on naturalness>\n     Preferred: <’Response (A)’ or ’Response (B)’ or ’TIE’>\n\n\n           Table 13: Judge prompt to do pairwise open-ended evaluation focused on naturalness.\n\n\n\n\n\n          Category         Business  Culture  Health  Other STEM\n\n            Translated             0.524      0.524     0.320    0.396    0.410\n\n            Naturalized            0.627      0.515     0.372    0.372    0.439\n            Cultural               0.534      0.568     0.372    0.399   0.482\n              Difficulty            0.689      0.581    0.399   0.372    0.444\n             Cult. + Diff. (mix)     0.588     0.587     0.393    0.411    0.447\n\n\nTable 14: Include44 Accuracy By Domain: average accuracy for each model by question domain. Highest\nvalue per domain is highlighted in bold.\n\n\n\n\n\n                                              30\n\nModel                 Cultural-Agnostic (CA)  Cultural-Sensitive (CS)\n\n      Translated                           0.677                       0.605\n\n      Naturalized                           0.676                       0.635\n       Cultural                              0.700                    0.670\n       Difficulty                         0.708                       0.666\n       Cultural + Difficulty Mix             0.683                       0.642\n\n\nTable 15: Performance on GMMLU by cultural sensitivity. Comparison of model accuracies on cultural-\nagnostic (CA) and cultural-sensitive (CS) subsets.  Higher is better.  Only includes English, German and\nSpanish, the CS-annotated languages that overlap with our subset of languages.\n\n              Model               Unsupported  Supported\n                   Translated                      0.716          0.882\n\n                   Naturalized                     0.719          0.890\n                    Cultural                        0.732          0.907\n                      Difficulty                       0.749          0.908\n                    Cultural + Difficulty Mix       0.742          0.905\n\nTable 16: Performance comparison in machine translation (XCometXL score on Flores), averaged across lan-\nguages grouped by pretraining support (see table 1).\n\n\nH.4  Downstream Results By Language\n\nTable 17, table 18, table 19, table 20, table 21 and table 22 present language breakdowns for the\nperformance of each model on the benchmarks. Table 23 presents the language breakdowns for the\nperformance of the Cultural+Difficulty model against an external model in open-ended generation.\n\n\n\n\n\n Model              cs     cy     de       el     en     es     eu     hr    hu       lt      lv     sk    uk   Average\n\n  Naturalized         0.552   0.537*  0.533*   0.550   0.800   0.568   0.592   0.517*   0.545   0.600   0.562   0.558   0.600     0.578\n  Cultural          0.623   0.606    0.646   0.629   0.846  0.700   0.622    0.636   0.640   0.666   0.602   0.601   0.723     0.657\n  Difficulty           0.597   0.564   0.653  0.630   0.815   0.667   0.544*   0.578   0.605   0.587   0.655   0.540*   0.600     0.618\n  Cult. + Diff. Mix   0.617   0.685   0.630   0.643  0.851   0.692   0.623  0.676  0.714  0.683  0.663  0.654  0.662   0.676\n\nTable 17: mArenaHard++ results:  win-rates over translated baseline.  Best score per language in bold.\nValues marked with an asterisks indicate win-rate differences are not significant according to 95% CIs.\n\n\n                                              31\n\nModel                     cs     cy    de       el     en      es     eu     hr    hu       lt      lv     sk    uk   Average\n\n  Naturalized                0.682   0.607   0.662   0.626   0.632   0.684   0.686   0.597   0.623   0.600   0.603   0.675   0.617     0.638\n  Cultural                 0.692   0.613   0.701   0.619    0.594   0.774   0.677   0.649   0.672   0.665   0.626   0.630   0.682    0.661\n  Difficulty                   0.646   0.598  0.766  0.555*  0.574*  0.787  0.697  0.571*  0.695  0.710   0.594   0.552*   0.656     0.646\n  Cultural + Difficulty Mix   0.591  0.668   0.740   0.568*  0.742   0.755   0.677   0.649   0.682   0.684  0.677   0.623   0.643    0.669\n\nTable 18: PolyWrite results: win-rates over Translated baseline. Best score per language in bold. Values\nmarked with an asterisks indicate win-rate differences are not significant according to 95% CIs.\n\n\n Model                     cs     de      el     en     es       lt    uk   Average\n\n Translated                 0.529   0.533   0.506   0.605   0.558   0.440   0.503     0.525\n\n Naturalized                0.535   0.546   0.507   0.602   0.567   0.450   0.510     0.531\n Cultural                 0.579  0.590  0.556  0.665  0.619   0.486   0.554    0.579\n Difficulty                   0.555   0.535   0.540   0.575   0.565  0.495  0.555    0.545\n Cultural + Difficulty Mix   0.573   0.583   0.550   0.649   0.613   0.488   0.551     0.572\n\n\n              Table 19: Global MMLU results: Accuracy. Best score per language in bold.\n\n\n Model                 de      el      es     eu     hr    hu       lt    uk   Average\n\n  Translated                 0.489   0.409   0.544   0.358   0.556   0.373   0.489   0.542     0.470\n\n  Naturalized                0.460   0.427   0.575   0.334   0.567   0.391   0.453   0.542     0.469\n  Cultural                   0.496   0.447   0.613   0.346   0.620   0.405   0.530  0.609    0.508\n  Difficulty                0.518   0.465   0.598   0.336  0.651  0.407  0.539   0.580     0.512\n  Cultural + Difficulty Mix   0.482  0.480  0.644  0.372   0.644   0.405   0.530   0.584    0.518\n\n\n                Table 20: Include 44 results: Accuracy. Best score per language in bold.\n\n\n Model                     cs     cy    de      el      es     eu     hr    hu       lt      lv     sk    uk   Average\n\n  Translated                 0.879   0.671   0.947   0.834   0.906   0.614   0.808   0.710   0.711   0.692   0.810   0.845     0.786\n\n  Naturalized                0.882   0.662   0.946   0.844   0.933   0.615   0.802   0.714   0.725   0.693   0.825   0.846     0.791\n  Cultural                   0.904   0.673  0.954   0.858   0.935   0.614   0.821   0.733   0.736   0.707   0.842   0.881     0.805\n  Difficulty                   0.905  0.698  0.954  0.861  0.936   0.588   0.831   0.764   0.767   0.746   0.856   0.884     0.816\n  Cultural + Difficulty Mix   0.900   0.684  0.954   0.856   0.933   0.601   0.822   0.747   0.757   0.731   0.855   0.880     0.810\n Gemma3-27B           0.918   0.623   0.924   0.855   0.928  0.633  0.901  0.886  0.824  0.776  0.916  0.888   0.839\n\n                  Table 21: Flores: XCometXL scores. Best score per language in bold.\n\n\n Model                     cs     cy    de      el     en     es     eu    hu   Average\n\n  Translated                 0.665   0.350   0.645   0.625   0.745   0.705   0.295   0.470    0.5625\n\n  Naturalized                0.640   0.295   0.665   0.680   0.750   0.735   0.265   0.490    0.5650\n  Cultural                   0.736   0.456   0.740   0.752   0.840  0.800   0.376   0.580     0.660\n  Difficulty                   0.750   0.385  0.775  0.780   0.810   0.790   0.320  0.595    0.6506\n  Cultural + Difficulty Mix  0.780  0.472   0.756   0.752  0.876   0.788  0.400   0.556    0.673\n\n\n                         Table 22: MGSM++: best score per language in bold.\n\n\n Benchmark    cs     cy    de      el    en     es     eu     hr    hu       lt      lv     sk    uk   Average\n\n  mArenaHard   0.427  0.794   0.438  0.696  0.226   0.421  0.766  0.578  0.603  0.660  0.648  0.568  0.556   0.568\n  PolyWrite     0.942  0.794  0.890  0.994  0.284  0.768  0.974  0.974  0.987  0.981  0.948  0.987  0.974   0.884\n\nTable 23: Open Ended Win-Rates against Qwen2.5-7B: Win-rates across languages from direct comparisons\nof the Cultural+Difficulty model against Qwen2.5-7B on mArenaHard and PolyWrite. Language where our\nmodel wins are in bold. All win-rate differences are significant according to 95% CIs.\n\n\n\n                                              32",
"headers": [
"The Art of Asking:",
"arXiv:2510.19806v1  [cs.CL]  22 Oct 2025",
"Multilingual Prompt Optimization for Synthetic Data",
"Abstract",
"1",
"Introduction",
"2",
"Method",
"3",
"Experiments",
"4",
"Results",
"5",
"Related Work",
"6",
"Conclusion",
"Limitations",
"Acknowledgments",
"References",
"A",
"Use of AI Disclosure",
"B",
"Focus Languages",
"C",
"Transformation Prompts",
"D",
"Example Transformations",
"E",
"Naturalness vs Translation",
"F",
"Training",
"G",
"Evaluation",
"H",
"Grouped results"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2510.19806v1.pdf"
}