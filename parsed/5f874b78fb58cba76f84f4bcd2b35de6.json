{
"text": "eARCO: Efficient Automated Root Cause Analysis with Prompt\n                            Optimization\n\n            Drishti Goel, Raghav Magazine, Supriyo Ghosh, Akshay Nambi, Prathamesh Deshpande, Xuchao\n                                  Zhang, Chetan Bansal, Saravan Rajmohan\n                                                                Microsoft\n\n      ABSTRACT                                                        to several information source such as troubleshooting guides, past\n          Root cause analysis (RCA) for incidents in large-scale cloud sys-      similar incidents, service properties and current incident metadata\n          tems is a complex, knowledge-intensive task that often requires      to identify the key reasons behind service disruptions. Accurately\n             significant manual effort from on-call engineers (OCEs). Improving      identifying the root cause early can significantly reduce time-to-\n        RCA is vital for accelerating the incident resolution process and re-      mitigate (TTM) by enabling faster execution of mitigation steps.\n           ducing service downtime and manual efforts. Recent advancements      Thus, automating RCA at an early stage could accelerate incident2025    in Large-Language Models (LLMs) have proven to be effective in       resolution, reduce service downtime, and minimize manual efforts\n           solving different stages of the incident management lifecycle in-     by OCEs.\n           cluding RCA. However, existing LLM-based RCA recommendations        Recent advancements in LLMs have proven to be effective for\n                                                                                  solving several problems in the incident management lifecycle, rang-Apr     typically leverage default finetuning or retrieval augmented genera-\n            tion (RAG) methods with static, manually designed prompts, which      ing from detection [13, 29] to triaging [4, 7] to automated query\n           lead to sub-optimal recommendations. In this work, we leverage      recommendations [20] to problem categorization [10] to root cause15           â€˜PromptWizardâ€™, a state-of-the-art prompt optimization technique,      generation [3, 16, 32]. For root cause generation, [3] first propose\n           to automatically identify the best optimized prompt instruction       to finetune a GPT-3 model with initial incident metadata (e.g., title,\n            that is combined with semantically similar historical examples for        initial summary and owning service name) as input and the cor-\n          querying underlying LLMs during inference. Moreover, by utiliz-     responding root cause as output in a supervised setting. However,\n           ing more than 180K historical incident data from Microsoft, we      finetuning LLMs is computationally expensive and maintenance-\n          developed cost-effective finetuned small language models (SLMs)      heavy. [32] proposed a retrieval-augmented generation (RAG) based[cs.SE]            for RCA recommendation generation and demonstrate the power      in-context learning (ICL) solution that dynamically retrieves simi-\n            of prompt optimization on such domain-adapted models. Our ex-       lar historical incidents during inference and prompts a pre-trained\n           tensive experimental results show that prompt optimization can    LLM with predefined, manually designed instructions. While this\n          improve the accuracy of RCA recommendations by 21% and 13%     RAG-based ICL method has demonstrated potential, significant chal-\n         on 3K test incidents over RAG-based LLMs and finetuned SLMs,      lenges arise when scaling it for RCA: (1) Static Prompts: Manually\n            respectively. Lastly, our human evaluation with incident owners      defined prompts lack the flexibility to adapt as tasks and models\n          have demonstrated the efficacy of prompt optimization on RCA rec-      evolve, making continual updates labor-intensive. (2) Sub-optimal\n         ommendation tasks. These findings underscore the advantages of     Guidance: Manually crafted prompts may not fully leverage the\n           incorporating prompt optimization into AI for Operations (AIOps)     LLMâ€™s potential, leading to sub-optimal RCA recommendations\n           systems, delivering substantial gains without increasing computa-     without automated optimization. (3) Cost and Scalability: While\n            tional overhead.                                       LLMs like GPT-4 are powerful, they are costly to deploy at scale.\n                                                                             This raises the need for more cost-effective solutions, such as fine-\n      KEYWORDS                                                   tuning smaller language models (SLMs), without compromising on\n                                                     RCA accuracy.\n          Root cause analysis, Incident Management, Prompt Optimization,\n                                                                                   In this paper, we introduce a novel framework for efficient\n         Domain Adaptation\n                                                                        automated root cause analysis with prompt optimization (eARCO).arXiv:2504.11505v1                                                            Our work addresses the following key research questions: (1) RQ1:        1  INTRODUCTION\n                                                                Can automatically optimized prompt instructions outperform man-\n          Over the past decade, large-scale cloud services have become essen-                                                                                   ually designed static instructions in improving the accuracy and\n               tial for deploying and maintaining complex interdependent systems.                                                                                  quality of RCA recommendations? (2) RQ2: Does the combina-\n           Despite significant reliability efforts, these services still experience                                                                                  tion of optimized prompt instructions and strategically selected\n           production incidents like unplanned outages or performance degra-                                                                                  in-context examples deliver superior RCA performance compared\n            dations, leading to customer dissatisfaction, revenue loss, and de-                                                                                     to using prompts or examples alone? and (3) RQ3: Can SLMs, when\n            clining trust. The current incident diagnosis process heavily relies                                                                                  paired with optimized prompt instructions, provide a cost-effective\n         on manual efforts from on-call engineers (OCEs), resulting in pro-                                                                                       alternative to querying expensive LLMs while maintaining compa-\n            ductivity loss. Additionally, limited expertise or domain knowledge                                                                                    rable RCA performance?\n        among OCEs can lead to sub-optimal actions, delaying service re-                                                                      To address these research questions, we leverage state-of-the-art\n           covery.                                                                   prompt optimization techniques to automatically generate opti-\n            Root cause analysis (RCA) is a critical and time-consuming step                                                                      mized prompt instructions tailored for the RCA task. The process\n            in the incident management lifecycle, requiring deep domain knowl-                                                                            begins by taking a task description and a few training examples\n          edge and back-and-forth communication among OCEs. OCEs refer\n\n                                                                                          1\n\nof incidents with their root causes, after which optimized prompts      Next, we discuss the potential advancements in incident manage-\nare derived. We then select in-context learning (ICL) examples that     ment enabled by recent developments in large language models\nclosely match the current incident.                                  (LLMs). This is followed by outlining our key research questions.\n   Specifically, we leverage PromptWizard [2], a discrete prompt op-       Lastly, we detail the data preparation strategy for the RCA task,\ntimization approach that evolves and adapts its prompts in real-time.     which includes steps for data curation, cleaning, and summariza-\nPromptWizard has shown superior performance across various NLP       tion.\ntasks by generating, critiquing, and refining prompts through an\niterative feedback loop. This \"critic-and-synthesize\" process con-     2.1  Incident Root Cause Analysis\ntinuously improves prompts without requiring additional model\n                                                                   Despite significant reliability efforts, large-scale cloud services in-\ntraining. By querying a pre-trained LLM fewer than 100 times, the\n                                                                       evitably encounter production incidents or outages, which can\nmethod remains computationally efficient while producing highly\n                                                                           result in significant customer impact and financial loss. On-call\noptimized prompt instructions for RCA. This automated prompt\n                                                                  engineers (OCEs) need extensive domain knowledge and expend\noptimization eliminates the need for manual prompt engineering,\n                                                                      considerable manual effort to diagnose and resolve these incidents.\nensuring that as pre-trained LLMs evolve, the system consistently\n                                                        The incident lifecycle typically involves four stages:\nprovides optimal instructions for improving model performance.\nOnce the optimized instruction is obtained, we develop a retrieval-         â€¢ Detection: Incidents are detected either by external/internal\naugmented generation (RAG) system inspired by Zhang et al. [32]             service users or through automated monitoring systems set\nto dynamically retrieve the top-K semantically similar historical in-          up to track service health and performance.\ncidents and their root causes. By incorporating these past incidents         â€¢ Triaging: Once detected, incidents are routed to the appro-\nand their root causes, the system adds valuable domain-specific              priate service teams or OCEs based on the incident properties\ncontext, enhancing the modelâ€™s ability to generate accurate RCA rec-          and team expertise.\nommendations. Our extensive experimental results, tested on 2,900         â€¢ Root cause analysis: OCEs engage in rounds of commu-\nreal-world incidents from Microsoft, demonstrate that combining             nication, analyzing logs, performance metrics, service de-\noptimized prompt instructions with semantically similar in-context            pendencies, and troubleshooting guides to identify the root\nexamples significantly improves the quality and accuracy of RCA             cause.\nrecommendations.                                                â€¢ Mitigation: Mitigation actions are taken based on the iden-\n   Despite the strong performance of large language models (LLMs)               tified root cause to resolve the incident and restore service\nfor RCA tasks, their use in production with long context lengths              functionality.\nis prohibitively expensive. To address this, we fine-tune smaller\n                                                               Root cause analysis is particularly challenging, requiring significant\nmodels like Phi-3-Mini, Phi-3-Medium [1], and Phi-3.5-Mini using\n                                                           manual effort and domain knowledge. Incidents may arise from var-\n180K historical incidents from Microsoft, incorporating metadata\n                                                                      ious sources, such as code bugs, configuration errors, dependency\nand root causes from over 1,000 services. Combining these fine-\n                                                                                failures, or hardware issues. Missteps in RCA can delay service\ntuned SLMs with optimized prompts from PromptWizard improves\n                                                                  recovery and exacerbate customer impact. Automating the identi-\nRCA accuracy by 13% compared to using incident context alone.\n                                                                            fication of potential root causes early in the incident lifecycle can\nThis demonstrates that fine-tuned SLMs with optimized prompts\n                                                                guide OCEs toward the correct resolution path, reducing overall\nprovide a cost-effective alternative to LLMs, reducing inference\n                                                                       time-to-mitigate (TTM) and minimizing customer disruption.\ncosts and complexity.\n   This paper makes three key contributions:\n                                                         2.2  Promise of LLMs in Incident Management\n    â€¢ We introduce eARCO, a novel framework that integrates     The rapid advancements in LLMs have led to exceptional perfor-\n      optimized prompts from PromptWizard with top-K semanti-     mance in a wide variety of natural language tasks, ranging from\n       cally similar historical incidents to generate accurate RCA     summarization and translation to question-answering and code\n      recommendations, dynamically adapting to task-specific needs.    completion. Furthermore, recent advancements in domain adap-\n    â€¢ We develop a cost-effective RCA solution by fine-tuning       tation using finetuning and few-shot learning have laid the foun-\n     SLMs on 180K historical incidents from over 1,000 services      dation for solving problems in different stages of the incident life-\n       at Microsoft. These SLMs, when queried with optimized       cycle. LLMs have shown great potential in automating detection,\n      prompts, offer a cost-effective alternative to expensive LLMs     problem categorization and accurate triaging of incidents which\n      without compromising performance.                        can alleviate the load and manual efforts from engineers. More-\n    â€¢ GPT-4-based evaluations show that prompt optimization im-      over, recent studies [3, 16, 32] have demonstrated the usefulness of\n      proves RCA accuracy by 21% for LLMs and 13% for SLMs.    LLMs in automatically identifying the root causes of an incident by\n     Human evaluations with domain experts confirm the practi-      leveraging initial incident metadata and similar historical incident\n       cal benefits, reporting enhanced RCA task performance.          properties, either by finetuning LLMs or using in-context (ICL)\n                                                                       learning framework. However, the use of static, manually designed\n                                                          prompt instructions can result in sub-optimal performance, and\n2 BACKGROUND                                       running LLMs with long context lengths for ICL remains costly.\nIn this section, we begin by introducing the incident management     To address these limitations, we propose using automatically opti-\ndomain and providing background on root cause analysis (RCA).     mized prompt instructions, coupled with cost-effective fine-tuned\n\n                                                                                          2\n\nsmaller language models (SLMs), to enhance root-cause generation      leading to hallucinations; and (c) accurate ground truth for root\naccuracy while minimizing operational costs.                         causes is essential for evaluating model effectiveness.\n                                                          To mitigate such interference, we employed a two-stage data\n2.3  Research Questions                                    cleaning strategy. First, we locally processed the data to remove\nOur goal is to address the following three main research questions       irrelevant HTML tags, stack traces, and image tags. In the second\n                                                                             stage, we utilized the GPT-3.5-turbo1 model to summarize the rootin this study:\n                                                                cause and summary fields, guided by a prompt as proposed by\n    â€¢ Can optimized prompt instruction improve the qual-\n                                                       Zhang et al. [32]. This summarization step also enabled the LLM\n       ity of RCA recommendation?: Manually designing high-\n                                                                       to identify and filter out noisy or non-informative root causes,\n       quality static prompts is challenging and often leads to sub-\n                                                              enhancing the overall quality and stability of the dataset.\n      optimal results. This requires significant domain knowledge\n      and is mostly trial-and-error. Additionally, performance may                                               3 PROMPT OPTIMIZATION FOR RCA\n      degrade as LLM models or their versions evolve. We aim\n                                            We now provide the details of our eARCO framework that pri-       to leverage the state-of-the-art prompt optimization tech-\n                                                                    marily consists of two components: Prompt instruction optimization      nique (â€˜PromptWizardâ€™) to automatically identify the best\n                                                          and In-Context example selection. We begin by explaining each of      prompt instructions and demonstrate their superiority over\n                                                                     these two components while emphasizing their respective roles in      manually crafted prompts.\n    â€¢ What role do in-context examples play in conjunc-      optimizing the quality of RCA recommendations. We then explain\n                                                                    the overall architecture of the eARCO system by integrating these      tion with optimized instructions? The effectiveness of\n                                                         two components together.     LLMs in reasoning tasks largely depends on the quality of in-\n      context examples included in the prompt. We will evaluate\n                                                         3.1  Prompt Optimization      the performance of static in-context examples identified by\n      â€˜PromptWizardâ€™ against dynamically retrieved semantically    LLM responses are highly sensitive to prompt instructions, making\n       similar examples.                                                                  it crucial to provide the right instructions for generating high-\n    â€¢ How do cost-effective finetuned SLMs perform with      quality RCA recommendations. To achieve this, we leverage a\n      optimized prompts? To reduce reliance on expensive LLMs       state-of-the-art prompt optimization technique called PromptWiz-\n      with large context lengths, we fine-tune SLMs using histor-      ard (PW) [2]. PW is a discrete optimization approach that refines\n        ical incident data. We will assess the performance of opti-     manually designed initial prompts using input-output pairs from\n      mized instructions generated by â€˜PromptWizardâ€™ on these      the training data. The optimization process involves four key steps:\n      fine-tuned SLMs.                                             Mutate, Score, Critique, and Synthesize. Each step employs LLMs to\n                                                                   execute pre-defined actions through specialized prompt templates.\n2.4  Data Preparation                                   These steps are run iteratively to progressively optimize both the\n                                                           prompt instructions and the selection of in-context examples.To address the research questions on the RCA task, we curated\n                                                    Prompt Instruction Tuning : In the Mutate stage, Prompta comprehensive incident dataset from Microsoft. The dataset in-\n                                                          Wizard (PW) takes the initial prompt instructions and task de-cludes over 180K historical incidents and their corresponding root\n                                                                             scription, applying predefined thinking styles to generate promptcauses. We performed data cleaning and pre-processing to ensure\n                                                                       variations in a single LLM call. In the Score stage, these mutatedconsistency, removed any incomplete or irrelevant records, and\n                                                              prompts are evaluated on a diverse batch of training samples, withsummarized the key incident properties.\n                                                              each prompt assigned a score based on performance. The best-\n2.4.1  Data Collection. Each service team at Microsoft logs their      performing prompt proceeds to the Critique stage, where it re-\nincident data in the internal incident management (IcM) portal. We      ceives targeted feedback regarding its strengths and weaknesses.\ncurated a dataset of approximately 180K high-severity historical      This feedback is used in the Synthesize stage to further refine and\nincidents from January 2022 to June 2024 from this portal. For     improve the prompt. The refined prompt is then fed back into the\neach incident, we collected key metadata, including the title, initial     Mutate stage, continuing iteratively until either a predefined per-\nsummary (written at the time of the incident), owning service name,     formance threshold is met or the maximum number of iterations is\nand the ground truth root cause identified and documented by the      reached. This feedback-driven loop effectively balances exploration\non-call engineers (OCEs). This data serves as the basis for both fine-     and exploitation, continuously enhancing the prompt.\ntuning models and evaluating root cause analysis performance.       In-Context Examples Tuning : The inclusion of in-context ex-\n                                                               amples (ICL) is optional for users. If selected, the optimized prompt\n2.4.2  Data Cleaning and Summarization. The raw summaries and\n                                                            from the previous phase is further tuned along with ICL examples.\nroot causes of incidents collected from the IcM portal often contain\n                                        PW begins by selecting a random set of 25 diverse examples, cover-\nsignificant amounts of extraneous information, such as HTML tags,\n                                                                    ing both positive and negative instances. These examples are opti-\nimages, stack traces, and code snippets. This noise can adversely\n                                                              mized in two phases: First, the Critique stage provides feedback on\nimpact model performance during both training and testing for\n                                                 how well the examples complement the prompt instructions, sug-\nseveral reasons: (a) the effectiveness of a fine-tuned model relies\n                                                                       gesting improvements. Then, in the Synthesize stage, this feedback\nheavily on high-quality training data; noise diminishes the modelâ€™s\n                                                                                       is used to refine the instructions and example selection. Second,\nability to learn effectively; (b) excessive noise can impair the rea-\nsoning capabilities of both base and finetuned models, potentially        1https://platform.openai.com/docs/models/gpt-3-5-turbo\n\n                                                                                          3\n\nOptimized prompt instruction for RCA.\n\n   You will be given a detailed description of an incident involving a system or service, including pertinent logs, error messages, and\n    other relevant information. Your objective is to methodically analyze the incident and identify the root cause. Follow these steps:\n     1. **Contextual Information**: Identify the service, relevant timestamps, environment, and key stakeholders involved. Mention\n    regional specifics where applicable.\n     2. **Categorization**: Categorize the type of incident (e.g., compute issues, storage conflicts, network anomalies).\n     3. **Identify Symptoms**: List all symptoms and error messages mentioned in the incident description.\n     4. **Detailed Historical Review**:\n     - Reflect on similar incidents and any historical data that might provide insights.\n     - Explicitly assess any past configuration changes or known historical script configurations related to the issue.\n     5. **Environmental Variables and Changes**:\n     - Identify and evaluate recent environmental changes, including recent configuration updates or external factors.\n     - Refer to specific timestamps leading up to and during the incident for environmental and systemic changes.\n     6. **Analyze Patterns and Logs**:\n     - Examine logs and error messages for recurring patterns.\n     - Cross-verify error logs against configuration settings and recent system changes.\n     - Look for specific script logs or monitor configurations and validate against system norms.\n     7. **Root Cause Analysis**:\n     - Synthesize findings from logs, historical data, and environmental variables.\n     - Clearly delineate between potential and confirmed root causes.\n     - Loop back to compare symptoms with broader historical and configuration data to ensure comprehensive scrutiny.\n     8. **Conclusion**: Clearly present the final root cause(s) wrapped between < ð´ð‘ð‘†_ð‘†ð‘‡ð´ð‘…ð‘‡> and < ð´ð‘ð‘†_ð¸ð‘ð·> tags.\n    Be thorough and evidence-based in your analysis, while eliminating any personal biases. Base your findings entirely on the provided\n     details to ensure accuracy.\n\n\n\n                          Figure 1: Optimized prompt instruction identified by PromptWizard.\n\n\nPW identifies the optimal mix of positive, negative, and synthesized      Persona guides the model to take on the role of an On-Call Engineer\nexamples through this iterative critique-synthesis loop, adapting     (OCE) with advanced analytical and reasoning skills, tasked with\nboth instructions and examples to the task at hand.                     identifying the root cause of cloud system incidents. To improve the\n  To further enhance performance, the prompt enters a Reason-     performance of PW optimization modules, we tune the following\ning stage, where chain-of-thought (CoT) reasoning is incorporated      hyper-parameters and its values are described in Section 5.1:\ninto the examples, followed by a Validation stage to verify the\ncorrectness of the examples and reasoning, preventing hallucina-         â€¢ mutate_refine_iterations : Number of iterations for conduct-\ntion and removing errors. PW also introduces a Task Intent and            ing rounds of mutation of task description and refinement\nan Expert Persona to the optimized prompt. The Task Intent helps             of instructions.\nthe LLM maintain relevance, while the Expert Persona ensures         â€¢ mutation_rounds : Number of rounds of mutation to be per-\nconsistency with domain expertise.                                    formed when generating different styles.\n  The final optimized prompt consists of four main components:         â€¢ refine_task_eg_iterations : Number of iterations for refining\na problem description, optimized instructions, static and diverse             task description and in-context examples.\nin-context examples, and task intent with an expert persona. Op-         â€¢ questions_batch_size : Number of questions to be asked to\ntionally, an answering format section can be added to specify how        LLM in a single batch during training.\nthe LLM should structure its response for downstream tasks. Impor-         â€¢ min_correct_count : Number of batches of questions to be\ntantly, this prompt optimization is a one-time process; the optimized             correctly answered, for a prompt to be considered for the\nprompt is then reused at the inference stage for all incident analy-            next steps.\nses.                                                            â€¢ few_shot_count : Number of in-context examples in the final\nAdapting to RCA Task : For our RCA task, we begin by select-           prompt.\ning 25 to 30 diverse historical incident examples from our training\n                                                            These parameters govern the extent of optimization and the final\ncorpus. These examples are chosen to ensure coverage across vari-\n                                                                         structure of the prompt. The optimized prompt instructions identi-\nous root-cause categories. We choose a manually designed prompt\n                                                                             fied by PromptWizard for the RCA task, using the GPT-4o model,\ninstruction from Zhang et. al. [32], as the initial input to PromptWiz-\n                                                                    are illustrated in Figure 1. A crucial aspect of PWâ€™s optimization\nard. PW then generates an optimized prompt instruction for RCA\n                                                                     process is the iterative refinement of both the instruction and the in-\ntask and identifies a set of diverse in-context examples. The Expert\n                                                                     context examples. During this stage, knowledge from the examples\n\n                                                                                          4\n\ninforms the refinement of the instructions, and vice versa. An exam-      section 3.2, eARCO then dynamically selects the top 10 semantically\nple of this in Figure 1 includes the line: Categorization: Categorize       similar incidents from the historical incident corpus.\nthe type of incident (e.g., compute issues, storage conflicts, network       The semantic similar examples are augmented with the static\nanomalies). Here, the examples list is generated based on informa-     optimized prompt instruction computed using PW (as outlined\ntion derived from the training data, highlighting the knowledge      in section 3). By integrating the optimized instructions, incident\ntransfer between the two prompt components. This back-and-forth      metadata, and semantically similar examples, eARCO enhances the\nrefinement leads to the optimal setting for both the instructions and       capabilities of LLMs to produce better quality automated RCA rec-\nin-context examples. Moreover, the generated prompt is structured     ommendations without additional training.\nin a clear, step-by-step format, guiding the model from the incident\ninformation through critical analysis and ultimately to the root\ncause. By leveraging the training data, PromptWizard produces    4 EARCO FOR FINETUNED SLMS\na task-specific prompt that is highly effective for RCA, ensuring     Another significant contribution of this paper is our exploration\nalignment with the problem domain.                                    of finetuned small language models, optimized eARCO framework.\n                                                                     In contrast to relying on retrieval-based methods to supply lan-\n3.2  In-context example selection                      guage models with historical incidents for domain-specific tasks\n                                                                             like root cause analysis, fine-tuning offers a more targeted and\nIn-context examples play a vital role during LLM inference, espe-\n                                                                                  efficient approach. Finetuning eliminates the need for maintaining\ncially for RCA task as the pre-trained LLMs do not have complete\n                                                               a large retrieval corpus, large contextual information in prompts\nknowledge of the incident management domain during training.\n                                                                      as ICL examples, inference time computations and significantly re-\nAlthough PW generates a set of static in-context examples which\n                                                                duces the risk of hallucinations. Previous efforts, of finetuning the\nshown superior performance on traditional NLP tasks, the context\n                                                        GPT-3 model, have demonstrated that adapting language models\nfor each incident typically varies significantly. Therefore, a set of\n                                                                            for industry-specific tasks can be highly effective, but as previously\nstatic examples may not be suitable for all incidents.\n                                                                 mentioned, finetuning and maintaining large language models is a\n  To dynamically understand the current incident context and\n                                                                          costly task. Our work addresses this by employing small language\nidentify semantically similar historical examples, we used a tradi-\n                                                            models (SLMs) of varying sizes, which not only optimize perfor-\ntional retrieval augmented generation (RAG) pipeline. For each of\n                                                      mance for specialized tasks but also presents a resource-efficient\nthe 180K incidents in training corpus, we first combine the title\n                                                                       solution for domain adaptation using finetuning.\nand cleaned summarized version of initial incident summary, and\n                                                              Finetuning Methodology. We fine-tuned the Phi-3.5-mini, Phi-\nencode them into vector representations using the Sentence Trans-\n                                                                         3-mini, and Phi-3-medium models for the root cause analysis (RCA)\nformer model [24]. All these training incidentsâ€™ metadata along\n                                                                     task using Hugging Faceâ€™s Supervised Fine-Tuning (SFT) Trainer,\nwith the corresponding embedding value are then stored in a vector\n                                                           an efficient framework for adapting pretrained models to domain-\ndatabase.\n                                                                             specific datasets. From the incident dataset which we curated, we\n  Once the embedding vectors are indexed, we leverage the FAISS\n                                                                       allocated 10K datapoints for validation, 2,891 for testing, and uti-\nlibrary [22] for the efficient similarity search and clustering of the\n                                                                              lized the remainder for training (160K+). The data was split tempo-\ndense vectors. The FAISS library utilizes a compressed representa-\n                                                                                        rally, with older incidents designated for training and more recent\ntion of the vectors, which eliminates the need to store the original\n                                                                      cases for testing. This temporal split was implemented to simulate\nvectors in memory. While this compression may result in a mar-\n                                                                       real-world scenarios, ensuring that the models could effectively gen-\nginal reduction in search precision, the key advantage lies in its\n                                                                             eralize historical knowledge to newer, unseen incidents, reflecting\nexceptional scalability. FAISS can efficiently handle billions of vec-\n                                                                     the applicability to evolving system behaviors. The finetuning pro-\ntors in main memory on a single server. For a given incident and its\n                                                                       cess was carried out over three epochs, utilizing a batch size of 64\ntext embedding vector during inference, FAISS then retrieves the\n                                                                 samples on a compute cluster featuring 8 x NVIDIA Tesla V100 (672\ntop-K similar incidents using the L2 (Euclidean) distance metrics.\n                                              GB RAM) and 8 x NVIDIA A100 (900 GB RAM) GPUs, depending\n                                                         on availability. To enhance model performance and generalization,\n3.3  Leveraging eARCO for RCA Generation        we employed the AdamW optimizer, incorporating weight decay\nIn this paper, we propose eARCO, a comprehensive framework      to mitigate the risk of over-fitting. Additionally, a linear learning\nfor efficient automated root cause analysis (RCA). eARCO com-      rate scheduler with a warm-up phase was implemented to promote\nbines the Prompt Instruction Optimization and In-context example       training stability and prevent abrupt changes in learning rates that\nselection techniques to significantly enhance the performance of      could hinder convergence. The training duration for the models is\nLLMs in generating accurate RCA recommendations. The detailed      given in table 1.\narchitecture of this framework is illustrated in fig. 2.                 Root Cause Generation. After finetuning the SLMs on the\n  There are two stages of eARCO, (1) Optimized prompt instruction       incident dataset, we performed inference and evaluated the models\ngeneration one-time and (2) Selection of ICL examples at inference      using a test set comprising of more recent incidents. This approach\ntime. Specifically, at inference time, given the current incident       closely simulates real-world scenarios where models must lever-\nmetadata (i.e., title and initial summary which are available at the      age historical knowledge to analyze and address newer, unseen\ntime of incident creation), we generate the encoded embedding       cases. By employing a temporal splitâ€”training on older incidents\nvector using the Sentence Transformer model which is used as     and testing on more recent onesâ€”we assess the modelâ€™s ability to\nan incident query vector. Using the retrieval pipeline outlined in      generalize across time and adapt to evolving system behaviors, a\n\n                                                                                          5\n\nInput                              Prompt Wizard                                  Final Prompt\n\n                                                                             Sequential                                      Problem Description                        Problem\n                                                                        Optimization\n                           Description                                                           Optimized\n                                                      Iterative                     Instruction    Prompt                             Optimized Prompt\n                                                                     Modified Prompt  Optimization             Self-generated\n                        Prompt           Refinement      Instruction\n                                                                                             Reasoning &\n                             Instruction           of Prompt                                                                         Optimized Few-Shot\n                                                                                                       Validation               with Reasoning                                                  Instructions                                      Synthesized\n                                                                              Synthetic                                                                                                Examples\n                                                                Example                           Training                              Diverse\n                        Examples                        Example      Generation                                    Task IntentPersona+ Expert\n                                                                        Selection\n\n\n\n\n\n                                                                                        Prompt Generation\n\n                                                 Incident Information                                                       Optimized Prompt Instructions\n                                                                                                       +\n                                                          (Title, Summary,                                                             Semantically Similar Incidents\n                                     Owning Tenant Name)               Incident                              +\n                                                                                   Retriever                                       Incident Metadata\n\n\n                                                               Query\n\n                                                                                           Model\n                                                                              (SLM/LLM)\n\n                                                                                             k-similar\n                                                                                                incidents\n                                                                              RCA\n                                                                                                                      Responses\n                                      Data              Incident                          Historical\n                                           Collection &      Summarization                           Incidents\n                                          Cleaning\n                                                                                 Retrieval Corpus\n                         RAG Pipeline\n                                                                        OCE Team\n\n\n                 Figure 2: Architecture of the eARCO Framework for Efficient Root Cause Analysis (RCA)\n\nTable 1: Training Duration: This table summarizes the total         In the next section, we describe the performance of fine-tuned\ntraining times for small language models utilized in the Root     SLMs on the incident data and demonstrate how prompt instruction\nCause Analysis (RCA) task, highlighting model size and com-      optimization enhances SLMsâ€™ ability to generate high-quality RCA\nputational resources employed.                                 recommendations. We detail the comparative results, showing the\n                                                               impact of optimized prompts on SLM accuracy and the quality of\nModel       Model Size (Parameters)  Compute Type  Training Time (Hours)      root cause identification, further validating the effectiveness of the\nPhi-3-mini                3.8B                8 x A100                   6.5              optimization process.\nPhi-3.5-mini               3.8B                8 x V100                 13.5\nPhi-3-medium           14B                8 x V100                30\n                                               5  EXPERIMENTAL SETUP\n                                                                      In this section, we explain the default configurations used to tune\n                                                                   the PromptWizard module, followed by explaining different ver-\n                                                                       sions of eARCO and baseline methods, and finally explain the eval-crucial requirement for dynamic, real-time environments such as\n                                                                    uation strategy and performance metrics.root cause analysis (RCA).\n  To ensure reproducibility and minimize randomness in the gen-\n                                                         5.1  PromptWizard Configurationserated responses, we set the temperature to approximately zero\nduring inference. A low temperature encourages the model to pro-     As discussed earlier, PromptWizard uses several configurable pa-\nduce deterministic and focused outputs by reducing variation in      rameters to balance exploration and exploitation efficiently, en-\ntoken selection, which is essential when generating structured re-      suring that prompt optimization remains robust for RCA task. To\nsponses such as root cause explanations. Additionally, we capped      derive an optimized prompt, we sample 25 random input-output\nthe maximum number of new tokens at 200, aligning the response       pairs from the IcM dataset as training data. The following configu-\nlength with both the readability requirements of OCEs and the      ration was used for the optimization process:\naverage token length of the ground truth root cause analyses.            â€¢ mutate_refine_iterations : 3\n\n                                                                                          6\n\nâ€¢ mutation_rounds : 3                                           allows us to evaluate the isolated impact of optimized instructions\n    â€¢ refine_task_eg_iterations : 3                              on the performance of the fine-tuned models.\n    â€¢ questions_batch_size : 5                                   Base SLM with PW Inst. & Ex (BaseSLM PW): In this base-\n    â€¢ min_correct_count : 3                                                   line, we investigate the performance of the base (non-finetuned)\n    â€¢ few_shot_count : 10                                       SLMs, when equipped with both the PW Instructions and PW static\n                                                                  Examples, excluding any finetuning.\n                                                          Base SLM with PW Inst (BaseSLM PW noEx.).: This con-\n5.2  Methods and Baselines                                  figuration uses the base SLMs with only the PW Instructions and\nTo thoroughly assess the impact of prompt optimization on both       Incident Details, omitting the PW static Examples. This method seeks\nLLMs and finetuned SLMs, we experiment the following 8 strategies:      to compare the impact of finetuning and PW static Examples.\n  Manual Prompt with Semantically Similar (SS) ICL exam-\nples [32] - (Manual-SS): The prompt, which is also proposed in     5.3  Evaluation Metrics\n[32], is designed with three key components: Default manual In-     Even with ground truth root cause information, evaluating the rec-\nstructions, In-context Examples, and Incident Details. The Default     ommendations generated by language models is a complex task.\nmanual Instructions, includes the prompt hand-designed by a do-     While expert evaluations from the OCEs would provide the most ac-\nmain expert which has not undergone any optimization. This prompt      curate assessments, conducting such large-scale human evaluation\nbriefly guides the model to assume the role of an OCE tasked with        is not a practical option due to time constraints of OCEs. Moreover,\nperforming root-cause analysis (RCA) for Cloud incidents. For the       traditional automatic metrics, whether lexical or semantic, often fail\nIn-context Examples, we retrieve the top 10 similar incidents using       to capture the nuanced, domain-specific similarities required for ef-\nthe RAG pipeline, as detailed in section 3.2, and provide their cor-       ficient evaluation as shown in previous studies [16, 32]. To address\nresponding titles, summaries, owning service names, and ground      these challenges, we implement a dual evaluation strategy to assess\ntruth root causes. Finally, we incorporate the details of the current      the similarity and accuracy between the automatically generated\nincident, including its title, summary, and owning service name.     recommendations and ground truth root causes: (1) an automated\nThis method is applied to both GPT-4, GPT4o and base SLMs. We      assessment using GPT-4 as the judge across the entire test dataset\ndeliberately exclude the finetuned models from this experiment, as       consisting of 2900 incidents; and (2) a small-scale human evaluation\nincluding the In-context examples defeats the purpose of finetuning     on a representative subset of incidents.\nthe models in the first place.\n                                                                                   5.3.1  Automated Evaluation Using GPT-4. In this evaluation strat-  Optimized Prompt with Static examples- (PW-Default): In\n                                                                          egy, the GPT-4 model is prompted with a structured task descriptionthis configuration, we use PW Instructions and PW static Examples\n                                                                        that specifies itâ€™s role as a scorer, tasked with comparing a gener-along with the Incident Details in the prompt. This is an out-of-the\n                                                                     ated string with a reference string based on a defined set of criteria.box usage of PromptWizard without any modifications and the\n                                                       The model assigns a score between 1 to 5, where a higher scoreinstructions and examples remain the same for all test incidents.\n                                                                          indicates a closer match in terms of content coverage, nuance, andThis setup shows how well ICL examples selected by PW generalizes\n                                                                       accuracy. The model also provides a justification for each score. Inacross all incidents.\n                                                                       addition to the reference and generated strings, we also provide the  Optimized Prompt with Semantic Similar examples (PW-\n                                                           GPT-4 model with the incident summary as contextual information,SS): In this configuration, we incorporate only the PW Instructions,\n                                                                 allowing the model to evaluate the generated responses with aexcluding the PW static Examples. The ICL examples are selected\n                                                                          clearer understanding of the incident.based on the semantic similarity of the incident at hand in run-time\nas described previously. This serves as an ablation study to isolate        5.3.2  Human Evaluation. As incident owners are domain experts\nand assess the impact of the instructions themselves, allowing us     and have specific knowledge about the root cause context, we chose\nto later compare and understand the contribution of the PW static      a subset of recent incidents, and interviewed the respective inci-\nExamples to the overall performance.                               dent owners (on-call engineers). Along with the OCEs, we also\n  Finetuned SLM (FtSLM): This method utilizes the finetuned      asked other researchers (who are not incident management do-\nSLMs with a standard, non-optimized prompt that contains the     main experts) to score two sets of incidents. Each evaluator was\nIncident Title, Incident Summary, and Owning Service Name in the      asked to score the generated responses based on two key criteria:\nsame format used during the fine-tuning process. This serves as       (1) accuracy in comparison to the ground truth root cause and (2)\na benchmark for evaluating the performance of finetuned models       readability in terms verbosity, grammatical correctness and struc-\nwithout any further prompt optimization.                               ture of the recommendations. With detailed guidelines provided to\n  Finetuned SLM with PW Inst. & Ex (FtSLM PW): In this      ensure consistent scoring, each model-generated recommendation\nconfiguration, we augment the finetuned SLMs with both the PW     was rated on a scale of 1 to 5 for both criteria.\nInstructions and PW static Examples, along with the Incident Details\nduring inference. This allows us to assess whether the optimized    6  EXPERIMENTAL RESULTS\ninstructions and static examples improve the performance of the\n                                                         6.1  Evaluating Performance of eARCO withfinetuned models.\n  Finetuned SLM with PW Inst.(FtSLM PW noEx.): As another         Large Models\nablation experiment, we provide only the PW Instructions along with      In this section, we compare the performance of manually crafted\nthe Incident Details, without including the PW static Examples. This     prompts with those optimized using PromptWizard for the RCA\n\n                                                                                          7\n\ntask. Table 2 presents the results across different configurations,            Table 4: Results for base and fine-tuned SLMs\nwith the LLM name indicating the model used for optimization and\nanswer generation. The table reports the performance across two      Experiment           Filtered Test Dataset  Complete Test Dataset\ndatasets: the â€˜Complete Test Datasetâ€™ and the â€˜Filtered Test Datasetâ€™.                          Phi-3.5-mini-128k-instruct\nThe latter is a subset of the former, containing only those incidents\n                                                               FtSLM                         2.09 Â± 0.90                  1.79 Â± 0.87\nwhere an incident summary is available in the Incident Details.\n                                                               FtSLM PW noEx.               2.13 Â± 0.87                  1.90 Â± 0.87\n  The optimized prompt generated by PromptWizard, denoted as      FtSLM PW                   2.37 Â± 0.79                2.01 Â± 0.84\nPW-Default, achieves average scores of 2.07 for GPT-4 and 2.13 for      BaseSLM PW                  2.26 Â± 0.71                  1.93 Â± 0.77\nGPT-4o, outperforming the manually designed Manual-SS prompts,     BaseSLM PW noEx.            2.14 Â± 0.61                  1.82 Â± 0.69\nwhich scored 2.03 and 2.07 for the same models, respectively. De-      Manual-SS - BaseSLM          1.79 Â± 0.58                  1.55 Â± 0.60\nspite using the same 10 in-context examples for all test instances,                       Phi-3-medium-128k-instruct\nPromptWizardâ€™s optimization outperforms Manual-SS, which dy-                                                               FtSLM                         2.11 Â± 0.99                  1.82 Â± 0.93\nnamically selects the 10 most semantically similar examples for      FtSLM PW noEx.               2.17 Â± 0.84                  1.87 Â± 0.86\neach test instance. This result highlights the limitations of manually      FtSLM PW                   2.21 Â± 0.86                1.93 Â± 0.90\ncrafted prompts and the advantage of PromptWizardâ€™s automated      BaseSLM PW                  2.00 Â± 0.44                  1.68 Â± 0.57\noptimization. Moreover, replacing the static in-context examples in      BaseSLM PW noEx.            2.01 Â± 0.58                  1.69 Â± 0.65\nPromptWizard with semantically similar examples (PW-SS) further      Manual-SS - BaseSLM                -                               -\nimproves the performance to 2.33 and 2.51 for GPT-4 and GPT-                          Phi-3-mini-128k-instruct\n4o, respectively, providing a massive 21% gain in accuracy over      FtSLM                         2.08 Â± 0.99                  1.79 Â± 0.92\nmanually designed prompts.                                       FtSLM PW noEx.               1.98 Â± 0.74                  1.77 Â± 0.74\n                                                               FtSLM PW                   2.12 Â± 0.84                1.83 Â± 0.84\n                                                               BaseSLM PW                  1.66 Â± 0.59                  1.44 Â± 0.67\n    Table 2: GPT-4 evaluation scores for PromptWizard          BaseSLM PW noEx.            2.10 Â± 0.63                  1.74 Â± 0.70\n                                                                     Manual-SS - BaseSLM          1.83 Â± 0.66                  1.58 Â± 0.66\n\nExperiment  Complete Test Dataset   Filtered Test Dataset\n                      GPT-4                            6.2  Evaluating Performance of eARCO with\nManual-SS             2.03 Â± 0.93                  2.35 Â± 0.94             Small Models\nPW-Default            2.07 Â± 0.91                  2.37 Â± 0.92          As discussed earlier, SLMs demonstrate significant potential when\nPW-SS               2.33 Â± 0.98                2.68 Â± 0.98            finetuned for domain specific tasks such as RCA. By leveraging\n                                                              such models, organizations can reduce the overhead costs associ-\n                     GPT-4o\n                                                                    ated with querying expensive LLMs or maintaining large retrieval\nManual-SS             2.07 Â± 1.01                  2.33 Â± 1.05            corpora for RAG pipelines. In this section, we present the evaluation\nPW-Default            2.13 Â± 0.97                  2.41 Â± 0.95              results of the responses generated by these SLMs. Furthermore, we\nPW-SS               2.51 Â± 1.01                2.91 Â± 1.01           demonstrate how the prompt-optimization framework improves\n                                                                   the performance of these models, enhancing both accuracy and\n                                                                                efficiency.\n  To assess the value of PromptWizardâ€™s multi-step optimization       The results presented in Table 4 provide a detailed comparison\nprocess, we performed an ablation study where prompts were evalu-      of various finetuning and prompting strategies for SLMs, as out-\nated at intermediate stages of optimization (as shown in Table 3). Re-      lined in section 5.2. Specifically, the three modelsâ€”Phi-3-medium,\nsults show a consistent improvement in performance as the prompt      Phi-3-mini, and Phi-3.5-miniâ€”were evaluated under different con-\nundergoes more stages of refinement, underscoring the importance      figurations to examine the impact of finetuning, PW Instructions,\nof the multi-step optimization employed by PromptWizard. This     and PW static Examples on model performance.\nconfirms that each stageâ€”mutation, scoring, critiquing, and synthe-       The table presents performance across two datasets: the Com-\nsizingâ€”contributes significantly to achieving optimal performance.       plete Test Dataset and the Filtered Test Dataset, where the latter\n                                                               only includes incidents with available summaries in the Incident\n                                                                              Details. Incident summaries provide crucial context for accurate\nTable 3: GPT-4 evaluation scores for PW prompt from GPT-    RCA generation, and their absence can hinder model performance.\n4o at various steps in the optimization process              When evaluating the Filtered Test Dataset, we observe a consistent\n                                                             improvement in accuracy and relevance. This highlights the critical\nOptimization Stage            Complete Test Dataset   Filtered Test Dataset       role of contextual information in enhancing RCA task performance\nBase Prompt (Manual-SS)                    2.07 Â± 1.01                  2.33 Â± 1.05            for SLMs.\nAfter Prompt Instruction Tuning             2.10 Â± 0.91                  2.22 Â± 0.95             Across all three models (Phi-3.5-mini, Phi-3-medium, Phi-3-mini),\nAfter In-Context Examples Tuning           2.22 Â± 1.05                  2.30 Â± 0.91           the finetuned models with PW Instructions and PW static Examples\nPW Final Prompt (PW-SS)                 2.51 Â± 1.01                2.91 Â± 1.01\n                                                                         consistently achieve the highest scores on both the filtered and com-\n                                                                          plete test datasets. For instance, the Phi-3.5-mini model shows the\n                                                                      highest performance amongst all the models, with an average score\n\n                                                                                          8\n\nof 2.37 on the filtered test dataset and 2.01 (FtSLM PW) on the com-     GPT-4o under the Manual-SS settings, GPT-4o using the PW-SS\nplete test dataset, indicating that optimized prompts significantly       configuration, and the Phi-3.5-mini model with the FtSLM PW setup.\nenhance the modelâ€™s ability to predict accurate root causes.            Table 6 illustrate the accuracy and readability scores assigned by\n  However, when comparing the performance of finetuned SLMs      the OCEs.\nwithout the PW static Examples (FtSLM PW noEx.), there is a no-       The GPT-4o model with optimized PromptWizard instructions\nticeable decrease in scores. For instance, in the Phi-3.5-mini model     and 10 semantically similar examples (PW-SS) achieved the highest\nthe scores drop to 2.13 on the filtered dataset, and to 1.90 on the      average accuracy score of 2.91, reflecting a 14.12% and 7.45% im-\ncomplete dataset. This demonstrates the importance of providing     provement over GPT-4 and GPT-4o using Manual-SS, respectively.\nthe static examples (PW static Examples) to the finetuned SLMs to      For readability, the GPT-4o model in both Manual-SS and PW-SS\nimpart crucial reasoning and domain-specific knowledge, without      configurations scored highly, with a rating of 4.21, showing a 3.19%\nincurring additional dynamic example retrieval cost.                improvement over GPT-4 under Manual-SS. Lastly, despite being\n  The base SLM models with PromptWizard instructions also per-      scalable and cost-effective solution, finetuned SLMs with optimized\nform reasonably well, though they lag behind the finetuned models.      instructions (FtSLM-PW) achieves an average accuracy of 2.23 and\nFor instance, the Phi-3.5-mini BaseSLM PW achieves 2.26 on the      even provided better recommendations than LLMs for a small num-\nfiltered dataset, showing improvement over the base model un-      ber of incidents, which suggests that FtSLM-PW can be an attractive\nder the Manual-SS setting (which scored 1.79). Nevertheless, the       alternative cost-effective option for RCA generation task.\nperformance gap between the base and finetuned SLM is evident,         In addition to evaluations from OCEs, we gathered scores from\nespecially when prompt optimization with examples is applied.        10 researchers who are not incident management domain experts,\n   Lastly, applying the Manual-SS configurations to the base SLMs      but have access to the ground truth root causes and model recom-\ndemonstrate the lowest performance across all settings. These mod-      mendations. These human evaluators are divided into two groups.\nels, without finetuning or prompt engineering, score the lowest     Each group evaluated 25 incidents, resulting in a total of 50 inci-\non both datasets, reflecting the baseline performance before any      dents. Table 7 presents the aggregated accuracy and readability\noptimizations are applied. In particular, the Phi-3-medium model       scores. Consistent with the OCEsâ€™ evaluation, PW-SS achieves the\nunder this configuration produces in null responses, underscor-      highest accuracy score of 3.50 and the highest readability score of\ning the limited effectiveness of the base models without further       4.30.\nenhancements.\n                                                            Table 6: Accuracy and Readability scores assigned by OCEs\n6.3  Ablation Results\nTo analyse the utility of the semantically similar in-context exam-              Model     Accuracy  Readability\nples, we perform an ablation by varying the number of examples                                                                    GPT-4        2.55 Â±1.26      4.08 Â±0.90\nin the prompt with PromptWizard instructions. Table 5 shows per-\nformance of the prompt with number of example ranging from 0 to               GPT-4o       2.74 Â±1.42     4.21 Â±0.90\n10. We observe that increasing the number of in-context examples              PW-SS       2.91 Â±1.36     4.21 Â±0.95\nleads to better performance on the evaluation set going from 1.97\n                                                                 FtSLM PW    2.23 Â±1.30      3.68 Â±1.23for the zero-shot setting to 2.51 for the 10-shot setting on the com-\nplete dataset and from 2.13 to 2.91 on the filtered dataset. Hence\nwe observe around 27% improvement on the complete evaluation\nset and 37% improvement on the filtered evaluation set dataset.        Table 7: Accuracy and Readability scores assigned by domain\n                                                              experts\nTable 5: GPT-4 evaluation scores for ablation on number of\nsemantically similar In-Context Examples with PW Instruc-                                                             Model     Accuracy  Readability\ntions\n                                                                    GPT-4        3.15 Â±1.25      3.93 Â±0.79\n\nNumber of Examples  Complete Test Dataset   Filtered Test Dataset               GPT-4o       3.38 Â±1.20      4.06 Â±0.82\n\n                         GPT-4o                                    PW-SS       3.50 Â±1.20     4.30 Â±0.64\n0                                 1.97 Â± 0.91                  2.13 Â± 0.88                    FtSLM PW    2.89 Â±1.13      3.86 Â±0.65\n3                                 2.07 Â± 0.98                  2.25 Â± 0.90\n5                                 2.24 Â± 1.02                  2.41 Â± 0.91\n7                                 2.40 Â± 0.97                  2.72 Â± 0.94\n10                             2.51 Â± 1.01                2.91 Â± 1.01        7  DISCUSSION\n                                                        Promise of prompt optimization. Prompt instruction optimiza-\n                                                                       tion has gained popularity recently and proven to be effective in\n6.4 Human Evaluation Results                    many traditional NLP tasks. In this work, we first analyse and\nTo understand the concrete impact of eARCO, we reached out to 47      demonstrate that optimized instructions can significantly improve\nOCEs involved in recent incident resolutions to assess responses      the performance on sensitive proprietary domain such as incident\ngenerated by various models. These models included GPT-4 and     management as well. Furthermore, we demonstrate that finetuned\n\n                                                                                          9\n\ndomain adapted SLMs can be an attractive alternative for AIOPs     8.2  Promise of LLMs in Incident Management\ntasks. Moreover, we demonstrate that the optimized instructions      In large-scale cloud services, effective and efficient handling of\nand static diverse synthetic in-context examples can significantly      incidents is essential. Given the superior performance of LLMs in\nboost the performance of domain adapted fintuned SLMs. These in-      several domain-specific software engineering tasks ranging includ-\nsights will be valuable for designing more efficient and cost-effective      ing code generation [9, 31], program synthesis([18]), code review\nAIOPs solutions in future.                                                  [25, 26], code repair [23, 30] and code-fix [21], LLMs have been\n  Deployment status and scale. We conducted our experiments      adopted increasingly for solving problems in incident management\nand evaluation by leveraging data from IcM of Microsoft. We have      domain. Several recent works propose to address the incident di-\ndeployed a large-scale RCA recommendation system as a service      agnosis and RCA [3, 11, 32] tasks using LLMs. Ahmed et. al. [3]\nby leveraging ICL pipeline with LLMs that is serving more than      propose to finetune a GPT model for learning domain specific\nhundred internal service teams for more than six months. For our ex-     knowledge about incident management and recommend poten-\nperiments, the recommendations for Manual-SS with GPT-4 method        tial root causes at the time of incident creation. Zhang et. al. [32]\nare directly obtained from production environment. We have also re-      subsequently propose an efficient RAG based in-context learning\ncently deployed the finetuned SLM solution to augment the recom-     method for RCA generation task. In contrast to the existing works\nmendations generated by the ICL pipeline. As eARCO demonstrate      that leverages manually designed static instructions with LLMs,\nsuperior performance on real-world production incident dataset,    we propose to identify an optimized prompt instructions with syn-\nwe plan to deploy eARCO for both the ICL and finetuned SLM in       thetic in-context examples and further demonstrate the potential\nnear future.                                                             of cost-effective finetuned SLMs for RCA generation tasks.\n  Threats to validity. Although the optimized prompt instruction\ndemonstrate superior performance with different settings of LLMs\nand finetuned SLMs, the performance of eARCO is highly depen-     8.3  Prompt Optimization\ndant on the underlying language model. As we have evaluated the      Existing LLM-based solutions for incident management heavily\nperformance of our models on incident data from Microsoft only,      rely on prompt engineering techniques. The prompts are carefully\nthe performance may vary if evaluated on different dataset from      chosen, often with multiple trials and errors, based on their ef-\nother organizations. While the GPT-4 based accuracy evaluation      fectiveness on the task being solved. This requires manual effort\nhas been widely adopted, these evaluation results can be slightly     and generated prompts could be sub-optimal due to lack of sys-\nnoisy due to hallucination problem of LLMs. Moreover, the human      tematic exploration of prompts. Furthermore, the performance of\nevaluation is conducted on a small set of incidents as it is chal-       static prompt varies significantly as the underlying LLM evolves.\nlenging to and time consuming to scale these feedback collection     Prompt optimization techniques address these limitations by opti-\nprocess. In some cases, the model may potentially generate hal-     mizing either soft-prompt [8, 27] or candidate prompts [12, 17, 34].\nlucinated responses and additional noisy mitigation suggestions,     PromptWizard [2] is a recent work that outlines and addresses\nwhich can be problematic and misguide incident owners. Lastly,      the limitations of existing approaches - (i) high computation cost,\nwe have only finetuned Phi-series of SLMs, but the performance         (ii) lack of human-interpretable prompts, (iii) sub-optimal prompt\nmight improve further with other open-source SLMs. Moreover, we      outputs for complex tasks, (iv) lack of feedback-based exploration.\nplan to use domain adaptation techniques using RLHF techniques      Given its efficiency and superior performance on several traditional\nto improve the performance of SLMs in future.                  NLP tasks, we leverage PromptWizard in this work for identifying\n                                                                 optimized prompt instruction and synthetic in-context examples.\n\n8  RELATED WORK\nIn this section, we summarize the existing work on incident man-    9  CONCLUSION\nagement, adoption of LLMs for RCA and AIOPs tasks, and existing      In this work, we propose eARCO, an efficient and scalable opti-\nprompt optimization techniques.                                  mized framework for automatically generating accurate root cause\n                                                           recommendations for incidents in large-scale cloud services. By\n                                                                      leveraging state-of-the-art prompt optimization technique, we auto-\n8.1  Incident Management                                    matically identified optimized prompt instruction that is augmented\nGiven its practical importance, AI for Operations (AIOPs) tech-     with dynamically retrieved semantically similar in-context exam-\nniques have become popular for automatically resolving issues       ples during real-time inference. Moreover, we develop scalable and\narising from different stages of incident lifecycle management. Em-       cost-effective finetuned SLMs and demonstrate that, with optimized\npirical studies have been adopted broadly to understand the gaps     prompt instructions, these can be an attractive alternative solution\nand limitations in existing large-scale cloud services, either delv-      for RCA generation task. Our extensive experimental evaluation\ning into the types incident root causes [14, 33] or system-level     by domain experts and GPT-4 as judge demonstrate that eARCO\nissues [15, 28]. Several AIOPs techniques have been proposed to ad-     improves the accuracy of RCA recommendations significantly for\ndress the challenges in detection [13, 29], triaging [6], diagnosis [5],     both LLMs and finetuned SLMs on real-world incidents from Mi-\nand mitigation [19] to either reduce human efforts or accelerating       crosoft. We believe these insights will motivate the need of prompt\nthe incident resolution process. Our work enhances the perfor-      optimization and adoption of cost-effective finetuned SLMs in solv-\nmance of automated root cause generation task where accuracy      ing various challenges arising from different stages of incident\nand efficiency are primary goals.                               management lifecycle.\n\n                                                                                          10\n\nREFERENCES                                                                        arXiv:2309.08532 (2023).\n [1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed        [18] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh\n     Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-             Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large lan-\n      rat Behl, et al. 2024. Phi-3 technical report: A highly capable language model           guage models meet program synthesis. In Proceedings of the 44th International\n      locally on your phone. arXiv preprint arXiv:2404.14219 (2024).                             Conference on Software Engineering. 1219â€“1231.\n [2] Eshaan Agarwal, Vivek Dani, Tanuja Ganu, and Akshay Nambi. 2024. PromptWiz-        [19] Jiajun Jiang, Weihai Lu, Junjie Chen, Qingwei Lin, Pu Zhao, Yu Kang, Hongyu\n      ard: Task-Aware Agent-driven Prompt Optimization Framework. arXiv preprint            Zhang, Yingfei Xiong, Feng Gao, Zhangwei Xu, et al. 2020. How to mitigate\n     arXiv:2405.18369 (2024).                                                                 the incident? an effective troubleshooting guide recommendation technique for\n [3] Toufique Ahmed, Supriyo Ghosh, Chetan Bansal, Thomas Zimmermann, Xuchao             online service systems. In Proceedings of the 28th ACM Joint Meeting on European\n                                                                                             Software Engineering Conference and Symposium on the Foundations of Software\n     Zhang, and Saravan Rajmohan. 2023. Recommending Root-Cause and Mitigation\n                                                                                                 Engineering. 1410â€“1420.\n     Steps for Cloud Incidents using Large Language Models. In 45th International\n                                                                                            [20] Yuxuan Jiang, Chaoyun Zhang, Shilin He, Zhihao Yang, Minghua Ma, Si Qin,\n     Conference on Software Engineering.\n                                                                          Yu Kang, Yingnong Dang, Saravan Rajmohan, Qingwei Lin, et al. 2023. Xpert:\n [4] Amar Prakash Azad, Supriyo Ghosh, Ajay Gupta, Harshit Kumar, Prateeti Mo-\n                                                                           Empowering Incident Management with Query Recommendations via Large\n     hapatra, Lena Eckstein, Leonard Posner, and Robert Kern. 2022. Picking Pearl\n                                                                                 Language Models. arXiv preprint arXiv:2312.11988 (2023).\n    From Seabed: Extracting Artefacts from Noisy Issue Triaging Collaborative Con-\n                                                                                            [21] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir\n     versations for Hybrid Cloud Services. In Proceedings of the AAAI Conference on\n                                                                                                       Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve\n      Artificial Intelligence, Vol. 36. 12440â€“12446.\n                                                                                              real-world github issues? arXiv preprint arXiv:2310.06770 (2023).\n [5] Chetan Bansal, Sundararajan Renganathan, Ashima Asudani, Olivier Midy, and\n                                                                                            [22] JeffJohnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019. Billion-scale similarity\n    Mathru Janakiraman. 2020. DeCaf: Diagnosing and Triaging Performance Issues\n                                                                                            search with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535â€“547.\n     in Large-Scale Cloud Services. In 2020 IEEE/ACM 42nd International Conference\n                                                                                            [23] Harshit Joshi, JosÃ© Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Verbruggen,\n     on Software Engineering: Software Engineering in Practice (ICSE-SEIP).\n                                                                                  and Ivan RadiÄek. 2023. Repair is nearly generation: Multilingual program repair\n [6]  J. Chen, X. He, Q. Lin, Y. Xu, H. Zhang, D. Hao, F. Gao, Z. Xu, Y. Dang, and D.\n                                                                                       with llms. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37.\n     Zhang. 2019. An Empirical Investigation of Incident Triage for Online Service\n                                                                                         5131â€“5140.\n     Systems. In 2019 IEEE/ACM 41st International Conference on Software Engineering:\n                                                                                            [24] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\n     Software Engineering in Practice (ICSE-SEIP). 111â€“120.\n                                                                                       Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\n [7] Junjie Chen, Xiaoting He, Qingwei Lin, Hongyu Zhang, Dan Hao, Feng Gao,\n                                                                          Domain Question Answering. In Proceedings of the 2020 Conference on Empirical\n    Zhangwei Xu, Yingnong Dang, and Dongmei Zhang. 2019. Continuous incident\n                                                                                     Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn,\n      triage for large-scale online service systems. In 2019 34th IEEE/ACM International\n                                                                                        Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,\n     Conference on Automated Software Engineering (ASE). IEEE, 364â€“375.\n                                                                                         6769â€“6781. https://doi.org/10.18653/v1/2020.emnlp-main.550\n [8] Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. 2023.\n                                                                                            [25] Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan Hua, Geng Liang,\n     Instructzero: Efficient instruction optimization for black-box large language\n                                                                                and Chun Zuo. 2022. AUGER: automatically generating review comments with\n     models. arXiv preprint arXiv:2306.03082 (2023).\n                                                                                               pre-training models. In Proceedings of the 30th ACM Joint European Software\n [9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\n                                                                                               Engineering Conference and Symposium on the Foundations of Software Engineering.\n      Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\n                                                                                         1009â€“1021.\n      et al. 2021. Evaluating large language models trained on code. arXiv preprint\n                                                                                            [26] Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep\n     arXiv:2107.03374 (2021).\n                                                                                    Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, et al. 2022. Automating\n[10] Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao,\n                                                                                   code review activities by large-scale pre-training. In Proceedings of the 30th\n    Xuedong Gao, Hao Fan, Ming Wen, et al. 2024. Automatic root cause analysis\n                                                         ACM Joint European Software Engineering Conference and Symposium on the\n     via large language models for cloud incidents. In Proceedings of the Nineteenth\n                                                                                            Foundations of Software Engineering. 1035â€“1047.\n     European Conference on Computer Systems. 674â€“688.\n                                                                                            [27] Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong\n[11] Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao,\n                                                                                   Ng, Patrick Jaillet, and Bryan Kian Hsiang Low. [n. d.]. Use Your INSTINCT:\n    Xuedong Gao, Hao Fan, Ming Wen, et al. 2024. Automatic root cause analysis\n                                                                                      INSTruction optimization for LLMs usIng Neural bandits Coupled with Trans-\n     via large language models for cloud incidents. In Proceedings of the Nineteenth\n                                                                                              formers. In Forty-first International Conference on Machine Learning.\n     European Conference on Computer Systems. 674â€“688.\n                                                                                            [28] Haopeng Liu, Shan Lu, Madan Musuvathi, and Suman Nath. 2019. What bugs\n[12] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and\n                                                                                         cause production cloud incidents?. In Proceedings of the Workshop on Hot Topics\n    Tim RocktÃ¤schel. 2023. Promptbreeder: Self-referential self-improvement via\n                                                                                                     in Operating Systems. 155â€“162.\n    prompt evolution. arXiv preprint arXiv:2309.16797 (2023).\n                                                                                            [29] Pooja Srinivas, Fiza Husain, Anjaly Parayil, Ayush Choure, Chetan Bansal, and\n[13] Vaibhav Ganatra, Anjaly Parayil, Supriyo Ghosh, Yu Kang, Minghua Ma, Chetan\n                                                                                       Saravan Rajmohan. 2024. Intelligent Monitoring Framework for Cloud Services:\n     Bansal, Suman Nath, and Jonathan Mace. 2023. Detection Is Better Than Cure:\n                                                         A Data-Driven Approach. In Proceedings of the 46th IEEE/ACM International\n   A Cloud Incidents Perspective. In Proceedings of the 31st ACM Joint European\n                                                                                            Conference on Software Engineering.\n     Software Engineering Conference and Symposium on the Foundations of Software\n                                                                                            [30] Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan\n     Engineering. 1891â€“1902.\n                                                                                             Natarajan, Aditya Kanade, Suresh Parthasarathy, and Sriram Rajamani. 2024.\n[14] Yu Gao, Wensheng Dou, Feng Qin, Chushu Gao, Dong Wang, Jun Wei, Ruirui\n                                                                        CORE: Resolving Code Quality Issues using LLMs. Proceedings of the ACM on\n     Huang, Li Zhou, and Yongming Wu. 2018. An empirical study on crash recovery\n                                                                                             Software Engineering 1, FSE (2024), 789â€“811.\n     bugs in large-scale distributed systems. In Proceedings of the 2018 26th ACM\n                                                                                            [31] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A\n      joint meeting on european software engineering conference and symposium on the\n                                                                                              systematic evaluation of large language models of code. In Proceedings of the 6th\n     foundations of software engineering. 539â€“550.\n                                                           ACM SIGPLAN International Symposium on Machine Programming. 1â€“10.\n[15] Supriyo Ghosh, Manish Shetty, Chetan Bansal, and Suman Nath. 2022. How to\n                                                                                            [32] Xuchao Zhang, Supriyo Ghosh, Chetan Bansal, Rujia Wang, Minghua Ma, Yu\n      fight production incidents? an empirical study on a large-scale cloud service. In\n                                                                                      Kang, and Saravan Rajmohan. 2024. Automated root causing of cloud incidents\n     Proceedings of the 13th Symposium on Cloud Computing. 126â€“141.\n                                                                                            using in-context learning with GPT-4. In Companion Proceedings of the 32nd ACM\n[16] Drishti Goel, Fiza Husain, Aditya Singh, Supriyo Ghosh, Anjaly Parayil, Chetan\n                                                                                                     International Conference on the Foundations of Software Engineering. 266â€“277.\n     Bansal, Xuchao Zhang, and Saravan Rajmohan. 2024. X-lifecycle learning for\n                                                                                            [33] Yongle Zhang, Junwen Yang, Zhuqi Jin, Utsav Sethi, Kirk Rodrigues, Shan Lu,\n     cloud incident management using llms. In Companion Proceedings of the 32nd\n                                                                              and Ding Yuan. 2021. Understanding and detecting software upgrade failures\n   ACM International Conference on the Foundations of Software Engineering. 417â€“\n                                                                                                 in distributed systems. In Proceedings of the ACM SIGOPS 28th Symposium on\n     428.\n                                                                                           Operating Systems Principles. 116â€“131.\n[17] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing\n                                                                                            [34] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,\n      Liu, Jiang Bian, and Yujiu Yang. 2023. Connecting large language models with\n                                                                                           Harris Chan, and Jimmy Ba. 2022.  Large language models are human-level\n     evolutionary algorithms yields powerful prompt optimizers.  arXiv preprint\n                                                                                prompt engineers. arXiv preprint arXiv:2211.01910 (2022).\n\n\n\n\n\n                                                                                          11",
"headers": [
"arXiv:2504.11505v1  [cs.SE]  15 Apr 2025",
"eARCO: Efficient Automated Root Cause Analysis with Prompt",
"Optimization",
"Drishti Goel, Raghav Magazine, Supriyo Ghosh, Akshay Nambi, Prathamesh Deshpande, Xuchao",
"Zhang, Chetan Bansal, Saravan Rajmohan",
"ABSTRACT",
"KEYWORDS",
"1",
"INTRODUCTION",
"2.1",
"Incident Root Cause Analysis",
"2.2",
"Promise of LLMs in Incident Management",
"2",
"BACKGROUND",
"2.3",
"Research Questions",
"3",
"PROMPT OPTIMIZATION FOR RCA",
"3.1",
"Prompt Optimization",
"2.4",
"Data Preparation",
"4",
"EARCO FOR FINETUNED SLMS",
"3.2",
"In-context example selection",
"3.3",
"Leveraging eARCO for RCA Generation",
"5",
"EXPERIMENTAL SETUP",
"5.1",
"PromptWizard Configurations",
"5.2",
"Methods and Baselines",
"5.3",
"Evaluation Metrics",
"6",
"EXPERIMENTAL RESULTS",
"6.1",
"Evaluating Performance of eARCO with",
"Large Models",
"6.2",
"Small Models",
"6.3",
"Ablation Results",
"7",
"DISCUSSION",
"6.4",
"Human Evaluation Results",
"8.2",
"8.3",
"8",
"RELATED WORK",
"9",
"CONCLUSION",
"8.1",
"Incident Management",
"REFERENCES"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2504.11505v1.pdf"
}