{
"text": "Better by Comparison: Retrieval-Augmented\n       Contrastive Reasoning for Automatic Prompt\n                      Optimization\n\n\n             Juhyeon Lee*       Wonduk Seo*        Hyunjin An       Seunghyun Lee         Yi Bu†\n              Peking University           Enhans              Enhans               Enhans          Peking University\n                 Beijing, China        Seoul, South Korea    Seoul, South Korea     Seoul, South Korea       Beijing, China\n           leejuhyeon@pku.edu.cn   wonduk@enhans.ai    hyunjin@enhans.ai    seunghyun@enhans.ai    buyi@pku.edu.cn\n\n\n\n\n           Abstract—Automatic  prompt   optimization  has   recently  were effective in white-box settings but inapplicable to black-\n        emerged as a strategy for improving the quality of prompts  box LLMs accessed via API. More recent work has shifted2025         used  in Large Language Models (LLMs), with the goal  of                                                                              to black-box approaches that explore prompts through trial-\n         generating more accurate and useful responses. However, most\n                                                                             and-error, such as PromptAgent’s Monte Carlo Tree Search          prior work focuses on direct prompt refinement or model fine-Oct   tuning, overlooking the potential of leveraging LLMs’ inherent   [9], hierarchical multi-agent workflows [10], and multi-agent\n         reasoning  capability  to learn from contrasting examples. In   topology optimization [11]. In parallel, some methods treat3\n           this paper, we present Contrastive Reasoning Prompt Opti-  LLMs themselves as optimizers, for instance, OPRO [12]\n         mization (CRPO), a novel framework that formulates prompt                                                                   frames optimization as iterative natural language refinement,\n         optimization as a retrieval-augmented reasoning process. Our\n                                                                    while evolutionary approaches like PromptBreeder [13] and        approach retrieves top-k reference prompt-response pairs from\n          the HelpSteer2 dataset, an open-source collection where each   branching  structures such  as AMPO  [14]  adapt prompts\n         response  is annotated for helpfulness, correctness, coherence,   through mutation, selection, and pruning.\n          complexity, and verbosity, and constructs two complementary     Despite these advances, current methods share several key[cs.CL]         optimization paradigms: (1) tiered contrastive reasoning, where                                                                                limitations:  (1) They often optimize prompts  in  isolation,\n          the LLM compares high-, medium-, and low-quality exemplars\n                                                                                 failing to learn from the comparative lessons offered by better         (both prompts and  responses)  to  refine  its own  generation\n         through  reflective reasoning, and  (2)  multi-metric  contrastive  and worse exemplars; (2) Many approaches also depend on\n          reasoning, where the LLM analyzes the best exemplars along   handcrafted optimization pipelines, reducing their generality\n         each evaluation dimension and integrates their strengths into  and  scalability across domains;  (3)  Finally, most  existing\n        an optimized prompt. By explicitly contrasting high- and low-                                                              methods focus primarily on improving answer quality, often\n          quality exemplars, CRPO enables the model to deduce why\n                                                                         neglecting human-centered dimensions such as interpretability          certain prompts succeed while others fail, thereby achieving more\n         robust and interpretable optimization. Experimental results on  and usability, which are essential for practical deployment in\n          the HelpSteer2 benchmark demonstrate that CRPO significantly   real-world human–AI interaction.\n         outperforms baselines. Our findings highlight the promise of con-    To address these limitations, we introduce Contrastive Rea-\n           trastive, retrieval-augmented reasoning for advancing automatic                                                                 soning Prompt Optimization (CRPO), a retrieval-augmented\n        prompt optimization.\n                                                              framework  that  explicitly  leverages  contrastive  reasoning           Index Terms—Automatic Prompt Optimization, Contrastive\n         Reasoning, Retrieval-Augmented Generation, Large Language   across prompts of varying quality. Unlike methods that only\n        Models (LLMs), Helpfulness Alignment.                         consider prompt text and scores, CRPO retrieves top-k refer-arXiv:2509.02093v2                                                            ence prompt-response pairs from the HelpSteer2 dataset [15]\n                                        I. INTRODUCTION                                                                      using sparse retrieval over prompt text, where each pair is\n          Large Language Models (LLMs) have demonstrated  re-   annotated for helpfulness, correctness, coherence, complexity,\n        markable capabilities across a wide spectrum of natural lan-  and verbosity. By incorporating both the retrieved prompts and\n        guage processing (NLP) tasks, including reasoning, summa-   their responses, CRPO allows the model to reason not only\n          rization, and code generation. However, their performance is   about how prompts are phrased but also how they shape the\n         highly sensitive to the quality of the input prompt [1, 2]. As   quality of generated outputs. It then performs two novel opti-\n         a result, automatic refining prompts to elicit better responses   mization strategies: (1) tiered contrastive reasoning, where the\n         has become a critical research direction.                    model reflects on high-, medium-, and low-quality exemplars\n           Early prompt optimization methods focused on continuous   to refine its own generation, and (2) multi-metric contrastive\n          soft prompt tuning [3–5] or discrete token search [6–8], which   reasoning, where the best exemplars along individual metrics\n                                                                          are analyzed and integrated into an optimized prompt.\n           The authors denoted as * have contributed equally to this work. Author\n           order is determined alphabetically by first name.                  CRPO improves prompt  quality without model updates\n            † denotes corresponding author.                              by explicitly reasoning about why prompts succeed or fail;\n\nVariant 1: Tiered Contrastive Reasoning\n\n                                                                                     Adopt strengths    p* = fθ(Reflect(PH, PM, PL))                 R(q) = {p1 ,…, p10 }, pi ∈ HelpSteer2                       PH\n                                                                                       Balance\n                                                                        PM              Optimized prompt (p*)\n                                             5 dimensional (   )\n                                                                      partitioning                    Avg(pi )                             quantile\n                                                                                        PL    Avoid weaknesses                                                            of Avg(pi).\n\n\n\n\n   Variant 2: Multi-Metric Contrastive Reasoning\n\n                                                                                                 p* = fθ(Integrate(Phelp, Pcorr, Pcoh, Pcomp, Pverb))                 R(q) = {p1 ,…, p10 }, pi ∈ HelpSteer2\n\n                                                                     Optimized prompt (p*)\n\n                                                                     Phelp  Pcorr  Pcoh Pcomp Pverb\n\n\n\nFig. 1: Overview of CRPO: The framework of CRPO-Tiered Contrastive Reasoning(top) and CRPO-Multi-Metric Contrastive\nReasoning(bottom).\n\n\non HelpSteer2  it consistently outperforms direct generation,                           III. METHODOLOGY\nChain-of-Thought  prompting (CoT)  [16], and  simple  re-                                          We formulate  the prompt optimization problem not  as\ntrieval augmentation1, underscoring the promise of contrastive,                                                                     directly  fine-tuning model parameters, but as a  task  that\nretrieval-augmented optimization for aligning LLM outputs                                                       maximizes the inherent reasoning  capability of LLMs by\ntoward more helpful, factual, and coherent responses.                                                                         facilitating learning from reference exemplars. Specifically,\n                                                           our framework Contrastive Reasoning Prompt Optimization\n                             II. DATASET                                             (CRPO)  first  retrieves the top-k  relevant prompt-response\n  We  conduct  our  experiments  on   the  HelpSteer2   pairs from the HelpSteer2 training set, and then applies con-\ndataset [15], a benchmark designed for evaluating prompt   trastive reasoning to construct optimized prompts. We design\noptimization and response helpfulness  in Large Language  two complementary variants, illustrated in Figure 1.\nModels (LLMs). The dataset consists of human-annotated\n                                                             A. Retrieval of Reference Prompt–Response Pairs\nprompt-response  pairs  and  is  divided  into  training  and\nvalidation  splits. Specifically,  it contains a training set of     Given an input query q, CRPO retrieves a set of reference\n20.3k rows and a validation set of 1.04k rows, which we   prompt–response pairs\nadopt for retrieval and evaluation, respectively.\n                                                    R(q) = {(p1, r1), . . . , (pk, rk)},   (pi, ri) ∈HelpSteer2,\n  Each prompt–response  pair  in HelpSteer2  is  annotated\nacross five evaluation dimensions. Scores for each attribute  where pi denotes a prompt and ri  its associated response.\nrange from 0 (lowest) to 4 (highest), providing a fine-grained  Each pair is annotated along five evaluation dimensions M =\nscale of response quality. These annotations capture diverse   {help, corr, coh, comp, verb}. To ensure  sufficient coverage\naspects of quality, making the dataset suitable for studying   across  all dimensions, we require k ≥5, so that at least\ncontrastive reasoning in prompt optimization.                one candidate pair is available per metric. These exemplars\n                                                                (including both prompts and responses) serve as contrasting\nAttribute     Description                                         cases that enable the LLM to perform explicit reasoning.\n\nHelpfulness    Overall helpfulness of the response to the prompt.\nCorrectness    Inclusion of all pertinent facts without errors.               B. Variant 1: Tiered Contrastive Reasoning\nCoherence     Consistency and clarity of expression.               We partition R(q) into 3 tiers according to overall quality\nComplexity    Intellectual depth required to write the response (e.g., basic\n             competency vs. domain expertise).                         scores. Specifically, for each retrieved pair (pi, ri), we com-\nVerbosity    Amount of detail included in the response relative to what is   pute its average score across all five evaluation dimensions:\n               asked.\n                                                                         1\n                                                                   Avg(pi, ri) =  X Score((pi, ri), m),      (1)\nTABLE I: HelpSteer2 Annotation Dimensions. Five human-                  |M|\nannotated metrics, each scored on a 0–4 scale.                          m∈M\n                                                      where M =  {help, corr, coh, comp, verb}.  Pairs  are then\n                                                                   partitioned  into high-quality (P H), medium-quality (P M),\n  1Detailed implementations of baselines are detailed in Section IV.        and low-quality (P L) tiers based on quantile thresholds of\n\nModel                                     Helpfulness   Correctness   Coherence   Complexity   Verbosity   Avg. Score\n\n   GPT-4o\n    Direct Generation                             0.3616        0.4700       0.7723       0.3280       0.4913      0.4846\n   Chain-of-Thought (CoT)                       0.2981        0.3958       0.7169       0.2888       0.4709      0.4341\n    Retrieval Augmented Generation (RAG)         0.4903        0.5745       0.8642       0.4161       0.6567      0.6003\n   CRPO-Tiered Contrastive Reasoning†           0.5274        0.6006       0.8711       0.4360       0.6506      0.6171\n   CRPO-Multi-Metric Contrastive Reasoning†     0.5219        0.6274       0.8876       0.4386       0.6982      0.6347\n   LLaMA-3-8B\n    Direct Generation                             0.2616        0.3636       0.7078       0.2942       0.4421      0.4139\n   Chain-of-Thought (CoT)                       0.1600        0.2545       0.6291       0.2421       0.3812      0.3334\n    Retrieval Augmented Generation (RAG)         0.3990        0.4711       0.7989       0.3722       0.5814      0.5245\n   CRPO-Tiered Contrastive Reasoning†           0.4224        0.5023       0.8092       0.3993       0.6435      0.5554\n   CRPO-Multi-Metric Contrastive Reasoning†     0.4264        0.5070       0.8159       0.4146       0.6404      0.5609\n\nTABLE II: Comparison across five evaluation metrics—helpfulness, correctness, coherence, complexity, and verbosity—and\ntheir mean (Avg. Score). Results are reported separately for GPT-4o and LLaMA-3-8B. The best value within each language\nmodel is bold, the second best is underlined, and methods marked with † denote our proposed CRPO variants.\n\n\nAvg(pi, ri). The optimized prompt p∗is generated via con-     2) Evaluation  Model  and  Metrics:  We  employ  the\ntrastive reasoning:                                      ArmoRM-Llama3-8B-v0.1 reward model as an interpretable\n                                                                 multi-objective judge (> 90% benchmark accuracy), which\n         p∗= fθ Reflect(P H, P M, P L)  ,            (2)   automatically scores response quality.\n                                                            For evaluation metrics, we assess effectiveness by feeding\nwhere fθ  is the LLM and Reflect(·) instructs the model to   the optimized prompt p∗together with its generated response\n(i) avoid weaknesses in P L,  (ii) adopt strengths from P H,  r∗to the evaluation model E, which returns five HelpSteer2\nand (iii) use P M as a stabilizing anchor that reduces bias. In   scores—helpfulness, correctness, coherence, complexity, ver-\nparticular, incorporating P M prevents overfitting to extreme   bosity—each in [0, 4]:\ncases, ensuring balanced refinement that maintains robustness\nwhile still progressing toward high-quality prompts.                    Score(p∗, r∗) =  shelp, scorr, scoh, scomp, sverb  .      (4)\n\n                                                     Each score is normalized by dividing by 4 and then averaged\nC. Variant 2: Multi-Metric Contrastive Reasoning               to yield a final [0, 1] quality score:\n  For each metric m ∈M, we select the top pair:                        Score(p∗, r∗) = |M|1 X  sm(p∗,r∗)4      .          (5)\n                                                       m∈M\n       P m = arg  max    Score((pi, ri), m).\n                       (pi,ri)∈R(q)                                   (p∗, r∗) is judged under identical conditions to the baselines.\n                                                                  3) Baselines: We compare our framework against three\nThe optimized prompt is then constructed as:                                                                  representative baselines,  all of which optimize the prompt\n                                                          based only on the prompt text and evaluation scores, without\n  p∗= fθ Integrate(P help, P corr, P coh, P comp, P verb)  ,    (3)\n                                                                incorporating structured reasoning assets:\n                                                                   • Direct Generation: The LLM directly produces an opti-where Integrate(·) encourages the LLM to combine comple-\n                                                          mized prompt from the input query, without retrieval ormentary strengths across evaluation axes, grounded in both\n                                                                               iterative refinement.prompts and their associated responses.\n                                                                   • Chain-of-Thought (CoT): The LLM  is instructed to\n                                                              perform step-by-step reasoning on the input query before\n                    IV. EXPERIMENTS\n                                                                   generating an optimized prompt, relying solely on the\nA. Setup                                                                original prompt text and scores.\n                                                                   • Retrieval-Augmented Generation (RAG): The LLM\n  We evaluate CRPO on the HelpSteer2 dataset, ensuring                                                                      receives the input query along with the top-k retrieved\nfairness by using  the same  retrieval pool and  evaluation                                                                         training prompts (with k = 10 for fair comparison) and\nsettings as the baselines.                                                                    generates an optimized prompt based on these examples\n  1) LLM and  Retrieval  Models: We  use GPT-4o  and       and their scores, but without leveraging the associated\nLLaMA-3-8B, both with temperature 0 and otherwise default       reasoning assets.\nhyperparameters. BM25  is specifically used for prompt re-\n                                                             B. Main Experiment Resultstrieval, selecting top-k reference prompts {p1, . . . , pk} per\nquery with k = 10 to balance coverage across the 5 evaluation     Baseline methods demonstrate modest, albeit limited im-\ndimensions and efficiency.                                   provements. Direct Generation often produces shallow or in-\n\nGPT-4o        LLaMA3-8B                                                                                                                 CRPO-Tierd-GPT\n                Helpfulness        Correctness        Coherence                                                                                                    CRPO-Tierd-LLaMA\n                           0.64               0.90                              0.60                                                           CRPO-Multi-GPTCRPO-Multi-LLaMA\n         0.48               0.56               0.85\n         0.40               0.48               0.80                              0.55\n\n               CRPO  CRPO   TPS      CRPO  CRPO   TPS      CRPO  CRPO   TPS\n                    Tiered                              Multi               Tiered                                                             Multi               Tiered                                                                                           Multi                      k=5       k=10       k=15       k=20                                                    Overall               Complexity                                     Verbosity\n         0.44\n                                              0.60                       Fig. 3: Ablation 2. Overall score comparison across Top-K                           0.64         0.40\n                                              0.54                        settings in the RAG stage of CRPO, averaged over five metrics.\n         0.36               0.56\n                                              0.48\n               CRPO  CRPO   TPS      CRPO  CRPO   TPS      CRPO  CRPO   TPS\n                    Tiered   Multi               Tiered   Multi               Tiered   Multi\n                                      CRPO with k ∈{5, 10, 15, 20}. As shown in Figure  3,\nFig. 2: Ablation 1. Performance comparison of the simplified   performance improves when moving from a small evidence\nmethod vs CRPO variants. Overall stands for the average   set to a moderate pool, but declines as the pool grows further.\nscore of 5 metrics.                                           Fixing k = 10 yields the best trade-off between evidence\n                                                                     diversity and inference cost, producing the most stable gains\n                                                               across language models and CRPO variants.\nconsistent outputs, since no external context is used. Chain-of-\nThought (CoT) prompting encourages stepwise reasoning but\n                                                                               V. CONCLUSION\ntends toward verbosity and repetition without improving fac-\ntuality. Retrieval-Augmented Generation (RAG) provides con-     In  this  paper, we  introduced  Contrastive  Reasoning\ntextual grounding through retrieved prompts, but introduces  Prompt Optimization (CRPO), a retrieval-augmented frame-\nredundancy and lacks a clear mechanism to filter high-quality  work that improves prompt quality through tiered and multi-\nsignals. Importantly, all three baselines optimize prompts only   metric contrastive reasoning. Unlike prior methods that opti-\nwith respect to prompt text and evaluation scores, without  mize prompts based only on prompt text and scores, CRPO\nleveraging the responses associated with those prompts. As a   explicitly reflects on high-, medium-, and low-quality prompt–\nresult, while baselines improve over one another incrementally,   response pairs, leveraging both the prompts and their gen-\nthey remain insufficient for producing balanced and robust   erated responses to guide optimization. By integrating com-\noutputs.                                                    plementary strengths across multiple evaluation dimensions,\n  In contrast, CRPO addresses these shortcomings through  CRPO enables LLMs to produce more robust, interpretable,\nexplicit contrastive reasoning over prompt–response pairs. The  and human-aligned outputs without fine-tuning. Experimental\nmulti-metric variant integrates the strongest exemplars along   results on the HelpSteer2 dataset show that CRPO consis-\nhelpfulness, correctness, coherence, complexity, and verbosity,   tently outperforms baselines, including direct generation, CoT\nensuring complementary strengths are preserved. The tiered   prompting, and simple retrieval augmentation, with ablation\nvariant contrasts high-, medium-, and low-quality pairs, adopt-   studies confirming the central role of contrastive reasoning\ning strengths from the best, avoiding weaknesses from the   over prompt–response pairs in achieving these gains.\nworst, and stabilizing with medium-quality anchors. Together,\nthese strategies yield optimized prompts—and consequently                    REFERENCES\ngenerated responses—that are more robust, interpretable, and\n                                                                     [1] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh,human-aligned than those produced by Direct Generation,\n                                                                      “Calibrate before use: Improving few-shot performanceCoT, or RAG.\n                                                                     of language models,”  in  International conference on\nC. Ablation studies                                           machine learning, pp. 12697–12706, PMLR, 2021.\n  1) Trimaximal-Prompt Selection (TPS) : To assess the role    [2] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp,\nof contrastive reasoning, we conduct an ablation where CRPO         “Fantastically ordered prompts and where to find them:\nis reduced to using only the top-3 highest-ranked prompts,       Overcoming few-shot prompt order  sensitivity,” arXiv\nas shown in Figure 2. In contrast, both CRPO variants con-        preprint arXiv:2104.08786, 2021.\nsistently outperform the ablated setting by explicitly reasoning    [3] B. Lester, R. Al-Rfou, and N. Constant, “The power\nover contrasts—adopting strengths from high-quality prompts,        of scale for parameter-efficient prompt tuning,” arXiv\navoiding weaknesses from low-quality ones, and integrating        preprint arXiv:2104.08691, 2021.\ncomplementary aspects across dimensions. This result shows    [4] X.  L.  Li and  P.  Liang,  “Prefix-tuning:  Optimizing\nthat the gains of CRPO stem not from retrieval alone, but from        continuous  prompts  for  generation,”  arXiv  preprint\nits reflective reasoning process, which yields more stable and        arXiv:2101.00190, 2021.\ninterpretable optimization.                                          [5] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,\n  2) Optimal K validation: To identify the optimal evidence        L. Wang, W. Chen, et al., “Lora: Low-rank adaptation of\nof retrieved pool size in our RAG framework, we evaluate        large language models.,” ICLR, vol. 1, no. 2, p. 3, 2022.\n\n[6] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and\n     S. Singh, “Autoprompt: Eliciting knowledge from lan-\n     guage models with automatically generated prompts,”\n     arXiv preprint arXiv:2010.15980, 2020.\n [7] M. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo, T. Shu,\n    M. Song, E. P. Xing, and Z. Hu, “Rlprompt: Optimizing\n      discrete text prompts with reinforcement learning,” arXiv\n     preprint arXiv:2205.12548, 2022.\n [8] T. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E.\n     Gonzalez, “Tempera: Test-time prompting via reinforce-\n    ment learning,” arXiv preprint arXiv:2211.11890, 2022.\n [9] X. Wang, C. Li, Z. Wang, F. Bai, H. Luo,  J. Zhang,\n    N. Jojic, E. P. Xing, and Z. Hu, “Promptagent: Strate-\n     gic planning with language models enables expert-level\n    prompt optimization,” arXiv preprint arXiv:2310.16427,\n     2023.\n[10] Y. Liu, J. Singh, G. Liu, A. Payani, and L. Zheng, “To-\n     wards hierarchical multi-agent workflows for zero-shot\n    prompt optimization,” arXiv preprint arXiv:2405.20252,\n     2024.\n[11] H. Zhou, X. Wan, R. Sun, H. Palangi, S. Iqbal, I. Vuli´c,\n    A. Korhonen, and S. ¨O. Arık, “Multi-agent design: Opti-\n     mizing agents with better prompts and topologies,” arXiv\n     preprint arXiv:2502.02533, 2025.\n[12] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou,\n     and X. Chen, “Large language models as optimizers,”\n      in The Twelfth International Conference on Learning\n     Representations, 2023.\n[13] C. Fernando, D. Banarse, H. Michalewski, S. Osindero,\n     and  T.  Rockt¨aschel,  “Promptbreeder:  Self-referential\n     self-improvement via prompt evolution,” arXiv preprint\n     arXiv:2309.16797, 2023.\n[14] S. Yang, Y. Wu, Y. Gao, Z. Zhou, B. B. Zhu, X. Sun,\n      J.-G. Lou, Z. Ding, A. Hu, Y. Fang,  et  al., “Ampo:\n     Automatic multi-branched prompt optimization,” arXiv\n     preprint arXiv:2410.08696, 2024.\n[15] Z. Wang, Y. Dong, O. Delalleau,  J. Zeng, G. Shen,\n    D. Egert, J. Zhang, M. N. Sreedhar, and O. Kuchaiev,\n     “Helpsteer  2:  Open-source  dataset  for  training  top-\n     performing reward models,” Advances in Neural Infor-\n     mation Processing Systems,  vol. 37, pp. 1474–1501,\n     2024.\n[16]  J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia,\n     E. Chi, Q. V. Le, D. Zhou, et  al., “Chain-of-thought\n     prompting elicits reasoning in large language models,”\n     Advances  in  neural  information  processing  systems,\n      vol. 35, pp. 24824–24837, 2022.",
"headers": [
"Better by Comparison: Retrieval-Augmented",
"Contrastive Reasoning for Automatic Prompt",
"Optimization",
"arXiv:2509.02093v2  [cs.CL]  3 Oct 2025"
],
"tables": [
"|Model Helpfulness Correctness Coherence Complexity Verbosity|Avg. Score|\n|---|---|",
"|Direct Generation 0.2616 0.3636 0.7078 0.2942 0.4421<br>Chain-of-Thought (CoT) 0.1600 0.2545 0.6291 0.2421 0.3812<br>Retrieval Augmented Generation (RAG) 0.3990 0.4711 0.7989 0.3722 0.5814<br>CRPO-Tiered Contrastive Reasoning† 0.4224 0.5023 0.8092 0.3993 0.6435<br>CRPO-Multi-Metric Contrastive Reasoning† 0.4264 0.5070 0.8159 0.4146 0.6404|0.4139<br>0.3334<br>0.5245<br>0.5554<br>0.5609|\n|---|---|",
"|Col1|Col2|Col3|CRPO-Tierd-L|\n|---|---|---|---|\n||||CRPO~~-~~Multi~~-~~<br>~~CRPO-Multi-~~|\n|||||"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2509.02093v2.pdf"
}