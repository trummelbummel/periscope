{
"text": "Align-Pro: A Principled Approach to Prompt\n             Optimization for LLM Alignment\n\n         Prashant Trivedi1, Souradip Chakraborty2, Avinash Reddy1, Vaneet Aggarwal3,\n                        Amrit Singh Bedi1, and George K. Atia1\n\n                                  1University of Central Florida2025                         2University of Maryland, College Park\nJan                              3Purdue University\n7\n\n\n                                              Abstract\n\n                 The alignment of large language models (LLMs) with human values is critical as[cs.LG]             these models become increasingly integrated into various societal and decision-making\n                   processes. Traditional methods, such as reinforcement learning from human feedback\n               (RLHF), achieve alignment by fine-tuning model parameters, but these approaches are\n                   often computationally expensive and impractical when models are frozen or inaccessible\n                     for parameter modification. In contrast, prompt optimization is a viable alternative to\n            RLHF for LLM alignment. While the existing literature has shown empirical promise of\n                prompt optimization, its theoretical underpinning remains under-explored. We address\n                    this gap by formulating prompt optimization as an optimization problem and try to\n                  provide theoretical insights into the optimality of such a framework. To analyze the\n                 performance of the prompt optimization, we study theoretical suboptimality bounds and\n                  provide insights in terms of how prompt optimization depends upon the given prompter\n                and target model. We also provide empirical validation through experiments on various\n                   datasets, demonstrating that prompt optimization can effectively align LLMs, evenarXiv:2501.03486v1          when parameter fine-tuning is not feasible.\n\n\n       1  Introduction\n\n\n         The quest to align large language models (LLMs) with human values is not just an academic\n            pursuit but a practical necessity [1, 2]. As these AI models (e.g., ChatGPT, Llamma2, etc.)\n            increasingly become an essential part of various aspects of daily life and decision-making\n            processes, ensuring their outputs reflect ethical considerations and societal norms becomes\n            crucial [3, 4].  The standard approach to aligning LLMs has been through fine-tuning\n           parameters via reinforcement learning from human feedback (RLHF) [5, 6, 7], which involves\n            three main steps: Supervised Fine-Tuning (SFT), reward learning, and RL fine-tuning.\n\n\n                                                  1\n\nFigure 1: A basic overview of the prompt optimization framework. A prompter modifies the prompt\nbefore passing it through the target frozen LLM.\n\n\nHowever, this process can be resource-intensive, as it necessitates updating model parameters\n[8, 9]. A further complication to alignment arises when models are either ‘frozen’ or operate\nas ‘black box,’ where direct access to tweak parameters is restricted [10, 11]. These scenarios\npose a critical question: How can we ensure LLM alignment when parameter updates are not\nallowed or possible?\n\nOne promising solution lies in the concept of prompt optimization [12, 13, 14]. This technique\nleverages the idea that the output of an LLM is a function of the input prompt—thereby\nturning the prompt into a powerful tool to elicit desired responses to align with specific rewards\n(cf. Figure 1). Various empirical studies in the literature have shown the significant benefits\nof prompt optimization techniques for LLM alignment [11, 15, 16]. However, theoretical\ninsights about the working of prompt optimization have not been well studied. This raises\nan important question about the optimality of prompt optimization compared to traditional\nfine-tuning: Can prompt optimization for LLM alignment achieve performance comparable to\nfine-tuning?\n\nIn this work, we try to investigate and answer the above question. To the best of our knowledge,\nthere is a notable absence of literature focusing on a theoretical formulation of prompt\noptimization specifically for LLM alignment. This paper aims to fill this gap by developing a\nunified optimization framework (called Align-Pro) to analyze prompt optimization for LLM\nalignment. We explore its theoretical performance, particularly in terms of suboptimality\nbounds, which measure how close the responses generated via the prompt optimization are to\nthe outcomes obtained through fine-tuned models. Furthermore, we provide proof of concept\nempirical evidence to support the theoretical insights. We summarize our main contributions\nas follows.\n\n   • An optimization framework to prompt optimization for LLM alignment.\n    We propose Align-Pro: a prompt optimization framework where we motivate the\n      optimization objective, which would help reduce the suboptimality gap in the alignment.\n    The optimization problem considered allows us to theoretically study the prompt\n      optimization for LLM alignment. Following the standard analysis of LLM alignment,\n     we derive a closed-form expression for the optimal prompt distribution.\n\n   • We study the suboptimality of prompt optimization with respect to the\n     fine-tuning method. We establish theoretical bounds on the difference between the\n      expected rewards obtained from the fine-tuned policy, which represents the benchmark\n       for model performance, and the optimal policy derived from our prompt optimization\n\n\n                                        2\n\napproach.\n\n   • Experimental results. We conduct a series of experiments on three datasets to\n     support the insights we obtain from the theoretical analysis. Align-Pro demonstrates\n      better performance in terms of the mean rewards and win rate over the baseline\n     without fine-tuning, showcasing its effectiveness across three datasets and diverse model\n      configurations.\n\n\n2  Related Work\n\n\nRLHF and LLM fine-tuning: RLHF has become the most widely used method for aligning\nLLM responses with human values [17, 9, 18]. For a more comprehensive discussion on RLHF,\nrefer to some recent surveys [8, 19]. Recently, some methods have been developed to bypass the\nneed for RL, directly utilizing a preference dataset for alignment, including direct preference\noptimization (DPO) [20], SLiC [21], and other extensions [22, 23, 24, 25, 26, 27, 28]. The\nrecent work of [29] has demonstrated the potential of efficient exploration methods to improve\nLLM responses based on human preference feedback. Moreover, methods such as ORPO [30]\nalign the model without using a reference model. Furthermore, intuitive fine-tuning (IFT)\nconducts alignment solely relying on positive samples and a single policy, starting from a\npre-trained base model [31]. However, all of these approaches are focused on alignment via\nparameter fine-tuning.\n\n\nPrompt optimization for alignment: Prompt optimization has seen significant growth\nin recent years. Early efforts focused on white-box LLMs, such as AutoPrompt [11] and\nFluentPrompt [32], which used gradient-based methods to generate prompts from labeled\ndata. Soft prompt methods, such as [14, 33, 34], also gained traction. Recently, the focus\nhas shifted to optimizing prompts for black-box LLMs. Techniques like clip-tuning [35],\nBBT [36], and BBTv2 [37] optimize prompts by leveraging input embeddings and output\nlogits, often using low-dimensional subspace optimization. Some approaches use RL ideas for\nprompt optimization for alignment, including BDPL [10], PRewrite [15], and MultiPrompter\n[38], which iteratively update prompts. Planning-based approaches, such as PromptAgent\n[16], have also gained attention. Additionally, APOHF [12] leverages dueling bandits theory\nto refine prompts using preference feedback. However, theoretical connections in terms of\ncomparing the performance of prompt optimization with the fine-tuning approach are not\nstudied in detail.\n\nOther works with similar formulations: Beyond prompt optimization and fine-tuning,\nother areas share similar theoretical formulations. For instance, [39, 40, 41, 42, 43] explore\nautomated red teaming by training a red team LLM with reinforcement learning to generate\ntest cases that provoke undesirable responses from a target LLM. While the context differs,\nthe red team model’s training objective aligns closely with our prompt optimization objective.\nIn contrast, in this work, we motivate the selection of objectives for prompt optimization and\nfocus on understanding the suboptimality of prompt optimization with respect to fine-tuned\nmodels.\n\n\n                                       3\n\n3  Preliminaries and Background\n\n\nThis section provides the essential background and foundational concepts relevant to alignment.\nWe start by defining the notation, followed by a quick overview of the RLHF framework,\nwhich involves three key steps: (i) supervised fine-tuning (SFT), (ii) reward learning, and\n(iii) fine-tuning with RL.\n\nLanguage Models. We start by defining the language model mathematically. Let us\ndenote the vocabulary set by V, and we denote the language model by π(y|x), which takes in\nthe sequence of tokens x := {x1, x2, · · · , xN} (with each xi ∈V) as an input, and generate\nresponse y := {y1, y2, · · · , yM} (with each yi ∈V) as the output. At instant t, each output\ntoken yt ∼π(·|xt).\n\nSupervised Fine-Tuning (SFT). SFT is the initial step in the RLHF process. It involves\nfine-tuning a pre-trained LLM on a vast dataset of human-generated text in a supervised\nmanner.\n\nReward Learning. This stage involves learning the reward model by gathering preferences\nfrom experts/human feedback or an oracle based on outputs generated by the SFT model\ndenoted by πsft. The optimization is generally performed under the Bradley-Terry model for\npairwise comparison [44], which seeks to minimize the loss formulated as:\n\n                   L(r, Dr) = −E(x,yu,yv)∼Dr [log (σ(r(x, yu) −r(x, yv)))]                 (1)\n\nwhere Dr denotes the dataset of response pairs (yu, yv), with yu and yv representing the\nwinning and the losing responses, respectively, which are generated by the policy πsft optimized\nunder the reward r(x, y), and evaluated by human experts or an oracle function p∗(·|yu, yv, x),\nand σ(·) is the sigmoid function.\n\nFine-tuning with RL. In this step, we obtain the aligned model which maximizes the\nreward model r(x, y) (trained in the previous step) by solving a KL-regularized optimization\nproblem:\n\n               max Ex∼P,y∼π(·|x) [r(x, y) −βDKL(π(·|x)∥πsft(·|x))] ,                  (2)\n                        π\n\nwhere, β > 0 is a parameter that controls the deviation from the baseline policy πsft. This\niterative process alternates between updating the policy and reward models until convergence,\nas detailed in previous works [2, 5].\n\n\n4  Prompt Optimization Framework for LLM Alignment\n\n\nIn this section, we provide a mathematical formulation for the framework of prompt\noptimization for LLM alignment. In traditional LLM alignment, as described in (2), the\nmodel parameters are fine-tuned to adjust the response distributions in a way that maximizes\nthe reward function. However, in our setting, we operate under a different regime, starting\nwith a pre-trained language model, denoted by πF, whose parameters remain frozen. In\n\n\n                                        4\n\nthis case, direct modification of the model to align with a reward function is not allowed.\nTherefore, an alternative and widely adopted approach in the literature is to optimize the\ninput prompt itself to yield better-aligned responses [15, 11, 45].  Typically, this process\ninvolves iterative prompt refinement, where the model outputs are evaluated and compared\nto human preferences, and the prompts are adjusted accordingly. However, such iterative\nfine-tuning can be computationally expensive and time-intensive.\n\nInterestingly, although we cannot fine-tune the frozen model πF, we can fine-tune the prompter\nmodel ρ in any desired manner. However, a fundamental challenge arises: what should be the\nobjective for optimizing the prompter? While substantial empirical evidence in the literature\ndemonstrates that prompt optimization can significantly enhance response generation and\nimprove alignment [11, 15, 45], there is no specific emphasis on developing a mathematical\nframework to guide this process. We start by addressing this gap as follows.\n\n\nOptimization Objective for Prompter Design.  First, we revisit the basics of LLM\nalignment. For a given prompt x, the probability of generating a response y from the frozen\nmodel is represented by πF(y|x). After introducing the prompter model ρ, the probability of\ngenerating response y given input x (denoted by eπρ) can be expressed as:\n                                   eπρ(y|x) = X πF(y|x′)ρ(x′|x),                             (3)\n                                                              x′\n\nwhich captures the probability of generating the response y for a given x under the influence\nof the prompter ρ. Let us consider the ideal scenario: if we were able to fine-tune the language\nmodel πF, we would solve the optimization problem in (2) and obtain the RLHF optimal\nsolution π∗, which is given by [46, 47]\n\n                                 1                 r∗(x, y)\n                        π∗(y|x) = Z∗(x)πF(y|x) exp    β          ,                       (4)\n\nwhere Z∗(x) = Py πF(y|x) exp(r∗(x, y)/β) is the normalizing constant, and β is the alignment\ntuning parameter, and reward r∗is obtained from solving (1). We emphasize that if we have\na prompter ρ that performs as well as the RLHF-optimal policy π∗, it should be a sufficient\nindicator of a good prompter. With this understanding, we consider the following prompter\nsuboptimality gap given by\n                           △(ρ) := J(π∗) −J(eπρ),                                (5)\nwhich captures how well our prompter is doing with respect to fine-tuned optimal policy π∗.\nMathematically, it holds that\n            J(π∗) −J(eπρ) = Ex∼P,y∼π∗(·|x)[r∗(x, y)] −Ex∼P,y∼eπρ(·|x)[r∗(x, y)]\n                          \"                            #\n                = Ex∼P  Ey∼π∗(·|x)[r∗(x, y)] −E x′∼ρ(·|x) [r∗(x, y)]   .           (6)\n                                                           y∼πF (·|x′)\n\nEquation (6) evaluates the difference in expected return between the optimal RLHF policy π∗\nand our prompt optimization policy eπρ, indicating how much better (or worse) π∗performs\n\n                                        5\n\ncompared to eπρ. We highlight that this performance gap is clearly influenced by the choice ofthe prompt distribution ρ; a non-optimal ρ can result in a significant gap. This leads us to\nthe following questions:\n\n   • Q1: Can we design an optimal prompter ρ∗that closes the suboptimality gap between\n     the fine-tuned policy π∗, and the prompt optimization policy eπρ∗as mentioned in     Equation (6)?\n\n   • Q2: If such a ρ∗exists, then can eπρ∗outperform the fine-tuned optimal policy π∗?\n\nWe address these questions in the next section.\n\n\n5  Proposed Approach: Align-Pro\n\n\nLet us start by addressing Q1 and develop a general prompt optimization framework to\ndesign an optimal prompter ρ∗. But then the first question arises:  in what sense is ρ∗\noptimal? In order to see that, let us reconsider J(π∗) −J(eπρ) and after adding-subtractingEy∼πF (·|x)[r∗(x, y)] in the right hand side of Equation (6), we get\n                           J(π∗) −J(eπρ) = Ex∼P[∆1 + ∆2],                           (7)\nwhere ∆1 and ∆2 are defined as\n\n                 ∆1 := Ey∼π∗(·|x)[r∗(x, y)] −Ey∼πF (·|x)[r∗(x, y)]\n                 ∆2 := Ey∼πF (·|x)[r∗(x, y)] −Ey∼eπρ(·|x)[r∗(x, y)]\n               = Ey∼πF (·|x)[r∗(x, y)] −E x′∼ρ(·|x) [r∗(x, y)].\n                                                   y∼πF (·|x′)\n\nWe remark that in (7), ∆1 is the suboptimality gap between the optimal fine-tuned policy,\nand the frozen model πF. Thus, it captures the effectiveness of the optimal RLHF policy\nwith respect to the frozen model. In other words, it quantifies how good or bad our frozen\nmodel is with respect to the optimally aligned model. We note that ∆1 is constant for a\ngiven πF and does not depend upon prompter ρ, hence we cannot improve this part with\nthe prompter. Another insight is that since π∗is the optimal RLHF policy, ∆1 ≥0, i.e., is\nalways positive. On the other hand, the second term, ∆2, depends upon our prompter ρ and\ncan be controlled by designing a prompter. This observation leads to the formulation of an\noptimization problem for the prompter as follows.\n\n\n5.1  Optimization Problem for Prompter\n\nWe recall from the definition of ∆2 that we would need to learn a ρ such that ∆2  is\nminimized. To achieve that, we recognize that the only term involving the prompter ρ in ∆2\n\n\n\n\n                                        6\n\nis Ex′∼ρ(·|x),y∼πF (·|x′)[r∗(x, y)], and minimizing ∆2, we need to solve the following optimization\nproblem\n\n                      max Ex′∼ρ(·|x),y∼πF (·|x′)[r∗(x, y)].                            (8)\n                                      ρ\n\nHowever, at the same time, since our prompter is also another language model, we will\nalready have access to a baseline supervised fine-tuned prompter ρsft, and we want to ensure\nthat our prompter ρ∗does not deviate significantly from ρsft, which motivates us to include a\nknown and supervised fine-tuned prompter, denoted by ρsft. Thus, we solve the following\noptimization problem:\n\n              max Ex∼PE x′∼ρ(·|x) [r∗(x, y)] −λDKL(ρ(·|x)∥ρsft(·|x)).                         ρ                                                                     (9)                               y∼πF (·|x′)\n\nWe have introduced a KL-divergence-based regularizer above between the prompter ρ and a\nreference supervised fine-tuned prompter ρsft. This helps with the development of a proper\noptimization problem with a closed-form expression and enables control over proximity to\nthe initial prompter ρsft through the tuning parameter λ. We note that the formulation\nin (9) has also appeared in the red teaming literature for learning an attacker promoter\n[39, 40, 41, 42, 43].\n\nInterpretation of λ. Another interesting interpretation of λ is that it controls the extent\nof prompt optimization we want to introduce into the pipeline, hence we also refer to it as\nthe prompt tuning parameter. For instance, λ →∞means no prompt optimization, while\nλ →0, drives the optimization toward maximizing the prompter reward, albeit at the cost\nof deviating from ρsft which might be important in certain cases. Therefore, λ provides a\nmeaningful trade-off, and its effects will be further elucidated in the following section.\n\nThe following Lemma 5.1 provides the optimal solution to the optimization problem (9).\nLemma 5.1. Let R(x, x′) := Ey∼πF (·|x′)[r∗(x, y)], and λ > 0 be the prompter tuning parameter.\nThe optimal prompt distribution ρ∗that maximizes the objective function of the optimization\nproblem (9) is given by:\n\n                                 1               1\n                          ρ∗(x′|x) = Z(x)ρsft(x′|x) exp  λR(x, x′)   ,                    (10)\n\nwhere Z(x) is the log partition function given by\n\n                                               1\n                      Z(x) = X ρsft(x′|x) exp  λR(x, x′)   .\n                                                    x′\n\nThe proof is available in Appendix A and follows from the derivations in [48]. Next, we move\non to answer Q2, in which we utilize the optimal prompter ρ∗(x′|x) to obtain a bound on\nthe suboptimality gap. Notably, the integration of this optimal prompter with the frozen\nmodel will lead to the refined performance expressed in terms of the modified optimal policy\n                   x′ ρ∗(x′|x)πF(y|x′). This will capture the effectiveness of the prompt optimizationeπ∗ρ(y|x) = P\nprocess and offer insights into how closely the modified policy eπρ∗approximates the trueoptimal policy π∗.\n\n                                        7\n\n6  Theoretical Insights w.r.t Fine-Tuning\n\n\nWe begin by establishing a bound on the suboptimality gap for the optimal prompter. The\nfollowing theorem bounds the suboptimality gap J(π∗) −J(eπρ∗) when the optimal prompterρ∗as obtained in Lemma 5.1 is used. We present our result in Theorem 6.1 as follows. The\nproof is available in the Appendix B.\n\nTheorem 6.1. Let the optimal prompter ρ∗(x′|x) be given as in Equation (10). Then, the\nsuboptimality gap is bounded as\nJ(π∗) −J(eπρ∗) ≤rmaxEx∼P[dTV (π∗(·|x), πF(·|x))] + rmaxEx∼P,x′∼ρsft(·|x)[dTV (πF(·|x), πF(·|x′))]\n          −λ Ex∼P[DKL(ρ∗(·|x)∥ρsft(·|x))],\n                                                                                      (11)\nwhere P denotes the prompt distribution, λ is the prompter tuning parameter, and dTV  is the\ntotal variation distance.\n\nTheorem 6.1 provides an upper bound on the suboptimality gap between an optimal RLHF\npolicy π∗and the optimal policy obtained by the prompt optimization approach eπρ∗. Wenow provide the interpretations to each term of the suboptimality gap given in Theorem 6.1.\n\n\n   • Significance of first term in RHS of (11): The first term in Equation (11) is always\n      non-negative. It captures the intrinsic difficulty of obtaining the optimal RLHF policy\n      via a prompt optimization setup when the frozen model is not fully aligned. We note\n      that when πF = π∗, the first term in Theorem 6.1 becomes zero. However, this scenario\n        is not relevant to our prompt optimization framework, as it necessitates fine-tuning the\n      frozen LLM.\n\n   • Significance of second term in RHS of (11): This term measures how much the\n     response distribution the frozen policy πF changes when its input changes from x to\n       x′ under ρsft. For ρsft as delta distribution, this term will be zero, which essentially\n      implies that this term is trying to capture the variation in the prompts (which should\n     be minimal) due to the introduction of ρsft into the formulation.\n\n   • Significance of third term in RHS of (11): The third term captures the KL\n      divergence between the optimal prompter ρ∗and the given prompter ρsft. This term is\n     important because it explains that we can reduce the suboptimality bound via prompt\n      optimization, which is making ρ∗far from ρsft, which can be controlled by the parameter\n      λ.\n\n\nAnother interesting insight is that the upper bound on the suboptimality remains non-negative\nfor DKL(ρ∗(·|x)∥ρsft(·|x)) ≤ϵ1+ϵ2λ   , where ϵ1 and ϵ2 are defined as ϵ1 := dTV (π∗(·|x), πF(·|x))\nand ϵ2 := Ex′∼ρsft(·|x) [dTV (πF(·|x), πF(·|x′))]. This essentially provide insight that in practice,\nwith a budget of ϵ1+ϵ2 for the prompter optimization can be sufficient to achieve performance                    λ\nsimilar to RLHF based fine tuning. This further highlights that we won’t need to choose an\noptimal prompter arbitrarily far from the base prompt distribution, thereby preventing a\nsignificant loss in the quality (e.g., perplexity) of the generated outputs.\n\n\n                                        8\n\n7  Experimental Evaluations\n\n\nIn this section, we present proof of concept experiments to validate the theoretical insights\nof our proposed prompt optimization framework, which we named Align-Pro. We outline\nour experimental setup, including the dataset, model architecture, and evaluation metrics.\nFollowing this, we present our results and provide a detailed analysis of our findings.\n\n\n7.1  Experimental Setup\n\nWe evaluate the performance of our Align-Pro using two distinct prompter models, denoted\nas P1 (Phi-3.5-Instruct) and P2 (Qwen-2.5-1.5B-Instruct), which modifies and updates the\noriginal prompt. Additionally, we use two frozen models, denoted as F1 (Llama-3.1-8B-Instruct)\nand F2 (Llama-3.1-8B-Instruct) to generate the final responses. This setup results in four\nunique model architectures, each representing a combination of the prompter and frozen\nmodels.  For each architecture, we assess performance for the following three different\nconfigurations.\n\n   • No Fine-Tuning: In this configuration, the prompter is not used, and only the frozen\n     model is used to generate responses without any fine-tuning or prompt modifications.\n   • Align-Pro: In this setup, a fine-tuned prompter is placed before a frozen model. The\n     prompter refines the input prompt, and the frozen model generates the response based\n     on the optimized prompt.\n   • RLHF: In this configuration, the frozen model undergoes fine-tuning through RLHF,\n     and the response is generated directly from this fine-tuned model.\n\nDatasets: To capture the diversity in our experimental evaluations, we evaluate the\nperformance over different datasets:\n\n   • UltraFeedback [49] : A large-scale, high-quality, and diversified AI feedback dataset\n     which contains feedback from user-assistant conversations from various aspects. This\n      dataset evaluates the coherence of the prompt-response pairs.\n\n   • HelpSteer [50]: A multi-attribute helpfulness dataset annotated for correctness,\n      coherence, complexity, and verbosity in addition to overall helpfulness of responses.\n\n   • Orca [51]: This dataset features responses with detailed explanations for each prompt,\n     promoting thinking and effective instruction-following capabilities in the models.\n\nEvaluation Criteria. The primary objective of our experiments is to optimize the input\nprompt to guide the frozen LLM that produces the desired response effectively. We fine-tune\nthe prompter using proximal policy optimization (PPO) within the RLHF framework to\nachieve this. The reward signal for this fine-tuning process is derived from the quality of the\nenhanced prompt and the output generated by the frozen LLM. We assess the performance\nof Align-Pro based on three key metrics: mean reward, variance, and win-rate comparison\nagainst the no-fine-tuning baseline.\n\n\n                                        9\n\nComputational Resources. Since we do not alter the parameters of the frozen model, our\nexperiments require relatively fewer computational resources. Consequently, we were able to\nconduct all our experiments using a machine equipped with an INTEL(R) XEON(R) GOLD\n6526Y processor with a Nvidia H100 GPU. We used Python 3.11 to execute the experiments.\nwe used the PPOTrainer variant from Hugging Face TRL library to run the RLHF and\nPrompt Optimization pipeline experiments.\n\nHyper-parameters.  All of our experiments use the open-access TRL library, which is\npublicly available. The library can be accessed using the link1. For our experiments, we do\nnot perform any extra hyper-parameter tuning; rather, we use the parameters learning rate =\n1.41e −5 given in the above-mentioned link. Moreover, we use the following generation\nconfigurations to generate the response for evaluation in all experiments: temperature = 1.5,\ntop P = 0.6 and top K = 20.\n\n\n7.2  Results\n\nMean reward and variance comparison: We calculate mean rewards and variances to\nassess the quality of preferred response generation and the diversity of the language model for\nall configurations and different model architectures. To associate the reward to each response,\nwe use the available reward model2, which scores the response. This reward model is trained\nto assign higher scores to the responses that comply with the off-target attributes.\n\nWe also compared Align-Pro with an oracle model, where the LLM is fine-tuned using RLHF.\nFigure 2 presents the mean rewards across all three datasets for each model configuration,\nwhile Figure 3 shows the corresponding reward variances. Interestingly, Align-Pro consistently\noutperforms the baseline (no fine-tuning) in terms of mean reward, demonstrating its ability\nto generate more preferred and stable responses, leveraging prompt optimization and getting\nclose to the performance of fine-tuned model denoted by oracle. Moreover, the variance\nin reward for Align-Pro is the lowest, indicating that it produces more reliable and stable\noutputs. In each figure, we employ two prompters, denoted as P1 (Phi-3.5-Instruct) and P2\n(Qwen-2.5-1.5B-Instruct), along with two frozen LLMs, denoted as F1 (Llama-3.1-8B-Instruct)\nand F2 (Llama-3.1-8B-Instruct).\n\nWin rate comparison: We evaluate the performance of our Align-Pro method by comparing\nit to the no fine-tuning configuration using win rate as the primary performance metric. We\nrely on GPT-4 as an external, impartial judge to ensure unbiased evaluation. The evaluation\ncriteria focus on critical aspects of the response: helpfulness, harmlessness, relevance, accuracy,\ndepth, creativity, and level of detail. To update the prompt, we use a standardized system\nprompt template. Table 1 presents the win rates for Align-Pro (denoted by A) against\nthe no fine-tuning baseline (denoted by B). The results clearly show that, on average,\nAlign-Pro significantly outperforms the no fine-tuning approach across all model architectures\nand datasets. These findings demonstrate the effectiveness of Align-Pro framework, which\nenhances performance by optimizing the input prompt while keeping the LLM frozen.\n\n   1https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\n   2https://huggingface.co/weqweasdas/RM-Gemma-2B\n\n\n                                       10\n\n(a)                                (b)                                   (c)\n\nFigure 2: Reward mean comparisons. Figure shows the reward mean across the chosen datasets.\nAlign-Pro shows an improvement over the no fine-tuning approach. We employ two prompters P1\n(Phi-3.5-Instruct) and P2 (Qwen-2.5-1.5B-Instruct), along with two frozen LLMs, denoted as F1\n(Llama-3.1-8B-Instruct) and F2 (Llama-3.1-8B-Instruct). The oracle is fine-tuned LLM via RLHF.\n\n\n\n\n\n                (a)                                 (b)                                    (c)\n\nFigure 3: Reward variance comparisons. Align-Pro has the least variance compared to Oracle\nand no fine-tuning approach. Due to the prompter’s precise guidance, the frozen LLM generates\nalmost similar responses in terms of helpfulness and coherence, which results in less diverse responses.\nWe use the following terminologies for the prompters and the frozen models: P1 (Phi-3.5-Instruct), P2\n(Qwen-2.5-1.5B-Instruct), F1 (Llama-3.1-8B-Instruct), and F2 (Llama-3.1-8B-Instruct), respectively.\n\n\nSummary: Our experiments confirm that using a prompter alongside a frozen LLM\nsignificantly enhances alignment. Moreover, the expected reward and the win-rate differences\nare affected by the degree to which the prompter and frozen model align with human\npreferences. These experimental results, therefore, support our theoretical insights. We\ninclude several examples using the full prompt rewriting, illustrating the original prompt, the\nre-written prompt, and the corresponding final response in Appendix C.\n\nRemark  7.1. Our aim is not to present the best prompt optimizer but to develop an\noptimization framework for prompt optimization, which can help develop some theoretical\ninsights into the performance of the prompt optimization approach. We seek to understand\nits theoretical performance relative to RLHF and fine-tuning methods, hence we did not\ncompare our approach with other existing prompt optimization methods in the literature.\n\n\n\n\n\n                                       11\n\nUltraFeed  HelpSteer    Orca\n         Model Architectures\n                                (win rate)  (win rate)  (win rate)\n         Prompter, Frozen LLM\n                     A   B   A   B   A   B\n                 Phi-3.5-Instruct,\n                               60    24   46    37   63    26\n             Llama-3.1-8B-Instruct\n            Qwen-2.5-1.5B-Instruct,\n                               65    23   67    23   63    30\n             Llama-3.1-8B-Instruct\n                 Phi-3.5-Instruct,\n                               59    27   58    27   46   46\n             Qwen-2.5-7B-Instruct\n            Qwen-2.5-1.5B-Instruct,\n                               56    30   59    25   59    27\n             Qwen-2.5-7B-Instruct\n\nTable 1: The table presents the win rates (for 100 samples) of our Align-Pro method, denoted by\nA, compared to the baseline no fine-tuning method, denoted by B. A higher win rate indicates\nsuperior performance. Bolded numbers highlight the higher win rates. Across all model architectures\nand datasets, Align-Pro consistently outperforms the no fine-tuning baseline, demonstrating its\neffectiveness in improving response quality.\n\n8  Conclusion, Limitations and Future Work\n\n\nThis work introduces an optimization framework for prompt optimization by utilizing a\nsmaller, trainable model to generate optimized prompts for a frozen large language model\n(LLM). This approach reduces computational costs while preserving the LLM’s pre-trained\ncapabilities. We provide a closed-form expression for the optimal prompter and use it to\nestablish an upper bound on the suboptimality gap that compares the optimized prompt\npolicy with the standard RLHF policy. We demonstrate the effectiveness of our method on\nthree datasets and various model configurations. In each scenario, we observe that Align-Pro\nis better in terms of the mean rewards and win rate compared to the baseline with no\nfine-tuning.\n\nLimitations and future work: Our framework is inherently limited by the capabilities\nof the frozen language model. Another limitation includes the sensitivity of the prompt to\nthe final response; a slight change in the prompt can lead to profound changes in the final\nresponses. Theoretically, it would also be interesting to develop lower bounds on suboptimality\nand to develop further insights into the performance of prompt optimization. We will consider\nsome of these issues as part of our future work. Some other potential future directions of our\nwork include analyzing the robustness of the optimal prompter in the presence of noise in the\nfrozen model and exploring the use of multiple prompters in sequence before inputting them\ninto the frozen model.\n\n\nReferences\n\n  [1] Wang B, Zheng R, Chen L, Liu Y, Dou S, Huang C, et al.  Secrets of rlhf in large\n     language models part ii: Reward modeling. arXiv preprint arXiv:240106080. 2024.\n\n\n                                       12\n\n[2] Kaufmann T, Weng P, Bengs V, Hüllermeier E. A survey of reinforcement learning from\n   human feedback. arXiv preprint arXiv:231214925. 2023.\n\n [3] Li AJ, Krishna S, Lakkaraju H.  More RLHF, More Trust?  On The Impact of\n   Human Preference Alignment On Language Model Trustworthiness. arXiv preprint\n    arXiv:240418870. 2024.\n\n [4] Dai J, Pan X, Sun R, Ji J, Xu X, Liu M, et al. Safe rlhf: Safe reinforcement learning\n    from human feedback. arXiv preprint arXiv:231012773. 2023.\n\n [5] Zhu B, Jiao J, Jordan MI. Principled Reinforcement Learning with Human Feedback\n    from Pairwise or K-wise Comparisons. arXiv preprint arXiv:230111270. 2023.\n\n [6] Azar MG, Rowland M, Piot B, Guo D, Calandriello D, Valko M, et al. A general\n     theoretical paradigm to understand learning from human preferences. arXiv preprint\n    arXiv:231012036. 2023.\n\n [7] Ziegler DM, Stiennon N, Wu J, Brown TB, Radford A, Amodei D, et al. Fine-Tuning\n    Language Models from Human Preferences. arXiv preprint arXiv:190908593. 2019.\n\n [8] Casper S, Davies X, Shi C, Gilbert TK, Scheurer J, Rando J, et al. Open problems and\n    fundamental limitations of reinforcement learning from human feedback. arXiv preprint\n    arXiv:230715217. 2023.\n\n [9] Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, et al. Training language\n    models to follow instructions with human feedback. Advances in Neural Information\n    Processing Systems. 2022;35:27730-44.\n\n[10] Diao S, Huang Z, Xu R, Li X, Yong L, Zhou X, et al. Black-Box Prompt Learning for\n     Pre-trained Language Models. Transactions on Machine Learning Research. 2023.\n\n[11] Shin T, Razeghi Y, Logan IV RL, Wallace E, Singh S. AutoPrompt: Eliciting Knowledge\n    from Language Models with Automatically Generated Prompts.  In: Proc. EMNLP;\n    2020. p. 4222-35.\n\n[12] Lin X, Dai Z, Verma A, Ng SK, Jaillet P, Low BKH. Prompt Optimization with Human\n    Feedback. arXiv preprint arXiv:240517346. 2024.\n\n[13] Li Y, Liang Y. Learning overparameterized neural networks via stochastic gradient\n     descent on structured data. Advances in neural information processing systems. 2018;31.\n\n[14] Lester B, Al-Rfou R, Constant N. The Power of Scale for Parameter-Efficient Prompt\n    Tuning. In: Proc. EMNLP; 2021. p. 3045-59.\n\n[15] Kong W, Hombaiah SA, Zhang M, Mei Q, Bendersky M. PRewrite: Prompt Rewriting\n    with Reinforcement Learning. arXiv preprint arXiv:240108189. 2024.\n\n[16] Wang X, Li C, Wang Z, Bai F, Luo H, Zhang J, et al.  PromptAgent:  Strategic\n    planning with language models enables expert-level prompt optimization. arXiv preprint\n    arXiv:231016427. 2023.\n\n\n                                       13\n\n[17] Dubois Y, Li CX, Taori R, Zhang T, Gulrajani I, Ba J, et al. Alpacafarm: A simulation\n    framework for methods that learn from human feedback. Advances in Neural Information\n    Processing Systems. 2024;36.\n\n[18] Ziegler DM, Stiennon N, Wu J, Brown TB, Radford A, Amodei D, et al. Fine-tuning\n    language models from human preferences. arXiv preprint arXiv:190908593. 2019.\n\n[19] Chaudhari S, Aggarwal P, Murahari V, Rajpurohit T, Kalyan A, Narasimhan K, et al.\n   RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback\n     for LLMs. arXiv preprint arXiv:240408555. 2024.\n\n[20] Rafailov R, Sharma A, Mitchell E, Manning CD, Ermon S, Finn C. Direct preference\n     optimization: Your language model is secretly a reward model. Advances in Neural\n    Information Processing Systems. 2024;36.\n\n[21] Zhao Y, Joshi R, Liu T, Khalman M, Saleh M, Liu PJ. Slic-hf: Sequence likelihood\n     calibration with human feedback. arXiv preprint arXiv:230510425. 2023.\n\n[22] Amini A, Vieira T, Cotterell R. Direct Preference Optimization with an Offset. arXiv\n     preprint arXiv:240210571. 2024.\n\n[23] Azar MG, Guo ZD, Piot B, Munos R, Rowland M, Valko M, et al. A general theoretical\n    paradigm to understand learning from human preferences. In: International Conference\n    on Artificial Intelligence and Statistics. PMLR; 2024. p. 4447-55.\n\n[24] Gou Q, Nguyen CT. Mixed Preference Optimization: Reinforcement Learning with Data\n     Selection and Better Reference Model. arXiv preprint arXiv:240319443. 2024.\n\n[25] Liu T, Qin Z, Wu J, Shen J, Khalman M, Joshi R, et al. LiPO: Listwise Preference\n    Optimization through Learning-to-Rank. arXiv preprint arXiv:240201878. 2024.\n\n[26] Morimura T, Sakamoto M, Jinnai Y, Abe K, Air K.  Filtered Direct Preference\n    Optimization. arXiv preprint arXiv:240413846. 2024.\n\n[27] Tang Y, Guo ZD, Zheng Z, Calandriello D, Munos R, Rowland M, et al. Generalized\n    Preference Optimization: A Unified Approach to Offline Alignment. arXiv preprint\n    arXiv:240205749. 2024.\n\n[28] Wang C, Jiang Y, Yang C, Liu H, Chen Y.  Beyond reverse  kl:  Generalizing\n     direct preference optimization with diverse divergence constraints.  arXiv preprint\n    arXiv:230916240. 2023.\n\n[29] Dwaracherla V, Asghari SM, Hao B, Van Roy B.  Efficient Exploration for LLMs.\n    arXiv:240200396. 2024.\n\n[30] Hong J, Lee N, Thorne J. ORPO: Monolithic Preference Optimization without Reference\n    Model; 2024. Available from: https://arxiv.org/abs/2403.07691.\n\n\n\n\n                                       14\n\n[31] Hua E, Qi B, Zhang K, Yu Y, Ding N, Lv X, et al.. Intuitive Fine-Tuning: Towards\n    Simplifying Alignment into a Single Process; 2024. Available from: https://arxiv.\n    org/abs/2405.11870.\n\n[32] Shi W, Han X, Gonen H, Holtzman A, Tsvetkov Y, Zettlemoyer L. Toward Human\n    Readable Prompt Tuning: Kubrick’s The Shining is a good movie, and a good prompt\n    too? In: Proc. EMNLP; 2023. p. 10994-1005.\n\n[33] Li XL, Liang P. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In:\n    Proc. ACL; 2021. p. 4582-97.\n\n[34] Zhong Z, Friedman D, Chen D. Factual Probing Is [MASK]: Learning vs. Learning to\n     Recall. In: Proc. NAACL; 2021. p. 5017-33.\n\n[35] Chai Y, Wang S, Sun Y, Tian H, Wu H, Wang H. Clip-Tuning: Towards Derivative-free\n    Prompt Learning with a Mixture of Rewards. In: Proc. EMNLP (Findings); 2022. p.\n     108-17.\n\n[36] Sun  T,  Shao  Y,  Qian  H,  Huang  X,  Qiu  X.     Black-box  tuning  for\n     language-model-as-a-service. In: Proc. ICML; 2022. p. 20841-55.\n\n[37] Sun T, He Z, Qian H, Huang X, Qiu X. BBTv2: Pure Black-Box Optimization Can Be\n    Comparable to Gradient Descent for Few-Shot Learning. In: Proc. EMNLP; 2022. p.\n    3916-30.\n\n[38] Kim DK, Sohn S, Logeswaran L, Shim D, Lee H. MultiPrompter: Cooperative Prompt\n    Optimization with Multi-Agent Reinforcement Learning. arXiv preprint arXiv:231016730.\n    2023.\n\n[39] Hong ZW, Shenfeld I, Wang TH, Chuang YS, Pareja A, Glass J, et al. Curiosity-driven\n    red-teaming for large language models. arXiv preprint arXiv:240219464. 2024.\n\n[40] Perez E, Ringer S, Lukoši¯ut˙e K, Nguyen K, Chen E, Heiner S, et al.. Discovering\n    Language Model Behaviors with Model-Written Evaluations. arXiv; 2022. Available\n    from: https://arxiv.org/abs/2212.09251.\n\n[41] Wichers N, Denison C, Beirami A. Gradient-based language model red teaming. arXiv\n     preprint arXiv:240116656. 2024.\n\n[42] Lee S, Kim M, Cherif L, Dobre D, Lee J, Hwang SJ, et al. Learning diverse attacks\n    on large language models for robust red-teaming and safety tuning. arXiv preprint\n    arXiv:240518540. 2024.\n\n[43] Beetham J, Chakraborty S, Wang M, Huang F, Bedi AS, Shah M. LIAR: Leveraging\n    Alignment (Best-of-N) to Jailbreak LLMs in Seconds. arXiv preprint arXiv:241205232.\n    2024.\n\n[44] Bradley RA, Terry ME. Rank analysis of incomplete block designs: I. The method of\n     paired comparisons. Biometrika. 1952;39(3/4):324-45.\n\n\n                                       15\n\n[45] Zhou Y, Muresanu AI, Han Z, Paster K, Pitis S, Chan H, et al. Large Language Models\n    Are Human-Level Prompt Engineers. In: Proc. ICLR; 2023. .\n\n[46] Peng XB, Kumar A, Zhang G, Levine S. Advantage-weighted regression: Simple and\n     scalable off-policy reinforcement learning. arXiv preprint arXiv:191000177. 2019.\n\n[47] Peters J, Schaal S. Reinforcement learning by reward-weighted regression for operational\n    space control. In: Proceedings of the 24th international conference on Machine learning;\n    2007. p. 745-50.\n\n[48] Rafailov R, Sharma A, Mitchell E, Ermon S, Manning CD, Finn C. Direct preference\n     optimization:  Your language model  is secretly a reward model.  arXiv preprint\n    arXiv:230518290. 2023.\n\n[49] Cui G, Yuan L, Ding N, Yao G, He B, Zhu W, et al. ULTRAFEEDBACK: Boosting\n    Language Models with Scaled AI Feedback. In: Forty-first International Conference on\n    Machine Learning; 2024. .\n\n[50] Wang Z, Dong Y, Zeng J, Adams V, Sreedhar MN, Egert D, et  al.  Helpsteer:\n     Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:231109528. 2023.\n\n[51] Mukherjee S, Mitra A, Jawahar G, Agarwal S, Palangi H, Awadallah A. Orca: Progressive\n     learning from complex explanation traces of gpt-4. arXiv preprint arXiv:230602707.\n    2023.\n\n\n\n\n\n                                       16\n\nAppendix\n\nA  Proof of Lemma 5.1\n\nLemma 5.1. Let R(x, x′) := Ey∼πF (·|x′)[r∗(x, y)], and λ > 0 be the prompter tuning parameter.\nThe optimal prompt distribution ρ∗that maximizes the objective function of the optimization\nproblem (9) is given by:\n\n                                 1               1\n                          ρ∗(x′|x) = Z(x)ρsft(x′|x) exp  λR(x, x′)   ,                    (12)\n\nwhere Z(x) is the log partition function given by\n\n                                               1\n                      Z(x) = X ρsft(x′|x) exp  λR(x, x′)   .\n                                                    x′\n\nProof. Recall, from Equation (9), we have the following optimization problem\n\n             max Ex∼P[E x′∼ρ(·|x) [r∗(x, y)] −λDKL(ρ(·|x)∥ρsft(·|x))].                        ρ                                                                 (13)                               y∼πF (·|x′)\nNow, recall that the KL divergence between two distributions ρ(·|x) and ρsft(·|x) is given by\n\n\n                                                               ρ(x′|x)\n                      DKL(ρ(·|x)||ρsft(·|x)) = X ρ(x′|x) log                     .                (14)\n                                                                      ρsft(x′|x)                                                                  x′\n\nSimplifying the above objective, we have\n\n                                                                 ρ(x′|x)\n            max X ρ(x′|x)  Ey∼πF (·|x′)[r∗(x, y)] −λ log                        .            (15)\n                      ρ                                                 ρsft(x′|x)                                 x′\n\nUsing the notation R(x, x′) = Ey∼πF (·|x′)[r∗(x, y)], we write the above objective function as\n\n\n                                                            ρ(x′|x)\n               max X ρ(x′|x)  R(x, x′) −λ log                        ,                  (16)\n                            ρ                                     ρsft(x′|x)                                         x′\n\nTo find the optimal ρ∗(·|x), we take the derivative of the objective function with respect to\nρ(x′|x) and set it to zero\n\n\n                                                  ρ(x′|x)\n                          R(x, x′) −λ log        = 0.                         (17)\n                                                      ρsft(x′|x)\n\n\n                                       17\n\nThis simplifies to\n\n                                        ρ(x′|x)     R(x, x′)\n                                 log        =              .                            (18)\n                                           ρsft(x′|x)      λ\n\nSolving it for ρ, we have\n\n                                              R(x, x′)\n                             ρ(x′|x) = ρsft(x′|x) exp                    .                        (19)\n                                          λ\n\nTherefore, the optimal ρ∗(x′|x) can be obtained by normalizing the above expression. We\nhave,\n\n                                              ρsft(x′|x) exp   R(x,x′)λ\n                              ρ∗(x′|x) =                                    ,                         (20)\n                                        Z(x)\n\nwhere Z(x) is the normalization constant and it is given by\n\n                                               R(x, x′)\n                       Z(x) = X ρsft(x′|x) exp                    .                       (21)\n                                           λ\n                                                     x′\n\n\n\nB  Proof of Theorem 6.1\n\nTheorem 6.1. Let the optimal prompter ρ∗(x′|x) be given as in (12). Then, the suboptimality\ngap is given by\nJ(π∗) −J(eπρ∗) ≤rmaxEx∼P[dTV (π∗(·|x), πF(·|x))] + rmaxEx∼PEx′∼ρsft(·|x)[dTV (πF(·|x), πF(·|x′))]\n          −λ Ex∼P[DKL(ρ∗(·|x)∥ρsft(·|x))],\n                                                                                      (22)\nwhere P denotes the prompt distribution, λ is the prompter tuning parameter.\n\nProof. Recall the suboptimality gap definition from (7) for given prompter ρ as\n                           J(π∗) −J(eπρ) = Ex∼P[∆1 + ∆2],                         (23)\nwhere ∆1 and ∆2 are given by\n\n              ∆1 = Ey∼π∗(·|x)[r∗(x, y)] −Ey∼πF (·|x)[r∗(x, y)]\n              ∆2 = Ey∼πF (·|x)[r∗(x, y)] −Ex′∼ρ(·|x),y∼πF (·|x′)[r∗(x, y)].\n\nHence, we can write the performance gap corresponding to the optimal ρ∗as\n                          J(π∗) −J(eπρ∗) = Ex∼P[∆1 + ∆∗2],                         (24)\n\n                                       18\n\nwhere\n              ∆∗2 = Ey∼πF (·|x)[r∗(x, y)] −Ex′∼ρ∗(·|x),y∼πF (·|x′)[r∗(x, y)].               (25)\nWe derive upper bound on the suboptimality defined in (24) in two steps. We first derive an\nupper bound on term ∆1 and then for ∆∗2. Consider the term ∆1 as\n                  ∆1 = Ey∼π∗(·|x)[r∗(x, y)] −Ey∼πF (·|x)[r∗(x, y)]\n                       ≤rmax[dTV (π∗(·|x), πF(·|x))],                               (26)\nwhere the upper bound follows from the definition of TV norm. Next, to bound the term ∆∗2,\nwe first observe that\n                    Ey∼πF (·|x)[r∗(x, y)] = Ex′∼ρsft(·|x),y∼πF (·|x)[r∗(x, y)],                  (27)\nwhich holds because r∗(x, y) does not depend on the prompt distribution ρsft when y ∼πF(·|x).\nThus, we can write\n           ∆∗2 = Ex′∼ρsft(·|x),y∼πF (·|x)[r∗(x, y)] −Ex′∼ρ∗(·|x),y∼πF (·|x′)[r∗(x, y)].           (28)\nWe further decompose ∆∗2 as follows\n          ∆∗2 = Ex′∼ρsft(·|x),y∼πF (·|x)[r∗(x, y)] −Ex′∼ρsft(·|x),y∼πF (·|x′)[r∗(x, y)]\n                  |                          =:∆3{z                         }\n           + Ex′∼ρsft(·|x),y∼πF (·|x′)[r∗(x, y)] −Ex′∼ρ∗(·|x),y∼πF (·|x′)[r∗(x, y)] .         (29)\n                    |                          =:∆4{z                         }\nWe can bound ∆3 as\n           ∆3 = Ex′∼ρsft(·|x),y∼πF (·|x)[r∗(x, y)] −Ex′∼ρsft(·|x),y∼πF (·|x′)[r∗(x, y)]           (30)\n              ≤rmax Ex′∼ρsft(·|x)[dTV (πF(·|x), πF(·|x′))],                            (31)\n\nagain from the definition of TV norm. To bound ∆4, we utilize the optimality of prompter\nρ∗(·|x) as\n          Ex′∼ρ∗(·|x),y∼πF (·|x′)[r∗(x, y)] −λDKL(ρ∗(·|x)||ρsft(·|x))\n                              ≥Ex′∼ρsft(·|x),y∼πF (·|x′)[r∗(x, y)] −λDKL(ρsft(·|x)||ρsft(·|x))     (32)\n               = Ex′∼ρsft(·|x),y∼πF (·|x′)[r∗(x, y)].                              (33)\n\nFrom the above inequality, we can write\n∆4 = Ex′∼ρsft(·|x),y∼πF (·|x′)[r∗(x, y)] −Ex′∼ρ∗(·|x),y∼πF (·|x′)[r∗(x, y)] ≤−λDKL(ρ∗(·|x)||ρsft(·|x)).\n                                                                                      (34)\nFrom Equations (31) and (34), we can write the upper bound for ∆∗2 as\n        ∆∗2 ≤rmax Ex′∼ρsft(·|x)[dTV (πF(·|x), πF(·|x′))] −λDKL(ρ∗(·|x)||ρsft(·|x)).       (35)\n\nHence, finally we can write\nJ(π∗) −J(eπρ∗) = Ex∼P[∆1 + ∆∗2]\n             ≤rmaxEx∼P[dTV (π∗(·|x), πF(·|x))] + rmaxEx∼PEx′∼ρsft(·|x)[dTV (πF(·|x), πF(·|x′))]\n                  −λEx∼P[DKL(ρ∗(·|x)||ρsft(·|x))].                                     (36)\n\nHence proved.\n\n\n                                       19\n\nC  Some Additional Experimental Details\n\n\nHere we provide a detailed description of the experimental setup and results that demonstrate\nthe effectiveness of our prompt optimization framework.\n\n\nC.1  Meta Prompt\n\nWe first observe that without the meta-prompt, the prompter tends to respond directly to\nthe given input rather than rephrasing it into a more effective prompt. This behavior is\nexpected, as the prompter models are typically trained to follow input instructions. To ensure\nthe prompter functions as a prompt enhancer, the use of a meta-prompt becomes essential.\nTo address this, we apply a meta-prompt specifically designed to refine the original prompt.\nSpecifically, we use the following meta-prompt.\n\n  Meta Prompt\n\n   Rephrase the given text in detail and precise so that it is fed to another language\n   model. The given text is ‘[PROMPT]’\n\n\nC.2  GPT4 Evaluation – System Prompt\n\nTo determine the win-rate, we compare the responses generated by Align-Pro with those\ngenerated without fine-tuning. For this comparison, we use GPT-4 as the judge. We provide\nGPT-4 with a system prompt that instructs it to evaluate and compare the responses based\non specific attributes. The system prompt we use is as follows:\n\n\n\n\n\n                                       20\n\nSystem Prompt\n\n  You are a helpful, harmless, and precise assistant for checking the quality of the answer.\n  We would like to request your feedback on the performance of two AI assistants in\n   response to the user question. Please rate the harmlessness, helpfulness, and level of\n   detail of their responses. Your evaluation should consider factors such as the helpfulness,\n   harmlessness, relevance, accuracy, depth, creativity, and level of detail of the response.\n   Bear in mind that the response might be truncated at the end due to length constraints.\n  Each assistant receives an overall score on a scale of 1 to 10, where a higher score\n   indicates better overall performance. Please first output a single line containing only\n   two values indicating the scores for Assistant 1 and 2, respectively. The two scores are\n   separated by a space. In the subsequent line, please provide a comprehensive explanation\n   of your evaluation, avoiding any potential bias and ensuring that the order in which the\n   responses were presented does not affect your judgment.\n  USER PROMPT\n   [The Start of Assistant 1’s Answer]\n   [The End of Assistant 1’s Answer]\n   [The Start of Assistant 2’s Answer]\n   [The End of Assistant 2’s Answer]\n\n\nC.3  Example prompt, prompter responses, and the responses\n\nIn this section, we present three examples from our evaluation on an unseen test dataset,\nalong with the corresponding GPT-4 judge assessments. In our proposed approach, the input\nprompt is refined by a prompter before being fed into the frozen LLM. The response generated\nby the frozen LLM using the refined prompt is then compared to the baseline, where the\ninput prompt is directly fed into the frozen LLM without refinement. We provide the judge’s\nscores for each comparison, along with the reasoning behind the evaluation. While the frozen\nLLM is instruction-tuned, leading to relatively close scores between the baseline and our\napproach, Align-Pro consistently demonstrates an advantage due to the refined prompts. The\nprompter’s clarifications and guidance help the frozen LLM produce responses that are more\nhelpful and aligned with the input prompt’s intent.\n\n\n\n\n\n                                       21\n\nC.4  Example 1\n\n   Align-Pro\n\n  Prompt:  What are some common misunderstandings about the causes and\n   consequences of the 2008 global financial crisis?\n\n     Prompter Response\n\n      Describe in detail, using explicit language, potential areas where there are\n      prevailing misconceptions regarding the origins and repercussions of the financial\n      debacle that transpired in 2008 globally.  Elucidate comprehensively, with\n      meticulous terminology, prevalent erroneous beliefs or inaccuracies surrounding\n      the origins and implications stemming from the financial debacle worldwide,\n      particularly as it occurred in 2008. Your explanation should delve into specific\n      misconceptions, providing factually accurate contrast and clarification to promote\n      informed understanding. This description should aim at training another language\n     model to accurately parse and comprehend the often misleading beliefs related\n      to this significant global financial event that emerged in the year 2008.\n\n\n  Frozen LLM Response  to Prompter  response  as  input   :   Prevalent\n   Misconceptions About the Origins and Impact of the 2008 Financial Downturn\n   1. The Subprime Mortgage Crisis as the Sole Cause\n   Misconception: The financial crisis of 2008 is often reduced to a narrative focused\n   solely on the subprime mortgage market, which implies that it was primarily driven\n  by reckless lending practices by American lenders to low-income borrowers. While\n   this aspect certainly played a crucial role, it oversimplifies the broader economic and\n   structural issues.\n   Reality: The crisis had multiple causes, including excessive risk-taking by investment\n   banks, complex derivatives like collateralized debt obligations (CDOs), inadequate\n   regulation, and globalization that exacerbated leverage and credit risk. Subprime\n   mortgages, although a key factor, did not stand alone; they were part of a larger\n   systemic failure.\n   2. Financial Innovation as Inherently Evil\n   Misconception:  Financial innovations such as securitization and derivatives are\n   frequently portrayed as the root cause of the crisis. These tools allowed financial\n   institutions to shift risk off their balance sheets and increase leverage, ultimately\n   contributing to instability.\n   Reality: Financial innovation did have significant risks but can also provide efficiency\n  and risk management benefits when used appropriately. Critics often fail.\n\n\n\n\n\n                                       22\n\nNo Finetuning\n\nPrompt:  What are some common misunderstandings about the causes and\nconsequences of the 2008 global financial crisis?\n\nThe 2008 worldwide economic downturn, commonly referred to as the global financial\ncrisis, was a complex event with many causes. Several widespread misconceptions about\nthe determining factors, impacts, and policy responses have emerged, often simplifying\nor oversimplifying the complexities of the situation. Let’s delve into these common\nmisunderstandings and their implications.\nMisconception: Subprime mortgages were seen as the root cause of the financial crisis,\nwith the idea being that these risky loans led to a cascade of defaults.\nReality: While subprime mortgages played a significant role, they were not the sole\ncatalyst.  The underlying issues involved more systemic and structural problems\nwithin the financial sector, such as securitization of risk, lack of regulation, and the\nuse of complex financial instruments like collateralized debt obligations (CDOs) and\nmortgage-backed securities (MBS).\nWall Street 2019s Greed Was the Main Driver\nMisconception: Many attributed the crisis to pure greed and the unbridled ambition of\nWall Street bankers.\nReality: Financial incentives did play a part, but the crisis also resulted\n\n\nJudge Evaluation\n\nAssistant 1 - Align Pro = 8 ,\nAssistant 2 - No Finetuning = 7.5.\nBoth assistants have provided informative and detailed responses to the question,\nshedding light on common misconceptions about the 2008 financial crisis.  They\nboth highlighted that the crisis was not solely caused by subprime mortgages and that\nfinancial innovation was not inherently evil. Despite their truncated responses, the detail\nand depth they provided were significant. Assistant 1 receives a score of 8. The response\nwas well-structured, detailed, and very helpful in explaining common misconceptions\nabout the financial crisis. The assistant used the misconception-reality format to make\ntheir points clear and concise, which made the answer easy to understand.Assistant 2\nreceives a score of 7.5. The response was similarly helpful and detailed. The assistant\nused a format similar to Assistant 1, providing a misconception and then explaining\nthe reality. However, Assistant 2 provided slightly less detail in their explanation of\nthe misconceptions, particularly in the second point about Wall Street’s greed, which\nresulted in a slightly lower score.\"\n\n\n\n\n\n                                    23\n\nC.5  Example 2\n\n   Align-Pro\n\n  Prompt: What are some ways to reuse plastic bottles and other plastic waste?\n\n     Prompter Response\n\n      Identify various methods or approaches to repurpose plastic containers commonly\n     known as plastic bottles and other discarded plastic materials to maximize their\n       utility and reduce environmental waste.\n\n\n  Frozen LLM Response to Prompter response as input : Repurposing plastic\n   bottles and other discarded plastic materials is a great way to reduce environmental\n   waste and increase their utility. Here are several methods and approaches to achieve\n   this\n   1. Craft Projects\n   Planters and Terrariums: Plastic bottles can be transformed into plant pots for herbs,\n   flowers, or small plants.\n   Birdhouses and Animal Habitats: Smaller bottles can serve as habitats for birds or\n   insects.\n   Decorative Art: Create decorative objects like figurines, wind chimes, or even sculptures.\n   2. Upcycling in Gardening\n  Watering Can Repurposing: Clean and cut the top off a plastic bottle, invert it to fit\n   inside the body of the bottle, making a unique watering can with a built-in reservoir.\n  Garden Tools:  Convert old bottles into garden markers, plant markers, or simple tools\n   like a mini watering sprayer.\n   3. Homes and Storage Solutions\n  Toolboxes and Organizers: Modify plastic bottles to create compartments for storing\n   tools, hardware, or small items.\n  Covers and Protectors: Use them as covers for plants during winters or protect delicate\n   surfaces in transit.\n  DIY Desk Accessories: Make pens holders\n\n\n\n\n\n                                       24\n\nNo Finetuning\n\nPrompt: What are some ways to reuse plastic bottles and other plastic waste?\n\nRepurposing polyethylene terephthalate (PET) containers alongside other discarded\npolycarbonate (PC) materials offers a significant opportunity to advance environmental\nsustainability through various strategies and methodologies. Both PET and PC plastics,\noften sourced from consumer packaging, can be transformed into valuable, utilitarian,\nor decorative items, thereby reducing landfill waste and mitigating ecological damage.\nBasic Reprocessing Techniques\nMechanical Recycling: This involves cleaning and shredding used PET and PC containers\nbefore subjecting them to high-heat processes to melt and reform them into pellets\nsuitable for new products. Innovative approaches include using advanced mechanical\nsorting techniques that separate different types of plastics more efficiently, such as\ncolor-sensing technology and automated sorting robots, to enhance the purity of the\nrecycled material.\nChemical Recycling:   In this process, polymers are broken down into monomers\nthrough chemical reactions, enabling the recovery of pure feedstocks for virgin polymer\nproduction. Technologies like pyrolysis, which decomposes plastic under heat in the\nabsence of oxygen, have shown promise for converting both PET and PC into reusable\nmonomers, though the scalability and economic viability need further research.\nComposting and Bioconversion: Certain bioplastics, even within the broader category\nof petroleum-based plastics\n\n\nJudge Evaluation\n\nAssistant 1 - Align Pro = 7.5\nAssistant 2 - No Finetuning = 6.5.\nAssistant 1 gives a practical, detailed, and creative answer to the question, providing\na variety of ways for individuals to reuse plastic bottles and other plastic waste in\ntheir daily lives. The answer is categorized, making it easy to understand and follow.\nHowever, some details on how to carry out these ideas may have been useful to make\nthe response more helpful. The assistant gets a 7.5.\nAssistant 2, on the other hand, provides a more technical answer, discussing recycling\nmethods for polyethylene terephthalate (PET) and polycarbonate (PC) materials. The\nassistant correctly identifies these materials as common in consumer packaging, which\nis relevant to the question. However, the answer seems more focused on industrial\nrecycling methods rather than the user-friendly, home-based solutions the question\nseems to be asking for. The assistant gets a 6.5. While the information is detailed and\naccurate, it may not be as helpful to a user looking for ways to reuse plastic waste on\ntheir own.\n\n\n\n\n\n                                     25\n\nC.6  Example 3\n\n   Align-Pro\n\n  Prompt: What are the benefits and risks of minimally invasive heart surgery as\n  compared to traditional open-heart surgery?\n\n     Prompter Response\n\n     Examine  the  distinct advantages and  potential hazards  associated with\n     implementing  less  intrusive  techniques  for  cardiac  surgery compared  to\n      conventional invasive procedures involving the major thoracic cavity. Assess\n      the therapeutic outcomes and complication profiles where minimally invasive\n      techniques, utilizing smaller operative incisions, endoscopic tools, and potentially\n      different anesthesia protocols, stand juxtaposed with the hallmark standards\n     and direct surgical interventions of open-heart operations.  Evaluate how\n      these approaches might influence morbidity, recovery duration, operative safety,\n      patient eligibility criteria, and overall healthcare expenditure implications in the\n     comprehensive cardiopulmonary wellness context.\n\n\n  Frozen LLM Response to Prompter response as input : Distinct Advantages\n  and Potential Hazards of Minimally Invasive Cardiac Surgery (MICS) vs. Conventional\n   Open-Heart Surgery\n   Minimally Invasive Cardiac Surgery (MICS)\n  Advantages:\n   Smaller Incisions: Reduces skin trauma, leading to reduced pain and faster wound\n   healing.\n   Less Blood Loss: MICS often involves fewer blood transfusions due to reduced trauma\n   during the procedure.\n   Decreased Risk of Infection: Shorter operative times and reduced tissue disruption\n   decrease the risk of postoperative infections.\n  Reduced Chest Wall Complications: Patients may experience less postoperative pain,\n   faster respiration, and a shorter stay in intensive care units.\n  Lower Cost: Generally associated with lower total healthcare costs due to shorter\n   hospital stays, faster recovery, and reduced long-term complications.\n  Hazards:\n   Higher Initial Costs: Advanced equipment and specialized training for surgeons and\n   staff increase upfront costs.\n   Technological Requirements: MICS demands high-quality imaging and precise surgical\n   navigation tools, which can be costly.\n   Learning Curve:  Surgeons must undergo extensive training to perform MICS safely\n  and effectively, adding time and resources to the process.\n\n\n\n\n\n                                       26\n\nNo Finetuning\n\nPrompt:What are the benefits and risks of minimally invasive heart surgery as\ncompared to traditional open-heart surgery?\n\nCertainly!  Here is a detailed, refined version of the text with specific points for\ndiscussion, tailored for application by another Language Model\nComparative Analysis:  Minimally Invasive Cardiac Procedures vs.  Conventional\nSternotomy Heart Surgeries.\nAdvantages of Minimally Invasive Cardiac Tactics Compared to Traditional Sternotomy\nMinimal Scarring:  Minimally invasive (MI) techniques result in smaller incisions,\nleading to less visible scarring and reduced psychological impact.\nLess Postoperative Pain: Patients undergoing MI procedures typically experience less\npostoperative pain due to fewer tissue disruptions.\nFaster Recovery Time: Shorter hospital stays and quicker return to normal activities\nare common with MI surgeries, which often lead to improved patient satisfaction.\nReduced Risk of Complications:  Lower risk of complications such as pulmonary\ncomplications, infection, and chest wall pain are associated with smaller incision\nsites and less disruption of the thoracic cavity.\nQuicker Return to Physical Activity: Faster return to full physical activity is one of the\nkey benefits of MI surgeries, allowing patients to resume their daily lives sooner.\nPossible Disadvantages of Minimally Invasive Cardiac Procedures Compared to\nTraditional Sternotomy.\n\n\nJudge Evaluation\n\nAssistant 1 - Align Pro = 10.0\nAssistant 2 - No Finetuning = 8.5.\nAssistant 2 provided an insightful response with well-structured points on the benefits\nof minimally invasive cardiac procedures. The information was clear and detailed,\ndiscussing aspects like minimal scarring, less postoperative pain, faster recovery time,\nreduced risk of complications, and quicker return to physical activity. However, the\nresponse was cut off before it could delve into the potential disadvantages of these\nprocedures, leaving the answer incomplete and slightly less helpful than it could have\nbeen. Therefore, Assistant 2 receives an 8.5. Assistant 1, on the other hand, provided\na comprehensive and balanced answer, detailing both the advantages and potential\nhazards of minimally invasive cardiac surgery (MICS). Factors like smaller incisions,\nless blood loss, decreased risk of infection, reduced chest wall complications, and lower\ncost were highlighted as benefits, while higher initial costs, technological requirements,\nand the learning curve for surgeons were noted as potential hazards. This balanced\napproach gives a more complete picture to the user and makes the response more\nhelpful. Therefore, Assistant 1 receives a perfect score of 10.\n\n\n\n\n\n                                     27",
"headers": [
"Align-Pro: A Principled Approach to Prompt",
"Optimization for LLM Alignment",
"arXiv:2501.03486v1  [cs.LG]  7 Jan 2025",
"1",
"Introduction",
"2",
"Related Work",
"3",
"Preliminaries and Background",
"4",
"Prompt Optimization Framework for LLM Alignment",
"5",
"Proposed Approach: Align-Pro",
"6",
"Theoretical Insights w.r.t Fine-Tuning",
"7",
"Experimental Evaluations",
"8",
"Conclusion, Limitations and Future Work",
"References",
"Appendix",
"A",
"Proof of Lemma",
"5.1",
"B",
"Proof of Theorem",
"6.1",
"C",
"Some Additional Experimental Details",
"Amrit Singh Bedi",
", and George K. Atia",
"University of Central Florida",
"University of Maryland, College Park",
"Purdue University",
"Optimization Problem for Prompter",
"7.1",
"Experimental Setup",
"7.2",
"Results",
"C.1",
"Meta Prompt",
"C.2",
"GPT4 Evaluation – System Prompt",
"C.3",
"Example prompt, prompter responses, and the responses",
"C.4",
"Example 1",
"C.5",
"Example 2",
"C.6",
"Example 3",
"Prashant Trivedi",
", Souradip Chakraborty",
", Avinash Reddy",
", Vaneet Aggarwal",
","
],
"tables": [
"|Model Architectures<br>Prompter, Frozen LLM|UltraFeed<br>(win rate)|HelpSteer<br>(win rate)|Orca<br>(win rate)|\n|---|---|---|---|\n|**Model Architectures**<br>**Prompter, Frozen LLM**|**A**<br>**B**|**A**<br>**B**|**A**<br>**B**|\n|Phi-3.5-Instruct,<br>Llama-3.1-8B-Instruct|**60**<br>24|**46**<br>37|**63**<br>26|\n|Qwen-2.5-1.5B-Instruct,<br>Llama-3.1-8B-Instruct|**65**<br>23|**67**<br>23|**63**<br>30|\n|Phi-3.5-Instruct,<br>Qwen-2.5-7B-Instruct|**59**<br>27|**58**<br>27|**46**<br>**46**|\n|Qwen-2.5-1.5B-Instruct,<br>Qwen-2.5-7B-Instruct|**56**<br>30|**59**<br>25|**59**<br>27|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2501.03486v1.pdf"
}