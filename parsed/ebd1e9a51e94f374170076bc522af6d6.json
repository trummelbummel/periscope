{
"text": "Inference-Aware Prompt Optimization for Aligning\n                          Black-Box Large Language Models\n\n                Saaduddin Mahmud, Mason Nakamura,  Kyle H. Wray, Shlomo Zilberstein\n                                   Manning College of Information and Computer Sciences\n                                                     University of Massachusetts Amherst\n\n\n\n                               Abstract                             achieved substantial success, they are typically agnostic to\n                                                       how model outputs are aggregated or sampled, overlooking\n            Prompt optimization methods have demonstrated significant                                                                            the impact of such inference methods. Our initial empiri-\n               effectiveness in aligning black-box large language models\n                                                                                cal investigation reveals that the performance of optimized\n            (LLMs). In  parallel, inference scaling strategies such as\n                                                                  prompts is highly sensitive to the choice of inference-scaling2025      BEST-OF-N Sampling and MAJORITY VOTING have also\n             proven to enhance alignment and performance by trading         approach. Furthermore, our theoretical analysis reveals that\n                off computation. However, existing prompt optimization ap-        decoupling prompt optimization from inference can lead to\n             proaches are inference strategy agnostic; that is, they opti-        misalignment. Finally, we observe that optimal alignmentAug      mize prompts without regard to the inference strategy em-         requires careful consideration of user-specific preferences\n             ployed during deployment. This constitutes a  significant         regarding the trade-offs among multiple objectives, as well8\n             methodological gap, as our empirical and theoretical anal-         as the computational resources they are willing to expend.\n               ysis reveals a strong interdependence between these two        These findings expose a critical gap in current methods:\n             paradigms. Moreover, we find that user preferences regard-                                                                            the absence of a unified framework that simultaneously ac-\n              ing trade-offs among multiple objectives and inference bud-\n                                                                       counts for prompt optimization, inference-scaling strategies,\n               gets substantially influence the choice of prompt and infer-\n                                                                          user preferences, and computational resource constraints.             ence configuration. To address this gap, we introduce a uni-\n                fied novel framework named IAPO (Inference-Aware Prompt         To bridge this gap, we introduce IAPO (Inference-Aware[cs.CL]              Optimization) that jointly optimizes the prompt and inference        Prompt Optimization), a novel prompt optimization frame-\n                scale, while being aware of the inference budget and differ-       work designed explicitly to produce aligned responses from\n               ent task objectives. We then develop a fixed-budget training         inference-scaled black-box LLMs. IAPO simultaneously op-\n              algorithm for IAPO, which we call PSST (Prompt Scaling via         timizes prompt design and inference scaling strategies while\n              Sequential Trimming), and analyze finite-budget guarantees         considering different task objectives and computational bud-\n            on error probability. Finally, we evaluate the effectiveness          gets. We formulate the task of identifying an optimal policy\n              of PSST on six different tasks, including multi-objective text                                                                                for the IAPO framework as a contextual best-arm identifica-\n              generation and reasoning, and demonstrate the critical role of\n                                                                                tion (BAI) problem. To efficiently solve this, we propose a\n              incorporating inference-awareness when aligning black-box\n                                                                          fixed-budget training algorithm named PSST (Prompt Scal-         LLMs through prompt optimization.\n                                                                         ing via Sequential Trimming). Additionally, we introduce a\n                                                            warm-up heuristic that further improves performance within\n                        Introduction                            the training budget.\n                                                 We begin our analysis by deriving theoretical  finite-          In recent years, most state-of-the-art large language mod-\n                                                                     budget guarantees on the error probability of PSST. Next, we            els (LLMs) are accessible only through black-box APIs.\n                                                                           empirically demonstrate the effectiveness of PSST for learn-           Traditional alignment methods that require access to model\n                                                                         ing IAPO policies across six diverse tasks, including multi-          weights or logits are therefore infeasible. To address thisarXiv:2508.10030v1                                                                      objective text generation, mathematical reasoning, and com-            issue, prompt optimization-based alignment methods have\n                                                               monsense reasoning benchmarks. Additionally, our analysis          garnered interest (Chang et al. 2024). These methods typi-\n                                                               shows that ignoring inference scaling during prompt opti-           cally enhance input prompts by rewording or appending ad-\n                                                                       mization can lead to substantial misalignment, highlighting           ditional instructions to better align the models’ outputs with\n                                                                            the critical role of inference-awareness in aligning black-box          a task’s objectives. Another broadly applicable alignment\n                                                        LLMs.           strategy for black-box models is scaling inference computa-\n           tions using strategies such as BEST-OF-N Sampling or MA-\n         JORITY VOTING. These inference scaling methods generate                    Related Work\n          multiple candidate responses for the same query and select     Over the years, considerable effort has been devoted to\n           the final response via ranking or voting mechanisms (Wang       aligning large language models (LLMs) with human ex-\n            et al. 2022; Krishna et al. 2022; Gui, Gˆarbacea, and Veitch       pectations in downstream tasks (Minaee et al. 2024). Many\n         2024; Yue et al. 2025).                                       widely adopted alignment approaches—such as Supervised\n           Although existing prompt optimization techniques have      Fine-Tuning (SFT), Reinforcement Learning from Human\n\nFeedback (RLHF), and Reinforcement Learning with Veri-       these approaches avoid the cost of sampling at inference\nfiable Rewards (RLVR) (Lambert 2025)—require access to       time, they require full access to model parameters and do\nmodel weights. This limitation has motivated increasing in-      not generalize beyond BoN-style strategies. In contrast, our\nterest in black-box alignment methods such as prompt opti-     method is complementary to inference-aware fine-tuning de-\nmization, which can align black-box models through input      signed for black-box LLM.\nmanipulation alone (Zhou et al. 2022; Ouyang et al. 2022;\nChang et al. 2024). Prompt optimization has demonstrated        Inference–Aware Prompt Optimization\nstrong performance in both single-objective (Cheng et al.                                                                In this section, we first formalize the problem setup and in-\n2023; Trivedi et al. 2025) and multi-objective (Jafari et al.                                                              troduce the IAPO framework. Next, we present an empirical\n2024; Zhao et al. 2025) settings. However, these methods                                                       example that highlights the need for inference-aware opti-\nare agnostic of the inference strategy used during deploy-                                                                mization. Building on these observations, we then establish\nment, potentially leading to suboptimal performance. In con-                                                                     theoretical conditions under which IAPO is necessary com-\ntrast, our approach explicitly addresses the interdependence                                                            pared to disjoint optimization.\nof inference-time strategy and prompt optimization.\n  Recently, Shi et al. framed prompt optimization as a fixed-     Problem Formulation\nbudget best-arm identification (BAI) problem. While effec-\n                                                            Let X be the distribution of user queries and P a finite\ntive under limited evaluation budgets, the method remains\n                                                       prompt set. A pair (x ∈X, p ∈P) is submitted to a frozen\ninference agnostic and was only explored in single-objective\n                                                            black-box LLM, which, under fixed decoding hyperparam-\nsettings. Our work builds on this foundation in two key\n                                                                           eters, generates N ∈{1, . . . , Nmax}  i.i.d. completionsways: (1) we introduce a contextual formulation that mod-\n                                                         y1:N = (y1, . . . , yN). K bounded (potentially vector) ob-els user preferences over multiple objectives and associ-\n                                                                       jectives score each completion Ok : X × Y →[omink   , omaxk    ]ated computational costs; and (2) we incorporate inference-\n                                                                           (e.g. helpfulness, harmlessness, exact-match). We also de-\nawareness to ensure alignment with the actual inference\n                                                                       fine the cost of producing a response as Cost(x, yi), a com-strategy. To learn an optimal policy, we introduce a fixed-\n                                                                   posite function that takes into account various computational\nbudget contextual BAI algorithm, PSST, inspired by Se-\n                                                                     factors such as token count, time, and energy. We add it as\nquential Halving (SH) (Karnin, Koren, and Somekh 2013).\n                                                            a (K+1)-st objective Ok+1 = −Cost(x, yi). An externalWhile SH was originally developed for the pure bandit set-\n                                                                      entity supplies a context c = (w1, . . . , wK+1) ∈C, whereting, the IAPO framework features both inter-context full-\n                                                             every wk is chosen from a finite discrete domain. Given theinformation feedback and intra-context semi-bandit feed-\n                                                         above setup, we formalize the inference strategy as follows.\nback. PSST leverages these structural properties to achieve\nmore efficient optimization, extending beyond what stan-     BEST-OF-N (BON).  BoN returns the largest weighted\ndard SH can accommodate.                                               utility:\n  Another relevant line of work focuses on inference-time                    K               N\n                                                                            wk ok x, yi + wk+1 X ok+1(x, yi) .   (1)alignment, where model outputs are improved during infer-       RBONx    (c, p, N) = maxi≤N X\nence without modifying model parameters. Some of these                           k=1                    i=1\nmethods, such as GenARM and DEAL (Xu et al. 2024;                            |      task {zreward     }   |    inference{z  cost    }\nHuang et al. 2024), require access to model logits, limiting\n                                            MAJORITY VOTING (MV).  For query x, the pair (p, N)their applicability in black-box settings. In contrast, BEST-\n                                                                   yields i.i.d. completions y1:N and extracted answers ℓi =OF-N sampling (BoN) and MAJORITY VOTING (MV) meth-\n                                                           h(x, yi). For each distinct answer s, define the vote countods operate purely on model outputs and have shown strong\nempirical gains by generating multiple candidates and se-      ns = PNi=1 1[ℓi = s], the maximum n⋆= maxs ns, and the\nlecting the best one (OpenAI 2024; Yue et al. 2025; Wang        tie multiplicity t = Ps 1[ns = n⋆]. MV predicts uniformly\net al. 2022; Krishna et al. 2022). However, these approaches        at random among the t maximizers. With gold answer a(x)\nintroduce a non-trivial computational cost, and to our knowl-      and the success credit defined as o1(x, p, N) = 1[ na(x)=n⋆]t\nedge, none of them explicitly optimize the trade-off be-     we define MV utility as:\ntween computational budget and output quality. Further, our\n                                                                               N\n                                     X                                                                                                                          o2(x, yi, c) .       (2)preliminary experiments show that such inference-scaling         RMVx   (c, p, N) = w1 o1(x, p, N) + w2\nstrategies interact non-trivially with prompt design: prompts                                 |     {z     }       i=1\noptimized for single-shot decoding may yield suboptimal                                           task reward      |       {z       }\n                                                                                                                                                   inference cost\nperformance under BoN or MV, and vice versa. This neces-\nsitates an inference-aware prompt optimization framework.      Remark. A mixed strategy arises when different objec-\n   Finally, some white-box methods have recently  inte-       tives require different aggregation rules, e.g., applying MV\ngrated inference-awareness into the training process. For       for binary correctness and BoN for stylistic quality in rea-\nexample, Chow  et  al.  (2025)  proposed an  inference-      soning tasks. It is trivial to define it on the basis of the above.\naware fine-tuning procedure that explicitly optimizes for\n                                            IAPO Frameworkexploration–exploitation trade-offs under BoN. Similarly,\nBOND (Sessa et al. 2024) and BonBon (Gui, Gˆarbacea, and      Let an inference configuration be a tuple θ ∈Θ (e.g. temper-\nVeitch 2024) aim to distill BoN policies into a single-pass       ature, top-p, max token). Then we define a set of arms A in\ngeneration process through supervised fine-tuning. While     IAPO as: a = (p, θ, N) ∈A := P × Θ × {1, . . . , Nmax}.\n\n(b) Helpful-Harmless, BEST-OF-N (Prompt-   (c) Helpful-Harmless, BEST-OF-N (Prompt-\n          (a) MATH, Majority Vote         A)                                    B)\n\nFigure 1: Prompt–Inference Interdependence. (a) Accuracy under MAJORITY VOTING with LLAMA-3.3-70B-INSTRUCT,\nshowing prompt dominance shifts with budget (shaded). (b, c) Cost-adjusted reward under BEST-OF-N decoding. Prompt and\ninference scales vary with user-defined trade-offs.\n\n  Thus, each arm fixes the prompt, the decoding hyperpa-      performs best at low budget, but is eventually surpassed\nrameter, and the number of sampled completions. However,     by the blue prompt as MAJORITY VOTING becomes more\nthroughout the text, we fold the inference configuration into       effective. Second, inference-agnostic optimization can be\nthe prompt p and write a = (p, N). Finally, an IAPO policy       short-sighted: selecting a prompt based solely on single-shot\nis defined as a mapping π  : C → A that selects an arm     (N=1) accuracy would favor the green prompt, overlooking\nafter observing a context c.                                       the fact that the blue prompt is strictly superior for any user\n  Given a dataset X, context c ∈C, and aggregator α ∈       willing to allocate more compute.\n{BON, MV} the expected utility of arm a, i.e., the context-       To see how the green and blue trend can emerge, consider\naction value function or Q-function is defined as:                 the following example. Suppose in a reasoning task with\n                                              MV, for Prompt 1 we have 40% in Query 1, 90% in Query\n           Qα(c, a) := Ex∼X Rαx(c, a) .             (3)                                                                       2, and for Prompt 2, 62% (both queries). Single-shot aver-\nNote that Rαx(c, a)  is a random variable. Now,  let the      ages favor A (0.65 vs. 0.62), but under MV with N = 10, A\ncontext-optimal arm is a⋆(c) = arg maxa Qα(a, c); hence      drops to ≈0.63 while B improves to ≈0.77.\nthe optimal IAPO policy is defined as: π⋆(c) = a⋆(c), ∀c ∈       (b,c) Best-of-N on Helpful–Harmless. We evaluate two\nC.                                                       prompts (A and B) on  the Helpful–Harmless  bench-\n  In this paper, we adopt a train-then-deploy setup to learn     mark (Bai et al. 2022) using BEST-OF-N decoding for N ≤\nthe optimal IAPO policy. Given a total completion budget of       24. Each curve corresponds to a different user-defined trade-\nT, the learner may adaptively select arms at = (pt, Nt) ∈A       off between helpfulness and harmlessness, plotting the cost-\nand query xt ∼X, then observe the full raw reward vector       adjusted reward averaged over 1000 queries (see the Ap-\nmt ∈RK+1 for all completions. This process may continue      pendix for details). The optimal choice of prompt (A vs. B)\nuntil the budget is exhausted (Pt Nt = T). After spending      and sampling budget (N) is highly sensitive to these prefer-\nthe entire budget, the learner returns a deployment policy πT .       ences. For example, the prompt A is strictly preferred when\nThe performance of this policy is evaluated by the Average       helpfulness is weighted more heavily.\nContextual Return:                                         Having established the need for inference-aware opti-\n                                                                mization, we now examine the precise conditions under          ACR(πT ) = Ec∼C Qα(c, a) ,           (4)\n                                                       which joint optimization becomes essential. We start by es-\nThe goal of a learning algorithm is to return a deployment       tablishing the Inference-Agnostic (IA) utility:\npolicy πT for a fixed pull budget T that maximizes the ACR.                                                           Proposition  1  (Inference-Agnostic  Utility).  Inference-\n                                                               agnostic  prompt-optimization  methods  optimize  cost-\nMotivating Case Study                                                      unaware arithmetic mean utility.\nTo illustrate the limitations of inference-agnostic prompt\n                                                                          1\noptimization—and to motivate the joint treatment formal-       RIAx (c, a = (p, N)) =  PNi=1 PKk=1 wkok(x, yi).   (5)ized above—we conducted two  diagnostic experiments                N\nwith LLAMA-3.3-70B-INSTRUCT (Grattafiori et al. 2024)                                           Now we show under what conditions the IA policy re-\nstrictly treated as a black-box API. The results are summa-                                                        mains optimal or an optimal policy can be trivially recovered\nrized in Figure 1.                                                       from the IA Q-function.\n(a) MAJORITY VOTING  on MATH.  We  evaluate                                                           Proposition  2  (Inference-Agnostic  Optimality).  The\nthree manually designed prompts on the MATH bench-                                                               Inference-Agnostic  prompt-optimization  policy  remains\nmark (Hendrycks et al. 2021) under MAJORITY VOTING                                                             optimal under linear transformation of RIAx (c, a), that is,\nwith N ∈{1, . . . , 16}. Accuracy is plotted against total de-     kRIAx (c, a), k > 0 and an optimal policy can be recovered\ncoding cost, averaged over 300 queries (see the Appendix                                                                            trivially from Q-function under affine transformation:\nfor details). Two key observations emerge. First, prompt\npreference shifts with compute budget: the green prompt    QAF (c, a) := Ex∼X aRIAx  (c, a) + b = kQIA(c, a) + b.\n\nMajority Vote (MV)           Best-of-N (BoN)                                                                             1.0            Algorithm 1: Prompt Scaling via Sequential Trimming\n    60                            60                                         1.0\n                                                                             0.8            Require: Context set C, prompt set P, scale set N, Scaling\n (N)                              (N)0.8                                                                        strategy α, Query Dataset X, total pull budget T;    40                                  40                                 0.6                                         0.6 Expected                                                                      Expected              1: for all (c, a) ∈C × A do\n                                                                             0.4\n                                         0.420                                             2:     Fc,a ←true   Samples 20                                                        Samples\n                                         0.2                                                                                   Utility                             0.2 Utility              3: end for\n     0                                 0.00                                 0.0                 4: R ←⌈log2(|A|)⌉       0.0           0.5           1.0     0.0           0.5           1.0\n                Bernoulli p                       Bernoulli p                             5: for r = 1 to R do\n                                                                              6:    nr ← T/RFigure 2: Expected utility (wk+1 = 0) for MV (left) and\nBoN (right). MV shows a sharp performance drop when         7:     λ(r) ←ALLOCATE F, nr\nthe correctness probability drops below 0.5, whereas BoN         8:   B ←{}\nis strictly concave.                                                        9:     for (a, nr) ∈λ(r) do\n                                                                      10:        for i = 1...nr do\n  The above also highlights that affine aggregation signifi-       11:          Sample x ∼X\ncantly simplifies inference-aware optimization. For instance,       12:        B ←B ∪(a, x)\nin a regression task where the aggregated prediction is the       13:       end for\nmean of multiple numeric predictions and the reward is de-       14:    end for\nfined by the mean squared error (MSE), in some cases can       15:   D ←BATCH-QUERY B)\nbecome an affine transformation of IA, eliminating the need       16:   Qα(r) ←ESTIMATE-Q(D)\nto simulate inference scaling during training. However, com-                                                                      17:     for all c ∈C do\nmon inference scaling strategies like BoN and MV gener-                                                                      18:        A(r)c ←{a : Fc,a = true}\nally do not admit such affine formulations. While they can\n                                                                          A(r)c  by Qα(r)(c, a)sometimes be expressed as non-affine transformations of       19:       Rank\nthe IA—such as in the Bernoulli case with large N, where       20:      Remove bottom ⌈|A(r)c |/2⌉arms // i.e. update F\nRIAx (c, a) ≈p (Figure 2)—these are special cases. Hence,       21:    end for\ntrying to determine the prompt based on QIA for BoN or       22: end for\nMV will result in misalignment. This motivates the next sec-       23: return {a⋆c}c∈C                    // one survivor per context\ntion, where we develop a training method that handles the\ngeneral IAPO setting beyond the affine regime.\n\n                                                                             • Cross-context reuse. One pull of (p, N) on query x  Prompt Scaling via Sequential Trimming\n                                                                      yields the completion set y1:N and objective vector set\nIn this section, we propose a fixed-budget arm elimination-         o1:N that can can be used to estimate Rαx(c, p, N) for all\nbased strategy for training policy πT , called PSST (Prompt         c ∈C.\nScaling via Sequential Trimming). We then provide a the-\n                                                                             • Nested sample reuse across inference scales. Pulling\noretical analysis that establishes error guarantees for PSST\n                                                               a larger scale subsumes smaller ones: a pull of (p, Ni)under a finite inference budget. Finally, we introduce a prac-\n                                                              produces Ni/Nj   i.i.d. block samples for arm (p, Nj)tical approximation heuristic that improves computational\n                                                        by partitioning the Ni draws into disjoint groups of sizeefficiency without significantly compromising performance\n                                                 Nj (e.g., to recompute BoN/MV on each block).in many practical settings.\n  Our focus on the fixed inference budget setting is moti-     A key consequence is that, for a prompt, the largest sur-\nvated by the fact that training cost is often the main bottle-      viving scale drives the budget. Let Nmax(p)(r)   = max{ N  :\nneck in real-world applications. Moreover, PSST is designed       (p, N) survives at the start of round r }. If we allocate K\nto operate in a batched-exploration mode, which further re-                                  (r)\n                                                                     pulls (blocks) to (p, Nmax(p)) in round r, then every surviv-\nduces costs since many black-box APIs offer significant dis-                                         (r)\n                                                               ing arm (p, N) with N ≤Nmax(p) automatically receives atcounts for batched inference compared to individual calls.\n                                                                         least K effective samples by block reuse. Thus, an effectiveImportantly, PSST is also hyper-parameter-free, requiring no\n                                                   arm elimination strategy should exploit both (i) cross-scaleadditional tuning.\n                                                               reuse within prompts and (ii) cross-context reuse when scor-  Classical arm–elimination methods such as Sequential\n                                                                      ing, while being aware of asymmetric cost.Elimination (Even-Dar, Mannor, and Mansour 2006) and\nSequential Halving (Karnin, Koren, and Somekh 2013) fol-     Round  Structure.  Algorithm  1  proceeds  in R  =\nlow a simple recipe: (i) split the elimination process into      ⌈log2 |A|⌉rounds, and tracks per context active arm using\nmultiple rounds; (ii) in each round, allocate the round bud-                                                                 the flag F. Each round is allocated an equal pull budget of\nget across the surviving arms; and (iii) trim a subset of arms      nr = ⌊T/R⌋. An allocation routine, ALLOCATE(F, nr),\nat the end of the round based on their estimates. However,                                                                 divides this budget among the current set of unique active\nIAPO departs from pure BAI settings in the following key      arms, aggregated across all contexts. Based on this alloca-\nways:                                                                          tion, a batch of inference calls is issued to the target LLM.\n • Asymmetric pull cost. When arm (p, N) is pulled dur-     The resulting completions are scored using a reward func-\n   ing training, it uses N training budget.                          tion or verifier and stored in the dataset D. The Q-values are\n\nthen estimated from the collected data. Within each context,     Top-K Screening.  To further reduce the budget require-\narms are ranked and the worst performing half are elimi-     ment of PSST, we introduce Top-K Screening, a practical\nnated. After all rounds are completed, the algorithm returns       heuristic that executes a short, uniform prompt screening\na single final arm for each context.                                   at unit scale to trim obviously suboptimal prompts before\n                                                            running full PSST. Top-K Screening takes a budget frac-\nStructure-Aware Allocation Policy.  The allocation pol-\n                                                                     tion T0 = ⌊ρT⌋(ρ ∈(0, 1)) from PSST. With scale re-icy is designed with cross-context and cross-scale informa-\n                                                                         striction of N=1, the budget is allocated uniformly across\ntion sharing in mind. Specifically, let A(r) denote the set of                                                            prompts: each p ∈P receives  T0/|P|   i.i.d. samples.\nunique active arms in round r, aggregated across all con-                                                       Based on this data, Qα(c, p, 1) is estimated ∀c ∈C, p ∈P.\ntexts. For each prompt p, define\n                                                          For each context c, we retain the K best prompts P(0)c  =                (r)      Np,max = max{N | (p, N) ∈A(r)}                                              Top-K{ bQα(c, p, 1)  : p ∈P } and discard the rest. The\nas the maximum inference scale for prompt p among the ac-      subsequent PSST run is then restricted to the reduced arm\ntive arms. Then, PSST allocates the budget to each arm ac-                                                                       sets A(1)c = {(p, N) : p ∈P(0)c   , N ∈N} for each c, and\ncording to the following scheme:                                                              uses the remaining budget T ′ = T −T0. In the next sec-\n     (  nrN(r)p,max             max                         tion, we demonstrate that the screening strategy can signifi-\n  λ(a) =            (⌊  M    ⌋   if a = (p, Np   ) ∈A(r),   (6)       cantly improve performance in low training budget settings\n          0             otherwise,                          without compromising quality for practical tasks. However,\n                                         (r)                               theoretical guarantees comparable to those of full PSST can-\n                      max )∈A(r) Np,max is the total cost ofwhere M = Pp:(p,N p                                                               not be established; counterexample tasks can be carefully\nsampling all such maximal arms once. This policy maintains      constructed within IAPO framework, where Top-K screen-\nuniform coverage over prompts while respecting cost asym-      ing will behave suboptimally for any K < |P|.\nmetries and ensures that the maximum scale of every prompt\nhas an equal number of samples.                                   Empirical Evaluation\n  We now derive error bounds1 for PSST under the alloca-\n                                                                In this section, we empirically evaluate the effectiveness\ntion policies described above.\n                                                                of PSST and highlight the importance of inference-aware\nTheorem 1 (Error of PSST). Let R = ⌈log2 |A|⌉be the      prompt optimization (IAPO). Our evaluation has two pri-\nnumber of trimming rounds, and [omink   , omaxk    ] = [−1, 1] and     mary objectives:\ndefine the cost–gap complexity\n                                                                             • To demonstrate that PSST and the Top-K Screening\n                     ¯Nmax                                         heuristic are highly effective at learning policy πT .      Hc1 =  max             ,  H1 = max Hc1.\n                   (c,ai)̸=ac1 ∆2c,ai              c                           • To show that IAPO improves the average cost-adjusted\n                                                             reward (ACR) compared to inference strategy agnostic\n                             i¯Nmax\n      Hc2 =  max              ,  H2 = max Hc2.                  optimization.\n                  (c,ai)̸=ac1 ∆2c,ai               c                                                                Baselines.  We compare PSST and Top-K Screening with\nwhere, ∆c,ai = Qαc,a1 −Qαc,ai, arms are indexed based on       several baselines. We denote Top-K Screening with K =\nascending order of Qαc,a under that context and ¯Nmax =       1, K =  4,and K = 8 as PSST+K1, PSST+K4, and\na1(N)+Nmax                                   PSST+K8 respectively. For these heuristics, we fix ρ = 0.2,      2         . Running PSST with the structure-aware al-                                                       which was found to perform best across all datasets. Fulllocation of for a total prompt complication T returns the\n                                                   PSST is parameter-free and does not require any tuning. Inoptimal arm in every context with probability at least\n                                                            our first set of experiments, we compare our proposed meth-\n       1 −3|C|R exp −         T                .              ods against several standard exploration strategies:                           min(2|P|H1,8|C|H2)R\n                                                                             • Uniform: Uniformly explores all arms in one batch andEquivalently, to ensure failure probability at most δ it suf-\n                                                                         selects the best arm at the end.fices to choose\n                                                                             • ϵ-greedy: Samples a random context at each step and\n     T = O min(|P|H1, |C|H2)R log |C|Rδ       .                   selects the best arm with probability 1 −ϵ. We set ϵ =\n                                                                     0.15, which yielded the best performance across datasets.\nNote that applying Sequential-Halving without leveraging\n                                                                             • Softmax: Samples arms according to a softmax distribu-the structure of IAPO—specifically, without any form of in-\n                                                                        tion over estimated Q values.formation sharing across scales or contexts—incurs a sam-\nple complexity of O(|C|Nmax) higher.                                  • UCB: At each turn, selects the arm with the highest op-\n                                                                           timistic Q estimate. The exploration constant 0.1 after\nRemark:  While we describe the algorithm as where we          tuning.\nuse a new set of data in each round D, it has been shown that\n                                                         Note that all baseline methods share information acrossin similar halving-style algorithms (Fabiano and Cazenave\n                                                                contexts and inference scales; however, none of them are2021), data accumulating all past observations—known as\n                                                            designed to exploit IAPO structure, i.e., they are structure-stockpiling—can improve the complexity of T by reducing\n                                                                   agnostic.the outer R-factor, and is recommended to use with PSST.\n                                                                  In the second set of experiments, we consider the well-\n   1Proof in the appendix                                known contextual variant of TRIPLE-SH (Shi et al. 2024)\n\nFigure 3: Comparison between exploration strategies across six datasets.\n\nEnvironments     α    |P|  Nmax  omaxk     |X|    |C|        some of the theoretical findings. The remaining four en-\n                                                           vironments are based on widely-used real-world datasets.\nSynth–Bernoulli   MV   32    32     1.0    520   3                                            Among these, MATH (Hendrycks et al. 2021) and COM-\nMATH       MV   25    32     1.0    316   3                                           MONSENSEQA (Talmor et al. 2018) are used to evaluate\nCommonsenseQA  MV   48    32     1.0   1500   3                                                              reasoning tasks under MAJORITY VOTING (MV), while\nSynth–Categorical  BoN   32    32     4.0    512   27       HELPFUL-HARMLESS (Bai et al. 2022) and SUMMARIZA-\nHelpful–Harmless  BoN   20    32     1.0   1355  27                                                    TION (Stiennon et  al. 2020) are chosen for BEST-OF-N\nSummarization    BoN   20    32     1.0   1201  27        (BoN) evaluation.\n                                                            For the MV tasks, the task objective is defined as an exact\n             Table 1: Environment summary.                                                       match with the correct answer. All three BoN tasks are bi-\n                                                                     objective, and we use publicly available reward models frommethod, which optimizes prompt selection as a pure best-\n                                                              previous multi-objective LLM alignment studies to scorearm identification (BAI) problem. However, it does not opti-\n                                                            completions (see appendix for links). The cost objective inmize the inference scale. Therefore, we include two variants:\n                                                                                all six tasks is defined to be proportional to the average num-\n • TRIPLE (N = 1): Only performs prompt optimization      ber of tokens per response. For context specification, MV\n   with single-sample inference.                                  tasks include a budget regime $low, mid, high$, while BoN\n • TRIPLE (N = Random): Optimizes the prompts while       tasks include both the budget and the bi-objective weights,\n   randomly assigning N for each query.                     which range from 0.1 to 0.9 for each objective. For example,\n                                                                     in the helpful–harmless task, a context might be represented\n  These baselines help isolate the benefits of jointly op-                                                                as {helpful : 0.3, harmless : 0.7, budget : high(1.0)}. Fur-\ntimizing prompts and inference scale. Further, PSST+K1                                                                     ther details, including all prompts, are provided in the sup-\nis particularly interesting in this experiment, as it approx-                                                           plementary material.\nimates a two-stage disjoint optimization:  it first selects a\n                                                      To construct the environments, we first generated a setcontext-specific single-shot prompt using a cost-aware ob-\n                                                                of instruction prompts for each task using ChatGPT-O3.jective, and then tunes the inference scale. The PSST+K4\n                                         We then generated 128 responses for each prompt–queryand PSST+K8 heuristics represent intermediate strategies\n                                                                     pair and estimated  the  score  distribution using a  cat-between disjoint and fully joint optimization.\n                                                                  egorical model.  All completions were produced  using  Note that all hyperparameter sweep results are in the sup-\n                                                                 the LLaMA-3.3-70B-Instruct, a widely used open-plementary material; we report results with the best setting\n                                                             source model (Meta AI 2024), which we treat as a black-found across all six datasets.\n                                                     box throughout our experiments. Generation was carried out\nEnvironments.  We evaluated inference-aware optimiza-      with vLLM(Kwon et al. 2023) on a cluster of 8 A100 GPUs,\ntion across a total of six environments. Key details are       totaling approximately 2,000 GPU-hours. Once the environ-\nprovided in Table 1. Environments one and four are syn-      ments are constructed, all experiments can be run via a stan-\nthetically constructed to mimic IAPO tasks, where prompt-      dard CPU quickly. We will publish the environments and\nquery pair score distributions oi(c, P, 1) are modeled us-      code with the paper, enabling full reproducibility without\ning categorical distributions. We introduce them to validate      any substantial computational resources.\n\nFigure 4: Effectiveness of inference-aware optimization across six datasets.\n\nEvaluation Protocol.  All reported curves are averages       to 20K inference calls in practical settings.\nover 200 independent runs. For synthetic environments, we\n                                                     Importance of Inference-Awareness (Fig. 4).  We exam-instantiate 200 independent environments and report the av-\n                                                                 ine the role of inference awareness in prompt optimization.erage performance across them. For the remaining four envi-\nronments, each run reshuffles the dataset, performs an 80/20      Across all six datasets, IAPO methods markedly outperform\n                                                                 the inference-agnostic methods, demonstrating the gainstrain–test split, and trains the policy on the training set. In\n                                                               achievable when jointly optimizing the prompt and inferenceall six environments, we evaluate ACR on the test set us-\ning 10, 000 samples. Performance for each budget is the       scale. TRIPLE(N = 1) fails as it does not leverage infer-\nmean across the 200 runs, with standard error of the mean      ence scaling. On the other hand, TRIPLE(N = Random)\n                                                                              fails because it does not optimize the scaling for different(SEM) error bars. Statistical significance is assessed using\n                                                                   contexts. The screening variant PSST+K1—which effec-the Wilcoxon paired two-sided test with alpha 0.05, and we\n                                                                       tively approximates a near-decoupled (prompt-only) proce-indicate when differences are significant in the discussion.\n                                                           dure—fails to reach the optimum in most cases, perform-The full set of results is in the appendix.\n                                                               ing competitively only on COMMONSENSEQA and show-\nComparison of Exploration Strategies (Fig. 3).  PSST      ing pronounced underperformance on summarization. This\nand the Top-K screening heuristic consistently outperform        is because it gets stuck with deceiving prompts that fail to\nall baselines. Across all six domains, where the per-context       scale compared to prompts that may not perform well under\naction spaces are large (|P|Nmax ∈[640, 1536]), UCB, soft-       single-shot but improve significantly under scaling. These\nmax, and ε-greedy methods struggle to explore effectively.       findings underscore the essential role of IAPO in aligning\nAmong the baselines, UCB performs comparably in some      black-box LLMs and the pitfalls of disjoint optimization.\ndomains after T = 20K, but only with extensive hyperpa-       Overall, IAPO outperforms disjoint optimization by up to\nrameter tuning. Furthermore, these baselines are fully se-    25% and prompt-only optimization by up to 50%.\nquential and cannot leverage the cost and computational ef-\nficiency benefits of batch exploration. Full PSST attains the\n                                                       Conclusions and Future Workbest final performance across four settings, while PSST+KX\ntypically reaches strong policies faster, matching or exceed-    We present an inference-aware prompt optimization (IAPO)\ning PSST on three of the four real-data tasks when the bud-      framework for aligning black-box LLMs, emphasizing that\nget is small. Under aggressive pruning (small K), however,      prompts and deployment-time inference scaling strategy are\nthe heuristic becomes suboptimal—most notably on summa-       tightly coupled and should be optimized jointly. Our pro-\nrization and on the synthetic benchmarks—suggesting that      posed PSST and Top-K Screening heuristic demonstrate con-\nPSST+KX  is attractive under tight budgets, whereas full       sistent improvements over strong baselines across six dif-\nPSST is preferable for critical tasks such as long-horizon,       ferent settings. Looking ahead, we plan to explore richer\nhigh-frequency deployment. Finally, the statistical test also       inference-scaling policies (e.g., adaptive BoN/MV sched-\nvalidates that PSST, along with Top-K screening, signifi-       ules, stopping rules, and tree search). We also aim to ex-\ncantly outperforms baselines in all six datasets and under      tend the framework to multi-objective alignment with ex-\nnearly all budgets. These findings indicate that our approach        plicit cost/latency constraints and to study long-horizon de-\nreliably discovers well-aligned solutions using as few as 5K      ployments under distribution shift.\n\nAcknowledgments                       Krishna, K.; Chang, Y.; Wieting, J.; and Iyyer, M. 2022.\nThis research was supported in part by the U.S. Army      Rankgen: Improving  text generation with large ranking\nDEVCOM Analysis Center (DAC) under contract num-      models. arXiv preprint arXiv:2205.09726.\nber W911QX23D0009, by the National Science Foundation     Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu,\ngrants 2205153, 2321786, and 2416460, and by Schmidt      C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Ef-\nSciences under the AI Safety Science program.                      ficient Memory Management for Large Language Model\n                                                            Serving with PagedAttention.  In Proceedings of the ACM\n                References                     SIGOPS 29th Symposium on Operating Systems Principles.\nBai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; Das-      Lambert, N. 2025.  Reinforcement learning from human\nSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.;      feedback. arXiv preprint arXiv:2504.12501.\net al. 2022. Training a helpful and harmless assistant with re-     Meta AI. 2024.   The Llama 3 Model Family: A Path\ninforcement learning from human feedback. arXiv preprint       to Openly Accessible Frontier Models.   arXiv preprint\narXiv:2204.05862.                                           arXiv:2404.11225.\nChang, K.; Xu, S.; Wang, C.; Luo, Y.; Liu, X.; Xiao, T.; and                                                         Minaee, S.; Mikolov, T.; Nikzad, N.; Chenaghlu, M.; Socher,\nZhu, J. 2024.  Efficient prompting methods for large lan-                                                                   R.; Amatriain, X.; and Gao, J. 2024. Large language models:\nguage models: A survey. arXiv preprint arXiv:2404.01077.                                     A survey. arXiv preprint arXiv:2402.06196.\nCheng, J.; Liu, X.; Zheng, K.; Ke, P.; Wang, H.; Dong, Y.;                                                      OpenAI. 2024.  Learning to reason with LLMs.   https:\nTang, J.; and Huang, M. 2023. Black-box prompt optimiza-                                                                 //openai.com/index/learning-to-reason-with-llms/. OpenAI\ntion: Aligning large language models without model train-                                                             Blog.\ning. arXiv preprint arXiv:2311.04155.\n                                                       Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nChow, Y.; Tennenholtz, G.; Gur,  I.; Zhuang, V.; Dai, B.;                                                           Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\nKumar, A.; Agarwal, R.; Thiagarajan, S.; Boutilier, C.; and                                                                          et al. 2022. Training language models to follow instructions\nFaust, A. 2025.  Inference-aware fine-tuning for best-of-N                                                            with human feedback. Advances in neural information pro-\nsampling in large language models. In The Thirteenth Inter-                                                               cessing systems, 35: 27730–27744.\nnational Conference on Learning Representations.\n                                                                 Sessa, P. G.; Dadashi, R.; Hussenot, L.; Ferret, J.; Vieillard,\nEven-Dar, E.; Mannor, S.; and Mansour, Y. 2006.  Ac-\n                                                                N.; Ram´e, A.; Shariari, B.; Perrin, S.; Friesen, A.; Cideron,\ntion elimination and stopping conditions for the multi-armed\n                                                                G.; et al. 2024. BOND: Aligning LLMs with best-of-n dis-\nbandit and reinforcement learning problems. Journal of Ma-\n                                                                               tillation. arXiv preprint arXiv:2407.14622.\nchine Learning Research, 7(39): 1079–1105.\n                                                                   Shi, C.; Yang, K.; Chen, Z.; Li, J.; Yang, J.; and Shen, C.Fabiano, N.; and Cazenave, T. 2021. Sequential halving us-\n                                                           2024. Efficient prompt optimization through the lens of besting scores.  In Advances in Computer Games: 17th Inter-\n                                                   arm identification. Advances in Neural Information Process-national Conference, ACG 2021, Virtual Event, November\n                                                               ing Systems, 37: 99646–99685.23–25, 2021, Revised Selected Papers, 41–52. Berlin, Hei-\ndelberg: Springer-Verlag. ISBN 978-3-031-11487-8.             Stiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D. M.; Lowe, R.;\n                                                               Voss, C.; Radford, A.; Amodei, D.; and Christiano, P. 2020.Grattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian,\n                                                           Learning to summarize from human feedback. In NeurIPS.A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.;\nVaughan, A.; et al. 2024. The Llama 3 herd of models. arXiv      Talmor,  A.;  Herzig,   J.;  Lourie,  N.;  and  Berant,   J.\npreprint arXiv:2407.21783.                                   2018.   Commonsenseqa: A  question answering  chal-\n                                                              lenge targeting commonsense knowledge.  arXiv preprintGui, L.; Gˆarbacea, C.; and Veitch, V. 2024. Bonbon align-\n                                                            arXiv:1811.00937.ment for large language models and the sweetness of best-\nof-n sampling. Advances in Neural Information Processing       Trivedi, P.; Chakraborty, S.; Reddy, A.; Aggarwal, V.; Bedi,\nSystems, 37: 2851–2885.                                  A. S.; and Atia, G. K. 2025.  Align-Pro: A principled ap-\nHendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart,      proach to prompt optimization for LLM alignment. In Pro-\nS.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring      ceedings of the AAAI Conference on Artificial Intelligence,\nmathematical problem solving with the math dataset. arXiv     volume 39, 27653–27661.\npreprint arXiv:2103.03874.                              Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nHuang,  J. Y.; Sengupta,  S.; Bonadiman, D.; Lai,  Y.-a.;       S.; Chowdhery, A.; and Zhou, D. 2022.  Self-consistency\nGupta, A.; Pappas, N.; Mansour, S.; Kirchhoff, K.; and Roth,      improves chain of thought reasoning in language models.\nD. 2024. Deal: Decoding-time alignment for large language      arXiv preprint arXiv:2203.11171.\nmodels. arXiv preprint arXiv:2402.06147.                    Xu, Y.; Sehwag, U. M.; Koppel, A.; Zhu, S.; An, B.; Huang,\nJafari, Y.; Mekala, D.; Yu, R.; and Berg-Kirkpatrick, T.        F.; and Ganesh, S. 2024. GenARM: Reward guided gener-\n2024.  MORL-Prompt: An empirical analysis of multi-       ation with autoregressive reward model for test-time align-\nobjective reinforcement learning for discrete prompt opti-      ment. arXiv preprint arXiv:2410.08193.\nmization. arXiv preprint arXiv:2402.11711.                   Yang, R.; Pan, X.; Luo, F.; Qiu, S.; Zhong, H.; Yu, D.; and\nKarnin, Z. S.; Koren, T.; and Somekh, O. 2013. Almost op-      Chen, J. 2024.  Rewards-in-context: Multi-objective align-\ntimal exploration in multi-armed bandits.  In International     ment of foundation models with dynamic preference adjust-\nConference on Machine Learning.                             ment. arXiv preprint arXiv:2402.10207.\n\nYue, Y.; Chen, Z.; Lu, R.; Zhao, A.; Wang, Z.; Song, S.; and\nHuang, G. 2025. Does reinforcement learning really incen-       E[Nr] = X  Pr ˆQα,(r)c,a1 < ˆQα,(r)c,ai\ntivize reasoning capacity in LLMs beyond the base model?\n                                                                             ai∈A(r)carXiv preprint arXiv:2504.13837.\nZhao, G.; Yoon, B.-J.; Park, G.; Jha, S.; Yoo, S.; and Qian,        ≤ X  exp −1              tri)∆2c,ai                                                                             2harmonic(tr1,\nX. 2025.  Pareto prompt optimization.  In The Thirteenth                  ai∈A(r)c\nInternational Conference on Learning Representations.\n                                       ≤ X  exp −∆2c,ai · 2|P| Ni¯ Tlog2 |A|Zhou, D.; Sch¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; et al.                  ai∈A(r)c\n2022.      Least-to-most                   prompting                              enables complex                                                       max                                              reasoning           ≤|A(r)c   |                                                                        exp −∆2c,ai · 2|P| Nmax¯  T log2 |A|                        arXiv preprintin large       language models.                                     arXiv:2205.10625.                                                                                      i∈A(r)                                                                                                             c\n                                                    ≤|A(r)c   | exp − 2|P|HcT 1R\n              Appendix A                                                          For the best arm to be eliminated in round r, it must hold\nProof of Theorem 1                                               that Nr ≥12 |A(r)c   |.\n                                                           h       1     i                               T\nTheorem 2 (Error of PSST). Let R = ⌈log2 |A|⌉be the     Pr Nr > 2|A(r)c   | ≤2 E[Nr]/|A(r)c   | ≤2 exp − 2|P|Hc1R\nnumber of trimming rounds, and [omink   , omaxk    ] = [−1, 1] and                                                       and the lemma follows.define the cost–gap complexity\n                                         Lemma 2. The probability that the best arm under context\n                     ¯Nmax                                 c is eliminated from context c on round r is at most      Hc1 =  max             ,  H1 = max Hc1.\n                   (c,ai)̸=ac1 ∆2c,ai              c                                               T\n                                                                     3 exp − 8|C|Hc2R\n                                                                Proof. The proof follows  directly (Karnin, Koren, and                             i¯Nmax\n      Hc2 =  max              ,  H2 = max Hc2.           Somekh 2013) Lemma 4.3. The only thing to recognize is\n                  (c,ai)̸=ac1 ∆2c,ai               c                         that:\n                                                          E[Nr] = X  Pr ˆQα,(r)c,a1 < ˆQα,(r)c,ai  where, ∆c,ai = Qαc,a1 −Qαc,ai, arms are indexed based\n\n                                                                                                          con ascending order of Qαc,a under that context and ¯Nmax =                     ai∈A(r)\na1(N)+Nmax\n      2         . Running PSST with the structure-aware al-         ≤ X  exp −∆2c,ai · 8|C||A|2rTNi¯ log2 |A|\nlocation of for a total prompt complication T returns the                                                                                ai∈A(r)c\noptimal arm in every context with probability at least\n\n       1 −3|C|R exp −         T                .                           min(2|P|H1,8|C|H2)R\n                                                            Proof of Theorem 1. The best arm needs to survive for all\nEquivalently, to ensure failure probability at most δ it suf-    R rounds and under all contexts C. Therefore, from the\nfices to choose                                  Lemma 1:\n                                              R\n                                                                                                     T                                                               ≤3|C|R exp −                                                                                                       2|P|H1R     T = O min(|P|H1, |C|H2)R log |C|Rδ       .    X X 2 exp − 2|P|HcT 1R\n                                                          r=1  c\nLemma 1. The probability that the best arm under context     From the Lemma 2:\nc is eliminated from context c on round r is at most            R\n                   X X 3 exp − 8|C|HcT 2R  ≤3|C|R exp − 8|C|H2RT                             T\n                2 exp − 2|P|Hc1R                         r=1  c\n                                                     Combining both:\nProof. Assume that the best arm was not eliminated before                                    T                                                                 3|C|R exp −\nround r. Then due to Hoeffding’s inequality for any arm                                min(2|P|H1,8|C|H2)R\nai ∈A(r)c   ,                                              which gives the theorem.\n\n                                                           Proposition  2  (Inference-Agnostic  Optimality).  The\nPr ˆQα,(r)c,a1 < ˆQα,(r)c,ai  ≤exp −1                 tri) ∆2c,ai    .   Inference-Agnostic  prompt-optimization  policy  remains                            2harmonic(tr1,\n                                                             optimal under linear transformation of RIAx (c, a), that is,\n                                                   kRIAx (c, a), k > 0 and an optimal policy can be recoveredHere, tr is the number of samples that were used to estimate\n                                                                            trivially from Q-function under affine transformation:\nthe Q value. Letting Nr denote the number of arms in A(r)c\n                                          QAF (c, a) := Ex∼X aRIAx  (c, a) + b = kQIA(c, a) + b.whose empirical average is larger than that of the optimal\narm, we have:                                                   Proof. Follows directly from Jensen’s inequality.\n\nAppendix B                   Llama-3.3-70B-Instruct at temperature T = 0.7,\n                                                               parsing each completion to its final integer answer.Synthetic-Bernoulli Environment.  We consider a setting\n                                                     The dataset is then processed as follows:with P = 32 prompts, each evaluated over a hidden mixture\nof query difficulty tiers—{easy, medium, hard}—spanning       1. For each problem, retain the global top-4 answers and\n|X| = 520 queries, with proportions 6  : 4  : 3. For each         group all other answers into a single OTHER bucket (C =\nprompt p and query x, the single-shot success probability is        5 categories in total).\ndenoted qp(x) ∈[0, 1].                                               2. Compute per-prompt costs as the normalized average to-\n A pull of N ≤Nmax for prompt p on example x gener-        ken length of its responses.\nates i.i.d. Bernoulli outcomes {ci}Ni=1 where Pr(ci = 1) =                                                              This yields a categorical environment (analogous to theqp(x), and each completion incurs a per-sample cost kp. The\n                 N                                     Synthetic-Categorical setting) with P = 25, Nmax = 32, a\nresult is an array   ci, kp  i=1.                               uniform context prior c ∈{low, mid, high}, and cost coef-\n  Majority vote (MV) sets M = 1 if Pi ci > N/2, M = 0        ficients {0, 0.2, 1.0}. Utility is evaluated via majority vote.\nif Pi ci < N/2, and assigns M = 0.5 (by fair coin) in the\n                                             CommonsenseQA  Environment.  We  randomly  sam-case of a tie (N even, Pi ci = N/2).                                                                 ple 1,500 multiple-choice questions from the Common-  The utility for cost for context c ∈{low, mid, high} is\n                                                    senseQA corpus3, and author 48 prompt templates usingcomputed as\n                                                       ChatGPT-o3. For each (prompt, question) pair, we query\n                        N                  Llama-3.3-70B-Instruct at temperature T = 1.1,\n              uc = w1M + w2(c) X kp,                       collecting 128 JSON-constrained answers (one of “Option\n                                  i=1                      A”–“Option E”). Each prompt is assigned a constant cost\n                                                         kp = 0.01.\nwhere w1 = 1 and w2(c) ∈{0, −0.2, −1.0} depending on                                                     The resulting data is used to construct a categorical en-\nthe cost tier.                                                          vironment (in analogy to the Synthetic-Categorical setting)\n  To instantiate the environment, we generate two prompt                                                            with P = 48, Nmax = 32, a uniform context prior, and cost\narchetypes: deceiving prompts, which achieve high aver-                                                                      coefficients {0, 0.2, 1.0}.\nage accuracy but exhibit low qp(x) on hard queries, and\nall-rounders, which maintain moderate accuracy more uni-      Helpful–Harmless  Environment.  We  filter  the HH-\nformly across tiers. Per-prompt costs kp are sampled from a    RLHF conversations4 to the 1,355 examples containing\nnormal distribution with mean 0.02 and variance 0.005.          a single user query and a single assistant response. Us-\n                                                               ing ChatGPT-o3, we craft 20 prompt templates. For each\nSynthetic-Categorical Environment.  We model P = 32                                                            (prompt, query) pair, we sample 128 continuations from\nprompts, each paired with |X| = 512 queries and K = 2     Llama-3.3-70B-Instruct at temperature T = 0.7.\npositive objectives. For every (p, x), there are M categorical                                                     Each continuation is scored by separate public reward mod-\noutcomes, each represented by a vector oj ∈RK. A pull       els (Yang et al. 2024) for helpfulness5 and harmlessness6,\nof N ≤Nmax(= 32) for prompt p on query x generates      with scores normalized to [−1, 1].\nN i.i.d. outcome vectors, resulting in rows [ oi,1, oi,2, kp ],       The two reward  scores  are  then binned on a  0.5-\nwhere kp denotes the per-completion cost for prompt p.          spaced  grid,  producing  a  categorical  distribution  per\n  Given a context c with weights w = (w1, w2, wcost),      (prompt, query); per-prompt costs are computed as the av-\nwhere w1 + w2 = 1 and wcost ≤0, the Best-of-N utility      erage token length. This data defines a categorical environ-\nis defined as                                           ment with P = 20, Nmax = 32, a uniform context prior\n                                                             over weight triples (wh, ws, wcost) with wh + ws = 1 and      uc = max (w1oi,1 + w2oi,2) + wcost N kp.\n          1≤i≤N                                                   wcost ∈{−0.1, −0.5, −1.0}.\n\n  To  construct  the  environment, outcome  vectors  are     Summarization  Environment.  We  randomly  sample\nsampled from {−4, . . . , 4}2. We instantiate two prompt      1,201 Reddit posts from the Summarize-from-Feedback cor-\narchetypes: HMLV (high mean, low variance; excels  at      pus7 and design 20 summarization prompt templates us-\nN=1) and LMHV (lower mean, high variance; benefits      ing ChatGPT-o3. For each (prompt, post) pair, we query\nfrom larger N), each specializing in one objective. For     Llama-3.3-70B-Instruct at temperature T = 0.7\neach (p, x), we add small per-query noise to the categori-      and collect 128 candidate summaries.\ncal outcome probabilities, introduce a mild train-to-test shift       Each summary is scored by two publicly available reward\nby perturbing these probabilities, sample per-prompt costs      models: Preference8 and Faithful9, with raw scores normal-\nkp ∈[0.02, 0.1], and draw context weights from a grid sat-\n                                                                          3https://huggingface.co/datasets/tau/commonsense qaisfying w1 + w2 = 1 with wcost ∈{−0.1, −0.5, −1.0}.\n                                                                             4https://huggingface.co/datasets/Anthropic/hh-rlhf\nMATH  Environment.  We  select  316  integer-answer          5Ray2333/gpt2-large-helpful-reward model\nproblems from the MATH dataset2. A set of 25 prompt          6Ray2333/gpt2-large-harmless-reward model\ntemplates   is  authored  using  ChatGPT-o3.  For  each           7https://huggingface.co/datasets/openai/summarize from\n(prompt, problem)  pair, we sample 128 responses from       feedback\n                                                                       8OpenAssistant/reward-model-deberta-v3-large-v2\n   2https://huggingface.co/datasets/HuggingFaceH4/MATH-500           9CogComp/bart-faithful-summary-detector\n\nized to [−1, 1]. We then bin each dimension in steps of 0.5,       for Benjamini–Hochberg FDR or no correction). We de-\nproducing a categorical distribution over the two reward di-       clare a winner  if the adjusted p < α =  0.05; the di-\nmensions, and compute per-prompt costs from average to-       rection is determined by the sign of the median difference\nken length.                                           median(x −y). In case of unequal environment counts\n  This data defines a categorical environment with P =       across algorithms, samples are truncated to the minimum\n20, Nmax = 32, and a uniform context prior over weight       length to preserve pairing. Figures visualize the outcome\ntriples (wh, ws, wcost) where wh + ws = 1 and wcost ∈      matrix with entries in {−1, 0, +1} indicating row-algorithm\n{−0.1, −0.5, −1.0}.                                                  loss, non-significance, or win against the column algorithm,\n  Note: All prompts are available under the prompts folder       respectively.\nof the code base.                                                 All the results are shown in Figs 5, 6, 7, 8, 9, 10. Across\n                                                                                all six datasets, we observe that PSST and the Top-K screen-\n              Appendix C                           ing heuristic consistently outperform competing methods\n                                                                across most budget settings, with statistical significance.\nTop-K screening.  For the screening variant, we fixed\nK = 4 candidates after screening and swept the burn-in\nfraction ρ ∈{0.05, 0.10, 0.20, 0.30, 0.40}, which allocates\na ρ-portion of the budget to obtain initial estimates before\ntrimming. Parameter sweep protocol matched the baselines.\nWe selected ρ = 0.20 for reporting, as it achieved the best\noverall performance while remaining robust across datasets\nand inference regimes 2.\n\nUCB.  We  tuned  the  exploration  constant  over  c  ∈\n{0.1, 0.5, 1.0, 2.0, 4.0, 8.0} under the same budgets and us-\ning 20% of the data per environment; identical seeds across\nsettings; 10,000 test contexts). The agent ranks arms by the\nstandard UCB index\n\n               s                                        ln t\n          UCBi(t) =  ˆµi(t) + c   ni(t),\n\nwhere ˆµi(t) is the empirical mean utility of arm i, ni(t) its\npull count, and t the total pulls. We selected c = 0.1 for\nreporting, as it achieved the best overall performance while\nremaining robust across datasets and inference regimes 3.\n\nϵ-greedy.  We        swept        ε          ∈\n{0.50, 0.75, 0.80, 0.85, 0.90, 0.95}      separately      for\neach   dataset  and   inference  regime  (MV,  BON).\nFor  every   ε,  agents  were   trained  under  budgets\nT ∈{3K, 5K, 10K, 20K, 30K, 40K}, using 20% of the\ndata per environment with deterministic reseeding; evalua-\ntion used 10,000 test contexts per environment. We selected\nϵ =  0.15 for reporting, as  it achieved the best overall\nperformance while remaining robust across datasets and\ninference regimes 4.\n\n              Appendix D\nStatistical testing.  For each dataset and budget T, we\nperform  all pairwise algorithm comparisons using  per-\nenvironment utilities as paired samples (identical train/test\nsplits via deterministic reseeding). Our default test is the\ntwo-sided Wilcoxon signed-rank test, which we apply to the\naligned vectors after removing non-finite values and drop-\nping exact ties (zero method=wilcox, mode=auto);\npairs with fewer than two effective samples are skipped.\nWhen requested, we also report the paired sign test (bino-\nmial test on the sign of differences) after removing ties.\nTo control multiplicity within each (dataset, T) grid, we\nuse Holm–Bonferroni adjustment by default (with options\n\nParam × T         HH      Summarization     SC        SB      MATH     CQA\n\n           ρ=0.05, T = 3000    0.40 ± 0.00     0.20 ± 0.00     2.77 ± 0.02   0.83 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\n           ρ=0.05, T = 5000    0.40 ± 0.00     0.22 ± 0.00     2.83 ± 0.02   0.83 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\n           ρ=0.05, T = 10000   0.42 ± 0.00     0.21 ± 0.00     2.83 ± 0.02   0.85 ± 0.01   0.81 ± 0.01   0.76 ± 0.00\n           ρ=0.05, T = 20000   0.43 ± 0.00     0.22 ± 0.00     2.87 ± 0.02   0.84 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\n           ρ=0.05, T = 30000   0.44 ± 0.00     0.23 ± 0.00     2.88 ± 0.01   0.84 ± 0.00   0.82 ± 0.01   0.77 ± 0.00\n           ρ=0.05, T = 40000   0.43 ± 0.00     0.23 ± 0.00     2.87 ± 0.02   0.84 ± 0.00   0.81 ± 0.00   0.77 ± 0.00\n           ρ=0.10, T = 3000    0.41 ± 0.00     0.21 ± 0.00     2.79 ± 0.02   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\n           ρ=0.10, T = 5000    0.41 ± 0.00     0.22 ± 0.00     2.84 ± 0.02   0.84 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\n           ρ=0.10, T = 10000   0.42 ± 0.00     0.21 ± 0.00     2.86 ± 0.02   0.84 ± 0.00   0.81 ± 0.01   0.77 ± 0.00\n           ρ=0.10, T = 20000   0.43 ± 0.00     0.23 ± 0.00     2.88 ± 0.02   0.84 ± 0.00   0.81 ± 0.01   0.77 ± 0.00\n           ρ=0.10, T = 30000   0.44 ± 0.00     0.23 ± 0.00     2.89 ± 0.02   0.84 ± 0.00   0.81 ± 0.01   0.77 ± 0.01\n           ρ=0.10, T = 40000   0.44 ± 0.00     0.23 ± 0.00     2.88 ± 0.02   0.84 ± 0.00   0.82 ± 0.00   0.77 ± 0.00\n           ρ=0.20, T = 3000    0.41 ± 0.00     0.20 ± 0.00     2.77 ± 0.02   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\n           ρ=0.20, T = 5000    0.41 ± 0.00     0.22 ± 0.00     2.84 ± 0.02   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\n           ρ=0.20, T = 10000   0.43 ± 0.00     0.22 ± 0.00     2.85 ± 0.02   0.83 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\n           ρ=0.20, T = 20000   0.43 ± 0.00     0.23 ± 0.00     2.87 ± 0.01   0.84 ± 0.00   0.81 ± 0.01   0.77 ± 0.00\n           ρ=0.20, T = 30000   0.44 ± 0.00     0.23 ± 0.00     2.89 ± 0.02   0.84 ± 0.00   0.82 ± 0.01   0.76 ± 0.00\n           ρ=0.20, T = 40000   0.44 ± 0.00     0.22 ± 0.00     2.88 ± 0.02   0.84 ± 0.00   0.82 ± 0.01   0.77 ± 0.00\n           ρ=0.30, T = 3000    0.41 ± 0.00     0.21 ± 0.00     2.81 ± 0.02   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\n           ρ=0.30, T = 5000    0.41 ± 0.00     0.22 ± 0.00     2.85 ± 0.01   0.83 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\n           ρ=0.30, T = 10000   0.42 ± 0.00     0.22 ± 0.00     2.85 ± 0.02   0.84 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\n           ρ=0.30, T = 20000   0.43 ± 0.00     0.22 ± 0.00     2.88 ± 0.01   0.83 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\n           ρ=0.30, T = 30000   0.44 ± 0.00     0.22 ± 0.00     2.88 ± 0.01   0.84 ± 0.00   0.82 ± 0.01   0.76 ± 0.00\n           ρ=0.30, T = 40000   0.44 ± 0.00     0.23 ± 0.00     2.89 ± 0.01   0.84 ± 0.00   0.82 ± 0.01   0.77 ± 0.00\n           ρ=0.40, T = 3000    0.41 ± 0.00     0.20 ± 0.00     2.80 ± 0.01   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\n           ρ=0.40, T = 5000    0.42 ± 0.00     0.20 ± 0.00     2.80 ± 0.02   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\n           ρ=0.40, T = 10000   0.43 ± 0.00     0.22 ± 0.00     2.85 ± 0.01   0.83 ± 0.00   0.81 ± 0.01   0.77 ± 0.00\n           ρ=0.40, T = 20000   0.43 ± 0.00     0.22 ± 0.00     2.87 ± 0.02   0.83 ± 0.00   0.81 ± 0.01   0.77 ± 0.00\n           ρ=0.40, T = 30000   0.44 ± 0.00     0.22 ± 0.00     2.88 ± 0.01   0.83 ± 0.00   0.81 ± 0.01   0.77 ± 0.00\n           ρ=0.40, T = 40000   0.44 ± 0.00     0.23 ± 0.00     2.89 ± 0.01   0.84 ± 0.00   0.82 ± 0.01   0.77 ± 0.00\n\n                       Table 2: PSST+K4: mean ± SEM across datasets (rows are param, ρ and T).\n\n\n\n\n\nFigure 5: Pairwise wins for Commonsense QA (MV) across six budgets (T in order: 3000, 5000, 10000, 20000, 30000, 40000).\n\nParam × T        HH      Summarization     SC        SB      MATH     CQA\n\nc=0.1, T = 3000    0.38 ± 0.00     0.19 ± 0.01     2.83 ± 0.02   0.82 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\nc=0.1, T = 5000    0.39 ± 0.00     0.21 ± 0.00     2.88 ± 0.01   0.83 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\nc=0.1, T = 10000   0.41 ± 0.00     0.23 ± 0.00     2.94 ± 0.01   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\nc=0.1, T = 20000   0.43 ± 0.00     0.25 ± 0.00     2.98 ± 0.01   0.86 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\nc=0.1, T = 30000   0.43 ± 0.00     0.25 ± 0.00     2.99 ± 0.01   0.88 ± 0.00   0.81 ± 0.01   0.76 ± 0.01\nc=0.1, T = 40000   0.44 ± 0.00     0.25 ± 0.00     3.00 ± 0.01   0.89 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\nc=0.5, T = 3000    0.37 ± 0.00     0.19 ± 0.01     2.85 ± 0.02   0.82 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\nc=0.5, T = 5000    0.38 ± 0.00     0.20 ± 0.00     2.90 ± 0.01   0.82 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\nc=0.5, T = 10000   0.41 ± 0.00     0.23 ± 0.00     2.94 ± 0.01   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\nc=0.5, T = 20000   0.43 ± 0.00     0.24 ± 0.00     2.98 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\nc=0.5, T = 30000   0.43 ± 0.00     0.24 ± 0.00     3.00 ± 0.01   0.86 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\nc=0.5, T = 40000   0.44 ± 0.00     0.25 ± 0.00     3.00 ± 0.01   0.88 ± 0.00   0.81 ± 0.01   0.75 ± 0.00\nc=1.0, T = 3000    0.37 ± 0.00     0.19 ± 0.01     2.88 ± 0.02   0.82 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\nc=1.0, T = 5000    0.37 ± 0.00     0.19 ± 0.01     2.91 ± 0.02   0.83 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\nc=1.0, T = 10000   0.41 ± 0.00     0.22 ± 0.00     2.94 ± 0.01   0.83 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\nc=1.0, T = 20000   0.42 ± 0.00     0.24 ± 0.00     2.98 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\nc=1.0, T = 30000   0.43 ± 0.00     0.24 ± 0.00     3.00 ± 0.01   0.86 ± 0.01   0.81 ± 0.01   0.76 ± 0.00\nc=1.0, T = 40000   0.43 ± 0.00     0.25 ± 0.00     3.00 ± 0.01   0.87 ± 0.00   0.81 ± 0.01   0.76 ± 0.01\nc=2.0, T = 3000    0.37 ± 0.00     0.18 ± 0.01     2.86 ± 0.02   0.82 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\nc=2.0, T = 5000    0.38 ± 0.00     0.19 ± 0.01     2.93 ± 0.01   0.83 ± 0.00   0.79 ± 0.01   0.76 ± 0.00\nc=2.0, T = 10000   0.40 ± 0.00     0.23 ± 0.00     2.94 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\nc=2.0, T = 20000   0.42 ± 0.00     0.24 ± 0.00     2.98 ± 0.01   0.85 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\nc=2.0, T = 30000   0.42 ± 0.00     0.24 ± 0.00     2.99 ± 0.01   0.86 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\nc=2.0, T = 40000   0.43 ± 0.00     0.25 ± 0.00     2.99 ± 0.01   0.88 ± 0.00   0.81 ± 0.01   0.75 ± 0.00\nc=4.0, T = 3000    0.37 ± 0.00     0.18 ± 0.01     2.85 ± 0.02   0.82 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\nc=4.0, T = 5000    0.37 ± 0.00     0.18 ± 0.00     2.91 ± 0.01   0.83 ± 0.00   0.79 ± 0.01   0.76 ± 0.00\nc=4.0, T = 10000   0.41 ± 0.00     0.22 ± 0.00     2.94 ± 0.01   0.84 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\nc=4.0, T = 20000   0.42 ± 0.00     0.24 ± 0.00     2.97 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\nc=4.0, T = 30000   0.42 ± 0.00     0.24 ± 0.00     2.99 ± 0.01   0.86 ± 0.00   0.81 ± 0.01   0.75 ± 0.00\nc=4.0, T = 40000   0.43 ± 0.00     0.25 ± 0.00     3.00 ± 0.01   0.87 ± 0.00   0.81 ± 0.01   0.76 ± 0.00\nc=8.0, T = 3000    0.37 ± 0.00     0.18 ± 0.01     2.86 ± 0.02   0.82 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\nc=8.0, T = 5000    0.38 ± 0.00     0.19 ± 0.01     2.90 ± 0.02   0.83 ± 0.00   0.79 ± 0.01   0.76 ± 0.00\nc=8.0, T = 10000   0.40 ± 0.00     0.22 ± 0.00     2.92 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\nc=8.0, T = 20000   0.42 ± 0.00     0.24 ± 0.00     2.97 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\nc=8.0, T = 30000   0.43 ± 0.00     0.24 ± 0.00     2.98 ± 0.01   0.86 ± 0.00   0.81 ± 0.01   0.75 ± 0.00\nc=8.0, T = 40000   0.43 ± 0.00     0.25 ± 0.00     2.99 ± 0.01   0.87 ± 0.00   0.81 ± 0.01   0.76 ± 0.01\n\n                 Table 3: UCB: mean ± SEM across datasets (rows are param, T).\n\nParam × T         HH      Summarization     SC        SB      MATH     CQA\n\ne=0.50, T = 3000    0.37 ± 0.00     0.17 ± 0.01     2.78 ± 0.02   0.83 ± 0.00   0.79 ± 0.01   0.76 ± 0.00\ne=0.50, T = 5000    0.39 ± 0.00     0.20 ± 0.01     2.82 ± 0.01   0.83 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\ne=0.50, T = 10000   0.41 ± 0.00     0.21 ± 0.00     2.90 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.50, T = 20000   0.42 ± 0.00     0.23 ± 0.00     2.94 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.74 ± 0.00\ne=0.50, T = 30000   0.43 ± 0.00     0.24 ± 0.00     2.97 ± 0.01   0.85 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\ne=0.50, T = 40000   0.43 ± 0.00     0.25 ± 0.00     2.98 ± 0.01   0.85 ± 0.00   0.80 ± 0.01   0.75 ± 0.01\ne=0.55, T = 3000    0.38 ± 0.00     0.16 ± 0.01     2.75 ± 0.03   0.83 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.75, T = 5000    0.39 ± 0.00     0.17 ± 0.01     2.86 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.76 ± 0.00\ne=0.75, T = 10000   0.40 ± 0.00     0.20 ± 0.01     2.91 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.76 ± 0.00\ne=0.75, T = 20000   0.42 ± 0.00     0.23 ± 0.00     2.95 ± 0.01   0.84 ± 0.00   0.79 ± 0.00   0.74 ± 0.00\ne=0.75, T = 30000   0.43 ± 0.00     0.23 ± 0.00     2.97 ± 0.01   0.85 ± 0.00   0.81 ± 0.01   0.75 ± 0.00\ne=0.75, T = 40000   0.43 ± 0.00     0.24 ± 0.00     2.96 ± 0.01   0.85 ± 0.00   0.80 ± 0.01   0.74 ± 0.00\ne=0.80, T = 3000    0.38 ± 0.00     0.18 ± 0.01     2.83 ± 0.02   0.83 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.80, T = 5000    0.39 ± 0.00     0.19 ± 0.00     2.86 ± 0.02   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\ne=0.80, T = 10000   0.40 ± 0.00     0.19 ± 0.01     2.91 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\ne=0.80, T = 20000   0.42 ± 0.00     0.23 ± 0.00     2.94 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.80, T = 30000   0.41 ± 0.00     0.23 ± 0.00     2.96 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.80, T = 40000   0.43 ± 0.00     0.24 ± 0.00     2.98 ± 0.01   0.85 ± 0.00   0.80 ± 0.01   0.74 ± 0.00\ne=0.85, T = 3000    0.38 ± 0.00     0.16 ± 0.01     2.72 ± 0.04   0.83 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\ne=0.85, T = 5000    0.38 ± 0.00     0.17 ± 0.01     2.87 ± 0.01   0.83 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\ne=0.85, T = 10000   0.40 ± 0.00     0.20 ± 0.00     2.90 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\ne=0.85, T = 20000   0.41 ± 0.00     0.22 ± 0.00     2.95 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.85, T = 30000   0.42 ± 0.00     0.23 ± 0.01     2.95 ± 0.01   0.85 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.85, T = 40000   0.42 ± 0.00     0.24 ± 0.00     2.97 ± 0.01   0.85 ± 0.00   0.80 ± 0.01   0.75 ± 0.01\ne=0.90, T = 3000    0.37 ± 0.00     0.17 ± 0.01     2.81 ± 0.03   0.82 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\ne=0.90, T = 5000    0.38 ± 0.00     0.17 ± 0.01     2.87 ± 0.02   0.83 ± 0.00   0.79 ± 0.01   0.76 ± 0.00\ne=0.90, T = 10000   0.40 ± 0.00     0.19 ± 0.01     2.90 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\ne=0.90, T = 20000   0.41 ± 0.00     0.22 ± 0.00     2.94 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.90, T = 30000   0.42 ± 0.00     0.23 ± 0.00     2.95 ± 0.01   0.85 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.90, T = 40000   0.42 ± 0.00     0.24 ± 0.00     2.97 ± 0.01   0.85 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\ne=0.95, T = 3000    0.37 ± 0.00     0.17 ± 0.01     2.76 ± 0.03   0.82 ± 0.00   0.78 ± 0.01   0.75 ± 0.00\ne=0.95, T = 5000    0.38 ± 0.00     0.17 ± 0.01     2.86 ± 0.01   0.83 ± 0.00   0.79 ± 0.01   0.76 ± 0.00\ne=0.95, T = 10000   0.39 ± 0.00     0.19 ± 0.01     2.92 ± 0.01   0.84 ± 0.00   0.80 ± 0.01   0.76 ± 0.00\ne=0.95, T = 20000   0.41 ± 0.00     0.21 ± 0.00     2.95 ± 0.01   0.84 ± 0.00   0.79 ± 0.01   0.75 ± 0.00\ne=0.95, T = 30000   0.42 ± 0.00     0.22 ± 0.00     2.95 ± 0.01   0.85 ± 0.00   0.80 ± 0.01   0.75 ± 0.00\ne=0.95, T = 40000   0.41 ± 0.00     0.23 ± 0.00     2.95 ± 0.01   0.84 ± 0.00   0.79 ± 0.00   0.75 ± 0.00\n\n                Table 4: ϵ-greedy: mean ± SEM across datasets (rows are param, T).\n\nFigure 6: Pairwise wins for Helpful-Harmless (BoN) across six budgets (T in order: 3000, 5000, 10000, 20000, 30000, 40000).\n\n\n\n\n\n     Figure 7: Pairwise wins for MATH (MV) across six budgets (T in order: 3000, 5000, 10000, 20000, 30000, 40000).\n\nFigure 8: Pairwise wins for Synthetic Bernoulli (MV) across six budgets (T in order: 3000, 5000, 10000, 20000, 30000, 40000).\n\n\n\n\n\nFigure 9: Pairwise wins for Synthetic Categorical (BoN) across six budgets (T in order: 3000, 5000, 10000, 20000, 30000,\n40000).\n\nFigure 10: Pairwise wins for Summarization (BoN) across six budgets (T in order: 3000, 5000, 10000, 20000, 30000, 40000).",
"headers": [
"arXiv:2508.10030v1  [cs.CL]  8 Aug 2025",
"Inference-Aware Prompt Optimization for Aligning",
"Black-Box Large Language Models",
"Saaduddin Mahmud, Mason Nakamura, Kyle H. Wray, Shlomo Zilberstein",
"Related Work",
"Introduction",
"Inference–Aware Prompt Optimization",
"Prompt Scaling via Sequential Trimming",
"Empirical Evaluation",
"Conclusions and Future Work",
"Acknowledgments",
"References",
"Appendix A",
"Appendix B",
"Appendix C",
"Appendix D",
"Problem Formulation",
"I",
"Framework",
"Motivating Case Study",
"Proof of Theorem 1"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2508.10030v1.pdf"
}