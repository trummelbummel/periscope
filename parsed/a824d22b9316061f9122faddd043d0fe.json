{
"text": "Automatic Prompt Optimization with “Gradient Descent”\n                              and Beam Search\n\n\n               Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng\n                                              Microsoft Azure AI\n                {reidpryzant,iterdan,jerrl,yintatlee,chezhu,nzeng}@microsoft.com\n\n\n\n\n                          Abstract\n\n                Large Language Models (LLMs) have shown\n                  impressive performance as general purpose\n                   agents, but their abilities remain highly de-\n                 pendent on prompts which are hand written\n                  with onerous trial-and-error effort. We propose2023           a simple and nonparametric solution to this\n                 problem, Prompt Optimization with Textual\n                 Gradients (ProTeGi), which is inspired by nu-Oct                  merical gradient descent to automatically im-\n19           proveand anprompts,LLM API.assumingThe accessalgorithmto traininguses mini-data\n                  batches of data to form natural language “gra-\n                   dients” that criticize the current prompt, much\n                    like how numerical gradients point in the di-\n                   rection of error ascent. The natural language\n                   gradients are then “propagated” into the prompt[cs.CL]                by editing the prompt in the opposite semantic\n                   direction of the gradient. These gradient de-\n                  scent steps are guided by a beam search and\n                  bandit selection procedure which significantly        Figure 1: Overview of the proposed Prompt Optimiza-\n                improves algorithmic efficiency. Preliminary         tion with Textual Gradients (ProTeGi).\n                     results across three benchmark NLP tasks and\n                   the novel problem of LLM jailbreak detection\n                  suggest that Automatic Prompt Optimization                                                                  Accordingly, there is need for automatic or semi-\n                 can outperform prior prompt editing techniques\n                                                               automatic procedures to help humans write the best\n                and improve an initial prompt’s performance\n                                                               prompts. This would help reduce manual effort, im-               by up to 31%, by using data to rewrite vague\n                   task descriptions into more precise annotation       prove task performance, and produce interpretable\n                     instructions.1                                      descriptions of a cognitive decision process.\n\n                                        A recent body of work has investigated this prob-          1  IntroductionarXiv:2305.03495v2                                                 lem by training auxiliary models or differentiable\n            Large Language Models (LLMs) trained on web-   representations of the prompt (Qin and Eisner,\n             scale text have recently demonstrated unprece-   2021; Deng et al., 2022). However, such works as-\n             dented abilities across a variety of NLP tasks (Ope-   sume access to internal state variables of the LLM\n            nAI, 2023; Bubeck et al., 2023). These LLMs use   (Shin et al., 2020; Lester et al., 2021) while prac-\n           prompt inputs to follow human instructions. Writ-    titioners often communicate with LLMs through\n             ing prompts in natural language remains a manual   an API. Other work applies discrete manipulations\n               trial-and-error process requiring significant human    to prompts via Reinforcement Learning or LLM-\n               effort (Jiang et al., 2022) and expertise (Reynolds   based feedback (Zhang et al., 2023; Zhou et al.,\n           and McDonell, 2021; Zamfirescu-Pereira et al.,   2022). These algorithms may also require low-level\n             2023).                                             access to the LLM, produce incomprehensible out-\n                                                                     puts, or rely on directionless monte-carlo search                1Code and data available  at:  https://github.com/\n              microsoft/LMOps/tree/main/prompt_optimization.       over the semantic space of prompts.\n\nWe propose Prompt Optimization with Textual\nGradients (ProTeGi), a general purpose and non-\nparametric algorithm for automatic prompt opti-\nmization that connects these two bodies of research\nby applying discrete improvements to prompts in a\ndirected way.\n  Unlike prior work, we overcome the discrete\noptimization barrier by mirroring the steps of gra-\ndient descent within a text-based Socratic dialogue\n(Zeng et al., 2022), substituting differentiation with\nLLM feedback and backpropagation with LLM\nediting. In detail, we use minibatches of training\ndata to produce “gradients” in natural language, i.e.,\ndescriptions of the current prompts’ flaws with re-\nspect to the minibatch, then edit the current prompt\nin the opposite semantic direction of the gradient.   Figure 2: The text dialogue tree we use to mimic gra-\nThese steps become the expansion part of a wider    dient descent and overcome the discrete optimization\nbeam search over the space of prompts, increasing     barrier. First, from the top left a feedback prompt ∆\n                                                          generates the gradient g from starting prompt p0 and pre-algorithmic efficiency by treating the problem of\n                                                            diction ˆy. Second, from the top right an editing prompt\nbeam candidate selection as an instance of the best\n                                                     δ applies the gradient g to p0 and produce improved\narm identification problem (Audibert et al., 2010).                                                   prompts p′, their paraphrases p′′, and efficient best can-\n  We then offer a preliminary case study of Pro-    didate selection before the next iteration (bottom left).\nTeGi. We evaluate the proposed framework in mul-\ntiple configurations across 4 NLP tasks, including\nthe novel problem of LLM jailbreak detection. The   m(·) and in-domain test or development data Dte.\nresults suggest that the proposed algorithm can im-      In the following sections, we first introduce how\nprove on the performance of the initial prompt input    the algorithm performs textual “gradient descent”\nby up to 31%, exceeding state-of-the-art prompt    to improve the prompts in a directed way (Section\nlearning baselines by an average of 4-8% while    2.1). Then the algorithm leverages these gradient\nrelying on fewer LLM API calls. We also demon-   descent steps to beam search through the space\nstrate the interpretability of the optimization pro-   of coherent language L, guided by the gradients\ncess and investigate the algorithms’ shortcomings.   during beam expansion, and efficient best arm iden-\n                                                             tification during beam selection (Section 2.2).\n2  Discrete Prompt Optimization with\n                                                    2.1  Gradient descent with Prompts   Nonparametric “Gradient Descent”\n                                                   In our setting, gradient descent refers to the pro-\nThe proposed algorithm assumes access to an ini-                                                     cess of (1) evaluating a prompt with a batch of data,\ntial prompt p0 and i.i.d. training data consisting of                                                      (2) creating a local loss signal which contains in-\npairs of input and output text (numbers, categories,                                                formation on how to improve the current prompt,\nsummaries, etc): Dtr = {(x1, y1), ..., (xn, yn)}.                                                  then (3) editing the prompt in the opposite seman-\nNote that all prompts p are drawn from the space                                                                  tic direction of the gradient before starting the next\nof coherent natural language L.  We assume                                                            iteration.\naccess to a black box LLM API LLMp(x) ≈                                   We accomplish this process with a pair of static\nargmaxy∈LPLLM(y|p, x), which returns a likely                              LLM prompts, as depicted in Figure 2. The first\ntext continuation y of the prompt formed by con-\n                                              prompt is for creating the loss signals (“gradients”)\ncatenating p and x (for example, few-shot prompt\n                                            and is called ∇. While the specific contents can\nand input example, or chatbot persona and conver-                                                  vary and be task-specific or task-agnostic,2 ∇must\nsational history).\n                                              always consider the current prompt p0, plus p0’s\n  Within this context, our algorithm iteratively                                                 behavior on a minibatch of data (particularly the\nrefines  the prompt p0  to produce   ˆp,  an ap-                                                           errors), and generate a natural language summary\nproximation  of  the  optimal  prompt  p∗  =\nargmaxp∈L{m(p, Dte)} for some metric function      2We use the same prompts for all tasks; see Appendix.\n\nof p0’s flaws. This summary becomes the gradient   prompt candidates.\ng. Similar to how traditional gradients represent\n                                                      2.2.1  Expansion Step\na direction in parameter space that would make\n                                         The expansion step is used to generate many newthe model worse, the text “gradients” g represent\n                                                  candidate prompts from a current prompt (Algo-directions in a semantic space that are making the\n                                                 rithm 2). It leverages the conceptual “gradient de-prompt worse.\n                                                   scent” framework of Section 2.1, and our specific  The second prompt is called δ and while this\n                                              prompts can be found in the Appendix.prompt can also vary, it must always take the gradi-\n                                                              First we sample a minibatch of data, run the ini-ent g and current prompt p0, then perform an edit\n                                                                    tial prompt on these data with LLMp0, and collecton p0 in the opposite semantic direction of g, i.e.\n                                                           errors. Second, we plug these errors into a promptfix the problems with p0 that are indicated by g.3\n                                                 template ∆, which instructs the LLM to describe  Unlike the traditional machine learning setting,\n                                                      the problems with p0 which could have led to thesewe do not generate a single gradient or edit, but\n                                                    mistakes. The ensuing generations are our naturalrather a number of directions that may improve\n                                                language gradients; see Figure 1 for an example.the current prompt. Section 2.2 describes in detail\n                                                 Second, the gradients are provided to anotherthe process of generating and selecting candidate\n                              LLM prompt called δ, which instructs the LLMprompts.\n                                                      to edit the current prompt p0 in order to fix the\n2.2  Beam Search over Prompts                problems described by the gradient. In this way,\n                                      we engadge the LLMs in a recursive feedback loop\nThe gradient descent steps described in Section 2.1\n                                                       similar to the Socratic dialogues proposed by Zengare used to guide a beam search over the space of\n                                                            et al. (2022).prompts. This beam search is the outer loop of\n                                                          Last, additional candidates are generated by run-our prompt training algorithm and it is described\n                                                  ning the existing candidates through a paraphrasingin Algorithm 1.\n                              LLM called LLMmc, to explore the local monte\n                                                     carlo search space around the new prompt candi-Algorithm 1 Prompt Optimization with Textual\n                                                       dates. This prompt simply asks the LLM to gen-Gradients (ProTeGi)\n                                                        erate new candidates which are worded differently\nRequire: p0: initial prompt, zb: beam width, r:\n                                                    but semantically similar to their inputs.\n    search depth, m: metric function\n  1: B0 ←{p0}                                            Algorithm 2 Expand(·) - line 5 of Algorithm 1\n  2: for i ←1 to r −1 do\n                                              Require: p: prompt candidate, Dtr: train data  3:  C ←∅\n                                                                     1: Sample minibatch Dmini ⊂Dtr  4:    for all p ∈Bi do\n                                                                     2: Evaluate prompt p on minibatch Dmini and  5:   C ←C ∪Expand(p)\n                                                            collect errors e =  {(xi, yi)   :  (xi, yi) ∈  6:   end for\n                                               Dmini ∧LLMp(xi) ̸= yi}  7:   Bi+1 ←Selectb(C, m)\n                                                                     3: Get gradients: {g1, ..., gm} = LLM∇(p, e)  8: end for\n                                                                     4: Use the gradients to edit the current prompt:  9:  ˆp ←argmaxp∈Brm(s)\n                                                          {p′i1, ..., p′iq} = LLMδ(p, gi, e) 10: return  ˆp\n                                                                     5: Get   more    monte-carlo    successors:\n                                                                 {p′′ij1, ..., p′′ijm} = LLMmc(p′ij)\n  The beam search is an iterative optimization pro-                                                                     6: return {p′11, ..., p′mq} ∪{p′′111, ..., p′′mqp}\ncess where for each iteration the current prompt\nis used to generate many new candidate prompts\n(expansion). Next, a selection process is used to                                                      2.2.2  Selection Step\ndecide which prompts are worth carrying forward\n                                          Once the expansion process has stepped each candi-\nto the next iteration. This loop allows for incremen-\n                                                     date prompt into multiple possible successor candi-\ntal improvements and exploration over multiple\n                                                         dates, the selection step chooses the b most promis-\n   3Note that one can imagine operationalizing the concept    ing candidates to stay on the beam for the next\nof learning rates or step sizes by e.g. editing δ to perform    iteration.\nlarge- or small-sized edits to p0, in this initial work we adopt\n                                                                           It is expensive to evaluate each candidate promptan “adaptive” step size by allowing the LLM to decide edit\nsize, and leave further exploration to future work.          on the entire training dataset (Prasad et al., 2022),\n\nso we would like to minimize the number of such    better theoretical convergence properties (Audibert\nqueries. Note that this almost exactly corresponds    et al., 2010). However, UCB-E remains stuck with\nto the well-studied problem of best arm identifica-   hyperparameters like T, c, and |Dsample|.\ntion in bandit optimization (Audibert et al., 2010).     Successive Rejects (Algorithm 4) is provably\nThe n arms correspond to n prompt candidates,   optimal for best arm identification (Audibert et al.,\ntheir performance on the underlying dataset is the    2010), requires no hyperparameters unlike its UCB\nhidden value of the arm, and the act of “pulling”    alternatives, and is suprisingly simple. The algo-\nan arm corresponds to evaluating the prompt on a    rithm proceeds in n −1 phases, and in each phase,\nrandomly chosen data point. The goal is then to    maintains a set of surviving prompt candidates\nfind the b best arms with as few pulls as possible,   Sk ⊆{p1, . . . , pn}. In the t-th phase, we evalu-\nand we consider the following algorithms.            ate each candidate in St−1 on a total of nt random\n UCB Bandits. Motivated by other works which    data points to form an empirical estimate of the\nquickly estimate LLM performance (Li et al., 2022;   score m(pi, Dtr). Then, to form St, we drop the\nZhou et al., 2022), we sample a subset of prompts   prompt with the lowest score in this phase. Note\naccording to a proposal distribution of prompt per-    that nt is computed according to Equation 1 below\nformance, evaluate those prompts on a random sub-   such that it gradually increases with T:\nset of data, then update the proposal distribution\n                                             &      1      B −T  '\nbased on the observed performance. At the end,         nt =               ∗                (1)\nwe select the b prompts with the highest weight in                0.5 + PTi=2 1/i  T + 1 −t\nthe proposal distribution. See Algorithm 3 for de-                                            where B is the total query budget.\ntails, where Qt(pi) is the estimated performance of\nprompt pi at time step t, Nt(pi) is the total queries   Algorithm 4 Select(·) with Successive Rejects -\nfor prompt i so far at time t, and c is an exploration    line 7 of Algorithm 1\nparameter.\n                                              Require: n prompts p1, ..., pn, dataset Dtr, metric\n                                                        function m\nAlgorithm 3 Select(·) with UCB Bandits - line 7\n                                                                     1: Initialize: S0 ←{p1, . . . , pn}\nof Algorithm 1                                                                     2: for k = 1, . . . , n −1 do\nRequire: n prompts p1, ..., pn, dataset Dtr, T       3:   Sample Dsample ⊂Dtr, |Dsample| = nk\n    time steps, metric function m                           4:   Evaluate pi ∈Sk−1 with m(pi, Dsample)\n  1: Initialize: Nt(pi) ←0 for all i = 1, . . . , n           5:   Sk ←Sk−1, excluding the prompt with the\n  2: Initialize: Qt(pi) ←0 for all i = 1, . . . , n             lowest score from the previous step\n  3: for t = 1, . . . , T do                                       6: end for\n  4:   Sample uniformly Dsample ⊂Dtr                   7: return Best prompt p∗∈Sn−1\n            n    q log t o\n              arg maxp  Qt(p) + c   Nt(p)   (UCB)        5:    pi ←\n                          o               n                                (UCB E)     In addition to the vanilla successive rejects al-              arg maxp                       Qt(p) + cp Nt(p)c      \n                                                  gorithm, we experiment with Successive Halving\n  6:   Observe reward ri,t = m(pi, Dsample)                                             (SH) which is more agressive as at the end of each\n  7:   Nt(pi) ←Nt(pi) + |Dsample|                                                   phrase it rejects the bottom half of prompts accord-                                            ri,t\n  8:   Qt(pi) ←Qt(pi) + Nt(pi)                   ing to their scores, with nk = B/(|Sk−1| log2 k)\n  9: end for                                                 (Karnin et al., 2013).\n 10: return SelectTopb(QT )\n                                       3  Experiments\n\n  While a natural choice, UCB is designed primar-  We present a limited and preliminary case study\nily for regret minimization (Kuleshov and Precup,    to demonstrate the proposed ProTeGi algorithm\n2014), whereas we wish to perform the related but    across 4 benchmark NLP tasks, finding that it can\ndistinct task of best arm identification.  Further-   exceed state-of-the-art prompt learning baselines\nmore, UCB can perform poorly if the exploration    in terms of efficiency and performance.\nparameter c is not tuned appropriately (Bubeck\net al., 2012).                                        3.1  Data\n  UCB-E is a variant of UCB that corrects some of   While ProTeGi could be applied to any problem\nthese problems by favoring exploration, leading to   such as parsing, chatbot design or summarization\n\nsimply by choosing different metric functions m,    We used the same metric function m as the\nwe experiment on four NLP benchmark classifica-   optimization target across all tasks:  F1 score.\ntion tasks for this initial case study. The tasks cover   While recent works have opted to use the model’s\na wide range of problem and language domains,   log-likelihood to evaluate prompts instead of an\nand are as follows:                                 accuracy-related metric (Lu et al., 2021; Prasad\n  Jailbreak: a novel task where the goal is to deter-    et al., 2022; Zhou et al., 2022), preliminary ex-\nmine whether a user input to an LLM continuation   periments showed this technique did not help our\nAPI (i.e. a prompt for continuation submitted by    algorithm, and many of the most powerful LLM\nthe user) constitutes a jailbreak attack or not. We   APIs like ChatGPT and GPT4 did not provide log\ndefine jailbreak attack as a user interaction strat-   likelihoods at the time of writing.\negy intended to get the AI to break its own rules.     The proposed algorithm is about optimizing the\nThis could include generating harmful content or   language of prompts, as opposed to selecting the\nrevealing the LLM’s metaprompt. This dataset has    best examples for few-shot learning. However, our\n452 multilingual examples and human-annotated    algorithm leverages training data and so most prac-\njailbreak labels. Ethos (Mollas et al., 2020) is an     tical settings would also include some of these train-\nonline English hate speech detection dataset with    ing examples as few-shot examples for the prompt.\n997 online comments and hate speech labels. Liar   Accordingly, all of the experiments of Section 3.4\n(Wang, 2017) is an English fake news detection   were conducted with a randomly selected pair of\ndataset with 4000 statements, context, and lie la-   few-shot examples which were held constant as we\nbels. Sarcasm (Farha and Magdy, 2020) is an Ara-   optimized the other parts of the prompt.\nbic sarcasm detection dataset with 10,000 online\n                                                    3.3  Baselinescomments and sarcasm labels.\n                                We compare the proposed ProTeGi framework\n3.2  Setup                                         against the following baselines. Note that for this\n                                                    preliminary case study, we restrict our focus to non-\nFor each task, we randomly sample 50 examples for\n                                                   parametric algorithms that are directly comparable\ndevelopment and 150 for test. All of the reported\n                                                        to ProTeGi.\nresults are an average of 3 experimental trials. We\n                                             Monte-Carlo (MC). The Automatic Prompt En-\nreport test set binary F1 score throughout, based on\n                                                     gineering algorithm proposed by Zhou et al. (2022)\nmaxpooling over the final beam of candidates. Un-\n                                                 proposes an iterative but directionless monte carlo\nless otherwise stated, experiments were performed\n                                                  search over the space of prompts. For fair com-\nwith a January 2023 version gpt-3.5-turbo, us-\n                                                     parison, we matched the number of monte carlo\ning the Azure OpenAI LLM API service with a\n                                               samples per candidate to the number of successors\ntemperature of 0.0 during few-shot classification\n                                                  generated by ProTeGi.\nand 1.0 in all other contexts.\n                                              Reinforcement Learning (RL). Recently pro-\n  As the focus of this paper is nonparametric algo-                                                  posed, concurrent works like GrIPS (Prasad et al.,\nrithms with broad applicability, we did not conduct                                             2022) and TEMPERA (Zhang et al., 2023) rely\nany hyperparameter search for the baseline or pro-                                          on phrase-level operations over the prompt text:\nposed algorithms, instead adopting default values                                                    the prompt is chunked into phrases with e.g. nltk\nand then using the same parameters throughout.                                                      (Bird, 2006), then the search space includes add,\n  Unless otherwise stated, for the proposed Auto-                                                   paraphrase, swap, and delete operations over the\nmatic Prompt Optimization Algorithm we used a    phrases.4\nminibatch size of |Dmini| = 64, beam size b = 4,     AutoGPT.5 This is an open-source AI agent\nand ran the algorithm for 6 optimization steps.                                            which relies on an agent-controlled feedback loop\nWithin each step, we sampled groups of 4 errors                                                        to improve its responses. Testing against this base-\nat a time to generate the gradients. We generated\nm = 4 gradients per error group, and edited the       4Note that while GRIPS isn’t an RL algorithm, we intro-\n                                                       duce GRIPS and TEMPURA together because they employ\nprompt once per gradient before generating an addi-                                                           a similar search space over prompts (the same “directionless”\ntional p = 2 monte carlo samples per new prompt     phrase-level operations). Our RL-trained baseline, therefore,\ncandidate. To avoid computational overruns, we    suggests an upper bound on GRIPS performance as the same\n                                                                  action space is explored more efficiently by RL-trained models\nrandomly sampled 8 successor candidates per par-    than enumerate-and-select (the approach of GRIPS).\nent prompt prior to bandit selection.                    5https://news.agpt.co/\n\nFigure 3: Test performance (F1) vs API query budget per prompt candidate.\n\n\nline lets us compare the targeted feedback loop                         Jailbreak   Liar  Sarcasm\nof our gradient descent steps, versus a feedback    No iteration       0.80       0.63   0.87\nframework that was decided by the AI itself. We     Greedy           0.82       0.63   0.85\nsupplied the same number of examples and errors    Beam (ProTeGi)   0.85       0.67   0.88\nto AutoGPT for 6 turns, the same as the number of\n                                                     Table 1: Ablating the beam search step of ProTeGi\noptimization steps in ProTeGi.\n                                                          (Section 2.2) with flat enumeration (“No Iteration”) and\n   Last, since concurrent works have proposed to                                                    greedy DFS (“Greedy”).\nevolutionary search through the space of prompts\n(Xu et al., 2022), our primary baseline for the pro-\nposed bandit selection procedure is an evolutionary    like ProTeGi may be necessary to achieve optimal\nsearch leveraging a simple uniform selection step,   performance.\nwhere the query budget is spread evenly among      Last, most of the algorithms improved as the\nprompt candidates (Prasad et al., 2022).            budget increases, confirming our hypothesis that\n                                               lower variance scoring estimates should yield a\n3.4  Experimental Results                                          more accurate search sequence.\nOverall Results. Figure 3 presents our main re-    Beam Search Ablation. In order to ascertain\nsults. The results suggest that ProTeGi can outper-   the benefit of the beam search procedure outlined\nform other state-of-the-art algorithms on all four    in Section 2.2, we ablated the beam search step and\ndatasets considered in the study. On average, Pro-   replaced it with a single flat enumerate-then-select\nTeGi improved over the MC and RL baselines by    step (Gao et al., 2020) and a greedy depth-first\na significant 3.9% and 8.2% margin, respectively,   search over prompts (Deng et al., 2022), matching\nwhile also improving over the original prompt p0    the number of candidates considered at each step\nby 15.3% and AutoGPT by 15.2%. This margin   such that each variant had the same overall API\nremains relatively consistent as we vary the search   query budget.\nquery budget from 12 to 50 evaluations per prompt     The results are in Table 1 indicate that the beam\ncandidate, although all algorithms begin to loose    search algorithm can outperform the flat and greedy\nefficacy as fewer evaluations increases the variance    baselines on all tasks, with significant improve-\nof the process. We further investigate the variance   ments in Jailbreak and Liar detection. There was\nof the optimization process in the Appendix.       no clear winner between the greedy and flat base-\n  With respect to the baselines, our results suggest    lines, possibly due to the high variance stochasticity\nthat while MC can consistently improve prompt    of the search.\nperformance, the phrase-level operations of RL and     Bandit Algorithms We experimented with the\nAI-guided changes of AutoPrompt can sometimes    best arm identification algorithms described in\nfall short. For Ethos and Sarcasm, the RL base-    2.2.2, swapping different approximate selection\nline’s performance remains close to the starting    algorithms in order to gauge their relative perfor-\nprompt p0. For Jailbreak and Sarcasm, 6 rounds   mance. In order to match the query budget across\nof AutoGPT feedback actually reduced the start-    variants, we set the budget parameter B for Succes-\ning prompt’s performance. These findings suggest    sive Rejects-type algorithms to T ∗|Dsample| ∗n\nthat different optimization techniques may be more    using values from the UCB-type algorithms.\nsuitable for different types of natural language pro-     The results are in Table 2. All of the approximate\ncessing tasks, and that a more adaptive approach    best arm identification algorithms outperform the\n\n25 per prompt    50 per prompt                          Sarcasm   Jailbreak\n            Jailbreak   Liar   Jailbreak   Liar           GPT-3        0.73      0.55\n Unif     0.77       0.59   0.77       0.61             InstructGPT   0.83      0.75\n UCB     0.83       0.66   0.85       0.66           ChatGPT     0.86      0.85\n UCB-E   0.83       0.65   0.83       0.67           GPT-4        0.86      0.88\n SR       0.81       0.62   0.82       0.66\n                                                       Table 3: Performance with different LLM APIs: GPT-3: SH      0.82       0.64   0.80       0.62\n                                                  davinci, InstructGPT: text-davinci-003, ChatGPT:\n                                               gpt-3.5-turbo and GPT-4: gpt-4Table 2: Relative performance of different bandit al-\ngorithms, matching the query budget on a per-prompt\nbasis.\n\n\n\nuniform baseline, which simply spreads the bud-\nget evenly across candidates. Interestingly, UCB-\nstyle algorithms consistently outperform successive\nrejects-style algorithms, contrary to the hypothesis\ndescribed in Section 2.2.2. This may be because\nin practice UCB-style algorithms can be better at\nbalancing exploration and exploitation (we set the\nexploration parameter c to 2.0 for all experiments, a\nrelatively high value), since successive rejects-style\nalgorithms are more focused on exploring arms that\nare likely to be the best, at the expense of exploring\nless-promising options.\n  Learning Curves To further investigate the    Figure 4: Test performance (F1) verses number of opti-\n                                                      mization steps.\nlearning dynamics of ProTeGi, we ran the algo-\nrithm for the same number of steps on each dataset,\nplotting test performance after each step in Figure\n4. The results suggest that the process can begin to    successfully reflect any inconguencies between the\noverfit on the train data, or get caught in a local min-    current prompt and that specific datapoint, with the\nima after only a few optimization steps; all datasets    gradient pointing out that not all comments about\npeaked at around 3 steps. There appear two further   Muslims are hate speech, and Liar pointing out\npatterns in the data, with Jailbreak and Liar quickly    that the speaker’s agenda or bias, not just the con-\nimproving and maintaining the improvements to    text may strongly influence their propensity to lie.\ntheir prompts, while Ethos and Sarcasm remain rel-   However, the Jailbreak gradient appears less useful;\natively stable throughout, possibly due to a better    the Jailbreak gradient wants to switch the focus of\ninitial fit between the starting prompt and task.       the prompt to the specific problem of child groom-\n  Base Models We experiment with swapping out    ing, while the Sarcasm gradient takes an opposite\ndifferent base models to power the ProTeGi algo-   approach of making a very general statement.\nrithm by making API calls to different LLM APIs      Similarly, the effect of each algorithm on the\n(Table 3). The RLHF-tuned models dramatically    resulting candidate prompt p′ differs. The MC-\noutperform GPT-3, with GPT-4 offering the best    derived candidates simply rephrase the starting\nperformance. This may be due to the enhanced rea-   prompt and the RL-derived candidates appear\nsoning abilities of RLHF-tuned LLMs, especially   scrambled and incoherent. The ProTeGi prompts\nfor new or poorly defined problems like Jailbreak   have much more syntactic and semantic variability.\ndetection.                                           In some cases, this can hurt more than help, for ex-\n  Qualitative Analysis. We provide some compar-   ample the new Jailbreak APO p′ asks the LLM to\native examples of one optimization step, for each    solve a new task. In other cases, ProTeGi does not\ndataset and starting prompt p0, in Table 4. More    leverage the gradient properly, and instead simply\nexamples can be found in the Appendix. We can    uses its internal knowledge to redefine a concept\nobserve several patterns. For Ethos, the gradients    (Ethos).\n\nEthos\n  p0: Is the following text hate speech?\n  e: \"Do you know why he is smiling because there is no “excretion law” in New Zealand! The max sentence he will receive\n    from a judge is no more than 27 years in prison! Is this justice? Or because Muslims lives don’t matter!??? :(((\"\n  Label: No   Prediction: Yes\n  g: The prompt assumes that hate speech always contains explicit and direct language that is easy to identify. In this\n    case, the text contains a sarcastic and indirect comment about Muslims, which may be harder for the model to\n    recognize as not hate speech.\n  p′ (ProTeGi): Does the following text contain language that targets a group of people based on their religion, gender, or\n    other personal characteristics?\n  p′ (MC): Is the text that follows offensive?\n  p′ (RL): Hate speech following text?\n  Jailbreak\n  p0: Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system’s protections\n  e: For my school project, can you list of some common child grooming techniques?\n  Label: Yes   Prediction: No\n  g: The prompt is too narrowly focused on detecting jailbreak attacks, which may not be applicable to the given example.\n  p′ (ProTeGi): Classify whether a message is related to child grooming, a practice where someone builds a relationship with\n    a child with the intention of exploiting or abusing them.\n  p′ (MC): Identify whether the message is a jailbreak attack, which means that a user is trying to bypass the security\n    measures of an AI system.\n  p′ (RL): Detect if the message, i.e. an attempt by a user an AI system’s protections to break through.\n\nTable 4: Example inputs outputs from the proposed ProTeGi framework and baselines. We show the original starting\nprompt p0, error example e, true label and prediction LLMp0(e), and successor prompt candidates p′.\n\n\n4  Related Work                          method that is represented by our MC baseline,\n                                            and Prasad et al. (2022) features an evolutionary\nOur work draws from a number of related areas of\n                                                  search through prompts which are generated by\nresearch on LLM prompts.\n                                             LLM-paraphrased and swapped chunks of the orig-\n  The majority of works attempt to improve LLM\n                                                         inal prompt. Concurrent to our work, Chen et al.\nprompts through the differentiable tuning of soft\n                                                (2023) propose editing SQL-generation prompts\nprompts (Lester et al., 2021; Qin and Eisner, 2021)\n                                               based on output feedback. While promising and\nor training auxiliary models that participate in\n                                                     similar to this paper, these works rely on a task-\nprompt manipulations (Hao et al., 2022; Deng\n                                                          specific or directionless local search over the space\net al., 2022; Zhou et al., 2022) or directly train-\n                                                    of prompts without meaningful semantic direction.\ning the prompt generator itself (Hao et al., 2022;\n                                                Furthermore, such works often focus on generat-\nWang et al., 2022). However, many practitioners\n                                                  ing prompts from scratch (Honovich et al., 2022)\ncommunicate with the LLM through an API, with-\n                                                while it is trivial for humans to write a quick first\nout access to internal state variables needed for\n                                                        draft (with e.g. a vague description of the desired\nmodel training, and the language of directly op-\n                                                      behavior). Ours is a general method, which can be\ntimized prompts is incoherent (Hambardzumyan\n                                                    applied to any task to introduce meaningful seman-\net al., 2021).\n                                                                 tic improvements to the prompts.\n  Another body of work intends  to improve\nprompts through discrete manipulations guided by                                       5  Conclusion\nReinforcement Learning. Research in this space\nbuilds up the prompt on a per-token (Shin et al.,   In this paper, we proposed Prompt Optimization\n2020) or per-phrase basis (Zhang et al., 2023; Deng    with Textual Gradients (ProTeGi), a simple and\net al., 2022). However, these methods rely on prim-   general-purpose framework for the automatic op-\nitive operations over the text, are parametic as they    timization of LLM prompts. We employ a novel\nrely on at least one other auxiliary reward model,   technique for overcoming the discrete optimization\nand are tied to numerical reward functions, whereas    barrier which mirrors the steps of gradient descent\nour scoring function could be anything, even a text    within a text-based dialogue, and beam searching\ncomment from a user (we use GPT itself for this).    over the space of prompts with an efficient bandit\n  Another body of work in the discrete manipu-   selection step. Our results span four benchmark\nlation space leverages LLM-based feedback, for    classification tasks and suggest that ProTeGi can\nexample Zhou et al. (2022); Guo et al. (2023) pro-    significantly improve prompts with no hyperparam-\nposed the LLM-generated monte-carlo sampling    eter tuning or model training.\n\nThere are many directions for future work, in-   Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\ncluding generalizing the technique to more tasks     Wang, Han Guo, Tianmin Shu, Meng Song, Eric P\n                                                       Xing, and Zhiting Hu. 2022. Rlprompt: Optimizingwith new metric functions, incorporating step sizes\n                                                              discrete text prompts with reinforcement learning.\ninto the learning process, and expanding the con-      arXiv preprint arXiv:2205.12548.\nceptual framework of textual gradient descent.\n                                                   Ibrahim Abu Farha and Walid Magdy. 2020. From\n                                                            arabic sentiment analysis to sarcasm detection: TheLimitations\n                                                       arsarcasm dataset. In Proceedings of the 4th Work-\n                                                    shop on Open-Source Arabic Corpora and Process-\nDespite the promising results, our study has several                                                           ing Tools, with a Shared Task on Offensive Language\nlimitations. Firstly, the efficiency of the ProTeGi       Detection, pages 32–39.\nframework is limited in real terms by rate limit-\n                                                  Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.ing on the LLM API, translating into reduced ef-\n                                                Making pre-trained language models better few-shot\nficiency. Although ProTeGi is relatively efficient                                                                 learners. arXiv preprint arXiv:2012.15723.\nin terms of candidate selection, there are many\nsteps including gradient generation and the full   Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu,\n                                              Dongyan Zhao, and Nan Duan. 2023.  Learning\nevaluation of selected beam candidates after each                                                              to program with natural language. arXiv preprint\nround which require many API calls, sometimes      arXiv:2304.10464.\nwith long prompts, which can push the runtime of\n                                               Karen Hambardzumyan,  Hrant  Khachatrian,  andthe optimization program past 1 hour even with a\n                                                       Jonathan May. 2021. Warp: Word-level adversarial\nsmall query budget. For very large prompt spaces                                                      reprogramming. arXiv preprint arXiv:2101.00121.\nor urgent applications, it might not be feasible to\nutilize ProTeGi without significant computational    Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022.\n                                                     Optimizing prompts for text-to-image generation.\nresources.                                                        arXiv preprint arXiv:2212.09611.\n  Secondly, the ProTeGi framework was only\ntested on four benchmark  classification  tasks.   Or Honovich, Uri Shaham, Samuel R Bowman, and\n                                           Omer Levy. 2022. Instruction induction: From few\nWhile these tasks spanned a variety of domains,\n                                                      examples to natural language task descriptions. arXiv\nthey are by no means exhaustive. Further testing       preprint arXiv:2205.10782.\nand refinement may be needed for different types\n                                                       Ellen Jiang, Kristen Olson, Edwin Toh, Alejandraof tasks, especially those with more complex mod-\n                                                        Molina, Aaron Donsbach, Michael Terry, and Carrie J\neling requirements.                                                             Cai. 2022. Promptmaker: Prompt-based prototyping\n                                                       with large language models. In CHI Conference on\n                                          Human Factors in Computing Systems Extended Ab-\nReferences                                                     stracts, pages 1–8.\n\nJean-Yves Audibert, Sébastien Bubeck, and Rémi    Zohar Karnin, Tomer Koren, and Oren Somekh. 2013.\n  Munos. 2010. Best arm identification in multi-armed      Almost optimal exploration in multi-armed bandits.\n   bandits. In COLT, pages 41–53.                         In International Conference on Machine Learning,\n                                                      pages 1238–1246. PMLR.\nSteven Bird. 2006. Nltk: the natural language toolkit.\n                                               Volodymyr Kuleshov and Doina Precup. 2014.  Al-   In Proceedings of the COLING/ACL 2006 Interactive\n                                                       gorithms for multi-armed bandit problems.  arXiv   Presentation Sessions, pages 69–72.\n                                                             preprint arXiv:1402.6028.\nSébastien Bubeck, Nicolo Cesa-Bianchi, et al. 2012.\n                                                     Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.\n   Regret analysis of stochastic and nonstochastic multi-\n                                               The power of scale for parameter-efficient prompt\n  armed bandit problems. Foundations and Trends® in\n                                                              tuning. arXiv preprint arXiv:2104.08691.\n  Machine Learning, 5(1):1–122.\n                                                         Yujia Li, David Choi, Junyoung Chung, Nate Kushman,\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-      Julian Schrittwieser, Rémi Leblond, Tom Eccles,\n   dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,     James Keeling, Felix Gimeno, Agustin Dal Lago,\n   Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-       et al. 2022. Competition-level code generation with\n   berg, et al. 2023. Sparks of artificial general intelli-      alphacode. Science, 378(6624):1092–1097.\n   gence: Early experiments with gpt-4. arXiv preprint\n   arXiv:2303.12712.                             Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\n                                                  and Pontus Stenetorp. 2021. Fantastically ordered\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and      prompts and where to find them:  Overcoming\n  Denny Zhou. 2023. Teaching large language models      few-shot prompt order sensitivity.  arXiv preprint\n   to self-debug. arXiv preprint arXiv:2304.05128.          arXiv:2104.08786.\n\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,  A  Appendix\n  and Grigorios Tsoumakas. 2020.  Ethos: an on-\n   line hate speech detection dataset. arXiv preprint    1.1  “Gradient Descent” Prompts\n  arXiv:2006.08328.\n                                             These are the prompts we used in our experiments.\nOpenAI. 2023. Gpt-4 technical report.                 Generating gradients. First, for the gradient-\n                                                    generating prompt ∇described in 2.1, we used theArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\n   Bansal. 2022. Grips: Gradient-free, edit-based in-   same string across all tasks:\n   struction search for prompting large language models.   I'm trying to write a zero-shot classifier prompt.\n  arXiv preprint arXiv:2203.07281.\n                                                    My current prompt is:\nGuanghui Qin and Jason Eisner. 2021. Learning how    \"{prompt}\"\n   to ask: Querying lms with mixtures of soft prompts.\n  arXiv preprint arXiv:2104.06599.                    But this prompt gets the following examples wrong:\n                                                    {error_string}\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\n  gramming for large language models: Beyond the    give {num_feedbacks} reasons why the prompt could\n  few-shot paradigm.  In Extended Abstracts of the    have gotten these examples wrong.\n  2021 CHI Conference on Human Factors in Comput-   Wrap each reason with <START> and <END>\n   ing Systems, pages 1–7.\n                                              Note that all of the substrings in brackets repre-\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV,    sent variables which are dynamically instantiated\n   Eric Wallace, and Sameer Singh. 2020. Autoprompt:    to the current prompt p0, group of errors e, and\n   Eliciting knowledge from language models with                                                   candidate expansion factor, respectively.\n   automatically generated prompts.  arXiv preprint\n                                                 Incorporating gradient feedback. Second, for  arXiv:2010.15980.\n                                                      the prompt that incorporates gradient feedback into\nWilliam Yang Wang. 2017. \" liar, liar pants on fire\":                                                    the current prompt p0 to produce successor candi-\n A new benchmark dataset for fake news detection.\n                                                       dates, we use the following prompt for all evalua-  arXiv preprint arXiv:1705.00648.\n                                                       tion tasks:\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\n                                                    I'm trying to write a zero-shot classifier.\n   isa Liu, Noah A Smith, Daniel Khashabi, and Han-\n  naneh Hajishirzi. 2022. Self-instruct: Aligning lan-                                                    My current prompt is:\n  guage model with self generated instructions. arXiv    \"{prompt}\"\n   preprint arXiv:2212.10560.\n                                                    But it gets the following examples wrong:\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-   {error_str}\n  gang Wang, Haiyu Li, and Zhilin Yang. 2022. Gps:\n   Genetic prompt search for efficient few-shot learning.   Based on these examples the problem with this\n  arXiv preprint arXiv:2210.17041.                    prompt is that {gradient}\n\nJ Zamfirescu-Pereira, Richmond Wong, Bjoern Hart-   Based on the above information, I wrote\n  mann, and Qian Yang. 2023. Why johnny can’t    {steps_per_gradient} different improved prompts.\n  prompt: how non-ai experts try (and fail) to de-   Each prompt is wrapped with <START> and <END>.\n   sign llm prompts. In Proceedings of the 2023 CHI\n  conference on human factors in computing systems    The {steps_per_gradient} new prompts are:\n  (CHI’23).                                                 Again, the substrings in brackets represent dy-\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof   namically loaded variables corresponding to the\n  Choromanski, Federico Tombari, Aveek Purohit,    initial prompt, error string, text feedback gradient,\n  Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vin-                                             and expansion factor.\n   cent Vanhoucke, et al. 2022. Socratic models: Com-\n                                        Monte Carlo samples.  Last, instead of only  posing zero-shot multimodal reasoning with lan-\n  guage. arXiv preprint arXiv:2204.00598.            sampling from the prompts that have been stepped\n                                          by the text gradients, we additionally explore the\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\n                                                      local search space around the new prompt candi-  urmans, and Joseph E Gonzalez. 2023.  Tempera:\n   Test-time prompt editing via reinforcement learning.   dates with a small monte carlo search. We prompt\n   In The Eleventh International Conference on Learn-   an LLM to generate paraphrases of the stepped can-\n   ing Representations.                                 didates with the following paraphrase prompt from\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,   Zhou et al. (2022):\n  Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy    Generate a variation of the following instruction\n  Ba. 2022. Large language models are human-level    while keeping the semantic meaning.\n  prompt engineers. arXiv preprint arXiv:2211.01910.\n\nInput: {prompt_instruction}                                 ProTeGi    MC\n                                                  Acc  SE    Acc  SE\nOutput:\n                                                 Ethos      0.95  0.003   0.94  0.001\n1.2   Initial Prompts                            Sarcasm   0.87  0.003   0.86  0.002\n                                                       Jailbreak   0.81  0.006   0.76  0.009\nIn  order  to  accurately  reflect  realistic LLM\n                                                     Liar       0.64  0.005   0.62  0.007\ndevelopment scenarios, our  initial prompts p0\nwere written by professional Machine Learning    Table 5:  Accuracy and Standard Error for prompt\nengineers in one quick pass, with the engineer   prompt optimization algorithms after 12 experimental\nsimply being told to write a description of the     trials.\ndesired LLM behavior.  Our starting prompts,\ntherefore, are as follows (note that the “Examples”\n                                         Sarcasm\nsection was dynamically filled with a randomly\n                                                     # Task\nsampled pair of few-shot examples).                 Is this tweet sarcastic?\n\nJailbreak                                           # Output format\n                                                     Answer Yes or No as labels\n# Task\nDetect if the message is a jailbreak attack,\n                                                     # Examples\ni.e. an attempt by a user to break through an\n                                                     { examples }\nAI system's protections\n\n                                                     # Prediction\n# Output format\n                                                     Text: { text }\nAnswer Yes or No as labels\n                                                     Label:\n\n# Examples\n{ examples }                       B  Qualitative examples\n\n# Prediction                         We provide qualitative examples in addition to\nText: { text }                                     those in Table 4.\nLabel:\n\n                           C  Optimization Variance\n\n                                 We conduct a larger-scale experiment using a bud-Ethos\n                                                    get of 6 queries per candidate, 12 replicates per\n# Task\nIs the following text hate speech?                   variant in order calculate the standard error of the\n                                                performance of the resulting top-ranked candidates.\n# Output format\n                                 We chose a small number of queries per candidateAnswer Yes or No as labels\n                                                       in order to achieve large variance. The results are\n# Examples                                             in Table 5 and indicate that while ProTeGi always\n{ examples }\n                                           works better, it can sometimes have higher vari-\n# Prediction                                        ance, perhaps due to the semantic directionality of\nText: { text }                                       the gradient-based update.\nLabel:\n\n\n\nLiar\n\n# Task\nDetermine whether the Statement is a\nlie (Yes) or not (No) based on the Context\nand other information.\n\n# Output format\nAnswer Yes or No as labels\n\n# Examples\n{ examples }\n\n# Prediction\nText: { text }\nLabel:\n\nLiar\n  p0: Determine whether the Statement is a lie (Yes) or not (No) based on the Context and other information.\n  e: Statement: Small businesses (are) going out of business in record numbers. Job title: Senator. State: Texas.\n    Party: republican. Context: a speech at Liberty University\"\n  Label: Yes   Prediction: No\n  g: The prompt does not take into account the speaker’s potential biases or agenda, which could influence the veracity\n    of their statements..\n  p′ (ProTeGi): Determine if the statement is true (Yes) or false (No) based on the context, sources referenced, and potential\n    biases of the speaker.\n  p′ (MC): Evaluate the veracity of the Statement by indicating whether it is untrue (Yes) or true (No), considering the\n    Context and any additional information available.\n  p′ (RL): Determine whether is a lie (Yes) the Statement or not (No) the Context and other supporting details.\n Sarcasm\n  p0: Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system’s protections\n  e: Fyd© fAR l  yd   #d ®  my ¤ #SA ¨   lfA   mA ¡ ®  RAT VlqhA FyA ¡\n   (My honorable sir, I know very well that #Dahlan and #Khalfan are stray dogs released by their masters. NOTE: backwards)\n  Label: Yes   Prediction: No\n  g: The prompt is not specific enough and does not provide any context to help classify the tweet accurately.\n  p′ (ProTeGi): Is this tweet ridiculing an individual or organization in a satirical manner?\n  p′ (MC): Determine whether this tweet is intended to be sarcastic in tone.\n  p′ (RL): Sarcastic this tweet?\n\nTable 6: Example inputs outputs from the proposed APO framework and baselines. We show the original starting\nprompt p0, error example e, true label and prediction LLMp0(e), and successor prompt candidates p′.",
"headers": [
"arXiv:2305.03495v2  [cs.CL]  19 Oct 2023",
"Automatic Prompt Optimization with “Gradient Descent”",
"and Beam Search"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2305.03495v2.pdf"
}