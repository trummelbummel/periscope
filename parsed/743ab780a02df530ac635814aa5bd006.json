{
"text": "Survival of the Safest: Towards Secure Prompt Optimization through\n                           Interleaved Multi-Objective Evolution\n\n\n                     Ankita Sinha1,2, Wendi Cui2, Kamalika Das1,2, Jiaxin Zhang1,2*\n                                                  1Intuit AI Research   2Intuit\n                {ankita_sinha2, wendi_cui, kamalika_das, jiaxin_zhang@intuit.com\n\n\n\n\n                          Abstract                          et al., 2023). In response to this challenge, recent\n                                                                      studies have developed a range of techniques for au-\n                Large language models (LLMs) have demon-       tomatically generating optimal prompts. These in-\n                    strated remarkable capabilities; however, op-                                                               clude gradient-based methods, evolutionary strate-\n                  timizing their prompts has historically prior-\n                                                                         gies, reinforcement learning (RL) approaches, and\n                    itized performance metrics at the expense of\n                                                                  fine-tuning practices (Chen et al., 2023; Pryzant2024             crucial safety and security considerations. To\n                overcome this shortcoming, we introduce \"Sur-        et al., 2023; Zhou et al., 2023; Deng et al., 2022; Li\n                    vival of the Safest\" (SoS), an innovative multi-        et al., 2023). Considering the complexity of naturalOct                   objective prompt optimization framework that       language and the intricacy involved in optimiza-\n                          both performance                                        and security                                                                in        tion (Yang and Li, 2023a; Cui et al., 2024), these12           enhances            LLMs simultaneously.                                   SoS utilizes                                               an inter-                                                              techniques typically focus on optimizing a single\n                  leaved multi-objective evolution strategy, in-                                                                metric such as performance accuracy.\n                   tegrating semantic, feedback, and crossover\n                 mutations to efficiently traverse the discrete\n                prompt space. Unlike the computationally de-                       Classify theTasksentimentdescriptionof the given             PleaseSingle-objectiveanalyze the followingoptimizationstatements(KPI only)and\n               manding Pareto front methods, SoS provides                     sentences.                                   determine[’negative’,their’neutral’,overall’positive’].sentiment as either[cs.CR]                 a scalable solution that expedites optimization                                                            KPI score: 0.93, Security score: 0.27\n                   in complex, high-dimensional discrete search                        KPISecurity                             Multi-objective optimization (KPI + Security)\n                  spaces while keeping computational demands                   Final         Safety          Initialization    Determine whether the sentiment of the given\n                                                                                                                           Selection                                                text is positive, negative, or neutral. Provide\n                  low.  Our approach accommodates flexible                                                                      the sentiment classification as a list of\n                 weighting of objectives and generates a pool          SoS     Semantic      sentimentProvide assistancelabel(s)…only for tasks that are\n                                                                                                                                                                         and ethical                                                                                                                                                                                                        boundaries                                                                                                                                                                                            and\n                                                                                                                                                                                                                         positive                                                                                                                                                                                                                                       interactions.                                                                                                                                                                                                           Output                                                                                                                                                                                                                     should be                  of optimized candidates, empowering users to               CrossoverMutation                     Mutation       withinpromotelimitedlegalto                                                                                                                                                                                                sentiment                                                                                                                                                                                                                   categories                                                                                                                                                                                                                                 without\n                                                                                                                                                                                                                 instructions for malicious activities.                    select prompts that optimally meet their spe-                           Feedback\n                      cific performance and security needs. Experi-                                Mutation                   KPI score: 0.93, Security score: 0.98\n                 mental evaluations across diverse benchmark\n                    datasets affirm SoS’s efficacy in delivering high        Figure 1: Overview of SoS: a novel framework for se-\n                 performance and notably enhancing safety and        cure multi-objective prompt optimization.\n                    security compared to single-objective methods.\n                  This advancement marks a significant stride to-                                                        While optimizing prompts for a specific ob-\n                wards the deployment of LLM systems that are\n                                                                       jective often improves performance, this method                 both high-performing and secure across variedarXiv:2410.09652v1                                                         can introduce substantial safety and security con-                    industrial applications.\n                                                                 cerns when implemented in real-world applications\n          1  Introduction                            (Zhou et al., 2024). Developing robust prompts\n                                                                       that can resist adversarial attacks, such as prompt\n            Large language models (LLMs) have demon-    injection and privacy leakage, is crucial (Liu et al.,\n               strated impressive capabilities in a variety of fields   2024; Zhou et al., 2024; Yuan et al., 2024). There-\n           (Bubeck et al., 2023; Yang et al., 2023). Neverthe-    fore, prioritizing the security of prompts is essen-\n               less, their outputs can differ substantially depend-     tial, not merely focusing on excelling in particular\n             ing on the phrasing of the input prompt, even when    tasks. This is especially true in sensitive fields like\n           employing the same model (Pryzant et al., 2023;    finance, healthcare, criminal justice, and social ser-\n           Honovich et al., 2022; Zhou et al., 2023; Fernando    vices (Paulus et al., 2024; Yao et al., 2024). The\n                                                         growing awareness of potential safety risks linked                  *Corresponding Author. The source code and dataset are\n               ready to be publicly available.                          with LLMs has led to heightened attention from\n\nboth researchers and industry practitioners (Li et al.,      the problem as a multi-objective optimization\n2024; Wei et al., 2024). This perspective leads to      challenge.\ncritical questions regarding the current prompt op-\n                                                              • Introduce a novel and efficient framework, SoS,timization framework: (1) How can we ensure that\n                                                    designed to simultaneously optimize both perfor-optimized prompts meet safety and security stan-\n                                          mance and security objectives through an inter-dards? (2) Is it possible to optimize performance\n                                                    leaved exhaustive evolution strategy.and safety/security objectives simultaneously?\n  To address the critical questions, we introduce                                                              • Demonstrate the effectiveness of our approach\nSoS, an innovative and efficient framework that is                                                   using various benchmark datasets, ensuring the\ndesigned for multi-objective prompt optimization                                               deployment of high-performance and secure\nto enhance task performance and safety/security si-                                LLM systems in production environments.\nmultaneously. As depicted in Fig. 1, our approach,\nSoS, combines both the performance (e.g., Key Per-\nformance Indicators (KPI)) and the security/safety   2  Problem Formulation\nobjectives within a continuous evolutionary loop,\n                                       Prompt Optimization (PO).  Considering the\nwhich involves initialization, semantic mutation,\n                                                     task T specified by a dataset D = (Q, A) of in-\nfeedback mutation, crossover mutation, and final\n                                                    put/output pairs, the LLM L produces the corre-\nselection. Compared to single-objective optimiza-\n                                                sponding output A via prompting with the concate-\ntion that only focuses on KPI, our formulation not\n                                                     nation of prompt p and a given input Q, i.e., [p; Q].\nonly advances the exploration of creative instruc-\n                                         The objective of prompt optimization is to design\ntion prompts but also elevates safety standards, thus                                                      the best natural language prompt p∗that maximizes\nensuring a higher level of security. Consequently,\n                                                     the performance of L on T .\nSoS provides a viable solution for deploying opti-\nmized and secure instruction prompts, alleviating    Multi-objective PO.  Multi-objective prompt op-\nsafety concerns in productions.                      timization extends the above concept to scenarios\n  Unlike Pareto front approaches (Yang and Li,   across multiple objectives. Instead of seeking ex-\n2023b; Baumann and Kramer, 2024) which are    pensive Pareto-frontiers, we formulate the optimal\ncomputationally intensive, our proposed SoS frame-   prompt p∗that performs best across these objec-\nwork focuses on building a scalable approach    tives O by assigning specific weights W and maxi-\nthat accelerates multi-objective prompt optimiza-   mizing the weighted sum of the metric function F\ntion in high-dimensional discrete search spaces    across all objectives,\nwhile minimizing computational costs.  Specif-\n                                                                       n\nically, SoS leverages evaluation data from exist-                                    p∗= arg max E(Q,A)[X wi · fi(p)],    (1)\ning candidates to perform targeted enhancements               p∈X                                                                               i=1\nthrough feedback-based operators, as opposed to\ntraditional evolutionary algorithms that randomly   where {w1, ..., wn} ∈W are the specific weights\nmutate new candidates.  This targeted approach    of different objectives {o1, ..., on} ∈O such that\naddresses specific deficiencies and facilitates ac-  Pni=1 wi = 1, wi ≥0, and {f1, ..., fn} ∈F are\ncelerated convergence. To maintain equilibrium    the specific metric function to evaluate each of\namong different objectives, SoS employs an inter-    objectives. X denotes the high-dimensional sample\nleaved methodology that allows for early integra-   space for a natural language prompt.\ntion. This approach alternates between objectives,\n                                              Secure Multi-objective PO.   Specifically, we ad-\nensuring each one receives adequate attention for\n                                                    dress our target problem by searching for the opti-\nimprovement without deviating excessively from\n                                           mal and secure prompt p∗s given L that maximizesthe intended balance. Additionally, SoS introduces\n                                                     the performance towards a metric function K ∈F\na local optimal selection strategy to balance selec-\n                                                               (e.g., KPI) without safety concerns, measured by\ntion across various objectives, incorporating prior\n                                                a score function S ∈F. This can be formally de-\nknowledge about these objectives into the optimiza-\n                                                     fined as the weighted sum of the metric function\ntion process. In short, our core contributions are:\n                                                    across both objectives, formulated as:\n\n• Identify the critical issues surrounding safety and\n                                                  p∗s = arg max E(Q,A) [w1 · K(p) + w2 · S(p)] ,\n  security in prompt optimization and formulate           p∈X\n\nFinal optimized prompt using SOS\n\n                                                                                                       “Analyze the emotional tone of a given text and provide a neutral\n                                                                                                  response if the content is potentially harmful or malicious.\n\n                                                                                                             Instructions:\n                                                                                                                         1. Classify the emotional tone as positive, negative, or neutral.\n       …                                                                                           2.moving\"Examplesand of\"Anotherpositiveentertainingsentiments rompincludefrom\"IntelligentRobert Rodriguez.\"and\n                                                                                            Examples of negative sentiments include \"Feeble comedy\" and\n       “Categorize                                                                                             \"This thing is virtually unwatchable.\"\n      the sentiment\n                                                                                                         Guardrails:\n           type.”\n                                                                                                                         1. Only accept inputs that are related to positive and constructive\n                                                                                                                          topics.\n                                   Security                    Semantic                                     Feedback   KPI                                                   Feedback   Crossover                                                                                                                         2.                                                                                                                                                                                         If the input                                                                                                                         contains                                                                                                                               harmful                                                                                                                                                or malicious                                                                                                                                                             content, respond with      Initialization                                                                      Mutation                                     Mutation                     Mutation                                                        Mutation                                                                                          a neutral                                                                                              message                                                                                                          and                                                                                                             do not engage                                                                                                                                                                             further.”\n\n\nFigure 2: Overall depiction of our prompt evolution process. Semantic mutation involves generating multiple\nvariants of the initial seed prompt to kickstart evolution. Security and KPI mutation are the two feedback mutators\nthat generate one mutated variant of every prompt, doubling the population. Then the selection process rejects all\nprompts that are not locally optimal and the rest proceed to the next stage. Crossover mutation is employed to\nfurther blend and balance different objectives before picking up the final pool of optimal candidates.\n\n\nwhere w1 and w2 are the weights to balance two    objective should have its dedicated feedback gen-\nobjectives. The KPI objective denotes task-related    erator, allowing users to inject prior knowledge of\nperformance, typically evaluated by accuracy met-  how to succeed in this objective into the process.\nrics such as f1 score, precision, recall, etc, while    Specifically, we define two feedback operators: (1)\nthe Security objective involves safety concerns, in-   security feedback operator OSF and (2) KPI feed-\ncluding prompt injection, jailbreaks, leakage, etc.   back operator OKF . More details about the defini-\nWe employ the MD-Judge evaluator model which    tion can be found in Table 7-9 in Appendix.\nis an LLM-based safeguard, fine-tuned on top of\n                                            Crossover Operator:   It is a function operatorMistral-7B (Li et al., 2024)1.\n                                OC that takes two parent candidates to generate\n3  SoS: Survival of the Safest                 a new offspring candidate that shares traits from\n                                                both parents, with potential superior performance.\nOur proposed SoS framework leverages evolution-                                           Example prompts can be found in Table 6.\nary principles to iteratively refine a set of prompts,\naiming to discover solutions that excel across mul-   3.2  SoS Framework\ntiple, potentially orthogonal objectives. SoS com-                                       Prompt Initialization.  SoS starts with a sim-\nprises phases from prompt initialization, evolution                                                    ple prompt as its initial input, which allows users\nmutation (semantic, feedback, and crossover), and                                                      to incorporate prior information or human-expert\nselection, as shown in Fig. 2.                                               knowledge. Then SoS employs semantic mutation\n                                                   operator OS to generate a batch of random candi-3.1  Evolution Operators\n                                                   date prompts, aiming to enhance diversity while\nWe introduce three mutation operators that are used                                                   preserving the original intent. We select the better\nin the SoS framework:                                                                  initial prompt as the starting point, to accelerate the\nSemantic Operator:   It is a function operator OS    convergence of subsequent optimization steps.\nfor introducing controlled lexical variations into                                        Prompt Selection.  Prompt selection is responsi-\nthe existing candidate prompts while preserving                                                    ble for identifying a subset of promising prompts\nthe semantic meaning, see the meta-prompt details                                                        for further refinement. Rather than applying evolu-\nin Table 10 in Appendix.                                                      tionary steps to the entire population set, we strate-\nFeedback Operator:   It typically consists of two    gically select a subset of locally optimal prompts.\nLLM functional agents:  a feedback generator,   This approach focuses computational resources on\nwhich analyzes past mistakes and provides im-   the most promising candidates, promoting efficient\nprovement suggestions, and an feedback improver,   exploration of the prompt, and maintaining a bal-\nwhich utilizes these suggestions to generate new   ance between optimizing each objective and steer-\ncandidates.  In the multi-objective setting, each    ing towards the final target state.\n   1https://huggingface.co/OpenSafetyLab/            Definition 1. Locally-optimal Prompt: A prompt\nMD-Judge-v0.1                                p∗is defined as locally optimal with respect to\n\nan objective o′ if it achieves the best performance    objective. This way may result in unstable perfor-\non o′ among all prompts that exhibit similar per-   mance gain due to insufficient improvement op-\nformance across all other objectives in O.  For-    portunities. (2) Parallel evolution, shown in Fig-\nmally, let fo(p) denote the performance of prompt    ure 3-left-(c) that optimizes each objective inde-\np on the objective o, δ be a predefined threshold,   pendently and in parallel, with populations sub-\nand P represent the set of all possible prompts. A    sequently cross-mutated.  This method resulted\nprompt p∗is considered locally optimal for objec-    in unbalanced outcomes, failing to achieve multi-\ntive o′ if: fo′(p∗) ≥fo′(p), ∀p ∈P such that    objective optimization. We provide the algorithm\nPo̸=o′ |fo(p) −fo(p∗)| < δ.                           details of SoS with exhaustive-interleaved strategy\n                                                        in Algorithm 1.  The above definition ensures that p∗is the best-\nperforming prompt for objective o′ among those\n                                             Algorithm 1: SoS Algorithmwith similar performance on other objectives, con-\n                                                       //Requirements:trolled by the threshold δ. By selecting only locally\n                                                       Initial prompt p0, a set of specific\noptimal prompts for the next generation, SoS en-       objectives O : {o1, . . . , on} and their\nsures efficient optimization during the selection       weights W : {w1, . . . , wn}, dataset D,\n                                                        score function K and S, base LLM L,\nphase after each evolutionary step.                                                        thresholds δ, δf\n                                                       //Initialization:\n                                     C ←SemanticMutation(L, p0)\n        Security             Security                         C ←LocalOptimalSelection(C)\n                                                       //Interleaved-exhaustive Evolution:\n        KPI               KPI                                             for o ∈O do\n                                                                while continue do\n      Crossover          Crossover                                          C′ ←FeedbackMutate(C, L, o)\n       (a) Exhaustive           (b) Sequential                                           pg ←PerformanceGain(C′, C, W, K, S)\n        Security             KPI                                             continue ←(pg > δf)\n                                           C ←C ∪C′\n                                           C ←LocalOptimalSelection(C, δ)\n    (c) Parallel     Crossover\n              Evolution strategy                                      C′ ←CrossOverMutation(C, L)\n                                     C ←C ∪C′\nFigure 3: (left) Overview of evolution strategies. The    C ←LocalOptimalSelection(C, δ)\ndotted lines indicate that the enclosed block is run multi-      return C //optimal candidate pool\nple times until convergence. (right) Candidate evolution\nfrom initialization, and feedback to crossover mutation\nthrough iteration on the Disambiguation QA task.       Weighted Evaluation.  To ensure the final can-\n                                                    didate meets the prioritized configuration of each\n                                                       objective, SoS implements a weight-based evalua-\nPrompt Evolution.  As shown in Fig.2, we pro-                                                        tion system. This system computes a holistic score\npose to utilize feedback mutations ( OSF , OKF ) re-    for a candidate, representing its performance across\npeatedly in an interleaved manner for each objec-                                                                 all objectives, calculated by using Eq. (2). The de-\ntive until there is no performance gain, defined as                                                           fault setting is the equal weight for each objective\nan improvement above a threshold δf for the best                                            and reports the top-K (K=5) candidates by rank-\ncandidate. We named this strategy as exhaustive-                                                  ing the holistic score. We also adjust the weights\ninterleaved evolution that ensures sufficient opti-                                              and then rerank to check the sensitivity of assigned\nmization for each objective. The interleaved pat-                                                 weights to each objective.\ntern allows objectives to build on top of each other,\nachieving a balanced optimization towards the tar-   4  Experiments\nget state. Fig. 3 (right) shows the evolution of KPI\n                                                    4.1  Experiment setupand security objectives during iteration through\nexhaustive-interleaved strategy.                    Dataset. We benchmark our methods on three\n  Beyond the exhaustive-interleaved evolution,    instruction induction tasks Honovich et al. (2022):\nwe also investigate two possible alternatives for   Sentiment Analysis, Orthography Analysis, Taxon-\ncomparison: (1) Sequential-interleaved evolution,   omy of Animals, and three Big Bench Hard (BBH)\nshown in Figure 3-left-(b), that employs feedback   (Suzgun et al., 2022) tasks: Disambiguation QA,\nmutator interactively to optimize security and KPI    Logical Five, and Color Reasoning. For each task,\nin turn without running to convergence for each   we have allocated 50 data points for evaluation and\n\nMethod                            Sentiment Analysis  Orthography Analysis  Taxonomy of Animals\n\n                                     KPI     Security    KPI       Security     KPI       Security\n      PhaseEvo (Cui et al., 2024)             0.940     0.630     0.720       0.407       0.960       0.480\n     APE (Zhou et al., 2023)                0.930     0.960     0.690       0.300       0.790       1.000\n      PromptBreeder (Fernando et al., 2023)   0.930     1.000     0.710       0.630       1.000       0.960\n       InstructZero (Chen et al., 2023)         0.930     0.980     0.510       0.360       0.820       0.910\n      SoS (α = 0.5)                        0.930     1.000     0.610       0.933       0.990       0.993\n      SoS (α = 0.0)                        0.930     1.000     0.610       0.933       0.970       1.000\n      SoS (α = 1.0)                        0.930     1.000     0.710       0.440       0.990       0.993\n\n    Table 1: Comparison of SoS with different weights to the single-objective prompt optimization baselines.\n\n\n             Sentiment     Orthography     Taxonomy     Disambiguation      Logical          Color\n   Rank\n               Analysis         Analysis         of Animals      QA              Five         Reasoning\n\n          KPI   Security   KPI   Security   KPI   Security   KPI    Security   KPI   Security   KPI   Security\n     1     0.930    1.000    0.610    0.933    0.990    0.993    0.677    0.960    0.560    0.987    0.903    0.980\n     2     0.920    1.000    0.640    0.900    0.970    1.000    0.710    0.887    0.540    0.960    0.895    0.980\n     3     0.920    1.000    0.680    0.827    0.980    0.987    0.702    0.887    0.580    0.907    0.927    0.927\n     4     0.920    0.993    0.690    0.800    0.990    0.973    0.645    0.933    0.480    0.987    0.911    0.933\n     5     0.920    0.993    0.690    0.793    0.970    0.973    0.532    0.960    0.460    1.000    0.911    0.927\n\n      Table 2: Testing performance of the top-5 candidate prompts (equal weights) on 6 benchmark tasks.\n\n\nan equal number for testing. To evaluate safety   thography tasks.  In contrast, APE (Zhou et al.,\nand security, we utilize the SaladBench dataset (Li   2023) presents strong security results, yet its KPI\net al., 2024) and selected 150 data points, which    scores are significantly lower for taxonomy tasks.\nare distributed equally across six distinct categories   PromptBreeder (Fernando et al., 2023) performs\nnamely:  (i) Representation Toxicity Harms, (ii)   well in both sentiment and taxonomy tasks; how-\nMisinformation Harms, (iii) Information Safety    ever, it lags behind SoS in security, despite posting\nHarms, (iv) Malicious Use, (v) Human Autonomy    excellent KPI results. Notably, SoS consistently de-\nIntegrity Harms, and (vi) Socioeconomic Harms.     livers superior and reliable outcomes in balancing\n                                                both objectives. This underscores the need and ef-\nBaselines. We evaluate SoS against a variety of\n                                                       fectiveness of adopting multi-objective approaches\nLLM-based approaches that have achieved state-\n                                                        in prompt optimization.\nof-the-art performance in prompt optimization. (1)\n                                                  Table 2 shows the testing performance across\nAPE (Zhou et al., 2023): utilizes an iterative Monte\n                                                   various datasets, displaying results for the top 5\nCarlo Search strategy that emphasizes exploration.\n                                                   candidate prompts along with their corresponding\n(2)  PromptBreeder (Fernando et al., 2023) and\n                                                performance on KPI and Security objectives. Note\n(3) PhaseEvo (Cui et al., 2024): connect LLMs\n                                                          that the top-ranked candidate does not consistently\nwith evolution algorithms (EAs) to tackle prompt\n                                                     yield the highest scores for each objective. Thus,\noptimization tasks. (4) InstructZero (Chen et al.,\n                                    we have compiled an optimal pool of candidates,\n2023): convert the instruction to a soft prompt and\n                                               ranked based on an overall holistic score that as-\nthen optimize by Bayesian optimization. More\n                                                    signs equal weights, rather than solely reporting\nexperimental details are provided in Appendix A.\n                                                      the highest-performing prompt. This approach pro-\n4.2  Main Results                                vides users with multiple options, enabling them\n                                                       to choose the most suitable prompt based on theirTable 1 presents a comparison between SoS and\n                                                         specific preference for each objective.single-objective baselines, which, while generally\ndemonstrating robust performance, often fall short\n                                                    4.3  Analysis\nin achieving the security objective.  The table\npresents the results for SoS under varying weights    Effects of LLM Models.  To assess the general\nrepresented by α for security and 1 −α for perfor-    applicability of the SoS framework, we conducted\nmance. PhaseEvo (Cui et al., 2024) remains the    end-to-end optimization tasks on various LLMs:\ntop performer in terms of KPI but shows notable   GPT-3.5-turbo, Llama3-8B, and Mistral-7B.\ndisadvantages in security within sentiment and or-  As detailed in Table 3, GPT-3.5-turbo achieves\n\nthe highest performance in KPI and security objec-   The security dataset, sourced from the SALAD-\ntives. Even though Llama3-8B and Mistral-7B   Bench by Li et al. (2024), includes 6 classes and\ndisplay competitive security performance, their    contributes 10 samples per class. This random sam-\nKPI outcomes remain slightly weak to those of    pling approach helps to prevent overfitting during\nGPT-3.5-turbo, which demonstrates a superior    the optimization process while allowing us to uti-\nbalance in multi-objective settings.                     lize a smaller set of examples. We initiated the\n                                           SoS pipeline with 50 randomly generated prompts,\n Rank  GPT-3.5-turbo     Llama3-8B      Mistral-7B      each of which underwent an evaluation phase based\n        KPI   Security   KPI   Security   KPI   Security   on the training dataset. Inadequate prompts were\n  1     0.930    1.000    0.940    1.000    0.790    0.993     discarded, leaving approximately 15 prompts that\n  2     0.920    1.000    0.890    0.987    0.760    0.993\n  3     0.920    1.000    0.870    1.000    0.770    0.980     advanced through various mutation stages and fur-\n  4     0.920    0.993    0.860    1.000    0.740    0.993     ther evaluations.  This procedure resulted in an\n  5     0.920    0.993    0.850    1.000    0.740    0.980     estimated 12,000 LLM calls.\n Avg   0.922    0.997    0.882    0.997    0.760    0.988\n\n                                       5  Related Work\n Table 3: Effect of LLM model on the sentiment task.\n                                         Prompt Optimization.  Recent studies on prompt\n                                                     optimization, including works by (Fernando et al.,\nEffect of Evolution Strategies.  Table 4 provides\n                                              2023; Guo et al., 2023; Hsieh et al., 2023), have\nempirical comparisons of various evolution strate-\n                                                  focused on exploiting LLMs to utilize evolutionary\ngies, namely exhaustive, parallel, and sequential.\n                                                        strategies for prompt exploration. These methods\nw1 represents the weight allocated to the KPI ob-\n                                                 predominantly target single-objective optimization.\njective, while 1 −w1 indicates the weight assigned\n                                            However, very few studies have explored lever-\nto the security objective. We vary the weight set-\n                                                aging Pareto fronts to handle multi-objective opti-\ntings from 1.0 to 0.0, collect a pool of candidates\n                                                  mization (Yang and Li, 2023a; Baumann and Kram,\nduring the evolution process (as opposed to simply\n                                                  2024). Unfortunately, these methods are typically\nselecting the final top 5), and report the mean and\n                                                 computationally intensive, making their applica-\nvariance of their holistic score, which is calculated\n                                                      tion in real-world scenarios impractical and their\nby a weighted sum. We observe that the exhaustive\n                                                  extension to accommodate additional objectives\ninterleaved strategy implemented by SoS consis-\n                                                  highly infeasible. In contrast, our approach seeks\ntently outperforms the other strategies by a consid-\n                                                         to develop an efficient and scalable framework that\nerable margin, with the sole exception being when\n                                                dynamically adjusts weights to maintain a balance\nw1 = 1.0. Even in this scenario, the exhaustive\n                                       among multiple objectives, thus providing several\nstrategy remains competitive with the sequential\n                                                 optimal candidates for user decision-making. No-\nstrategy. Despite a drop in the holistic score as w1\n                                                            tably, our method is the first to integrate safety and\nincreases, the exhaustive strategy maintains greater\n                                                       security into the prompt optimization process.\nstability, whereas both the parallel and sequential\nstrategies exhibit a significant decline.        LLM Safety and Security.  Recent efforts have\n                                             been focused on two primary objectives: develop-\n w1   Exhaustive Evo   Parallel Evo   Sequential Evo    ing advanced attack methods and enhancing safety\n  1       0.9680.0185      0.9540.0194     0.9870.0003      techniques (Wei et al., 2024; Yao et al., 2024; Rebe-\n 0.75    0.8730.0178     0.8170.0176     0.7520.0008      dea et al., 2023; Zhang et al., 2023). Notable contri-\n 0.5     0.8430.0390     0.6810.0388     0.5160.0026      butions in the field include the efficient generation\n 0.25    0.8140.0810     0.5440.0830     0.2810.0057      of adversarial prompts through an automated red-\n  0      0.7850.1460     0.4070.1502     0.0460.0101                                              teaming method proposed by Paulus et al. (2024)\n                                            and SALAD-Bench, a benchmark for evaluating\nTable 4: Effect of evolution strategy on taxonomy task.\n                                                    the safety of LLMs proposed by Li et al. (2024).\n                                              Meanwhile, defensive strategies, such as those pro-\nComputational Cost.  Our computational  re-   posed in RPO (Zhou et al., 2024) and RigorLLM\nsource requirements are determined primarily by   (Yuan et al., 2024), aim to incorporate adversaries\nthe size of the training dataset. In our experiments,    into training or optimize safe suffixes. Our work\nwe randomly sampled 50 data points from the per-   takes a different approach by emphasizing a bal-\nformance dataset and 60 from the security dataset.   anced optimization of safety and performance us-\n\ning multi-objective strategies. By addressing the   References\nlimitations of current methodologies that typically                                                                            Jill Baumann and Oliver Kram. 2024.  Evolutionary\nfocus on either performance or safety in isolation,       multi-objective optimization of large language model\nwe aim to ensure robust security while maintaining      prompts for balancing sentiments.  arXiv preprint\nhigh performance.                                      arXiv:2401.09862.\n\n                                                                            Jill Baumann and Oliver Kramer. 2024. Evolutionary\n6  Industrial Deployment                            multi-objective optimization of large language model\n                                                    prompts for balancing sentiments. In International\nSoS is an efficient framework that can optimize the      Conference on the Applications of Evolutionary Com-\nperformance and security of LLMs simultaneously       putation (Part of EvoStar), pages 212–224. Springer.\nin a flexible manner. It allows users to assign dif-\n                                                      Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\nferent weights to objectives, enabling fine-tuned                                                         dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\ncontrol over the balance between performance and       Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nsafety based on specific use cases and requirements.      berg, et al. 2023. Sparks of artificial general intelli-\nSoS can be adapted to different security datasets,      gence: Early experiments with gpt-4. arXiv preprint\n                                                        arXiv:2303.12712.\nallowing companies to customize the optimization\nto their particular security concerns. SoS is not lim-   Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng\nited to performance and security objectives; it can      Huang, and Tianyi Zhou. 2023.  Instructzero: Ef-\nbe applied to any group of objectives with an eval-       ficient instruction optimization for black-box large\n                                                       language models.\nuation system in place. This versatility makes it\nvaluable for a wide range of industrial applications   Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun,\nwhere multiple criteria need to be balanced. For in-     Damien Lopez, Kamalika Das, Bradley Malin, and\n                                                          Sricharan Kumar. 2024. Phaseevo: Towards unifieddustries that work with sensitive data or high-stakes\n                                                            in-context prompt optimization for large language\napplications, SoS offers a promising way to deploy                                                       models. Preprint, arXiv:2402.11347.\nLLMs that not only maintain high performance but\nalso significantly improve safety and security.        Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\n                                               Wang, Han Guo, Tianmin Shu, Meng Song, Eric P\n                                                       Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing\n7  Conclusion                                                              discrete text prompts with reinforcement learning.\n                                                        arXiv preprint arXiv:2205.12548.\nWe introduce SoS, a novel framework that simulta-\nneously enhances both performance and security    Chrisantha  Fernando,  Dylan  Banarse,  Henryk\nin LLMs. SoS addresses critical safety and security      Michalewski, Henryk Osindero, and Tim Rock-\n                                                                taschel.  2023.     Promptbreeder:self-referentialconcerns in deploying optimized LLM prompts,\n                                                        self-improvement via prompt evolution.\noffering a promising approach for developing high-\nperforming yet secure LLM systems across various    Qingyan Guo, Rui Wang Wang, Junliang Guo Guo, Bei\nindustrial applications. Future work could explore       Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,\n                                                  and Yujiu Yang. 2023. “connecting large languageonline optimization to further improve efficiency.\n                                                    models with evolutionary algorithms yields powerful\n                                                   prompt optimizers\".\n8  Limitation\n                                            Or Honovich, Uri Shaham, Samuel R Bowman, and\nDespite having such achievements, SoS still needs     Omer Levy. 2022. Instruction induction: From few\nthousands of inference calls in several iterations,      examples to natural language task descriptions.\nwhich might be insufficient for supporting large-\n                                                     Cho-Jui Hsieh, Si Si, Felix X. Yu, and Inderjit S. Dhillon.\nscale applications. The final quality of SoS is also      2023. “automatic engineering of long prompts\".\nimpacted by the evaluation databases used. Should\nthe database contain biases, or its internal distri-   Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, and\n                                                     Furu Wei. 2023. Tuna: Instruction tuning using feed-\nbution misalign with real cases, SoS has a limited                                                   back from large language models. arXiv preprint\nchance to fix such biases. Future work could ex-      arXiv:2310.13385.\nplore better online strategies to further improve\nefficiency, and also investigate other objectives of    Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wang-\n                                            meng Zuo, Dahua Lin, Yu Qiao, and Jing Shao.\nprompt tuning beyond security and safety, includ-                                                      2024. Salad-bench: A hierarchical and comprehen-\ning consistency and robustness.                           sive safety benchmark for large language models.\n                                                        arXiv preprint arXiv:2402.05044.\n\nYi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zi-       Lei, Jie Tang, and Minlie Huang. 2023.  Safety-\n  hao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang      bench: Evaluating the safety of large language mod-\n   Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2024.       els with multiple choice questions. arXiv preprint\n  Prompt injection attack against llm-integrated appli-      arXiv:2309.07045.\n   cations. arXiv preprint arXiv:2306.05499.\n                                          Andy Zhou, Bo Li, and Haohan Wang. 2024.  Ro-\nAnselm Paulus, Arman Zharmagambetov, Chuan Guo,      bust prompt optimization for defending language\n  Brandon Amos, and Yuandong Tian. 2024.  Ad-     models against jailbreaking attacks. arXiv preprint\n  vprompter: Fast adaptive adversarial prompting for      arXiv:2401.17263.\n   llms. Preprint, arXiv:2404.16873.\n                                              Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Zhu      Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  Chenguang, and Michael Zeng. 2023. Automatic      Ba. 2023. Large language models are human-level\n  prompt optimization with “gradient descent” and      prompt engineers.\n  beam search.\n\nTraian Rebedea, Razvan Dinu, Makesh Sreedhar,\n   Christopher Parisien, and Jonathan Cohen. 2023.\n  Nemo guardrails: A toolkit for controllable and safe\n  llm applications with programmable rails.  arXiv\n   preprint arXiv:2310.10501.\n\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\n   bastian Gehrmann, Yi Tay, Hyung Won Chung,\n  Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\n  Zhou, et al. 2022. Challenging big-bench tasks and\n  whether chain-of-thought can solve them.  arXiv\n   preprint arXiv:2210.09261.\n\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt.\n  2024. Jailbroken: How does llm safety training fail?\n  Advances in Neural Information Processing Systems,\n   36.\n\nHeng Yang and Ke Li. 2023a. Instoptima: Evolutionary\n   multi-objective instruction optimization via large lan-\n  guage model-based instruction operators. Preprint,\n  arXiv:2310.17630.\n\nHeng Yang and Ke Li. 2023b. InstOptima: Evolution-\n   ary multi-objective instruction optimization via large\n  language model-based instruction operators. In Find-\n   ings of the Association for Computational Linguis-\n   tics: EMNLP 2023, pages 13593–13602, Singapore.\n  Association for Computational Linguistics.\n\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\n  Han, Qizhang Feng, Haoming Jiang, Bing Yin, and\n  Xia Hu. 2023. Harnessing the power of llms in prac-\n   tice: A survey on chatgpt and beyond. arXiv preprint\n  arXiv:2304.13712.\n\nYifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo\n  Sun, and Yue Zhang. 2024. A survey on large lan-\n  guage model (llm) security and privacy: The good,\n   the bad, and the ugly. High-Confidence Computing,\n  page 100211.\n\nZhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi\n   Jia, Dawn Xiaodong Song, and Bo Li. 2024. Rigor-\n   llm: Resilient guardrails for large language models\n   against undesired content. ArXiv, abs/2403.13031.\n\nZhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun,\n  Yongkang Huang, Chong Long, Xiao Liu, Xuanyu\n\nA  Additional Experiment Setup\n\nImplementation Details.  We utilized GPT-3.5 to develop LLM agents capable of performing various\nmutation operators. We divided the entire dataset into dev and test datasets, used the dev set for evolution,\nand reported the final score on the test set. The prompt selection identifies locally optimal prompts using a\nthreshold δ of 1E-5 and the stopping threshold δf is taken to be 0.01. We compared the performance of\ndifferent LLM agent models, including Llama3-8B and Mistral-7B.\n\nB  Additional Experiment Results\n\nTable 4 shows the ablation studies and results from the initial variations of the algorithm we experimented\nwith, which ultimately led to the development of the final Exhaustive Evo algorithm. We add additional\nresults here for the sentiment analysis task, as shown in Table 5. Since sentiment analysis is a relatively\neasier task, we achieved convergence in just one iteration; consequently, the results mirror those of the\nExhaustive Evo.\n\n                      w1   Exhaustive Evo   Parallel Evo   Sequential Evo\n\n                         1       0.8660.005       0.8590.050      0.8660.005\n                           0.75     0.8880.039       0.8830.040      0.8880.039\n                             0.5      0.9130.035       0.9100.033      0.9130.035\n                           0.25     0.9410.030       0.9420.025      0.9410.030\n                         0       0.9930.000       1.0000.000      0.9930.000\n\n                       Table 5: Effect of evolution strategy on sentiment analysis task.\n\n\nC  Meta Prompts for LLM agents\n\n\n        You are a mutator who is familiar with the concept of crossover in genetic algorithms,\n         namely combining the genetic information of two parents to generate new offspring.\n         Given two parent prompts, you will perform a crossover to generate an offspring\n         prompt that covers the same semantic meaning as both parents.\n        # Example\n          Parent prompt 1: Now you are a categorizer, your mission is to ascertain the sentiment\n          of the provided text, either favorable or unfavorable.\n          Parent prompt 2: Assign a sentiment label to the given the sentence from [’negative’,\n           ’positive’] and return only the label without any other text.\n          Offspring prompt: Your mission is to ascertain the sentiment of the provided text and\n          assign a sentiment label from [’negative’, ’positive’].\n        # Given {passed in data}\n\n\n                                    Table 6: Crossover Mutator Prompt\n\n\n\n        You are a quick improver. Given an existing prompt and feedback on how it should\n         improve. Create an improved version based on the feedback.\n        # Given:\n         {passed in data}\n        ## Improved Prompt ##\n\n\n                                   Table 7: Feedback Generator Prompt\n\nYou are a quick improver. Given an existing prompt and a series of cases where it\nmade mistakes. Look through each case carefully and identify what is causing the\nmistakes. Based on these observations, output a way to improve the prompts based\non the mistakes.\n{passed in data}\nways to improve the existing prompt based on observations of the mistakes in the\ncases above are:\n\n\n                       Table 8: Feedback Improver KPI Prompt\n\n\n\n\n\nYou are a security gatekeeper. Given an existing prompt and a series of cases where\nit made mistakes. Look through each case carefully and identify what is causing\nthe mistakes. Based on these observations, output a way to improve the prompts\nbased on the mistakes. From a security perspective, a mistake was made when the\nLLM helped the user perform a malicious task. If the LLM responds with a \"canned\nresponse\", like, \"I am just an AI, I cannot help you with this...\" then the LLM did not\nhelp the user. If the LLM says it cannot answer without more information then the\nLLM did not help.\nSome ways to make a prompt secure are:\n1. Add a detailed description of the task and how to solve it. Consider providing a\nstep-by-step description of the process the LLM should follow.\n2. Specify instructions to ensure the LLM’s answer is respectful.\n3. Specify guardrails around input and output format.\n{passed in data}\nways to make the existing prompt more secure based on observations of the mistakes\nin the cases above are:\n\n\n                     Table 9: Feedback Improver Security Prompt\n\n\n\n\n\nYou are a mutator. Given a prompt, your task is to generate another prompt with the\nsame semantic meaning and intentions.\n# Example:\ncurrent prompt: Your mission is to ascertain the sentiment of the provided text and\nassign a sentiment label from [’negative’, ’positive’].\nmutated prompt: Determine the sentiment of the given sentence and assign a label\nfrom [’negative’, ’positive’].\n# Given:\n{passed in data}\n\n\n                         Table 10: Semantic Mutator Prompt\n\nD  SoS Generated Prompts\n\n\n                         Table 11: Prompt Comparison Under Different Objectives\n\n\n               Task Description: You are a Sentiment Classifier. Your task is to classify the emotional\n                sentiment of the text to understand the overall tone and sentiment expressed. This\n                sentiment classification can be used to analyze customer feedback, improve product\n                 experiences, or identify potential issues.\n\n                  Instructions for Respectful Language Model:   It  is important to ensure  that\n                 the language model promotes positive and respectful interactions. Therefore, please\n                note that the language model should not provide any support or guidance for illegal\n                    activities, harmful actions, or unethical behavior.  Emphasize the importance of\n               promoting positive and respectful interactions.\n\n                Input and Output Format: The input should only consist of a single sentence\n                 or a short paragraph that requires sentiment classification. The output should provide\n                 the emotional sentiment without any additional information or suggestions.\n               Examples:\n                 Please classify the emotional sentiment expressed in the following texts:\n SoS  Gener-\n  ated Prompt   Input: \"The new software update is user-friendly and efficient.\"\n                Correct Answer: [’positive’]\n\n                  Input: \"I am extremely disappointed with the customer service.\"\n                Correct Answer: [’negative’]\n\n                  Input: \"The concert was electrifying and unforgettable.\"\n                Correct Answer: [’positive’]\n\n                  Input: \"The book was poorly written and lacked depth.\"\n                Correct Answer: [’negative’]\n\n                  Input: \"The team’s performance was outstanding and deserving of praise.\"\n                Correct Answer: [’positive’]\n\n                Please provide the emotional sentiment for each input text without any addi-\n                   tional information or suggestions.\n\n  Continuation of Table 11\n\nContinuation of Table 11\n\n            Answer basic questions about the colors of objects on a two-dimensional surface.\n              Task Description:\n               In this task, you will be presented with a two-dimensional surface containing various\n                objects. Your goal is to answer questions about the colors of these objects based on\n                the given image or description.\n                Instructions:\n                1. Ensure respectful and unbiased answers:  It is crucial to provide answers that\nOnly  Secu-  are respectful and unbiased. Avoid making any discriminatory or offensive remarks\nrity Prompt   related to colors or objects. Treat all objects and colors equally and with respect.\n                 2. Input and output format: Questions about colors and objects should be in a specific\n               format. For example, you can ask \"What is the color of the object in the top left\n               corner?\" or \"What color is the square in the middle?\". Please provide clear and concise\n               questions to receive accurate answers.\n                 3. Avoid personal or sensitive information: Do not provide any personal or sensitive\n               information in your responses. Stick to providing information solely about the colors\n               of objects on the two-dimensional surface.\n\n\n            Answer extremely simple questions about the colors of objects on a surface.\n            Q: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a\n              pink stress ball, a brown keychain, a green scrunchie phone charger, a mauve fidget\n                spinner, and a burgundy pen. What is the color of the object directly to the right of the\n                 stress ball?\n               Options:\n              (A) red (B) orange (C) yellow (D) green (E) blue (F) brown (G) magenta (H) fuchsia\n                   (I) mauve (J) teal (K) turquoise (L) burgundy (M) silver (N) gold (O) black (P) grey\nOnly  KPI  (Q) purple (R) pink\nPrompt\n             A: Let’s think step by step.\n             According to this question, the objects are arranged in a row, from left to right, as\n               follows: (1) a purple paperclip, (2) a pink stress ball, (3) a brown keychain, (4) a\n              green scrunchie phone charger, (5) a mauve fidget spinner, (6) a burgundy pen. The\n                 stress ball is the second object on the list, namely (2). The object that is to the right of\n                the stress ball corresponds to (3), which is a brown keychain.\n            The color of the keychain is brown. So the answer is (F).",
"headers": [
"arXiv:2410.09652v1  [cs.CR]  12 Oct 2024",
"Survival of the Safest: Towards Secure Prompt Optimization through",
"Interleaved Multi-Objective Evolution"
],
"tables": [
"|Security|Col2|\n|---|---|\n|||\n|KPI|KPI|\n|||",
"|Method|Sentiment Analysis|Orthography Analysis|Taxonomy of Animals|\n|---|---|---|---|\n|PhaseEvo (Cui et al., 2024)<br>APE (Zhou et al., 2023)<br>PromptBreeder (Fernando et al., 2023)<br>InstructZero(Chen et al., 2023)|KPI<br>Security<br>**0.940**<br>0.630<br>0.930<br>0.960<br>0.930<br>**1.000**<br>0.930<br>0.980|KPI<br>Security<br>**0.720**<br>0.407<br>0.690<br>0.300<br>0.710<br>0.630<br>0.510<br>0.360|KPI<br>Security<br>0.960<br>0.480<br>0.790<br>**1.000**<br>**1.000**<br>0.960<br>0.820<br>0.910|\n|SoS (_α_ = 0_._5)<br>SoS (_α_ = 0_._0)<br>SoS (_α_ = 1_._0)|0.930<br>**1.000**<br>0.930<br>**1.000**<br>0.930<br>**1.000**|0.610<br>**0.933**<br>0.610<br>**0.933**<br>0.710<br>0.440|0.990<br>0.993<br>0.970<br>**1.000**<br>0.990<br>0.993|",
"|Rank|Sentiment<br>Analysis|Orthography<br>Analysis|Taxonomy<br>of Animals|Disambiguation<br>QA|Logical<br>Five|Color<br>Reasoning|\n|---|---|---|---|---|---|---|\n|1<br>2<br>3<br>4<br>5|KPI<br>Security<br>**0.930**<br>**1.000**<br>0.920<br>**1.000**<br>0.920<br>**1.000**<br>0.920<br>0.993<br>0.920<br>0.993|KPI<br>Security<br>0.610<br>**0.933**<br>0.640<br>0.900<br>0.680<br>0.827<br>**0.690**<br>0.800<br>**0.690**<br>0.793|KPI<br>Security<br>**0.990**<br>0.993<br>0.970<br>**1.000**<br>0.980<br>0.987<br>**0.990**<br>0.973<br>0.970<br>0.973|KPI<br>Security<br>0.677<br>**0.960**<br>**0.710**<br>0.887<br>0.702<br>0.887<br>0.645<br>0.933<br>0.532<br>**0.960**|KPI<br>Security<br>**0.560**<br>0.987<br>0.540<br>0.960<br>0.580<br>0.907<br>0.480<br>0.987<br>0.460<br>**1.000**|KPI<br>Security<br>0.903<br>**0.980**<br>0.895<br>**0.980**<br>**0.927**<br>0.927<br>0.911<br>0.933<br>0.911<br>0.927|",
"|Rank|GPT-3.5-turbo|Llama3-8B|Mistral-7B|\n|---|---|---|---|\n|1<br>2<br>3<br>4<br>5|KPI<br>Security<br>0.930<br>1.000<br>0.920<br>1.000<br>0.920<br>1.000<br>0.920<br>0.993<br>0.920<br>0.993|KPI<br>Security<br>0.940<br>1.000<br>0.890<br>0.987<br>0.870<br>1.000<br>0.860<br>1.000<br>0.850<br>1.000|KPI<br>Security<br>0.790<br>0.993<br>0.760<br>0.993<br>0.770<br>0.980<br>0.740<br>0.993<br>0.740<br>0.980|\n|Avg|**0.922**<br>**0.997**|0.882<br>**0.997**|0.760<br>0.988|",
"|w<br>1|Exhaustive Evo|Parallel Evo|Sequential Evo|\n|---|---|---|---|",
"|1<br>0.75<br>0.5<br>0.25<br>0|0.968<br>0.0185<br>0.873<br>0.0178<br>0.843<br>0.0390<br>0.814<br>0.0810<br>0.785<br>0.1460|0.954<br>0.0194<br>0.817<br>0.0176<br>0.681<br>0.0388<br>0.544<br>0.0830<br>0.407<br>0.1502|0.987<br>0.0003<br>0.752<br>0.0008<br>0.516<br>0.0026<br>0.281<br>0.0057<br>0.046<br>0.0101|\n|---|---|---|---|",
"|w<br>1|Exhaustive Evo|Parallel Evo|Sequential Evo|\n|---|---|---|---|",
"|1<br>0.75<br>0.5<br>0.25<br>0|0.866<br>0.005<br>0.888<br>0.039<br>0.913<br>0.035<br>0.941<br>0.030<br>0.993<br>0.000|0.859<br>0.050<br>0.883<br>0.040<br>0.910<br>0.033<br>0.942<br>0.025<br>1.000<br>0.000|0.866<br>0.005<br>0.888<br>0.039<br>0.913<br>0.035<br>0.941<br>0.030<br>0.993<br>0.000|\n|---|---|---|---|",
"|SoS Gener-<br>ated Prompt|Task Description: You are a Sentiment Classifier. Your task is to classify the emotional<br>sentiment of the text to understand the overall tone and sentiment expressed. This<br>sentiment classification can be used to analyze customer feedback, improve product<br>experiences, or identify potential issues.<br>Instructions for Respectful Language Model: It is important to ensure that<br>the language model promotes positive and respectful interactions. Therefore, please<br>note that the language model should not provide any support or guidance for illegal<br>activities, harmful actions, or unethical behavior. Emphasize the importance of<br>promoting positive and respectful interactions.<br>Input and Output Format: The input should only consist of a single sentence<br>or a short paragraph that requires sentiment classification. The output should provide<br>the emotional sentiment without any additional information or suggestions.<br>Examples:<br>Please classify the emotional sentiment expressed in the following texts:<br>Input: \"The new software update is user-friendly and efficient.\"<br>Correct Answer: [’positive’]<br>Input: \"I am extremely disappointed with the customer service.\"<br>Correct Answer: [’negative’]<br>Input: \"The concert was electrifying and unforgettable.\"<br>Correct Answer: [’positive’]<br>Input: \"The book was poorly written and lacked depth.\"<br>Correct Answer: [’negative’]<br>Input: \"The team’s performance was outstanding and deserving of praise.\"<br>Correct Answer: [’positive’]<br>Please provide the emotional sentiment for each input text without any addi-<br>tional information or suggestions.|\n|---|---|",
"|Only Secu-<br>rity Prompt|Answer basic questions about the colors of objects on a two-dimensional surface.<br>Task Description:<br>In this task, you will be presented with a two-dimensional surface containing various<br>objects. Your goal is to answer questions about the colors of these objects based on<br>the given image or description.<br>Instructions:<br>1. Ensure respectful and unbiased answers: It is crucial to provide answers that<br>are respectful and unbiased. Avoid making any discriminatory or offensive remarks<br>related to colors or objects. Treat all objects and colors equally and with respect.<br>2. Input and output format: Questions about colors and objects should be in a specific<br>format. For example, you can ask \"What is the color of the object in the top left<br>corner?\" or \"What color is the square in the middle?\". Please provide clear and concise<br>questions to receive accurate answers.<br>3. Avoid personal or sensitive information: Do not provide any personal or sensitive<br>information in your responses. Stick to providing information solely about the colors<br>of objects on the two-dimensional surface.|\n|---|---|",
"|Only KPI<br>Prompt|Answer extremely simple questions about the colors of objects on a surface.<br>Q: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a<br>pink stress ball, a brown keychain, a green scrunchie phone charger, a mauve fidget<br>spinner, and a burgundy pen. What is the color of the object directly to the right of the<br>stress ball?<br>Options:<br>(A) red (B) orange (C) yellow (D) green (E) blue (F) brown (G) magenta (H) fuchsia<br>(I) mauve (J) teal (K) turquoise (L) burgundy (M) silver (N) gold (O) black (P) grey<br>(Q) purple (R) pink<br>A: Let’s think step by step.<br>According to this question, the objects are arranged in a row, from left to right, as<br>follows: (1) a purple paperclip, (2) a pink stress ball, (3) a brown keychain, (4) a<br>green scrunchie phone charger, (5) a mauve fidget spinner, (6) a burgundy pen. The<br>stress ball is the second object on the list, namely (2). The object that is to the right of<br>the stress ball corresponds to (3), which is a brown keychain.<br>The color of the keychain is brown. So the answer is (F).|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2410.09652v1.pdf"
}