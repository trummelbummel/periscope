{
"text": "AMPO: Automatic Multi-Branched Prompt Optimization\n\n            Sheng Yang1*†  Yurong Wu1*†  Yan Gao2‡  Zineng Zhou3†  Bin Benjamin Zhu2\n                      Xiaodi Sun2  Jian-Guang Lou2‡  Zhiming Ding1  Anbang Hu2\n                    Yuan Fang2  Yunsong Li2  Junyan Chen2  Linjun Yang2\n                         1Institute of Software, CAS & University of Chinese Academy of Sciences\n                                                   2Microsoft\n                 3Institute of Computing Technology, CAS & University of Chinese Academy of Sciences\n                        {yangsheng22, wuyurong20, zhouzineng22}@mails.ucas.ac.cn;\n                           {yan.gao, binzhu, sunstifler, jlou, anbhu, juliefang, yunsongli,\n                       Junyan.Chen, Yang.Linjun}@microsoft.com; zhiming@iscas.ac.cn\n                           Abstract\n\n                Prompt engineering is very important to en-\n                 hance the performance of large language mod-\n                     els (LLMs). When dealing with complex is-\n                   sues, prompt engineers tend to distill multi-2024            ple patterns from examples and inject rele-\n                   vant solutions to optimize the prompts, achiev-\n                  ing satisfying results. However, existing auto-Oct            matic prompt optimization techniques are only\n11            limitedstrugglingto producingwith handlingsinglediverseflow instructions,patterns. In        Figure 1: Our prompt optimization approach aims to\n                     this paper, we present AMPO, an automatic         iteratively optimize a single flow of instructions into a\n                prompt optimization method that can itera-       multi-branched format to handle multiple patterns.\n                     tively develop a multi-branched prompt using\n                     failure cases as feedback. Our goal is to ex-        ration (Zhou et al., 2022; Yang et al., 2023; Schn-\n                   plore a novel way of structuring prompts with       abel and Neville, 2024). These studies generally[cs.CL]            multi-branches to better handle multiple pat-      employ LLMs as prompt optimizers to progres-\n                     terns in complex tasks, for which we introduce        sively enhance prompts for target models. Particu-\n                   three modules: Pattern Recognition, Branch                                                                               larly, one promising paradigm is feedback-based\n                  Adjustment, and Branch Pruning. In experi-\n                                                                optimization methods, where LLM-based prompt\n                ments across five tasks, AMPO consistently\n                                                               optimizers act like human experts by analyzing                  achieves the best results. Additionally, our ap-\n                 proach demonstrates significant optimization       and fixing failed cases (Pryzant et al., 2023; Wang\n                    efficiency due to our adoption of a minimal        et al., 2023b; Ye et al., 2023). These methods ex-\n                  search strategy.                                         plicitly leverage the self-reflection ability of LLMs\n                                                                    for prompt refinement and mirrors the steps of\n          1  Introduction                                                                  “gradient” in the direction of error descent and then\n            Prompt Engineering involves creating the best pos-   “propagate” to the prompt. Currently, this paradigm\n               sible prompts to enhance the performance of large    of prompt optimization has achieved promising ad-\n            language models (LLMs). It has become increas-   vancements and widespread interest.arXiv:2410.08696v1              ingly significant in the development of LLM appli-      Despite the success of feedback-based optimiza-\n              cations (Brown et al., 2020; Welleck et al., 2022).    tion methods, we find that the optimized prompts\n             Creating suitable prompts frequently demands con-   are primarily achieved by meticulously rewriting\n              siderable human effort, specialized knowledge,    certain steps, attempting to provide more details.\n            and numerous trial-and-error attempts (Zamfirescu-    Essentially, they are mainly a form of single flow\n              Pereira et al., 2023). Therefore, investigating effi-    instructions. As shown in Figure 1, when deal-\n              cient automatic prompt engineering techniques is    ing with complex issues, categorizing problems in\n              crucial (Zhang et al., 2022; Chen et al., 2023).      advance can help LLM analyze issues more sys-\n                In recent times, automatic prompt optimization    tematically and easier to find appropriate solutions\n            methods based on LLMs have seen extensive explo-   (Thomas et al., 2023; Ren et al., 2024; Mao et al.,\n                                                              2023).  It is evident that an LLM optimizer gen-\n                  *Equal contribution.\n                   †This work was done during an internship at Microsoft.     erating a single flow instructions struggles with\n                  ‡Corresponding author.                            handling various patterns. For example, current\n\nfeedback-based methods either fail to provide ef-   plex tasks.\nfective fixes or cause numerous regressions when      (3) We conduct experiments across five tasks.\nfailed cases are not addressed by the single flow  AMPO consistently outperforms other feedback-\ninstructions (Ma et al., 2024).                     based prompt optimization methods. Meanwhile,\n  Given the complexity of real-world scenarios,   experimental results show that AMPO exhibits\nwe contend that a single flow of instructions is in-   impressive optimization efficiency due to our adop-\nadequate. The LLM optimizer should identify mul-    tion of a minimal search strategy.\ntiple patterns from failure cases, which in turn al-\nlows for the exploration and development of more   2  Motivating Example\nadaptable prompt structures. Motivated by this, we                                           To illustrate the importance of recognizing and han-\npropose AMPO, a prompt optimization method                                                     dling different patterns when dealing with complex\nthat moderately transforms a prompt into a multi-                                                          issues, we take search query understanding task as\nbranched format and then optimizes the prompt                                              an example, In this task, we employ LLM to predict\nusing failure cases as a guide. The key aspect of                                                 a personalized query intent using the current query\nthis process is to create an adaptive prompt struc-                                            and user’s search history as input. As the saying\nture that aligns with the complexity of the task.                                              goes “there are a thousand Hamlets in a thousand\nTo achieve this, we introduce three modules: (1)                                                   people’s eyes”, predicting personalized intents is\nPattern Recognition is responsible for analyzing                                                       quite difficult. According to expert experience, the\nthe pattern of the failure cases. (2) Branch Adjust-                                                    best approach is to first categorize users’ search\nment can adaptively choose between adding the                                                 behaviors into distinct classes and then develop a\nbranches to address the new pattern and providing                                                       strategy for predicting personalized intent for each\nmore details to enhance the existing branches. (3)                                                      category. For example, if the current query is a re-\nBranch Pruning takes both pre-pruning and post-                                                      finding query—identical to a previous query—the\npruning techniques to prevent the prompt from                                           most likely intent is to revisit the same webpage\noverfitting. The advantage of our multi-branched                                           from the user’s recent clicks. If the current query\nprompt structure is that it allows the target LLM                                                                  is a reformulation query derived from any previous\nto autonomously determine the most appropriate                                                      queries, the intent should be personalized based\npath for each sample during inference, thereby en-                                          on the nature of the reformulation, such as an ex-\nhancing its ability to handle diverse patterns and                                               pansion or filtering of the original query. When\ncomplexities.                                                     the query is a domain preference query, where the\n  We conduct experiments across five tasks, en-                                                      user’s history indicates a preference for specific\ncompassing various levels of complexity. Our ex-                                            domains related to the current query, we should\nperimental results show that AMPO consistently                                                        refine the query intent using this domain prefer-\nachieves the best results across five tasks from Gen-                                                    ence. Inspired by this, AMPO introduces multiple\neral NLU and Domain Knowledge. Additionally,                                               branches to handle various categories, adaptively\ncompared with other feedback-based optimization                                              expanding or pruning them to achieve the most\napproaches, our approach exhibits impressive opti-                                                    appropriate coverage.\nmization efficiency due to our adoption of a mini-\nmal search strategy.                         3  Related Work\n  Our contributions are as follows:\n                                                    3.1  The Structure of Prompting Instructions   (1) We introduce AMPO, an automatic prompt\noptimization method that can iteratively construct a   Prompts significantly influences the performance\nmulti-branched prompt using failure cases as feed-   of LLMs (White et al., 2023; Liu et al., 2023). Nu-\nback. Notably, AMPO is currently the first-known   merous studies confirm that breaking down ques-\nprompt optimization method designed to generate    tions into multiple intermediate steps can markedly\nmulti-branched prompt.                         enhance the quality of outputs (Wei et al., 2022),\n   (2) To the best of our knowledge, we are the first    particularly in tasks requiring reasoning (Wang and\nto explore structuring prompts with multi-branches   Zhou, 2024; Yasunaga et al., 2023) and planning\nto better handle multiple patterns in complex tasks.   (Wang et al., 2023a; Zhang et al., 2023). To further\nOur results show that AMPO can automatically   improve these intermediate steps, many researchers\nconstruct an effective multi-branched prompt from    utilize the model’s in-context learning abilities (Ko-\nthe data distribution, thereby accommodating com-   jima et al., 2022; Shum et al., 2023). While these\n\nprompt design methods enhance LLMs’ reason-           InputInput                    Ini  alizer\n                                                                                  Input                                                   Ini  al Prompt\ning and generation capabilities, they still rely on         AnswerAnswerOutput           (LLM Agent)\na sequential, single-flow structure, which remains\ninadequate for more complex situations. In this\n                                                                                       Prompt         Failed Cases\nwork, we want to explore a novel way of structur-          Training Set            Evaluator\ning prompts with multi-branches to better handle\nmultiple patterns in complex tasks. We hypothe-          Prompt            Best Prompt              Pa ern\nsized that a multi-branched prompt structure could           Evaluator               (only 1)               Recogni on\nallow the target LLM to autonomously determine                                                         Pa erns\nthe most appropriate path for each sample during             Mul -branched         Mul -branched\n                                                                             Prompts      Branch    Prompts      Branch\ninference.                                                                                   Pruning               Adjustment                                                                                      itera on\n\n3.2  Automatic Prompt Optimization                                                             Figure 2: Overall framework of AMPO\nAdvances in LLMs have led to numerous studies\n                                               This can be formally defined as an optimizationexploring automatic prompt optimization technolo-\n                                               problem: P ∗= argmaxP∈S PiR(pβ(ai|qi, P)),gies (Ye et al., 2023; Schnabel and Neville, 2024).\n                                           where S denotes the sample space for a naturalThese studies often fall into two categories: using\n                                                language prompt.search algorithms (Guo et al., 2023; Zhou et al.,\n2022) or leverage the self-reflection capabilities                                                    4.1  Overview\nof LLMs (Pryzant et al., 2023; Yang et al., 2023)\n                                         The goal of AMPO is to iteratively develop ato identify optimal prompts. Recently, there have\n                                                  multi-branched prompt using failure cases as feed-been attempts to combine both approaches by em-\n                                                  back. The key aspect of this process is generatingploying search algorithms like Monte Carlo tree\n                                             an adaptive prompt structure that aligns with thesearch (MCTS) integrated with self-reflection ca-\n                                               complexity of the task. To achieve this, our ap-pabilities (Wang et al., 2023b). In this combined\n                                               proach mainly consists of three modules: (1) Pat-approach, each prompt is treated as a state, and\n                                                    tern Recognition is responsible for analyzing theeach optimization as an action, enabling more re-\n                                                       patterns of failure cases. (2) Branch Adjustment isfined prompt optimization through traceable tree\n                                                     responsible for balancing the adaptability betweensearch. Our approach falls under the self-reflection\n                                                adding new branches to address emerging patternscategory.  The key difference between AMPO\n                                            and enhancing existing branches based on recog-and existing methods is that AMPO considers\n                                                   nized patterns. A proper balance is crucial for adap-the prompt’s structure as an optimization objec-\n                                                          tive prompt structure. (3) Branch Pruning takestive. Traditional methods typically treat prompts\n                                               both pre-pruning and post-pruning techniques toas a single block of text, optimizing them as a\n                                                   prevent the prompt from overfitting, further ensur-whole. In contrast, AMPO identifies multiple pat-\n                                                   ing the adaptivity of prompt structure. The overallterns from error cases and refines the prompt into a\n                                            framework of AMPO is shown in Figure 2.multi-branched structure by adding new branches\n                                            At the beginning, an initial prompt P0 is gen-or enhancing existing ones. This multi-branched\n                                                    erated based on few-shot examples (Zhou et al.,structure enables LLMs to handle tasks with highly\n                                                 2022)1. Then, we start to iteratively optimize thediverse data distributions more effectively.\n                                            prompt by using failed cases as feedback. Details\n4  Methodology                                 of the algorithm can be found in Algorithm 1. Be-\n                                                   low, we provide a detailed introduction to each\nGiven a task τ specified by a provided training   module.\nset as Dtrain =  {(q1, a1), (q2, a2)..., (qn, an)},\n                                                    4.2  Pattern Recognitionwhere qi/ai are input/output pairs from each entry,\nwe start with an initial prompt P0. The model input   Given the current prompt P and K bad examples\nconsists of P and qi, and the base LLM β makes   drawn from the training set Dtrain, the goal of\nthe prediction based on pβ(ai|qi, P). The goal of    this module is to perform error analysis to uncover\nprompt optimization is to find the optimal prompt\n                                                         1Our approach can also optimize human-written prompts.\nP ∗that maximizes the performance towards a                                                              In this paper, we use LLM-based prompt initialization to\nmeasure function R (e.g., accuracy) over Dtrain.     facilitate human effort.\n\nAlgorithm 1 AMPO                                  for each pattern. In our paper, we select top N pat-\nRequire: Initial prompt: P0, Training set: Dtrain, Valida-    terns by importance scores. It enables the selection\n    tion set: Dval, Iteration limit: T                       of important patterns during the branch adjustment\n 1:\n 2:  // LLM agents                                    process, further reducing the number of explored\n 3: Analyzer: Conduct error analysis.                   prompts and thereby improving efficiency. The\n 4: Summarizer: Condense error reasons into patterns and   meta prompt for the LLM-Summarizer is provided\n    assign important scores.\n 5: Revisor: Edit the multi-branched prompt.                 in Appendix A.4.\n 6:\n 7: Initiate P = P0                                    4.3  Branch Adjustment\n 8: while t ≤T do\n 9:     Evaluate P on Dtrain and sample K failed cases   The goal of the branch adjustment module is to\n  E = {(xi, yi)|(xi, yi) ∈Dtrain ∧LLM(xi; p) ̸= yi}    optimize a multi-branched prompt based on the\n10:       // Pattern Recognition\n11:    LLM-Analyzer conducts error analysis and iden-   summarized patterns from the LLM-Summarizer.\n     tifies the error reasons for each failed case: R =     It is critical to clarify whether a pattern should\n    {r1, r2, . . . rK}                                             be used to enhance an existing branch or create12:    LLM-Summarizer condenses R reasons into M pat-\n    terns and assign important scores from each pattern:   a new one. The optimal choice depends on the\n   Patt = {(patt1, s1), . . . (pattM, sM)}                  specific task. For instance, more complex tasks are\n13:       // Branch Adjustment\n                                                        likely to exhibit a wider range of patterns, mak-14:     Selects top N patterns by important scores.\n15:    LLM-Revisor optimizes P based on top N Patt and    ing a multi-branched prompt preferable due to its\n    obtained optimized prompts Popt = {P1, . . . PN}.         scalability and capacity to handle various patterns.\n16:       // Branch Pruning\n                                                  Conversely, for simpler tasks, single-flow instruc-17:    LLM-Revisor prunes the optimized prompts Popt and\n    obtained Ppruned = {P1, . . . PN}                      tions are more effective, as they are easier to follow\n18:     Evaluate N  pruned  prompts  Ppruned  =   and more robust.\n   {P1, . . . PN} on Dval and  select the best prompt\n   P ∗.\n19:    Update P = P ∗                             Adaptivity between adding the branches to ad-\n20: end while                                     dress the new pattern and providing more de-\n21: return The best prompt.                                 tails to enhance the existing branches  In this\n                                             module, we employ an LLM agent, referred to\n                                                   as \"LLM-Revisor,\" to modify the multi-branched\nthe root causes of each bad example. To facilitate    prompt. Specifically, we define two types of opera-\nthis, we have created step-by-step instructions for    tions for the LLM-Revisor: (1) adding branches to\nan LLM agent named LLM-Analyzer. The meta    address new patterns and (2) providing additional\nprompt for LLM-Analyzer is provided in Appendix    details to enhance existing branches. By analyzing\nA.3. By analyzing the output reasons, we found    the existing branches and new pattern, the LLM-\nthat the reasons produced by LLM are often similar,   Revisor determines whether to expand the prompt\neven though the failed cases differ.  This leads    in depth (by adding more details) or in breadth (by\nto low optimization efficiency of feedback-based   adding more branches). The advantage of this ap-\nmethods. Furthermore, in AMPO, similar feedback   proach lies in its flexibility, allowing for the prompt\ncan lead to redundant branches within the prompt,    structure to be tailored to the task’s complexity and\nultimately affecting its performance (as shown in    pattern distribution. The step-by-step instructions\nSection 5.5).                                         for the LLM-Revisor are provided in Appendix\n  To address the aforementioned issue, we employ   A.5.\nanother LLM agent (named as LLM-Summarizer),\n                                                    4.4  Branch Pruningto summarize the causes of all failed cases into\ndifferent patterns for each iteration. Incorporating   By thoroughly examining the edition process of\nthis summarization offers several benefits: (1) It    the LLM-Revisor, we observed that the multi-\nreduces repetitive reasons, significantly enhanc-   branched prompt is susceptible to overfitting. Typ-\ning the efficiency of optimization compared to    ically, after several rounds of iteration, the per-\nother feedback-based methods.  (2) Summariz-   formance of the prompt on the test set begins to\ning reasons into patterns improves generalizabil-   decline. Meanwhile, the prompt appears to have\nity, thereby minimizing redundant branches in the   more and more branches perceptually. This hap-\nmulti-branched prompt. Additionally, we ask the   pens when the LLM memorizes corner cases in the\nLLM-Summarizer to assign an importance score    training data and fails to pick up essential patterns.\n\nInspired by pruning techniques in machine learn-   5.2  Tasks\ning (Esposito et al., 1997; Kwon et al., 2022), we                                         To thoroughly evaluate the effectiveness of our\npropose two possible solutions: (1) pre-pruning                                          method across a wide range of applications, we\nprevents the prompt from further growth by early                                                     carefully select 5 tasks from different domains\nstopping. After each iteration, we check the cross-                                                      for in-depth experimentation:  the well-known\nvalidation error.  If the error does not decrease                                                         text classification task TREC (Voorhees and Tice,\nsignificantly then we stop. By pruning early, we                                                  2000), the widely recognized sentiment classifica-\nobtain a more streamlined prompt that is less prone                                                        tion task SST-5 (Socher et al., 2013) and the large-\nto overfitting the training data. (2) post-pruning                                                       scale reading comprehension task RACE (Lai et al.,\ndoes the opposite of pre-pruning and allows the                                                   2017). Additionally, we chose two domain-specific\nprompt to grow to its full depth. Particularly, we                                                     tasks from the biomedical field, which explicitly\nadd a step at the end of the meta prompt for the                                                      require domain insights when crafting expert-level\nLLM-Revisor to review the entire set of instruc-                                                prompts, namely the medical question-answering\ntions again and delete any branches to enhance the                                                     tasks MedQA (Jin et al., 2021) and MedMCQA\ninstructions’ generalization ability.                                                     (Pal et al., 2022). Detailed dataset information is\n                                                      available in Appendix A.1.\n\n5  Experimental                                                    5.3  Implementation Details\n\n                                                    In our study, we utilize GPT-3.5-turbo and GPT-4-\n5.1  Baselines\n                                                  turbo as the target model, with GPT-4-turbo serv-\n                                                  ing as the optimizer. To comprehensively captureHuman Instruction: Human prompts are the in-\n                                                      potential error feedback, we set the temperaturestructions crafted by humans based on their un-\n                                               parameter of the Analyzer to 1. For the Revisor,derstanding and cognition of the original dataset.\n                                    we set the temperature to 0 to ensure precision inFor each task, we take instructions written by do-\n                                                    the edits. We sample K=5 bad cases and selectmain experts, sourced from scholarly databases\n                                                    the top N = 1 pattern for LLM-Revisor to op-or professional websites, and make corresponding\n                                                      timize. Therefore, we keep only one prompt tomodifications to adapt them to our tasks.\n                                                                iterate. Throughout the iterative processes of APO,\n  Chain-of-Thought (CoT) (Wei et al., 2022) ap-\n                                            PromptAgent and AMPO, we sample 10% of the\npends \"Let’s think step by step.\" after the question\n                                                        training data as the validation set to assess prompt\nto trigger the model’s reasoning process.\n                                                        effectiveness. In our experiments, we run each ex-\n  Chain-of-Thought Instructions (CoT Instruc-   periment three times and report the average of the\ntions) (Wei et al., 2022): We randomly select 5    results evaluated on the test set.\ncases and generate corresponding thought chains\nby few-shot learning. After that, we manually op-   5.4  Main Results\ntimize the prompts to enhance performance. This    In Table 1, we present a comparison of prompts\nprompt is also used as the initial prompt for other   generated by AMPO against Human Instruction,\nfeedback-based prompt optimization methods.      CoT, CoT Instructions, APO, OPRO, and Promp-\n APO (Pryzant et al., 2023): APO generates nat-   tAgent across five tasks in three domains. We ob-\nural language-level gradients from incorrect exam-   served that our method significantly outperforms\nples, and then utilizes these gradients to reverse-   accuracy in all tasks, which validates the effective-\nedit the prompt.                                   ness of our approach in optimizing prompts.\n\n OPRO (Yang et al., 2023): OPRO leverages                          AMPO significantly surpasses other methods\nhistorical prompts, scores, and error examples                                                   in complex tasks  Taking the MedQA task as\nto guide the LLM in generating higher-scoring                                             an example, this task includes various conditions\nprompts. Unlike APO, OPRO does not provide                                            and complex situations. Therefore, when address-\nexplicit feedback during the optimization process.                                                 ing such issues,  it is crucial for LLMs to iden-\n  PromptAgent (Wang et al., 2023b): PromptA-    tify different patterns based on the specific con-\ngent utilizes the Monte Carlo Tree Search (MCTS)    ditions of patients and provide the most suitable\nalgorithm to strategically optimize the prompting    treatment plan. As shown in Table 1, whether\nprocess.                                         using GPT-3.5-turbo or GPT-4-turbo, the human-\n\nGeneral NLU               Domain Knowledge\n LLMs        Methods\n                                 SST-5     TREC     RACE    MedQA   MedMCQA\n\n             Human              51.56         69.60         79.25         61.25         45.75\n             CoT                 50.00         63.00         77.75         50.50         47.25\n    GPT-3.5    CoT Instructions      49.56         67.75         78.25         68.25         48.25\n      -turbo    APO                52.00         69.00         78.00         72.25         48.00\n            OPRO               52.44         70.50         78.75         70.50         46.75\n                PromptAgent         54.22         72.50         80.75         71.75         47.50\n           OURS           55.78↑1.56   76.00↑3.50   81.75↑1.00   76.50↑4.25   48.75↑0.50\n\n             Human              52.34         70.50         89.75         64.50         65.75\n             CoT                 53.86         63.50         88.50         63.50         69.75\n             CoT Instructions      50.33         71.25         91.00         71.75         70.75\n  GPT-4-turbo\n           APO                55.25         75.25         90.75         83.25         71.50\n            OPRO               56.44         79.50         90.00         76.50         66.00\n                PromptAgent         57.33         81.50         91.00         77.00         70.25\n           OURS           59.78↑2.45   82.00↑0.50   91.25↑0.25   89.00↑5.75   73.00↑1.50\n\nTable 1: Performance comparison of GPT-3.5-turbo and GPT-4-turbo across five tasks, highlighting the highest\naccuracy results in bold. The up arrow indicates the amount by which OURS exceeds the second-highest score.\n\n\n  Model           MedQA  TREC   SST-5      shot examples both perform well. Then, the im-\n AMPO                 89.00     82.00    59.78\n      - Summarization       86.75     81.50    57.00      provements brought by using feedback-based meth-\n    ∆                -2.25%   -0.50%   -2.78%     ods like APO and PromptAgent are minimal, or\n      - Enhance existing                           86.25     78.25    55.33      even detrimental. For example, on the RACE task,\n       branches\n    ∆                -2.75%   -3.75%   -4.45%      the APO-optimized prompt performs 0.25% lower\n      - Add new branches    82.25     77.75    53.78      than the initial prompt (i.e., CoT-Instructions) with\n    ∆                -6.75%   -4.25%   -6.00%     GPT-4-turbo as the target model.  Meanwhile,\n                            AMPO consistently surpassed other methods andTable 2: The ablation study results of AMPO without\nsummarization, enhancing existing branches and adding    achieved state-of-the-art performance. It indicates\nnew branches. The exact match score is reported.         that our method performs well even under normal\n                                                  circumstances. This means that our method can\nconstructed prompt performs the worst, possible    flexibly create an adaptive multi-branched prompt\nbecause the instructions are general and do not   from the data distribution, thereby adapting to com-\nalign closely with the input information. Then,   plex or normal tasks.\nprompts generated through few-shot examples per-\n                                                    5.5  Ablation Studyform better. Next, when supplemented with failure\ncases as feedback, APO, OPRO and PromptAgent   To systematically study the effects of the pattern\ncan further improve the performance. Finally, our   summarization, adaptivity of multi-branched and\nmethod achieved relative improvements of 24.50%,   branch pruning in AMPO, we conduct thorough\n25.50%, 17.25%, 5.75% and 12.00% compared to    ablation experiments across three tasks. The results\nthe other methods (i.e., Human Instruction, CoT,    are shown in the Table 2.\nCoT-Instructions, APO, OPRO, and PromptAgent)\n                                         Summarization  For each iteration of AMPO,\nusing GPT-4-turbo. Because our method offers a\n                                    we incorporate the LLM-Summarizer to summa-\nmulti-branched prompt to handle complex situa-\n                                                        rize the error reasons of all K sampled failed\ntions by categorizing the problems first rather than\n                                                  cases into M patterns. By removing the LLM-\nusing a single flow to address all issues. The ex-\n                                              Summarizer, the error reasons produced by LLM-\nperimental results demonstrate that AMPO signifi-\n                                               Analyzer are directly fed into the LLM-Revisor in\ncantly surpasses other methods in complex tasks.\n                                                    batch. According to the results in the Table 2, in\n                                                    the MedQA, TREC, and SST-5 tasks, the perfor-\nAMPO also outperforms the other methods   mance respectively decreased by 2.25%, 0.50%,\nin normal tasks  Take a relatively normal task   and 2.78%. It indicates that it is crucial to summa-\nRACE as an illustration. As we can see from Ta-    rize the error reasons to common patterns before\nble 1, the prompts constructed by human and few-    revising.\n\n0.95\n                                              Random                       0.7950                                                      Ours                    0.90  SST-5\n                               0.8200                                                    0.85                                                           TaskMedQA\n                                                                                                                                   MedMCQA\n                                                                                                            0.80                                                          TREC\n                                                                                                                                                                             SST-5                                             0.8625                                                                                                                                                 RACE                                                                                                            0.75\n MedQA\n                                                                                                                                Method                                                      0.8900                         Accuracy0.70                                                                                                                                           APO\n                                                                                                                                                                  PromptAgent\n       0.75       0.78       0.81       0.84       0.87       0.90                    0.65                                                             AMPO-No-S\n                          Accuracy                                                                                        AMPO\n                                                                                                            0.60\n\nFigure 3: Comparison between our pattern selection           0.55\nstrategy and the random strategy on MedQA and SST-               0          50         100        150        200        250\n                                                                              The Number of Explored Prompts\n5.\n\n                                                      Figure 4: Exploration efficiency analysis. Our method\nAdaptive Adjustment  One of the significant in-   achieved the best results with the fewest exploration\nnovations of AMPO is its adaptivity between ex-   prompts. The horizontal axis represents the number\npanding new branches and enhancing the existing    of intermediate exploratory prompts, while the vertical\nbranches of a multi-branched prompt, making it    axis represents accuracy. Here, AMPO-No-S refers to\n                               AMPO without the Summarizer.necessary to validate the importance of this fea-\nture through ablation experiments.  Specifically,        0.90      AMPO\n                                                                                              PromptAgent\nwe modified the meta prompt of LLM-Revisor by                  APO\n                                                                                        CoT\nremoving options to enhance existing branches or         0.85\nadding new branches. The experimental results\nshow that the performance significantly decreases         0.80                                                                                                                                                   Accuracyby an average of 5.67% across three tasks without\nadding new branches. Notably, by removing the         0.75\naddition operation of new branches, AMPO would\nactually degrade into APO. When LLM-Revisor         0.70\n                                                                        1        2        3        4        5        6\ncan only add new branches, the performance also                                  Iteration\ndecreases by an average of 3.65%, but still remains\nhigher than enhancing existing branches-only by              Figure 5: Convergence Analysis\n3.03%. Through this study, we have arrived at two\n                                                    6.2  Exploration Efficiency Analysisconclusions: (1) both adding new branches and\nenhancing existing ones are crucial for develop-   Exploration efficiency is critical to reduce the com-\ning an adaptive multi-branched prompt; (2) adding    putation cost. We thus compare the number of\nnew branches is more significant than enhancing    explored prompts between our method and three\nexisting ones when handling complex tasks.          strong baselines. In Figure 4, ours achieves the best\n                                                          results with the fewest exploration prompts across\n6  Analysis                                       various tasks. Take the MedQA task as an example,\n                                     we performed 5 iterations for 4 different methods,\n6.1  Pattern Selection Strategy\n                                                      calculating the number of prompts they generated.\nIn our experimental design, we start by randomly   For each iteration, APO generates 3 prompts from\nsampling K=5 bad cases and analyzing them with   each of the 4 prompts of the previous iteration, and\nthe LLM-Analyzer, without specifying a fixed num-    selects the 4 prompts with the best performance on\nber of causes. After that, all identified reasons are    the validation set for the next iteration, resulting\nhanded over to the LLM-Summarizer, which con-    in 240 exploration prompts. PromptAgent, using\nsolidates them into several main categories. Mean-   an MCTS Search strategy(Winands et al., 2008)\nwhile, we ask the LLM-Summarizer to assign an   with a depth of 3 and a breadth of 3, produces a\nimportance score for each main reason. After that,    total of 52 prompts after 5 iterations. Compared\nwe select the top N=1 most critical pattern by im-    to PromptAgent and APO, AMPO uses 6.4 times\nportant scores. Figure 3 shows that our search strat-   and 48 times fewer explored prompts respectively,\negy has improved by 2.75% over random sampling    yet achieved performance improvements of 12%\non average, which demonstrates the effectiveness   and 5.75%. The high efficiency of our method\nof our pattern selection strategy.                         is mainly due to three reasons: (1) We employ a\n\nQuestion\n\n  A 67-year-old man is brought to the emergency department with severe, crushing, retrosternal chest pain for the last 45 minutes. The\n  pain radiates to his left shoulder and is S4 and bilateral ... started on dopamine, morphine, nitroglycerin, and streptokinase. Which of the\n  following would be the most concerning development over the next 24h this patient?\n\n    AMPO-Optimized Prompt\n\n    1. Read the input carefully to understand the context and specifics of the scenario:\n  - If the scenario is clinical, focus on the patient's condition and any special circumstances (like pregnancy).\n  - Else if the scenario is non-clinical, such as a biochemical experiment, ensure comprehension of the scientific principles involved.\n   2. Identify key information.\n     ...\n   3. Consider the context:\n  - If it's a clinical scenario, ... If the scenario is diagnostic in nature and suggests an embryologic error, ... .\n  - Else if the scenario is treatment-oriented, proceed to the next steps with a focus on treatment selection.\n  - Else if it's a non-clinical scenario, ... .\n   4. Review options based on the context:. ...- Else if treating, review treatment options and prioritize based on safety and\n   effectiveness. ...- If the case involves enzyme kinetics, provide specific guidance on interpreting data such as Km and Vmax.\n   5. If multiple options seem appropriate:- If in a clinical case, ....\n   6. Make a decision:\n  - If in a clinical scenario, ... .\n  - If diagnosing, ... .\n  - Else if in a non-clinical scenario, choose the conclusion or next step that best fits the experimental data and scientific knowledge.\n\n       Analysis                                                                                   Options\n     Firstly, this scenario is clinical, dealing with a patient who presents with severe, retrosternal             (A) Hemoptysis\n    chest pain, radiating to the left shoulder and associated with sweating-clear indications of an          (B) Easy fatigability\n   acute myocardial infarction. ... .treatment-oriented, since immediate medical interventions              (C) Persistent ST-segment elevation\n   have been applied and their outcomes must be monitored closely. ... . So the answer is A.               (D) Shortness of breath\n\n\nFigure 6: Here is an example from MedQA where the AMPO-optimized prompt led the LLM to make the correct\nchoice, while other methods all failed. Intuitively, the multi-branched prompt generated by AMPO has branches at\ndifferent steps, considering various conditions with if-else statements. Compared to a single flow instruction, it can\nhandle a wider variety of patterns, thereby achieving better performance. We use different colors to highlight various\njudgment conditions of our prompts. From this example, the multi-branched prompt first guides the LLMs to\ndifferentiate problems into clinical and non-clinical, further judge whether they are diagnostic or treatment-oriented,\nwhile also assessing whether the patient is suitable for treatment. For cases involving enzyme kinetics, it will guide\nthe LLMs to provide insights specific to that field.\n\ngreedy search strategy, meaning that in each iter-   new branches besides modifying existing ones, al-\nation, only one best prompt is retained. (2) Our   lowing it to handle more situations and reducing\nLLM-Summarizer condenses all the error reasons    the difficulty of modifying original prompts.\nfrom failure cases into several patterns. (3) Ad-\nditionally, the LLM-Summarizer assigns an im-   6.4  Case Study\nportant score for each pattern, allowing the LLM-   To illustrate how AMPO utilizes multi-branched\nRevisor to filter the patterns, which further reduces   prompts to solve issues, we conducted a qualita-\nthe number of exploration prompts.                    tive analysis. By using an example from MedQA,\n                                     we demonstrated that our approach can accurately\n6.3  Convergence analysis                        categorize complex scenarios from intricate data.\n                                          Then it can design detailed solutions for each case,\nTo further investigate the learning process of\n                                                     ultimately leading to the correct answer. From the\nAMPO, we monitored and visualized the perfor-\n                                                      analysis result of this example, we can see that the\nmance changes of prompts over each round of the\n                              LLM first judges the patient’s condition as clini-\nprocess. Specifically, we recorded and plotted the\n                                                      cal and finds that there are clear indications of an\nperformance trajectories of all baselines across six\n                                                  acute myocardial infarction. Next, it judges the\nrounds in the MedQA task, illustrating the evolu-\n                                                       situation as treatment-oriented and finally arrives\ntion of the prompt optimization methods’ perfor-\n                                                            at the correct answer.\nmance in Figure 5. We observed that all three opti-\nmization methods showed an overall upward trend,                                       7  Conclusion\nbut AMPO’s increase is notably greater, jumping\ndirectly from 71.25% to 83.75% only one iteration.   In  this paper, we proposed Automatic Multi-\nThe multi-branched prompt has better scalability   Branched Prompt Optimization (AMPO), which\nand a larger capacity to handle different patterns.    explicitly converts a prompt into a multi-branched\nUnlike other methods, our approach can expand   format and then iteratively refines it using fail-\n\nure cases as feedback.  Specifically, we employ    Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\nthree LLM agents working in tandem and propose       taka Matsuo, and Yusuke Iwasawa. 2022.  Large\n                                                      language models are zero-shot reasoners. Advancesguiding principles to balancing the adaptability\n                                                                in neural information processing systems, 35:22199–\nof the multi-branched structure. Experimental re-      22213.\nsults demonstrate that AMPO outperforms exist-\ning state-of-the-art feedback-based optimization   Woosuk Kwon, Sehoon Kim, Michael W Mahoney,\napproaches while significantly improving optimiza-      Joseph Hassoun, Kurt Keutzer, and Amir Gholami.\n                                                      2022. A fast post-training pruning framework for\ntion efficiency.                                                            transformers. Advances in Neural Information Pro-\n                                                           cessing Systems, 35:24101–24116.\n8  Limitations\n                                           Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nMulti-branched prompt  optimization  requires                                                   and Eduard Hovy. 2017. RACE: Large-scale ReAd-\nLLMs to possess strong logical reasoning abilities.      ing comprehension dataset from examinations. In\nTo reduce complexity, we have adopted the divide      Proceedings of the 2017 Conference on Empirical\n                                                   Methods in Natural Language Processing, pages 785–and conquer approach, designing three roles: LLM-\n                                                       794, Copenhagen, Denmark. Association for Com-\nAnalyzer, LLM-Summarizer, and LLM-Revisor.                                                              putational Linguistics.\nAdditionally, we have designed step-by-step meta\ninstructions to guide how to generate an adaptive    Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nprompt structure to accommodate tasks of varying      Hiroaki Hayashi, and Graham Neubig. 2023. Pre-\n                                                                      train, prompt, and predict: A systematic survey ofdifficulties. Despite this, our method still depends\n                                                    prompting methods in natural language processing.\non the capabilities of the LLMs themselves. Due to    ACM Computing Surveys, 55(9):1–35.\ncurrent limitations in their abilities, there are times\nwhen the models may not strictly follow the meta    Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan\ninstructions, leading to suboptimal results. How-     Du, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024.\n                                                  Are large language models good prompt optimizers?\never, by utilizing better LLMs in the future, we can                                                       arXiv preprint arXiv:2402.02101.\nfurther enhance the effectiveness of our method.\n                                                 Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou,\n                                               Haonan Chen, and Hongjin Qian. 2023. Large lan-\nReferences                                       guage models know your contextual search intent:\n                                 A prompting framework for conversational search.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie      arXiv preprint arXiv:2303.06573.\n   Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n   Neelakantan, Pranav Shyam, Girish Sastry, Amanda                                                   Ankit Pal, Logesh Kumar Umapathi, and Malaikan-\n   Askell, et al. 2020. Language models are few-shot                                                  nan Sankarasubbu. 2022. Medmcqa: A large-scale\n   learners. Advances in neural information processing                                                           multi-subject multi-choice dataset for medical do-\n   systems, 33:1877–1901.                                                main question answering. In Conference on health,\n                                                               inference, and learning, pages 248–260. PMLR.\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng\n  Huang, and Tianyi Zhou. 2023.  Instructzero: Ef-\n                                                 Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\n   ficient instruction optimization for black-box large\n                                                 guang Zhu, and Michael Zeng. 2023. Automatic\n   language models.\n                                                  prompt optimization with\" gradient descent\" and\n                                             beam search. arXiv preprint arXiv:2305.03495.Floriana Esposito, Donato Malerba, Giovanni Semeraro,\n  and J Kay. 1997. A comparative analysis of methods\n                                              Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi   for pruning decision trees. IEEE transactions on\n                                                    Cheng, Junfeng Wang, Dawei Yin, and Chao Huang.   pattern analysis and machine intelligence, 19(5):476–\n                                                      2024. Representation learning with large language   491.\n                                                  models for recommendation. In Proceedings of the\n                                   ACM on Web Conference 2024, pages 3464–3475.Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\n  Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\n   jiu Yang. 2023. Connecting large language models    Tobias Schnabel and Jennifer Neville. 2024. Prompts\n   with evolutionary algorithms yields powerful prompt       as programs: A structure-aware approach to efficient\n   optimizers. arXiv preprint arXiv:2309.08532.           compile-time prompt optimization. arXiv preprint\n                                                       arXiv:2404.02319.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\n  Hanyi Fang, and Peter Szolovits. 2021. What disease   KaShun Shum, Shizhe Diao, and Tong Zhang. 2023.\n  does this patient have? a large-scale open domain      Automatic prompt augmentation and selection with\n   question answering dataset from medical exams. Ap-      chain-of-thought from labeled data. arXiv preprint\n   plied Sciences, 11(14):6421.                           arXiv:2302.12822.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason    Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\n  Chuang, Christopher D Manning, Andrew Y Ng,      Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\n  and Christopher Potts. 2013. Recursive deep mod-     Denny Zhou. 2023. Large language models as ana-\n   els for semantic compositionality over a sentiment       logical reasoners. arXiv preprint arXiv:2310.01714.\n   treebank. In Proceedings of the 2013 conference on\n   empirical methods in natural language processing,   Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and\n  pages 1631–1642.                                       Fereshte Khani. 2023. Prompt engineering a prompt\n                                                             engineer. arXiv preprint arXiv:2311.05661.\nPaul Thomas, Seth Spielman, Nick Craswell, and\n                                         JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern  Bhaskar Mitra. 2023. Large language models can ac-\n                                                    Hartmann, and Qian Yang. 2023. Why johnny can’t   curately predict searcher preferences. arXiv preprint\n                                                     prompt: how non-ai experts try (and fail) to design  arXiv:2309.10621.\n                                                      llm prompts. In Proceedings of the 2023 CHI Con-\n                                                          ference on Human Factors in Computing Systems,Ellen M Voorhees and Dawn M Tice. 2000. Building a\n                                                      pages 1–21.   question answering test collection. In Proceedings\n   of the 23rd annual international ACM SIGIR confer-\n                                             Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu\n  ence on Research and development in information\n                                                       Ding, Joshua B Tenenbaum, and Chuang Gan. 2023.\n   retrieval, pages 200–207.\n                                                        Planning with large language models for code gener-\n                                                                  ation. arXiv preprint arXiv:2303.05510.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\n  Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.    Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\n  2023a. Plan-and-solve prompting: Improving zero-      urmans, and JosephE. Gonzalez. 2022.  Tempera:\n   shot chain-of-thought reasoning by large language       Test-time prompting via reinforcement learning.\n  models. arXiv preprint arXiv:2305.04091.\n                                              Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai,      Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P      Ba. 2022. Large language models are human-level\n  Xing,  and  Zhiting Hu. 2023b.    Promptagent:      prompt engineers. arXiv preprint arXiv:2211.01910.\n   Strategic planning with language models enables\n   expert-level prompt optimization.  arXiv preprint  A  Appendix\n  arXiv:2310.16427.\n                                            A.1  Data split\nXuezhi Wang and Denny Zhou. 2024.   Chain-of-\n   thought reasoning without prompting. arXiv preprint    In our experimental setup, tasks are organized\n  arXiv:2402.10200.                                   into two primary categories: General NLU (Natu-\n                                                             ral Language Understanding) and Domain Knowl-\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten                                                  edge.\n  Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\n                                                  Within the General NLU category, we have three   et al. 2022. Chain-of-thought prompting elicits rea-\n   soning in large language models. Advances in neural    tasks: SST-5, TREC, and RACE. Each task is al-\n   information processing systems, 35:24824–24837.     located 100 training samples and 50 evaluation\n                                                samples. For the test sets, SST-5 comprises 450\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\n                                                samples, while TREC and RACE each have 400\n  man, Tianxiao Shen, Daniel Khashabi, and Yejin\n  Choi. 2022. Generating sequences by learning to    samples.\n   self-correct. arXiv preprint arXiv:2211.00053.          For the Domain Knowledge category, there are\n                                          two tasks: MEDQA and MEDMCQA. Like the\nJules White, Quchen Fu, Sam Hays, Michael Sandborn,                                                General NLU tasks, each of these tasks is assigned\n  Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse\n                                           100 training samples. However, the table indicates  Spencer-Smith, and Douglas C Schmidt. 2023. A\n  prompt pattern catalog to enhance prompt engineer-    that there are 50 evaluation samples for these tasks\n   ing with chatgpt. arXiv preprint arXiv:2302.11382.   samples for these tasks and 400 test samples.  It\n                                               should be noted that there are some formatting\nMark HM Winands, Yngvi Björnsson, and Jahn-Takeshi                                                    inconsistencies in the Eval and Test columns for\n   Saito. 2008. Monte-carlo tree search solver. In Com-\n                                                      the MEDQA and MEDMCQA tasks which need to   puters and Games: 6th International Conference,\n  CG 2008, Beijing, China, September 29-October 1,   be addressed for clarity. For detailed information,\n  2008. Proceedings 6, pages 25–36. Springer.          please refer to Table 3.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,   A.2  Different Pattern Results\n  Quoc V Le, Denny Zhou, and Xinyun Chen. 2023.\n  Large language models as optimizers. arXiv preprint  We explored the impact of selecting the top N\n  arXiv:2309.03409.                                  patterns on the results. As evidenced by the table,\n\nType              Task         Train  Eval  Test\n\n\n                   SST-5       100    50    450\n\n\n General NLU\n               TREC       100    50    400\n\n\n\n               RACE       100    50    400\n\n\n\n             MEDQA     100    50    400\n\n Domain Knowledge\n\n             MEDMCQA  100    50    400\n\n\n       Table 3: Experimental Data Distribution\n\n\n                                                    AMPO\n\n     0.58\nAccuracy0.560.54\n\n\n     0.52\n\n\n     0.50\n        1          2          3          4          5\n                           Pattern\n\nFigure 7: The performance of choosing different num-\nbers of top patterns on SST-5 task. We use GPT-3.5-\nturbo as the target model.\n\n\naccuracy peaks when N = 5, while it is lowest at N\n= 4, exhibiting an overall oscillatory trend within\na reasonable range. Consequently, to enhance our\nefficiency, we opted for N = 1.\n\nA.3  LLM-Analyzer Meta-Prompt\n\nA.4  LLM-Summarizer Meta-Prompt\n\nA.5  LLM-Revisor Meta-Prompt\n\n---ProblemStart---\nI have some instructions for a specific problem:\n---InstructionsStart---\n{{initial_prompt}}\n---InstructionsEnd---\n\nBut it gets the following cases wrong:\n---BadCasesStart---\n{{bad_examples}}\n---BadCasesEnd---\n\nYour task is to identify the underlying causes for my [# Instructions] as an analyzer. Please follow these\nsteps:\n(1) Identify what perspectives there are to consider for my problem. Please think as comprehensively as\npossible, considering all aspects.\n(2) Based on these potential perspectives you identified, analyze the pattern of the failed cases.\n(3) Carefully review each step of my [# Instructions] and identify which step neglects the key information\nin the pattern, resulting in failure.\n(4) Write your reasons and wrap each reason with <START>and <END>.\n\n\n                                         Table 4: LLM-Analyzer\n\n\n\n\n\n---ProblemStart---\nI have some instructions for a specific problem:\n---InstructionsStart---\n{{initial_prompt}}\n---InstructionsEnd---\n\nHere are some reasons why my current instructions cannot solve some problem :\n---Reasons---\n{{Reasons}}\n---Reasons---\n\nYour task is to summarize the many reasons provided above into a few major categories and assign an\nimportant score for each category. Be careful to eliminate repetitive and similar reasons. Each\nsummarized pattern should be wrapped with <START>and <END>.\n\n\n                                       Table 5: LLM-Summarizer\n\n---ProblemStart---\nYou have some instructions for a specific task:\n---InstructionsStart---\n{{initial_prompt}}\n---InstructionsEnd---\n\nHowever, due to the complexity of real-world situations, a single flow of instructions (i.e., sequential\ninstructions) cannot apply to all cases. Therefore, you should transform the instructions into a conditional\napproach, which means adopting different instructions for different patterns.\n\nNotably, the key aspect of this process is to create an adaptive prompt structure, thereby accommodating\ntasks of varying difficulties. To achieve this, you should find the golden mean between adding the\nbranches to address the new pattern and providing more details to enhance the existing branches based on\nthe difficulty of your task and the distribution of recognized patterns.\n\nAn expert has pointed some patterns that you don’t considered before for your instructions:\n---ExpertAnalysisStart---\n{{patterns}}\n---ExpertAnalysisEnd---\n\nPlease optimize your [# Instructions] based on expert analysis step-by-step:\n(1) Carefully review each step of your instructions.\n\n(2) Identify the steps that went wrong due to a lack of key information mentioned in expert analysis.\n\n(3) For each suboptimal step, you have the following options:\n- 3.1 Consider improving the step to include the key information.\n- 3.2 Otherwise, you can also consider adding **sub-steps** using an **if** or **if-else** structure to\nhandle the **new** patterns. Ensure that each substep is specific and avoids vague instructions.\nNote that if a step needs to consider multiple situations, break it down into substeps to make it easier to\nfollow.\n\n(4) Include Tips or Cautions: If merely optimizing existing steps with branches like if-else does not\nsufficiently to address all aspects, add new tips or cautions to the current instructions to handle different\npatterns.\n\n(5) Maintain the other main steps unchanged from the initial prompt, in order to not lose information.\n\n(6) At last, review the whole steps and prune the branches to avoid the instructions overfitting.\n\nPlease only output the optimized prompt without anything else.\n\n\n                                         Table 6: LLM-Revisor",
"headers": [
"arXiv:2410.08696v1  [cs.CL]  11 Oct 2024",
"AMPO: Automatic Multi-Branched Prompt Optimization"
],
"tables": [
"|Col1|Input|Col3|Col4|\n|---|---|---|---|\n||p<br>Input<br>|||\n||Answer<br><br><br>Input|||\n||Answer<br>|||",
"|Col1|OURS|55.78↑1.56|76.00↑3.50|81.75↑1.00|76.50↑4.25|48.75↑0.50|\n|---|---|---|---|---|---|---|",
"|Col1|OURS|59.78↑2.45|82.00↑0.50|91.25↑0.25|89.00↑5.75|73.00↑1.50|\n|---|---|---|---|---|---|---|",
"|Col1|Col2|0.79|50|Col5|Ra<br>O|ndom<br>urs|\n|---|---|---|---|---|---|---|\n|SS~~T-~~5|||0.8200||||\n|MedQA||||0.|0.8<br>8625|900|\n|MedQA|||||||",
"|0.95|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n|0<br>50<br>100<br>150<br>200<br>250<br>The Number of Explored Prompts<br>0.55<br>0.60<br>0.65<br>0.70<br>0.75<br>0.80<br>0.85<br>0.90<br>0.95<br>Accuracy<br>Met<br>APO<br>Pro<br>AMP<br>AMP<br>Tas<br>Med<br>Med<br>TRE<br>SS~~T-~~<br>RAC|||||\n|0<br>50<br>100<br>150<br>200<br>250<br>The Number of Explored Prompts<br>0.55<br>0.60<br>0.65<br>0.70<br>0.75<br>0.80<br>0.85<br>0.90<br>0.95<br>Accuracy<br>Met<br>APO<br>Pro<br>AMP<br>AMP<br>Tas<br>Med<br>Med<br>TRE<br>SS~~T-~~<br>RAC||||Tas<br>Med<br>Med<br>TRE<br>SS~~T~~|\n|0<br>50<br>100<br>150<br>200<br>250<br>The Number of Explored Prompts<br>0.55<br>0.60<br>0.65<br>0.70<br>0.75<br>0.80<br>0.85<br>0.90<br>0.95<br>Accuracy<br>Met<br>APO<br>Pro<br>AMP<br>AMP<br>Tas<br>Med<br>Med<br>TRE<br>SS~~T-~~<br>RAC||||Met<br>APO<br>Pro<br>AMP<br>AMP<br><br>RAC|\n|0<br>50<br>100<br>150<br>200<br>250<br>The Number of Explored Prompts<br>0.55<br>0.60<br>0.65<br>0.70<br>0.75<br>0.80<br>0.85<br>0.90<br>0.95<br>Accuracy<br>Met<br>APO<br>Pro<br>AMP<br>AMP<br>Tas<br>Med<br>Med<br>TRE<br>SS~~T-~~<br>RAC|||||\n|0<br>50<br>100<br>150<br>200<br>250<br>The Number of Explored Prompts<br>0.55<br>0.60<br>0.65<br>0.70<br>0.75<br>0.80<br>0.85<br>0.90<br>0.95<br>Accuracy<br>Met<br>APO<br>Pro<br>AMP<br>AMP<br>Tas<br>Med<br>Med<br>TRE<br>SS~~T-~~<br>RAC|||||\n|0<br>50<br>100<br>150<br>200<br>250<br>The Number of Explored Prompts<br>0.55<br>0.60<br>0.65<br>0.70<br>0.75<br>0.80<br>0.85<br>0.90<br>0.95<br>Accuracy<br>Met<br>APO<br>Pro<br>AMP<br>AMP<br>Tas<br>Med<br>Med<br>TRE<br>SS~~T-~~<br>RAC|50<br>100<br>|150<br>200<br>25|150<br>200<br>25|150<br>200<br>25|",
"|AMPO|Col2|Col3|Col4|\n|---|---|---|---|\n|~~AMPO~~<br>PromptAgent<br>APO<br>CoT||||\n|||||\n|||||\n|||||\n|||||",
"|clini|cal|\n|---|---|",
"|non-|clini|cal|\n|---|---|---|",
"|hey are|Col2|Col3|dia|gnos|tic|or|treat|Col9|ment-|ori|ented|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|in|volv|ing|ing|enzyme|enzyme|ki|net|ic|s, it will guide|s, it will guide|s, it will guide|",
"|Col1|Col2|Col3|Col4|AM|AM|PO|\n|---|---|---|---|---|---|---|\n|||||AM|AM||\n||||||||\n||||||||\n||||||||\n||||||||"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2410.08696v1.pdf"
}