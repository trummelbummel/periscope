{
"text": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large\n                               Language Models\n\n                   Chenzhuo Zhao1*   Ziqian Liu2*  Xinda Wang1   Junting Lu1\n                                         Chaoyi Ruan3†\n                1Peking University   2Independent Researcher   3National University of Singapore\n                        {cyzcz, nev_settle, aidan.lew.37}@stu.pku.edu.cn\n                                     liuziqian25@gmail.com\n                                     ruancy@comp.nus.edu.sg\n\n\n\n                          Abstract\n\n               Prompt optimization is a practical and widely\n                  applicable alternative to fine tuning for im-2025                 proving large language model performance.\n                 Yet many existing methods evaluate candidate\n                prompts by sampling full outputs, often cou-Sep\n                  pled with self critique or human annotated pref-\n18             erences, which limits scalability, especially for                  smaller models or models that are not instruc-\n                   tion tuned. We present PMPO (Probabilistic\n                 Metric Prompt Optimization), a unified frame-\n               work that uses token level cross entropy as a\n                      direct, lightweight evaluation signal. PMPO lo-\n                   cates low quality prompt segments via a mask-       Figure 1: Overview of PMPO. Top: Iterative prompt[cs.CL]\n                  ing based analysis and iteratively rewrites them        refinement via loss-based evaluation. Bottom: Compar-\n                    to propose improved variants. Crucially, during         ative strengths over prior methods in terms of evaluation\n                   evaluation, PMPO selects among variants by         strategy, efficiency, model support, and task generality.\n                 minimizing loss in a single forward pass, elim-\n                   inating output sampling and human or judge\n                 based scoring for selection while still using        et al., 2024; Wang et al., 2023). Recent methods\n                  standard generation only to propose rewrites.                                                                   tackle this by iteratively refining prompts through\n                 This unified, loss based strategy supports both\n                                                     model feedback (Xiang et al., 2025), human pref-\n                  supervised and preference based tasks. Across\n                                                              erences (Cheng et  al., 2023), or reward-based               model sizes and datasets, PMPO outperforms\n                    prior prompt optimizers: it achieves the highest       search (Zheng et al., 2024). While effective, these\n                 average accuracy on BBH, performs strongly       approaches often face three main challenges: (1)\n               on GSM8K and AQuA RAT, and raises Al-       high cost due to output generation and evaluation\n                 pacaEval 2.0 win rates by over 19 points. These        loops, (2) reliance on large models capable of in-\n                     results demonstrate PMPO’s effectiveness, effi-arXiv:2505.16307v2                                                              trospection or multi-step reasoning, and (3) lack of\n                   ciency, and broad applicability.\n                                                                    generality across task types and model sizes.\n\n                                                  Some methods, such as PromptWizard (Agarwal\n                                                                            et al., 2024) and PromptAgent (Wang et al., 2023),\n          1  Introduction\n                                                                      rely on model-internal critique and iterative analy-\n           Prompt design has emerged as a critical factor     sis, achieving strong results on complex tasks but\n              in steering large language models (LLMs) to-   requiring significant computation and model coop-\n           ward reliable performance across diverse tasks.    eration. Others like BPO (Cheng et al., 2023) and\n          As fine-tuning becomes increasingly costly or re-  PAS (Zheng et al., 2024) improve efficiency via\n               stricted , automatic prompt optimization has be-    offline learning of prompt-rewriting models, but\n          come a practical alternative to improve model be-    require substantial labeled data or preference anno-\n             havior without modifying parameters (Agarwal    tations, and their applicability to new tasks or small\n                   *Contributed equally.                            models is limited. SPO (Xiang et al., 2025) avoids\n                  †Corresponding author.                          ground truth by using the model’s own judgment\n\nto compare outputs, but this self-evaluation is less    • Support for small models. PMPO requires only\nreliable for smaller LMs.                           forward passes and likelihoods, not introspec-\n A central limitation across these approaches is      tion or reasoning, making it usable for smaller\nhow prompt quality is evaluated. Most techniques    LMs that cannot critique their outputs or support\ntreat prompt evaluation as a generative task, where     complex prompting.\noutputs are generated and then scored externally,    • High sample efficiency. Since loss evaluation\nmaking the process computationally expensive and       is batchable and cheap, PMPO can explore more\nless reliable. External scoring often lacks consis-     candidate prompts under fixed budget constraints,\ntency or sufficient granularity, especially when re-     enabling extensive search with minimal over-\nlying on models to assess complex outputs. More-     head.\nover, methods tend to specialize: some focus on\nsupervised tasks with explicit labels, while others\ntarget alignment tasks defined by preferences or   2  Related Works\nstyle. Few offer a unified, efficient mechanism\n                                                    2.1  Prompt Engineering\napplicable to both.\n                                                 Early prompt engineering methods rely on man-   In this work, we introduce PMPO (Probabilistic\n                                                     ually crafted instructions and few-shot exemplarsMetric Prompt Optimization), a novel prompt opti-\n                                                      to guide language model behavior (Brown et al.,mization framework that directly uses the model’s\n                                                2020; Deng et al., 2023; Zheng et al., 2023c; Wangcross-entropy loss as the evaluation signal. PMPO\n                                                           et al., 2024b). Chain-of-Thought (CoT) prompt-frames prompt optimization as a classic loss min-\n                                                  ing (Wei et al., 2022) enhances this paradigm byimization problem: given a prompt and input, it\n                                                 encouraging step-by-step reasoning, leading to sub-computes the likelihood of the desired or preferred\n                                                           stantial gains on complex tasks. Other structuredoutput without requiring the model to generate or\n                                              prompting methods, such as Step-Back prompt-rank outputs explicitly. This evaluation is efficient,\n                                                  ing (Zheng et al., 2023a), introduce abstractionfully automated, and compatible with models of\n                                                   or meta-reasoning steps to improve logical infer-varying scales, making PMPO applicable to both\n                                                   ence.  Similarly, Rephrase-and-Respond (Denglarge and small language models in diverse deploy-\n                                                           et al., 2024) enhances model comprehension byment settings.  Moreover, PMPO supports both\n                                              prompting it to rephrase questions before answer-accuracy-based tasks (e.g., classification, QA) and\n                                                       ing. These techniques improve performance bypreference-based tasks (e.g., summarization, in-\n                                                    enriching intermediate reasoning or input interpre-struction following) under a unified loss-based eval-\n                                                           tation, but they are based on fixed templates anduation strategy: it minimizes cross-entropy loss for\n                                          do not perform automated prompt generation ortasks with labeled outputs and maximizes the like-\n                                                     optimization.lihood of preferred responses over less preferred\n                                                     In  contrast,  another  line of work exploresones when comparative preference information is\n                                                automatic prompt construction.  Methods likeavailable.\n                                  APE (Shen et al., 2023) treat prompt design as a\n  Figure 1 presents an overview of the PMPO re-\n                                                    search problem, using language models to generate\nfinement loop and summarizes its advantages over\n                                             and rank instruction variants based on task perfor-\nexisting approaches. Compared to prior methods,\n                                             mance. While these approaches reduce manual ef-\nPMPO offers the following key advantages:\n                                                                 fort, they often rely on large models for generation\n• Loss-based evaluation. Unlike prior methods,\n                                              and evaluation, limiting scalability in low-resource\n  PMPO’s evaluation of candidate prompts re-\n                                                     or small-model settings. Overall, this body of work\n  quires no output sampling or human evaluation,\n                                                  underscores the importance of prompt formulation\n  relying only on a single forward pass to com-\n                                             and motivates the development of more systematic\n  pute log-likelihoods, which makes its evaluation\n                                             and efficient optimization frameworks.\n  process highly efficient.\n                                                    2.2  Prompt Optimization• General  applicability.    PMPO   unifies\n  preference-based and supervised settings by   Recent work on automated prompt optimization\n  treating prompt optimization as maximizing   can be broadly categorized into generation-based\n  either output likelihood or reward, depending on   and introspection-based methods (Yan et al., 2024;\n  the task signal available.                  Wang et al., 2024a; Guo12 et al.; Zhou et al.,\n\nFigure 2: Overview of the PMPO framework. Step 1: Mask Analysis estimates the impact of each prompt part\nby measuring loss changes when masked, identifying positive, negative, or neutral effects. Step 2: Evaluation\nand Generation selects hard cases and generates refined candidates, scored with the same loss-based metric. The\nbest-performing prompt is retained. Step 3: Iterative Refinement repeats this process until stop.\n\n\n2022). OPRO (Yang et al., 2023) treats the op-  SPO (Xiang et al., 2025) and TextGuard (Pei et al.,\ntimization process as a black-box loop, where   2023) reduce reliance on labels by comparing out-\nan LLM proposes new prompts based on previ-   puts or generating improvement suggestions, yet\nous attempts and their performance (e.g., accu-     still depend on costly output sampling or heuristic\nracy). EvoPrompt (Tong et al., 2025) and Prompt-   reward models.\nBreeder (Fernando et al., 2023) further adopt evo-\nlutionary strategies—maintaining populations of\nprompts that evolve through mutation and selec-   3 PMPO Framework\ntion, with PromptBreeder uniquely co-evolving its\n                                  We introduce PMPO, a unified framework designed\nmutation strategies. While these approaches of-\n                                                      to iteratively refine prompts using fine-grained\nten achieve strong performance, they rely on re-\n                                                   cross-entropy evaluation and adaptive rewriting\npeated output generation and scoring, which makes\n                                                        strategies (see Figure 2). PMPO systematically\nthem resource-intensive and task-specific. More-\n                                                 analyzes prompts by applying a model-adaptive\nover, their effectiveness depends heavily on large\n                                             masking technique to quantify token-level impacts\nLLMs serving as optimizers, limiting applicability\n                                                  (Step 1, detailed in Section3.2), and selectively\nto smaller models or open-ended preference-based\n                                                       rewrites the lowest-performing segments using lan-\ntasks.\n                                              guage model-generated variants (Step 2, described\n  Another direction leverages the model’s own in-    in Sections3.3 and3.4). Through an iterative cycle\ntrospective signals to refine prompts. PromptWiz-   of prompt generation, evaluation, and refinement\nard (Agarwal et al., 2024) uses model-internal    (Step 3, corresponding to the overall loop in Sec-\ncritiques to iteratively improve prompts and in-    tion3.1), PMPO automatically enhances prompt\ncontext examples. TextGuard (Pei et al., 2023)    effectiveness without human intervention, adapting\nadopts a unit-test framework, generating adversar-    flexibly across different model architectures and\nial test cases to probe and correct prompt weak-    tasks.\nnesses. Though effective in structured tasks with   Problem Definition Given a task T , our frame-\nclear correctness criteria, these methods often as-   work initializes with a base instruction P and lever-\nsume explicit feedback or reference outputs, and   ages a problem description paired with a dataset\nrequire natural-language generation for both diag-  D = (xi, yi, ri)Ni=1, where each example consists\nnosis and revision. Self-supervised approaches like    of an input xi, an output yi, and an associated la-\n\nbel or preference score ri indicating the quality or   based loss across the entire dataset and selects\ndesirability of yi for xi. The language model M    the most challenging examples, defined as those\ngenerates outputs with probabilities PM(y | x, P)   with the highest losses, as priority targets for im-\nconditioned on input x and instruction P. Our   provement. Based on the insights from the mask-\nobjective is to derive an optimal prompt P∗that   guided analysis, the framework generates multiple\nmaximizes the expected weighted log-probability,    refined prompt variants, each designed to address\nthat is, E(x,y,r)∼D [r · log PM(y | x, P)], where r    the detected weaknesses. These candidates are re-\ncan represent either a binary label or a scalar pref-   evaluated using batch-level loss performance, and\nerence score. Concretely, in accuracy-based tasks    the variant demonstrating the greatest improvement\n(e.g., QA/classification) we set ri ≡1, whereas     is retained for the next iteration. This refinement\nin preference-based tasks we form pairs and as-   process continues until the maximum number of\nsign r = +1 to the preferred output and r = −1    iterations is reached.\nto the non-preferred one.  This unified formula-\n                                                    3.2  Mask-Guided Importance Evaluation\ntion enables the framework to enhance the model’s\ntarget-aligned generation capability by amplifying   To identify influential components within a instruc-\npreferences over desired outputs.                      tion P, we first decompose it into a set of semantic\n                                                      units s1, s2, ..., sm.  Instead of relying on prede-\n3.1  Iterative Framework for PMPO                                                      fined rules, we leverage the language model’s own\nWe propose an iterative algorithm that progres-   reasoning capability to perform this segmentation.\nsively refines prompt quality based on evaluation    This process is driven by a carefully designed meta-\nmetrics and targeted modifications. The full proce-   prompt, which instructs the model to analyze the\ndure is outlined in Algorithm 1.                      instruction and segment it into up to 5 indepen-\n                                                 dent and removable semantic units, such as meth-\nAlgorithm 1 An Overview of PMPO                ods, rules, or examples, whose removal would not\nRequire: Dataset D, Initial prompt P0, Language    disrupt the overall coherence of the prompt. The\n    model M, Max iterations T, Top-k samples k,   model then wraps these selected segments with\n    Variants per sample n                      <mask>...</mask> tags. A detailed description of\nEnsure: Optimized prompt P∗                    our masking formulation and the full meta-prompt\n  1: P∗←P0                                     template can be found in Appendix A.1.\n  2: for iteration ←1 to T do                      These identified semantic units are then used to\n  3:    Compute metric L(P∗, M) on D            quantify each component’s importance. For each\n  4:     Select top-k samples {(xj, yj)}              unit sj, we create a perturbed prompt variant by\n  5:      Initialize variant set V ←{P∗}            masking it:\n  6:     for each (xj, yj) do                    P−j = {s1, s2, ..., sj−1, <MASK>, sj+1, ..., sm}\n  7:       Analyze failure and token importance                                                                                                        (1)\n    using M                                   We then compute the change in batch-level cross-\n  8:        Generate n variants {P′j,1, ..., P′j,n}      entropy loss when sj is masked:\n  9:      V ←V ∪{P′j,1, ..., P′j,n}\n 10:    end for                           ∆Lj = Lbatch(P−j, M) −Lbatch(P, M)   (2)\n 11:     Evaluate all prompts in V and select best      Here, Lbatch(·, M) denotes the average cross-\n    Pnew                                        entropy loss over the dataset under model M. A\n 12:       if L(P∗, M) > L(Pnew, M) then            positive ∆Lj indicates that sj contributes posi-\n 13:     P∗←Pnew                                tively to task performance (its removal increases\n 14:    end if                                              loss), whereas a negative value suggests a detrimen-\n 15: end for                                                   tal or redundant effect. Values of ∆Lj close to zero\n 16: return P∗                                  imply negligible impact on model behavior.\n\n                                                    3.3  Prompt Evaluation via Loss-based  In each iteration, the PMPO framework begins\n                                                   Metricswith a detailed, mask-guided analysis using the\nmodel M to identify specific segments within   To  quantitatively  assess  the  effectiveness  of\nthe current prompt that affect performance.   It    prompts, we utilize loss-based metrics derived from\nthen computes the cross-entropy or preference-   the model’s internal probability estimates.\n\nGiven a model M, a instruction P, input x, and      it merely suggests areas where targeted edits may\nexpected output y, we define the token-level cross-   be beneficial. During candidate generation, the\nentropy loss as:                               model is encouraged to explore multiple rewriting\n                                                      variants that are not strictly limited to the masked\n                                  |y|\n                                                       regions, ensuring comprehensive exploration. This\nLCE(x, y, P, M) = − X log PM(yi | y<i, x, P)                                                approach also confers robustness: even if the initial\n                       i=1\n                                         LLM-based segmentation is imperfect, the frame-                                                 (3)\n                                         work is iterative and combines variant selectionwhere yi is the i-th token of the output sequence\n                                                 with evaluation to enable continuous improvementand y<i denotes all preceding tokens. The batch-\nlevel loss over dataset D is computed as:           and refinement across multiple cycles.\n                                           The model is guided to first diagnose flaws in\n                    n\n                1                   P and then apply a multi-step refinement strategy.\n  Lbatch(P, M) = X LCE(xi, yi, P, M)   (4)\n             n                           These steps include rephrasing rigid wording, re-\n                     i=1\n                                                        fining task constraints, removing redundancy, sim-\n  Unlike binary accuracy metrics, Lbatch captures    plifying overly complex instructions, improving\ntoken-wise generation probabilities, provides a con-    logical flow, expanding underspecified parts, merg-\ntinuous evaluation space, and reflects variations in    ing overlapping rules, and enhancing overall lan-\nmodel confidence induced by different prompts.     guage quality. These edits are applied in an adap-\n  In scenarios where candidate outputs are asso-    tive and integrated manner based on the specific\nciated with preference signals, we additionally in-   issues observed, preventing overfitting to individ-\ncorporate a pairwise preference loss inspired by    ual cases and encouraging broadly effective im-\npreference optimization techniques. Given a pre-   provements. Each hard example (xj, yj) yields a\nferred output y+ and a less preferred alternative    set of revised prompts P′j, 1, . . . , P′j, n generated\ny−, we define:                                      via temperature-controlled top-p sampling. These\n                                                        variants are pooled and evaluated using batch-level\n   Lpref(x, y+, y−, P, M) =                                                     cross-entropy, and the best-performing candidate,\n −log σ β · sM(x, y+, P) −sM(x, y−, P)       measured by loss or accuracy, is selected as the\n                                                updated prompt for the next iteration.                                                 (5)\n\n                                                    3.5  Efficiency of PMPO\n  where sM(x, y, P) = log PM(y  | x, P) is the\nmodel-assigned log-probability, σ is the sigmoid  A core advantage of PMPO is its computational\nfunction, and β is a scaling factor.                 and architectural efficiency across both evaluation\n                                              and candidate prompt generation. For each prompt\n3.4  Prompt Variant Generation                                                      variant under consideration, PMPO directly lever-\nTo generate new prompt candidates, PMPO em-   ages token level likelihoods, computing cross en-\nploys a model-in-the-loop rewriting mechanism    tropy over the target outputs in a single forward\nthat leverages the language model itself to revise    pass per example and thereby bypassing costly au-\ninstructions. Rather than using predefined rules    toregressive decoding. While the overall optimiza-\nor templates, a rewriting prompt is constructed    tion framework is iterative, cycling through prompt\nfor each selected hard example (x, y) identified    generation, selection, and refinement, this highly\nby high preference-based cross-entropy loss under    efficient per candidate scoring enables rapid evalua-\nthe current instruction P. This rewriting prompt    tion of many variants under a fixed compute budget\nincludes five key elements: the task description T    without relying on external classifiers, preference\nto maintain generality, the current instruction P as   models, or second pass LLM judges. This enables\nthe base for revision, the hard example (x, y) to   batched scoring of many prompt candidates even\nexpose weaknesses, editing instructions focused on   under limited compute. Prompt rewriting is like-\nimproving clarity, specificity, and structural qual-   wise streamlined: for each high-loss example, the\nity, and a token-level mask analysis that highlights   model performs a one-shot analysis and proposes\nsegments in P contributing most to the loss.         revised prompt variants without iterative reason-\n  The mask-guided analysis functions as a soft sig-    ing, multi-step prompting, or introspective feed-\nnal rather than a hard pruning mechanism. It does    back. As summarized in Table 1, this combina-\nnot constrain the optimization to specific regions;    tion of token-level evaluation and multi-candidate\n\nscoring yields a favorable efficiency profile rela-  20% of the examples for training (capped at 50)\ntive to generation-heavy methods such as OPRO or   and evaluate on the remaining set. The preference\nPromptWizard, whose multi-step refinement loops    scaling factor β is fixed at 1 across all experiments.\nare computationally intensive and can underper-   In each optimization round, we choose the top-\nform on smaller models with limited reasoning   k = 3 most challenging samples and generate 4\ncapacity (Zhang et al., 2024; Mayilvaghanan et al.,   prompt variants per sample. Optimization runs for\n2025).                                      up to 20 iterations. All experiments are conducted\n                                          on a single NVIDIA H800 GPU, with each full\n  Method        Evaluation Level   Candidate Count                                                   optimization taking approximately 20 minutes.\n  SPO             Sequence-Level          Single\n  PromptWizard    Sequence-Level         Multiple\n  EvoPrompt       Sequence-Level         Multiple         4.2  Experimental Results and Analysis\n  OPRO           Sequence-Level          Single\n                                           Reasoning Tasks.  As shown in Table 2, our PMPO            Token-Level          Multiple\n                                           method consistently outperforms existing prompt-\n         Table 1: Comparison of Efficiency.            ing approaches across a wide range of reasoning\n                                            and understanding tasks, evaluated on Qwen2.5-\n                                                    14B-Instruct under a 1 shot setting for fair compar-\n                                                         ison. PMPO achieves the highest average accuracy\n                                                     of 80.6%, surpassing strong baselines such as Evo-\n4  Experiment                             Prompt (78.0%), OPRO (77.1%), and PromptWiz-\n                                                  ard (72.8%), and ranks first on 11 out of 23 tasks,\n4.1  Experiment Settings\n                                                        significantly more than any other method. Un-\nDataset. We evaluate PMPO on a diverse set of    like generation heavy strategies like OPRO and\nbenchmarks covering mathematical reasoning, log-   EvoPrompt, which use black box search or popu-\nical inference, and open-ended instruction follow-    lation based mutation, PMPO’s single pass, loss\ning. For math problem solving, we use GSM8K   based evaluation enables more efficient optimiza-\nand AQUA-RAT, which require models to perform    tion and stronger performance, especially on tasks\nmulti-step numerical reasoning. To assess logical    that require multi step or spatial reasoning. While\nreasoning capabilities, we adopt the BBH bench-   PromptWizard is competitive on some logic tasks,\nmark, designed to challenge models with complex     its self critique based rewrites often create verbose,\ninference tasks under minimal guidance. For evalu-    rigid prompts that limit generalization. In contrast,\nating general instruction-following and open-ended  PMPO produces lightweight and structurally adap-\ntask performance, we use AlpacaEval 2.0, which    tive prompts that more effectively align with model\nincludes a broad range of user instructions and uses    behavior, yielding robust performance across both\nGPT-4 Turbo as an automatic evaluator to compare   symbolic and naturalistic reasoning.\nmodel responses against reference answers.       Math Tasks. As shown in Table 3, our method\nBaseline. We compare PMPO against two cate-   achieves the best performance on two widely used\ngories of prompting methods across benchmark   math reasoning benchmarks, GSM8K and AQUA-\ndatasets. The first category includes conventional   RAT, with accuracies of 94.0% and 84.6% respec-\nmanually designed prompting strategies:  CoT,    tively. It outperforms all baselines, including APE,\nRaR, and StepBack, which enhance model rea-  CoT (90.7% on GSM8K, 84.3% on AQUA-RAT),\nsoning or answer formulation through structural   and PromptBreeder. Notably, both datasets pro-\nor logical heuristics. The second category com-   vide complete solution steps rather than just final\nprises recent automated prompt optimization ap-   answers, which we incorporate as positive targets\nproaches: OPRO, EvoPrompt, PromptWizard, Text-   during training. This enables PMPO to iteratively\nGuard, and PromptBreeder, which leverage lan-    refine prompts using token-level confidence (cross-\nguage models to search, mutate, or iteratively im-   entropy loss), effectively guiding the model to gen-\nprove prompts without human intervention.           erate detailed, accurate multi-step reasoning. In\nImplementation Details.  We conduct experi-    contrast to methods like Chain-of-Thought that rely\nments using a mix of open-source and propri-   on manually designed heuristics, PMPO’s approach\netary language models, including Qwen2.5 (0.5B,   not only enhances final answer accuracy but also\n14B, 32B), LLaMA3.1 (8B), DeepSeek-R1-Distill-    significantly improves the quality and fidelity of\nQwen (1.5B). For each dataset, we randomly select    intermediate steps. It encourages the model to em-\n\nTable 2: Average test accuracy in the 1-shot setting across multiple tasks for different prompting methods, evaluated\non Qwen2.5-14B-Instruct. Bold values indicate the best-performing method for each task.\n\n   Task Name              AO  CoT  RaR  StepBack OPRO EvoPrompt PromptWizard  Ours\n   boolean_expressions              0.756  0.920  0.952    0.936    0.972     0.952         0.976      0.984\n   causal_judgement                 0.674  0.631  0.695    0.658    0.636     0.647         0.599      0.695\n   date_understanding               0.684  0.740  0.708    0.752    0.800     0.772         0.636      0.784\n   disambiguation_qa                0.656  0.776  0.716    0.640    0.848     0.760         0.892      0.736\n   dyck_languages                  0.096  0.240  0.236    0.228    0.392     0.308         0.220      0.256\n    formal_fallacies                  0.704  0.800  0.784    0.808    0.856     0.792         0.816      0.816\n   geometric_shapes                 0.440  0.616  0.576    0.684    0.580     0.620         0.508      0.676\n   hyperbaton                       0.632  0.704  0.768    0.848    0.740     0.756         0.752      0.896\n    logical_deduction                 0.692  0.856  0.844    0.847    0.767     0.864         0.845      0.864\n   movie_recommendation           0.564  0.636  0.624    0.640    0.636     0.596         0.676      0.684\n   multistep_arithmetic_two          0.052  0.968  0.972    0.956    0.988     0.948         0.976      0.988\n    navigate                         0.660  0.908  0.856    0.924    0.896     0.944         0.848      0.960\n   object_counting                  0.508  0.812  0.772    0.756    0.688     0.876         0.832      0.884\n   penguins_in_a_table              0.753  0.945  0.932    0.952    0.932     0.959         0.726      0.952\n   reasoning_about_colored_objects   0.708  0.892  0.768    0.880    0.876     0.872         0.912      0.888\n   ruin_names                      0.632  0.660  0.556    0.716    0.788     0.680         0.692      0.840\n    salient_translation_error_detection  0.600  0.572  0.604    0.644    0.624     0.604         0.504      0.600\n   snarks                           0.831  0.809  0.837    0.848    0.843     0.882         0.787      0.826\n   sports_understanding              0.752  0.660  0.680    0.804    0.828     0.812         0.544      0.836\n   temporal_sequences               0.832  0.908  0.864    0.900    0.964     0.916         0.900      0.944\n    tracking_shuffled_objects          0.599  0.900  0.852    0.847    0.860     0.871         0.839      0.880\n   web_of_lies                      0.536  0.900  0.972    0.920    0.820     0.900         0.716      0.976\n   word_sorting                     0.276  0.444  0.624    0.600    0.388     0.608         0.544      0.580\n   Best performing tasks            0     1     2       2       5        3            2         11\n   Average Accuracy               0.593  0.752  0.747    0.773    0.770     0.780         0.728      0.806\n\n\n      Method      GSM8K  AQUA-RAT            five instruction categories. As shown in Figure 3,\n     AO               0.871        0.760           PMPO-optimized prompts raise the average win\n     APE              0.939        0.827\n                                                         rate of Qwen2.5-14B from 31.81% to 51.52%,\n     COT              0.907        0.843\n      RaR              0.932        0.843            a substantial improvement. Notably, our method\n        Step-back         0.925        0.811             boosts performance across all instruction subsets,\n     OPRO            0.936        0.819              including difficult ones like helpful (from 17.83%\n       PromptBreeder    0.917        0.831              to 47.29%) and oasst (from 34.57% to 55.61%).\n       PromptWizard     0.882        0.799                                              These results show that PMPO enables mid-\n        Textguard         0.939        0.807\n                                                    sized models like Qwen2.5-14B to produce com-      Ours             0.940        0.846\n                                                         petitive outputs, surpassing larger models such\nTable 3: Accuracy on math reasoning datasets GSM8K    as LLaMA3.1-70B (39.1%) and GPT-4 Turbo\nand AQUA-RAT using different prompt optimization   (46.1%),  and  nearly matching GPT-4 Omni\nmethods(0-Shot), use Qwen2.5-14B as their optimiza-   (51.3%).  This highlights PMPO’s effectiveness\ntion models.                                                       in enhancing instruction alignment without model\n                                                      fine-tuning or explicit preference labels.\nulate human-like problem-solving behaviors such\n                                                    4.3  Open-Source Cross-Model Generalizationas decomposition, variable definition, and numeric\n                                                     Analysisjustification, thereby demonstrating stronger align-\nment with complex mathematical reasoning tasks   To assess adaptability across model scales and ar-\nwhile maintaining generalizability.                    chitectures, we conduct cross model evaluation by\nOpen-ended Dataset. To evaluate PMPO on open-   applying prompts optimized on one model to others\nended instruction-following tasks, we conduct ex-   (Table 4, BBH:Navigate). Models of various sizes,\nperiments on the AlpacaEval 2.0 benchmark. Using    including small, medium, and large, are evaluated\nGPT-4 Turbo as the evaluator, we compare model-    in a zero shot manner using the optimized prompts\ngenerated responses with reference outputs across    for inference.\n\nTarget \\     Qwen2.5  DeepSeek  LLaMA  Qwen2.5  Qwen2.5\n                   Prompt Source    0.5B      1.5B     3.1–8B    14B      32B\n                     Qwen2.5-0.5B     0.580      0.568      0.464     0.500      0.580\n                     DeepSeek-1.5B     0.612      0.772      0.700     0.640      0.584\n                   LLaMA-3.1–8B    0.708      0.792      0.800     0.852      0.860\n                    Qwen2.5-14B      0.912      0.948      0.896     0.960      0.956\n                    Qwen2.5-32B      0.972      0.948      0.952     0.972      0.980\n\nTable 4: Cross-model accuracy on Navigate. Each row corresponds to a target model and each column to the\nprompt source model. Diagonal entries represent same-model optimization. Models are ordered from smallest to\nlargest.\n\n\n\n\n\n                                                      Figure 4: Cross-model performance on BBH:Navigate\n                                                       using prompts optimized on Qwen2.5-32B.\n\n\n                                                     step-wise input sequences by incrementally append-\n                                                  ing target tokens and querying the model at each\n                                                     step to obtain token-level probabilities. However,\nFigure 3: Win rate comparisons on AlpacaEval 2.0.\n                                                           this procedure can lead to substantial token con-Left: original Qwen2.5-14B win rates across instruction\nsources. Right: PMPO-optimized results using the same   sumption and high latency. Therefore, we do not\nmodel. Bottom: average win rates across models.       recommend applying the full PMPO optimization\n                                                  process directly on these models in practice.\n                                                     Despite the limited access to loss signals on pro-\n  Results reveal a notable instruction-following\n                                                       prietary systems, prompts optimized via PMPO\ncapacity gap: prompts optimized on large mod-\n                                          on open-source models still demonstrate strong\nels, though effective on similarly large or medium\n                                                cross-model transferability. Although optimized\nmodels, often degrade when transferred to smaller\n                                          on Qwen2.5-32B, these prompts consistently im-\nones, which may struggle with complex or verbose\n                                               prove performance on the BBH:Navigate bench-\ninstructions.  Furthermore, prompts consistently\n                                           mark when applied to GPT-3.5 Turbo 0613, Claude\nperform best on the originating model, indicating\n                                                    3.5 Haiku 20241022, and GPT-4o (Figure 4).\nthat prompt effectiveness is closely linked to the\ninternal reasoning and instruction-following mech-\n                                                    4.5  Reasoning Process Fidelity\nanisms of each model.\n                                         To quantitatively validate our claim that PMPO\n4.4  Transferability to Proprietary Models                                              improves the fidelity of intermediate reasoning\nWhile PMPO is fully applicable to open-source    steps, we conducted an additional analysis on the\nmodels that support token-level likelihood access,  GSM8K benchmark using a specialized Process\napplying it to proprietary systems introduces addi-   Reward Model (PRM), Qwen2.5-Math-PRM-7B.\ntional constraints. These models typically provide    This model is specifically trained to assess the qual-\nlog-probabilities only for generated tokens, with-    ity of individual steps in mathematical solutions.\nout supporting full-sequence evaluation through a   For each prompting method, we parsed the gener-\nsingle forward pass. As a result, it is not straightfor-   ated solutions into individual reasoning steps using\nward to compute cross-entropy loss over an entire    the newline character as a delimiter. Each step\ndataset. One potential workaround is to construct   was then independently scored by the PRM, and\n\nwe computed the average score across all steps to    ergistic gain beyond the core iterative loop, under-\ndetermine the final process reward for each sample.   scoring the importance of fine-grained evaluation\n  As shown in Table 5, PMPO achieves the high-   and targeted rewriting for maximizing performance.\nest average process reward score (0.9950), indi-\ncating that the reasoning steps it generates are of\n                                                                                     Setting             TIM   BCA     PrefLoss     Acc. (%)\nthe highest quality among all compared methods.        Full              ✓     ✓      ✓          80.63\n                                                                         w/o TIM                ✗     ✓      ✓          79.05\nThis result provides direct, quantitative evidence       w/o TIM,BCA            ✗      ✗      ✓          77.96\n                                                                         w/o TIM,BCA,PrefLoss     ✗      ✗        ✗          76.74\nthat PMPO not only excels in final answer accu-\nracy but also generates more reliable and logically    Table 6:  Ablation  results on BBH dataset using\nsound intermediate steps. This validates our claim   Qwen2.5-14B. Removing individual modules leads to\nthat PMPO’s loss-guided refinement encourages    progressive performance degradation.\nthe model to emulate more structured and correct\nhuman-like problem-solving behaviors.\n                                       5  Conclusion\n     Prompting Method   Process Reward Score\n    AO                         0.5510          We present PMPO, a unified and efficient frame-\n      PromptWizard               0.8867\n                                         work for prompt optimization that relies on loss-     CoT                        0.9637\n      PromptBreeder               0.9776               based evaluation and iterative rewriting. Instead of\n     RaR                        0.9910                relying on output generation or human feedback,\n    OPRO                      0.9926\n    APE                        0.9930          PMPO uses token-level likelihoods to identify and\n      Step-back                   0.9932                 refine underperforming prompt segments, enabling\n      TextGrad                    0.9940                scalable and efficient optimization. Experimental\n    PMPO (Ours)               0.9950\n                                                           results on reasoning, mathematical, and instruction-\nTable 5: Process reward scores on GSM8K evaluated by   following benchmarks show that PMPO consis-\nQwen2.5-Math-PRM-7B. Higher scores indicate better    tently outperforms existing methods in both accu-\nintermediate reasoning quality. PMPO achieves the    racy and efficiency.  Its lightweight design, com-\nhighest score.                                                           patibility with smaller models, and minimal need\n                                                         for manual supervision make it well-suited for both\n4.6  Ablation Study                           academic research and practical deployment.\n\nTo assess the contribution of individual compo-                                            Limitations\nnents within PMPO, we conduct a cumulative abla-\ntion study on the BBH benchmark using Qwen2.5-   Despite the promising results, our study has several\n14B for both optimization and evaluation. Hold-    imitations.\ning the iterative refinement mechanism fixed, we     While PMPO demonstrates strong efficiency and\nsuccessively ablate Token Importance Masking    effectiveness in optimizing prompts across a range\n(TIM), Bad Case Analysis (BCA), and Preference    of open-source models, its application to propri-\nLoss (PrefLoss). TIM localizes low-performing    etary, closed-source language models remains lim-\nprompt spans via token-level loss attribution, en-    ited. Most commercial APIs (e.g., OpenAI, An-\nabling edits to concentrate where the model strug-   thropic) do not expose full log-likelihoods, which\ngles; removing  it drops accuracy from 80.63%    restricts the direct use of PMPO’s loss-based eval-\nto 79.05%. Eliminating BCA, which prioritizes    uation due to privacy constraints and concerns\nhigh-loss examples for targeted refinement, yields   around model behavior leakage. Although approxi-\na further decline to 77.96%. Finally, omitting Pre-   mate likelihoods can be estimated via autoregres-\nfLoss, our preference-learning objective used to    sive token-by-token querying, this significantly in-\nrank and select candidate rewrites, reduces accu-   creases latency and token usage, making it impracti-\nracy to 76.74%.                                        cal for large-scale optimization. Nonetheless, some\n  The strictly monotonic degradation observed in   API-based frameworks, such as vLLM, do provide\nTable 6 indicates that each module makes a non-   access to token-level log-probabilities (e.g., via\nredundant, complementary contribution: TIM iden-   prompt_logprobs), allowing PMPO to be applied\ntifies where to edit, BCA focuses which cases to fix    in those settings. We view this as a positive di-\nfirst, and PrefLoss guides how to prefer improved    rection, and hope that more commercial providers\nrewrites. Together, these mechanisms deliver a syn-    will consider offering similar transparency to facil-\n\nitate research on prompt optimization and model    Chrisantha  Fernando,  Dylan  Banarse,  Henryk\nalignment.                                            Michalewski, Simon Osindero, and Tim Rock-\n                                                                täschel. 2023.   Promptbreeder:  Self-referential\n   Additionally, in extremely low-resource scenar-                                                       self-improvement via prompt evolution.   arXiv\nios (e.g., using only one training example), PMPO       preprint arXiv:2309.16797.\nmay exhibit reduced robustness. Since the opti-\n                                               Qingyan Guo12, Rui Wang, Junliang Guo, Bei Li23,mization directly minimizes model loss on a lim-\n                                                        Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and\nited number of instances, it is prone to overfitting                                                         Yujiu Yang. Connecting large language models with\nin such cases. If no additional data is introduced,       evo-lutionary algorithms yields powerful prompt op-\nthe resulting prompt variants may align too closely       timizers.\nwith the few observed examples, leading to reduced\n                                          Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\ngeneralization. While this setup is inherently chal-                                                   som. 2017. Program induction by rationale genera-\nlenging for most learning algorithms, it highlights       tion: Learning to solve and explain algebraic word\na fundamental limitation of data-scarce prompt op-      problems. arXiv preprint arXiv:1705.04146.\ntimization.\n                                            Kawin Mayilvaghanan, Varun Nathan, and Ayush Ku-\n                                                         mar. 2025. Propel: Prompt optimization with expert\n                                                                priors for small and medium-sized llms. In Proceed-\nReferences                                               ings of the 4th International Workshop on Knowledge-\n                                                 Augmented Methods for Natural Language Process-\nEshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav       ing, pages 272–302.\n  Magazine, Tanuja Ganu, and Akshay Nambi. 2024.\n  Promptwizard:  Task-aware prompt optimization    Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, and Dawn\n  framework. arXiv preprint arXiv:2405.18369.           Song. 2023.  Textguard: Provable defense against\n                                                    backdoor attacks on text classification.  Preprint,\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie      arXiv:2311.11225.\n   Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n   Neelakantan, Pranav Shyam, Girish Sastry, Amanda   Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan\n   Askell,  Sandhini Agarwal,  Ariel  Herbert-Voss,      Zhang, Ke Li, Xing Sun, Yunsheng Wu, Shaohui\n  Gretchen Krueger, Tom Henighan, Rewon Child,       Lin, and Rongrong Ji. 2023. Aligning and prompting\n  Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,      everything all at once for universal visual perception.\n  Clemens  Winter,  and 12  others. 2020.   Lan-       Preprint, arXiv:2312.02153.\n  guage models are few-shot learners.   Preprint,\n   arXiv:2005.14165.                               Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\n                                                            bastian Gehrmann, Yi Tay, Hyung Won Chung,\nJiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning      Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\n  Wang, Yuxiao Dong, Jie Tang, and Minlie Huang.     Zhou, and 1 others. 2022. Challenging big-bench\n  2023.  Black-box prompt optimization: Aligning       tasks and whether chain-of-thought can solve them.\n   large language models without model training. arXiv      arXiv preprint arXiv:2210.09261.\n   preprint arXiv:2311.04155.\n                                                      Zeliang Tong, Zhuojun Ding, and Wei Wei. 2025. Evo-\n                                                      prompt: Evolving prompts for enhanced zero-shotKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\n                                               named entity recognition with large language models.  Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\n                                                           In Proceedings of the 31st International Conference   Plappert, Jerry Tworek, Jacob Hilton, Reiichiro\n                                                  on Computational Linguistics, pages 5136–5153.  Nakano, and 1 others. 2021.   Training verifiers\n   to solve math word problems.   arXiv preprint\n                                               Wenyi Wang, Hisham A Alyahya, Dylan R Ashley, Oleg\n   arXiv:2110.14168.\n                                                           Serikov, Dmitrii Khizbullin, Francesco Faccio, and\n                                                       Jürgen Schmidhuber. 2024a. How to correctly do\nYihe Deng, Weitong Zhang, Zixiang Chen, and Quan-                                                         semantic backpropagation on language-based agentic\n  quan Gu. 2023. Rephrase and respond: Let large                                                          systems. arXiv preprint arXiv:2412.03624.\n   language models ask better questions for themselves.\n   arXiv preprint arXiv:2311.04205.                                               Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai,\n                                                      Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P\nYihe Deng, Weitong Zhang, Zixiang Chen, and Quan-      Xing,  and  Zhiting Hu.  2023.    Promptagent:\n  quan Gu. 2024. Rephrase and respond: Let large       Strategic planning with language models enables\n   language models ask better questions for themselves.       expert-level prompt optimization.  arXiv preprint\n   Preprint, arXiv:2311.04205.                            arXiv:2310.16427.\n\nYann Dubois, Balázs Galambosi, Percy Liang, and Tat-   Xu Wang, Cheng Li, Yi Chang, Jindong Wang, and Yuan\n   sunori B. Hashimoto. 2025.  Length-controlled al-     Wu. 2024b. Negativeprompt: Leveraging psychology\n   pacaeval: A simple way to debias automatic evalua-       for large language models enhancement via negative\n   tors. Preprint, arXiv:2404.04475.                       emotional stimuli. arXiv preprint arXiv:2405.02814.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n  Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\n  and 1 others. 2022. Chain-of-thought prompting elic-\n   its reasoning in large language models. Advances\n   in neural information processing systems, 35:24824–\n  24837.\n\nJinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng,\n   Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin Wu,\n  and Yuyu Luo. 2025. Self-supervised prompt opti-\n   mization. arXiv preprint arXiv:2502.06855.\n\nCilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao,\n  Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang\n  Kang, and Yangyang Kang. 2024.  Efficient and\n   accurate prompt optimization: the benefit of mem-\n  ory in exemplar-guided reflection.  arXiv preprint\n  arXiv:2411.07446.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\n  Quoc V Le, Denny Zhou, and Xinyun Chen. 2023.\n  Large language models as optimizers. arXiv preprint\n  arXiv:2309.03409.\n\nTuo Zhang, Jinyue Yuan, and Salman Avestimehr. 2024.\n   Revisiting opro: The limitations of small-scale llms\n   as optimizers. Preprint, arXiv:2405.10276.\n\nHuaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,\n  Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny\n  Zhou. 2023a.  Take a step back: Evoking reason-\n   ing via abstraction in large language models. arXiv\n   preprint arXiv:2310.06117.\n\nHuaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,\n  Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny\n  Zhou. 2023b. Take a step back: Evoking reason-\n   ing via abstraction in large language models. arXiv\n   preprint arXiv:2310.06117.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\n  Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\n  Zhuohan Li, Dacheng Li, Eric Xing, and 1 others.\n  2023c. Judging llm-as-a-judge with mt-bench and\n   chatbot arena. Advances in Neural Information Pro-\n   cessing Systems, 36:46595–46623.\n\nMiao Zheng, Hao Liang, Fan Yang, Haoze Sun, Tian-\n  peng Li, Lingchu Xiong, Yan Zhang, Youzhen Wu,\n  Kun Li, Yanjun Shen, and 1 others. 2024. Pas: Data-\n   efficient plug-and-play prompt augmentation system.\n  arXiv preprint arXiv:2407.06027.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\n  Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  Ba. 2022. Large language models are human-level\n  prompt engineers.  In The Eleventh International\n  Conference on Learning Representations.\n\nA  Appendix\n\n\n\nA.1  Detailed Prompts of ours\n\nIn this section, we present the meta prompts used in ours for prompt optimization, evaluation, and\nsegment-level masking. These prompts enable the framework to analyze, rewrite, and assess instructions\nwith minimal human intervention. We include variants adapted for large and small models, as well as the\nmasking strategy used for loss attribution.\n\nPrompt Optimization (Large Model). We first show the optimization prompt used with capable\ninstruction-following LMs:\n\n    Optimization Prompt for Large Models\n\n\n\n     You are an expert prompt engineer tasked with dynamically improving prompts to generate more effective, diverse solutions.\n    ,→  When analyzing a prompt, first diagnose its core weaknesses, then apply multiple strategic modifications as needed.\n\n     Given the current prompt and task description, your objective is to produce a significantly improved version that will\n    ,→  better solve the intended task.\n\n     Your optimization must center on the task description: {task_description}\n\n     CRITICAL WARNING - MAINTAIN TASK SCOPE:\n         1. The task_description defines the FULL SCOPE of what your prompt must address\n         2. Examples are provided ONLY to understand the FORMAT, not to narrow the task\n         3. Your prompt MUST maintain the original breadth of the task_description\n         4. NEVER specialize the prompt to only handle specific examples you've seen\n\n         Example Input: {user_input}\n         Expected Answer: {true_answer}\n\n     These examples are provided ONLY for pattern analysis. Do NOT directly incorporate these exact examples into your prompt\n    ,→  or design your prompt specifically for these examples. Instead:\n     - Extract the underlying patterns and reasoning these examples demonstrate\n     - Understand the general skills or knowledge being tested\n     - Focus your prompt improvements on the task_description and solving the general problem\n\n     First, analyze the prompt for:\n     - Gaps in instruction clarity or specificity\n     - Unnecessary constraints limiting creative problem-solving\n     - Missing guidance that would help solve the general task type\n     - Overly rigid structure that hinders diverse approaches\n     - Places where more natural, professional language would improve understanding\n     - Redundancies or contradictions causing confusion\n\n     Then, apply a strategic combination of these techniques (using multiple approaches rather than just one):\n     (1) ENHANCE STRUCTURE, (2) ADD RULES OR PRINCIPLES, (3) REMOVE UNNECESSARY ELEMENTS,\n     (4) REPHRASE FOR CLARITY, (5) SIMPLIFY COMPLEXITY, (6) EXPAND WITH DETAILS,\n     (7) CONSOLIDATE SIMILAR RULES, (8) PROFESSIONAL REFRAMING, (9) DIVERSIFY APPROACH\n\n     IMPORTANT: Your response must focus on creating a prompt that will produce substantively better results on the general\n    ,→  task, not just on the specific examples provided.\n\n     Additionally, here is an analysis of the current prompt, segmented by mask (for reference in your optimization):\n     {mask}\n\n     Current prompt:\n     {current_prompt}\n\n     For the final prompt, please wrap it with <prompt></prompt>.\n\n\n\nPrompt Optimization (Small Model). To accommodate smaller LMs with limited instruction-following\ncapacity, we use a simplified variant:\n\n    Optimization Prompt for Small Models\n\n\n\n     Task_description: {task_description}\n     Example Input: {user_input}\n     Expected Answer: {true_answer}\n     Mask Analysis: {mask}\n\nCurrent Prompt: {current_prompt}\n\n     Based on the following examples where the current cross-entropy is relatively high, please analyze the reasons and modify\n    ,→  the prompt to improve performance. Rather than directly quoting the examples, focus on deeply analyzing the\n    ,→  underlying patterns and issues that contribute to high cross-entropy. Prioritize identifying the root causes of\n    ,→  performance problems and make labeled prompt modifications that address these specific issues. Concentrate on what\n    ,→  changes will most effectively improve task outcomes rather than structural coherence or theoretical correctness.\n\n     Your response must focus on creating a prompt that will produce substantively better results on the general task, not\n    ,→  just on the specific examples provided.\n\n     Please only wrap the optimized final prompt with <prompt></prompt> tags.\n\n\n\nPrompt Evaluation. To evaluate candidate outputs, we use a lightweight semantic comparison prompt\nthat tolerates format variation:\n\n\n    Evaluation Prompt\n\n\n\n     You are an expert evaluator determining if an answer matches the ground truth. Consider equivalent formats like 'A',\n    ,→  '(A)', 'A.', etc. as correct. Focus on the meaning rather than exact string matching. For questions where the answer\n    ,→  format is important, verify that the model answers in the correct format. For example, in Dyck language problems, if\n    ,→  the question asks what follows '([{}'  and the ground truth is '])', but the model answers with '([{}])', this should\n    ,→  be considered correct as it includes the proper closing brackets.\n\n\n\nMask Generation. For prompt segmentation, we generate masked variants to localize ineffective segments\nbased on their contribution to loss:\n\n\n   Prompt for Mask Generation\n\n\n\n     Given the following prompt, analyze it to identify up to 5 relatively independent units (segments) that are not tightly\n    ,→  connected to their surrounding content. Such units can include individual methods, rules, or examples. For each unit,\n    ,→  consider whether masking (removing) it would leave the surrounding prompt logically coherent and understandable.\n\n     Your task:\n     1. Carefully read the current prompt: {prompt}\n     2. If possible, segment the prompt into up to 5 independent units. Only select units whose removal would not disrupt the\n    ,→  overall flow or meaning of the prompt.\n     3. For each selected unit, wrap it in <mask></mask> tags.\n     4. Wrap the entire prompt in <prompt></prompt> tags.\n     5. If you find the prompt does not contain any truly independent units suitable for masking, simply output the prompt\n    ,→  wrapped in <prompt></prompt> without any <mask></mask> tags.\n\n     Formatting requirements:\n     - Do not exceed 5 masked segments in total.\n     - Do not create overlapping or nested masks.\n     - Each masked unit should represent a coherent, removable segment (such as a method, rule, or example), not a random\n    ,→  phrase.\n     - Maintain all other prompt content unchanged.\n\n     Example input:\n     current prompt: {prompt}\n\n     Example output with masking:\n     <prompt> ... Some instructions ... <mask>This is an example rule to be masked.</mask> ... More instructions ... </prompt>\n\n     Example output with no masking:\n     <prompt> ... Full original prompt (no <mask> tags) ... </prompt>\n\n\n\nThis prompt is used during the mask-guided analysis phase in PMPO to identify local prompt segments\nwhose removal yields significant loss changes. These segments are considered candidates for targeted\nrewriting in subsequent iterations.\n\nA.2  Experiment Details\n\nA.2.1  Tasks and Data Details\n\nWe evaluate PMPO across multiple datasets covering symbolic reasoning, math problem-solving, and\ninstruction-following. Table A7 summarizes dataset sizes and splits used in our experiments.\n\nTable A7: Dataset sizes and data splits used for training and evaluation.\n\n                                     Dataset Name  Test Size Train (max)\n                           BBH*           6,511      1304\n                         GSM8K         1,319       50\n                           AQUA-RAT      254        50\n                                     AlpacaEval 2.0    805        50\n\n\n  BBH*  The BIG-Bench Hard (BBH) benchmark (Suzgun et al., 2022) comprises 23 challenging tasks\nselected from the BIG-Bench suite, focusing on areas where language models previously underperformed\ncompared to average human raters. Each task includes 250 test examples, totaling 6,511 samples. We\nutilize the full BBH dataset for evaluation and randomly sample 50 examples for training.\n\n GSM8K GSM8K (Cobbe et al., 2021) is a math word problem benchmark requiring multi-step\nnumerical reasoning. We use the standard test split (1,319 samples) for evaluation. For training, we\nrandomly sample up to 50 examples from the training set.\n\n  AQUA-RAT  AQUA-RAT (Ling et al., 2017) contains multiple-choice math questions requiring\nalgebraic reasoning and textual comprehension. Following prior work, we use the full test set (2,371\nquestions) and randomly sample 50 training examples for optimization.\n\n  AlpacaEval 2.0  AlpacaEval 2.0 (Dubois et al., 2025) is a benchmark for evaluating instruction-\nfollowing ability of language models using GPT-4 Turbo as the judge. The dataset contains 805 diverse\nprompts spanning tasks such as open-ended generation, roleplay, summarization, and reasoning. We\nuse the full evaluation set for testing. For prompt optimization training, we use preference-labeled pairs\n(selected vs. rejected responses) from the public repository reciprocate/alpaca-eval, which offers\nhigh-quality alignment signals derived from automatic LLM feedback. From this set, we sample 50\ntraining pairs with clear preference margins. These examples allow PMPO to optimize prompts for\nalignment-style objectives using cross-entropy on preferred vs. dispreferred completions.\n\nNote: * BBH refers to the complete set of 23 tasks in the BIG-Bench Hard benchmark.\n\n• Boolean Expressions: Evaluate the truth value of a random Boolean expression consisting of Boolean\n  constants (True, False) and basic Boolean operators (and, or, and not).\n\n• Causal Judgment: Given a short story (involving moral, intentional, or counterfactual analysis),\n  determine how a typical person would answer a causal question about the story.\n\n• Date Understanding: Given a small set of sentences about a particular date, answer the provided\n  question (e.g., “The concert was scheduled to be on 06/01/1943, but was delayed by one day to today.\n  What is the date yesterday in MM/DD/YYYY?”).\n\n• Disambiguation QA: Given a sentence with an “ambiguous” pronoun, either determine whether the\n  sentence is inherently ambiguous or state the antecedent of the pronoun.\n\n• Dyck Languages: Predict the sequence of the closing parentheses of a Dyck-4 word without its last\n  few closing parentheses.\n\n• Formal Fallacies Syllogisms Negation: Determine whether an argument—presented informally—can\n  be logically deduced from a provided set of statements.\n\n• Geometric Shapes: Given a full SVG path element containing multiple commands, determine the\n  geometric shape that would be generated.\n\n• Hyperbaton (Adjective Ordering): Given two English-language sentences, determine the one with\n  the correct adjective order.\n\n• Logical Deduction: Deduce the order of a sequence of objects based on clues about their spatial\n  relationships and placements.\n\n• Movie Recommendation: Given a list of movies a user might have watched and liked, recommend a\n  new, relevant movie from a list of candidates.\n\n• Multi-Step Arithmetic: Solve multi-step equations involving basic arithmetic operations.\n\n• Navigate: Given a series of navigation steps, determine whether the agent would return to the starting\n  point.\n\n• Object Counting: Given a list of items and quantities, determine the count of a certain object class\n   (e.g., fruits).\n\n• Penguins in a Table: Given a table of penguins (and sometimes new information), answer a question\n  about the penguins’ attributes.\n\n• Reasoning about Colored Objects: Answer a simple question about the color of an object based on a\n  given context.\n\n• Ruin Names: Given an artist, band, or movie name, identify a one-character edit that changes the\n  meaning humorously.\n\n• Salient Translation Error Detection: Given a German sentence and its English translation, determine\n  the type of translation error present.\n\n• Snarks: Given two nearly-identical sentences, determine which one is sarcastic.\n\n• Sports Understanding: Determine whether a fictitious sports-related sentence is plausible.\n\n• Temporal Sequences: Given a person’s daily schedule, determine when they could perform another\n   activity.\n\n• Tracking Shuffled Objects: Given initial object positions and a series of swaps, determine final\n  positions.\n\n• Web of Lies: Evaluate the truth value of a Boolean function expressed as a word problem.\n\n• Word Sorting: Given a list of words, sort them lexicographically.\n\nA.2.2  Configuration\nWe compare ours against two categories of baseline methods: manually designed prompting strategies and\nautomated prompt optimization frameworks. The former apply structural heuristics to improve reasoning,\nwhile the latter leverage LLMs or search algorithms to generate and refine prompts automatically.\n\nManually Designed Prompting Strategies\n\n  Chain-of-Thought (CoT) (Wei et al., 2022).  CoT prompting enhances reasoning by encouraging the\nmodel to generate intermediate steps before the final answer. It improves performance in arithmetic and\nmulti-step tasks by including phrases like “Let’s think step by step” to elicit structured reasoning chains.\n\n  Step-Back (Zheng et al., 2023b).  Step-Back prompting first guides the model to abstract high-level\nconcepts before applying them to the specific task. This abstraction step enables more principled reasoning,\nparticularly in STEM tasks, by helping the model organize relevant knowledge before execution.\n\n  Rephrase-and-Respond (Deng et al., 2024).  Rephrase-and-Respond (RaR) improves answer quality\nby prompting the model to reformulate the input question before solving it. This internal clarification\nreduces ambiguity and enhances robustness, especially in under-specified or complex queries.\n\nAutomated Prompt Optimization Approaches\n\n OPRO (Yang et al., 2023). OPRO treats prompt design as a black-box optimization problem. An\nLLM iteratively proposes and evaluates new prompts based on previous ones and their accuracy on\ntraining examples. Although effective, it depends on repeated generation and evaluation over full datasets.\n\n  EvoPrompt (Tong et al., 2025).  EvoPrompt combines evolutionary algorithms with LLMs to maintain\nand evolve a population of prompts through mutation and selection. New prompts are generated via\nLLM-guided variation, and top-performing variants are retained across generations.\n\nPromptWizard (Agarwal et al., 2024).  PromptWizard uses a critique-and-synthesis loop where\none LLM analyzes prompt weaknesses and another generates refinements. This self-reflective mecha-\nnism incrementally improves prompts based on failure analysis, with high sample efficiency and task\nspecialization.\n\n  TextGrad (Pei et al., 2023).  TextGrad performs optimization via pseudo-gradients derived from\nLLM-generated feedback. It applies gradient descent-like updates to prompts, reverting bad updates using\nvalidation feedback, and supports optimization without requiring explicit supervision labels.\n\n  PromptBreeder (Fernando et al., 2023).  PromptBreeder evolves both task prompts and mutation\nstrategies in parallel.  It uses co-evolution to generate increasingly effective prompts without human\nintervention, achieving state-of-the-art performance on complex reasoning benchmarks.\n\n  SPO (Xiang et al., 2025).  SPO avoids human-labeled data by comparing outputs from multiple\nprompts and selecting better-performing variants through self-judgment.  This self-supervised loop\nincrementally refines prompts based on model preference rather than external metrics.\n\n PMPO (Probabilistic Metric Prompt Optimization).  Ours introduces a unified loss-based prompt\noptimization framework. It evaluates prompt quality using token-level cross-entropy or preference loss,\nrequiring only forward passes without output sampling. In each iteration, ours identifies low-utility prompt\nsegments via model-adaptive masking, generates variants based on hard examples, and selects improved\ncandidates via likelihood minimization. Unlike black-box or introspection-based methods, ours supports\nboth supervised and preference tasks under a consistent evaluation scheme and demonstrates scalability to\nsmaller models with limited data.\n\nA.2.3  Baseline Prompt\n\nIn this section, we present the baseline prompts used for comparison across all methods. For fairness, all\nprompt optimization approaches that require an initial prompt use the same COT Prompt as the starting\npoint.\n\n  AO Prompt\n\n\n\n     Please output directly and only output the final answer.\n\n\n\n   COT Prompt\n\n\n\n     Let's think step by step.\n\n\n\n    Step-back Prompt\n\n\n\n     Please first think about the principles involved in solving this task which could be helpful.\n     And then provide a solution step by step for this question.\n\n\n\n   RaR Prompt\n\n\n\n     Please rephrase the question in a way that is easier to understand, minimizing ambiguity and considering edge cases.\n     And then provide a solution step by step for the question.\n\n\n\nA.3  Case Study\n\nTo better understand how PMPO progressively improves prompt quality during optimization, we conduct\na case study on the AQUA-RAT dataset. Starting with the base prompt Let’s think step by step, we\nobserve how PMPO rewrites the prompt over 20 iterations by identifying high-loss examples and refining\n\nFigure A5: Cross-entropy loss per iteration during prompt optimization on AQUA-RAT.\n\n\nthe prompt to better capture general mathematical reasoning patterns. Table A5 shows selected steps in\nthis evolution along with the corresponding cross-entropy losses. As observed, the prompts gradually\nbecome more structured and specific in mathematical reasoning and format expectations.\n  Figure A5 shows the loss curve, highlighting the performance gains across the 20 iterations. Notably,\nearly iterations achieve rapid improvements, while later refinements focus on polishing structure and\nclarity.\n\n    Iter 1, Loss = 1.974\n\n\n     Let's think step by step\n\n\n\n    Iter 2, Loss = 1.926\n\n\n     Given a mathematical problem involving factors and divisibility, your task is to determine which of the provided options\n    ,→  correctly identifies all numbers that the given positive whole number must be divisible by. Follow these steps to\n    ,→  solve the problem effectively:\n\n     1. Identify the prime factors of the given number based on the information provided.\n     2. Determine the least common multiple (LCM) of these prime factors, which represents the smallest number that the given\n    ,→  number must be divisible by.\n     3. Evaluate each option against the LCM to check if it meets the divisibility criteria.\n     4. Select the correct option(s) that accurately reflect the divisibility rules derived from the prime factors.\n\n     Ensure your solution clearly explains your reasoning process and highlights why certain options are correct or incorrect\n    ,→  based on the divisibility properties of the given number.\n\n\n\n    Iter 4, Loss = 1.913\n\n\n     Given a mathematical problem involving geometric shapes or numerical properties, your task is to solve the problem and\n    ,→  identify the correct answer among the provided options. Here are the steps to follow for an effective solution:\n\n     1. Carefully read and comprehend the problem statement, identifying the key elements and requirements.\n     2. Apply relevant mathematical formulas, principles, or theorems to derive a solution. This may involve calculations,\n    ,→  logical deductions, or both.\n     3. Evaluate each provided option in light of your solution to determine which one accurately reflects the correct answer.\n     4. Clearly explain your reasoning process and highlight why certain options are correct or incorrect based on the\n    ,→  problem's specific requirements.\n\n     Maintain precision in your calculations and ensure that your solution aligns with the core mathematical concepts involved.\n    ,→  Use clear and concise language to articulate your thought process and conclusions.\n\n\n\n    Iter 6, Loss = 1.894\n\n\n     To effectively solve the given math problem and select the correct answer option, follow these steps:\n\n     1. Carefully read and understand the problem statement to identify all relevant conditions and relationships among the\n    ,→  numbers involved.\n\n2. Apply mathematical principles and formulas related to the conditions identified to derive necessary calculations or\n,→  conclusions.\n3. Analyze each answer option in light of the derived conclusions and evaluate whether it satisfies the conditions of the\n,→  problem.\n4. Clearly explain the rationale behind selecting the correct answer option, detailing how the option meets the specified\n,→  conditions.\n\nEnsure your response includes a clear explanation of the reasoning process used to arrive at the correct answer,\n,→  demonstrating a thorough understanding of the problem's underlying mathematical concepts.\n\n\n\nIter 8, Loss = 1.871\n\n\nGiven a mathematical problem involving numerical properties, your task is to solve the problem and identify the correct\n,→  answer among the provided options. Follow these steps for an effective solution:\n\n1. Thoroughly read and understand the problem statement, identifying key elements such as given factors, divisibility\n,→  rules, or other numerical properties.\n2. Apply appropriate mathematical principles, theorems, or logical deductions to derive a solution. This may include\n,→  factorization, divisibility tests, or other relevant calculations.\n3. Evaluate each provided option against your derived solution to determine the correct answer.\n4. Clearly explain your reasoning process, highlighting why certain options are correct or incorrect based on the\n,→  problem's specific requirements.\n\nEnsure your solution is precise and aligned with the core mathematical concepts involved. Use clear, concise language to\n,→  articulate your thought process and conclusions.\n\nBy following these guidelines, you will be able to effectively solve the problem and select the correct answer from the\n,→  given options.\n\n\n\nIter 10, Loss = 1.871\n\n\nGiven a mathematical problem involving geometric or numerical properties, your task is to accurately solve the problem\n,→  and identify the correct answer from the provided options. To achieve this, follow these steps:\n\n1. Carefully read the problem statement to identify all given elements and required calculations, such as dimensions,\n,→  formulas, or numerical relationships.\n2. Apply relevant mathematical principles, formulas, or logical reasoning to derive a solution. This could involve using\n,→  geometric formulas, algebraic equations, or numerical analysis.\n3. Compare your solution with each provided option to pinpoint the correct answer.\n4. Provide a clear explanation of your reasoning, detailing how you arrived at your solution and why certain options are\n,→  valid or invalid.\n\nYour solution should be precise, demonstrating a thorough understanding of the underlying mathematical concepts. Use\n,→  clear and concise language to articulate your thought process and justify your conclusions.\n\nBy adhering to these guidelines, you will be able to effectively solve the problem and confidently select the correct\n,→  answer from the given options.\n\n\n\nIter 12, Loss = 1.860\n\n\nGiven a mathematical problem involving numerical properties or relationships, your task is to accurately solve the\n,→  problem and identify the correct answer from the provided options. Follow these steps to ensure a comprehensive and\n,→  accurate solution:\n\n1. **Comprehend the Problem**: Read the problem statement carefully to identify all given elements and the required\n,→  calculations, including any numerical relationships, formulas, or conditions.\n2. **Apply Mathematical Principles**: Utilize appropriate mathematical concepts, formulas, or logical reasoning to solve\n,→  the problem. This may include applying algebraic equations, numerical analysis, or other relevant mathematical tools.\n3. **Verify the Solution**: Compare your derived solution with each provided option to determine the correct answer.\n4. **Explain Your Reasoning**: Provide a clear and detailed explanation of your thought process and the steps taken to\n,→  arrive at your solution. Justify why certain options are valid or invalid based on your calculations and reasoning.\n\nYour response should demonstrate a deep understanding of the mathematical concepts involved and use clear, concise\n,→  language to explain your approach and conclusions.\n\nBy following these guidelines, you will be able to effectively solve the problem and confidently choose the correct\n,→  answer from the given options.\n\n\n\nIter 14, Loss = 1.854\n\n\nGiven a mathematical problem involving divisibility rules or factor relationships, follow these steps to find the correct\n,→  answer:\n\n1. **Identify the Core Elements**: Clearly define the key numbers and conditions provided in the problem statement.\n\n2. **Apply Mathematical Principles**: Use divisibility rules and factorization techniques to deduce the required\n    ,→  conditions.\n     3. **Evaluate Each Option**: Carefully examine each given option against the derived conditions to determine which ones\n    ,→  satisfy the problem's criteria.\n     4. **Select the Correct Answer**: Based on your evaluation, choose the most accurate set of options that fulfill the\n    ,→  problem's requirements.\n\n     Ensure your solution is logically sound and aligns with fundamental mathematical concepts. Avoid making assumptions not\n    ,→  supported by the given information.\n\n     Your response should include a clear explanation of your reasoning process and the final selected answer.\n\n\n\n    Iter 15, Loss = 1.850\n\n\n     Given a mathematical problem, such as finding the curved surface area of a geometric shape, follow these steps to\n    ,→  accurately solve the problem and select the correct answer from the provided options:\n\n     1. **Understand the Problem**: Carefully read and comprehend the given problem, identifying all relevant numerical values\n    ,→  and geometric properties mentioned.\n     2. **Recall Relevant Formulas**: Identify and recall the appropriate mathematical formulas or principles necessary to\n    ,→  solve the problem.\n     3. **Calculate the Solution**: Apply the identified formulas to the given data, performing all necessary calculations\n    ,→  step-by-step.\n     4. **Verify Each Option**: Compare your calculated result with each provided answer option to determine which one matches\n    ,→  your solution.\n     5. **Choose the Correct Answer**: Select the option that correctly represents the solution to the problem based on your\n    ,→  calculations.\n\n     Ensure your solution process is logically consistent and grounded in fundamental mathematical principles. Avoid making\n    ,→  unsupported assumptions and verify each step of your calculation.\n\n     Your response should include a detailed rationale explaining your reasoning process, leading to the selection of the\n    ,→  correct answer.\n\n\n\n    Iter 17, Loss = 1.846\n\n\n     Given a mathematical problem, such as calculating the curved surface area of a geometric shape, follow these steps to\n    ,→  accurately solve the problem and select the correct answer from the provided options:\n\n     1. **Analyze the Problem Statement**: Carefully read the problem statement to identify all relevant numerical values and\n    ,→  geometric properties involved.\n     2. **Identify Applicable Formulas**: Recall and list the relevant mathematical formulas or principles that are pertinent\n    ,→  to solving the problem.\n     3. **Perform Calculations**: Apply the identified formulas to the given data, ensuring each step of the calculation is\n    ,→  accurate and logically sound.\n     4. **Evaluate Answer Options**: Compare your calculated result against each provided answer option to determine the\n    ,→  correct match.\n     5. **Select the Correct Answer**: Choose the answer option that precisely corresponds to your calculated solution.\n\n     In your response, provide a detailed rationale that outlines your reasoning process, clearly demonstrating how you\n    ,→  arrived at your solution. Ensure that your approach adheres to fundamental mathematical principles and avoids any\n    ,→  unsupported assumptions. Verify each step of your calculation for accuracy and consistency.\n\n\n\n    Iter 19, Loss = 1.796\n\n\n     Given a mathematical problem involving geometric shapes, such as finding the curved surface area, follow these guidelines\n    ,→  to accurately solve the problem and select the correct answer from the provided options:\n\n     1. **Understand the Problem**: Thoroughly comprehend the given problem statement to identify all pertinent numerical\n    ,→  values and geometric characteristics.\n     2. **Recall Relevant Formulas**: Identify and list the applicable mathematical formulas or principles needed to solve the\n    ,→  problem.\n     3. **Execute Calculations**: Apply the recalled formulas to the given data, ensuring each calculation step is precise and\n    ,→  logically consistent.\n     4. **Compare with Options**: Match your calculated result against each provided answer option to pinpoint the correct\n    ,→  choice.\n     5. **Confirm Selection**: Select the answer option that aligns exactly with your calculated solution.\n\n     Provide a detailed rationale explaining your reasoning process, highlighting how you applied fundamental mathematical\n    ,→  principles to arrive at your solution. Ensure your approach is free from unsupported assumptions and verify each\n    ,→  calculation step for accuracy and consistency.\n\n\n  Even with the use of mask-based analysis and model-in-the-loop rewriting for supervision, PMPO does\nnot guarantee improvement in every single iteration. In cases where no generated candidate outperforms\nthe current prompt, the original prompt is retained as the starting point for the next round of optimization.\n  The degree of improvement across steps may vary due to factors like sampling randomness, batch\n\ncomposition, and the inherent difficulty of refining an already strong prompt. Unlike model fine-tuning,\nprompt generation lacks a stable optimization trajectory, making step-to-step changes inherently more\nvariable.\n\nTable A8: Performance comparison on the BBH benchmark using the smaller Qwen2.5-7B model. PMPO (Ours)\nachieves the highest average accuracy and outperforms other methods on the majority of tasks. Bold values indicate\nthe best-performing method(s) for each task.\n\n\nTask                  AO   PromptWizard  StepBack  CoT  OPRO  EvoPrompt  RaR  Ours\n\nboolean_expressions              0.840       0.904         0.936    0.892   0.868      0.804     0.964  0.940\ncausal_judgement                0.572       0.503         0.599    0.487   0.503      0.567     0.583  0.615\ndate_understanding               0.544       0.536         0.616    0.580   0.636      0.620     0.604  0.652\ndisambiguation_qa               0.592       0.640         0.700    0.648   0.660      0.664     0.616  0.644\ndyck_languages                  0.392       0.248         0.168    0.148   0.180      0.176     0.156  0.320\nformal_fallacies                  0.596       0.612         0.728    0.724   0.748      0.748     0.660  0.740\ngeometric_shapes                0.304       0.440         0.624    0.596   0.612      0.616     0.608  0.652\nhyperbaton                      0.568       0.632         0.808    0.808   0.784      0.824     0.772  0.828\nmovie_recommendation           0.604       0.584         0.444    0.352   0.388      0.412     0.368  0.592\nmultistep_arithmetic_two         0.044       0.972         0.928    0.976   0.984      0.948     0.944  0.988\nnavigate                         0.680       0.700         0.872    0.872   0.848      0.912     0.900  0.920\nobject_counting                  0.484       0.520         0.800    0.824   0.636      0.804     0.672  0.816\npenguins_in_a_table              0.541       0.568         0.904    0.856   0.897      0.904     0.863  0.904\nreasoning_about_colored_objects  0.588       0.948         0.940    0.896   0.908      0.924     0.908  0.948\nruin_names                      0.596       0.608         0.592    0.596   0.636      0.492     0.552  0.784\nsalient_translation                0.444       0.516         0.496    0.504   0.512      0.648     0.520  0.620\nsnarks                           0.747       0.640         0.860    0.753   0.584      0.747     0.680  0.742\nsports_understanding             0.648       0.084         0.536    0.324   0.720      0.296     0.248  0.584\ntemporal_sequences              0.652       0.736         0.688    0.608   0.736      0.656     0.604  0.760\ntracking_shuffled_objects         0.188       0.269         0.809    0.853   0.845      0.832     0.764  0.829\nlogical_deduction                0.589       0.641         0.756    0.747   0.753      0.765     0.739  0.761\nweb_of_lies                     0.576       0.500         0.844    0.916   0.676      0.912     0.384  0.836\nword_sorting                    0.348       0.464         0.324    0.228   0.448      0.272     0.488  0.552\n\nAverage Accuracy               0.531       0.582         0.705    0.687   0.704      0.698     0.648  0.733\nBest performing tasks            2          1            3       3      1         3        1     13\n\n\n\nA.4  Further Analysis\n\nA.4.1  Qualitative Analysis of Sample Efficiency\n\nOur claim of high sample efficiency is rooted in PMPO’s computational design and the granularity of\nits evaluation signal. Computationally, unlike methods such as PromptWizard or OPRO that require\ngenerating full outputs via costly autoregressive decoding, PMPO evaluates prompts by computing token-\nlevel likelihoods through a single forward pass for each sample. This bypasses the expensive decoding\nstep, making the optimization loop significantly faster and more scalable. Furthermore, the optimization\nsignal in PMPO is the cross-entropy loss, which captures per-token deviations from the target output\nrather than relying on a binary correctness score. This fine-grained signal provides a more informative\ngradient, allowing the framework to learn more effectively from fewer samples.\n  However, the number of samples remains a critical factor. When optimizing on only one sample, PMPO\nis prone to overfitting to lexical artifacts or task-specific details. For instance, in a sentiment classification\ntask, training on a single negative review might yield an overly rigid instruction like:\n\n     “If the reviewer says the filmmakers tried hard but the movie seems awfully sloppy, and\n    mentions many factual errors about Lucille Ball, respond with ‘Negative’. Otherwise, respond\n    with ‘Positive’.”\n\nSuch a prompt fails to generalize. In contrast, when optimizing over a small batch of top-k examples\n(k ≥3), we observe that the learned prompts consistently capture generalized patterns aligned with the\n\nunderlying task (e.g., abstract reasoning steps or classification criteria). This suggests PMPO can achieve\nstable optimization with very few samples (e.g., 3–5) by leveraging the cross-entropy signal across a small\nbut diverse set of examples to avoid overfitting.\n\nA.4.2  Performance Evaluation on Smaller Models\n\nTo further demonstrate the empirical reliability and scalability of PMPO across different model sizes, we\nconducted a comprehensive evaluation on a smaller model, Qwen2.5-7B. Table A8 shows that PMPO\ncontinues to outperform strong baselines across the diverse tasks in the BBH benchmark, achieving the\nhighest average accuracy.\n  Notably, the instruction rewrites generated by methods like PromptWizard tend to be significantly more\nverbose. This verbosity can lead to performance degradation in smaller models, which may struggle\nwith capacity constraints and overly specified instructions. These findings further emphasize PMPO’s\ngeneralizability and robustness, particularly in resource-constrained settings where concise and effective\nprompts are crucial.\n\nA.4.3  Complementary Effects with Few-Shot Prompting\n\nPMPO and n-shot prompting are not mutually exclusive; rather, they target complementary dimensions of\nprompt design. Few-shot prompting conveys task format and solution patterns through concrete examples,\nwhile PMPO focuses on refining the core instruction to improve its clarity, coverage, and alignment with\nthe task’s intent. The benefits of both approaches can be additive, with PMPO enhancing the instruction’s\ngeneralizability and few-shot examples providing task-specific grounding.\n  To empirically validate this relationship, we conducted experiments on the BBH benchmark under\nvarious few-shot configurations, using both a standard base prompt and a PMPO-optimized prompt. The\nresults are summarized in Table A9.\n\nTable A9: Comparison of a base prompt and a PMPO-optimized prompt across different numbers of few-shot\nexamples (n). Scores are averaged over BBH tasks. The results show that for every value of n, the PMPO-optimized\nprompt outperforms the base prompt.\n\n                Number of Shots (n)  Prompt Type  Average Score\n\n                                            Base           73.75\n                           0\n                               PMPO          79.21\n\n                                            Base           75.02\n                           1\n                               PMPO          80.69\n\n                                            Base           76.69\n                           3\n                               PMPO          81.11\n\n                                            Base           76.89\n                           5\n                               PMPO          82.19\n\n                                            Base           76.58\n                           7\n                               PMPO          81.59\n\n                                            Base           77.97\n                           9\n                               PMPO          82.37\n\n                                            Base           78.17\n                           16\n                               PMPO          82.23\n\n\n  The results lead to several key observations. First, the PMPO-optimized prompt in a zero-shot setting\n(79.21) already outperforms the base prompt with 16-shots (78.17), demonstrating the substantial impact\nof instruction optimization alone. Second, combining PMPO with few-shot examples consistently yields\nthe highest scores, confirming the additive nature of the two techniques. Finally, performance does not\nmonotonically increase with the number of shots; for example, the 7-shot performance is lower than the\n\n5-shot performance in both settings. This suggests that an excessive number of examples can introduce\nnoise or cause instruction drift. In contrast, PMPO systematically improves the base prompt’s structure,\nmaking the instruction more interpretable and robust to input variation. Thus, PMPO offers a significant\nimprovement over few-shot prompting alone, and the two approaches are best utilized in tandem.\n\nA.5  Prompt Optimized by ours\n\nIn this section, we present the optimized prompts produced by our method during the main experiments.\nAll prompts were optimized using Qwen2.5-14B as the optimization model, with the original model also\nserving as the evaluation and execution model. For brevity, we show representative examples for each\nbenchmark dataset.\n\nBBH (Big-Bench Hard)  For PMPO training on BBH, we treat the correct answer option as the selected\noutput, and the remaining distractor options as rejected, enabling preference-based supervision using the\nmodel’s likelihoods.\n\n   BBH Prompt (boolean_expressions)\n\n\n\n     Your task is to accurately evaluate the truth value of a given Boolean expression. This expression may include Boolean\n    ,→  constants (True, False) and fundamental Boolean operators (and, or, and not). Adhere strictly to standard Boolean\n    ,→  logic throughout this process.\n\n     ### Key Considerations:\n     - **Parentheses Handling**: Nested conditions within parentheses require careful management. Start by evaluating the\n    ,→  innermost parentheses and proceed outward.\n     - **Operator Precedence**: Observe the correct order of operations: \"not\" has the highest precedence, followed by \"and\",\n    ,→  and then \"or\". This ensures proper simplification of the expression.\n     - **Logical Reasoning**: Clearly articulate the step-by-step logical reasoning applied during the evaluation process.\n    ,→  Maintain clarity without sacrificing depth.\n\n     ### Input Format:\n     A string containing a Boolean expression, such as \"not not True and True and not True\".\n\n\n\n   BBH Prompt (causal_judgement)\n\n\n\n     Given a short story involving moral, intentional, or counterfactual analysis, your task is to determine how a typical\n    ,→  person would respond to a causal question about the story. Your response should provide a clear chain of thought that\n    ,→  leads to a reasoned conclusion.\n\n     Begin by thoroughly reading the story and the causal question. Identify the key elements and the specific causal\n    ,→  relationship highlighted in the question. Analyze the cause-and-effect dynamics from a typical person's perspective.\n    ,→  Formulate your answer by clearly stating whether a typical person would consider the proposed causal link valid,\n    ,→  supported by a logical rationale.\n\n     Guidelines for responding:\n     1. Carefully read and understand the story and the causal question.\n     2. Identify the critical causal factors within the story.\n     3. Evaluate the causal relationship from the viewpoint of a typical person.\n     4. Provide a logical and well-supported explanation for your answer.\n     5. Ensure your response comprehensively addresses the task without being overly specific to any particular example.\n\n\n\n   BBH Prompt (date_understanding)\n\n\n\n     To effectively solve date-related questions based on given narratives, follow this structured yet flexible approach:\n\n     ### Objective:\n     Determine the correct date from a brief narrative containing specific details about dates and events. Provide the final\n    ,→  answer in the \"MM/DD/YYYY\" format, accompanied by a detailed chain-of-thought explanation.\n\n     ### Detailed Instructions:\n\n     #### Step 1: Comprehend the Narrative\n     Read the provided sentences carefully to identify all relevant dates and events. Pay special attention to any changes in\n    ,→  dates due to scheduling adjustments or other factors.\n\n     #### Step 2: Pinpoint Key Dates\n     Identify all explicit dates mentioned in the narrative. Also, consider any shifts in these dates caused by external\n    ,→  conditions.\n\n#### Step 3: Evaluate Required Calculations\nDetermine whether the question necessitates adding or subtracting days, weeks, months, or years from the key dates.\n,→  Remember to factor in calendar nuances, including the varying lengths of months and the impact of leap years.\n\n#### Step 4: Execute Calculations\nPerform the required calculations accurately, taking into account complexities such as leap years and differences in\n,→  month lengths. Ensure your calculations are precise and logical.\n\n#### Step 5: Deliver the Result\nPresent the final answer in the \"MM/DD/YYYY\" format. Provide a clear rationale explaining each step of your reasoning\n,→  process and any assumptions made.\n\n\n\nBBH Prompt (disambiguation_qa)\n\n\n\nGiven a sentence that contains a potentially ambiguous pronoun, your goal is to determine whether the pronoun clearly\n,→  refers to a specific noun within the sentence or if its meaning is uncertain due to insufficient context. This\n,→  process involves identifying the most likely referent of the pronoun or recognizing its ambiguity, providing a\n,→  thorough rationale for your conclusion.\n\nTo accomplish this task effectively, follow these steps:\n\n1. **Identify the Pronoun**: Locate the pronoun in the sentence.\n2. **Analyze Context**: Carefully examine the sentence for any clues that might reveal the pronoun's antecedent. Consider\n,→  the logical relationship between the pronoun and all potential antecedents.\n3. **Decide on Clarity or Ambiguity**: Based on your analysis, determine if the pronoun's reference is clear or ambiguous.\n4. **Justify Your Conclusion**: Provide a clear and well-supported explanation for your decision, referencing pertinent\n,→  details from the sentence.\n\nYour response should conform to one of the following formats:\n\n- For sentences deemed ambiguous: \"The sentence is ambiguous because [detailed reasoning based on context].\"\n- For sentences where the pronoun's reference is clear: \"The pronoun refers to [specific noun] because [detailed\n,→  rationale based on context].\"\n\n\n\nBBH Prompt (dyck_languages)\n\n\n\nYour task is to predict and complete the sequence of closing parentheses to form a valid Dyck-4 word. A Dyck-4 word\n,→  requires a balanced arrangement of four types of parentheses: round (), square [], curly {}, and angle <>. Each\n,→  opening bracket must be correctly matched with its corresponding closing bracket, ensuring proper nesting and balance\n,→  throughout the sequence.\n\nFollow these guidelines to solve the problem effectively:\n\n1. **Identify Unmatched Brackets**: Start by identifying all opening brackets in the input sequence that do not yet have\n,→  corresponding closing brackets. This step is essential for determining the types of closing brackets required next.\n2. **Prioritize Innermost Structures**: Always give priority to closing the innermost unmatched brackets first,\n,→  respecting the hierarchical matching rules of the different types of brackets.\n3. **Ensure Balance**: Maintain the balance between the counts of each type of opening and closing brackets. This ensures\n,→  that each closing bracket is placed appropriately and maintains the overall balance of the sequence.\n4. **Continuous Validation**: After adding each closing bracket, continuously validate that the sequence remains balanced\n,→  and properly nested.\n5. **Explain Chain-of-Thought**: Include a detailed Chain-of-Thought section that explains your reasoning at each step,\n,→  clearly demonstrating how you ensured compliance with the Dyck-4 word criteria.\n\nYour response should include:\n- The completed sequence of closing parentheses.\n- A detailed Chain-of-Thought explanation outlining the decision-making process.\n\n\n\nBBH Prompt (formal_fallacies)\n\n\n\nGiven a context that includes a series of logical statements and premises, your task is to evaluate whether a presented\n,→  argument can be logically deduced from the given context. Your response should clearly indicate whether the argument\n,→  is valid or invalid, supported by a detailed explanation of your reasoning process.\n\nTo approach this task effectively, follow these steps:\n\n1. Carefully analyze each premise to understand the relationships and conditions established.\n2. Compare the argument's conclusion with the premises. Assess whether the conclusion logically follows from the premises\n,→  provided.\n3. If the argument is valid, outline how each premise contributes to the logical progression leading to the conclusion.\n,→  If invalid, specify which premise(s) or logical step(s) fail to support the conclusion adequately.\n\nGuidelines for your response:\n- Clearly state your answer as either \"valid\" or \"invalid\".\n\n- Provide a concise but thorough explanation of your reasoning process.\n- Use clear and precise language to articulate your thoughts.\n\n\n\n\n\nBBH Prompt (geometric_shapes)\n\n\n\nYour task is to interpret an SVG path element specified within a \"d\" attribute and deduce the geometric shape that would\n,→  be formed upon executing this path. This process involves understanding a variety of path commands such as \"M\" for\n,→  moving to a point, \"L\" for drawing lines, and other commands according to the SVG standard.\n\nTo achieve this, follow these steps:\n\n1. **Command Analysis**: Carefully analyze the \"d\" attribute to identify various commands and their corresponding\n,→  parameters. Recognize how each command contributes to forming the overall path.\n\n2. **Path Sequence Understanding**: Grasp the sequence and relationships between commands and points. Note how movements\n,→  and connections between points influence the final shape's structure.\n\n3. **Geometric Shape Identification**: Based on the parsed commands and parameters, deduce the geometric shape\n,→  represented by the path. Consider both simple and complex shapes that could emerge from the given path commands.\n\n4. **Shape Matching**: Choose the most suitable geometric shape from the provided options. Ensure your selection\n,→  accurately reflects the interpreted path data without simplifying or misinterpreting it.\n\nRemember, the path may describe intricate shapes beyond basic polygons. Your analysis should accurately capture the\n,→  complexity of the path data to determine the precise geometric form.\n\n\n\n\n\nBBH Prompt (hyperbaton)\n\n\n\nYour task is to evaluate two given English sentences and determine which one correctly follows the standard adjective\n,→  order in English. To accomplish this, adhere to the following guidelines:\n\n1. **Adjective Order Principle**: In English, adjectives typically follow a specific order: Opinion, Size, Age, Shape,\n,→  Color, Origin, Material, Purpose.\n\n2. **Analyze Adjectives**: Carefully scrutinize each sentence to identify and note the adjectives and their positions\n,→  relative to the nouns they modify. Pay special attention to their sequence.\n\n3. **Compare Sequences**: Utilize the established order to assess and compare the sequences of adjectives in both\n,→  sentences. Highlight any discrepancies from the correct order.\n\n4. **Select the Correct Sentence**: Choose the sentence that accurately adheres to the English adjective order rules.\n\n**Response Format**: Indicate your chosen sentence using a single letter (A or B).\n\n\n\n\n\nBBH Prompt (logical_deduction_five_objects)\n\n\n\nGiven a description of several objects arranged in a specific order based on their spatial relationships, determine the\n,→  correct sequence of these objects.\n\n**Task Description**: Your primary goal is to deduce the precise order of a series of objects based on the clues provided\n,→  about their relative positions. The clues may involve terms like \"left,\" \"right,\" \"above,\" \"below,\" or any other\n,→  directional indicators relevant to the arrangement of the objects. Ensure that the solution aligns perfectly with all\n,→  given clues without contradiction.\n\n**Guidelines**:\n1. Carefully read through the entire description to fully understand the spatial relationships between the objects.\n2. Use a systematic approach to map out the positions of each object according to the clues.\n3. Consider multiple perspectives if necessary to verify the accuracy of your deduction.\n4. If the sequence is ambiguous after applying the clues, re-examine the descriptions for additional hints or rephrase\n,→  the clues for clarity.\n5. Provide a clear and concise answer detailing the order of the objects from left to right or top to bottom, depending on\n,→  the orientation described.\n\n**Example Format**: Your answer should clearly state the order of the objects as determined by the clues.\n\nBBH Prompt (logical_deduction_seven_objects)\n\n\n\nYour task is to accurately determine the sequential order of a series of objects based on the provided clues about their\n,→  spatial relationships and placements. This requires meticulous analysis and logical deduction to arrange the objects\n,→  in their specified sequence.\n\n### Guidelines for Solving:\n1. **Comprehend Clues**: Thoroughly read and understand each statement to grasp the positional relationships among the\n,→  objects.\n2. **Direct Position Identification**: Identify clues that specify the exact position of an object (e.g., \"Object X is\n,→  the leftmost\").\n3. **Relative Position Interpretation**: Analyze clues indicating the relative positions of objects (e.g., \"Object Y is\n,→  three spots before Object Z\").\n4. **Elimination Process**: Use the process of elimination to determine the correct positions for objects once some\n,→  placements are known.\n5. **Solution Verification**: Ensure that your final sequence aligns with all given clues without any conflicts.\n\n### Problem-Solving Strategy:\n- Start by identifying any direct placement clues that definitively place objects.\n- Apply relative positioning clues to establish relationships between objects.\n- Utilize elimination techniques to fill in the remaining positions.\n- Confirm your sequence to make sure it satisfies all provided clues.\n\n### Logical Reasoning Framework:\nDevelop a systematic approach to analyze and resolve the clues:\n1. **Initial Setup**: List all objects and note any direct placements from the clues.\n2. **Relationship Mapping**: Outline the relationships between objects as described by the clues.\n3. **Reasoning Deduction**: Combine the mapped relationships with elimination techniques to deduce the order.\n4. **Validation Check**: Review your sequence to ensure it adheres to all clue requirements.\n\n### Response Structure:\nProvide your answer as an ordered list of objects, accompanied by a detailed explanation of your reasoning process.\n\n\n\nBBH Prompt (logical_deduction_three_objects)\n\n\n\nYour task is to deduce the precise order of a series of objects based on the provided clues about their spatial\n,→  relationships and placements. Each scenario will detail a set of objects along with statements indicating their\n,→  relative positions or rankings. Your goal is to accurately establish the exact order of these objects according to\n,→  the orientation described in the input.\n\n**Instructions:**\n1. Thoroughly examine the input paragraph that describes the objects and their relationships.\n2. Utilize the given clues to logically deduce the correct sequence of the objects.\n3. Present the objects in their correct order, starting with the object at the highest rank or position as per the clues.\n4. If the input lacks sufficient information to determine a unique order, clearly state that the order cannot be\n,→  definitively established based on the provided clues.\n\n**Guidelines:**\n- Refrain from making assumptions that go beyond what is explicitly stated in the input.\n- Ensure your response is concise and focuses exclusively on identifying and listing the correct order of objects,\n,→  without additional commentary or explanations unless required by the task.\n- Scenarios may involve various types of objects and different numbers of objects, but the core principle remains\n,→  consistent: use the provided clues to determine the correct order.\n\n\n\nBBH Prompt (movie_recommendation)\n\n\n\nGiven a list of movies a user has enjoyed, your task is to recommend a new, relevant movie from a set of four potential\n,→  choices. Follow the steps below to ensure a thoughtful and accurate recommendation:\n\n1. **Understand User Preferences:** Carefully examine the given list of movies to identify common themes, genres, and\n,→  styles that the user appreciates. This analysis will form the basis of the user's taste profile.\n\n2. **Evaluate Movie Choices:** Compare each of the four movie options against the user's taste profile. Pay attention to\n,→  genre, theme, director, actors, and overall style. Aim to find a movie that closely aligns with the user's\n,→  preferences, avoiding any significant deviations.\n\n3. **Choose the Optimal Match:** Select the movie that best matches the user's preferences, demonstrating a deep\n,→  understanding of both the user's tastes and the distinct characteristics of each movie option.\n\n**Instructions for Response:**\n- Provide only the letter corresponding to the recommended movie from the four options.\n\n**Guiding Principles:**\n- Highlight thematic, genre, and stylistic consistency with the user's preferences.\n- Avoid suggesting movies that diverge markedly from the user's established tastes.\n- Exhibit a refined understanding of the user's preferences and the unique qualities of each movie option.\n\n**Goal:**\nIncrease user satisfaction by recommending a movie that closely reflects their established preferences.\n\n\n\n\n\nBBH Prompt (multistep_arithmetic_two)\n\n\n\nYour task is to accurately solve a complex multi-step arithmetic problem that involves a variety of operations, such as\n,→  addition, subtraction, multiplication, and potentially division. It's important to pay close attention to\n,→  parentheses, as they determine the order of operations according to standard mathematical principles.\n\nTo tackle this problem effectively and precisely, follow these steps:\n\n1. Start by evaluating all expressions within parentheses, beginning with the deepest ones. This ensures that you resolve\n,→  the operations inside the parentheses before proceeding to other parts of the equation.\n2. After resolving all parentheses, move on to multiplication and division operations from left to right. This step\n,→  adheres to the standard order of operations.\n3. Lastly, perform addition and subtraction operations from left to right until you reach the final result. Carefully\n,→  execute each operation to prevent errors.\n4. Clearly present your final answer, supported by a detailed, step-by-step explanation of your calculations to ensure\n,→  transparency and accuracy.\n\nWhen crafting your response, make sure to:\n- Break down the problem systematically, addressing each component sequentially and methodically.\n- Provide a precise and professionally articulated explanation, with each calculation step clearly documented and\n,→  logically explained.\n\n\n\n\n\nBBH Prompt (navigate)\n\n\n\nGiven a set of navigation instructions for an agent, your task is to determine if the agent ends up back at its initial\n,→  starting point. Follow these structured guidelines to methodically analyze the navigation instructions:\n\n**Guidelines for Analyzing Navigation Instructions:**\n1. **Initial Setup**: Position the agent at the Origin (Point 0) and orient it Facing Forward.\n2. **Command Handling**:\n   - **Movement Commands**: Adjust the agent's position based on its orientation (Forward, Backward).\n   - **Orientation Commands**: Alter the agent's direction (Turn around, Turn left, Turn right) without changing its\n   ,→  position.\n3. **Sequential Execution**:\n   - Process each command one by one.\n   - Update the agent's position and orientation after executing each command.\n4. **Final Position Verification**:\n   - After all commands have been executed, check if the agent's position matches the initial Origin.\n   - Determine if the agent has returned to the starting point.\n5. **Response Format**:\n   - Give chain of thought.\n   - Provide a clear \"Yes\" or \"No\" answer.\n   - Include a detailed explanation of your reasoning process.\n\n\n\n\n\nBBH Prompt (object_counting)\n\n\n\nYour task is to calculate the total quantity of items belonging to a specific category from a provided list of\n,→  possessions and their quantities. This involves identifying the target category, categorizing each item, and summing\n,→  up the quantities of those that fit the criteria.\n\nFollow these steps to complete the task:\n\n1. **Understand the Target Category**: Carefully read the question to understand which category of items needs to be\n,→  counted (e.g., fruits, vegetables, musical instruments).\n\n2. **Review the List of Possessions**: Go through the list of items and their quantities provided in the input.\n\n3. **Categorize Each Item**: For every item listed, decide whether it falls under the specified category.\n\n4. **Sum Up Quantities**: Add together the quantities of all items that belong to the target category.\n\n5. **Present the Result**: Clearly state the total count of items in the specified category.\n\nBBH Prompt (penguins_in_a_table)\n\n\n\nGiven a unique table containing detailed information about penguins (and sometimes additional data), your task is to\n,→  accurately answer questions about the attributes of the penguins listed in the table. Ensure that your response\n,→  strictly relies on the provided data without incorporating any external information.\n\nEach table includes essential columns such as \"name,\" \"age,\" \"height (cm),\" and \"weight (kg),\" potentially supplemented\n,→  with other relevant details. Each row corresponds to a distinct penguin, highlighting their individual\n,→  characteristics.\n\nTo approach this task effectively, follow these steps:\n1. Thoroughly examine the provided table(s) to understand the structure and content.\n2. Clearly identify the question posed about the penguins' attributes.\n3. Utilize the data from the table to deduce the correct answer.\n4. If additional information is provided, integrate it seamlessly into your analysis.\n5. Deliver your answer precisely and succinctly, ensuring it accurately addresses the question.\n\n\n\nBBH Prompt (reasoning_about_colored_objects)\n\n\n\nGiven a detailed description of objects placed on a surface, your goal is to accurately identify and answer a question\n,→  about the color of a specific object within that description. The response should be concise and directly answer the\n,→  question, optionally including a brief rationale if requested.\n\nTo accomplish this task efficiently and effectively, follow these steps:\n\n1. **Understand the Description**: Carefully read through the provided description to familiarize yourself with each\n,→  object and its corresponding color.\n2. **Pinpoint the Object of Interest**: Identify which object's color is being queried in the question.\n3. **Retrieve the Color Information**: Refer back to your initial notes to locate the color associated with the object of\n,→  interest.\n4. **Provide a Clear Response**: Directly state the color of the object in a straightforward manner.\n5. **Offer Optional Explanation**: If asked, briefly explain your reasoning process.\n\n\n\nBBH Prompt (ruin_names)\n\n\n\nGiven an artist, band, or movie name, your goal is to craft a single-character edit that transforms the meaning into\n,→  something humorous. Follow these guidelines to ensure your response is both creative and effective:\n\n1. **Input Understanding**: Carefully read the provided name, whether it's an artist, band, or movie title.\n2. **Creative Transformation**: Modify the name by changing exactly one character. The change should significantly alter\n,→  the meaning in a comedic manner.\n3. **Humor Explanation**: For each edit, provide a short explanation highlighting why the alteration is funny. Consider\n,→  creating a play on words, introducing an absurd situation, or crafting a pun.\n4. **Avoid Repetition**: Ensure that each edit is unique and adds a fresh perspective to the name.\n\n**Execution Strategy**:\n- **Character Impact Analysis**: Evaluate how changing each character affects the overall meaning and potential for humor.\n- **Phonetic & Literal Changes**: Consider both sound-based and literal alterations that could create a humorous effect.\n- **Cultural References**: Think about popular culture, idioms, and common phrases that could be humorously altered with\n,→  a single character change.\n\n**Guidance for Humorous Edits**:\n- Strive for humor that appeals to a wide audience while also offering depth for those who enjoy sophisticated wordplay.\n- Ensure the edits are clear and understandable, avoiding ambiguity that might detract from the humor.\n\n\n\nBBH Prompt (salient_translation_error_detection)\n\n\n\nGiven a German sentence and its English translation, your task is to carefully analyze the translation for errors and\n,→  categorize them into one of the following six types:\n\n- **Named Entities**: Any change to names, places, or other identifiable entities.\n- **Numerical Values**: Errors involving incorrect changes to numbers, dates, or units.\n- **Modifiers or Adjectives**: Improper modifications to descriptive terms associated with nouns.\n- **Negation or Antonyms**: Issues related to the introduction or removal of negations, or the incorrect transformation\n,→  of comparatives into their opposites.\n- **Facts**: Trivial factual inaccuracies that do not fit into the aforementioned categories.\n- **Dropped Content**: Omission of a key component of the sentence in the translation.\n\nTo complete this task effectively, follow these structured steps:\n\n1. Carefully read and comprehend both the German source sentence and its English translation.\n2. Perform a detailed comparison between the two sentences to identify any discrepancies.\n3. Categorize these discrepancies according to the predefined error types.\n\n4. Select the most appropriate category for the identified error(s).\n\n\n\nBBH Prompt (snarks)\n\n\n\nTo effectively distinguish between two nearly identical sentences and identify the one that uses sarcasm, follow the\n,→  enhanced guidelines below:\n\n### Objective\nYour goal is to recognize which of the two given sentences is intended to convey sarcasm. Sarcasm is characterized by\n,→  language that expresses a meaning opposite to its literal sense, often used to mock or criticize.\n\n### Analytical Approach\n1. **Analyze Word Choice and Sentence Structure**: Carefully examine the words and structure of each sentence for subtle\n,→  cues that hint at sarcasm.\n2. **Imagery of Real-Life Context**: Visualize a plausible scenario in which the sentences might be said to better\n,→  understand the speaker's intent.\n3. **Tone Evaluation**: Assess whether the tone of each sentence suggests sincerity or mockery/criticism.\n4. **Examine Literal vs. Implied Meaning**: Look for significant discrepancies between the literal interpretation and the\n,→  implied message, which often signals sarcasm.\n5. **Identify Signs of Irony**: Pay attention to any exaggerated elements or unexpected twists that indicate irony or\n,→  mockery.\n\n### Response Format\nProvide a clear indication of which sentence is sarcastic and support your conclusion with a rationale focusing on ironic\n,→  or mocking elements.\n\n\n\nBBH Prompt (sports_understanding)\n\n\n\nGiven a fictitious sentence related to sports, evaluate its plausibility. Your evaluation should follow these steps:\n\n1. **Comprehend the Sentence**: Thoroughly read and understand the sentence, taking note of the sport or activity it\n,→  might relate to, the key actors involved, and any specific details about the action or conditions described.\n\n2. **Analyze Key Components**: Identify the central elements of the sentence, including the athlete's name, the action\n,→  performed, and any particular specifications like body part used or environmental factors that may influence the\n,→  action.\n\n3. **Assess Logical Possibility**: Determine if the described scenario fits within the logical and practical boundaries\n,→  of sports. Consider the typical rules, physical constraints, and standard practices relevant to the sport implied by\n,→  the sentence.\n\n4. **Justify Your Conclusion**: Clearly explain your judgment regarding the plausibility of the sentence. Use specific\n,→  references from the sentence to support your reasoning, detailing how these elements align with or deviate from\n,→  established norms in sports.\n\n5. **Detail the Reasoning Process**: Provide a detailed Chain-of-Thought that outlines each step of your reasoning\n,→  process. This should include initial observations, intermediate deductions, and the final justification for your\n,→  conclusion.\n\n\n\nBBH Prompt (temporal_sequences)\n\n\n\nGiven a detailed account of a person's daily schedule, including various activities and their corresponding times, your\n,→  objective is to identify potential periods of availability throughout the day when the person could have engaged in\n,→  an additional activity. Follow these steps to achieve this:\n\n1. **Comprehend the Schedule**: Carefully review the provided sequence of events and note down the start and end times\n,→  for each activity. Ensure you capture all pertinent details about the day's beginning and end, along with any\n,→  specific operational hours or restrictions related to the locations involved.\n\n2. **Detect Intervals of Availability**: Examine the timeline for intervals where no activity is specified. These gaps\n,→  suggest potential windows of availability for engaging in additional activities.\n\n3. **Apply Contextual Constraints**: Take into account additional contextual factors such as daily routines, operational\n,→  hours of places (like parks or shops), and other known limitations or rules (such as curfews).\n\n4. **Define Availability Periods**: Clearly outline the identified periods of availability in your response, making sure\n,→  they align with the provided schedule and any supplementary context.\n\n5. **Use Concise Language**: Respond using clear, professional language focused exclusively on the periods of\n,→  availability without adding extraneous details.\n\n6. **Systematic Examination**: Employ a thorough, step-by-step approach to ensure all potential gaps in the schedule are\n,→  analyzed accurately.\n\nBBH Prompt (tracking_shuffled_objects_five_objects)\n\n\n\nTo effectively determine the final positions of a set of entities after a series of pairwise swaps, adopt a clear,\n,→  structured, and flexible approach focusing on precision, accuracy, and comprehensive coverage. This method aims to\n,→  streamline the process of tracking transformations while ensuring thorough adherence to the task's requirements.\n\n### Enhanced Problem-Solving Framework\n\n#### Step 1: Define Initial States\n1. Identify and label each entity uniquely (e.g., using names, labels).\n2. Document the initial position or state of each entity clearly to establish a baseline for tracking changes.\n\n#### Step 2: Execute Swaps Systematically\n1. For each swap in the sequence:\n   - Identify the two entities involved.\n   - Update their positions or states according to the swap.\n   - Record the swap chronologically to ensure traceability and validation.\n\n#### Step 3: Confirm Final States\n1. After applying all swaps:\n   - Verify the final positions or states of all entities.\n   - Cross-reference the final configuration against the recorded swaps to ensure accuracy and completeness.\n\n### Core Principles\n- **Sequential Order**: Apply swaps in the order provided to maintain the integrity of the transformation sequence.\n- **Clear Documentation**: Use consistent methods for documenting position or state updates to reduce errors and enhance\n,→  traceability.\n- **Thorough Validation**: Conduct rigorous checks on the final positions or states to ensure they accurately reflect the\n,→  intended outcomes based on the swaps executed.\n\n### Practical Application Examples\nThis framework can be applied to various situations, such as:\n- Students exchanging books.\n- Dancers swapping partners.\n- Entities undergoing pairwise exchanges in any context.\n\nThe fundamental requirement remains constant: transitioning from an initial setup to a final configuration through a\n,→  series of swaps.\n\n#### Detailed Task Execution\n1. **Define Initial Setup**:\n   - Clearly outline the starting positions or states of all entities.\n2. **Implement Sequential Swaps**:\n   - Apply each swap systematically, updating positions or states as specified.\n3. **Validate Final Configuration**:\n   - Thoroughly verify the final positions or states to ensure they correctly reflect the sequence of swaps executed.\n\n\n\nBBH Prompt (tracking_shuffled_objects_seven_objects)\n\n\n\nYour task involves determining the final positions of a set of entities after undergoing a series of pairwise swaps. To\n,→  effectively tackle this task, adhere to the following guidelines designed to ensure accuracy and efficiency:\n\n1. **Initial Position Identification**: Clearly define and list each entity alongside its initial position. Employ a\n,→  structured format such as a table or list to facilitate easy identification and ensure a clear understanding of the\n,→  starting configuration.\n\n2. **Detailed Recording of Swaps**: Closely document each swap operation, specifying which entities are involved and the\n,→  order in which these swaps take place. This meticulous documentation is essential for accurate tracking and proper\n,→  application of each swap.\n\n3. **Step-by-Step Swap Execution**: Execute each swap operation strictly in the recorded sequence. Update the positions\n,→  of the entities after every swap, maintaining an organized and clear record to ensure precision and adherence to the\n,→  swap order.\n\n4. **Final Position Compilation**: Upon completing all swap operations, compile and present the final positions of all\n,→  entities in a clear and accessible format.\n\nTo enhance your approach, consider the following strategies:\n- Utilize a structured format like a table or list to keep detailed records of positions at each stage, thereby\n,→  increasing clarity and minimizing errors.\n- Stress the significance of meticulous documentation and sequential execution of swap operations to maintain accuracy\n,→  and consistency.\n- Implement a flexible yet robust method capable of efficiently managing diverse swap sequences and varying quantities of\n,→  entities.\n\nKey points to remember:\n- Maintain precision and thoroughness when dealing with multiple pieces of information concurrently.\n- Ensure your methodology is adaptable to various scenarios involving differing numbers of entities and swap sequences.\n- Avoid dependence on specific examples; instead, cultivate a versatile approach applicable across different contexts.\n\nBBH Prompt (tracking_shuffled_objects_three_objects)\n\n\n\nGiven a set of objects and a sequence of pairwise swaps, the goal is to determine the final positions of each object after\n,→  all swaps have been executed. The solution should be generalizable, capable of handling any number of objects and any\n,→  sequence of swaps, without being constrained to specific examples.\n\n### Objective\nDetermine the final position of each object following a series of pairwise swaps.\n\n### Input Format\n- The initial positions of objects (e.g., \"Object A starts at position X\").\n- A list of swaps (e.g., \"Swap Object A with Object B\").\n\n### Output Format\n- The final positions of all objects after executing all swaps.\n\n### Guidelines for Solving\n1. **Initialization**: Begin by defining the starting positions of all objects.\n2. **Execution of Swaps**: Apply each swap in the sequence systematically.\n   - Identify the pair of objects involved in the swap.\n   - Temporarily store the positions of these objects.\n   - Perform the swap by exchanging their positions.\n3. **Verification**: Confirm the final positions of the objects after processing all swaps.\n4. **Presentation**: Clearly state the final positions of all objects.\n\n### Chain-of-Thought Guidance\nTo solve this problem efficiently and accurately:\n1. **Initialization Step**: List out the initial positions of all objects.\n2. **Swap Processing Step**: For every swap in the sequence:\n   - Note the objects involved in the swap.\n   - Store their current positions temporarily.\n   - Exchange their positions.\n3. **Verification Step**: After completing all swaps, review the final positions of the objects to ensure they align with\n,→  the sequence of swaps.\n4. **Conclusion Step**: Provide a clear statement of the final positions for all objects.\n\n\n\n\n\nBBH Prompt (web_of_lies)\n\n\n\nTo accurately evaluate the truth value of a Boolean function presented as a natural-language word problem, adhere to the\n,→  following detailed and systematic approach:\n\n1. **Understanding the Core Principle**: Recognize that every individual in the scenario is consistently either truthful\n,→  or deceitful. A truthful individual will never contradict themselves or anyone known to be truthful, whereas a\n,→  deceitful individual will always contradict statements of truthfulness.\n\n2. **Identifying Assertions**: Closely examine each individual's claim regarding the honesty or deceitfulness of others.\n,→  These claims serve as the foundational elements for your logical deduction process.\n\n3. **Progressive Deduction**: Begin with the first confirmed truth or lie and progressively deduce the honesty status of\n,→  each individual. For each new statement, assess its alignment with the established truths or falsehoods from previous\n,→  statements.\n\n4. **Maintaining a Chain of Thought Record**: Throughout the analysis, keep a detailed log of your logical progression.\n,→  Note any contradictions or confirmations encountered during the examination of each statement.\n\n5. **Concluding Evaluation**: After compiling all deductions, determine the truth value of the final statement in\n,→  question. Ensure that your conclusion aligns with all preceding deductions.\n\n**Key Guidelines for Logical Reasoning**:\n- A truthful person's statement accurately reflects reality.\n- A liar's statement misrepresents reality.\n- Consistency across all statements is essential for establishing the honesty of each individual.\n\n**Step-by-Step Detailed Process**:\n- Start with the initial statement and evaluate its truth value.\n- For each subsequent statement, carefully examine how it aligns with the established truths or falsehoods from earlier\n,→  statements.\n- Maintain a thorough record of any logical inconsistencies or confirmations throughout the analysis.\n- Ensure that your final evaluation is consistent with the entire sequence of deductions made.\n\n**Expected Output Format**:\nProvide a comprehensive explanation of your chain of thought leading to your conclusion. Each step should be logically\n,→  coherent and clearly articulated.\n\nBBH Prompt (word_sorting)\n\n\n\n     Your task is to organize a list of words into alphabetical order. To accomplish this, provide a detailed chain-of-thought\n    ,→  explanation that covers the following aspects: word comparison, handling duplicates, ensuring accuracy, and managing\n    ,→  edge cases like empty or excessively long lists. Your response should include the sorted list and a structured\n    ,→  thought process, as outlined below:\n\n     **Input**: Specify the list of words to be sorted.\n\n     **Thought Process**:\n     - **Initial Setup**: Outline any preliminary actions or configurations needed before sorting begins.\n     - **Comparison Strategy**: Elaborate on how individual words are compared to determine their order in the list.\n     - **Dealing with Duplicates**: Explain your approach to manage and handle repeated words within the list.\n     - **Final Check**: Describe the steps taken to verify the correctness of the sorted list.\n     - **Edge Cases Consideration**: Address special scenarios such as empty lists or lists containing a very high number of\n    ,→  entries.\n\n     **Sorted Output**: Display the final list arranged alphabetically.\n\n\nAQUA-RAT  AQUA-RAT (Ling et al., 2017) contains multiple-choice math questions requiring algebraic\nreasoning and textual comprehension. The dataset includes detailed rationales for each question, making it\nsuitable for supervision via explanation. In our experiments, we adopt a non-preference-based optimization\nsetup by treating the step-by-step rationale as the supervision target. This enables PMPO to optimize\nprompts using cross-entropy loss over full reasoning sequences.\n   Additionally, since AQUA-RAT is a multiple-choice task, we also support a preference-based training\nvariant, where the correct answer choice is treated as the selected output and the incorrect options as\nrejected, allowing PMPO to operate under both supervised and preference paradigms.\n\n   AQUA-RAT Prompt\n\n\n\n     Given a math problem that requires determining a specific value through methods such as algebraic manipulation, geometric\n    ,→  reasoning, or other applicable techniques, solve the problem systematically, providing clear explanations for each\n    ,→  step to illustrate your reasoning process. Once you have found the solution, select the correct answer from the given\n    ,→  options and justify your choice logically. Ensure your approach demonstrates flexibility in addressing various problem\n    ,→  types, including quantitative relationships, algebraic expressions, and geometric calculations. Focus on applying\n    ,→  fundamental mathematical principles in a broad context without relying on specific examples. Your response should be\n    ,→  clear, concise, and professionally framed, guiding the reader through the problem-solving process effectively.\n\n\nGSM8K  GSM8K (Cobbe et al., 2021) is a benchmark of grade-school math word problems that require\nmulti-step reasoning and precise arithmetic. Since each instance in the dataset comes with a detailed\nrationale, we treat the full explanation as the supervision target in PMPO training. The optimization\nobjective is to minimize cross-entropy loss over the complete reasoning sequence.\n  As a result, the optimized prompts tend to preserve the distinctive answer format seen in GSM8K (e.g.,\n«calculation=answer») to better align with the ground truth and reduce token-level loss. This shows\nthat PMPO can adapt prompt style and structure based on dataset-specific signal during training.\n\n   GSM8K Prompt\n\n\n\n     To solve any math word problem precisely, follow this structured approach, focusing on clear understanding, logical\n    ,→  steps, and accurate calculations:\n\n     1. Understand the Problem & Identify the Goal\n        - Briefly restate what the problem is asking (e.g., total amount, difference, remainder, cost).\n        - Determine the main operation(s) involved (e.g., addition, subtraction, multiplication, division, unit conversion).\n\n     2. Extract Key Information\n        - List all given numbers, units, and relevant conditions from the problem.\n\n     3. Plan the Solution\n        - Define what you need to find.\n        - Outline a clear step-by-step approach to solve it.\n\n     4. Step-by-Step Reasoning and Calculation\n        - For each step:\n          1. Explain what you are calculating and why.\n          2. Perform the calculation, using the format `<<calculation=answer>>`, including units if needed.\n          3. If not the final step, explain how this leads to the next one.\n\n5. Conclude and Verify\n        - Confirm that the result answers the original question.\n        - Check consistency in logic, units, and values.\n\n     6. Final Answer\n        - Provide only the final result, starting with `####`, and include units if applicable.\n\n\nAlpacaEval 2.0  For AlpacaEval 2.0 (Dubois et al., 2025), we use the reciprocate/alpaca-eval\ndataset as our training source. This dataset contains a large number of paired outputs annotated as selected\n(preferred) and rejected (less preferred) responses across various instruction-following tasks.\n  We construct preference-based supervision by treating the selected outputs as positive targets and\nthe rejected ones as negatives. PMPO is then trained to minimize loss on preferred examples while\ndiscouraging prompts that increase likelihood on rejected ones. This setup enables effective instruction\ntuning without requiring explicit gold answers.\n\n\n    AlpacaEval Prompt\n\n\n\n     To generate an informative, helpful, and accurate response to a user's query, follow these steps carefully:\n\n     1. **Comprehend the Query**: Read the user's question or request thoroughly to grasp its essence and identify any key\n    ,→  terms or concepts that require attention.\n     2. **Research Thoroughly**: Utilize your extensive knowledge and research abilities to gather precise and current\n    ,→  information that aligns with the user's query.\n     3. **Structure the Response**: Organize the content logically, prioritizing clarity and conciseness to ensure that the\n    ,→  most relevant information is easily accessible to the user.\n     4. **Enhance with Context**: Provide additional context that can enrich the user's understanding and appreciation of the\n    ,→  topic, making the response more valuable and engaging.\n     5. **Maintain Accuracy**: Double-check all factual information and data included in the response to uphold the highest\n    ,→  standards of accuracy and reliability.\n     6. **Communicate Clearly**: Write in a straightforward and professional tone, using terminology that is appropriate and\n    ,→  understandable given the context of the query.\n     7. **Recommend Further Resources**: Where applicable, suggest additional sources or materials that the user can consult\n    ,→  for deeper insights or further exploration of the topic.\n\n     **Guiding Principles**:\n     - Always aim to fully address the user's request, providing a comprehensive and well-rounded response.\n     - Prioritize user-centric thinking, focusing on the relevance and utility of the information provided.\n     - Employ a systematic approach to problem-solving, breaking down complex queries into manageable parts and addressing\n    ,→  them methodically.\n     - Encourage engagement and curiosity by offering insightful and thought-provoking content.\n\n\n\nA.5.1  Cross-Model Prompt Analysis\n\nWe compare the final prompts optimized by our PMPO framework across different language models,\nall targeting the same BBH-Navigate task. The results demonstrate that PMPO consistently tailors\nits optimizations to the reasoning abilities and strengths of the underlying model, while maintaining\nadherence to the general task formulation. Below, we present the optimized prompts with no additional\nexamples, as used in our final evaluations.\n\n\n    Qwen2.5-0.5B\n\n\n     To further enhance navigation accuracy, consider implementing additional strategies such as:\n\n     1. **Path Reconstruction**: Implement algorithms to reconstruct the user's actual route from start to finish, taking into\n    ,→  account potential obstacles, turns, and other navigational factors.\n     2. **Feedback Loop**: Incorporate real-time feedback loops to continuously adjust the user's course based on their\n    ,→  current location and movement status.\n     3. **Error Correction**: Introduce mechanisms to automatically correct errors in the user's movements, ensuring smooth\n    ,→  transitions between different areas of the map.\n     4. **Dynamic Path Generation**: Develop dynamic paths that adapt to changing conditions, such as adjusting routes based\n    ,→  on weather conditions or sudden changes in terrain.\n     5. **Customizable Paths**: Allow users to specify custom paths or waypoints, which can help them avoid dead ends or reach\n    ,→  previously unvisited locations more efficiently.\n     6. **Real-Time Updates**: Implement real-time updates to the navigation system, allowing users to see how their journey\n    ,→  is progressing and make informed decisions about their next moves.\n\n     These enhancements not only improve navigation accuracy but also provide a more intuitive and personalized experience for\n    ,→  users.\n\nAnalysis: For smaller models like Qwen2.5-0.5B, PMPO generates prompts that emphasize procedural\nredundancy and compensatory strategies (e.g., feedback loops, real-time updates). This suggests the\nmodel benefits from prompts that scaffold task understanding through expanded heuristics rather than\nstrict symbolic reasoning.\n\n   LLaMA3.1-8B\n\n\n     Given a sequence of instructions, determine whether a user returns to the origin after following the instructions. The\n    ,→  instructions can include taking steps in a specific direction and turning in a specific direction.\n\n     - Use explicit directions (e.g., \"north\", \"south\")\n     - Keep all step sizes uniform (1 unit)\n     - Define the origin clearly as (0, 0)\n     - Use structured input like \"Take X steps [direction]\" or \"Turn Y degrees [left/right]\"\n\n     The user starts at (0, 0) facing north. Analyze whether they return to origin after completing all steps.\n\n\n  Analysis: On LLaMA3.1-8B, the optimized prompt prioritizes precision and input standardization.\nCompared to Qwen2.5-0.5B, this prompt uses less redundancy and more explicit spatial constraints,\naligning well with the model’s stronger logical parsing ability.\n\n   Qwen2.5-14B\n\n\n     Given a set of navigation instructions for an agent, your task is to determine if the agent ends up back at its initial\n    ,→  starting point. Follow these structured guidelines to methodically analyze the navigation instructions:\n\n     **Guidelines for Analyzing Navigation Instructions:**\n     1. **Initial Setup**: Position the agent at the Origin (Point 0) and orient it Facing Forward.\n     2. **Command Handling**:\n        - **Movement Commands**: Adjust the agent's position based on its orientation (Forward, Backward).\n        - **Orientation Commands**: Alter the agent's direction (Turn around, Turn left, Turn right) without changing its\n      ,→  position.\n     3. **Sequential Execution**:\n        - Process each command one by one.\n        - Update the agent's position and orientation after executing each command.\n     4. **Final Position Verification**:\n        - After all commands have been executed, check if the agent's position matches the initial Origin.\n        - Determine if the agent has returned to the starting point.\n     5. **Response Format**:\n        - Give chain of thought.\n        - Provide a clear \"Yes\" or \"No\" answer.\n        - Include a detailed explanation of your reasoning process.\n\n     **Systematic Reasoning Approach**:\n     - Start with the agent at the Origin and Facing Forward.\n     - For every movement command, move the agent according to its current orientation.\n     - For every orientation command, change the agent's direction without modifying its position.\n     - At the end, assess if the final position corresponds to the Origin.\n     - Explain your thought process thoroughly to justify the conclusion.\n\n\n  Analysis: Qwen2.5-14B benefits from richer instruction structures. PMPO’s optimized prompt includes\nexplicit reasoning breakdowns and distinguishes between movement and orientation changes, showcasing\nthe model’s ability to follow hierarchical and procedural plans.\n\n   Qwen2.5-32B\n\n\n     To effectively determine if a user returns to their origin after following a set of directional instructions, follow this\n    ,→  structured approach:\n     1. **Comprehend the Directions**: Thoroughly review the sequence of movements. Each command includes a direction (forward,\n    ,→  backward, left, right) and a step count.\n     2. **Set Up the Coordinate Plane**: Define a coordinate plane with the origin at (0,0). This plane will help track the\n    ,→  user's precise location throughout the journey.\n     3. **Update Coordinates According to Movements**:\n        - Increase the X-coordinate for forward moves by the step count.\n        - Decrease the X-coordinate for backward moves by the step count.\n        - Decrease the Y-coordinate for leftward moves by the step count.\n        - Increase the Y-coordinate for rightward moves by the step count.\n     4. **Assess the End Position**: After processing all commands, verify the end coordinates. A return to the origin is\n    ,→  confirmed when the user's final location is (0,0).\n     5. **Conclude with Clarity**: Based on the final coordinates, clearly communicate whether the user has reached the\n    ,→  starting point again.\n\n     This systematic procedure guarantees a precise and clear determination of the user's final position relative to the\n    ,→  initial point, addressing the navigation challenge comprehensively.\n\nAnalysis: The prompt tailored for Qwen2.5-32B reflects its strong symbolic reasoning capacity. PMPO\noptimizes for Cartesian reasoning by introducing coordinate systems, directional vectors, and arithmetic\nevaluation—enhancing spatial comprehension without overcomplicating the structure.\n\n    DeepSeek-R1-Distill-Qwen-1.5B\n\n\n     To check if someone returns to their origin point, add up all movements: count right steps as positive numbers and left\n    ,→  steps as negative numbers. If the sum equals zero, they've returned to the starting point. If not, they've ended\n    ,→  elsewhere.\n\n     Example: three steps right (+3) and three steps left (-3) gives 0, meaning they returned to origin. Four steps right (+4)\n    ,→  and three steps left (-3) gives +1, meaning they didn't.\n\n\n   Analysis: For distilled or small-scale models like DeepSeek-R1, PMPO simplifies the logic into linear\nadditive operations. This aligns with the model’s limited abstraction depth, ensuring the task is achievable\nwith minimal symbolic overhead.",
"headers": [
"arXiv:2505.16307v2  [cs.CL]  18 Sep 2025",
"PMPO: Probabilistic Metric Prompt Optimization for Small and Large",
"Language Models",
"Chenzhuo Zhao",
"Ziqian Liu",
"Xinda Wang",
"Junting Lu",
"Chaoyi Ruan",
"Peking University",
"Independent Researcher",
"National University of Singapore",
"{cyzcz, nev_settle, aidan.lew.37}@stu.pku.edu.cn",
"liuziqian25@gmail.com",
"ruancy@comp.nus.edu.sg",
"Abstract",
"1",
"Introduction",
"2",
"Related Works",
"3",
"PMPO Framework",
"4",
"Experiment",
"5",
"Conclusion",
"Limitations",
"References",
"A",
"Appendix",
"et al.",
",",
"2024",
";",
"Wang et al.",
"2023",
"). Recent methods",
"erences (",
"Cheng et al.",
"), or reward-based",
"approaches often face three main challenges: (1)",
"loops, (2) reliance on large models capable of in-",
"Prompt design has emerged as a critical factor",
"in steering large language models (LLMs) to-",
"ward reliable performance across diverse tasks.",
"As fine-tuning becomes increasingly costly or re-",
"stricted , automatic prompt optimization has be-",
"havior without modifying parameters (",
"Agarwal",
"PAS (",
"Zheng et al.",
") improve efficiency via",
"offline learning of prompt-rewriting models, but",
"ground truth by using the model’s own judgment",
"forward passes and likelihoods, not introspec-",
"tion or reasoning, making it usable for smaller",
"High sample efficiency.",
"Since loss evaluation",
"enabling extensive search with minimal over-",
"Early prompt engineering methods rely on man-",
"to guide language model behavior (",
"Brown et al.",
"2024b",
"). Chain-of-Thought (CoT) prompt-",
"ing (",
"Wei et al.",
"2022",
") enhances this paradigm by",
"stantial gains on complex tasks. Other structured",
"prompting methods, such as Step-Back prompt-",
"2023a",
"), introduce abstraction",
"or meta-reasoning steps to improve logical infer-",
"ence.",
"Similarly, Rephrase-and-Respond (",
"Deng",
") enhances model comprehension by",
"ing. These techniques improve performance by",
"tation, but they are based on fixed templates and",
"do not perform automated prompt generation or",
"In contrast, another line of work explores",
"automatic prompt construction.",
"Methods like",
"APE (",
"Shen et al.",
") treat prompt design as a",
"A central limitation across these approaches is",
"outputs are generated and then scored externally,",
"less reliable. External scoring often lacks consis-",
"over, methods tend to specialize: some focus on",
"target alignment tasks defined by preferences or",
"style. Few offer a unified, efficient mechanism",
"frames prompt optimization as a classic loss min-",
"imization problem: given a prompt and input, it",
"fully automated, and compatible with models of",
"varying scales, making PMPO applicable to both",
"ment settings. Moreover, PMPO supports both",
"preference-based tasks (e.g., summarization, in-",
"lihood of preferred responses over less preferred",
"existing approaches. Compared to prior methods,",
"Loss-based evaluation.",
"Unlike prior methods,",
"PMPO’s evaluation of candidate prompts re-",
"relying only on a single forward pass to com-",
"General",
"applicability.",
"PMPO",
"unifies",
"preference-based and supervised settings by",
"treating prompt optimization as maximizing",
"Recent work on automated prompt optimization",
"can be broadly categorized into generation-based",
"2024a",
"Guo12 et al.",
"Zhou et al.",
"puts or generating improvement suggestions, yet",
"). OPRO (",
"Yang et al.",
") treats the op-",
"timization process as a black-box loop, where",
"an LLM proposes new prompts based on previ-",
"ous attempts and their performance (e.g., accu-",
"lutionary strategies—maintaining populations of",
"prompts that evolve through mutation and selec-",
"mutation strategies. While these approaches of-",
"ten achieve strong performance, they rely on re-",
"them resource-intensive and task-specific. More-",
"over, their effectiveness depends heavily on large",
"ard (",
"Agarwal et al.",
") uses model-internal",
"critiques to iteratively improve prompts and in-",
"context examples. TextGuard (",
"Pei et al.",
")",
"ial test cases to probe and correct prompt weak-",
"nesses. Though effective in structured tasks with",
"clear correctness criteria, these methods often as-",
"sume explicit feedback or reference outputs, and",
"to iteratively refine prompts using fine-grained",
"cross-entropy evaluation and adaptive rewriting",
"strategies (see Figure",
"). PMPO systematically",
"analyzes prompts by applying a model-adaptive",
"(Step 1, detailed in Section3.2), and selectively",
"of prompt generation, evaluation, and refinement",
"(Step 3, corresponding to the overall loop in Sec-",
"tion3.1), PMPO automatically enhances prompt",
"flexibly across different model architectures and",
"Problem Definition",
"Given a task",
", our frame-",
"ages a problem description paired with a dataset",
", where each example consists",
"of an input",
", an output",
", and an associated la-",
"based loss across the entire dataset and selects",
"the most challenging examples, defined as those",
"with the highest losses, as priority targets for im-",
"provement. Based on the insights from the mask-",
"is retained for the next iteration. This refinement",
"process continues until the maximum number of",
"desirability of",
"for",
". The language model",
"conditioned on input",
"and instruction",
". Our",
"objective is to derive an optimal prompt",
"that",
"that is,",
", where",
"(e.g., QA/classification) we set",
", whereas",
"in preference-based tasks we form pairs and as-",
"sign",
"to the preferred output and",
"to the non-preferred one. This unified formula-",
"We propose an iterative algorithm that progres-",
"sively refines prompt quality based on evaluation",
"units",
". Instead of relying on prede-",
"prompt, which instructs the model to analyze the",
"instruction and segment it into up to 5 indepen-",
"disrupt the overall coherence of the prompt. The",
"model then wraps these selected segments with",
"quantify each component’s importance. For each",
"unit",
", we create a perturbed prompt variant by",
"Evaluate all prompts in",
"and select best",
"Here,",
"denotes the average cross-",
"entropy loss over the dataset under model",
". A",
"positive",
"indicates that",
"contributes posi-",
"tively to task performance (its removal increases",
"To quantitatively assess the effectiveness of",
"In each iteration, the PMPO framework begins",
"with a detailed, mask-guided analysis using the",
"model",
"to identify specific segments within",
"the current prompt that affect performance.",
"It",
"then computes the cross-entropy or preference-",
"where",
"is the",
"-th token of the output sequence",
"and",
"denotes all preceding tokens. The batch-",
"In scenarios where candidate outputs are asso-",
"corporate a pairwise preference loss inspired by",
"preference optimization techniques. Given a pre-",
"ferred output",
"and a less preferred alternative",
"be beneficial. During candidate generation, the",
"work is iterative and combines variant selection",
"The model is guided to first diagnose flaws in",
"These steps include rephrasing rigid wording, re-",
"plifying overly complex instructions, improving",
"ing overlapping rules, and enhancing overall lan-",
"guage quality. These edits are applied in an adap-",
"tive and integrated manner based on the specific",
"issues observed, preventing overfitting to individ-",
"ual cases and encouraging broadly effective im-",
"provements. Each hard example",
"yields a",
"measured by loss or accuracy, is selected as the",
"model-assigned log-probability,",
"is the sigmoid",
"A core advantage of PMPO is its computational",
"ages token level likelihoods, computing cross en-",
"tropy over the target outputs in a single forward",
"generation, selection, and refinement, this highly",
"batched scoring of many prompt candidates even",
"under limited compute. Prompt rewriting is like-",
"revised prompt variants without iterative reason-",
"ing, multi-step prompting, or introspective feed-",
"back. As summarized in Table",
", this combina-",
"To generate new prompt candidates, PMPO em-",
"ploys a model-in-the-loop rewriting mechanism",
"that leverages the language model itself to revise",
"instructions. Rather than using predefined rules",
"or templates, a rewriting prompt is constructed",
"for each selected hard example",
"identified",
"the current instruction",
". This rewriting prompt",
"the base for revision, the hard example",
"to",
"improving clarity, specificity, and structural qual-",
"scoring yields a favorable efficiency profile rela-",
"are computationally intensive and can underper-",
"form on smaller models with limited reasoning",
"20% of the examples for training (capped at 50)",
"In each optimization round, we choose the top-",
"most challenging samples and generate 4",
"on a single NVIDIA H800 GPU, with each full",
"Dataset.",
"We evaluate PMPO on a diverse set of",
"ing. For math problem solving, we use",
"GSM8K",
"reasoning capabilities, we adopt the",
"BBH",
"bench-",
"Baseline.",
"We compare PMPO against two cate-",
"gories of prompting methods across benchmark",
"manually designed prompting strategies:",
"CoT,",
"RaR, and StepBack, which enhance model rea-",
"soning or answer formulation through structural",
"or logical heuristics. The second category com-",
"prises recent automated prompt optimization ap-",
"Guard, and PromptBreeder, which leverage lan-",
"guage models to search, mutate, or iteratively im-",
"Implementation Details.",
"We conduct experi-",
"ments using a mix of open-source and propri-",
"Reasoning Tasks.",
"As shown in Table",
", our",
"ing approaches across a wide range of reasoning",
"and understanding tasks, evaluated on Qwen2.5-",
"significantly more than any other method. Un-",
"like generation heavy strategies like OPRO and",
"EvoPrompt, which use black box search or popu-",
"lation based mutation, PMPO’s single pass, loss",
"based evaluation enables more efficient optimiza-",
"Math Tasks.",
", our method",
"and PromptBreeder. Notably, both datasets pro-",
"vide complete solution steps rather than just final",
"erate detailed, accurate multi-step reasoning. In",
"not only enhances final answer accuracy but also",
"significantly improves the quality and fidelity of",
"five instruction categories. As shown in Figure",
"PMPO-optimized prompts raise the average win",
"rate of Qwen2.5-14B from",
"31.81%",
"51.52%",
"a substantial improvement. Notably, our method",
"These results show that PMPO enables mid-",
"sized models like Qwen2.5-14B to produce com-",
"petitive outputs, surpassing larger models such",
"as LLaMA3.1-70B (39.1%) and GPT-4 Turbo",
"(46.1%),",
"and nearly matching GPT-4 Omni",
"(51.3%). This highlights PMPO’s effectiveness",
"ing target tokens and querying the model at each",
"step to obtain token-level probabilities. However,",
"this procedure can lead to substantial token con-",
"sumption and high latency. Therefore, we do not",
"prietary systems, prompts optimized via PMPO",
"on open-source models still demonstrate strong",
"cross-model transferability. Although optimized",
"on Qwen2.5-32B, these prompts consistently im-",
"prove performance on the BBH:Navigate bench-",
"Results reveal a notable instruction-following",
"capacity gap: prompts optimized on large mod-",
"instructions. Furthermore, prompts consistently",
"that prompt effectiveness is closely linked to the",
"While PMPO is fully applicable to open-source",
"log-probabilities only for generated tokens, with-",
"To quantitatively validate our claim that PMPO",
"improves the fidelity of intermediate reasoning",
"GSM8K benchmark using a specialized Process",
"ity of individual steps in mathematical solutions.",
"the newline character as a delimiter. Each step",
"was then independently scored by the PRM, and",
"est average process reward score (0.9950), indi-",
"cating that the reasoning steps it generates are of",
"the highest quality among all compared methods.",
"This result provides direct, quantitative evidence",
"that PMPO not only excels in final answer accu-",
"that PMPO’s loss-guided refinement encourages",
"We present PMPO, a unified and efficient frame-",
"work for prompt optimization that relies on loss-",
"relying on output generation or human feedback,",
"following benchmarks show that PMPO consis-",
"racy and efficiency. Its lightweight design, com-",
"patibility with smaller models, and minimal need",
"To assess the contribution of individual compo-",
"14B for both optimization and evaluation. Hold-",
"ing the iterative refinement mechanism fixed, we",
"successively ablate Token Importance Masking",
"Loss (PrefLoss). TIM localizes low-performing",
"prompt spans via token-level loss attribution, en-",
"gles; removing it drops accuracy from 80.63%",
"to 79.05%. Eliminating BCA, which prioritizes",
"fLoss, our preference-learning objective used to",
"rank and select candidate rewrites, reduces accu-",
"Table",
"6",
"indicates that each module makes a non-",
"of open-source models, its application to propri-",
"ited. Most commercial APIs (e.g., OpenAI, An-",
"uation due to privacy constraints and concerns",
"mate likelihoods can be estimated via autoregres-",
"access to token-level log-probabilities (e.g., via",
"in those settings. We view this as a positive di-",
"itate research on prompt optimization and model",
"may exhibit reduced robustness. Since the opti-",
"mization directly minimizes model loss on a lim-",
"in such cases. If no additional data is introduced,",
"In this section, we present the meta prompts used in ours for prompt optimization, evaluation, and",
"Prompt Optimization (Large Model).",
"We first show the optimization prompt used with capable",
"whose removal yields significant loss changes. These segments are considered candidates for targeted",
"We evaluate PMPO across multiple datasets covering symbolic reasoning, math problem-solving, and",
"compared to average human raters. Each task includes 250 test examples, totaling 6,511 samples. We",
"GSM8K (",
"Cobbe et al.",
"2021",
") is a math word problem benchmark requiring multi-step",
"numerical reasoning. We use the standard test split (1,319 samples) for evaluation. For training, we",
"AQUA-RAT",
"AQUA-RAT (",
"Ling et al.",
"2017",
") contains multiple-choice math questions requiring",
"algebraic reasoning and textual comprehension. Following prior work, we use the full test set (2,371",
"AlpacaEval 2.0",
"AlpacaEval 2.0 (",
"Dubois et al.",
"2025",
") is a benchmark for evaluating instruction-",
"prompts spanning tasks such as open-ended generation, roleplay, summarization, and reasoning. We",
"(selected vs. rejected responses) from the public repository",
", which offers",
"high-quality alignment signals derived from automatic LLM feedback. From this set, we sample 50",
"training pairs with clear preference margins. These examples allow PMPO to optimize prompts for",
"Causal Judgment:",
"Given a short story (involving moral, intentional, or counterfactual analysis),",
"Date Understanding:",
"Given a small set of sentences about a particular date, answer the provided",
"Dyck Languages:",
"Predict the sequence of the closing parentheses of a Dyck-4 word without its last",
"Geometric Shapes:",
"Given a full SVG path element containing multiple commands, determine the",
"Logical Deduction:",
"Deduce the order of a sequence of objects based on clues about their spatial",
"Object Counting:",
"Given a list of items and quantities, determine the count of a certain object class",
"Ruin Names:",
"Given an artist, band, or movie name, identify a one-character edit that changes the",
"Tracking Shuffled Objects:",
"Given initial object positions and a series of swaps, determine final",
"by prompting the model to reformulate the input question before solving it. This internal clarification",
"OPRO (",
").",
"OPRO treats prompt design as a black-box optimization problem. An",
"LLM iteratively proposes and evaluates new prompts based on previous ones and their accuracy on",
"and evolve a population of prompts through mutation and selection. New prompts are generated via",
"PromptWizard (",
"PromptWizard uses a critique-and-synthesis loop where",
"one LLM analyzes prompt weaknesses and another generates refinements. This self-reflective mecha-",
"nism incrementally improves prompts based on failure analysis, with high sample efficiency and task",
"TextGrad (",
"TextGrad performs optimization via pseudo-gradients derived from",
"strategies in parallel. It uses co-evolution to generate increasingly effective prompts without human",
"SPO (",
"Xiang et al.",
"SPO avoids human-labeled data by comparing outputs from multiple",
"prompts and selecting better-performing variants through self-judgment. This self-supervised loop",
"this evolution along with the corresponding cross-entropy losses. As observed, the prompts gradually",
"early iterations achieve rapid improvements, while later refinements focus on polishing structure and",
"The degree of improvement across steps may vary due to factors like sampling randomness, batch",
"prompt generation lacks a stable optimization trajectory, making step-to-step changes inherently more",
"its evaluation signal. Computationally, unlike methods such as PromptWizard or OPRO that require",
"signal in PMPO is the cross-entropy loss, which captures per-token deviations from the target output",
"“If the reviewer says the filmmakers tried hard but the movie seems awfully sloppy, and",
"Such a prompt fails to generalize. In contrast, when optimizing over a small batch of top-k examples",
"conducted a comprehensive evaluation on a smaller model, Qwen2.5-7B. Table",
"A8",
"shows that PMPO",
"verbose. This verbosity can lead to performance degradation in smaller models, which may struggle",
"with capacity constraints and overly specified instructions. These findings further emphasize PMPO’s",
"To empirically validate this relationship, we conducted experiments on the BBH benchmark under",
"serving as the evaluation and execution model. For brevity, we show representative examples for each",
"setup by treating the step-by-step rationale as the supervision target. This enables PMPO to optimize",
"variant, where the correct answer choice is treated as the",
"selected",
"output and the incorrect options as",
"multi-step reasoning and precise arithmetic. Since each instance in the dataset comes with a detailed",
"rationale, we treat the full explanation as the supervision target in PMPO training. The optimization",
") to better align with the ground truth and reduce token-level loss. This shows",
"For AlpacaEval 2.0 (",
"), we use the",
"We construct preference-based supervision by treating the",
"outputs as positive targets and",
"the",
"rejected",
"ones as negatives. PMPO is then trained to minimize loss on preferred examples while",
"We compare the final prompts optimized by our PMPO framework across different language models,",
"all targeting the same BBH-Navigate task. The results demonstrate that PMPO consistently tailors",
"its optimizations to the reasoning abilities and strengths of the underlying model, while maintaining",
"redundancy and compensatory strategies (e.g., feedback loops, real-time updates). This suggests the",
"model benefits from prompts that scaffold task understanding through expanded heuristics rather than",
"Analysis:",
"On LLaMA3.1-8B, the optimized prompt prioritizes precision and input standardization.",
"Compared to Qwen2.5-0.5B, this prompt uses less redundancy and more explicit spatial constraints,",
"model feedback (",
"), human pref-",
"ard (72.8%), and ranks first on 11 out of 23 tasks,",
"boosts performance across all instruction subsets,",
"To assess adaptability across model scales and ar-",
"steps, we conducted an additional analysis on the",
"Our claim of",
"high sample efficiency",
"is rooted in PMPO’s computational design and the granularity of",
"rather than relying on a binary correctness score. This fine-grained signal provides a more informative",
"come a practical alternative to improve model be-",
"dent and removable semantic units, such as meth-",
"not constrain the optimization to specific regions;",
"dataset. One potential workaround is to construct",
"the model to emulate more structured and correct",
"scalable and efficient optimization. Experimental",
"high cost due to output generation and evaluation",
"ually crafted instructions and few-shot exemplars",
"prompting it to rephrase questions before answer-",
"Figure",
"presents an overview of the PMPO re-",
"cross-entropy, and the best-performing candidate,",
"multi-step numerical reasoning. To assess logical",
"perform best on the originating model, indicating",
"scoring the importance of fine-grained evaluation",
", PMPO achieves the high-",
"Temporal Sequences:",
"Given a person’s daily schedule, determine when they could perform another",
"sis, achieving strong results on complex tasks but",
"refined prompt variants, each designed to address",
"Analyze failure and token importance",
"model performs a one-shot analysis and proposes",
"datasets. The first category includes conventional",
"ited number of instances, it is prone to overfitting",
"level likelihoods through a single forward pass for each sample. This bypasses the expensive decoding",
"the highest scores, confirming the additive nature of the two techniques. Finally, performance does not",
"output without requiring the model to generate or",
"without relying on external classifiers, preference",
"answers, which we incorporate as positive targets",
"ment with complex mathematical reasoning tasks",
"For each prompting method, we parsed the gener-",
"Hyperbaton (Adjective Ordering):",
"Given two English-language sentences, determine the one with",
"trospective signals to refine prompts. PromptWiz-",
"generates outputs with probabilities",
"it merely suggests areas where targeted edits may",
"we computed the average score across all steps to",
"Disambiguation QA:",
"Given a sentence with an “ambiguous” pronoun, either determine whether the",
"PromptBreeder (",
"Fernando et al.",
"PromptBreeder evolves both task prompts and mutation",
"ones when comparative preference information is",
"variants that are not strictly limited to the masked",
"LLM-based segmentation is imperfect, the frame-",
"tently outperforms existing methods in both accu-",
"supervised tasks with explicit labels, while others",
"erence score. Concretely, in accuracy-based tasks",
"variant under consideration, PMPO directly lever-",
"task performance, we use",
", which",
"etary language models, including Qwen2.5 (0.5B,",
"during training. This enables PMPO to iteratively",
"ended instruction-following tasks, we conduct ex-",
"(",
"), we observe that the learned prompts consistently capture generalized patterns aligned with the",
"quires no output sampling or human evaluation,",
"Breeder (",
") further adopt evo-",
"the detected weaknesses. These candidates are re-",
"ciated with preference signals, we additionally in-",
"wise streamlined: for each high-loss example, the",
"ical inference, and open-ended instruction follow-",
"ulate human-like problem-solving behaviors such",
"continues to outperform strong baselines across the diverse tasks in the BBH benchmark, achieving the",
"5-shot performance in both settings. This suggests that an excessive number of examples can introduce",
"discouraging prompts that increase likelihood on rejected ones. This setup enables effective instruction",
"racy). EvoPrompt (",
"Tong et al.",
") and Prompt-",
"maximizes the expected weighted log-probability,",
"models that support token-level likelihood access,",
"first, and PrefLoss guides how to prefer improved",
"Prompt Evaluation.",
"To evaluate candidate outputs, we use a lightweight semantic comparison prompt",
"optimization framework. It evaluates prompt quality using token-level cross-entropy or preference loss,",
"the prompt to better capture general mathematical reasoning patterns. Table",
"A5",
"shows selected steps in",
"tackle this by iteratively refining prompts through",
"eration. Others like BPO (",
") and",
"and then apply a multi-step refinement strategy.",
"via temperature-controlled top-",
"sampling. These",
"thropic) do not expose full log-likelihoods, which",
"adherence to the general task formulation. Below, we present the optimized prompts with no additional",
"and rank instruction variants based on task perfor-",
"tency or sufficient granularity, especially when re-",
"require natural-language generation for both diag-",
"recommend applying the full PMPO optimization",
"out supporting full-sequence evaluation through a",
"abling edits to concentrate where the model strug-",
"composition, and the inherent difficulty of refining an already strong prompt. Unlike model fine-tuning,",
"Unlike binary accuracy metrics,",
"captures",
"method consistently outperforms existing prompt-",
"CoT (90.7% on GSM8K, 84.3% on AQUA-RAT),",
"ward to compute cross-entropy loss over an entire",
"a further decline to 77.96%. Finally, omitting Pre-",
"This prompt is used during the mask-guided analysis phase in PMPO to identify local prompt segments",
"following ability of language models using GPT-4 Turbo as the judge. The dataset contains 805 diverse",
"noise or cause instruction drift. In contrast, PMPO systematically improves the base prompt’s structure,",
"evaluated using batch-level loss performance, and",
"can represent either a binary label or a scalar pref-",
"that require multi step or spatial reasoning. While",
"including small, medium, and large, are evaluated",
"Additionally, in extremely low-resource scenar-",
"question (e.g., “The concert was scheduled to be on 06/01/1943, but was delayed by one day to today.",
"shows the loss curve, highlighting the performance gains across the 20 iterations. Notably,",
"reasoning capability to perform this segmentation.",
"nal rather than a hard pruning mechanism. It does",
"RAT, with accuracies of 94.0% and 84.6% respec-",
"restricts the direct use of PMPO’s loss-based eval-",
"use the full evaluation set for testing. For prompt optimization training, we use preference-labeled pairs",
"Movie Recommendation:",
"Given a list of movies a user might have watched and liked, recommend a",
"to compare outputs, but this self-evaluation is less",
"lying on models to assess complex outputs. More-",
"adopts a unit-test framework, generating adversar-",
"tion of token-level evaluation and multi-candidate",
"includes five key elements: the task description",
"PromptWizard is competitive on some logic tasks,",
"The strictly monotonic degradation observed in",
"model to generate intermediate steps before the final answer. It improves performance in arithmetic and",
"search (",
"). While effective, these",
"how prompt quality is evaluated. Most techniques",
"bel or preference score",
"indicating the quality or",
"and evaluate on the remaining set. The preference",
"ergistic gain beyond the core iterative loop, under-",
"(TIM), Bad Case Analysis (BCA), and Preference",
"rection, and hope that more commercial providers",
"Penguins in a Table:",
"Given a table of penguins (and sometimes new information), answer a question",
"monotonically increase with the number of shots; for example, the 7-shot performance is lower than the",
"mance. While these approaches reduce manual ef-",
") reduce reliance on labels by comparing out-",
"in Sections3.3 and3.4). Through an iterative cycle",
"These identified semantic units are then used to",
"in enhancing instruction alignment without model",
"generated responses with reference outputs across",
"els, though effective on similarly large or medium",
"high-loss examples for targeted refinement, yields",
"lenging for most learning algorithms, it highlights",
"In this section, we present the optimized prompts produced by our method during the main experiments.",
"ods, rules, or examples, whose removal would not",
"Given a model",
", a instruction",
", input",
", and",
"and architectural efficiency across both evaluation",
"GPT-4 Turbo as the evaluator, we compare model-",
"tional constraints. These models typically provide",
"The results lead to several key observations. First, the PMPO-optimized prompt in a zero-shot setting",
"generality across task types and model sizes.",
"rely on model-internal critique and iterative analy-",
"•",
"complex prompting.",
"head.",
"2.1",
"Prompt Engineering",
"optimization.",
"and efficient optimization frameworks.",
"reliable for smaller LMs.",
"applicable to both.",
"tasks with labeled outputs and maximizes the like-",
"available.",
"PMPO offers the following key advantages:",
"process highly efficient.",
"2.2",
"Prompt Optimization",
"the task signal available.",
"reward models.",
"tasks.",
"T",
"P",
"D",
"= (",
"x",
", y",
", r",
"y",
"iterations is reached.",
"3.2",
"Mask-Guided Importance Evaluation",
"r",
"M",
"|",
"x,",
"E",
"[",
"·",
"log",
")]",
"≡",
"= +1",
"=",
"−",
"preferences over desired outputs.",
"3.1",
"Iterative Framework for PMPO",
"dure is outlined in Algorithm",
".",
"Algorithm 1",
"An Overview of PMPO",
"Require:",
"k",
"Variants per sample",
"n",
"Ensure:",
"Optimized prompt",
"←P",
"iteration",
"←",
"do",
"Compute metric",
"L",
", M",
"on",
"Select top-",
"samples",
"{",
"}",
"s",
", s",
", ..., s",
"<mask>...</mask>",
"template can be found in",
"Appendix A.1",
"masking it:",
"Initialize variant set",
"V ←{P",
"each",
"using",
"Generate",
"variants",
"{P",
", ...,",
"<MASK>",
"(1)",
"entropy loss when",
"is masked:",
"V ←V ∪{P",
"∆",
"−L",
"(2)",
"end for",
"V",
"if",
">",
"then",
"end if",
"return",
"imply negligible impact on model behavior.",
"3.3",
"Prompt Evaluation via Loss-based",
"Metrics",
"the model’s internal probability estimates.",
"entropy loss as:",
"X",
"x, y,",
") =",
", x,",
"(3)",
"i",
"level loss over dataset",
"is computed as:",
"(4)",
"model confidence induced by different prompts.",
", we define:",
"x, y",
"σ",
"\u0010",
"β",
"\u0000",
"\u0001",
"\u0011",
"model is encouraged to explore multiple rewriting",
"and refinement across multiple cycles.",
"j,",
", . . . ,",
"j, n",
"p",
"updated prompt for the next iteration.",
"(5)",
"3.5",
"Efficiency of PMPO",
") = log",
"function, and",
"is a scaling factor.",
"3.4",
"Prompt Variant Generation",
"segments in",
"contributing most to the loss.",
"= 3",
"optimization taking approximately 20 minutes.",
"4.2",
"Experimental Results and Analysis",
"4.1",
"Experiment Settings",
"model responses against reference answers.",
"prove prompts without human intervention.",
"Prompt (78.0%), OPRO (77.1%), and PromptWiz-",
"symbolic and naturalistic reasoning.",
"math reasoning benchmarks, GSM8K and AQUA-",
"helpful",
"to 47.29%) and",
"oasst",
"(from 34.57% to 55.61%).",
"fine-tuning or explicit preference labels.",
"4.3",
"Analysis",
"for inference.",
"while maintaining generalizability.",
"process directly on these models in practice.",
"3.5 Haiku 20241022, and GPT-4o (Figure",
"4.5",
"Reasoning Process Fidelity",
"anisms of each model.",
"4.4",
"Transferability to Proprietary Models",
"applying it to proprietary systems introduces addi-",
"human-like problem-solving behaviors.",
"academic research and practical deployment.",
"4.6",
"Ablation Study",
"racy to 76.74%.",
"imitations.",
"prompt_logprobs",
"alignment.",
"timization.",
"A.1",
"Detailed Prompts of ours",
"masking strategy used for loss attribution.",
"instruction-following LMs:",
"capacity, we use a simplified variant:",
"that tolerates format variation:",
"based on their contribution to loss:",
"rewriting in subsequent iterations.",
"A.2",
"Experiment Details",
"A.2.1",
"Tasks and Data Details",
"instruction-following. Table",
"A7",
"summarizes dataset sizes and splits used in our experiments.",
"utilize the full BBH dataset for evaluation and randomly sample 50 examples for training.",
"randomly sample up to 50 examples from the training set.",
"questions) and randomly sample 50 training examples for optimization.",
"reciprocate/alpaca-eval",
"alignment-style objectives using cross-entropy on preferred vs. dispreferred completions.",
"Note:",
"* BBH refers to the complete set of 23 tasks in the BIG-Bench Hard benchmark.",
"constants (True, False) and basic Boolean operators (and, or, and not).",
"determine how a typical person would answer a causal question about the story.",
"What is the date yesterday in MM/DD/YYYY?”).",
"sentence is inherently ambiguous or state the antecedent of the pronoun.",
"few closing parentheses.",
"be logically deduced from a provided set of statements.",
"geometric shape that would be generated.",
"the correct adjective order.",
"relationships and placements.",
"new, relevant movie from a list of candidates.",
"Multi-Step Arithmetic:",
"Solve multi-step equations involving basic arithmetic operations.",
"point.",
"(e.g., fruits).",
"about the penguins’ attributes.",
"given context.",
"meaning humorously.",
"the type of translation error present.",
"Snarks:",
"Given two nearly-identical sentences, determine which one is sarcastic.",
"Sports Understanding:",
"Determine whether a fictitious sports-related sentence is plausible.",
"activity.",
"positions.",
"Web of Lies:",
"Evaluate the truth value of a Boolean function expressed as a word problem.",
"Word Sorting:",
"Given a list of words, sort them lexicographically.",
"A.2.2",
"Configuration",
"while the latter leverage LLMs or search algorithms to generate and refine prompts automatically.",
"Manually Designed Prompting Strategies",
"multi-step tasks by including phrases like “Let’s think step by step” to elicit structured reasoning chains.",
"particularly in STEM tasks, by helping the model organize relevant knowledge before execution.",
"reduces ambiguity and enhances robustness, especially in under-specified or complex queries.",
"Automated Prompt Optimization Approaches",
"LLM-guided variation, and top-performing variants are retained across generations.",
"specialization.",
"validation feedback, and supports optimization without requiring explicit supervision labels.",
"intervention, achieving state-of-the-art performance on complex reasoning benchmarks.",
"incrementally refines prompts based on model preference rather than external metrics.",
"smaller models with limited data.",
"A.2.3",
"Baseline Prompt",
"A.3",
"Case Study",
"Let’s think step by step",
"become more structured and specific in mathematical reasoning and format expectations.",
"clarity.",
"variable.",
"A.4",
"Further Analysis",
"A.4.1",
"Qualitative Analysis of Sample Efficiency",
"gradient, allowing the framework to learn more effectively from fewer samples.",
"task, training on a single negative review might yield an overly rigid instruction like:",
"with ‘Positive’.”",
"≥",
"but diverse set of examples to avoid overfitting.",
"A.4.2",
"Performance Evaluation on Smaller Models",
"highest average accuracy.",
"prompts are crucial.",
"A.4.3",
"Complementary Effects with Few-Shot Prompting",
"generalizability and few-shot examples providing task-specific grounding.",
"results are summarized in Table",
"A9",
"Number of Shots (",
"Prompt Type",
"Average Score",
"0",
"Base",
"73.75",
"79.21",
"75.02",
"80.69",
"76.69",
"81.11",
"76.89",
"82.19",
"7",
"76.58",
"81.59",
"9",
"77.97",
"82.37",
"16",
"78.17",
"82.23",
"improvement over few-shot prompting alone, and the two approaches are best utilized in tandem.",
"A.5",
"Prompt Optimized by ours",
"benchmark dataset.",
"model’s likelihoods.",
"prompts using cross-entropy loss over full reasoning sequences.",
", allowing PMPO to operate under both supervised and preference paradigms.",
"objective is to minimize cross-entropy loss over the complete reasoning sequence.",
"«calculation=answer»",
"that PMPO can adapt prompt style and structure based on dataset-specific signal during training.",
"(preferred) and",
"(less preferred) responses across various instruction-following tasks.",
"tuning without requiring explicit gold answers.",
"A.5.1",
"Cross-Model Prompt Analysis",
"examples, as used in our final evaluations.",
"strict symbolic reasoning.",
"aligning well with the model’s stronger logical parsing ability.",
"the model’s ability to follow hierarchical and procedural plans.",
"evaluation—enhancing spatial comprehension without overcomplicating the structure.",
"with minimal symbolic overhead.",
"mization framework that directly uses the model’s",
"still depend on costly output sampling or heuristic",
"models, or second pass LLM judges. This enables",
"behavior, yielding robust performance across both",
"generalization. While this setup is inherently chal-",
"step, making the optimization loop significantly faster and more scalable. Furthermore, the optimization",
"making the instruction more interpretable and robust to input variation. Thus, PMPO offers a significant",
"optimizes for Cartesian reasoning by introducing coordinate systems, directional vectors, and arithmetic",
"fining task constraints, removing redundancy, sim-",
"up to 20 iterations. All experiments are conducted",
"mark, designed to challenge models with complex",
"chitectures, we conduct cross model evaluation by",
"models, often degrade when transferred to smaller",
"sive token-by-token querying, this significantly in-",
"Navigate:",
"Given a series of navigation steps, determine whether the agent would return to the starting",
"segments via model-adaptive masking, generates variants based on hard examples, and selects improved",
"mentions many factual errors about Lucille Ball, respond with ‘Negative’. Otherwise, respond",
"cross-entropy loss",
"as the evaluation signal. PMPO",
"tinuous evaluation space, and reflects variations in",
"pass per example and thereby bypassing costly au-",
"intermediate steps. It encourages the model to em-",
"Reward Model (PRM),",
"Qwen2.5-Math-PRM-7B",
"racy but also generates more reliable and logically",
"sound intermediate steps. This validates our claim",
"not guarantee improvement in every single iteration. In cases where no generated candidate outperforms",
"generalizability and robustness, particularly in resource-constrained settings where concise and effective",
"enriching intermediate reasoning or input interpre-",
"target-aligned generation capability by amplifying",
"toregressive decoding. While the overall optimiza-",
"tion and stronger performance, especially on tasks",
"achieves the best performance on two widely used",
"PMPO (Probabilistic Metric Prompt Optimization).",
"Ours introduces a unified loss-based prompt",
"prompt optimization approaches that require an initial prompt use the same",
"COT Prompt",
"as the starting",
"various few-shot configurations, using both a standard base prompt and a PMPO-optimized prompt. The",
"Additionally, since AQUA-RAT is a multiple-choice task, we also support a preference-based training",
"finement loop and summarizes its advantages over",
"Another direction leverages the model’s own in-",
"by high preference-based cross-entropy loss under",
"prompt variants per sample. Optimization runs for",
"etary, closed-source language models remains lim-",
"Step-Back (",
"2023b",
"Step-Back prompting first guides the model to abstract high-level",
"candidates via likelihood minimization. Unlike black-box or introspection-based methods, ours supports",
"In this section, we present the baseline prompts used for comparison across all methods. For fairness, all",
"of instruction optimization alone. Second, combining PMPO with few-shot examples consistently yields",
"accuracy-based tasks (e.g., classification, QA) and",
"tion, with PromptBreeder uniquely co-evolving its",
"guided analysis, the framework generates multiple",
"expected output",
", we define the token-level cross-",
"ios (e.g., using only one training example), PMPO",
"segment-level masking. These prompts enable the framework to analyze, rewrite, and assess instructions",
"while PMPO focuses on refining the core instruction to improve its clarity, coverage, and alignment with",
"output, and the remaining distractor options as",
", enabling preference-based supervision using the",
"For distilled or small-scale models like DeepSeek-R1, PMPO simplifies the logic into linear",
"requiring significant computation and model coop-",
"LLMs serving as optimizers, limiting applicability",
"(Table",
", BBH:Navigate). Models of various sizes,",
"justification, thereby demonstrating stronger align-",
"All prompts were optimized using Qwen2.5-14B as the optimization model, with the original model also",
") and PromptAgent (",
"),",
"models is limited. SPO (",
") avoids",
", Max iterations",
", Top-",
"fined rules, we leverage the language model’s own",
"our masking formulation and the full meta-prompt",
"to maintain generality, the current instruction",
"as",
"ity, and a token-level mask analysis that highlights",
"PromptWizard, whose multi-step refinement loops",
"rigid prompts that limit generalization. In contrast,",
"PMPO uses token-level likelihoods to identify and",
"Reasoning about Colored Objects:",
"Answer a simple question about the color of an object based on a",
"To further demonstrate the empirical reliability and scalability of PMPO across different model sizes, we",
"(79.21) already outperforms the base prompt with 16-shots (78.17), demonstrating the substantial impact",
"rank outputs explicitly. This evaluation is efficient,",
"pute log-likelihoods, which makes its evaluation",
"metrics and targeted modifications. The full proce-",
"with evaluation to enable continuous improvement",
"Open-Source Cross-Model Generalization",
"tion study on the BBH benchmark using Qwen2.5-",
"Boolean Expressions:",
"Evaluate the truth value of a random Boolean expression consisting of Boolean",
"underscores the importance of prompt formulation",
"and introspection-based methods (",
"Yan et al.",
"to smaller models or open-ended preference-based",
"masking technique to quantify token-level impacts",
"guage model-generated variants (Step 2, described",
"PMPO produces lightweight and structurally adap-",
"tively. It outperforms all baselines, including APE,",
"internal reasoning and instruction-following mech-",
"will consider offering similar transparency to facil-",
"with minimal human intervention. We include variants adapted for large and small models, as well as the",
"Metric Prompt Optimization), a novel prompt opti-",
"capacity (",
"Zhang et al.",
"Mayilvaghanan et al.",
"as decomposition, variable definition, and numeric",
"effectiveness in optimizing prompts across a range",
"a fundamental limitation of data-scarce prompt op-",
"Formal Fallacies Syllogisms Negation:",
"Determine whether an argument—presented informally—can",
"and motivates the development of more systematic",
"treat prompt evaluation as a generative task, where",
"either output likelihood or reward, depending on",
"Dataset",
", Initial prompt",
", Language",
"14B-Instruct under a 1 shot setting for fair compar-",
"of 80.6%, surpassing strong baselines such as Evo-",
"To better understand how PMPO progressively improves prompt quality during optimization, we conduct",
"underlying task (e.g., abstract reasoning steps or classification criteria). This suggests PMPO can achieve",
"regions, ensuring comprehensive exploration. This",
"14B, 32B), LLaMA3.1 (8B), DeepSeek-R1-Distill-",
"based evaluation and iterative rewriting. Instead of",
"API-based frameworks, such as vLLM, do provide",
"Prompt Optimization (Small Model).",
"To accommodate smaller LMs with limited instruction-following",
"Salient Translation Error Detection:",
"Given a German sentence and its English translation, determine",
"the current prompt, the original prompt is retained as the starting point for the next round of optimization.",
"fort, they often rely on large models for generation",
") and TextGuard (",
"benchmarks covering mathematical reasoning, log-",
"automated prompt optimization frameworks. The former apply structural heuristics to improve reasoning,",
"generating full outputs via costly autoregressive decoding, PMPO evaluates prompts by computing token-",
"As a result, the optimized prompts tend to preserve the distinctive answer format seen in GSM8K (e.g.,",
"is batchable and cheap, PMPO can explore more",
"computes the likelihood of the desired or preferred",
"logical flow, expanding underspecified parts, merg-",
"and candidate prompt generation. For each prompt",
"ison. PMPO achieves the highest average accuracy",
"in a zero shot manner using the optimized prompts",
"Despite the limited access to loss signals on pro-",
"observe how PMPO rewrites the prompt over 20 iterations by identifying high-loss examples and refining",
"the task’s intent. The benefits of both approaches can be additive, with PMPO enhancing the instruction’s",
"For smaller models like Qwen2.5-0.5B, PMPO generates prompts that emphasize procedural",
"and evaluation, limiting scalability in low-resource",
"refine prompts using token-level confidence (cross-",
"entropy loss), effectively guiding the model to gen-",
"the resulting prompt variants may align too closely",
"selected from the BIG-Bench suite, focusing on areas where language models previously underperformed",
"trospection or multi-step reasoning, and (3) lack of",
"require substantial labeled data or preference anno-",
"tion enables the framework to enhance the model’s",
"its self critique based rewrites often create verbose,",
"refine underperforming prompt segments, enabling",
"), allowing PMPO to be applied",
"is prone to overfitting to lexical artifacts or task-specific details. For instance, in a sentiment classification",
"PMPO and n-shot prompting are not mutually exclusive; rather, they target complementary dimensions of",
"explicit reasoning breakdowns and distinguishes between movement and orientation changes, showcasing",
"LMs that cannot critique their outputs or support",
"making the process computationally expensive and",
"large and small language models in diverse deploy-",
"uation strategy: it minimizes cross-entropy loss for",
"rewrites the lowest-performing segments using lan-",
"variants are pooled and evaluated using batch-level",
"training examples. Although effective, it depends on repeated generation and evaluation over full datasets.",
"The prompt tailored for Qwen2.5-32B reflects its strong symbolic reasoning capacity. PMPO",
"In this work, we introduce",
"(Probabilistic",
"scaling factor",
"is fixed at 1 across all experiments.",
"ones, which may struggle with complex or verbose",
"around model behavior leakage. Although approxi-",
"BBH*",
"The BIG-Bench Hard (BBH) benchmark (",
"Suzgun et al.",
") comprises 23 challenging tasks",
"Rephrase-and-Respond (",
"Deng et al.",
"Rephrase-and-Respond (RaR) improves answer quality",
"Some methods, such as PromptWizard (",
"tations, and their applicability to new tasks or small",
"Support for small models.",
"PMPO requires only",
"candidate prompts under fixed budget constraints,",
"2020",
"2023c",
"Wang",
"encouraging step-by-step reasoning, leading to sub-",
"search problem, using language models to generate",
"or small-model settings. Overall, this body of work",
"struction following) under a unified loss-based eval-",
"peated output generation and scoring, which makes",
"nosis and revision. Self-supervised approaches like",
"We introduce PMPO, a unified framework designed",
"effectiveness without human intervention, adapting",
"work initializes with a base instruction",
"and lever-",
"the variant demonstrating the greatest improvement",
"To identify influential components within a instruc-",
"tion",
", we first decompose it into a set of semantic",
"This process is driven by a carefully designed meta-",
"tags. A detailed description of",
"We then compute the change in batch-level cross-",
"loss), whereas a negative value suggests a detrimen-",
"tal or redundant effect. Values of",
"close to zero",
"prompts, we utilize loss-based metrics derived from",
"token-wise generation probabilities, provides a con-",
"approach also confers robustness: even if the initial",
"set of revised prompts",
"generated",
"tion framework is iterative, cycling through prompt",
"efficient per candidate scoring enables rapid evalua-",
"tion of many variants under a fixed compute budget",
"expose weaknesses, editing instructions focused on",
"The mask-guided analysis functions as a soft sig-",
"tive to generation-heavy methods such as OPRO or",
", which require models to perform",
"inference tasks under minimal guidance. For evalu-",
"ating general instruction-following and open-ended",
"includes a broad range of user instructions and uses",
"GPT-4 Turbo as an automatic evaluator to compare",
"proaches: OPRO, EvoPrompt, PromptWizard, Text-",
"Qwen (1.5B). For each dataset, we randomly select",
"tive prompts that more effectively align with model",
"contrast to methods like Chain-of-Thought that rely",
"on manually designed heuristics, PMPO’s approach",
"including difficult ones like",
"(from 17.83%",
"applying prompts optimized on one model to others",
"Open-ended Dataset.",
"To evaluate PMPO on open-",
"periments on the AlpacaEval 2.0 benchmark. Using",
"step-wise input sequences by incrementally append-",
"mark when applied to GPT-3.5 Turbo 0613, Claude",
"single forward pass. As a result, it is not straightfor-",
"This model is specifically trained to assess the qual-",
"ated solutions into individual reasoning steps using",
"and targeted rewriting for maximizing performance.",
"determine the final process reward for each sample.",
"results on reasoning, mathematical, and instruction-",
"for manual supervision make it well-suited for both",
"nents within PMPO, we conduct a cumulative abla-",
"redundant, complementary contribution: TIM iden-",
"tifies where to edit, BCA focuses which cases to fix",
"rewrites. Together, these mechanisms deliver a syn-",
"Despite the promising results, our study has several",
"While PMPO demonstrates strong efficiency and",
"creases latency and token usage, making it impracti-",
"cal for large-scale optimization. Nonetheless, some",
"with the few observed examples, leading to reduced",
"Mask Generation.",
"For prompt segmentation, we generate masked variants to localize ineffective segments",
"We compare ours against two categories of baseline methods: manually designed prompting strategies and",
"Chain-of-Thought (CoT) (",
"CoT prompting enhances reasoning by encouraging the",
"concepts before applying them to the specific task. This abstraction step enables more principled reasoning,",
"EvoPrompt (",
"EvoPrompt combines evolutionary algorithms with LLMs to maintain",
"LLM-generated feedback. It applies gradient descent-like updates to prompts, reverting bad updates using",
"requiring only forward passes without output sampling. In each iteration, ours identifies low-utility prompt",
"both supervised and preference tasks under a consistent evaluation scheme and demonstrates scalability to",
"a case study on the AQUA-RAT dataset. Starting with the base prompt",
", we",
"Even with the use of mask-based analysis and model-in-the-loop rewriting for supervision, PMPO does",
"However, the number of samples remains a critical factor. When optimizing on only one sample, PMPO",
"stable optimization with very few samples (e.g., 3–5) by leveraging the cross-entropy signal across a small",
"Notably, the instruction rewrites generated by methods like PromptWizard tend to be significantly more",
"prompt design. Few-shot prompting conveys task format and solution patterns through concrete examples,",
"BBH (Big-Bench Hard)",
"For PMPO training on BBH, we treat the correct answer option as the",
") contains multiple-choice math questions requiring algebraic",
"reasoning and textual comprehension. The dataset includes detailed rationales for each question, making it",
"suitable for supervision via explanation. In our experiments, we adopt a non-preference-based optimization",
") is a benchmark of grade-school math word problems that require",
"dataset as our training source. This dataset contains a large number of paired outputs annotated as",
"Qwen2.5-14B benefits from richer instruction structures. PMPO’s optimized prompt includes",
"additive operations. This aligns with the model’s limited abstraction depth, ensuring the task is achievable"
],
"tables": [
"|Method|Evaluation Level|Candidate Count|\n|---|---|---|\n|SPO<br>PromptWizard<br>EvoPrompt<br>OPRO<br>**PMPO**|Sequence-Level<br>Sequence-Level<br>Sequence-Level<br>Sequence-Level<br>Token-Level|Single<br>Multiple<br>Multiple<br>Single<br>Multiple|",
"|Task Name AO CoT RaR StepBack|OPRO EvoPrompt PromptWizard|Ours|\n|---|---|---|\n|boolean_expressions<br>0.756 0.920 0.952<br>0.936<br>causal_judgement<br>0.674 0.631** 0.695**<br>0.658<br>date_understanding<br>0.684 0.740 0.708<br>0.752<br>disambiguation_qa<br>0.656 0.776 0.716<br>0.640<br>dyck_languages<br>0.096 0.240 0.236<br>0.228<br>formal_fallacies<br>0.704 0.800 0.784<br>0.808<br>geometric_shapes<br>0.440 0.616 0.576<br>**0.684**<br>hyperbaton<br>0.632 0.704 0.768<br>0.848<br>logical_deduction<br>0.692 0.8560.844<br>0.847<br>movie_recommendation<br>0.564 0.636 0.624<br>0.640<br>multistep_arithmetic_two<br>0.052 0.968 0.972<br>0.956<br>navigate<br>0.660 0.908 0.856<br>0.924<br>object_counting<br>0.508 0.812 0.772<br>0.756<br>penguins_in_a_table<br>0.753 0.945 0.932<br>0.952<br>reasoning_about_colored_objects<br>0.7080.8920.768<br>0.880<br>ruin_names<br>0.632 0.660 0.556<br>0.716<br>salient_translation_error_detection 0.600 0.572 0.604<br>**0.644**<br>snarks<br>0.831 0.809 0.837<br>0.848<br>sports_understanding<br>0.752 0.660 0.680<br>0.804<br>temporal_sequences<br>0.832 0.908 0.864<br>0.900<br>tracking_shuffled_objects<br>0.599** 0.900** 0.852<br>0.847<br>web_of_lies<br>0.536 0.9000.972<br>0.920<br>word_sorting<br>0.276 0.444** 0.624**<br>0.600|0.972<br>0.952<br>0.976<br>0.636<br>0.647<br>0.599<br>**0.800**<br>0.772<br>0.636<br>0.848<br>0.760<br>**0.892**<br>**0.392**<br>0.308<br>0.220<br>**0.856**<br>0.792<br>0.816<br>0.580<br>0.620<br>0.508<br>0.740<br>0.756<br>0.752<br>0.767<br>**0.864**<br>0.845<br>0.636<br>0.596<br>0.676<br>**0.988**<br>0.948<br>0.976<br>0.896<br>0.944<br>0.848<br>0.688<br>0.876<br>0.832<br>0.932<br>**0.959**<br>0.726<br>0.876<br>0.872<br>**0.912**<br>0.788<br>0.680<br>0.692<br>0.624<br>0.604<br>0.504<br>0.843<br>**0.882**<br>0.787<br>0.828<br>0.812<br>0.544<br>**0.964**<br>0.916<br>0.900<br>0.860<br>0.871<br>0.839<br>0.820<br>0.900<br>0.716<br>0.388<br>0.608<br>0.544|**0.984**<br>**0.695**<br>0.784<br>0.736<br>0.256<br>0.816<br>0.676<br>**0.896**<br>**0.864**<br>**0.684**<br>**0.988**<br>**0.960**<br>**0.884**<br>0.952<br>0.888<br>**0.840**<br>0.600<br>0.826<br>**0.836**<br>0.944<br>0.880<br>**0.976**<br>0.580|\n|**Best performing tasks**<br>0<br>1<br>2<br>2<br>**Average Accuracy**<br>0.593 0.752 0.747<br>0.773|5<br>3<br>2<br>0.770<br>0.780<br>0.728|**11**<br>**0.806**|",
"|Method|GSM8K|AQUA-RAT|\n|---|---|---|\n|AO<br>APE<br>COT<br>RaR<br>Step-back|0.871<br>0.939<br>0.907<br>0.932<br>0.925|0.760<br>0.827<br>0.843<br>0.843<br>0.811|\n|OPRO<br>PromptBreeder<br>PromptWizard<br>Textguard|0.936<br>0.917<br>0.882<br>0.939|0.819<br>0.831<br>0.799<br>0.807|\n|**Ours**|**0.940**|**0.846**|",
"|Target \\<br>Prompt Source|Qwen2.5<br>0.5B|DeepSeek<br>1.5B|LLaMA<br>3.1–8B|Qwen2.5<br>14B|Qwen2.5<br>32B|\n|---|---|---|---|---|---|\n|Qwen2.5-0.5B<br>DeepSeek-1.5B<br>LLaMA-3.1–8B<br>Qwen2.5-14B<br>Qwen2.5-32B|**0.580**<br>0.612<br>0.708<br>0.912<br>0.972|0.568<br>**0.772**<br>0.792<br>0.948<br>0.948|0.464<br>0.700<br>0.800<br>0.896<br>0.952|0.500<br>0.640<br>0.852<br>**0.960**<br>0.972|0.580<br>0.584<br>**0.860**<br>0.956<br>**0.980**|",
"|Setting|TIM|BCA|PrefLoss|Acc. (%)|\n|---|---|---|---|---|\n|Full<br>w/o TIM<br>w/o TIM,BCA<br>w/o TIM,BCA,PrefLoss|✓<br>✗<br>✗<br>✗|✓<br>✓<br>✗<br>✗|✓<br>✓<br>✓<br>✗|**80.63**<br>79.05<br>77.96<br>76.74|",
"|Prompting Method|Process Reward Score|\n|---|---|\n|AO<br>PromptWizard<br>CoT<br>PromptBreeder<br>RaR<br>OPRO<br>APE<br>Step-back<br>TextGrad<br>**PMPO(Ours)**|0.5510<br>0.8867<br>0.9637<br>0.9776<br>0.9910<br>0.9926<br>0.9930<br>0.9932<br>0.9940<br>**0.9950**|",
"|Dataset Name|Test Size Train (max)|\n|---|---|\n|BBH*<br>GSM8K<br>AQUA-RAT<br>AlpacaEval 2.0|6,511<br>1304<br>1,319<br>50<br>254<br>50<br>805<br>50|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2505.16307v2.pdf"
}