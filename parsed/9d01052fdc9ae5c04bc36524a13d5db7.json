{
"text": "PLHF: Prompt Optimization with Few-Shot Human Feedback\n\n\n                           Chun-Pai Yang†, Kan Zheng‡, and Shou-De Lin∗\n                              Intelli-Train.ai†‡∗, ZRT Technology‡, National Taiwan University†∗\n           chunpai@intelli-train.ai, kanzheng05@intelli-train.ai, sdlin@csie.ntu.edu.tw\n\n\n\n\n\n                          Abstract                   prompt for the LLM, existing methods often em-\n                                                            ploy gradient descent or other algorithms (Yang\n                Automatic prompt optimization frameworks\n                                                                           et al., 2024; Zhou et al., 2023; Guo et al., 2023) to\n                   are developed to obtain suitable prompts for\n                                                             optimize the performance with respect to desired                   large language models (LLMs) with respect2025                   to desired output quality metrics. Although       metrics (i.e., definition of output quality). Over-\n                   existing approaches can handle conventional          all, the key to success heavily relies on the output\n                    tasks such as fixed-solution question answering,        quality evaluations which shall precisely reveal theMay             defining the metric becomes complicated when      model performance to the optimizer.\n                   the output quality cannot be easily assessed\n11          by comparisons with standard golden samples.        Although such output quality metrics are often                                                                well-defined for the tasks which can be modeled\n                 Consequently, optimizing the prompts effec-\n                                                                as the traditional discriminative tasks (e.g., classi-                     tively and efficiently without a clear metric be-\n               comes a critical challenge. To address the issue,        fications and regressions) where the performance\n             we present PLHF (which stands for Prompt       can be directly evaluated given ground-truths, grad-\n                 Learning with Human Feedback), a few-shot       ing the outputs often becomes non-trivial for most[cs.CL]           prompt optimization framework inspired by the       generation-type of tasks. The absence of a precise\n                well-known RLHF technique. Different from       metric would hinder the effectiveness of the prompt\n                   näive strategies, PLHF employs a specific eval-                                                                optimization process for a generative task.\n                  uator module acting as the metric to estimate\n                                                      Most prompt optimization methods adopt two\n                   the output quality. PLHF requires only a sin-\n                                                                 types of mechanisms to evaluate the generated out-                   gle round of human feedback to complete the\n                    entire prompt optimization process. Empirical        puts. The first is employing simple evaluators, such\n                    results on both public and industrial datasets        as exact-matching or soft-matching (based on sim-\n               show that PLHF outperforms prior output grad-         ilarity) to compare the generated outputs with the\n                  ing strategies for LLM prompt optimizations.        observed samples. The second common strategy is\n                                                                     to utilize pre-trained LLMs for the output grading.\n          1  Introduction\n                                                            For instance, several studies (Wang et al., 2023; Fu\n            General-purpose large language models (LLMs)    et al., 2024) leverage advanced OpenAI GPT mod-\n            have shown substantial capabilities across vari-    els (Achiam et al., 2023) as the judges to grade thearXiv:2505.07886v1      ous fields in recent years. However, solving com-   generated outputs. Nevertheless, such graders suf-\n             plex tasks with LLMs often requires appropriate    fer from a critical drawback as generic pre-trained\n             customizations on LLMs to fit the task require-  LLMs might not have enough contextual or back-\n            ments. While fine-tuning pre-trained LLMs is a   ground knowledge to behave as accurate evaluators.\n          common approach, it may be infeasible when there   (See Figure 1 for the examples of failure cases.)\n                is limited training data, restricted computational   As a result, it is highly demanded to involve real\n              resource, or when working with a black-box LLM.  human experts to evaluate generated results from\n              Alternatively, previous studies (Wang et al., 2022;   LLMs, yet such schemes often suffer from budget\n            Shin et al., 2020) have shown that the potential    constraints of employing human experts.\n             of LLMs can also be fully leveraged with suitable      For most of the automatic prompt optimization\n             prompts. Recent literature develops automatic few-   frameworks (Khattab et al., 2024; Yuksekgonul\n             shot prompt optimization for LLM usages, such as    et al., 2024; Pryzant et al., 2023; Deng et al., 2022;\n        DSPy (Khattab et al., 2024) and TextGrad (Yuk-  Wen et al., 2024), multiple iterations are performed\n            sekgonul et al., 2024). To determine an effective   with a given output quality metric. With human ex-\n\nUser                        Say a joke with novelty                      Provide an easy math problem\n                                                                                                    for students\n\n                Why did the scarecrow become a                   Sarah has 7 apples and her friend\n                       successful neurosurgeon?                         John gives her 3 more apples.\n  ChatBot          Because he was outstanding in his               How many apples does Sarah\n                                field... and great at brain picking!                    have in total?\n\n                 GPT-3.5: 7/10 GPT-4o: 7/10  Target Users: 5/10        GPT-3.5: 8/10 GPT-4o: 9/10 Target Users : 5/10\n  Scoring                        * User Feedback: “Not so funny; no obvious                      * User Feedback: “Indeed an easy problem, but way\n                   connections between scarecrow and neurosurgeon.”          too easy; in common sense, we don’t need AI for this.”\n\nFigure 1: Demonstrations of the actual failure cases that the evaluations from pre-trained LLMs have different\npreference from specific humans. The first (left) example is the task of joke generation, where the grading is\naccording to funniness and novelty. The second scenario is a math problem generation bot, where the response\nquality is evaluated based on helpfulness and problem quality. As shown above, the verdicts of state-of-the-art\nLLMs could still differ from real human’s preferences.\n\n\n\n Positive       Samples\n  Pairs: (input,                output)      LLM MR, init prompt PR     Output O0              Triplets:Labeled(input,Samplesoutput, score)    LLM ME, init prompt PE     Scores S0\n\n               Iteration 1   PO     Score E(O0)       Evaluator E                             Iteration 1        PO        Loss L(S0, Slabel)\n\n                   LLM MR, prompt P’R      Output O1                           LLM ME, prompt P’E      Scores S1\n\n              Iteration 2   PO     Score E(O0)       Evaluator E                             Iteration 2        PO        Loss L(S1, Slabel)\n\n\n                  LLM MR, prompt P’R      Output O2                           LLM ME, prompt P’E      Scores S2\n         …                   …\n       (a) Prompt optimizations for Responder R.            (b) Prompt optimizations for Evaluator E with metric L.\n\nFigure 2: Workflow framework of PLHF. The entire LLM program contains two modules, Responder R and\nEvaluator E, where PO can be PO arbitrary prompt optimization method.\n\n\npert inquiries acting as the metric, once the prompt   module is relatively typical (e.g., binary classifi-\nis updated with any modifications, we have to ask    cations or regressions), we can leverage any exist-\nhuman experts for their judgment repeatedly for    ing automatic few-shot prompt optimization frame-\neach sample — this causes serious efficiency bot-   works (e.g., DSPy (Khattab et al., 2024)) with triv-\ntleneck. To address the aforementioned issues, we     ial metrics (e.g., Accuracy or Mean Absolute Error)\npresent PLHF, a novel few-shot prompt optimiza-    to obtain the evaluator module. Finally, we can take\ntion framework with two specific modules. Inspired   advantage on a prompt-optimized evaluator acting\nby the famous Reinforcement Learning from Hu-   as the metric for the original task.\nman Feedback (RLHF) technique (Ouyang et al.,     To verify the actual effectiveness of PLHF, we\n2022; Li et al., 2023), PLHF introduces a particu-   conduct experiments on multiple public datasets\nlar evaluator module that requires human grading   and a real-world industrial data collected from a\nno greater than linear (with respect to the number   customer support chat-bot product of an AI com-\nof training samples) times during the optimization    pany. The experimental results shown in Section 3\nprocess. To leverage human feedback in few shots,   demonstrate that PLHF can boost the output quality\nwe consider utilizing a prompt-optimized LLM as    of existing automatic few-shot prompt optimization\nE to evaluate the output of the main responder R.   frameworks with our duo-module design.\nThe overall framework is depicted in Figure 2.          In summary, our contributions are as follows:\n\n   Specifically speaking, first, we employ human                                                                 • We study few-shot prompt optimization for\nexperts to provide judgments as scores on a set                                     LLMs with limited number of human feed-\nof training samples, containing input-output pairs.                                                 back calls, which is a more reasonable and\nThen, we perform prompt optimization on another                                                             feasible setting in real-world applications.\nLLM to mimic the human experts’ preference pat-\ntern. Since the prompting task on the evaluator        • We introduce PLHF, a novel prompt optimiza-\n\ntion framework that does not directly rely on    the entire evaluation process operates without any\n     well-defined evaluation metrics. Instead, we   human labor expenses.\n     design an evaluator module to provide an au-\n                                                    2.2  Responder Task     tomatic grading mechanism.\n                                         The task for responder R is performing the orig-\n    • With extensive experiments on public datasets                                                        inal assignment. The core of this component is\n    and an  industrial  dataset, we show  that                                                       also a base LLM, denoted as MR, which generates\n   PLHF has superiority toward output qual-                                                  outputs for input queries. For MR, the prompt is\n       ity, compared with the approaches employing                                                        starting with an initial setting PR which simply\n     string matching or adopting the state-of-the-                                                   describes the task. The training samples for R in-\n      art LLMs (e.g., GPT-4o) as the evaluator.                                                   clude input-output pairs with positive score/verdict\n  We have also surveyed relevant prior efforts,   labeled by human experts. The data positivity can\nwhich are summarized into a Related Work sec-   be specifically defined to align with the require-\ntion in Appendix A.                            ments of the assigned AI task.  The generated\n                                                 responses by the LLM MR with prompt PR are\n2  PLHF: Prompt Optimization with                                                 then judged by our evaluator module E to per-\n   Few-shot Human Feedback                                            form prompt optimizations on LLM MR to obtain\n  As shown in Figure 2, our entire framework,   a prompt-optimized prompt P R′ for MR.\nPLHF, is designed to perform prompt optimiza-\n                                                    2.3  Duo-module Integration\ntions for typical language model program usages —\n                                            With the integration of responder R and evaluatoroutput a proper response based on the given input.\n                                         E, the entire system operates as described in Al-The whole process is guided by the principal intu-\n                                                gorithm 1. At the beginning, we initialize promptsition of few-shot in-context learning (Brown et al.,\n                                                   as PR′ := PR and P E′ := PE for modules R and2020) to capture contextual patterns from a limited\n                                         E, respectively. In each iteration of PLHF, first,number of labeled samples. Since there is no ex-\n                                                      training data samples D are used to optimize theplicit metric available, a grading function is needed\n                                                    evaluator E (i.e., to update PE).′   Then, we opti-for existing prompt optimization methods. Hence,\n                                            mize the responder R (i.e., update P R)′  regardingwe introduce an evaluator module E, acting as the\n                                                     the evaluator E with prompt PE′ as the metric. Af-grading function for the main responder module\n                                                               ter an iteration of prompt optimizations for P R′ andR. The two modules correspond to two respective\n                                  P E,′ we obtain a version of optimized prompts forsubtasks.\n                                                     the responder R and the evaluator E, respectively.  In Appendix B, we provide an example of a toy\n                                                         Overall, the proposed PLHF framework is capa-problem to demonstrate the initial prompts and the\n                                                     ble of performing prompt optimizations for LLMsoptimized prompts obtained from PLHF.\n                                                even when occurring challenges of (a) no available\n2.1  Evaluator Task                              well-defined metrics to evaluate the LLM output\nThe auxiliary task for evaluator E is designed to    quality for the specific task, (b) limited number\ngrade the outputs generated by the responder R.   (few-shot) of labeled samples for LLM prompting,\nThe evaluator E is built with a base LLM, denoted   and (c) multiple valid outputs for a single input.\nas ME. The training samples for E are triplets of\n                                       3  Experiments\ninput, output, and the corresponding score graded\nby human experts. To provide a nuanced verdict,   To evaluate the performance and robustness of our\nwe also optimize the prompt PE for ME with a   proposed model framework, PLHF, we conducted\ntrivial metric L (e.g., the conventional Accuracy   a series of experiments across various tasks. The\nor Mean Absolute Error) to evaluate the quality of    tasks were selected to test the model ability to gen-\nthe response. The metric L is specifically defined    erate accurate and contextually relevant outputs,\nas the loss to estimate the difference between a   while also assessing the effectiveness of the auxil-\npredicted score and the actual graded score.           iary evaluator task in refining responses.\n  With the grading ability of the evaluator E,\n                                                    3.1  DatasetsPLHF ensures that the outputs from the responder\nR are not only technically accurate but also contex-  We conduct the experiments on three public\ntually appropriate for the task. Most importantly,    datasets and one industrial dataset from a real-\n\nAlgorithm 1 PLHF: A duo-module Framework for Few-shot LLM Prompt Optimizations\nInput: Training data samples D = {d1, d2, . . . , dn} (each di is a triplet of input/query qi, output oi and\n    score/verdict ri), Base LLMs MR, ME, Initial prompts PR, PE, The trivial metric L for evaluator E.\nOutput: Optimized prompts PR′ and P E.′\n                   ′                            ′\n  1: Set P R := PR and P E := PE.\n  2: while there are new training samples added into D do\n                             ′\n  3:   while P E is not yet optimized (converged) for D do\n  4:      for each training sample di = (qi, oi, ri) ∈D do\n                                                                                                                                                                        ′\n  5:        Input (qi, oi) pair to generate the score (or verdict) ˜ri by ME with prompt PE.\n  6:     end for\n  7:     Compute conventional metric score SE = L({˜r1, . . . , ˜rn}, {r1, . . . , rn}).\n                                                     ′                                                              ′\n  8:      Consider the D, PE and SE to update the prompt PE.\n  9:   end while\n                             ′\n 10:   while P R is not yet optimized (converged) for D do\n 11:      for each training sample di = (qi, oi, ri) ∈D with positive rating ri do\n                                                                                                                                 ′\n 12:        Input qi to generate the output ˜oi by MR with prompt P R.\n 13:     end for\n                                                                                                                                                                             ′\n 14:      Evaluate the outputs by evaluator E. Obtain the score SR = ME({˜o1, . . . , ˜on}; P E).\n                                                     ′                                                              ′\n 15:      Consider the D, PR and SR to update the prompt PR.\n 16:   end while\n 17: end while\n                           ′       ′\n 18: return P R, PE\n\n\nworld scenario of question answering chat-bot gen-  We use the training set of the first essay set for\nerating SQL commands.                          our experiments. The actual text of the student’s\n                                                response is included. We consider average score\n3.1.1  Schema Guided Dialogue Dataset                                                     as the aggregated result from the raters. The scores\nThe Schema Guided Dialogue (SGD) dataset (Sun    are distributed from 1 to 30. In our experiments, we\net al., 2021) is a large-scale dataset designed for    discard the essays with transcription errors (marked\ntask-oriented dialogue systems. It comprises dia-   as “illegible” or containing placeholder text such\nlogues collected in English, specifically designed    as “???”) from the training data.\nto encompass a wide range of dialogue scenarios,     Apart from AES-ASAP, our experiments also\nschema-based actions, and services. The dataset    include a newer essay scoring dataset of from\ncontains 1,000 dialogues, contributing to a total   an online competition hosted by (Kaggle, 2024).\nof 13,833 utterances. Each user utterance in the   The dataset, named as AES-2.0, contains 24,000\ndataset is labeled with a satisfaction score on a 5-   student-written argumentative essays. Each essay\npoint Likert scale (Likert, 1932). Rating 1 indicates   was scored on a scale of 1 to 6 as the holistic rating\nthe lowest level of satisfaction, while rating 5 de-    1 judged by human experts.\nnotes the highest satisfaction. The human-assigned\n                                                      3.1.3  Industrial SQL Command Questionsatisfaction scores are valuable for assessing chat-\n                                                Answering Datasetbot responses with respect to user satisfaction.\n                                                   In addition to the previously mentioned public\n3.1.2  Automated Essay Scoring Datasets                                                        datasets, we have also deployed PLHF on a real-\nThe dataset is originally provided by (Ben et al.,   world question-answering system, which is cur-\n2012) for the Automated Student Assessment Prize    rently an actual product of a commercial AI com-\n(ASAP). The dataset, named as AES-ASAP, con-   pany.  This test, named as SQL-QA, comprises\nsists of eight essay sets varying in length, ranging   100 real-world queries involving various database\nfrom an average of 150 to 550 words per response.   inquiry requests from the clients. The database en-\nThe responses were written by students in grades\n                                                                   1https://storage.googleapis.com/\nseven through ten, and all essays were hand-graded                                                         kaggle-forum-message-attachments/2733927/20538/\nby human experts. Each essay was double-scored.    Rubric_HolisticEssayScoring.pdf\n\ntries contains daily transaction records and other        • PO with GPT-4o: employ the state-of-the-art\nlogs sourced from multiple banks in China. As the      GPT-4o (gpt-4o-2024-05-13) as the evalua-\ntraining data for prompt optimizations, human ex-         tor for prompt optimizations on the Base LLM\nperts from the company labeled 10 positive samples        (GPT-3.5).\nand 20 negative samples for the prompt optimiza-\n                                                                 • PO with Exact Matching: consider hard-tions. In Appendix C, we provide some examples\n                                                  matching as the grading function for theof queries used in this test.\n                                                prompt optimization on the Base LLM. Let\n3.2  Experiment Setups                               score = 1 if and only if the output of R is\n                                                         exactly the same as the ground-truth.To perform prompt optimizations (denoted as PO)\nin each subtask, we consider two state-of-the-                                                                 • PO with Embedding Similarity: similar to\nart automatic prompting frameworks, DSPy and                                                         the Exact Matching one, but this time con-\nTextGrad. Same experiments are conducted for                                                            sider cosine similarity as the score between\nboth frameworks, and respective results are shown.                                               embedding vectors. The outputs are embed-\n  For the experiments, first, we estimate the ef-                                                ded by OpenAI text-embedding-ada-002\nfectiveness of the evaluator E, which solves the                                               model (OpenAI, 2024).\ntask of predicting the labeled scores based on each\ninput-output pair. The comparisons include several        • PLHF: our framework, where LLMs MR and\nbaseline methods:                     ME are both set to be GPT-3.5 (gpt-3.5-\n                                                  turbo-0125) for fair comparisons.\n   • Base LLM (GPT-3.5): the grounding base-\n      line — simply employ OpenAI gpt-3.5-turbo-   3.3  Evaluations\n    0125 model to predict the labeled score each                                               For the model output, we employ multiple human\n     input-output pair.                                                      experts as the judges to provide professional scores\n                                                with respect to the score scales of the original\n   • MLP with Text Embedding: Leveraging a\n                                                       data. However, for the public datasets, the orig-\n     3-layer Perception model, based on (Popescu\n                                                        inal people who labeled the data are unavailable\n      et al., 2009), to predict the score of each input-\n                                                       to give their judgments for our new generated out-\n     output pair. The texts are transformed into em-\n                                                         puts. Therefore, we introduce a concept of pseudo-\n     bedding vectors via OpenAI text-embedding-\n                                        human judge to perform output evaluations. Specif-\n    ada-002 model (OpenAI, 2024).\n                                                              ically, we use GPT-4o with prompt optimizations\n   • SVM with Text Embedding: similar to the    via DSPy as the pseudo-human judge. Since we\n   MLP one, but adopt Support Vector Machines    consider GPT-3.5 as Base LLMs for all the meth-\n    (SVMs) as the score predictor. We adopt the   ods in the experiments, this pseudo-human judge is\n     implementation provided by (Chang and Lin,   a relatively powerful model that can provide more\n    2011) with the default configuration.           convincing evaluations toward output quality.\n\n   • GPT-3.5 PO via DSPy: perform prompt opti-   3.4  Experimental Results\n     mizations with DSPy (Khattab et al., 2024).    Table 1 lists the experimental results of the evalu-\n                                                      ator task. As shown in the table, we can observe\n   • GPT-3.5 PO via TextGrad: perform prompt\n                                                        that the conventional methods (MLP/SVM with\n     optimizations with TextGrad (Yuksekgonul\n                                                 Text Embedding) seem struggled in predicting the\n      et al., 2024).\n                                                     labeled scores from the given embedded inputs. In\n                                                        contrast, the LLM-based methods performed sig-  Then, for the main responder R, output quality\n                                                          nificantly better on both public datasets and the in-of the LLM with optimized prompts are judged\n                                                           dustrial tests. A possible reason is that LLMs mightby test queries. We consider various types of the\n                                                have superior fitting and understanding capabilitiesevaluators for prompt optimizations.\n                                                      to handle text inputs. With prompt optimizations,\n   • Base LLM (GPT-3.5, no PO): the ground-   both DSPy and TextGrad provided more effective\n     ing baseline — simply utilize OpenAI gpt-   prompt for more accurate evaluators.\n    3.5-turbo-0125 model to generate the output     As for the experimental results of the responder\n    based on the given input.                         task, shown in Table 2, we consider both DSPy\n\nTable 1: Summary of experimental results for the evaluator subtask across each dataset. For the public datasets, the\npresented values are RMSE losses (lower is better) of the output scores from E; for the industrial dataset SQL-QA,\nthe values indicate Accuracy scores (higher is better). The best ones are marked in bold font.\n\n                Method         SGD   AES-ASAP  AES-2.0  SQL-QA\n             Base LLM (GPT-3.5)       1.02        4.75        0.46       0.53\n        MLP with Text Embedding     1.17        7.22        1.08       0.33\n       SVM with Text Embedding     1.25        6.43        1.10       0.40\n            Base LLM PO via DSPy      0.43        2.36        0.33       0.80\n          Base LLM PO via TextGrad    0.40        2.42        0.38       0.73\n\nTable 2: Summary of experimental results for the responder subtask across each dataset. For the industrial dataset\nSQL-QA, the overall Accuracy score are given by actual human experts in the company; for the public datasets, the\nscores from the pseudo-human judge are shown. The values for Base LLM are the actual scores, whereas for the\nother methods, relative improvements are shown in percentages. The best ones are marked in bold font.\n\n     PO             Method         SGD   AES-ASAP  AES-2.0  SQL-QA\n                  Base LLM (GPT-3.5)        4.25       26.50        5.35       0.74\n               PO with GPT-4o        +1.18%    +3.70%    +0.75%   +5.41%\n    DSPy     PO with Exact Matching       - 4.00%    -10.87%      - 5.61%    0.00%\n           PO with Embedding Similarity  +1.65%      - 4.27%    +0.56%   +10.81%\n                   PLHF           +6.59%   +8.45%    +2.62%  +18.92%\n               PO with GPT-4o        +4.71%    +3.28%    +1.31%   +2.70%\n             PO with Exact Matching     -10.59%    -15.92%    -10.84%   -18.92%\n    TextGrad  PO with Embedding Similarity  +3.53%      - 0.64%    +0.93%   +2.70%\n                   PLHF           +8.71%   +8.68%    +4.30%  +18.92%\n\n\nand TextGrad as the prompt optimization (PO)   a more practical and achievable approach in real-\ntool for each method in the comparisons. Over-   world applications. To address the challenges of\nall, the results are relatively similar in same di-   no well-defined metrics and the scarce human re-\nrections for each pair of the scores between the    sources, we introduced PLHF, a few-shot prompt\ntwo PO selections. In summary, for all the four    optimization with an evaluator module design to au-\ndatasets, the proposed PLHF framework achieved    tomatically grade the outputs generated by LLMs.\nthe best performance in output quality, in terms  We performed extensive experiments with public\nof the metric for each task.  Moreover, PLHF    datasets and a real industrial dataset to verify the\nused GPT-3.5 as the evaluator’s base LLM ME    effectiveness of PLHF. The experimental results\nto achieve superior performance than ‘PO with   shown that PLHF outperforms existing methods\nGPT-4o’, which conducted prompting with a more    across from simple string matching functions to\npowerful GPT-4o as the evaluator. For the other   even the latest publicly available LLMs as output\nbaselines, ‘PO with GPT-4o’ consistently outper-   evaluators in terms of the output quality. Overall,\nformed ‘Base LLM (raw GPT-3.5)’. Regarding   our proposed framework is practically effective,\nthe conventional matching-based grading functions,    especially for the scenarios when directly apply-\nboth hard-matching (Exact Matching) and soft-   ing pre-trained general-purpose LLMs are not the\nmatching (Embedding Similarity) produced out-   best option. Our future work involves enhancing\nputs with worse quality. We have also tested the   and deploying the proposed framework across di-\nmodel performance under various numbers of train-   verse applications, particularly for tasks that utilize\ning samples. The additional results are shown in   multi-modal data.\nAppendix D.\n\n                                          References4  Conclusion\n\n                                                    Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nIn this paper, we focused on LLM prompt optimiza-     Ahmad,  Ilge Akkaya,  Florencia Leoni Aleman,\ntions with a limited amount of human feedback —     Diogo Almeida, Janko Altenschmidt, Sam Altman,\n\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.       pipelines. In The Twelfth International Conference\n  arXiv preprint arXiv:2303.08774.                    on Learning Representations.\n\nHamner Ben, Morgan Jaison, lynnvandev, Shermis    Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\n  Mark, and Ark Tom Vander. 2012. The hewlett foun-      Fu, Kyle Richardson, Peter Clark, and Ashish Sab-\n   dation: Automated essay scoring.                        harwal. 2023. Decomposed prompting: A modular\n                                                       approach for solving complex tasks. In The Eleventh\nTom Brown, Benjamin Mann, Nick Ryder, Melanie       International Conference on Learning Representa-\n   Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind        tions.\n   Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n   Askell, et al. 2020. Language models are few-shot    Zihao Li, Zhuoran Yang, and Mengdi Wang. 2023. Re-\n   learners. Advances in neural information processing      inforcement learning with human feedback: Learn-\n   systems, 33:1877–1901.                                ing dynamic choices via pessimism. arXiv preprint\n                                                        arXiv:2305.18438.\nChih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:\n  a library for support vector machines. ACM transac-   Rensis Likert. 1932. A technique for the measurement\n   tions on intelligent systems and technology (TIST),      of attitudes. Archives of psychology.\n   2(3):1–27.\n                                                  Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yi-     Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang\n  han Wang, Han Guo, Tianmin Shu, Meng Song, Eric      Low. 2024. Prompt optimization with human feed-\n  Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing       back. arXiv preprint arXiv:2405.17346.\n   discrete text prompts with reinforcement learning.\n   In Proceedings of the 2022 Conference on Empiri-  Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler\n   cal Methods in Natural Language Processing, pages       Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\n  3369–3391.                                   Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,\n                                                                   et al. 2024.  Self-refine: Iterative refinement with\nChrisantha  Fernando,  Dylan  Banarse,  Henryk       self-feedback. Advances in Neural Information Pro-\n  Michalewski, Simon Osindero, and Tim Rock-      cessing Systems, 36.\n   täschel. 2023.   Promptbreeder:  Self-referential\n  self-improvement via prompt evolution.   arXiv    Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,\n   preprint arXiv:2309.16797.                          and Richard Socher. 2018. The natural language\n                                                            decathlon: Multitask learning as question answering.\nJinlan Fu, See Kiong Ng, Zhengbao Jiang, and Pengfei      arXiv preprint arXiv:1806.08730.\n   Liu. 2024.  Gptscore: Evaluate as you desire.  In\n  Proceedings of the 2024 Conference of the North   OpenAI. 2024. Openai platform: Embeddings — em-\n  American Chapter of the Association for Computa-      bedding models.\n   tional Linguistics: Human Language Technologies\n  (Volume 1: Long Papers), pages 6556–6576.         Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\n                                                             Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.      Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n  Making pre-trained language models better few-shot      2022. Training language models to follow instruc-\n   learners. In Proceedings of the 59th Annual Meet-       tions with human feedback. Advances in neural in-\n   ing of the Association for Computational Linguistics      formation processing systems, 35:27730–27744.\n  and the 11th International Joint Conference on Natu-\n   ral Language Processing (Volume 1: Long Papers),    Marius-Constantin Popescu, Valentina E Balas, Lil-\n  pages 3816–3830.                                       iana Perescu-Popescu, and Nikos Mastorakis. 2009.\n                                                          Multilayer perceptron and neural networks. WSEAS\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao      Transactions on Circuits and Systems, 8(7):579–588.\n  Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\n   jiu Yang. 2023. Connecting large language models    Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\n  with evolutionary algorithms yields powerful prompt     Noah A Smith, and Mike Lewis. 2023. Measuring\n   optimizers. arXiv preprint arXiv:2309.08532.           and narrowing the compositionality gap in language\n                                                        models. In Findings of the Association for Computa-\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2024.       tional Linguistics: EMNLP 2023, pages 5687–5711.\n  Optimizing prompts for text-to-image generation.\n  Advances in Neural Information Processing Systems,   Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang\n   36.                                                 Zhu, and Michael Zeng. 2023. Automatic prompt op-\n                                                             timization with “gradient descent” and beam search.\nKaggle. 2024. Automated essay scoring 2.0.                In Proceedings of the 2023 Conference on Empiri-\n                                                             cal Methods in Natural Language Processing, pages\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,      7957–7968.\n  Zhiyuan Zhang, Keshav Santhanam, Saiful Haq,\n  Ashutosh Sharma, Thomas T Joshi, Hanna Moazam,   Alec Radford, Karthik Narasimhan, Tim Salimans, and\n  Heather Miller,  et  al. 2024.  Dspy:  Compiling       Ilya Sutskever. 2018.  Improving language under-\n   declarative language model calls into state-of-the-art       standing by generative pre-training.\n\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV,   Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\n   Eric Wallace, and Sameer Singh. 2020. Autoprompt:      Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\n   Eliciting knowledge from language models with au-     Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\n   tomatically generated prompts. In Proceedings of the      Judging llm-as-a-judge with mt-bench and chatbot\n  2020 Conference on Empirical Methods in Natural       arena. Advances in Neural Information Processing\n  Language Processing (EMNLP), pages 4222–4235.       Systems, 36:46595–46623.\n\nWeiwei Sun, Shuo Zhang, Krisztian Balog, Zhaochun   Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\n  Ren, Pengjie Ren, Zhumin Chen, and Maarten de Ri-      Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n   jke. 2021. Simulating user satisfaction for the evalu-      Ba. 2023. Large language models are human-level\n   ation of task-oriented dialogue systems. In Proceed-     prompt engineers. Preprint, arXiv:2211.01910.\n   ings of the 44rd International ACM SIGIR Confer-\n  ence on Research and Development in Information   Rubens Zimbres. 2024. Evaluating llms with langchain:\n   Retrieval, SIGIR ’21. ACM.                          Using gpt-4 to evaluate google’s open model gemma-\n                                                                     2b-it.\nTing Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao\n   Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng,  A  Related Work\n  Daniel Lim, Yuhe Ke, et al. 2024. Fine-tuning large\n  language model (llm) artificial intelligence chatbots                                                Various strategies have been developed to obtain\n   in ophthalmology and llm-based evaluation using\n                                                      suitable LLM prompts. Earlier studies adopt au-   gpt-4. arXiv preprint arXiv:2402.10083.\n                                              tomated data sample searching techniques (Gao\nChaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun    et al., 2021) to learn prompts through gradient-\n  Peng, Hongyu Zhang, and Michael R Lyu. 2022.                                              based searching methods (Shin et al., 2020; Wen\n  No more fine-tuning? an experimental evaluation of\n                                                            et al., 2024; Pryzant et al., 2023), refining prompts  prompt tuning in code intelligence. In Proceedings\n   of the 30th ACM joint European software engineer-   using evolutionary algorithms (Guo et al., 2023;\n   ing conference and symposium on the foundations of   Fernando et al., 2023) and utilizing other LLMs\n   software engineering, pages 382–394.                 for prompt generation (Yang et al., 2024; Zhou\n                                                             et al., 2023). Several studies have also attempted toJiaan Wang, Yunlong Liang, Fandong Meng, Zengkui\n  Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng    optimize prompts using reinforcement learning, ex-\n  Qu, and Jie Zhou. 2023.   Is chatgpt a good nlg    ploring prompt editing at different granular levels\n   evaluator?  a preliminary study.  arXiv preprint   such as word-level (Deng et al., 2022), phrase-level\n  arXiv:2303.04048.\n                                             (Zhang et al., 2023), and within text-to-image gen-\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Gold-    eration tasks (Hao et al., 2024).\n  blum, Jonas Geiping, and Tom Goldstein. 2024. Hard     As LLMs are increasingly applied in real-world\n  prompts made easy: Gradient-based discrete opti-    scenarios, in-context learning (McCann et al., 2018;\n   mization for prompt tuning and discovery. Advances\n                                               Radford et al., 2018; Brown et al., 2020) is becom-\n   in Neural Information Processing Systems, 36.\n                                                   ing an emerging trend for effective LLM program-\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao   ming. Instruction tuning (Ouyang et al., 2022) fur-\n   Liu, Quoc V Le, Denny Zhou, and Xinyun Chen.    ther enhances this process by enabling complex\n  2024.  Large language models as optimizers.  In                                                 behaviors through the use of structured prompts\n  The Twelfth International Conference on Learning\n                                                    (Press et al., 2023; Yao et al., 2023; Khot et al.,   Representations.\n                                               2023; Madaan et al., 2024).\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak     For automatic few-shot prompt optimization,\n   Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.                                                (Khattab et al., 2024) introduced DSPy, a state-\n   React: Synergizing reasoning and acting in language\n                                                        of-the-art prompt optimization framework, which  models. In The Eleventh International Conference\n  on Learning Representations.                       considers LLM usages in a programmatic fash-\n                                                       ion. DSPy parameterizes each module to learn\nMert Yuksekgonul, Federico Bianchi, Joseph Boen,                                                     the data pattern and the desired behaviors by itera-\n  Sheng Liu, Zhi Huang, Carlos Guestrin, and James\n                                                         tively bootstrapping useful demonstrations. On  Zou. 2024. Textgrad: Automatic \"differentiation\" via\n   text.                                              the other hand, (Yuksekgonul et  al., 2024) in-\n                                                    spired by LLM fine-tuning procedures and pro-\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-   posed TextGrad, a framework refining the prompt\n  urmans, and Joseph E. Gonzalez. 2023. TEMPERA:\n                                                 with the back-propagation algorithm. Instead of de-   Test-time prompt editing via reinforcement learning.\n   In The Eleventh International Conference on Learn-    riving numeral-valued gradients, TextGrad regards\n   ing Representations.                       LLMs’ text feedback as the ‘gradient’ in texts.\n\nInput: “Why did the jellyfish refuse the job offer?”                     Input: “Why did the shoe store owner become a detective?”\n  Output: “Because it couldn't handle the pressure!”                 Output: “Because he was good at solving sneaker cases!”\n  Labeled Score: 6                                           Labeled Score: 9\n\n   Input: “Why did the peacock sit on the fence all day?”                Input: “Why don't astronauts throw parties in space?”\n  Output: “It had some serious trust issues with its own reflection!”     Output: “Because there's no atmosphere for a good vibe!”\n  Labeled Score: 5                                           Labeled Score: 8\n\n   Input: “Why did the vegetarian vampire only drink tomato juice?”     Input: “Why did the smartphone go to therapy?”\n  Output: “Because he couldn't stand steaks through the heart!”       Output: “It had too many hang-ups!”\n  Labeled Score: 7                                           Labeled Score: 8\n          …                     …    Positive Samples\n\n                     Training Samples D (Labeled Samples)\n\n\n  Predict the Labeled Score for the given Input-Output pair.          Generate the Output for the given Input, where Input is the\n   Input is the “setup” and Output is the “punchline” of a joke.           “setup” and Output is the “punchline” of a joke.\n    ---                                                                                           ---\n  Follow the following format.                                       Follow the following format.\n   Input: ${input}                                                            Input: ${input}\n   Output: ${output}                                                  Output: ${output}           Initial Prompt PR (Instruction)\n  Labeled Score: ${score}    Initial Prompt PE (Instruction)                                                                                                  ---\n    ---                                                                         Input: “Why did the shoe store owner become a detective?”\n   Input: “Why did the shoe store owner become a detective?”          Output: “Because he was always good at solving sneaker cases!”\n   Output: “Because he was always good at solving sneaker cases!”\n  Labeled Score: 9                                                         Input: “Why don't astronauts throw parties in space?”\n                                                                     Output: “Because there's no atmosphere for a good vibe!”\n   Input: “Why did the jellyfish refuse the job offer?”\n   Output: “Because it couldn't handle the pressure!”                      Input: “Why did the smartphone go to therapy?”\n  Labeled Score: 6                                                  Output: “It had too many hang-ups!”\n          …                     …\n\n       Optimized Prompt P’E               Optimized Prompt P’R\n\nFigure 3: A toy example to illustrate the subtask designs of PLHF. The targeted generative AI task for this example\nis “generate the punchline for a joke setup.” The training samples D are triplets (Input, Output, Labeled Score),\nwhere Input is the joke setup, Output is a sample output of the punchline for the corresponding Input, and Labeled\nScore is the rating judged by human experts. For this example, we consider Labeled Score ≥8 as the condition of\npositive samples. Examples of optimized prompts PE′ and P R′ (for the evaluator E and the responder R, respectively)\nare shown.\n\n\n  All these techniques rely on well-defined met-  B  An Optimization Example of PLHF\nrics to set their objectives. While advanced general-\n                                                   In Figure 3, we provide an example of a toy prob-\npurpose LLMs like GPT-4 can be adopted for text-\n                                          lem of automatic joke generation to demonstrate\noutput evaluations (Zimbres, 2024; Zheng et al.,\n                                                     the initial prompts PE and PR, as well as the opti-\n2023; Tan et al., 2024), they may lack the contex-                                        ′              ′\n                                            mized prompts P R and P E obtained from PLHF.tual or background knowledge needed for accurate\nevaluation in specific tasks. Hence, involving hu-  C  Industrial SQL Command Question\nman feedback becomes inevitable for the prompt      Answering Dataset\noptimizations in such tasks. To address the issue,\n(Lin et al., 2024) inspired by dueling bandits and    In Table 3, we provide two actual query samples of\ndesigned a strategy to choose pairs of prompts to    the industrial SQL-QA dataset which we consider\nquery for human feedback during the prompt opti-    in the experiments.\nmizations to reduce the number of needed calls of\n                           D  Performance Analysis with Various\nhuman feedback. In this paper, we consider a dif-\n                                      Numbers of Training Samples\nferent approach to tackle the issue — we focus on\nthe metric in the prompt optimization process. We    In addition to the overall performance, we also an-\ndeveloped a duo-module framework to obtain an    alyze the robustness and the relationship between\nevaluator module acting as the metric of the main   model effectiveness and the number of training\ntask to perform the desired LLM prompt optimiza-   samples involved in the prompt optimization pro-\ntions, requiring minimal human feedback.            cess. As examples, Figure 4 demonstrates the per-\n\nTable 3: Examples of the queries in our industrial SQL-QA test.\n\n    User Query (English translation)  SQL Statement (Output)\n\n\n     Please list the top 3                SELECT CUST_ID, CUST_NAME, DEPO_BAL\n     clients by total deposits            FROM acct\n     at the Beijing branch                WHERE DATA_DT='20240131' AND ORG_NAME='Beijing'\n     as of January 31, 2024.              ORDER BY DEPO_BAL DESC\n                                          LIMIT 3\n\n\n     Please inquire about the             SELECT ORG_NAME, ASSET_BAL\n     top 5 banks with the                 FROM acct\n     highest asset balances,              WHERE DATA_DT='20240331'\n     grouped by institution,              ORDER BY ASSET_BAL DESC\n     as of March 31, 2024.                LIMIT 5\n\n\n\n                     Evaluator E (SGD)                       Responder R (SGD)\n\n                1.8                                                     5\n                1.5                                                              4.8\n                                                                                 4.6\n                1.2\n                                                                                 4.4\n                0.9                 RMSE                                                                                                              Score 4.2\n                0.6\n                                                                     4\n                0.3                                                              3.8\n\n               0                                                              3.6\n                    0     5     10    20    40    70    100               0     5     10    20    40    70    100\n                             # samples                                      # samples\n\n                  Evaluator E (AES-ASAP)                  Responder R (AES-ASAP)\n\n              7                                                    30\n\n              6                                                    28\n\n              5                                                    26\n                 RMSE 43                                                                                                         Score 2422\n              2                                                    20\n\n              1                                                    18\n\n              0                                                    16\n                   0     5     10    20    40    70    100              0     5     10    20    40    70    100\n                             # samples                                      # samples\n\nFigure 4: Performance curves of PLHF on the datasets SGD and AES-ASAP. For the plots, we consider DSPy as\nthe PO method for PLHF. The x-values are the number of (randomly selected) training samples. The y-values are\nmean values of the RMSE losses for E and the output scores for R, respectively. The vertical bar of each point\nindicates the standard deviations estimated in 30 runs.\n\n\nformance curves for datasets SGD and AES-ASAP.   but not least, as expected, the standard deviations\nNote that, to analyze the performance of a respon-   lower along with the increasing number of training\nder with n data samples, we also use the evaluator    samples.\noptimized with the same n samples. For the curves\nof evaluator E, we can observe that the RMSE loss\nraised for initial samples, then the RMSE value\ndropped significantly after few shots of data. For\nthe curves toward responder R, the pattern is sim-\nilar to the curves of E (but in opposite way), the\noutput quality score dropped for initial samples,\nwhile the score then bouncing back and achieving\nnew highs with greater number of samples. Last",
"headers": [
"arXiv:2505.07886v1  [cs.CL]  11 May 2025",
"PLHF: Prompt Optimization with Few-Shot Human Feedback",
"Training Samples",
"D",
"Optimized Prompt",
"P’"
],
"tables": [
"|SGD|AES-ASAP|AES-2.0|\n|---|---|---|\n|1.02|4.75|0.46|\n|1.17<br>1.25|7.22<br>6.43|1.08<br>1.10|",
"|Method|SGD|AES-ASAP|AES-2.0|\n|---|---|---|---|\n|Base LLM(GPT-3.5)|4.25|26.50|5.35|\n|PO with GPT-4o<br>PO with Exact Matching<br>PO with Embedding Similarity<br>PLHF|+1.18%<br>- 4.00%<br>+1.65%<br>**+6.59%**|+3.70%<br>-10.87%<br>- 4.27%<br>**+8.45%**|+0.75%<br>- 5.61%<br>+0.56%<br>**+2.62%**|",
"|Evaluator E (SGD) Responder R (SGD)<br>1.8 5<br>1.5 4.8<br>4.6<br>1.2<br>4.4 RMSE Score<br>0.9<br>4.2<br>0.6<br>4<br>0.3 3.8<br>0 3.6<br>0 5 10 20 40 70 100 0 5 10 20 40 70 100<br># samples # samples<br>Evaluator E (AES-ASAP) Responder R (AES-ASAP)<br>7 30<br>6 28<br>5 26<br>4 24 RMSE Score<br>3 22<br>2 20<br>1 18<br>0 16<br>0 5 10 20 40 70 100 0 5 10 20 40 70 100<br># samples # samples|Col2|Col3|\n|---|---|---|\n|0<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>0<br>5<br>10<br>20<br>40<br>70<br>100<br>RMSE<br># samples|16<br>18<br>20<br>22<br>24<br>26<br>28<br>30<br>0<br>5<br>10<br>20<br>40<br>70<br>100<br>Score<br># samples|16<br>18<br>20<br>22<br>24<br>26<br>28<br>30<br>0<br>5<br>10<br>20<br>40<br>70<br>100<br>Score<br># samples|\n||||",
"|1.8<br>1.5<br>1.2<br>RMSE<br>0.9<br>0.6<br>0.3<br>0<br>0 5 10 20 40 70 100<br># samples|5<br>4.8<br>4.6<br>4.4 Score<br>4.2<br>4<br>3.8<br>3.6<br>0 5 10 20 40 70 100<br># samples|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2505.07886v1.pdf"
}