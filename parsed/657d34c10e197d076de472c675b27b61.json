{
"text": "Prompt Optimization with Human Feedback\n\n\n\n\n                                Xiaoqiang Lin1, Zhongxiang Dai2∗, Arun Verma1, See-Kiong Ng1,3,\n                                              Patrick Jaillet2, Bryan Kian Hsiang Low1\n                                  1Department of Computer Science, National University of Singapore\n                                  2LIDS and EECS, Massachusetts Institute of Technology\n                                               3Institute of Data Science, National University of Singapore\n                          xiaoqiang.lin@u.nus.edu, daizx@mit.edu, arun@comp.nus.edu.sg,\n                           seekiong@nus.edu.sg, jaillet@mit.edu, lowkh@comp.nus.edu.sg2024\n\n\n                                                 AbstractMay\n27                         variousLarge languagetasks. However,models (LLMs)the performancehave demonstratedof LLMs heavilyremarkabledependsperformanceson the inputin\n                              prompt, which has given rise to a number of recent works on prompt optimization.\n                             However, previous works often require the availability of a numeric score to assess\n                                the quality of every prompt. Unfortunately, when a human user interacts with a\n                              black-box LLM, attaining such a score is often infeasible and unreliable. Instead,\n                                               it is usually significantly easier and more reliable to obtain preference feedback\n                           from a human user, i.e., showing the user the responses generated from a pair of[cs.LG]\n                            prompts and asking the user which one is preferred. Therefore, in this paper, we\n                               study the problem of prompt optimization with human feedback (POHF), in which\n                       we aim to optimize the prompt for a black-box LLM using only human preference\n                               feedback. Drawing inspiration from dueling bandits, we design a theoretically\n                                principled strategy to select a pair of prompts to query for preference feedback\n                                  in every iteration, and hence introduce our algorithm named automated POHF\n                              (APOHF). We apply our APOHF algorithm to various tasks, including optimizing\n                               user instructions, prompt optimization for text-to-image generative models, and\n                               response optimization with human feedback (i.e., further refining the response using\n                               a variant of our APOHF). The results demonstrate that our APOHF can efficiently find\n                              a good prompt using a small number of preference feedback instances. Our code\n                             can be found at https://github.com/xqlin98/APOHF.\n\n\n                1  IntroductionarXiv:2405.17346v1\n                     Large language models (LLMs) have shown impressive performances in a variety of tasks [17, 31].\n                    However, the performances of LLMs are significantly dependent on the prompt given to them [56].\n                        Unfortunately, finding the best prompt for an LLM to perform a task is often challenging, especially\n                       considering that the most powerful LLMs nowadays are often black-box models to which only\n                  API access is available [31]. This challenge has given rise to a number of recent works on prompt\n                       optimization for black-box LLMs, which aim to efficiently find the best prompt for a black-box\n              LLM [9, 26, 56]. These works have shown that prompt optimization can dramatically improve the\n                      performances of black-box LLMs in various tasks. However, these works often impose a potentially\n                          unrealistic requirement on the tasks: They usually require access to a numeric score to evaluate the\n                     performance of every prompt. This significantly limits their practicality in real-world use cases.\n\n                         Specifically, some works on prompt optimization have assumed the availability of a validation\n                            set, which can be used to evaluate (the response generated from) a candidate prompt [9, 20, 26].\n                     Meanwhile, other works have used a separate LLM (often referred to as the scorer LLM) to provide\n\n                          ∗Corresponding author.\n\ninitial task description\n                                                                                                 0.65\n\n   User         preference feedback   APOHF                                                       Score 0.60\n                                                                                                 0.55\n\n                                         prompt 1     prompt 2                     0.50\n                                                                                          0    25    50    75   100\n                                                                           # Iterations\n                      response 1                                                               Random                    DoubleTS\n                                                                                                                                    Linear Dueling Bandits       APOHF                   LLM\n                      response 2\n                                                      Figure 2:  Latent scores of different\nFigure  1:   Illustration  of our automated prompt    methods in user instruction optimization,\noptimization with human feedback (APOHF).              averaged over 30 tasks (Sec. 4.1).\n\na score indicating the efficacy of (the response produced by) a prompt [50, 56]. However, when a\nhuman user directly interacts with a black-box LLM to perform a task (i.e., the most common use\ncases of LLMs nowadays), these methods to obtain a score are often unrealistic. This is because in\nsuch use cases, a validation set is usually unavailable and the scorer LLM is unlikely to provide an\naccurate assessment of a prompt for the task the user has in mind. Therefore, these previous prompt\noptimization methods are inapplicable for such use cases. In addition, directly asking a user for a\nnumeric score to assess (the response generated by) a candidate prompt is usually infeasible and\nunreliable [51]. Instead, a human user is often significantly more willing to and reliable at providing\npreference feedback, i.e., examining the responses generated by a pair of prompts and indicating\nwhich one is preferred [51]. This naturally begs the question: Can we achieve prompt optimization\nusing only human preference feedback? In this work, we tackle this important problem, which we\nrefer to as prompt optimization with human feedback (POHF).\n\nThe significance of POHF can also be highlighted by drawing an analogy to reinforcement learning\nwith human feedback (RLHF) [57].  RLHF, as well as its variants such as direct preference\noptimization [37], uses a dataset of human preference feedback to fine-tune the parameters of\nan LLM in order to align the LLM with human values [37]. The tremendous success of RLHF is\nevidence of the advantage of using human preference feedback to adapt LLMs. While RLHF has\nrelied on fine-tuning the model parameters to adapt the response of an LLM (to align with human\nvalues), our POHF aims to use prompt optimization to adapt the response of an LLM to perform a task\nfor a human. Interestingly, our algorithm for POHF can be extended to further refine the response\nof an LLM through response optimization with human feedback (Sec. 4.3). Specifically, for every\nreceived prompt, we can use the LLM to generate a large pool of responses and then strategically\nselect a pair of responses from the pool to query for user preference feedback [15]. Our goal here is to\nfind the best response for every given prompt while using only human preference feedback. This can\nbe useful in applications where we do not have the flexibility to choose the prompt, but can sample a\nlarge number of responses from the LLM. For example, it may be adopted by an LLM provider to\nfurther refine its response to user prompts while only collecting user preference feedback.\n\nSimilar to RLHF, in our POHF, it is of paramount importance to find a good prompt using a small\nnumber of human feedback instances. This is because collecting human feedback can usually be\nexpensive and time-consuming. To achieve this, inspired by [26], we adopt the embedding from\na pre-trained language model as the continuous representation of the prompts, and train a neural\nnetwork (NN), which takes the embedding as input, to predict the performance (i.e., the latent score,\nsee Sec. 3.1) of different prompts. Based on the trained NN, we draw inspiration from dueling bandits\n[4, 39] and design a theoretically principled strategy to select the pair of prompts (to query for human\nfeedback) in every iteration. Specifically, we choose the first prompt following a greedy strategy,\ni.e., by selecting the prompt that is predicted to have the best performance by the trained NN. Next,\nwe select the second prompt based on the principle of upper confidence bound, which allows us to\nsimultaneously exploit the performance prediction from the NN and explore those prompts whose\nperformance prediction has large uncertainty. As a result of the accurate performance prediction of\nthe NN (thanks to the expressive power of the pre-trained embedding and the NN) and our principled\nprompt selection strategy, our algorithm, named Automated POHF (APOHF), is able to find a good\nprompt using only a small number of human preference feedback instances.\n\nWithin our problem setting (illustrated in Fig. 1), our APOHF algorithm acts as an interface between\nthe user and the LLM. To adopt our APOHF in practice, the user only needs to provide (1) an initial\n\n\n                                       2\n\ntask description (e.g., a few input-output exemplars or an initial prompt) and subsequently (2) a series\nof preference feedback between pairs of responses (more details in Sec. 3.3). We adopt a number of\ntasks to validate the performance of our APOHF, including optimizating user instructions (Sec. 4.1),\nprompt optimization for text-to-image generative models (Sec. 4.2), and response optimization with\nhuman feedback (Sec. 4.3). In these tasks, our APOHF consistently achieves better performances than\nbaseline methods, demonstrating its immense potential in real-world applications.\n\n2  Problem Setting\n\nIn POHF, we aim to find a prompt x ∈X that maximizes an unknown function u, which we refer\nto as the latent score/utility function. That is, we aim to solve the following optimization problem:\nx⋆= argmaxx∈X u(x) while only observing human preference feedback. In every iteration t, we\nselect a pair of prompts xt,1 and xt,2 to obtain their corresponding LLM-generated responses and\nshow them to the user. Then, we collect a binary observation yt = 1(xt,1 ≻xt,2), which is equal\nto 1 if the human user prefers the response from xt,1 over that from xt,2 and 0 otherwise. To\nmodel the preference feedback, we adopt the commonly used Bradley-Terry-Luce (BTL) model [21].\nThat is, for any pair of prompts x1 and x2, the probability that x1 is preferred over x2 is given by\nP(x1 ≻x2) = σ(u(x1) −u(x2)), in which σ(·) denotes the logistic function: σ(x) = 1/(1 + e−x).\nThe binary observation yt is then sampled from a Bernoulli distribution with probability P(x1 ≻x2).\nThe stochastic nature of yt allows us to naturally account for the noise in human preferences between\na pair of prompts. The noise may arise due to different sources of randomness, such as the randomness\nin the LLM-generated response for a given prompt, the variability in human decisions, among others.\n\nFollowing recent works on query-efficient prompt optimization [9, 26], we convert POHF into a\ncontinuous optimization problem. Specifically, for every prompt x ∈X in the domain, we extract the\nembedding from a pre-trained language model as its continuous representation. Of note, the previous\nworks of [9] and [26] adopted a separate white-box LLM so that the soft prompt (i.e., a part of the\ninput to the white-box LLM to generate the prompt) can be used as the continuous representation\nof the prompt. Therefore, compared to [9] and [26], our method of adopting the embedding from\na pre-trained model removes the need for the white-box LLM, and hence significantly reduces\nthe complexity and computational cost. To simplify notations, hereafter, we use x to denote the\ncontinuous embedding of a prompt in the domain. Before the beginning of our algorithm, we use the\ninitial task description from the user (Fig. 1) to generate the discrete domain of prompts X, which we\ndiscuss in more detail in Sec. 3.3.\n\n3  Automated Prompt Optimization with Human Feedback (APOHF)\n\nOverview of APOHF (illustrated in Fig. 1). In every iteration t of our APOHF algorithm (Algo. 1),\nwe firstly use the current history of preference observations Dt−1 = {(xs,1, xs,2, ys)}s=1,...,t−1 to\ntrain a neural network (NN) for score prediction (Sec. 3.1). Next, we leverage the trained NN to\nselect the next pair of prompts xt,1 and xt,2 to query (Sec. 3.2). Then, the pair of prompts xt,1 and\nxt,2 are used to generate their respective responses, which are shown to the human user who gives\npreference feedback yt = 1(xt,1 ≻xt,2) (Sec. 3.3). The newly collected observation (xt,1, xt,2, yt)\nis then added to the history, which is subsequently used to train the NN for the next iteration t + 1.\n\nAlgorithm 1 Automated Prompt Optimization with Human Feedback (APOHF)\n  1: for t = 1, . . . , T do\n  2:    Train NN using history Dt−1 = {(xs,1, xs,2, ys)}s=1,...,t−1 by minimizing loss function (1)\n  3:   Choose the first prompt xt,1 by maximizing the NN prediction\n  4:   Choose the second prompt xt,2 by maximizing the upper confidence bound in Eq. (2)\n  5:   Obtain the responses from xt,1 and xt,2, and observe user preference: yt = 1(xt,1 ≻xt,2)\n  6: Train NN using entire history, report x∗T = arg maxx∈{xs,1,xs,2}s=1,...,T h(x; θT ) as best prompt\n\n\n3.1  Training the Neural Network for Latent Score Prediction\n\nIn our APOHF, we adopt an NN (more specifically, a multi-layer perceptron, or MLP) with parameters\nθ, denoted as h(x; θ). The NN takes as input the pre-trained embedding x of a prompt and predicts\nits latent score u(x). Therefore, for a pair of prompts x1 and x2, we use σ (h(x1; θ) −h(x2; θ)) to\nmodel the probability that x1 is preferred over x2: P(x1 ≻x2) = σ(u(x1) −u(x2)).\n\n\n                                       3\n\nIn iteration t, given the current history of preference observations Dt−1 = {(xs,1, xs,2, ys)}s=1,...,t−1,\nwe train the NN using gradient descent to minimize the following loss function:\n lt(θ) = − Pt−1s=1 h ys log σ h(xs,1; θ) −h(xs,2; θ) + (1 −ys) log σ h(xs,2; θ) −h(xs,1; θ) i + λ ∥θ∥22 .\n                                                                                                    (1)\nRecall that ys = 1(xs,1 ≻xs,2). Intuitively, minimizing this loss function (1) corresponds to\nobtaining the maximum log-likelihood estimate of the MLP parameters θ (with L2 regularization)\nusing the preference dataset Dt−1. The strong expressive power of the pre-trained embedding and\nthe NN helps us accurately estimate the latent score function u, which is crucial for the strong\nperformance of our APOHF algorithm. After the NN is trained, the resulting NN with parameters\nθt = arg minθlt(θ) is used to select the pair of prompts to query in iteration t (Sec. 3.2).\n\n3.2  Selecting the Next Pair of Prompts\n\nThe prompt selection strategy of our APOHF is designed by drawing inspirations from the theoretically\nprincipled linear dueling bandits [4, 39]. However, note that instead of using a linear model to learn\nthe score function [4, 39], we adopt an NN (Sec. 3.1) to make our APOHF not only theoretically\ngrounded but also practically effective. As we verify in Sec. 4, our APOHF substantially outperforms\nlinear dueling bandits in all our experiments. We also provide some high-level theoretical justifications\nfor our prompt selection strategy in App. C.\n\nWe choose the first prompt greedily, i.e., by selecting the one predicted to have the largest latent\nscore using the trained NN (Sec. 3.1): xt,1 = arg maxx∈X h(x; θt). Next, after the first prompt xt,1\nis selected, we choose the second prompt xt,2 by maximizing an upper confidence bound:\n\n                    xt,2 = arg max                                                                      −1 ,                (2)                               h(x; θt) + ν ∥∇h(x; θt) −∇h(xt,1; θt)∥Vt−1                       x∈X\nin which Vt = Pts=1 φ′sφ′⊤s + λI, and φ′s = ∇h(xs,1; θs) −∇h(xs,2; θs). Our strategy to select\nthe second prompt (2) is able to balance the exploration-exploitation trade-off. Specifically, the first\nterm h(x; θt) allows us to exploit the predicted score of the trained NN. Meanwhile, the second\nterm in (2) characterizes our uncertainty about the score of x given (a) the prompts selected in the\nprevious iterations Xt−1 = {(xs,1, xs,2)}s=1,...,t−1 and (b) the first selected prompt xt,1. Intuitively,\na larger value of the second term (i.e., a larger uncertainty) suggests that x is more different from\nthe previously queried prompts Xt−1 and the first selected prompt xt,1. Therefore, maximizing the\nsecond term in (2) helps us explore the domain of prompts by promoting the selection of a prompt\nthat is different from the previously selected prompts (including those in Xt−1 and xt,1). Here, ν is a\nparameter that controls the trade-off between exploration and exploitation.\n\nIn addition to being theoretically principled, another advantage of our prompt selection strategy is\nthat it provides us with a natural method to choose the prompt to report as the best prompt. In POHF,\nwe only have access to binary preference feedback between pairs of prompts and cannot observe\nnumeric scores indicating the efficacy of different prompts. Therefore, it is non-trivial to choose\nwhich prompt to recommend as the best prompt. Interestingly, our strategy to select the first prompt\nprovides a natural and principled way to choose the prompt to recommend. Specifically, after any\niteration, we train the NN using the current history of preference observations, and choose the prompt\n(among all previously selected prompts) which maximizes the predicted score of the trained NN\nto report as the best prompt (line 6 of Algo. 1). This is in fact analogous to a common practice in\nBayesian optimization, i.e., choosing the input (among all previously queried inputs) that maximizes\nthe predicted function value (i.e., the Gaussian process posterior mean) to report as the best input [30].\n\n3.3  Collecting User Preference Feedback\n\nAfter the pair of prompts xt,1 and xt,2 are selected, we then separately pass them to the target\nblack-box LLM to produce their corresponding responses. Next, these two responses are shown to the\nuser, who then gives preference feedback yt = 1(xt,1, xt,2) indicating which one of the two responses\n(generated from xt,1 and xt,2) is preferred. Then, the newly collected observation (xt,1, xt,2, yt) is\nadded to the history of preference observations to yield Dt = {(xs,1, xs,2, ys)}s=1,...,t, after which\nwe use the updated history Dt to train our NN (Sec. 3.1) and proceed to the next iteration t + 1.\n\nIn addition to the above-mentioned preference feedback, at the beginning of our APOHF, the user\nneeds to provide some initial task description (Fig. 1), which our APOHF algorithm uses to generate\nthe domain of prompts (Sec. 2). The initial task description may be in the form of some input-output\n\n\n                                       4\n\nexemplars for the task (we follow this in our experiments in Sec. 4.1), which our APOHF algorithm\ncan use as input to a powerful LLM to produce the domain of prompts via in-context learning [26].\nAs another example, the initial task description from the user may also be an initial prompt for the\ntask (we follow this in our experiments in Sec. 4.2), and our APOHF algorithm uses a powerful LLM\n(e.g., ChatGPT) to rephrase this initial prompt to produce the domain of prompts. This renders our\nAPOHF algorithm highly flexible and versatile across a broad spectrum of real-world applications.\n\n4  Experiments\n\nWe test the performance of our APOHF using 3 sets of tasks: optimization of user instructions (Sec. 4.1),\nprompt optimization for text-to-image generative models (Sec. 4.2), and response optimization with\nhuman feedback (Sec. 4.3). To the best of our knowledge, our APOHF is the first algorithm that is\ndesigned to efficiently solve the problem of POHF. We compare our APOHF with 3 natural baseline\nmethods which we adapt to POHF. (1) Random Search randomly selects a prompt in every iteration\nand hence ignores the preference feedback. (2) Linear Dueling Bandits [4] uses a linear function to\nmodel the latent score function u and adopts a strategy from [4] to select the pair of prompts (more\ndetails in App. C). After every iteration, the prompt predicted by the linear model to achieve the\nlargest score is reported as the best prompt. (3) Double Thompson Sampling (DoubleTS) was\nrecently applied to the problem of response optimization with human feedback by [15] and was\nshown to be the best-performing method. We follow the implementation of DoubleTS from [15]: We\nchoose the pair of prompts by independently running Thompson sampling (TS) twice, in which the\nreward/score uncertainty is modeled using Epistemic NNs (which consists of 10 individual MLPs).\nWe also use TS to choose the prompt to report as the best prompt after every iteration. Note that\nDoubleTS incurs significantly more computational costs than our APOHF, mainly because DoubleTS\nneeds to train 10 MLPs (in contrast to 1 MLP needed by our APOHF) in every iteration.\n\n4.1  Optimization of User Instructions\n\nTo begin with, we simulate real-world scenarios in which a user aims to find the optimal instruction\nfor a task while only giving human preference feedback. We adopt 30 instruction induction tasks\nfrom [9, 26], which have been commonly used by previous works on instruction optimization\nfor black-box LLMs [9, 20, 26]. For every task, a dataset of input-output exemplars is available,\nwhich we use to simulate the human preference feedback. Specifically, for selecting every pair of\ninstructions/prompts xt,1 and xt,2, we use the validation dataset for this task to calculate the validation\naccuracy achieved by both instructions, which we adopt as their ground-truth latent score values:\nu(xt,1) and u(xt,2). Then, we calculate the preference probability P(x1 ≻x2) = σ(u(x1) −u(x2)),\nand use it as the probability in a Bernoulli distribution to sample the binary preference observation\nyt = 1(xt,1 ≻xt,2). This also naturally allows us to report the validation accuracy achieved by an\ninstruction x as its corresponding latent score value u(x), which we plot in our results (Fig. 2). Of\nnote, unlike the previous works [9, 20, 26], the validation dataset for each task is not used by our\nalgorithm; instead, it is only used to simulate the human preference feedback.\n\nHere, we consider the scenario where the user provides a small number of input-output exemplars as\nthe initial task description (Fig. 1), and we use these exemplars to generate the domain of prompts for\nour APOHF via in-context learning (Sec. 3.3). Specifically, to generate each prompt/instruction in the\ndomain, we randomly sample 5 exemplars from the dataset of 100 exemplars (which are separate from\nthe validation set), and ask ChatGPT to generate the instruction that best describes the input-output\nrelationship of these 5 exemplars via in-context learning. We provide the ChatGPT template used here\nin Example 1 (App. A.3). Fig. 2 displays the performances of different methods averaged over 30\ntasks. After each iteration, every method reports a prompt as the best prompt, and its corresponding\nlatent score (i.e., validation accuracy in this case) is plotted in Fig. 2. The figure shows that our\nAPOHF algorithm consistently and significantly outperforms the other methods. We also demonstrate\nthe progression of the best instruction discovered by our APOHF in Table 1, which further illustrates\nthe capability of our APOHF to efficiently find good instructions using only preference feedback.\n\n4.2  Prompt Optimization for Text-to-Image Generative Models\n\nModern text-to-image generative models, such as DALLE-3 [5], have shown remarkable capabilities\nin generating visually appealing images [10, 38, 43]. These models take a text prompt as input and\ngenerate a corresponding image. When a user adopts DALLE-3 to generate an image, they may need\n\n\n                                       5\n\nto manually try a number of different prompts in order to obtain a desirable image. Interestingly,\nin such applications, our APOHF algorithm can also be adopted to efficiently find the best prompt\nfor a user. Specifically, in every iteration, we can use our APOHF algorithm to select a pair of text\nprompts and generate two corresponding images using DALLE-3, and then ask the user for preference\nfeedback between the two images. We simulate such scenarios using the experiments in this section.\n\nTo begin with, we adopt an initial prompt that describes a complex scene using several sentences (see\nApp. 3 for more details), and rephrase the initial prompt to produce a large number of text prompts\n(more details in App. A). These prompts are used as the domain of prompts for our APOHF, and we\nselect one of the prompts from the domain as the ground-truth prompt. Our implicit assumption is\nthat the image generated by this ground-truth prompt is the image which is most desirable by the user.\nTherefore, for every candidate prompt x in the domain, we measure the similarity of its generated\nimage with the image generated by the ground-truth prompt and use the similarity as the latent score\nu(x) of this prompt. As a result, for every pair of selected prompts xt,1 and xt,2, we can calculate\ntheir preference probability using the BTL model: P(x1 ≻x2) = σ(u(x1)−u(x2)), and then sample\na binary preference observation yt from a Bernoulli distribution with probability P(x1 ≻x2). In this\ncase, the goal of our APOHF is to efficiently find a prompt to produce an image that is most preferred\nby a user, while only requiring a small number of user preference feedback instances.\n\nWe repeat the experiment for 4 different scenes and report the scores of different methods in Fig. 3.\nThe results show that our APOHF consistently outperforms the other baselines across different scenes.\nThat is, our APOHF is able to efficiently discover a prompt to generate an image that satisfies the user’s\npreferences. We also demonstrate in Fig. 4 the evolution of the images generated by the best prompts\ndiscovered by our APOHF across different iterations. The results suggest that as more user feedback\nis collected, our APOHF can efficiently produce images which better align with the image the user\nhas in mind. Note that here we intend for the generated images to match the high-level semantic\ninformation of the ground-truth image rather than the image details, which are usually uncontrollable\ndue to the inherent randomness in image generation. This experiment showcases the considerable\npotential of our APOHF beyond text-generation tasks, suggesting its applicability to a wide range of\nmulti-modal tasks where using human feedback is preferable.\n\n              Garden                           Street                          Cafe                           Sports\n     200                             200                             200\n\n     100                             100                             100                             100\n                                                                       0                                                                      Score  Score    0                                                                      Score    0                                                                      Score                                                                                                        0\n\n                                                               −100\n   −100                          −100\n                                                                                             −100\n         0     50    100    150          0     50    100    150          0     50    100    150          0     50    100    150\n           # Iterations                  # Iterations                  # Iterations                  # Iterations\n\n                       Random         Linear Dueling Bandits        DoubleTS       APOHF\n\nFigure 3: Performances in prompt optimization for image generation in Sec. 4.2 (4 different scenes).\n\n\n4.3  Response Optimization with Human Feedback\n\nIn addition to adapting the response of an LLM by optimizing the prompt (i.e., by solving POHF),\nour APOHF algorithm can also be used to further refine the response from the LLM by tackling the\nproblem of response optimization with human feedback (Sec. 1). Specifically, given a prompt from a\nuser, we can let the LLM generate a large pool of responses and then try to choose the best response\nfrom the pool. Similar to POHF, instead of requesting the user for a numeric score, it is much easier\nto ask the user for preference feedback between a pair of responses (Sec. 1). This problem setting has\nalso been adopted by the recent work of [15].\n\nThis problem can be tackled by a contextual variant of our APOHF. That is, every prompt p can be seen\nas a context, and the pool of responses r’s generated from this prompt can be considered the domain\nof actions. Here, we need to make an important modification to our APOHF. That is, in iteration t\nafter receiving the prompt pt, every input x in the domain is now the embedding of the concatenation\nof the prompt pt and one of the LLM-generated responses r, which we denote as x = [pt, r]. As an\nimplication, the domain Xt from which we choose a pair of inputs changes in every iteration (as a\nresult of the changing prompt pt). However, the strategy for selecting the pair of inputs remains the\nsame (Sec. 3.2), except that the fixed domain X is now replaced by the changing domain Xt.\n\n\n                                       6\n\nGround Truth        Iteration 0         Iteration 10        Iteration 20        Iteration 30\n\n\n\n\n\n        Ground Truth        Iteration 0          Iteration 5         Iteration 10        Iteration 15\n\n\n\n\n\n        Ground Truth        Iteration 0         Iteration 10        Iteration 20        Iteration 30\n\n\n\n\n\n        Ground Truth        Iteration 0         Iteration 30        Iteration 60        Iteration 90\n\n\n\n\n\nFigure 4: Images generated by the best prompt discovered by our APOHF across different iterations.\n\n\n\n                                                                  Response Optimization\nTo simulate  the user preferences between  different           60\nresponses, we adopt the same approach as [15].  That\nis, we use a reward model which is pre-trained using           40\nthe Anthropic Helpfulness and Harmlessness datasets [3].                                                                                                                                                                                          Score\nThen, given a user prompt pt, for every LLM-generated           20\nresponse r, we use the output from the pre-trained\nreward model as the latent score value u([pt, r]) for this            0\nprompt-response pair. Then, for every pair of selected              0      50     100    150\n                                                             # Iterations\nresponses rt,1 and rt,2 by our APOHF, we can calculate                   Random                    DoubleTS\nthe preference probability following the BTL model                          Linear Dueling Bandits       APOHF\nP {rt,1 ≻rt,2} =  σ(u([pt, rt,1]) −u([pt, rt,2])) and\n                                                        Figure 5: Scores of different methods forthen use it to sample a binary preference observation\n                                                       response optimization (Sec. 4.3).yt.  The results are shown in Fig. 5, in which our\nAPOHF significantly outperforms the other methods, including DoubleTS, which is found to be\nthe best-performing method in [15]. We also show an example of how the response optimized by our\nAPOHF is improved across iterations in Table 2. The response discovered by our APOHF after only 20\niterations is both well organized (via a numbered list) and detailed, which aligns well with human\npreferences. This demonstrates the ability of our APOHF to further refine the response of an LLM to\nmake it more preferable for human users, while only requiring human preference feedback.\n\n5  Ablation Study\n\nHere we perform ablation study of our APOHF using the experiments in Sec. 4.3.\n\nEffectiveness of Our Prompt Selection Strategy. Here, we further verify the effectiveness of our\ntheoretically principled prompt selection strategy. We replace the strategy of our APOHF to select a pair\nof prompts by uniform random selection while keeping all other components of our APOHF fixed. That\nis, after every iteration, we still train the NN using the current history of observations as described in\nSec. 3.1, and report the prompt maximizing the prediction of the NN as the best prompt. The results\n\n\n                                       7\n\nTable 1:  The best instructions selected by our  Table 2: The best responses selected by our\nAPOHF in different iterations (Sec. 4.1). Full table APOHF across iterations (Sec. 4.3). The prompt\ncan be found in Table 6.                                   is “Human: What is there to do in Atlantic\n  Task       Iter                   Instruction                   Score   City?”. We omit some words here (replaced by\n                add the prefix \"un-\" to the given words to form\n           0                                                                   0.45    “..”) for shorter display. Full table in App. 5.\n                     their opposites.\n  antonyms\n           5    remove the \"un-\" prefix from each word.            0.45        Iter                     Response                       Score\n           10   provide the opposite of the given words.            0.70             There are many activities to do in Atlantic City! You\n                 choose the second animal in each pair, so the output                    can visit the famous Atlantic City Boardwalk, try your\n           0                                                                   0.30              luck at the casinos, enjoy the beach and water activities,  larger                        is the second animal in each pair.\n  animal         choose the animal that is not a type of fish, and               0    shop at the outlets and specialty stores, dine at top-rated   81.96\n           60    if both animals are not fish, then choose the first    0.55                restaurants, enjoy live entertainment and nightlife, visit\n                                                                         museums and historical sites, and attend events and\n                   animal.                                                                              festivals throughout the year.\n                 choose the animal that is larger in size, so the                    There are plenty of things to do in Atlantic City! You\n           120  output would be: wildebeest shark elk pit bull    1.00             can visit the famous boardwalk, try your luck at the\n                 manatee                                                                    casinos, enjoy a show or concert at one of the many\n           0    provide negative responses to the given inputs.      0.00       10    entertainment venues, dine at a variety of restaurants,   91.46\n  sentiment  60   provide an output based on the given input.          0.00             shop at the outlets and specialty stores, relax on the\n                  provide the sentiment (positive/negative) of the                       beach, or take a stroll through the historic district. There’s\n           120                                                                   0.90             something for everyone in Atlantic City!                  given inputs.\n           0    \"Please alphabetize the following list of words.\"     0.40             There are many attractions and activities in Atlantic City,\n  word                                                                                           including:\n                   rearrange the words in the list in alphabetical order\n  sorting                                                                                              1. Casinos: Atlantic City is known for its many casinos,\n           30   and the output provided is the rearranged list of    0.75       20   where you can try your luck at slots, poker, blackjack, and   180.14\n                  words.                                                             more. 2. Boardwalk: ... 3. Beach: .. 4. Steel Pier: ... 5.\n                   rearrange the words in the list in alphabetical order\n           60                                                                   0.85           Shows and concerts: ... 6. Nightlife: ... 7. Dining: ... 8.\n                and output the sorted list.                                             Shopping: ...\n\n             60\n\n                                                                   40\n\n             40\n                                                                   30\n                            Score                                                                                                                                               Score\n                                                                   20                   º=0             20\n                                                                                       º=1\n                                                                   10                    º=10\n              0\n                0      50     100    150                             0     20     40     60\n                 # Iterations                                  # Iterations\n\n                      APOHF with random prompt selection\n                      APOHF                            Figure 7: Comparison of the performance of our\nFigure 6:  Comparison of our arm selection APOHF algorithm with different values of ν (i.e.,\nstrategy with random selection.                  the exploration parameter).\n\n(Fig. 6) show that randomly selecting the pair of prompts significantly degrades the performance of\nour APOHF, further validating the effectiveness of our prompt selection strategy (Sec. 3.2).\n\nImpact of the Exploration Parameter. Here, we examine the impact of the exploration parameter\nν on our APOHF algorithm. The results (Fig. 7) show that setting ν = 0 (i.e., not performing any\nexploration) degrades the performance of our APOHF. This is because it limits the ability of our\nAPOHF to sufficiently explore the space of possible prompts. On the other hand, using a large value\nof ν = 10 does not significantly affect the performance of APOHF. This is because although a large\nν may result in excessive exploration when selecting the second prompt, the value of ν does not\nalter our strategy to choose the first prompt. Therefore, a large exploration parameter ν does not\nsignificantly diminish the ability of our APOHF to exploit the prediction of the NN.\n\nImpact of the Level of Noise in Preference Feedback. Here, we study the impact of the level of\nnoise in preference feedback on the performance of different algorithms. We alter the level of noise\nin preference feedback by adjusting the scale of the latent score function u. A smaller scale of the\nscores results in noisier preference observations and hence leads to a more difficult optimization\nproblem. This is because according to the BTL model P(x1 ≻x2) = σ(u(x1) −u(x2)), a smaller\nscale of u(·) generally makes the preference probability closer to 0.5. This renders the resulting\nbinary observation yt = 1(xt,1 ≻xt,2) more similar to a purely random sample (with a probability\nof 0.5) and hence noisier. The results (Fig. 8) verify that the smaller the noise, the more pronounced\nthe advantage of our APOHF. Meanwhile, as the noise level becomes too large, the problem becomes\nexcessively difficult for all methods, and eventually, all algorithms achieve similar performances.\n\n\n6  Related Work\n\nPrompt optimization, also referred to as instruction optimization, has been gaining popularity thanks\nto its ability to improve the performance of LLMs without parameter fine-tuning. Earlier works aimed\n\n\n                                       8\n\ns=0.1            s=1            s=10           s=100          s=1000\n    50                 50                 50                                    25\n                                                             25\n\n                                                                                0     Score                                              Score                                              Score                                              Score 0                                        Score\n     0                  0                  0\n       0     100          0     100          0     100          0     100          0     100\n      # Iterations       # Iterations       # Iterations       # Iterations       # Iterations\n\n                       Random         Linear Dueling Bandits        DoubleTS       APOHF\n\nFigure 8: Comparison of the performances of different algorithm under different levels of noise in\nhuman feedback. Here s controls the level of noise, such that a larger s results in a higher noise level.\n\nto optimize the prompt for white-box LLMs, such as AutoPrompt [42], FluentPrompt [41], as well as\nother works based on soft prompt [24, 25, 54]. Recently, more focus has been shifted to optimizing\nthe prompt for black-box LLMs. Among them, BBT [46], BBTv2 [45] and Clip-Tuning [7] require\naccess to the input embedding and output logits of the black-box LLM. Other recent works have\nremoved this restriction. For example, GRIPS [33] and APO [34] used edit-based operations to select\ncandidate prompts for prompt optimization. Other works have adopted evolutionary algorithms (e.g.,\nEvoPrompt [19] and Promptbreeder [16]), reinforcement learning (e.g., BDPL [12] and PRewrite\n[23]), and planning-based methods (e.g., PromptAgent [49]) to achieve prompt optimization for\nblack-box LLMs. The work of [56] proposed APE, which generates candidate instructions using an\nLLM and selects those high-scoring candidates for further refinement. The OPRO algorithm [50]\nwas developed to use an LLM to solve generic black-box optimization problems and was applied\nto the problem of prompt optimization. The work of [28] introduced OPT2I, which uses an LLM\nto sequentially revise the prompt for text-to-image generative models, in order to maximize a score\nmeasuring the consistency of the generated image with the given prompt.\n\nSome recent works have tackled prompt optimization for black-box LLMs by converting it to a\ncontinuous optimization problem. InstructZero [9] adopted a separate white-box LLM to convert\nprompt optimization to optimizing the soft prompt and used Bayesian optimization to solve the\nresulting continuous optimization problem. INSTINCT [26] used neural bandits to sequentially select\nthe instructions to query and leveraged the strong expressive power of neural networks to achieve\nbetter function modeling and hence better prompt optimization. ZOPO [20] adopted zeroth-order\noptimization (ZOO) while estimating the gradient based on a neural network, and further improved\nthe performances of InstructZero and INSTINCT. In addition, [40] demonstrated the potential of\ndrawing inspirations from best arm identification for prompt optimization, and [11] used neural\nbandits for personalized content generation using white-box LLMs. Importantly, to the best of our\nknowledge, these previous works are not able to tackle the problem of POHF considered in our work,\nbecause they require a numeric score to evaluate the efficacy of each prompt.\n\nRLHF has become the most widely used method for aligning the responses of LLMs with human\nvalues [14, 32, 57]. More comprehensive discussions on RLHF can be found in recent surveys [6, 8].\nMore recently, some methods have been developed to sidestep the need for RL and directly use a\npreference dataset for alignment, including direct preference optimization (DPO) [37], SLiC [53], as\nwell as other extensions [1, 2, 18, 27, 29, 47, 48]. The recent work of [15] has shown the potential of\nefficient exploration methods to improve the response of LLMs with human preference feedback.\n\n7  Conclusion and Limitations\n\nWe have introduced the problem of POHF, in which our goal is to optimize the prompt for black-box\nLLMs while using only human preference feedback. To address POHF, we have proposed the\nAPOHF algorithm, which uses a neural network trained using preference feedback to model the latent\nscore function, and chooses the pair of prompts to query based on a principled strategy inspired by\ndueling bandits. By using various tasks, including user instruction optimization, prompt optimization\nfor text-to-image generative models, and response optimization with human feedback, we empirically\nvalidate that our APOHF is able to find a good prompt for a task using a small number of human\nfeedback instances. A potential limitation of our APOHF is that it currently does not accommodate the\nscenario where more than 2 prompts are selected in every iteration, and the user provides feedback\nregarding the ranking of the responses from these prompts. We plan to tackle this in future work by\ndeveloping novel and theoretically principled strategies to choose more than 2 prompts to query.\n\n\n                                       9\n\nReferences\n\n [1] A. Amini, T. Vieira, and R. Cotterell. Direct preference optimization with an offset. arXiv\n     preprint arXiv:2402.10571, 2024.\n\n [2] M. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello. A\n     general theoretical paradigm to understand learning from human preferences. In International\n     Conference on Artificial Intelligence and Statistics, pages 4447–4455. PMLR, 2024.\n\n [3] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\n     T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from\n    human feedback. arXiv preprint arXiv:2204.05862, 2022.\n\n [4] V. Bengs, A. Saha, and E. Hüllermeier. Stochastic Contextual Dueling Bandits under Linear\n     Stochastic Transitivity Models. In Proc. ICML, pages 1764–1786. PMLR, 2022.\n\n [5]  J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo,\n      et al. Improving image generation with better captions. Computer Science. https://cdn. openai.\n     com/papers/dall-e-3. pdf, 2(3):8, 2023.\n\n [6] S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak,\n    D. Lindner, P. Freire, et al. Open problems and fundamental limitations of reinforcement\n     learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.\n\n [7] Y. Chai, S. Wang, Y. Sun, H. Tian, H. Wu, and H. Wang. Clip-tuning: Towards derivative-free\n     prompt learning with a mixture of rewards. In Proc. EMNLP (Findings), pages 108–117, 2022.\n\n [8] S. Chaudhari,  P. Aggarwal,  V. Murahari,  T. Rajpurohit, A. Kalyan, K. Narasimhan,\n    A. Deshpande, and B. C. da Silva. RLHF Deciphered: A Critical Analysis of Reinforcement\n     Learning from Human Feedback for LLMs. arXiv preprint arXiv:2404.08555, 2024.\n\n [9] L. Chen, J. Chen, T. Goldstein, H. Huang, and T. Zhou. InstructZero: Efficient Instruction\n     Optimization for Black-Box Large Language Models. arXiv:2306.03082, 2023.\n\n[10] M. Chen, Y. Liu,  J. Yi, C. Xu, Q. Lai, H. Wang, T.-Y. Ho, and Q. Xu.  Evaluating\n     text-to-image generative models: An empirical study on human image synthesis. arXiv preprint\n     arXiv:2403.05125, 2024.\n\n[11] Z. Chen, W. Daniel, P.-y. Chen, and F. Buet-Golfouse. Online personalizing white-box llms\n     generation with neural bandits. arXiv preprint arXiv:2404.16115, 2024.\n\n[12] S. Diao, Z. Huang, R. Xu, X. Li, L. Yong, X. Zhou, and T. Zhang. Black-box prompt learning\n      for pre-trained language models. Transactions on Machine Learning Research, 2023.\n\n[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\n    M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\n    image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\n[14] Y. Dubois, C. X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. S. Liang, and T. B.\n     Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback.\n    Advances in Neural Information Processing Systems, 36, 2024.\n\n[15] V. Dwaracherla, S. M. Asghari, B. Hao, and B. Van Roy.  Efficient exploration for llms.\n     arXiv:2402.00396, 2024.\n\n[16] C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. Rocktäschel. Promptbreeder:\n      Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.\n[17] Google. PaLM 2 Technical Report. arXiv:2305.10403, 2023.\n\n[18] Q. Gou and C.-T. Nguyen. Mixed preference optimization: Reinforcement learning with data\n     selection and better reference model. arXiv preprint arXiv:2403.19443, 2024.\n\n[19] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, and Y. Yang. Connecting\n     large language models with evolutionary algorithms yields powerful prompt optimizers. In\n     Proc. ICLR, 2024.\n\n[20] W. Hu, Y. Shu, Z. Yu, Z. Wu, X. Lin, Z. Dai, S.-K. Ng, and B. K. H. Low. Localized zeroth-order\n    prompt optimization. arXiv preprint arXiv:2403.02993, 2024.\n[21] D. R. Hunter. MM Algorithms for Generalized Bradley-Terry Models. Annals of Statistics,\n     pages 384–406, 2004.\n\n\n                                       10\n\n[22] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in\n     neural networks. In Proc. NeurIPS, pages 8580–8589, 2018.\n[23] W. Kong, S. A. Hombaiah, M. Zhang, Q. Mei, and M. Bendersky. PRewrite: Prompt Rewriting\n     with Reinforcement Learning. arXiv preprint arXiv:2401.08189, 2024.\n[24] B. Lester, R. Al-Rfou, and N. Constant. The Power of Scale for Parameter-Efficient Prompt\n     Tuning. In Proc. EMNLP, pages 3045–3059, 2021.\n[25] X. L. Li and P. Liang. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proc.\n    ACL, pages 4582–4597, 2021.\n[26] X. Lin, Z. Wu, Z. Dai, W. Hu, Y. Shu, S.-K. Ng, P. Jaillet, and B. K. H. Low. Use Your\n    INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. In\n     Proc. ICML, 2024.\n[27] T. Liu, Z. Qin, J. Wu, J. Shen, M. Khalman, R. Joshi, Y. Zhao, M. Saleh, S. Baumgartner,\n       J. Liu, et al. LiPO: Listwise Preference Optimization through Learning-to-Rank. arXiv preprint\n     arXiv:2402.01878, 2024.\n[28] O. Mañas,  P.  Astolfi, M.  Hall,  C.  Ross,  J. Urbanek,  A.  Williams,  A. Agrawal,\n    A. Romero-Soriano, and M. Drozdzal. Improving text-to-image consistency via automatic\n    prompt optimization. arXiv preprint arXiv:2403.17804, 2024.\n[29] T. Morimura, M. Sakamoto, Y. Jinnai, K. Abe, and K. Air. Filtered direct preference optimization.\n     arXiv preprint arXiv:2404.13846, 2024.\n[30] Q. P. Nguyen, Z. Dai, B. K. H. Low, and P. Jaillet. Value-at-risk optimization with Gaussian\n     processes. In International Conference on Machine Learning, pages 8063–8072. PMLR, 2021.\n[31] OpenAI. GPT-4 Technical Report. arXiv:2303.08774, 2023.\n[32] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\n     K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\n    Advances in neural information processing systems, 35:27730–27744, 2022.\n[33] A. Prasad, P. Hase, X. Zhou, and M. Bansal. GrIPS: Gradient-free, Edit-based Instruction\n     Search for Prompting Large Language Models. In Proc. ACL, pages 3827–3846, 2023.\n[34] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng. Automatic Prompt Optimization with\n     “Gradient Descent\" and Beam Search. arXiv:2305.03495, 2023.\n[35] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n      P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\n     In Proc. ICML, 2021.\n[36] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\n     unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[37] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference\n     optimization: Your language model is secretly a reward model. Advances in Neural Information\n     Processing Systems, 36, 2024.\n[38] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\n     with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision\n    and pattern recognition, pages 10684–10695, 2022.\n[39] A. Saha. Optimal algorithms for stochastic contextual preference bandits. Advances in Neural\n     Information Processing Systems, 34:30050–30062, 2021.\n[40] C. Shi, K. Yang, J. Yang, and C. Shen. Best arm identification for prompt learning under a\n     limited budget. arXiv preprint arXiv:2402.09723, 2024.\n[41] W. Shi, X. Han, H. Gonen, A. Holtzman, Y. Tsvetkov, and L. Zettlemoyer. Toward human\n     readable prompt tuning: Kubrick’s the shining is a good movie, and a good prompt too? In\n     Proc. EMNLP, pages 10994–11005, 2023.\n[42] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh. AutoPrompt: Eliciting Knowledge\n    from Language Models with Automatically Generated Prompts.  In Proc. EMNLP, pages\n    4222–4235, 2020.\n[43]  J. Song, C. Meng, and S. Ermon.  Denoising diffusion implicit models.  arXiv preprint\n     arXiv:2010.02502, 2020.\n\n\n                                       11\n\n[44] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mpnet: Masked and permuted pre-training for\n     language understanding. In Proc. NeurIPS, 2020.\n[45] T. Sun, Z. He, H. Qian, X. Huang, and X. Qiu. BBTv2: Pure black-box optimization can be\n     comparable to gradient descent for few-shot learning. In Proc. EMNLP, pages 3916–3930,\n     2022.\n[46] T.  Sun,  Y.  Shao,  H.  Qian,  X.  Huang,  and  X.  Qiu.     Black-box  tuning  for\n     language-model-as-a-service. In Proc. ICML, pages 20841–20855, 2022.\n[47] Y. Tang, Z. D. Guo, Z. Zheng, D. Calandriello, R. Munos, M. Rowland, P. H. Richemond,\n    M. Valko, B. Á. Pires, and B. Piot. Generalized preference optimization: A unified approach to\n      offline alignment. arXiv preprint arXiv:2402.05749, 2024.\n[48] C. Wang, Y. Jiang, C. Yang, H. Liu, and Y. Chen. Beyond reverse kl: Generalizing direct\n     preference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240,\n     2023.\n[49] X. Wang, C. Li, Z. Wang, F. Bai, H. Luo, J. Zhang, N. Jojic, E. P. Xing, and Z. Hu. PromptAgent:\n     Strategic planning with language models enables expert-level prompt optimization. arXiv\n     preprint arXiv:2310.16427, 2023.\n[50] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen. Large language models as\n     optimizers. In Proc. ICLR, 2024.\n[51] Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. Journal\n      of Computer and System Sciences, 78(5):1538–1556, 2012.\n[52] W. Zhang, D. Zhou, L. Li, and Q. Gu. Neural Thompson sampling. In Proc. ICLR, 2021.\n[53] Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. Slic-hf: Sequence likelihood\n      calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.\n[54] Z. Zhong, D. Friedman, and D. Chen. Factual probing is [MASK]: Learning vs. learning to\n      recall. In Proc. NAACL, pages 5017–5033, 2021.\n[55] D. Zhou, L. Li, and Q. Gu. Neural contextual bandits with UCB-based exploration. In Proc.\n    ICML, pages 11492–11502, 2020.\n[56] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large Language\n    Models Are Human-Level Prompt Engineers. In Proc. ICLR, 2023.\n[57] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano,\n    and G. Irving.  Fine-tuning language models from human preferences.  arXiv preprint\n     arXiv:1909.08593, 2019.\n\n\n\n\n\n                                       12\n\nA  Addtional Details for Experiments\n\nA.1  License for datasets\n\n(1) Instruction induction dataset [9, 26] for optimizing the user instruction: MIT License; (2)\nAnthropic Helpfulness and Harmlessness datasets [3] for response optimization: MIT License.\n\nA.2  Computational resources\n\nAll the experiments are run on a server with AMD EPYC 7763 64-Core Processor, 1008GB RAM,\nand 8 NVIDIA L40 GPUs.\n\nA.3  Additional details on experimental settings\n\nHyper-parameters.  We use an MLP with 2 hidden layers as the NN for the latent score prediction.\nEach hidden layer has a width of 32. At each iteration of our APOHF we re-initialize the NN and train\nthe NN using all available human feedback data for 1000 epochs with Adam optimizer and a learning\nrate of 0.001. We run all algorithms for 150 iterations. We normalize the score distributions for all\napplications to N(0, 100) such that the simulated feedback obtained by the BTL model will not be\ntoo noisy. We use the hyper-parameters of ν = 1 and λ = 0.1 for our APOHF and Linear Dueling\nBandits. For the prompt optimization for text-to-image generative models, we use a larger ν = 10 for\nboth algorithms for better exploration. All the experiments are run at least 2 times to obtain the error\nbars and the average performances. For ChatGPT queries used in all experiments, we use the specific\nversion of “gpt-3.5-turbo-1106” API provided by OpenAI.\n\nUser instruction optimization.  We generate a prompt domain with 200 prompts/instructions. The\nvalidation dataset has a size of 20. The exemplar dataset provided by the user has a size of 100.\nThe validation accuracy for a prompt/instruction is evaluated by using the validation dataset and\nquerying ChatGPT, which is the same as previous works [9, 26]. We use MPNet [44] to obtain the\nrepresentations of the prompts to be the inputs to our NN for the latent score prediction.\n\nPrompt optimization for text-to-image generative models.  We generate a prompt domain with\n200 prompts. Specifically, we use the template in Example 2 to rephrase the initial prompt for each\nscene in Table 3 to obtain the ground-truth prompt. We use the template in Example 2 to rephrase the\ninitial prompt again to obtain 10 different prompts as good candidates in the prompt domain. This is\nto make sure that the domain contains some prompts that are very close to the ground-truth prompt.\nFor the generation of the other 190 prompts in the domain, we first select a subset of sentences from\nthe initial prompt. Specifically, each sentence in the initial prompt is selected with a probability of\n0.3 independently. This is to simulate real-life scenarios where the prompts provided by the users\nmay only contain a fraction of the information needed to generate the ground-truth or ideal images.\nWe combine the selected subset of sentences to form a new prompt and use the template in Example 3\nto rephrase it to obtain a new element in the prompt domain. We repeat the above procedures to\nobtain the other 190 prompts. We use the DALLE-3 model with the generation quality as “standard”\nand the generation size as “1024 × 1024”. We use CLIP [35] to obtain the representations of the\nground-truth image and the generated images. We use the cosine similarity function to calculate the\nsimilarity score between the representations of the ground-truth image and the generated image as\nthe quality measure for the corresponding generated image. We use vision transformer [13] to obtain\nthe representations of the generated images to be the inputs to the NN for the latent score prediction.\nThe reason for using a different representation model for the latent score prediction is to simulate\nreal-life scenarios in which we do not have prior knowledge about the ground-truth score function.\n\nResponse optimization with human feedback. We randomly select 10 questions from the test\ndataset of the Anthropic Helpfulness dataset as the prompts. For each prompt, we generate 50\nresponses from ChatGPT. We set the temperature parameter of ChatGPT to be 1.0 so that the\ngenerated responses are different from each other. We use a fine-tuned GPT-2 model [36] to obtain\nthe ground-truth scores for prompt-response pairs. Specifically, the GPT-2 is fine-tuned on Anthropic\nHelpfulness dataset [3] to determine the helpfulness of the response w.r.t. a prompt by outputting a\nscore. For each response, we concatenate its corresponding prompt as the prefix and input it to the\nfine-tuned GPT-2 model to obtain the ground-truth score. We use MPNet to obtain the representations\n\n\n                                       13\n\nTable 3: Initial prompts for generating images with different scenes.\n\n Scene   Prompt\n            In a vibrant garden, a grand marble fountain gushes clear water, dazzling in the\n             sunlight. Nearby, a centuries-old oak tree stands with sprawling, gnarled branches. A\n Garden   vintage wrought iron bench with floral patterns offers a quaint seat. Beside the path, a\n           whimsical, brightly painted gnome statue holds a fishing rod towards a small pond. In\n            the pond, lily pads float with blooming white lilies.\n        On a lively city street, a striking vintage red telephone booth pops against the muted\n             city colors. Nearby, a vibrant graffiti mural adds color to a plain brick wall, featuring\n          an abstract mix of urban elements. A futuristic bicycle with a shiny, aerodynamic\n  Street\n             silver frame is locked to a lamppost. A small vendor’s stall on the sidewalk displays\n          handmade, colorful beaded jewelry, glistening in the afternoon sun. In the background,\n          an ornate old-fashioned street lamp emits a warm glow as dusk approaches.\n            In a quaint cafe corner, a vintage espresso machine with polished brass fixtures and a\n           matte black body gleams under an antique lamp. A rustic wooden bookshelf, brimming\n           with well-worn books, stands against a distressed cream wall. A marble table at the\n  Cafe\n           room’s center holds a delicate porcelain teapot with intricate blue flowers, from which\n           steam gently rises. Beside the table, a colorful glass mosaic cat sculpture perches on a\n          mismatched velvet chair, casting playful reflections around.\n       A sleek grand piano with a glossy black surface speckled with white spots stands at\n            the room’s center. On the wall, a colorful clock features a face marked by vibrant,\n           multicolored spots for each hour. Beside it, a tall floor lamp sports a leopard-spot\n  Sports\n            patterned lampshade in black and gold. A plush armchair in the corner showcases\n           bold red polka dots on a white background. On a nearby table, a delicate glass vase\n            captivates with swirling, iridescent spots that shimmer in the light.\n\n\n\n\nof the prompt-response pairs to be the inputs to our NN for the latent score prediction. For each\niteration of the algorithm, a prompt is selected in a round-robin fashion with a fixed order. This is for\nthe purpose of result visualization and fair comparison since different algorithms select responses\nfor the same prompt in each corresponding iteration, and their performances are evaluated based\non the same domain in this way. The score for each algorithm in an iteration in Fig. 5 is calculated\nby using the trained latent score prediction model in this iteration from each algorithm to select the\nbest responses for each of the 10 prompts and evaluating these 10 prompt-response pairs with the\nground-truth score function to obtain an average score.\n\n\n   Example Query 1: Instruction Induction Template\n\n    Input: [INPUT]\n    Output: [OUTPUT]\n\n   <More exemplars...>\n\n    Input: [INPUT]\n    Output: [OUTPUT]\n\n   The instruction was to:\n\n\n\n\n\n   Example Query 2: Image Generation Instruction Rephrasing Template 1\n\n   Rephrase the following description: [Initial instruction]\n   The rephrased description is:\n\n\n\n                                       14\n\nTable 4: The best responses selected by our APOHF in different iterations. The prompt is “Human:\nShould you buy life insurance? Assistant: I’m sure life insurance is useful for a lot of people. I can\nalso tell you how you can get insurance, depending on your income and needs. Human: Is term or\nwhole life insurance better?”\n\n  Iter                                 Response                                   Score\n            It really depends on your individual circumstances and financial goals. Term life\n         insurance provides coverage for a specific period of time, while whole life insurance\n   0    provides coverage for your entire life and also has an investment component.  It’s   45.54\n        important to carefully consider your financial situation and consult with a financial\n         advisor before making a decision.\n       Term life insurance is typically more affordable and provides coverage for a specific\n         period of time, while whole life insurance provides coverage for your entire life and\n  10    also includes a cash value component. The best option for you will depend on your   100.96\n         individual needs and financial situation.  It’s important to carefully consider your\n         options and consult with a financial advisor or insurance expert.\n       Term life insurance is typically more affordable and provides coverage for a specific\n         period of time, while whole life insurance provides coverage for your entire life and\n  20    also includes a cash value component. The best option for you will depend on your   100.96\n         individual needs and financial situation.  It’s important to carefully consider your\n         options and consult with a financial advisor or insurance expert.\n\n\n\n   Example Query 3: Image Generation Instruction Rephrasing Template 2\n\n   Modify the following description by rephrasing and changing some information: [Initial\n    instruction]\n   The modified description is:\n\n\nB  Addtional Experimental Results\n\nTable 4 and Table 5 show the best responses selected by our APOHF in different iterations. An\nobservation is that both the response quality and the score increase when more iterations are performed.\nSpecifically, for Table 4, the response in iterations 10 and 20 include more information on the\naffordability of different kinds of insurance compared to iteration 0. For Table 5, as more iterations\nof our APOHF are performed, the response provides more details on the things to do in Atlantic City.\nIn iteration 20, the response even includes bullet points for the recommended activities for the users\nand hence is the most useful response.\n\nTable 6 shows the best instructions selected by our APOHF for different tasks. As more iterations are\nperformed, the instructions become more relevant to the tasks and hence have higher scores.\n\n\nC  Theoretical Justifications for Our APOHF Algorithm\n\nHere, we provide theoretical justifications for the prompt selection strategy of our APOHF algorithm,\nto show that our APOHF algorithm is theoretically principled. Our goal here is to provide high-level\nintuitions rather than giving a complete theoretical analysis.\n\nRecall that as we have introduced in Sec. 3.2, our APOHF algorithm selects the first prompt greedily by\nmaximizing the predicted score (from the trained NN), and chooses the second prompt by maximizing\nupper confidence bound, which is a weighted combination of the score prediction and an uncertainty\nterm (2). This strategy is inspired by previous works on linear dueling bandits [4, 39].\n\nHere, we adopt the simplifying assumption that the utility/score function u is a linear function:\nu(x) = θ⊤x, ∀X with unknown parameter θ. With this assumption, our prompt selection strategy\ncan be seen as a modified version of the algorithm from [4]. Therefore, we follow the notations from\n[4] and present below the most important modifications to the theoretical analysis of [4]. We use z1,2\nto denote the difference between (the features vectors of) a pair of prompts: z1,2 = x1 −x2 and use\n\n\n                                       15\n\nTable 5: The best responses selected by our APOHF in different iterations. The prompt is “Human:\nWhat is there to do in Atlantic City?”.\n\n  Iter                                 Response                                   Score\n        There are many activities to do in Atlantic City! You can visit the famous Atlantic City\n        Boardwalk, try your luck at the casinos, enjoy the beach and water activities, shop at the\n   0     outlets and specialty stores, dine at top-rated restaurants, enjoy live entertainment and   81.96\n          nightlife, visit museums and historical sites, and attend events and festivals throughout\n         the year.\n        There are plenty of things to do in Atlantic City! You can visit the famous boardwalk,\n          try your luck at the casinos, enjoy a show or concert at one of the many entertainment\n  10    venues, dine at a variety of restaurants, shop at the outlets and specialty stores, relax on   91.46\n         the beach, or take a stroll through the historic district. There’s something for everyone\n          in Atlantic City!\n        There are many attractions and activities in Atlantic City, including:\n          1. Casinos: Atlantic City is known for its many casinos, where you can try your luck at\n           slots, poker, blackjack, and more.\n          2. Boardwalk: Take a stroll on the iconic Atlantic City Boardwalk, lined with shops,\n          restaurants, and amusement attractions.\n          3. Beach: Enjoy a day of sun and sand at the Atlantic City beach, a popular spot for\n        swimming, sunbathing, and water sports.\n          4.  Steel Pier:  Visit this historic amusement park featuring rides, games, and\n         entertainment for the whole family.\n  20                                                                                 180.14\n          5. Shows and concerts: Catch a live performance or concert at one of the many\n         entertainment venues in Atlantic City, including the Hard Rock Hotel & Casino and\n         the Borgata Hotel Casino & Spa.\n          6. Nightlife: Experience the vibrant nightlife of Atlantic City, with numerous bars,\n         nightclubs, and lounges offering live music, DJs, and dancing.\n          7. Dining: Indulge in a variety of dining options, from upscale restaurants to casual\n          eateries serving fresh seafood, steaks, and international cuisine.\n          8. Shopping: Explore the shops and boutiques in Atlantic City, offering everything\n        from designer fashions to unique souvenirs.\n\n\nzt,1,2 = xt,1 −xt,2 to denote the difference between the pair of selected prompts in iteration t. The\nmatrix Mt = Pts=1 z⊤t,1,2zt,1,2 intuitively characterizes the information collected up to iteration t.\nWith these notations, ˆθ⊤z represents the predicted reward difference between a pair of prompts x1 and\nx2, which in our case, corresponds to h(x1; θt) −h(x2; θt). Then, θ⊤z represents the ground-truth\nreward difference. Following the standard practice of the analysis of bandit algorithms [4], we assume\nthat the validity of the confidence bound providing a theoretical guarantee on the quality of reward\n                                          −1 . With these, the regret incurred in iteration t can bedifference estimation: |θ⊤z −ˆθ⊤z| ≤ν ∥z∥M t\nanalyzed as:\n   2rt = u(x∗) −u(xt,1) + u(x∗) −u(xt,2)\n\n         (a)\n    = θ⊤(x∗−xt,1) + θ⊤(x∗−xt,2)\n\n         (b)\n    = θ⊤z∗t,1 + θ⊤z∗t,2\n    = (θ −ˆθt)⊤z∗t,1 + ˆθ⊤t z∗t,1 + (θ −ˆθt)⊤z∗t,2 + ˆθ⊤t z∗t,2\n\n         (c)\n\n                                                   −1     ≤ˆθ⊤t z∗t,1 + ν  z∗t,1 M t                            −1 + ˆθ⊤t z∗t,2 + ν  z∗t,2 M t                                                                                                    (3)\n         (d)\n\n                                                                    −1      ≤2ˆθ⊤t (x∗−xt,1) + 2ν ∥x∗−xt,1∥M t                                          −1 + ˆθ⊤t zt,1,2 + ν ∥zt,1,2∥M t\n\n         (e)\n\n                                                                                  −1      ≤2ˆθ⊤t (xt,2 −xt,1) + 2ν ∥xt,2 −xt,1∥M t                                             −1 + ˆθ⊤t (xt,1 −xt,2) + ν ∥xt,1 −xt,2∥M t\n\n\n                                      −1      ≤ˆθ⊤t (xt,2 −xt,1) + 3ν ∥zt,1,2∥M t\n\n        (f)\n                      −1 .    ≤3ν ∥zt,1,2∥M t\n\n\n                                       16\n\nTable 6: The best instructions selected by our APOHF in different iterations.\n\n  Task         Iter                           Instruction                         Score\n              0    add the prefix \"un-\" to the given words to form their opposites.     0.45\n  antonyms    5    remove the \"un-\" prefix from each word.                         0.45\n             10    provide the opposite of the given words.                         0.70\n                     rephrase the given sentences, so I have provided the rephrased\n              0    versions of the input sentences as output. If this is not what you    0.39\n  informal\n                   were looking for, please provide more specific instructions.\n  to formal\n              5    rephrase the given sentences using formal language.              0.44\n             10    rephrase each input sentence using a more formal or polite tone.    0.47\n                    choose the second animal in each pair, so the output is the second\n              0                                                                  0.30\n  larger             animal in each pair.\n  animal            choose the animal that is not a type of fish, and if both animals\n             60                                                                  0.55\n                       are not fish, then choose the first animal.\n                    choose the animal that is larger in size, so the output would be:\n             120                                                                 1.00\n                      wildebeest shark elk pit bull manatee\n                       identify the word in the sentence that is in Russian, and for the\n                              first three sentences, the word \"Russian\" was correctly identified.\n              0                                                                  0.00\n  orthography       However, for the last two sentences, there were no words in\n  starts with          Russian, so the output should have been \"N/A\" or \"none.\"\n             20    identify the adjective in each sentence.                           0.15\n             40    provide the word that starts with the given letter.                  0.80\n                    change the first letter of the word to \"inv\" and then add the rest of\n              0                                                                  0.00\n                       the word.\n  rhymes\n              4     find a word that is an anagram of the given word.                 0.00\n                   change the word to a new word that rhymes with the original\n              8                                                                  0.40\n                    word.\n                     \"Provide the index of the first occurrence of the letter ’a’ in each\n              0                                                                  0.00\n  second            word.\"\n  word letter         \"Provide the index of the first occurrence of the letter ’a’ in each\n              2                                                                  0.00\n                    word.\"\n              4    \"Output the second letter of each word.\"                         1.00\n              0    provide negative responses to the given inputs.                   0.00\n  sentiment    60    provide an output based on the given input.                       0.00\n             120   provide the sentiment (positive/negative) of the given inputs.       0.90\n                      rearrange the words in alphabetical order, so the output for each\n              0                                                                  0.00\n  taxonomy           input would be the words listed in alphabetical order.\n  animal             rearrange the words in alphabetical order, so the output lists the\n             30                                                                  0.00\n                   words in alphabetical order.\n             60   \"Output the animals from the given list.\"                         0.95\n              0    \"Please alphabetize the following list of words.\"                  0.40\n  word\n                      rearrange the words in the list in alphabetical order and the output\n  sorting      30                                                                  0.75\n                     provided is the rearranged list of words.\n                      rearrange the words in the list in alphabetical order and output\n             60                                                                  0.85\n                       the sorted list.\n\n\n\nStep (a) follows because here we have assumed that the score function u is a linear function; in\nstep (b), we have defined z∗t,1 = x∗−xt,1 and z∗t,2 = x∗−xt,2; step (c) follows because we have\nassumed the validity of the confidence bound as described above; step (d) follows simply because\nz∗t,2 = x∗−xt,2 = x∗−xt,1 + xt,1 −xt,2 = z∗t,1 + zt,1,2 (we have also made use of the triangle\ninequality).\n\nSelection of the Second Prompt.  Step (e) follows from the way the second prompt is selected:\n                                      −1 . This, importantly, is analogous to the way in which ourxt,2 = arg maxx∈X ˆθ⊤t x+ν ∥x −xt,1∥M t\nAPOHF algorithm selects the second prompt using Eq. (2). Note that we have replaced the linear score\n\n\n                                       17\n\nprediction ˆθ⊤t x by the prediction from our NN: h(x; θt). We have also used the gradient ∇h(x; θt)\nto replace the original feature vector x, which is justified by the theory of the neural tangent kernel\n(NTK), which has shown that ∇h(x; θt) can be used to approximate the random Fourier features for\nthe NTK [22]. Also note that compared to the theory of NTK, we have designed our APOHF algorithm\nto be more practical following the common practice of neural bandits [52, 55]. Specifically, in the loss\nfunction to train our NN (1), for the regularization parameter, we have replaced the theoretical choice\nof 12mλ ∥θ −θ0∥22 (m is the width of the NN) by simply λ ∥θ∥22; regarding the random features of\nthe NTK, we have replaced the theoretical choice of √m∇h(x;1        θt) by simply ∇h(x; θt).\n\nSelection of the First Prompt.  Step (f) results from the way in which the first prompt is chosen:\nxt,1 = arg maxx∈X ˆθ⊤t x. This is analogous to the way in which our APOHF algorithm selects the first\nprompt: xt,1 = arg maxx∈X h(x; θt).\n\nThe subsequent analysis follows from standard analysis techniques for linear dueling bandits [4].\nTherefore, our strategy to select the two prompts is theoretically principled.\n\nNote that in this section, we have provided some high-level theoretical justifications for the prompt\nselection strategy of our APOHF algorithm. Our prompt selection strategy can, in fact, be seen as a\nvariant of neural dueling bandit algorithms.\n\nD  Broader Impacts\n\nWe expect our work to have important positive societal impacts. Specifically, our algorithm can be\nused to automatically optimize the prompt for LLMs while requiring only preference feedback from\nthe human user. So, our algorithm is likely to make LLMs easier to use for users and hence contribute\nto the easier and wider adoption of LLMs as well as other advanced AI algorithms. This is expected\nto positively impact society by improving productivity at both the individual and the societal levels.\nOn the other hand, a potential negative societal impact is that our algorithm may be adopted by\nmalicious users. These users could intentionally provide misleading preference feedback to the LLM,\nin order to find inappropriate prompts for tasks associated with malicious intents. For example,\nmalicious attackers could use our algorithm to find prompts for jailbreaking LLMs. Developing\neffective safeguarding methods to prevent such potential malicious use presents interesting future\nresearch topics.\n\n\n\n\n\n                                       18",
"headers": [
"arXiv:2405.17346v1  [cs.LG]  27 May 2024",
"Prompt Optimization with Human Feedback",
"User",
"APOHF",
"LLM",
"Abstract",
"1",
"Introduction",
"2",
"Problem Setting",
"3",
"Automated Prompt Optimization with Human Feedback (",
")",
"4",
"Experiments",
"5",
"Ablation Study",
"6",
"Related Work",
"7",
"Conclusion and Limitations",
"References",
"A",
"Addtional Details for Experiments",
"B",
"Addtional Experimental Results",
"C",
"Theoretical Justifications for Our",
"Algorithm",
"D",
"Broader Impacts"
],
"tables": [
"|Col1|Random<br>Linear Dueling Ban|DoubleTS<br>dits APOHF|\n|---|---|---|",
"|potential of our APOHF beyond text-generation tasks, suggesting its applicability to a wide range of multi-modal tasks where using human feedback is preferable.|Col2|Col3|Col4|\n|---|---|---|---|\n|0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Garden<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Street<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Cafe<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>Score<br>Sports<br><br> <br><br>|0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Garden<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Street<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Cafe<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>Score<br>Sports<br><br> <br><br>|0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Garden<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Street<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Cafe<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>Score<br>Sports<br><br> <br><br>|0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Garden<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Street<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>200<br>Score<br>Cafe<br>0<br>50<br>100<br>150<br># Iterations<br>−100<br>0<br>100<br>Score<br>Sports<br><br> <br><br>|\n|||Random<br>Linear Dueling Bandits<br>DoubleTS<br>APOHF||",
"|Response Optimization<br>60<br>40<br>Score<br>20<br>0<br>0 50 100 150<br># Iterations|Col2|Col3|\n|---|---|---|\n||Random<br>Linear Dueling Band|its<br>DoubleTS<br>APOHF|",
"|Task|Iter|Instruction|Col4|Col5|Score|\n|---|---|---|---|---|---|\n|antonyms|0<br>|add the prefix \"un-\" to the given words to form<br>their opposites.<br>|add the prefix \"un-\" to the given words to form<br>their opposites.<br>|add the prefix \"un-\" to the given words to form<br>their opposites.<br>|0.45<br>|\n|antonyms|~~5~~|~~remove the \"un-\"pref~~i~~x from each word.~~|~~remove the \"un-\"pref~~i~~x from each word.~~|~~remove the \"un-\"pref~~i~~x from each word.~~|~~0.45~~|\n|antonyms|~~10~~|~~provide the opposite of thegiven words.~~|~~provide the opposite of thegiven words.~~|~~provide the opposite of thegiven words.~~|~~0.70~~|\n|larger<br>animal|0|choose the second animal in each pair, so the output<br>is the second animal in eachpair.<br>|choose the second animal in each pair, so the output<br>is the second animal in eachpair.<br>|choose the second animal in each pair, so the output<br>is the second animal in eachpair.<br>|0.30|\n|larger<br>animal|60|~~choose the animal that is not a type of f~~i~~sh, and~~<br>if both animals are not fish, then choose the first<br>animal.<br>|~~choose the animal that is not a type of f~~i~~sh, and~~<br>if both animals are not fish, then choose the first<br>animal.<br>|~~choose the animal that is not a type of f~~i~~sh, and~~<br>if both animals are not fish, then choose the first<br>animal.<br>|0.55|\n|larger<br>animal|120|~~choose the animal that is larger in size, so the~~<br>output would be: wildebeest shark elk pit bull<br>manatee|~~choose the animal that is larger in size, so the~~<br>output would be: wildebeest shark elk pit bull<br>manatee|~~choose the animal that is larger in size, so the~~<br>output would be: wildebeest shark elk pit bull<br>manatee|1.00|\n|sentiment|0<br>|provide negative responses to thegiven inputs.<br>|provide negative responses to thegiven inputs.<br>|provide negative responses to thegiven inputs.<br>|0.00<br>|\n|sentiment|~~60~~|~~provide an output based on thegiven input.~~<br>|~~provide an output based on thegiven input.~~<br>|~~provide an output based on thegiven input.~~<br>|~~0.00~~|\n|sentiment|120|~~provide the sentiment (positive/negative) of the~~<br>given inputs.|~~provide the sentiment (positive/negative) of the~~<br>given inputs.|~~provide the sentiment (positive/negative) of the~~<br>given inputs.|0.90|\n|word<br>sorting|0|<br>\"Please alphabetize the following list of words.\"<br>|<br>\"Please alphabetize the following list of words.\"<br>|<br>\"Please alphabetize the following list of words.\"<br>|0.40|\n|word<br>sorting|30|rearrange the words in the list in alphabetical order<br>and the output provided is the rearranged list of<br>words.<br>|rearrange the words in the list in alphabetical order<br>and the output provided is the rearranged list of<br>words.<br>|rearrange the words in the list in alphabetical order<br>and the output provided is the rearranged list of<br>words.<br>|0.75|\n|word<br>sorting|60|rearrange the words in the list in alphabetical order<br>and output the sorted list.|rearrange the words in the list in alphabetical order<br>and output the sorted list.|rearrange the words in the list in alphabetical order<br>and output the sorted list.|0.85|\n||0<br>50<br>100<br>150<br># Iterations<br>0<br>20<br>40<br>60<br>Score<br>|0<br>50<br>100<br>150<br># Iterations<br>0<br>20<br>40<br>60<br>Score<br>|0<br>50<br>100<br>150<br># Iterations<br>0<br>20<br>40<br>60<br>Score<br>|F|F|\n|||APOHF with random prompt selection<br>APOHF||||",
"|Iter|Response|Col3|Col4|Score|\n|---|---|---|---|---|\n|0|There are many activities to do in Atlantic City! You<br>can visit the famous Atlantic City Boardwalk, try your<br>luck at the casinos, enjoy the beach and water activities,<br>shop at the outlets and specialty stores, dine at top-rated<br>restaurants, enjoy live entertainment and nightlife, visit<br>museums and historical sites, and attend events and<br>festivals throughout theyear.|There are many activities to do in Atlantic City! You<br>can visit the famous Atlantic City Boardwalk, try your<br>luck at the casinos, enjoy the beach and water activities,<br>shop at the outlets and specialty stores, dine at top-rated<br>restaurants, enjoy live entertainment and nightlife, visit<br>museums and historical sites, and attend events and<br>festivals throughout theyear.|There are many activities to do in Atlantic City! You<br>can visit the famous Atlantic City Boardwalk, try your<br>luck at the casinos, enjoy the beach and water activities,<br>shop at the outlets and specialty stores, dine at top-rated<br>restaurants, enjoy live entertainment and nightlife, visit<br>museums and historical sites, and attend events and<br>festivals throughout theyear.|81.96|\n|10|<br>There are plenty of things to do in Atlantic City! You<br>can visit the famous boardwalk, try your luck at the<br>casinos, enjoy a show or concert at one of the many<br>entertainment venues, dine at a variety of restaurants,<br>shop at the outlets and specialty stores, relax on the<br>beach, or take a stroll through the historic district. There’s<br>something for everyone in Atlantic City!<br>|<br>There are plenty of things to do in Atlantic City! You<br>can visit the famous boardwalk, try your luck at the<br>casinos, enjoy a show or concert at one of the many<br>entertainment venues, dine at a variety of restaurants,<br>shop at the outlets and specialty stores, relax on the<br>beach, or take a stroll through the historic district. There’s<br>something for everyone in Atlantic City!<br>|<br>There are plenty of things to do in Atlantic City! You<br>can visit the famous boardwalk, try your luck at the<br>casinos, enjoy a show or concert at one of the many<br>entertainment venues, dine at a variety of restaurants,<br>shop at the outlets and specialty stores, relax on the<br>beach, or take a stroll through the historic district. There’s<br>something for everyone in Atlantic City!<br>|91.46|\n|20|There are many attractions and activities in Atlantic City,<br>including:<br>1. Casinos: Atlantic City is known for its many casinos,<br>where you can try your luck at slots, poker, blackjack, and<br>more. 2. Boardwalk: ... 3. Beach: .. 4. Steel Pier: ... 5.<br>Shows and concerts: ... 6. Nightlife: ... 7. Dining: ... 8.<br>Shopping: ...|There are many attractions and activities in Atlantic City,<br>including:<br>1. Casinos: Atlantic City is known for its many casinos,<br>where you can try your luck at slots, poker, blackjack, and<br>more. 2. Boardwalk: ... 3. Beach: .. 4. Steel Pier: ... 5.<br>Shows and concerts: ... 6. Nightlife: ... 7. Dining: ... 8.<br>Shopping: ...|There are many attractions and activities in Atlantic City,<br>including:<br>1. Casinos: Atlantic City is known for its many casinos,<br>where you can try your luck at slots, poker, blackjack, and<br>more. 2. Boardwalk: ... 3. Beach: .. 4. Steel Pier: ... 5.<br>Shows and concerts: ... 6. Nightlife: ... 7. Dining: ... 8.<br>Shopping: ...|180.14|\n|||0<br>20<br>40<br>60<br># Iterations<br>10<br>20<br>30<br>40<br>Score<br>º=0<br>º=1<br>º=10|||",
"|s=0.1 s=1 s=10 s=100 s=1000<br>50 50 50 25<br>25 Score Score Score Score Score<br>0<br>0<br>0 0 0<br>0 100 0 100 0 100 0 100 0 100<br># Iterations # Iterations # Iterations # Iterations # Iterations|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n|||Random<br>Linear Dueling Bandits<br>DoubleTS<br>APOHF|||",
"|Scene|Prompt|\n|---|---|\n|Garden|In a vibrant garden, a grand marble fountain gushes clear water, dazzling in the<br>sunlight. Nearby, a centuries-old oak tree stands with sprawling, gnarled branches. A<br>vintage wrought iron bench with floral patterns offers a quaint seat. Beside the path, a<br>whimsical, brightly painted gnome statue holds a fishing rod towards a small pond. In<br>thepond, lily pads float with blooming white lilies.|\n|Street|On a lively city street, a striking vintage red telephone booth pops against the muted<br>city colors. Nearby, a vibrant graffiti mural adds color to a plain brick wall, featuring<br>an abstract mix of urban elements. A futuristic bicycle with a shiny, aerodynamic<br>silver frame is locked to a lamppost. A small vendor’s stall on the sidewalk displays<br>handmade, colorful beaded jewelry, glistening in the afternoon sun. In the background,<br>an ornate old-fashioned street lamp emits a warmglow as dusk approaches.|\n|Cafe|In a quaint cafe corner, a vintage espresso machine with polished brass fixtures and a<br>matte black body gleams under an antique lamp. A rustic wooden bookshelf, brimming<br>with well-worn books, stands against a distressed cream wall. A marble table at the<br>room’s center holds a delicate porcelain teapot with intricate blue flowers, from which<br>steam gently rises. Beside the table, a colorful glass mosaic cat sculpture perches on a<br>mismatched velvet chair, casting playful reflections around.|\n|Sports|A sleek grand piano with a glossy black surface speckled with white spots stands at<br>the room’s center. On the wall, a colorful clock features a face marked by vibrant,<br>multicolored spots for each hour. Beside it, a tall floor lamp sports a leopard-spot<br>patterned lampshade in black and gold. A plush armchair in the corner showcases<br>bold red polka dots on a white background. On a nearby table, a delicate glass vase<br>captivates with swirling, iridescent spots that shimmer in the light.|",
"|Iter|Response|Score|\n|---|---|---|\n|0|It really depends on your individual circumstances and financial goals. Term life<br>insurance provides coverage for a specific period of time, while whole life insurance<br>provides coverage for your entire life and also has an investment component. It’s<br>important to carefully consider your financial situation and consult with a financial<br>advisor before making a decision.|45.54|\n|10|Term life insurance is typically more affordable and provides coverage for a specific<br>period of time, while whole life insurance provides coverage for your entire life and<br>also includes a cash value component. The best option for you will depend on your<br>individual needs and financial situation. It’s important to carefully consider your<br>options and consult with a financial advisor or insurance expert.|100.96|\n|20|Term life insurance is typically more affordable and provides coverage for a specific<br>period of time, while whole life insurance provides coverage for your entire life and<br>also includes a cash value component. The best option for you will depend on your<br>individual needs and financial situation. It’s important to carefully consider your<br>options and consult with a financial advisor or insurance expert.|100.96|",
"|Iter|Response|Score|\n|---|---|---|\n|0|There are many activities to do in Atlantic City! You can visit the famous Atlantic City<br>Boardwalk, try your luck at the casinos, enjoy the beach and water activities, shop at the<br>outlets and specialty stores, dine at top-rated restaurants, enjoy live entertainment and<br>nightlife, visit museums and historical sites, and attend events and festivals throughout<br>theyear.|81.96|\n|10|There are plenty of things to do in Atlantic City! You can visit the famous boardwalk,<br>try your luck at the casinos, enjoy a show or concert at one of the many entertainment<br>venues, dine at a variety of restaurants, shop at the outlets and specialty stores, relax on<br>the beach, or take a stroll through the historic district. There’s something for everyone<br>in Atlantic City!|91.46|\n|20|There are many attractions and activities in Atlantic City, including:<br>1. Casinos: Atlantic City is known for its many casinos, where you can try your luck at<br>slots, poker, blackjack, and more.<br>2. Boardwalk: Take a stroll on the iconic Atlantic City Boardwalk, lined with shops,<br>restaurants, and amusement attractions.<br>3. Beach: Enjoy a day of sun and sand at the Atlantic City beach, a popular spot for<br>swimming, sunbathing, and water sports.<br>4.<br>Steel Pier: Visit this historic amusement park featuring rides, games, and<br>entertainment for the whole family.<br>5. Shows and concerts: Catch a live performance or concert at one of the many<br>entertainment venues in Atlantic City, including the Hard Rock Hotel & Casino and<br>the Borgata Hotel Casino & Spa.<br>6. Nightlife: Experience the vibrant nightlife of Atlantic City, with numerous bars,<br>nightclubs, and lounges offering live music, DJs, and dancing.<br>7. Dining: Indulge in a variety of dining options, from upscale restaurants to casual<br>eateries serving fresh seafood, steaks, and international cuisine.<br>8. Shopping: Explore the shops and boutiques in Atlantic City, offering everything<br>from designer fashions to unique souvenirs.|180.14|",
"|Task|Iter|Instruction|Score|\n|---|---|---|---|\n|antonyms|0<br>|add theprefix \"un-\" to thegiven words to form their opposites.<br>|0.45<br>|\n|antonyms|~~5~~<br>|~~remove the \"un-\"pref~~i~~x from each word.~~<br>|~~0.45~~<br>|\n|antonyms|~~10~~|~~provide the opposite of thegiven words.~~|~~0.70~~|\n|informal<br>to formal|0<br>|rephrase the given sentences, so I have provided the rephrased<br>versions of the input sentences as output. If this is not what you<br>were looking for, pleaseprovide more specific instructions.<br>|0.39<br>|\n|informal<br>to formal|~~5~~<br>|~~rephrase thegiven sentences using formal language.~~<br>|~~0.44~~<br>|\n|informal<br>to formal|~~10~~|~~rephrase each input sentence using a more formal orpolite tone.~~|~~0.47~~|\n|larger<br>animal|0|choose the second animal in each pair, so the output is the second<br>animal in eachpair.<br>|0.30|\n|larger<br>animal|60|~~choose the animal that is not a type of f~~i~~sh, and if both animals~~<br>are not fish, then choose the first animal.<br>|0.55|\n|larger<br>animal|120|~~choose the animal that is larger in size, so the output would be:~~<br>wildebeest shark elkpit bull manatee|1.00|\n|orthography<br>starts with|0<br>|identify the word in the sentence that is in Russian, and for the<br>first three sentences, the word \"Russian\" was correctly identified.<br>However, for the last two sentences, there were no words in<br>Russian, so the output should have been \"N/A\" or \"none.\"<br>|0.00<br>|\n|orthography<br>starts with|~~20~~<br>|~~identify the adjective in each sentence.~~<br>|~~0.15~~<br>|\n|orthography<br>starts with|~~40~~|~~provide the word that starts with thegiven letter.~~|~~0.80~~|\n|rhymes|0<br>|change the first letter of the word to \"inv\" and then add the rest of<br>the word.<br>|0.00<br>|\n|rhymes|~~4~~|~~f~~i~~nd a word that is an anagram of thegiven word.~~<br>|~~0.00~~|\n|rhymes|8|~~change the word to a new word that rhymes with the original~~<br>word.|0.40|\n|second<br>word letter|0|\"Provide the index of the first occurrence of the letter ’a’ in each<br>word.\"<br>|0.00|\n|second<br>word letter|2<br>|~~\"Provide the index of the f~~i~~rst occurrence of the letter ’a’ in each~~<br>word.\"<br>|0.00<br>|\n|second<br>word letter|~~4~~|~~\"Output the second letter of each word.\"~~|~~1.00~~|\n|sentiment|0<br>|provide negative responses to thegiven inputs.<br>|0.00<br>|\n|sentiment|~~60~~<br>|~~provide an output based on thegiven input.~~<br>|~~0.00~~<br>|\n|sentiment|~~120~~|~~provide the sentiment(positive/negative) of thegiven inputs.~~|~~0.90~~|\n|taxonomy<br>animal|0|rearrange the words in alphabetical order, so the output for each<br>input would be the words listed in alphabetical order.<br>|0.00|\n|taxonomy<br>animal|30<br>|~~rearrange the words in alphabetical order, so the output lists the~~<br>words in alphabetical order.<br>|0.00<br>|\n|taxonomy<br>animal|~~60~~|~~\"Output the animals from thegiven list.\"~~|~~0.95~~|\n|word<br>sorting|0|\"Please alphabetize the following list of words.\"<br>|0.40|\n|word<br>sorting|30|rearrange the words in the list in alphabetical order and the output<br>provided is the rearranged list of words.<br>|0.75|\n|word<br>sorting|60|~~rearrange the words in the list in alphabetical order and output~~<br>the sorted list.|0.85|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/test/2405.17346v1.pdf"
}