{
"text": "Prompt Optimization via Retrieved Reasoning Assets and\n                         Multi-Agent Analysis\n\n               Wonduk Seo                  Juhyeon Lee                   Junseo Koh\n                       Enhans                        Peking University                   Peking University\n                       Seoul, South korea                         Beijing, China                           Beijing, China\n                  wonduk@enhans.ai               leejuhyeon@stu.pku.edu.cn           junseokoh@stu.pku.edu.cn\n\n                 Hyunjin An                        Jian Park                 Seunghyun Lee\n                       Enhans                       Fudan University                       Enhans\n                          Seoul, Korea                        Shanghai, China                          Seoul, Korea\n                    hyunjin@enhans.ai                jianpark@m.fudan.edu.cn             seunghyun@enhans.ai\n2025                          Haihua Chenâˆ—                       Yi Bu*\n                                      University of North Texas                Peking University\n                                             Texas, USA                             Beijing, ChinaOct                              haihua.chen@unt.edu                  buyi@pku.edu.cn\n\n        Abstract                                CCS Concepts18\n         Prompt optimization has emerged as an effective alternative to re-       â€¢ Computing methodologies â†’Multi-agent systems; Natural\n            training for improving the performance of Large Language Models     language generation; â€¢ Information systems â†’Query repre-\n           (LLMs). However, most existing approaches treat evaluation as a      sentation; Relevance assessment.\n           black box, relying solely on numerical scores while offering limited\n            insight into why a prompt succeeds or fails. They also depend heav-    Keywords\n             ily on trial-and-error refinements, which are difficult to interpret\n                                                                       Automatic Prompt Optimization, Score-Aware Prompt Optimiza-[cs.MA]    and control. In this paper, we introduce MA-SAPO, a Multi-Agent                                                                                           tion, Retrieval-Augmented Generation, Large Language Models         framework for Score-Aware Prompt Optimization. Compared to\n                                                                             (LLMs), Multi-Agent System            prior methods, MA-SAPO explicitly couples evaluation outcomes\n          with structured reasoning to guide systematic edits. The frame-\n         work specifically consists of two stages: during the Reasoning Phase,    ACM Reference Format:\n                                                                   Wonduk Seo, Juhyeon Lee, Junseo Koh, Hyunjin An, Jian Park, Seunghyun\n           agents collaboratively explain metric scores, diagnose weaknesses,\n                                                                                            Lee, Haihua Chen, and Yi Bu*. 2018. Prompt Optimization via Retrieved\n          and synthesize targeted refinements that are stored as reusable rea-                                                                                Reasoning Assets and Multi-Agent Analysis . In Proceedings of Make sure\n           soning assets; during the Test Phase, agents retrieve these assets to        to enter the correct conference title from your rights confirmation email\n           analyze optimized prompts and apply only evidence-grounded edits.      (Conference acronym â€™XX). ACM, New York, NY, USA, 11 pages. https:\n         By turning evaluation signals into interpretable reasoning chains,     //doi.org/XXXXXXX.XXXXXXX\n        MA-SAPO produces prompt refinements that are more transpar-\n            ent, auditable, and controllable. Experiments on the HelpSteer1/2\n          benchmarks demonstrate consistent improvements over single-pass\n                                                       1  Introduction          prompting, retrieval-augmented baselines, and prior multi-agent\n             strategies, validating the effectiveness of our approach.1.              Large Language Models (LLMs) have emerged as powerful tools\n                                                                               capable of tackling a wide range of tasks, from reasoning and sum-\n                                                                               marization to complex dialogue and code generation [4, 7, 33]. How-arXiv:2510.16635v1\n                                                                                        ever, their performance remains highly sensitive to the wording,\n              âˆ—denotes Corresponding authors.\n            1Code is available at: https://anonymous.4open.science/r/MA-SAPO-F313                structure, and examples embedded within prompts [5, 12, 15, 27, 28].\n                                                                             Consequently, prompt optimization has rapidly emerged as a prac-\n                                                                                                     tical alternative for improving model behavior, as it avoids the need\n                                                                                       for costly retraining or parameter updates.\n             Permission to make digital or hard copies of all or part of this work for personal or\n              classroom use is granted without fee provided that copies are not made or distributed         Early research explored single-pass prompt optimization strate-\n                for profit or commercial advantage and that copies bear this notice and the full citation       gies that treated the model itself as the optimizer. For instance,\n            on the first page. Copyrights for components of this work owned by others than the     methods such as Chain-of-Thought (CoT) prompting [37], role as-\n               author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n                republish, to post on servers or to redistribute to lists, requires prior specific permission      signment through carefully designed system instructions [17, 38],\n             and/or a fee. Request permissions from permissions@acm.org.                      or structured variants such as Tree-of-Thought (ToT) [40], Graph-\n              Conference acronym â€™XX, Woodstock, NY                                     of-Thought (GoT) [3], and step-back prompting [42] demonstrated\n         Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n         ACM ISBN 978-1-4503-XXXX-X/2018/06                                          that reasoning patterns or role signals embedded into prompts could\n            https://doi.org/XXXXXXX.XXXXXXX                                          substantially improve downstream performance. While effective,\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                                           Seo et al.\n\n\n     Direct Generation             RAG                 MARS                    MA-SAPO (OURS)\n\n           Input Prompt                            Input Prompt                               Input Prompt                                         Input Prompt\n\n         LLM                                   Retrieved 1                             Userproxy (LLM)                           LLM\n      Direct Generation Prompt                (Prompt, Response, 5 metric scores)\n                                                                                                                 ...                                                        (ours)             Planner (LLM)                       MetricAgentExplainer        DiagnosticianAgent          ActionAgentSynthesizer\n         LLM                                  Retrieved 10       5\n                                               (Prompt, Response, 5 metric scores)   â†’              Teacher (LLM)\n        Regeneration Prompt                                                                                                                                                                                                                                                                                                                                                                                                n-step,                            Retrieved 1\n                                                                                                                 Critic (LLM)                                  (Prompt, Response, 5 metric scores)                                  LLM                                                   (MARS)             Response\n                                 RAG Prompt                                  iters               Student (LLM)                                          Retrieved 3                                                                        10                                                                                                                                                                                                                                       non-iterative                   (Prompt, Response, 5 metric scores)\n\n                                  LLM                                  Target (LLM)\n                                                                                                                                               Analyzer Agent (LLM)\n                                                 Regeneration Prompt\n\n                                                             LLM\n                                                                                                                                                        Refiner Agent (LLM)\n                                                    Response                              Regeneration Prompt\n\n                                                                                              LLM\n                                                                                             Response                                        Regeneration Prompt\n\n\n                                                                                                                                                Response\n\n\nFigure 1: Comparative overview of prompt optimization methods. MA-SAPO enhances interpretability by linking evaluation\nscores to reasoning-driven and evidence-based refinements.\n\n\nthese approaches typically operate in isolation: a single model in-      phase, a retrieval-augmented pipeline leverages these assets: (1) an\nstance refines prompts without incorporating systematic feedback     Analyzer Agent contrasts the current prompt with retrieved ex-\nfrom prior attempts or external resources.                           emplars to identify improvement opportunities, while (2) a Refiner\n  As LLMs have advanced, multi-agent frameworks have gained     Agent applies targeted edits supported by diagnostic evidence,\ntraction as a way to diversify perspectives and coordinate multiple      ensuring that optimizations are interpretable and controllable.\nroles in the optimization process [14, 41, 43]. By assigning diverse         Extensive experiments on the open-source HelpSteer1/2 bench-\nspecialized agents to different tasks such as decomposition, evalua-     marks [35, 36] demonstrate that MA-SAPO consistently outper-\ntion, or refinement, these systems move beyond single-pass rewrit-     forms single-pass prompting, retrieval-augmented generation, and\ning to more structured optimization pipelines. At the same time,      prior multi-agent baselines, highlighting the importance of struc-\noptimization-theoretic methods [8, 22] and label-efficient pipelines      tured reasoning as a bridge between evaluation and optimization.\n[9, 39] have sought to reduce the reliance on costly feedback while     Our main contributions include:\nensuring scalability across tasks.                                                               â€¢ We propose a score-aware multi-agent training pipeline that\n   Despite these advances, several common limitations remain: (1)\n                                                                                                  distills evaluation outcomes into reusable, semi-structured\nmost existing frameworks reduce evaluation to outcome scores,\n                                                                reasoning assets, enabling transparent prompt explana-\ntreating it as a black box and leaving practitioners uncertain about\n                                                                                       tions, fine-grained diagnoses, and actionable edit directives.\nwhy certain prompts succeed or fail; (2) optimization often relies on                                                               â€¢ We design MA-SAPO, a modular and computationally ef-\nrepetitive trial-and-error refinements that consume significant com-\n                                                                                       ficient framework that achieves interpretable, auditable,\nputation while offering little transparency; (3) even when reasoning\n                                                           and controllable improvements across diverse datasets and\nis incorporated, it usually remains implicit and is not transformed\n                                                                 model backbones, while substantially reducing token and\ninto explicit, auditable artifacts that can guide systematic edits; (4)\n                                                        API call budgets.\ncurrent pipelines largely optimize for higher outcome scores but                                                               â€¢ We conduct extensive experiments on HelpSteer1 and\noffer limited interpretability and controllability, preventing users\n                                                                      HelpSteer2 benchmarks, demonstrating consistent gains\nfrom understanding trade-offs or applying targeted adjustments.\n                                                                       across multiple evaluation metrics and outperforming six\n  To address these limitations, we propose MA-SAPO, a sequen-\n                                                                                     state-of-the-art baselines in both effectiveness and efficiency.\ntial reasoning framework for prompt optimization that explicitly\nlinks metric outcomes to actionable edits through structured rea-\n                                               2  Related Worksoning artifacts. Specifically, MA-SAPO operates in two comple-\nmentary phases. In the reasoning phase, three agents transform      Single-Pass Prompt Optimization. Prompt optimization has\nannotated scores into progressively enriched artifacts: (1) a Metric       rapidly evolved as a practical means of improving the performance\nExplainer Agent interprets evaluation dimensions, (2) a Diag-      of LLMs without retraining [23, 27, 32, 37]. Early studies high-\nnostician Agent uncovers error sources and trade-offs, and (3)      lighted the centrality of prompts as control variables for steering\nan Action Synthesizer Agent produces concrete edit directives.     model behavior. Specifically, few-shot prompting [4] demonstrated\nThese artifacts are stored as reusable reasoning assets. In the test       that models could adapt to unseen tasks using in-context examples\n                                                                           alone, while retrieval-augmented generation (RAG) [18, 19] injected\n\nPrompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis                                 Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\n\n\nexternal knowledge to improve factuality and adaptability. These      to optimize new prompts and their responses. Thus, the training\ntwo paradigms, context construction and evidence injection, laid      phase serves as data generation, while the test phase operates as a\nthe foundation for single-pass prompt optimization methods. Sub-      retrieval-augmented optimization pipeline.\nsequent research explored reasoning-augmented prompting. Chain-       Throughout this framework, we employ several specialized agents\nof-Thought (CoT) [37] introduced intermediate reasoning steps.     denoted as ğº. Each agent is designed with a distinct functional role:\nBuilding on this idea, Tree-of-Thought (ToT) [40] generalized linear      ğºexp explains metric-level outcomes, ğºdiag diagnoses weaknesses\nreasoning chains into branching structures with self-evaluation     and trade-offs, ğºsyn synthesizes actionable directives, ğºana analyzes\nvia lookahead and backtracking. Other structured strategies, such      retrieved examples, and ğºref refines prompts into optimized ver-\nas Skeleton-of-Thought (SoT) [25], separate planning from execu-      sions. These agents are executed in sequential order within their\ntion to parallelize generation, while Graph-of-Thought (GoT) [3]      respective phases, and their outputs are explicitly stored for later\norganizes reasoning trajectories into graph topologies. Evolution-       retrieval and reasoning. The overview of the MA-SAPO framework\nary and heuristic single-pass approaches have also been explored.        is shown in Figure 2.\nFor instance, PromptBreeder [13] evolves a population of prompts\nthrough mutation and selection, while PromptWizard [2] refines     3.2  Training Phase: Reasoning Asset\nprompts via feedback-driven critique-and-synthesis loops.              Construction\n  Multi-Agent Based Prompt Optimization. Beyond single-\n                                                                     In the training phase, three agents are executed sequentially for\npass methods, multi-agent systems have emerged as a powerful                                                              each ğœğ‘–âˆˆDğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. All three agents operate on the same triplet ğœğ‘–=\nparadigm for prompt optimization [6, 10, 21, 24]. Early agentic                                                                                     (ğ‘ğ‘–,ğ‘Ÿğ‘–, Sğ‘–), ensuring that their reasoning is consistently grounded\nframeworks such as Self-Ask [26] introduced decomposition by\n                                                                         in both the input and output of the original instance.\ngenerating sub-questions and integrating external retrieval, lay-\ning the groundwork for cooperative prompting strategies. Also,       3.2.1  Metric Explainer Agentğºexp. The Metric Explainer Agent pro-\nMulti-Agent Debate (MAD) [10] advanced this idea by allowing      vides natural language justifications for the given scores. Formally,\nagents to iteratively present and critique answers in a roundtable      given ğœğ‘–, the agent produces a reasoning card Cğ‘–:\nformat, thereby improving factual grounding and reasoning quality.\n                                                                 Cğ‘–= ğºexp(ğœğ‘–, ğ‘ƒexp),                        (1)Reflection-based frameworks [16, 31] further enhanced adaptabil-\nity by storing feedback as memory for subsequent optimization     where ğ‘ƒexp is the system prompt guiding explanation. Each card ex-\nattempts. Building on this foundation, recent systems formalized        plicitly outlines the reason whyğœğ‘–received its annotated scores. The\nagent roles and workflows to optimize prompts more systemati-      reasoning card Cğ‘–then serves as input to the next agent, together\ncally [20]. Subsequent work explicitly targeted prompt optimiza-     with ğœğ‘–.\ntion as the objective. For instance, MASS [43] searches over agent\ntopologies while performing block-level prompt refinement, fol-       3.2.2  Diagnostician Agent ğºdiag. The Diagnostician Agent oper-\nlowed by global optimization. Additionally, MARS [41] leverages      ates on ğœğ‘–and the reasoning card Cğ‘–. It extends Cğ‘–by analyzing\na Planner and Teacherâ€“Criticâ€“Student dialogue to iteratively pro-      metric-level weaknesses and trade-offs. ğºdiag produces a diagnos-\npose, critique, and revise prompts in structured rounds, and also        tic summary Dğ‘–that identifies: (1) the key causes of low-scoring\nMAPGD [14] integrates pseudo-gradient feedback with bandit-style      dimensions, and (2) trade-offs across metrics (e.g., verbosity vs.\nexploration, offering sample-efficient optimization through agent      coherence).\ncollaboration. However, most existing frameworks treat evaluation                Dğ‘–= ğºdiag(ğœğ‘–, Cğ‘–, ğ‘ƒdiag).                      (2)\nas a black box with limited interpretability and rely on costly trial-\n                                                         The diagnostic summary Dğ‘–is then passed to the next stage, along\nand-error refinements. MA-SAPO addresses this by grounding                                                               with ğœğ‘–.\noptimization in score-aware reasoning assets that link evaluation\noutcomes to actionable, interpretable, and auditable refinements.         3.2.3  Action Synthesizer Agentğºsyn. The Action Synthesizer Agent\n                                                                       receives ğœğ‘–together with Cğ‘–and Dğ‘–. It converts these enriched in-\n3  Methodology                                                  sights into actionable edit directives (EDs). Each directive corre-\n                                                              sponds to a concrete modification strategy for improving ğœğ‘–:3.1  Preliminaries\nWe denote the training dataset as                                       Eğ‘–= ğºsyn(ğœğ‘–, Cğ‘–, Dğ‘–, ğ‘ƒsyn),                    (3)\n             Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›= {ğœğ‘–}ğ‘ğ‘–=1,  ğœğ‘–= (ğ‘ğ‘–,ğ‘Ÿğ‘–, Sğ‘–),                  where Eğ‘–= {ğ‘’1,ğ‘’2, . . . ,ğ‘’ğ‘š} is a set of recommended edits.\nwhere each entry ğœğ‘–consists of a prompt ğ‘ğ‘–, its associated response         Definition (Reasoning Assets). For notational convenience, we\nğ‘Ÿğ‘–, and a set of scores Sğ‘–= {ğ‘ â„ğ‘’ğ‘™ğ‘,ğ‘ ğ‘ğ‘œğ‘Ÿğ‘Ÿ,ğ‘ ğ‘ğ‘œâ„,ğ‘ ğ‘ğ‘œğ‘šğ‘,ğ‘ ğ‘£ğ‘’ğ‘Ÿğ‘}, corre-      define the collection of training outputs for instance ğ‘–as:\nsponding to the dimensions of helpfulness, correctness, coherence,\n                                                             Rğ‘–= (Cğ‘–, Dğ‘–, Eğ‘–).\ncomplexity, and verbosity. The test set is denoted as\n                                                      As a result, Rğ‘–is stored as the reasoning assets aligned with ğœğ‘–,                    Dğ‘¡ğ‘’ğ‘ ğ‘¡= {ğ‘ğ‘—}ğ‘€ğ‘—=1,                                                               forming the retrieval corpus for the test phase. Importantly, these\nwhere only prompts are available, and corresponding responses are      reasoning assets are designed as semi-structured text, which makes\ngenerated during evaluation. Our objective is to generate structured     them machine-parseable and directly usable by downstream agents\nreasoning assets from Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›that can later be retrieved and utilized       in the test phase.\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                                           Seo et al.\n\n\n  Training Phase: Reasoning Asset Construction\n\n\n\n\n     Prompt(  )\n                        Train data(   )      Metrics      Reasoning Card(   )  Diagnostician   Diagnostic Summary(   )     Action           Edit Direcives(   )    Reasoning\n         A                i = 1, 2, ..., n      Explainer                     Agent                           Synthesizer                          Assets\n    Response(  )               Agent(     )                             (     )                         Agent(     )\n\n\n     Scores(  )\n\n\n  Test Phase: Retrieval and Reformulation\n\n\n                                                                                                                        Optimized\n                                                          Top-k prompt-response pairs(                )                    Improvement                        Optimized                                                                                                                          Counterpart\n                                                       and corresponding reasoning assets.      Analyzer        report(   )      Refiner        Prompt(   )  Test Prompt(     )            Retriever                                                                                                                          (     )\n                                                                        Agent                  Agent\n                                                  Retrieve based on                                         (     )                           (     )\n                                            Prompts from\n      Training                         (+ attached reasoning assets     ; fetched with top-k)\n     Dataset (    )\n\nFigure 2: Overview of the MA-SAPO framework. The training phase constructs reasoning assets (Rğ‘–= (Cğ‘–, Dğ‘–, Eğ‘–)) from annotated\npromptâ€“response pairs, while the test phase retrieves top-k training examples and their reasoning assets to refine a new test\nprompt.\n\n\n3.3  Test Phase: Retrieval and Reformulation            final optimized prompt Ë†ğ‘is then used to generate a corresponding\nIn the test phase, optimization is performed via retrieval-augmented      optimized response Ë†ğ‘Ÿ.\ngeneration. Given a new prompt ğ‘test, we adopt a sparse lexical\nretriever that ranks training prompts based on term-frequency and     3.4  Evaluation\ninverse document-frequency statistics. Each training prompt ğ‘ğ‘—     To assess the effectiveness of our framework, we directly compare\nis represented in the sparse lexical space, and a relevance score is      the optimized counterpart ( Ë†ğ‘, Ë†ğ‘Ÿ) produced by the Refiner Agent.\ncomputed between ğ‘test and ğ‘ğ‘—according to token overlap weighted     Both are fed into an external evaluation model ğ¸, which outputs\nby discriminative importance. The top-ğ‘˜most relevant prompt-     comparative scores across the five annotated dimensions M =\nresponse pairs are then retrieved along with their corresponding       {help, corr, coh, comp, verb}:\nreasoning assets:\n                                                                               Score( Ë†ğ‘, Ë†ğ‘Ÿ) = âŸ¨ğ‘ â„ğ‘’ğ‘™ğ‘,ğ‘ ğ‘ğ‘œğ‘Ÿğ‘Ÿ,ğ‘ ğ‘ğ‘œâ„,ğ‘ ğ‘ğ‘œğ‘šğ‘,ğ‘ ğ‘£ğ‘’ğ‘Ÿğ‘âŸ©,           (6)\n                             {(ğœğ‘—, Rğ‘—)}ğ‘˜ğ‘—=1.\n                                                           where eachğ‘ ğ‘šâˆˆ[0, 4] forğ‘šâˆˆM. These scores are normalized into\n3.3.1  Analyzer Agent ğºana. The Analyzer Agent compares ğ‘test      the [0, 1] range and then aggregated into a single composite value.2\nagainst retrieved examples to identify improvement points. It out-      Additionally, We complement automatic metrics with a human\nputs an improvement report A:                                       evaluation (Section 5.3) that tests (H1) the usefulness, accuracy, and\n                                                                     consistency of the reasoning and (H2) whether optimized prompts        A = ğºana(ğ‘test, {ğœğ‘–, Rğ‘—}ğ‘˜ğ‘—=1, ğ‘ƒğ‘ğ‘›ğ‘).                (4)\n                                                                     preserve the original intent; 14 expert annotators, all data scientists\nThe Analyzer Agent transforms raw retrieval into structured in-      or AI engineers, participated.\nsights rather than merely transferring edits from similar prompts.\nThe agent highlights concrete weaknesses and improvement op-    4  Experimental Design and Setup\nportunities by contrasting the test prompt with retrieved reasoning\n                                                         4.1  Datasets\nassets.\n                                              We conduct our experiments on the HelpSteer family of datasets [35,\n3.3.2  Refiner Agent ğºref. The Refiner Agent regenerates an opti-       36], which provide human-annotated prompt-response pairs for\nmized prompt Ë†ğ‘by incorporating the improvement report:             evaluating prompt optimization in Large Language Models (LLMs).\n                  Ë†ğ‘= ğºref(ğ‘test, A, ğ‘ƒref).                      (5)     Both datasets share the same annotation schema across five quality\n                                                                   dimensions, but differ in size and design.\nIt operationalizes the analyzerâ€™s insights into a concrete optimized      We specifically build the retrieval corpus from the HelpSteer2\nprompt. Compared to a direct rewriting approach, which may in-      training dataset, where each instance ğœğ‘–= (ğ‘ğ‘–,ğ‘Ÿğ‘–, Sğ‘–) is augmented\ntroduce irrelevant or unjustified changes, the refiner explicitly con-     with the reasoning assets Rğ‘–= (Cğ‘–, Dğ‘–, Eğ‘–) produced during our\nditions on the improvement report, producing refinements that are\nfocused, justifiable, and consistent with diagnostic evidence. The        2This evaluation methodology is implemented in our setup in Section 4.2.\n\nPrompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis                                 Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\n\ntraining pipeline. Evaluation is conducted on both HelpSteer1 and     4.2  Evaluation Method and Metrics\nHelpSteer2 validation prompts.                                       Judge model. We employ the ArmoRM-Llama3-8B-v0.1 reward\n   HelpSteer1 contains 35.3k training and .79k validation samples,     model [34] as an interpretable multi-objective evaluator (> 90%\neach comprising a prompt, a response, and five human-annotated     benchmark accuracy), which automatically scores response qual-\nattributes scored on a 0-4 scale. It served as the first large-scale        ity. For any prompt-response pair, the judge model returns five\nbenchmark for aligning model outputs with human feedback and      HelpSteer-aligned scores (shown in Table 1):\nprovides broad coverage of prompt-response styles.\n   HelpSteer2 is a newer dataset containing 20.3k training and 1.04k\nvalidation samples, with higher-quality annotations, multi-turn\nprompts (about 29% of cases), and additional preference annota-                               M = {help, corr, coh, comp, verb},  ğ‘ ğ‘šâˆˆ[0, 4].\ntions between responses [35]. We adopt it as our retrieval corpus\nsince (i) its richer annotations and multi-turn coverage yield more\nrepresentative reasoning assets, (ii) preference signals provide finer-                                                                         Evaluation Metrics. Each candidate pair (ğ‘,ğ‘Ÿ) is evaluated across\ngrained evidence for our Analyzer and Refiner agents, and (iii) it is                                                                   the five HelpSteer dimensions M = {help, corr, coh, comp, verb},\nthe current benchmark standard for training and evaluating reward                                                           where each raw score ğ‘ ğ‘šâˆˆ[0, 4]. We normalize scores by dividing\nmodels, ensuring alignment with best practices in the field.                                                          by 4, yielding Ëœğ‘ ğ‘š= ğ‘ ğ‘š/4 âˆˆ[0, 1]. The overall quality score is then\n                                                                     defined as the average over all metrics:\nTable 1: Overview of HelpSteer1/2 Annotation Dimensions.\nEach scored on a 0â€“4 scale.\n\nAttribute    Description                                                             Score(ğ‘,ğ‘Ÿ) = |M|1  âˆ‘ï¸ ğ‘ ğ‘š4  .\n                                                                    ğ‘šâˆˆM\nHelpfulness   Overall helpfulness of the response to the prompt.\nCorrectness   Inclusion of all pertinent facts without errors.\nCoherence    Consistency and clarity of expression.\nComplexity   Intellectual depth required to write the response                                                                   This produces a single composite value in the range [0, 1], reflecting\n                   (e.g., basic competency vs. domain expertise).                                                                    the overall response quality.\nVerbosity    Amount of detail included in the response relative\n               to what is asked.\n\n                                                         4.3  Prompt Design\n                                                          To guide each agent in MA-SAPO, we carefully designed specialized\n   Reasoningâ€“asset generation (corpus construction). To construct                                                              prompts tailored to their roles in both the training and test phases.4\nthe retrieval corpus, we employ the three reasoning agents (ğºexp,\nğºdiag, ğºsyn) using the o4-mini reasoning model from OpenAI [1].\nThis model is used exclusively for reasoning asset construction rather           (1) The training prompts (Metric Explainer, Diagnostician,\nthan downstream inference. All generations follow default API hy-          and Action Synthesizer) focus on generating structured rea-\nperparameters. For each training instanceğœğ‘–, it produces a reasoning            soning assets by interpreting evaluation scores, diagnosing\ncard Cğ‘–, a diagnostic summary Dğ‘–, and a set of edit directives Eğ‘–,            weaknesses, and synthesizing actionable improvements.\nwhich together form the stored reasoning assets Rğ‘–.                           (2) The test prompts (Analyzer and Refiner) leverage these as-\n                                                                                     sets to contrast retrieved exemplars with the original prompt\n   Generation models for evaluation. For downstream evaluation, we                                                                and iteratively refine it while preserving the userâ€™s intent.\nadopt a representative API model GPT-4o [1], and an open-source\nmodel LLaMA-3-8B-Instruct [11] as the backbone models. These\nmodels are responsible for generating (i) optimized prompts Ë†ğ‘from        These prompts form the backbone of MA-SAPOâ€™s reasoning-\nthe Refiner Agent and (ii) the corresponding optimized responses Ë†ğ‘Ÿ      driven optimization pipeline, ensuring interpretability, controlla-\nconditioned on Ë†ğ‘. Both models are configured with temperature = 0        bility, and consistency across all stages. The prompt templates are\nto ensure deterministic decoding, and all other hyperparameters      provided below:\nremain at their defaults.\n\n   Retrieval. We adopt a sparse retriever based on BM25 [29] over\nprompt text. For each test prompt ğ‘test, we retrieve the top-ğ‘˜train-\ning prompts with ğ‘˜= 3, which we identify as the optimal setting for\nretrieval3 and attach their paired responses and reasoning assets:\n                          {(ğ‘ğ‘—,ğ‘Ÿğ‘—, Rğ‘—)}ğ‘˜ğ‘—=1.\n\n3A detailed analysis of ğ‘˜values and their effects on performance is in Section 5.1.1.         4For brevity, we show shortened prompts; full templates are in our code repository.\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                                           Seo et al.\n\n\n   Prompt Templates for Different Agents in Training and                (1) Direct Generation: The LLM rewrites the input prompt\n    Testing Phases                                                            into an optimized one, without intermediate reasoning.\n                                                                                       (2) Chain-of-Thought (CoT) [2022] [37]: The LLM performs\n    Metric Explainer Agent Prompt                                          step-by-step reasoning about how to improve the prompt,\n    You are an evaluation explainer. Given a user prompt, a model\n                                                                      then outputs an optimized version.\n     response, and five 0â€“4 scores (helpfulness, correctness, coherence,\n                                                                                       (3) Role Assignment [2023] [30]: A crafted system role (e.g.,\n     complexity, verbosity), write ONE cohesive paragraph (6â€“9 sen-\n                                                                    â€œYou are a meticulous assistant optimizing prompts for help-     tences) that explains WHY each score was assigned and HOW to\n    improve the response.                                                           fulness, correctness, coherence, complexity, and verbosity.â€)\n     Rules: - Output must be a single paragraph in plain text (no lists,                guides the LLM to output an optimized prompt in one pass.\n    no JSON, no headings) (...)                                                       (4) Retrieval-Augmented Generation (RAG) [2020] [19]:\n                                                      M25 retrieves the top-ğ‘˜= 10 training prompts with their\n                                                                       responses and corresponding scores. The LLM references\n    Diagnostician Agent Prompt\n                                                                           these exemplars to produce an optimized prompt.    You are a precise evaluation doctor for LLM outputs.\n     Task: Write ONE cohesive paragraph that (1) diagnoses the main                (5) MAD [2023] [10]: MAD (Multi-Agent Debate) framework\n    Strength and Weaknesses, (2) explains why they matter in                organizes several LLM â€œdebatersâ€ that each propose an an-\n    terms of the five metrics, and (3) prescribes concrete fixes. Rules: -              swer with a rationale, then engage in multi-round critiques\n     Evidence-first: rely only on the prompt + response + reasoning.                 of one anotherâ€™s reasoning. A separate judge (or a consen-\n     - Mention each metric once (helpfulness, correctness, coherence,                sus rule) evaluates the arguments at each round and selects\n     complexity, verbosity) with a brief, specific rationale (...)                        the final answer, which measurably improves factuality and\n    Output: One paragraph.                                                    step-by-step reasoning on strategic-reasoning tasks.\n                                                                                       (6) MARS [2025] [41]: MARS is a hierarchical system of seven\n    Action Synthesizer Agent Prompt                                          agents. A Planner designs the optimization trajectory, while\n     Mission: Based on the structured diagnosis (Diagnostician Agent),                a Teacher, Critic, Student Socratic dialogue iteratively refines\n     provide a single actionable prompt suggestion that guides improve-                the prompt. At each iteration, a Target module evaluates the\n    ments while preserving the original intent. Focus on highlighting                 candidates, logs the history, and the system finally outputs\n     strengths, addressing weaknesses, and suggesting ...                           the best prompt.\n    Task: Using the diagnosis, propose concise guidance that: 1.\n    Maintains the core topic and original goal of the prompt. 2.\n    Suggests ways to strengthen depth, clarity, and structure. 3.            For all baselines, we adopt GPT-4o and LLaMA-3-8B-Instruct as\n    Recommends addition or adjustment (...)                              the backbone models, ensuring consistency and fairness in com-\n                                                                  parison by aligning with our main experimental setup. Baseline\n                                                               experiments were conducted with the following settings for fair-\n    Analyzer Agent Prompt\n                                                                 ness and fairness of our framework: all LLM temperatures were    You are the Analyzer Agent.\n    Task: (1) Independently evaluate the original prompt (strengths           set to 0 and all other settings were set to LLM defaults. For MAD,\n    and weaknesses). (2) Analyze retrieved promptâ€“responseâ€“feature               it was fixed as ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡= 2 and ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘= 3, and MARS designed the\n     triples from three agents. (3) Integrate insights into a unified anal-          plannerâ€™s steps in steps 3 âˆ’4 and set the entire iteration to 5.\n     ysis.\n     Rules: - Begin with direct evaluation of the original prompt. -\n     Evaluate each agentâ€™s output concisely (strengths/weaknesses) (...)       5  Experimental Results and Analysis\n    Output: A structured but cohesive paragraph-length analysis.\n                                                         5.1  Main Results\n                                                                       In our main experiments, we compare MA-SAPO with three fami-    Refiner Agent Prompt\n    You are the Refiner Agent.                                                     lies of baselines: (1) single-pass prompting methods including Di-\n    Task: Using Analyzer feedback, optimize the original prompt to           rect Generation, Chain-of-Thought (CoT), and Role Assignment, (2)\n     resolve all issues while preserving intent.                             retrieval-augmented generation (RAG) without reasoning assets,\n     Rules: - Extract the core improvement points from the Analyzer. -        and (3) multi-agent frameworks such as MAD and MARS. As out-\n     Prioritize Action Synthesizer suggestions over others (...)                  lined in Section 4.2, we evaluate on both HelpSteer1 and HelpSteer2\n    Output: Final optimized prompt.                                    using the five annotated quality dimensions and their normalized\n                                                                 average score as metrics. The results are shown in Table 2.\n                                                                          In detail, Single-pass methods provide the simplest form of opti-\n4.4  Baselines                                                  mization, but their weakness lies in performing edits in isolation.\nWe implement six baselines under three different categories: single-      Direct Generation and CoT are essentially one-shot rewrites that\npass without retrieval (1, 2, and 3), retrieval-augmented without       lack diagnostic grounding, which makes their improvements brittle\nreasoning assets (4), and multi-agent prompt optimization (5 and 6).     and often inconsistent across metrics. Role Assignment can guide\nAll methods generate an optimized prompt from the initial prompt,      the model to consider multiple aspects explicitly, yet it still suffers\nand the final response is produced from that optimized prompt by     from over-reliance on handcrafted role descriptions and cannot\nthe same backbone LLM used for the method. Specifically, the six      adapt when those roles fail to cover task-specific subtleties. RAG\nbaseline methods are detailed below:                               improves upon these by injecting exemplars, but the optimization\n\nPrompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis                                 Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\n\n\nTable 2: Main results on HelpSteer1 and HelpSteer2. Columns report the five HelpSteer metrics (Help, Corr, Coh, Comp, Verb)\nnormalized to [0, 1] and their mean (Avg). Methods are grouped into (i) single-pass prompting, (ii) retrieval-augmented generation\n(no reasoning assets), and (iii) multi-agent frameworks. Best within each backbone is bold; second best is underlined.\n\n\n                                                GPT-4o                                  LLaMA-3-8B\n Dataset    Methods\n                               Help   Corr   Coh  Comp   Verb    Avg    Help   Corr   Coh  Comp   Verb    Avg\n\n              Direct Generation         0.3216   0.3866   0.7583   0.2951   0.4215   0.4366   0.2549   0.3247   0.7054   0.2702   0.4062   0.3927\n             Chain-of-Thought (CoT)   0.3223   0.3876   0.7595   0.2935   0.4192   0.4364   0.2359   0.3077   0.6906   0.2627   0.3967   0.3787\n             Role Assignment          0.3988   0.4679   0.8024   0.3427   0.5008   0.5025   0.2887   0.3516   0.7287   0.3164   0.4517   0.4274\n HelpSteer1  RAG (sparse, ğ‘˜=10)        0.3751   0.4402   0.7871   0.3116   0.4779   0.4784   0.2930   0.3594   0.7452   0.3007   0.4377   0.4272\n        MAD                     0.3774   0.4764   0.7954   0.3210   0.5248   0.4990   0.3774   0.5049   0.8067   0.4084   0.6708   0.5531\n         MARS                    0.4159   0.4876   0.7963   0.3293   0.5188   0.5096   0.3901   0.4591   0.7745   0.3512   0.5801   0.5110\n         MA-SAPO (Ours)        0.5183  0.6260  0.8614  0.5013  0.7363  0.6486   0.4110   0.4868   0.8326  0.4720  0.8433  0.6091\n\n              Direct Generation         0.3616   0.4700   0.7723   0.3280   0.4913   0.4846   0.2616   0.3636   0.7078   0.2942   0.4421   0.4139\n             Chain-of-Thought (CoT)   0.2981   0.3958   0.7169   0.2888   0.4709   0.4341   0.1600   0.2545   0.6291   0.2421   0.3812   0.3334\n             Role Assignment          0.4400   0.5175   0.8221   0.4025   0.5992   0.5563   0.2555   0.3303   0.7050   0.3267   0.4441   0.4123\n HelpSteer2  RAG (sparse, ğ‘˜=10)        0.4903   0.5745   0.8642   0.4161   0.6567   0.6003   0.3990   0.4711   0.7989   0.3722   0.5814   0.5245\n        MAD                     0.4167   0.5049   0.8067   0.4084   0.6708   0.5615   0.3971   0.4532   0.7898   0.3998   0.6439   0.5368\n         MARS                    0.4791   0.5569   0.8268   0.4095   0.6234   0.5791   0.4296  0.5019   0.7994   0.3957   0.6181   0.5482\n         MA-SAPO (Ours)        0.5072  0.6038   0.8527   0.5244  0.7570  0.6490   0.4005   0.4754   0.8294  0.4833  0.8441  0.6065\n\n\nTable 3: Ablation Studies on Retrieval Depth and Test Agent Combination. Varying the number of retrieved training examples\n(ğ‘˜) shows that ğ‘˜= 3 (our default, denoted as MA-SAPO (k=3)) provides the best balance between contextual coverage and\nrobustness. Collapsing the Analyzer and Refiner into a single agent yields competitive but less consistent results. Best is\nhighlighted in bold; second best is underlined.\n\n\n                                                GPT-4o                                 LLaMA-3-8B\n Dataset   Methods\n                                Help   Corr   Coh  Comp   Verb   Avg    Help   Corr   Coh  Comp   Verb   Avg\n        ğ‘˜= 1                      0.5182   0.6247   0.8543   0.4943   0.7349   0.6453   0.4045   0.4769   0.8268   0.4729  0.8446   0.6051\n        ğ‘˜= 2                    0.5211  0.6287   0.8591   0.4960   0.7318   0.6473   0.4002   0.4747   0.8236   0.4726   0.8384   0.6019\n HelpSteer1 ğ‘˜= 4                      0.5204   0.6266   0.8606   0.4965   0.7369   0.6482   0.4028   0.4778   0.8272   0.4708   0.8389   0.6035\n              Test Agents Combination   0.4672   0.5415   0.8619   0.3670   0.6403   0.5756   0.4649  0.5203   0.8323   0.4363   0.7249   0.5957\n         MA-SAPO (k=3)           0.5183   0.6260   0.8614  0.5013   0.7363   0.6486  0.4110  0.4868  0.8326   0.4720   0.8433   0.6091\n        ğ‘˜= 1                      0.5058   0.5982   0.8458   0.5113   0.7402   0.6403   0.3910   0.4616   0.8215   0.4784   0.8420   0.5989\n        ğ‘˜= 2                    0.5107  0.6044   0.8504   0.5158   0.7450   0.6452   0.3980   0.4713   0.8268   0.4845   0.8396   0.6041\n HelpSteer2 ğ‘˜= 4                      0.5066   0.6012   0.8480   0.5202   0.7451   0.6442   0.3947   0.4685   0.8251   0.4786   0.8395   0.6013\n              Test Agents Combination  0.5404   0.6012   0.8891   0.4455   0.7398   0.6432   0.4379   0.4720   0.7968   0.4918   0.7321   0.5861\n         MA-SAPO (k=3)           0.5072   0.6038   0.8527  0.5244  0.7570  0.6490  0.4005  0.4754  0.8294   0.4833   0.8441  0.6065\n\n\nprocess remains unguided: retrieved examples are consumed with-       assets. The Analyzer to Refiner pipeline applies only evidence-\nout explanation of why they are useful, leaving the refinement pro-     backed edits drawn from these assets, ensuring that optimizations\ncess heuristic and prone to inconsistency. As a result, these methods      are not only effective but also interpretable and controllable6. For\neither underperform on complex prompts or achieve higher scores       instance, on HelpSteer1 with GPT-4o, MA-SAPO achieves 0.6486\nonly at the cost of verbosity and drift in task semantics.               average score compared to 0.5096 for MARS and 0.4784 for RAG.\n   Recent multi-agent frameworks such as MAD and MARS perform       Similar margins hold across HelpSteer2 and with LLaMA-3-8B, con-\nbetter by coordinating multiple roles in a structured workflow, yet      firming that structured reasoning, rather than trial-and-error search,\nthey remain outcome-driven and heavy in cost5. Their reliance on        is the main driver of performance. Overall, MA-SAPO combines\nrepeated debate or critique cycles incurs significant computational      accuracy and efficiency, making it a more practical and principled\noverhead, while still treating evaluation as a scalar target rather     framework for prompt optimization than competitive baselines.\nthan a source of actionable reasoning. In contrast, our MA-SAPO\nconsistently surpasses all baselines across datasets and backbones\nby explicitly converting evaluation signals into reusable reasoning\n\n\n\n5A detailed cost and latency comparisons are provided in Section 5.4.                  6A detailed case study is provided in Section 5.5.\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                                           Seo et al.\n\n5.2  Ablation Study                                                   5.3.1  Evaluating Multi-Agent Reasoning Quality (H1). To assess\n5.2.1  Top-k Retrieval Variations. To examine how the number of      whether MA-SAPO enhances reasoning quality, we randomly sam-\nretrieved training examples influences the performance of MA-      pled 30 reasoning cases from the HelpSteer1 training set. Each case\nSAPO, we vary the retrieval depth ğ‘˜âˆˆ{1, 2, 3, 4}. Table 3 reports      included two reasoning outputs: one generated by a single-agent\nresults for both GPT-4o and LLaMA-3-8B models across HelpSteer1      baseline and one by MA-SAPOâ€™s multi-agent reasoning pipeline,\nand HelpSteer2.                                                          resulting in 60 reasoning outputs in total. These outputs were inde-\n   For both backbones, increasing ğ‘˜from 1 to 3 generally improves      pendently evaluated by 14 human annotators, yielding 840 individ-\noverall performance. With ğ‘˜= 1, the Analyzer is under-informed,      ual evaluations (30 cases Ã— 2 outputs Ã— 14 annotators). Each output\nas it can only compare the test prompt against a single exemplar,     was rated along three qualitative dimensions:\nlimiting the diversity of reasoning assets available. Moving to ğ‘˜= 2         â€¢ Usefulness: How useful the response is in addressing the\nprovides broader evidence and modest gains, while ğ‘˜= 3 yields the            prompt.\nbest balance across all five metrics and achieves the highest average         â€¢ Accuracy: How factually correct the reasoning content is.\nperformance. At ğ‘˜= 4, results remain competitive but show slight         â€¢ Consistency: How coherent and logically structured the\ndeclines in some dimensions, suggesting that introducing too many            writing is.\nexemplars adds noise and creates conflicting diagnostic signals.          Each dimension was rated on a scale from 1 (very low) to 5 (very\n  These findings highlight a clear trade-off: smaller ğ‘˜restricts       high).\ncontextual diversity, while larger ğ‘˜risks diluting the relevance of\nretrieved assets. Overall, ğ‘˜= 3 achieves the most consistent and          Results. As summarized in Table 4, MA-SAPO outperformed the\n                                                                        single-agent baseline across all three dimensions. The largest gainsrobust performance across datasets and backbones, and we adopt\n                                                          were observed in perceived usefulness and factual accuracy, bothit as the default retrieval setting in all subsequent experiments.\n                                                           showing statistically significant improvements (*p* < 0.05, paired\n5.2.2  Test Agents Combination. To further assess the robustness        ğ‘¡-test, ğ‘›= 30). These findings indicate that multi-agent reasoning\nof MA-SAPO, we conduct an ablation study where the two test-      yields more helpful, accurate, and consistent responses.\nphase agents are merged into a single combined agent, referred to\nas Test Agents Combination. Table 3 summarizes the results for both                                                           Table 4: Human evaluation of reasoning quality. MA-SAPO\nGPT-4o and LLaMA-3-8B on HelpSteer1 and HelpSteer2.                                                         outperforms the single-agent baseline in usefulness, factual\n  We observe that the combined variant achieves competitive per-                                                                 accuracy, and consistency. Scores are averaged on a 1â€“5 scale;\nformance, occasionally surpassing the full MA-SAPO in a single                                                                   asterisks denote significance levels from paired ğ‘¡-tests (âˆ—ğ‘<\nmetric. However, the average performance consistently falls behind       0.05, ğ‘›= 30).\nthe full framework. This suggests that while a unified agent can\ncapture certain improvements, it lacks the structured diagnostic\n                                                                        Method           Usefulness  Accuracy  Consistency  Mean\npipeline of MA-SAPO, where the Analyzer first produces evidence-\ngrounded reports and the Refiner then executes targeted edits.                   Single-Agent             3.64          3.63           3.81         3.69\n                                                                 MA-SAPO (Ours)      3.89âˆ—        3.87âˆ—         4.02       3.93\n  These findings demonstrate the necessity of separating the two\nroles in the test phase. The Analyzer to Refiner pipeline enforces          Î” (Improvement)     +0.25        +0.24         +0.21       +0.23\na more interpretable and controllable workflow, ensuring that re-\nfinements are justified by retrieved reasoning assets rather than                                                                                     5.3.2  Evaluating Directional Consistency of Optimized Prompts (H2).\nintroduced implicitly by a single agent. As a result, the modular                                                        To evaluate whether MA-SAPO preserves the semantic intent of\ndesign of MA-SAPO is not only more reliable but also essential for                                                           prompts during optimization, we randomly sampled 40 prompt\nachieving consistent gains across datasets and backbones.                                                                               pairs, each consisting of an original prompt (ğ´) and its optimized\n                                                                     version (ğµ) generated by MA-SAPO. Each pair was independently\n5.3  Qualitative Analysis                                                                       rated by 14 human annotators, resulting in a total of 560 evaluations\nTo complement the automatic benchmark results, we conducted a       (40 pairs Ã— 14 annotators). Annotators rated the directional consis-\nhuman evaluation designed to assess both the interpretability and      tency of each pair on a 4-point Likert scale (1 = completely changed,\ncontrollability of MA-SAPOâ€™s optimization process. This evalua-     2 = partially changed, 3 = mostly preserved, 4 = fully preserved),\ntion focused on two complementary hypotheses: (H1) that multi-      indicating how well the optimized prompt maintained the intent,\nagent reasoning produces higher-quality reasoning than a single-      goal, and domain of the original.\nagent baseline, and (H2) that MA-SAPO preserves the semantic\ndirection and intent of the original prompts during optimization.           Results. Figure 3 visualizes the distribution of annotator ratings\n   All evaluations were conducted by 14 independent human anno-      for directional consistency. Most scores are concentrated between\ntators (mean age = 29.1, SD = 3.4), who were either data scientists, AI      3 and 4, indicating that the optimized prompts largely preserved\nengineers, or graduate students majoring in data science or related      the original semantic intent. The mean rating was 3.36 with a stan-\nfields. Each annotator was familiar with language model evaluation      dard deviation of 0.74 (ğ‘›= 560), demonstrating that MA-SAPO\nand prompt optimization procedures. The evaluation employed a       effectively enhances prompt clarity and structure while maintain-\n5-point Likert scale for reasoning quality (H1) and a 4-point scale      ing semantic stability. The relatively moderate standard deviation\nfor directional consistency (H2). The following subsections describe       further suggests that annotators consistently judged the optimized\nthe evaluation setup and results.                                 prompts as semantically aligned with their originals.\n\nPrompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis                                 Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\n\n\n                                                               281            @ Initial Input:                 Case                 Mean = 3.36\n       250      SD = Â±0.74                                                                                        HelpSteer 2 Prompt: what are 10 primary ways ChatGPT will change 4th-12th grade education in the united states?...\n                                                211                                                  Response:  Here are three business ideas that leverage ChatGPT to improve assessments in schools: 1. Adaptive-\n                                                                                                                                         Testing Platform: Develop a platform that uses ChatGPT to create and administer adaptive tests to students...           (n=560) 200\n\n                                                                                                                                       Scores: \"helpfulness\": 4, \"correctness\": 4, \"coherence\": 4, \"complexity\": 2, \"verbosity\": 2\n       150                                                                                                               Retrieved Reasoning Asset 1                       Retrieved Reasoning Asset 3           Ratings\n   of                                                                                                           Retrieved Prompt 1: what are of scenarios and        Retrieved Prompt 2: Write some brief text for a\n       100                                                                                                               prospect of applying ChatGPT in language            pamphlet for parents about a high school physics\n                                                                                                                                    learning and teaching?...                                      class...\n                                  57                                                                                                                      Retrieved                                                                                                                      Response                                                                                                                                                       1:                                                                                                                       ChatGPT                                                                                                                                                                                                             is a                                                                                                                                                                                 large                                                                                                                                                                       Retrieved                                                                                                                                                                   Response                                                                                                                                                                                                              2: Sure, here                                                                                                                                                                                                                                are                                                                                                                                                                                         10 out-of-         Number 50                                                                                                                        language                                                                                                                    model                                                                                                             (LLM)                                                                                                                                            developed                                                                                                                                        by                                                                                                                                 OpenAI                                                                                                                                                                                                  that      the-box                                                                                                                                                                                                   ideas                                                                                                                                                                     on how ChatGPT                                                                                                                                                                                                  can                                                                                                                                                                                                 be used\n                   11                                                                                         can assist teachers in a variety of ways due to...         education, students, teachers, and schools... for\n         0                                                                                                                                                                            \"helpfulness\":3,\"correctness\":3,\"coherence\":4,\"                1            2            3            4                               \"helpfulness\":3,\"correctness\":3,\"coherence\":3,\"complexity\":2, \"verbosity\": 2\n                               Directional Consistency Score                                          Metirc Explainer Agent: The response earned a      complexity\":2, \"verbosity\": 2\n                                                                                                                    moderate helpfulness score because it outlines         Metric Explainer Agent: Helpfulness was rated 3\n                                                                                                                                 general ways ChatGPT can support cultural            because while the response adds a- problem-solving\nFigure 3: Distribution of directional consistency ratings             immersion yet stops short of concrete use cases or       section and notes relevant skills,...\n                                                                                                                                     step-by-step guidance...                                   ...  Diagnostician Agent: The draftâ€™s chief strength is(n=560). Ratings on a 4-point scale (1 = changed, 4 = pre-                                                                                           its clear, coherent (4/5) presentation of how physics\nserved). The dashed line marks the mean (3.36) and the                strengthDiagnosticianlies in itsAgent:helpfulThebreadthâ€”itresponseâ€™senumeratesmain            buildshelpfulnessproblemâ€solving(3/5) by lackingskills,distinctbut it fallssectionshorttitlesin\nshaded band shows one standard deviation (SD = 0.74).                           five credible ways ChatGPT can support cultural       and structure, in correctness (3/5) by glossing over\n                                                                                                                                 immersion...                                           the differences among Regents, AP 1 & 2...\nTable 5: Cost and Latency Comparison. MA-SAPO requires               disclaimerAction Synthesizerabout possibleAgent:mistranslations,Begin with a briefthen        Actionsix clearSynthesizer:headingsâ€”Introduction,Organize theHands-On-pamphlet under\nfar fewer tokens and calls than other competitive multi-agent               reorganize the answer into a clear three-stage           Learning, Technology Integration, Experiments,\nbaselines, while maintaining superior optimization perfor-             immersionengagement...frameworkâ€”(1) orientation,(2)- active      CourseSteps... Options (Regents, AP 1 & 2, IB), and Next\nmance. Lower is better (â†“).                             @ Test Phase (Output):\n                                                                                                            Analyzer Agent: The original prompt effectively communicates its intent by asking for a specific output\n                                                                        â€”a list of TV shows with five votesâ€”yet it lacks clarity in terms of expected formatting and context. My\n   Method              # Input Tokens â†“       # Output Tokens â†“       # Calls â†“                 evaluation indicates that...\n  MAD                           5,217                      2,170                  8                      Refiner Agent(Optimized Prompt) : How will ChatGPT transform 4th-12th grade education in the\n   MARS                         17,149                      6,789                 60                    United States? Please provide a list of 10 primary ways, including a brief explanation for each...\n   MA-SAPO (Ours)             4,968                  526                2\n                                                @ Result:\n                                                                                                           Regenerated Response : How ChatGPT Will Transform 4th-12th Grade Education in the United\n                                                                                                                                     States\\n\\n1. Personalized Learning: ChatGPT can tailor educational content to meet individual student\n5.3.3  Summary of Human Evaluation. The human evaluation re-             needs, adapting lessons based on their learning pace and style, which can help improve engagement and\nsults highlight two complementary aspects of MA-SAPO. First,              understanding...\nthe multi-agent reasoning framework generates responses that are\nmore useful, accurate, and consistent than those from a single-                                                            Figure 4: Case study illustrating MA-SAPOâ€™s multi-agent\nagent baseline, confirming the advantage of role-specialized rea-                                                   prompt optimization process. Starting from an initial user\nsoning. Second, MA-SAPO maintains strong semantic stability                                                    prompt and evaluation scores, the framework retrieves rea-\nduring prompt optimization, ensuring that improved clarity and                                                         soning assets, analyzes weaknesses, and iteratively refines\nstructure do not distort the original task intent. Together, these find-                                                             the prompt. The regenerated response shows improved clar-\nings demonstrate that MA-SAPO achieves both reasoning quality                                                                                     ity, structure, and contextual depth compared to the original.\nand semantic controllability, reinforcing the quantitative evidence\npresented in Section 5.3.\n\n                                                                            trade-off. While MAD and MARS achieve moderate gains at the\n5.4  Cost and Latency                                                              expense of heavy resource usage, MA-SAPO provides consistent\nTo complement the effectiveness results, we also compare the com-     improvements while keeping computational demands.\nputational cost of different multi-agent frameworks. Table 5 reports\nthe average number of input tokens, output tokens, and calls per     5.5  Case Study\ninstance. We observe that existing multi-agent systems such as                                                        To further illustrate the effectiveness and practical utility of MA-\nMAD and MARS incur substantial overhead. MAD requires around                                                   SAPO, we present a case study drawn from the HelpSteer2 val-\n5K input tokens, 2K output tokens, and 8 calls on average due to                                                                       idation dataset.7 This example highlights MA-SAPOâ€™s strengths\nrepeated multi-round debates. MARS is even heavier, consuming                                                                         in collaborative reasoning, context-aware prompt refinement, and\nmore than 17K input tokens, nearly 7K output tokens, and about                                                                   robust adaptation across diverse input types, thereby providing\n60 calls per instance, reflecting its iterative plannerâ€“criticâ€“student                                                                   concrete evidence of the frameworkâ€™s ability to enhance prompt\nworkflow. In contrast, MA-SAPO is significantly more efficient: by                                                                    optimization in complex reasoning scenarios.\nconstructing reasoning assets offline and adopting a single Analyzer                                                           The case shown in Figure 4 demonstrates the end-to-end prompt\nto Refiner loop at test time, it reduces the runtime cost to roughly                                                                      optimization process. It begins with an initial input prompt, â€œWhat\n5K input tokens, 0.5K output tokens, and only 2 calls per instance.\n  These results highlight that MA-SAPO not only improves opti-       7Additional case studies demonstrating diverse application scenarios are available in\nmization quality but also achieves a more favorable costâ€“latency       our code repository.\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                                           Seo et al.\n\n\nare 10 primary ways ChatGPT will change 4thâ€“12th grade edu-         [3] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\ncation in the United States?â€, alongside the corresponding model            Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr\n                                                                                    Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with large\nresponse and evaluation scores. MA-SAPO then dynamically re-            language models. In Proceedings of the AAAI conference on artificial intelligence,\ntrieves three semantically and thematically relevant exemplars from              Vol. 38. 17682â€“17690.\nthe training corpus (e.g., â€œapplying ChatGPT in language learning          [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,                                                                                                    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nand teachingâ€). These retrieved assets introduce diverse educational              Askell, et al. 2020. Language models are few-shot learners. Advances in neural\ncontexts and prompt formulations, implicitly guiding refinements             information processing systems 33 (2020), 1877â€“1901.\n                                                                                                  [5] Bowen Cao, Deng Cai, Zhisong Zhang, Yuexian Zou, and Wai Lam. 2024. On\nsuch as narrowing task scope and aligning conceptual goals with             the worst prompt performance of large language models. Advances in Neural\nactionable applications.                                                                 Information Processing Systems 37 (2024), 69022â€“69042.\n  The Analyzer Agent identifies weaknesses in the original prompt          [6] Pei Chen, Boran Han, and Shuai Zhang. 2024. Comm: Collaborative multi-agent,\n                                                                                           multi-reasoning-path prompting for complex problem solving. arXiv preprint\nnotably, vague task specification and overly broad phrasing-both of              arXiv:2404.17729 (2024).\nwhich risk eliciting generic responses. Building on this analysis, the          [7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nRefiner Agent reformulates the prompt to preserve its educational             Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\n                                                                                                 bastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\nintent while incorporating a more actionable subtask, such as ex-             Journal of Machine Learning Research 24, 240 (2023), 1â€“113.\nploring business opportunities tied to ChatGPT-based assessment          [8] Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das,\n                                                                                         Bradley A. Malin, and Sricharan Kumar. 2025. Automatic Prompt Optimization\npractices. The optimized prompt exhibits clearer structure, explicit             via Heuristic Search: A Survey. arXiv (2025). arXiv:2502.18746 [cs.CL] https:\nsegmentation, and richer contextual depth. When regenerated with              //arxiv.org/abs/2502.18746\nthe refined prompt, the model produces a response that is more          [9] Ximing Dong, Shaowei Wang, Dayi Lin, and Ahmed E. Hassan. 2025. Model\n                                                                                    Performance-Guided Evaluation Data Selection for Effective Prompt Optimiza-\ncoherent, pedagogically grounded, and practically insightful.                      tion. arXiv (2025). arXiv:2505.10736 [cs.CL] https://arxiv.org/abs/2505.10736\n   Overall, this case underscores how MA-SAPO integrates multi-        [10] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch.\nagent reasoning and retrieved exemplars to achieve superior prompt              2023. Improving factuality and reasoning in language models through multiagent                                                                                                  debate. In Forty-first International Conference on Machine Learning.\noptimization, outperforming conventional single-agent or static        [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nfine-tuning approaches in terms of coherence, interpretability, and             Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,\n                                                                                                         et al. 2024. The llama 3 herd of models. arXiv e-prints (2024), arXivâ€“2407.\nalignment with user intent.                                                         [12] Federico Errica, Giuseppe Siracusano, Davide Sanvito, and Roberto Bifulco. 2024.\n                                                                        What did I do wrong? Quantifying LLMsâ€™ sensitivity and consistency to prompt\n                                                                                              engineering. arXiv preprint arXiv:2406.12334 (2024).\n6  Conclusion                                                                   [13] C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. RocktÃ¤schel. 2023.\n                                                                                        PromptBreeder: Self-Referential Self-Improvement via Prompt Evolution. arXiv\nWe propose MA-SAPO, a novel multi-agent framework for inter-              preprint (2023). arXiv:2309.16797 [cs.CL]\npretable and controllable prompt optimization. Compared to prior        [14] Yichen Han, Bojun Liu, Guanyu Liu, Zeng Zhang, Yang Yang, Wenli Wang,\n                                                                              and Tianyu Shi. 2025. MAPGD: Multi-Agent Prompt Gradient Descent for\nmethods that either perform one-shot rewrites, rely on unguided             Collaborative Prompt Optimization.  arXiv (2025).  arXiv:2509.11361 [cs.CL]\nretrieval, or employ heavy multi-agent debates, MA-SAPO explic-             https://arxiv.org/abs/2509.11361\nitly transforms evaluation outcomes into reusable reasoning assets        [15]  Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, and\n                                                                                            Sadid Hasan. 2024. Does prompt formatting have any impact on llm performance?\nand grounds refinements in evidence-backed Analyzer to Refiner             arXiv preprint arXiv:2411.10541 (2024).\ninteractions. Our contributions extend beyond framework design        [16] Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh\nto include comprehensive experiments on two benchmarks and            Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim RocktÃ¤schel, and                                                                                  Ethan Perez. 2024. Debating with more persuasive llms leads to more truthful\nmultiple backbones, ablation studies on retrieval depth and agent             answers. arXiv preprint arXiv:2402.06782 (2024).\nconfiguration, and human evaluations of reasoning quality and se-        [17] Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou,\n                                                                                        Enzhi Wang, and Xiaohang Dong. 2023. Better zero-shot reasoning with role-play\nmantic consistency. Extensive results demonstrate that MA-SAPO            prompting. arXiv preprint arXiv:2308.07702 (2023).\nconsistently outperforms existing approaches, achieving higher        [18] Juhyeon Lee, Wonduk Seo, Hyunjin An, Seunghyun Lee, and Yi Bu. 2025. Better by\noptimization quality while remaining efficient in cost and latency.            Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt\n                                                                                             Optimization. arXiv preprint arXiv:2509.02093 (2025).\n   Despite these advantages, MA-SAPO currently depends on the        [19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nquality of human-annotated scores and a fixed semi-structured         Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel,\nschema for asset construction, while also relying on a sparse BM25               et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.                                                                                     Advances in neural information processing systems 33 (2020), 9459â€“9474.\nretriever and a single reward model-factors that may overlook        [20] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nsemantically relevant exemplars and introduce evaluator bias. In           Ghanem. 2023. Camel: Communicative agents for\" mind\" exploration of large\n                                                                                     language model society. Advances in Neural Information Processing Systems 36\nfuture work, we plan to improve robustness by integrating a dedi-              (2023), 51991â€“52008.\ncated Feedback Agent to validate asset quality and mitigate bias, as        [21] Renhao Li, Minghuan Tan, Derek F Wong, and Min Yang. 2024. Coevol: Construct-\nwell as by developing hybrid denseâ€“sparse retrieval strategies. We             ing better responses for instruction finetuning through multi-agent cooperation.\n                                                                                          arXiv preprint arXiv:2406.07054 (2024).\nalso aim to extend the framework to multi-turn prompts to enable        [22] Wenwu Li, Xiangfeng Wang, Wenhao Li, and Bo Jin. 2025.  A Survey of\nmore comprehensive evaluation.                                               Automatic Prompt Engineering: An Optimization Perspective.  arXiv (2025).\n                                                                                             arXiv:2502.11560 [cs.CL] https://arxiv.org/abs/2502.11560\n                                                                                            [23] Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, and Yu Cheng. 2025. Test-time pref-\nReferences                                                                        erencepreprintoptimization:arXiv:2501.12895On-the-fly(2025). alignment via iterative textual feedback. arXiv\n [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-        [24] Yuchi Liu, Jaskirat Singh, Gaowen Liu, Ali Payani, and Liang Zheng. 2024. To-\n      cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal           wards hierarchical multi-agent workflows for zero-shot prompt optimization.\n     Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774             arXiv preprint arXiv:2405.20252 (2024).\n      (2023).                                                                                [25] Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang.\n [2] E. Agarwal et al. 2024. PromptWizard: Task-Aware Prompt Optimization Frame-             2023. Skeleton-of-thought: Large language models can do parallel decoding.\n     work. arXiv preprint (2024). arXiv:2405.18369 [cs.CL]                                      Proceedings ENLSP-III (2023).\n\nPrompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis                                 Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\n\n\n\n[26] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike               (2024). arXiv:2406.08673 [cs.CL] https://arxiv.org/abs/2406.08673\n     Lewis. 2022. Measuring and narrowing the compositionality gap in language        [36] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar,\n     models. arXiv preprint arXiv:2210.03350 (2022).                                          Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope,\n[27] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng.              et al. 2023. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv\n     2023. Automatic prompt optimization with\" gradient descent\" and beam search.              preprint arXiv:2311.09528 (2023).\n     arXiv preprint arXiv:2305.03495 (2023).                                                 [37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\n[28] Amirhossein Razavi, Mina Soltangheis, Negar Arabzadeh, Sara Salamat, Morteza           Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\n     Zihayat, and Ebrahim Bagheri. 2025. Benchmarking prompt sensitivity in large              in large language models. Advances in neural information processing systems 35\n     language models. In European Conference on Information Retrieval. Springer,              (2022), 24824â€“24837.\n     303â€“313.                                                                              [38] Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. 2023. Large\n[29] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance            language models are diverse role-players for summarization evaluation. In CCF\n     framework: BM25 and beyond. Foundations and TrendsÂ® in Information Retrieval              international conference on natural language processing and Chinese computing.\n      3, 4 (2009), 333â€“389.                                                                         Springer, 695â€“707.\n[30] Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with        [39] Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang,\n     large language models. Nature 623, 7987 (2023), 493â€“498.                                       Sirui Hong, Chenglin Wu, and Yuyu Luo. 2025. Self-Supervised Prompt Optimiza-\n[31] Noah Shinn, Peter Labash, and Manuela Veloso. 2023.  Reflexion: Language               tion. arXiv (2025). arXiv:2502.06855 [cs.CL] https://arxiv.org/abs/2502.06855\n    Agents with Verbal Reinforcement Learning. arXiv (2023). arXiv:2303.11366        [40] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and\n     https://arxiv.org/abs/2303.11366                                                       Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with\n[32] Hong Sun, Xue Li, Yinchuan Xu, Youkow Homma, Qi Cao, Min Wu, Jian Jiao,             large language models. Advances in neural information processing systems 36\n    and Denis Charles. 2023. Autohint: Automatic prompt optimization with hint               (2023), 11809â€“11822.\n     generation. arXiv preprint arXiv:2307.07415 (2023).                                     [41] Jian Zhang, Zhangqi Wang, Haiping Zhu, Jun Liu, Qika Lin, and Erik Cambria.\n[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne             2025. MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n     Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal           Automated Prompt Optimization. arXiv (2025). arXiv:2503.16874 [cs.CL] https:\n     Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv              //arxiv.org/abs/2503.16874\n      preprint arXiv:2302.13971 (2023).                                                       [42] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H\n[34] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024.             Chi, Quoc V Le, and Denny Zhou. 2023. Take a step back: Evoking reasoning via\n     Interpretable preferences via multi-objective reward modeling and mixture-of-             abstraction in large language models. arXiv preprint arXiv:2310.06117 (2023).\n      experts. arXiv preprint arXiv:2406.12845 (2024).                                        [43] Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan\n[35] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert,              VuliÄ‡, Anna Korhonen, and Sercan Ã–. ArÃ­k. 2025.   Multi-Agent Design:\n    Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024. Help-            Optimizing Agents with Better Prompts and Topologies.   arXiv  (2025).\n      Steer2: Open-source dataset for training top-performing reward models. arXiv             arXiv:2502.02533 [cs.AI] https://arxiv.org/abs/2502.02533",
"headers": [
"arXiv:2510.16635v1  [cs.MA]  18 Oct 2025",
"Prompt Optimization via Retrieved Reasoning Assets and",
"Multi-Agent Analysis",
"Wonduk Seo",
"Juhyeon Lee",
"Junseo Koh",
"Hyunjin An",
"Jian Park",
"Seunghyun Lee",
"Haihua Chen",
"Yi Bu*",
"CCS Concepts",
"Abstract",
"Keywords",
"1",
"Introduction",
"2",
"Related Work",
"3.2",
"Training Phase: Reasoning Asset",
"Construction",
"3",
"Methodology",
"3.1",
"Preliminaries",
"3.3",
"Test Phase: Retrieval and Reformulation",
"3.4",
"Evaluation",
"4",
"Experimental Design and Setup",
"4.1",
"Datasets",
"4.2",
"Evaluation Method and Metrics",
"4.3",
"Prompt Design",
"5",
"Experimental Results and Analysis",
"5.1",
"Main Results",
"4.4",
"Baselines",
"5.2",
"Ablation Study",
"5.3",
"Qualitative Analysis",
"5.4",
"Cost and Latency",
"5.5",
"Case Study",
"6",
"Conclusion",
"References"
],
"tables": [
"|Dataset Methods|GPT-4o<br>Help Corr Coh Comp Verb Avg|LLaMA-3-8B<br>Help Corr Coh Comp Verb Avg|\n|---|---|---|\n|_HelpSteer1_<br>Direct Generation<br>Chain-of-Thought (CoT)<br>Role Assignment<br>RAG (sparse,_ ğ‘˜_=10)<br>MAD<br>MARS<br>**MA-SAPO (Ours)**|0.3216<br>0.3866<br>0.7583<br>0.2951<br>0.4215<br>0.4366<br>0.3223<br>0.3876<br>0.7595<br>0.2935<br>0.4192<br>0.4364<br>0.3988<br>0.4679<br>0.8024<br>0.3427<br>0.5008<br>0.5025<br>0.3751<br>0.4402<br>0.7871<br>0.3116<br>0.4779<br>0.4784<br>0.3774<br>0.4764<br>0.7954<br>0.3210<br>0.5248<br>0.4990<br>0.4159<br>0.4876<br>0.7963<br>0.3293<br>0.5188<br>0.5096<br>**0.5183**<br>**0.6260**<br>**0.8614**<br>**0.5013**<br>**0.7363**<br>**0.6486**|0.2549<br>0.3247<br>0.7054<br>0.2702<br>0.4062<br>0.3927<br>0.2359<br>0.3077<br>0.6906<br>0.2627<br>0.3967<br>0.3787<br>0.2887<br>0.3516<br>0.7287<br>0.3164<br>0.4517<br>0.4274<br>0.2930<br>0.3594<br>0.7452<br>0.3007<br>0.4377<br>0.4272<br>0.3774<br>**0.5049**<br>0.8067<br>0.4084<br>0.6708<br>0.5531<br>0.3901<br>0.4591<br>0.7745<br>0.3512<br>0.5801<br>0.5110<br>**0.4110**<br>0.4868<br>**0.8326**<br>**0.4720**<br>**0.8433**<br>**0.6091**|\n|_HelpSteer2_<br>Direct Generation<br>Chain-of-Thought (CoT)<br>Role Assignment<br>RAG (sparse,_ ğ‘˜_=10)<br>MAD<br>MARS<br>**MA-SAPO (Ours)**|0.3616<br>0.4700<br>0.7723<br>0.3280<br>0.4913<br>0.4846<br>0.2981<br>0.3958<br>0.7169<br>0.2888<br>0.4709<br>0.4341<br>0.4400<br>0.5175<br>0.8221<br>0.4025<br>0.5992<br>0.5563<br>0.4903<br>0.5745<br>**0.8642**<br>0.4161<br>0.6567<br>0.6003<br>0.4167<br>0.5049<br>0.8067<br>0.4084<br>0.6708<br>0.5615<br>0.4791<br>0.5569<br>0.8268<br>0.4095<br>0.6234<br>0.5791<br>**0.5072**<br>**0.6038**<br>0.8527<br>**0.5244**<br>**0.7570**<br>**0.6490**|0.2616<br>0.3636<br>0.7078<br>0.2942<br>0.4421<br>0.4139<br>0.1600<br>0.2545<br>0.6291<br>0.2421<br>0.3812<br>0.3334<br>0.2555<br>0.3303<br>0.7050<br>0.3267<br>0.4441<br>0.4123<br>0.3990<br>0.4711<br>0.7989<br>0.3722<br>0.5814<br>0.5245<br>0.3971<br>0.4532<br>0.7898<br>0.3998<br>0.6439<br>0.5368<br>**0.4296**<br>**0.5019**<br>0.7994<br>0.3957<br>0.6181<br>0.5482<br>0.4005<br>0.4754<br>**0.8294**<br>**0.4833**<br>**0.8441**<br>**0.6065**|",
"|Dataset Methods|GPT-4o<br>Help Corr Coh Comp Verb Avg|LLaMA-3-8B<br>Help Corr Coh Comp Verb Avg|\n|---|---|---|\n|_HelpSteer1_<br>_ğ‘˜_= 1<br>_ğ‘˜_= 2<br>_ğ‘˜_= 4<br>Test Agents Combination<br>**MA-SAPO (k=3)**|0.5182<br>0.6247<br>0.8543<br>0.4943<br>0.7349<br>0.6453<br>**0.5211**<br>**0.6287**<br>0.8591<br>0.4960<br>0.7318<br>0.6473<br>0.5204<br>0.6266<br>0.8606<br>0.4965<br>**0.7369**<br>0.6482<br>0.4672<br>0.5415<br>**0.8619**<br>0.3670<br>0.6403<br>0.5756<br>0.5183<br>0.6260<br>**0.8614**<br>**0.5013**<br>0.7363<br>**0.6486**|0.4045<br>0.4769<br>0.8268<br>**0.4729**<br>**0.8446**<br>0.6051<br>0.4002<br>0.4747<br>0.8236<br>0.4726<br>0.8384<br>0.6019<br>0.4028<br>0.4778<br>0.8272<br>0.4708<br>0.8389<br>0.6035<br>**0.4649**<br>**0.5203**<br>0.8323<br>0.4363<br>0.7249<br>0.5957<br>**0.4110**<br>**0.4868**<br>**0.8326**<br>0.4720<br>0.8433<br>**0.6091**|\n|_HelpSteer2_<br>_ğ‘˜_= 1<br>_ğ‘˜_= 2<br>_ğ‘˜_= 4<br>Test Agents Combination<br>**MA-SAPO (k=3)**|0.5058<br>0.5982<br>0.8458<br>0.5113<br>0.7402<br>0.6403<br>**0.5107**<br>**0.6044**<br>0.8504<br>0.5158<br>0.7450<br>0.6452<br>0.5066<br>0.6012<br>0.8480<br>0.5202<br>0.7451<br>0.6442<br>**0.5404**<br>0.6012<br>**0.8891**<br>0.4455<br>0.7398<br>0.6432<br>0.5072<br>0.6038<br>**0.8527**<br>**0.5244**<br>**0.7570**<br>**0.6490**|0.3910<br>0.4616<br>0.8215<br>0.4784<br>0.8420<br>0.5989<br>0.3980<br>0.4713<br>0.8268<br>**0.4845**<br>0.8396<br>0.6041<br>0.3947<br>0.4685<br>0.8251<br>0.4786<br>0.8395<br>0.6013<br>**0.4379**<br>0.4720<br>0.7968<br>**0.4918**<br>0.7321<br>0.5861<br>**0.4005**<br>**0.4754**<br>**0.8294**<br>0.4833<br>**0.8441**<br>**0.6065**|",
"|Method Usefulness Accuracy Consistency|Mean|\n|---|---|\n|Single-Agent<br>3.64<br>3.63<br>3.81<br>**MA-SAPO** (Ours)<br>**3.89**âˆ—<br>**3.87**âˆ—<br>**4.02**|3.69<br>**3.93**|\n|Î”** (Improvement)**<br>+0.25<br>+0.24<br>+0.21|+0.23|",
"|SD|= Â±0.74|Col3|Col4|Col5|21|Col7|1|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n||5|5|7|7||||||||\n|1|1|||||||||||",
"|Col1|Retrieved Reasoning Asset 1|\n|---|---|\n|** Retrieved Prompt 1:**what are of scenarios and<br>prospect of applying ChatGPT in language<br>learning and teaching?...<br>|** Retrieved Prompt 1:**what are of scenarios and<br>prospect of applying ChatGPT in language<br>learning and teaching?...<br>|\n|** Retrieved Response 1:**ChatGPT is a large<br>language model (LLM) developed by OpenAI that<br>can assist teachers in a variety of ways due to...|** Retrieved Response 1:**ChatGPT is a large<br>language model (LLM) developed by OpenAI that<br>can assist teachers in a variety of ways due to...|\n|**\"helpfulness\":3,\"correctness\":3,\"coherence\":3,**<br>**\"complexity\":2, \"verbosity\": 2**|**\"helpfulness\":3,\"correctness\":3,\"coherence\":3,**<br>**\"complexity\":2, \"verbosity\": 2**|\n|** Metirc Explainer Agent:**The response earned a<br>moderate helpfulness score because it outlines<br>general ways ChatGPT can support cultural<br>immersion yetstops short of concrete use cases or<br>step-by-step guidance...|** Metirc Explainer Agent:**The response earned a<br>moderate helpfulness score because it outlines<br>general ways ChatGPT can support cultural<br>immersion yetstops short of concrete use cases or<br>step-by-step guidance...|\n|** Diagnostician Agent:**The responseâ€™s main<br>strength lies in its helpful breadthâ€”itenumerates<br>five credible ways ChatGPT can support cultural<br>immersion...|** Diagnostician Agent:**The responseâ€™s main<br>strength lies in its helpful breadthâ€”itenumerates<br>five credible ways ChatGPT can support cultural<br>immersion...|\n|** Action Synthesizer Agent:**Begin with a brief<br>disclaimer about possible mistranslations, then<br>reorganize the answer into a clearthree-stage<br>immersion frameworkâ€”(1) orientation,(2)- active<br>engagement...|** Action Synthesizer Agent:**Begin with a brief<br>disclaimer about possible mistranslations, then<br>reorganize the answer into a clearthree-stage<br>immersion frameworkâ€”(1) orientation,(2)- active<br>engagement...|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2510.16635v1.pdf"
}