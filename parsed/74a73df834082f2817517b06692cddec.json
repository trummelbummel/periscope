{
"text": "Published in Transactions on Machine Learning Research (02/2025)\n\n\n     QPO: Query-dependent Prompt Optimization via\n       Multi-Loop Offline Reinforcement Learning\n\n\n           Yilun Kong1‚àó, Hangyu Mao2‚Ä†, Qi Zhao1, Bin Zhang34, Jingqing Ruan34, Li Shen5,\n          Yongzhe Chang1, Xueqian Wang1‚Ä†, Rui Zhao2, Dacheng Tao6\n\n              1Tsinghua University; 2SenseTime Research;  3Institute of automation,Chinese academy of science;\n              4School of Artificial Intelligence,University of Chinese Academy of Sciences; 5Sun Yat-Sen University;\n            6Nanyang Technological University\n\n           Reviewed on OpenReview: https: // openreview. net/ forum? id= bqMJToTkvT\n2025\nMay                                          Abstract\n                 Prompt engineering has demonstrated remarkable success in enhancing the performance of\n30                 large language models (LLMs) across diverse tasks.  However, most existing prompt op-                      timization methods only focus on the task-level performance, overlooking the importance\n                        of query-preferred prompts, which leads to suboptimal performances.  Additionally, these\n                  methods rely heavily on frequent interactions with LLMs to obtain feedback for guiding the\n                     optimization process, incurring substantial redundant interaction costs. In this paper, we in-\n                     troduce Query-dependent Prompt Optimization (QPO), which leverages multi-loop offline[cs.AI]               reinforcement learning to iteratively fine-tune a small pretrained language model to generate\n                    optimal prompts tailored to the input queries, thus significantly improving the prompting ef-\n                         fect on the large target LLM. We derive insights from offline prompting demonstration data,\n                   which already exists in large quantities as a by-product of benchmarking diverse prompts on\n                    open-sourced tasks, thereby circumventing the expenses of online interactions. Furthermore,\n                 we continuously augment the offline dataset with the generated prompts in each loop, as\n                     the prompts from the fine-tuned model are supposed to outperform the source prompts in\n                     the original dataset. These iterative loops bootstrap the model towards generating optimal\n                    prompts. Experiments on various LLM scales and diverse NLP and math tasks demonstrate\n                     the efficacy and cost-efficiency of our method in both zero-shot and few-shot scenarios.\n\n\n\n         1  IntroductionarXiv:2408.10504v2       Large Language Models (LLMs) have exhibited impressive prowess in various domains of natural language\n              processing (NLP) (Ouyang et al., 2022; Touvron et al., 2023; Achiam et al., 2023). Prompt engineering, a\n           method that simply adds an instruction to the input query, emerges as a lightweight and promising solution\n               for adapting LLMs to downstream tasks without the need for parameter tuning (Liu et al., 2023a; Ajith\n               et al., 2023).  Since the performance of LLMs towards a particular task is significantly influenced by the\n              quality of the prompt, the key challenge of prompting lies in how to design the optimal prompts.\n\n           Numerous prompt engineering algorithms have been proposed in recent years. Some algorithms (Zhou et al.,\n             2022; Wang et al., 2023b; Guo et al., 2023; Wang et al., 2024b) leverage LLMs as a prompt optimizer, em-\n              ploying black-box optimization to derive the best prompts. Others utilize reinforcement learning (RL) (Deng\n               et al., 2022; Zhang et al., 2022; Kong et al., 2024) to train a policy model to generate the optimal prompts.\n             Despite these advances, prompt engineering still grapples with great challenges:\n\n\n               ‚àóWork done during an internship at SenseTime Research.\n                  ‚Ä†corresponding authors: Hangyu Mao<maohangyu@sensetime.com>; Xueqian Wang<wang.xq@sz.tsinghua.edu.cn>\n\n\n                                                           1\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\n                          Zero-Shot Accuracy                                    Few-Shot Accuracy\n\n                              AG News\n\n                                         79.079.0                                         AG News                          BoolQ                QPO\n                                                                                                         85.685.6                   76.5\n              SVAMP                                     BoolQ                                                                                       Prompt-OIRL\n                         86.8                             71.0\n                                                                                                                                             APE\n\n                                                                                                                                                               ChatGPT-Rewrite\n\n\n\n\n                  90.3                                          91.2                          71.0                                          91.4\n       GSM8K                                              IMDB       HellaSwag                                              IMDB\n\n\n\n\n\n                         69.7                             68.7\n\n                                                                                                         51.2                   69.1\n              HellaSwag                 45.3                 Tweet Emotion\n\n                                                                                CosmosQA                           Tweet Emotion\n\n                                CosmosQA\n\n\n\n\nFigure 1: QPO delivers state-of-the-art performance across a wide variety of tasks, superior to existing query-\ndependent and query-agnostic prompt optimization methods.\n\n  (1) Most of previous algorithms only focus on obtaining task-level optimal prompts, aiming to attain an\n      optimal average performance across all queries within a specific task.  However, they overlook the\n       critical insight that no single prompt can be ideal for every possible query (Sun et al., 2023).\n\n  (2) All the aforementioned methods require frequent interactions with the target LLMs to obtain feedback\n     on the current prompts to guide optimization, incurring significant costs due to the high expense of\n       inferences with target LLMs.\n\nIn fact, query-level optimal prompts can yield better performance (Zhang et al., 2022). Meanwhile, offline\noptimization can serve as a promising method for reducing the costs associated with LLM interactions based\non the existence of a wealth of prompt optimization datasets, which can be easily available as by-products\nduring the evaluation of existing prompting strategies. However, research on the two important domains\nremains critically scarce.\n\nTo tackle the above challenges, we propose QPO, a Query-dependent Prompt Optimization method through\nmulti-loop offline reinforcement learning. The overall process of our framework is illustrated in Figure 2.\nFirstly, we employ offline RL for the initial loop to fine-tune a small-scale pretrained language model (PLM)\nas a policy model on the pre-collected dataset, enabling it to generate specific prompts based on input\nqueries and given rewards for the target LLM. Subsequently, we utilize the trained policy model to efficiently\nexplore new queries and prompts to augment the dataset, as the generated novel prompts are supposed to\nbe more suitable for the current queries than other prompts in the dataset. This process eliminates the\nnecessity to evaluate all collected prompts for each query, while suffices to assess only a few query-specific\nhigh-quality prompts, significantly reducing the required interactions with the target LLM. The augmented\ndataset, enriched with more queries and superior prompts, can further train the policy model and enhance\nits performance. Consequently, we establish the iterative process wherein the policy model, fine-tuned via\noffline reinforcement learning, efficiently generates improved data, which is then utilized to further fine-tune\nthe model, creating a cyclical feedback loop. This bootstrapping learning process can achieve substantial\nimprovements in limited loops with minimal interactions with the LLMs.\n\nWe evaluate our method across different LLM scales on various neural language understanding and math\nreasoning tasks using both zero-shot and few-shot settings. As shown in Figure 1, QPO reaches state-of-\nthe-art performance, superior to both query-dependent and query-agnostic prompting methods. Compared\nto online optimization approaches, our method only needs a minimal number of interactions with the target\nLLM to augment the dataset after each loop of offline optimization, which strikes a great balance between\nperformance and LLM interaction costs. In addition, we perform extensive ablations on different aspects\n\n\n                                              2\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\n                      Step0: Dataset Construction                                             Metric of\n                                                           Target LLM                         Target Task                      √ó ùëÅùëÄ\n                          Query 1           Prompt 1                            Output 1                              Score 1\n             ‚Ä¶       ‚Ä¶              ‚Ä¶               ‚Ä¶\n                                                                                                                                            {Query ùëñ, Prompt ùëó, Score ùëñùëó}\n                          Query ùëÅ          Prompt ùëÄ                          Output ùëÅùëÄ                            Score ùëÅùëÄ\n\n\n\n                                                              Dataset\n\n\n                      Step1: Training Policy Model with Offline RL\n\n     The Train set               {Reward ùëñùëó, Query ùëñ}                                                          { Predicted Prompt ·àòùëó}      Loss            {Prompt ùëó}\n      of Target Task.\n\n                          Reward Design           PLM as Policy Model\n\n\n                      Step2: Dataset Augmentation\n\n                                                                                                      Target LLM   Metric             √ó ùëÅ‚Ä≤\n                          Query 1‚Ä≤                                             Novel Prompt 1‚Ä≤\n             ‚Ä¶           Maximum                                      ‚Ä¶                   +                                           Reward                                                                                                                                            {Query ùëñ‚Ä≤, Prompt ùëñ‚Ä≤, Score ùëñ‚Ä≤}\n                          Query ùëÅ‚Ä≤                                             Novel Prompt ùëÅ‚Ä≤\n\n\n\n             Test Queries:                                                 Query-level optimal prompts:\n                                       Maximum\n                     [Question]: is house tax and property tax are same   Reward                             Evaluate whether the answer aligns with the information\n                     [Passage]: Property tax or ‚Äòhouse tax‚Äô is ‚Ä¶‚Ä¶                                                presented in the text passage..\n\n                     [Question]:                                            is barq‚Äôs                                       root beer                                           a\n                     [Passage]: Barq‚Äôs                                        /‚Äôb…ëÀêrks/                                                                is an                                ‚Ä¶‚Ä¶                                                 pepsi product               The well-trained  Determinethe providedif thetext.,statementdeterminecorrespondsif so, determineto thethedetailsanswer.in        Target      Task-level\n                                                                      Policy Model                                       LLM     Performance\n\n\nFigure 2: The overview of our framework. We aim at training a small pretrained language model (PLM) as the\npolicy model, which can generate query-level optimal prompts based on the given queries, and ultimately improve\nthe target LLM‚Äôs task-level performance. In the training phase, depicted by the blue frame above, the blue arrows\nrepresent our primary iterative process, while the gray arrows indicate the initial steps.  Specifically, in Step0, the\noffline dataset is constructed as a by-product of evaluating existing prompts. The below frame with gray background\nindicates the testing phase.\n\nof the proposed algorithm. The policy model demonstrates great cross-model generalization ability, further\nenhancing the value of our approach. In conclusion, our contributions are three-fold:\n\n   ‚Ä¢ We identify the overlooked query-dependent prompt optimization problem, and introduce an Offline\n     RL paradigm to fine-tune a PLM to generate query-specific optimal prompts.\n\n   ‚Ä¢ We propose a comprehensive framework, QPO, which leverages the existence of offline datasets and\n       employs a novel Multi-Loop Augmentation technique to facilitate data augmentation with minimal\n         interactions, and ultimately bootstrap the performance of prompt optimization through offline RL\n         fine-tuning.\n\n   ‚Ä¢ We benchmark our method across various LLMs on extensive datasets, demonstrating its superiority\n       and great potential in broader applications.\n\n\n2  Methodology\n\nTo automatically generate query-level optimal prompts in a cost-efficiency method, we propose QPO, a\nnovel query-dependent prompt optimization framework. The overall process is shown in Figure 2. In the\nfirst round, QPO harnesses the readily existing datasets to fine-tune a pre-trained language model into a\npolicy model through offline reinforcement learning, enabling it to generate query-specific optimal prompts\nbased on input queries.  Subsequently, this policy model efficiently explores a broader query space and\nenriches the prompt space with more diverse and high-quality prompts, both of which contribute to the\naugmentation of the dataset. The augmented dataset, in turn, further refines the policy model for the\nnext loop. Consequently, through multi-loop training and data augmentation, the performance of prompt\noptimization is bootstrapped for improvement.\n\n\n                                              3\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\n2.1  Problem Formulation\n\nThis study focuses on training the policy model to generate prompts, which can instruct a target LLM to\noutput expected answers that fulfill the query-dependent objective, based on the given queries.\n\nQuery and Answer. We consider the task of answering queries q ‚ààQ = V‚àûexpressed in a natural\nlanguage, where V denotes the vocabulary. Each query q is supposed to have an expected answer y‚àó‚ààY\nas ground truth. Queries annotated with ground-truth answers can be employed as demonstrations d for\nfew-shot scenario.\n\nPrompt and Policy Model. The performance of an LLM can be significantly enhanced through appro-\npriate prompts. The prompt p ‚ààP = V‚àûis a natural language instruction that explains to the LLM how\nto complete the query and output the predicted answer. In this paper, we utilize a fine-tuned policy model\nœÄ : Q ‚ÜíP to generate query-specific prompts based on the given queries: ÀÜp = œÄ(q).\n\nTarget LLM. These tasks are performed by employing a target LLM ‚Ñì: Q ‚ÜíY, feeding queries q into\nthe LLM to get answers. With prompts, the answers can be obtained by ÀÜy(i,j,k) = ‚Ñì(p(j), d(k), q(i)), where\nq(i), p(j), d(k) denote the i-th query, j-th prompt, and k-th combination of demonstrations, respectively. For\nzero-shot setting, d is null, so we use ÀÜy(i,j) as a shorthand unless otherwise specified.\n\nQuery-dependent Objective. Given a task with queries and ground-truth answers, the quality of the\npredicted answers can be evaluated by a metric œÅ(y‚àó, ÀÜy), such as correctness.  The objective of query-\ndependent prompt optimization can be formulated as below:\n                    œÄ‚àó= arg max œÅ(y‚àó(i), ‚Ñì(œÄ(q(i)), q(i)))i‚àà[N],                               (1)\n                                         œÄ\n which aims to optimize the policy model to generate a query-specific prompt that enhance the target LLM‚Äôs\nperformance on this particular query. As its performance improves across each single query, the overall task\nperformance will consequently improve.\n\n2.2  Model Query-dependent Prompt Optimization as Offline RL\n\nReinforcement Learning Formulation. As a text generation problem, prompt generation can be formu-\nlated as a Markov Decision Process (MDP) ‚ü®S, A, r, f‚ü©with a finite state space S, action space A, reward\nfunction r, and state-transition probability function f, where a sparse reward can be obtained only after the\ncomplete prompt is generated.  Instead of dedicatedly designing intricate proxy rewards, we formulate the\nMDP as a single-step decision-making process. In offline RL (Levine et al., 2020), given the dataset:\n                 D = {q(i), p(j), r(i,j) = R(q(i), p(j))}i‚àà[N],j‚àà[M],\n\nwhere the initial state q ‚ààS is the input query with n tokens q = (q1, ..., qn) and the corresponding action\np ‚ààA represents the generated prompt with m tokens p = (p1, ..., pm), where each token q, p is from the\nvocabulary V. In a single-step decision episode, the policy model generates a complete prompt based on the\ngiven expected reward and query p ‚àºœÄ(p|r, q). The reward guides the quality of the generated prompt. To\nachieve this, we adopt Decision Transformer (DT) (Chen et al., 2021) approach to fine-tune the policy model,\nwhich is aligned with the autoregressive predicting paradigm of language models. For further discussion on\nwhy prompt generation is formulated as a single-step decision process, please refer to Appendix A.2.\n\nReward Design. In our single-step prompt generation process, the reward plays a crucial role in aligning\nwith the true capabilities of the prompt. In this paper, we mainly focus on neural language understanding\ntasks and math reasoning tasks rather than generative tasks, which are challenging to assess objectively\nbecause of the poor correlation between the evaluation metrics and human preferences (Liang et al., 2022;\nGoyal et al., 2022). We design the reward based on two dimensions: query-level and task-level. Query-level\nreward measures whether the prompt can instruct LLM to answer the specific question correctly, while task-\nlevel reward measures the prompt‚Äôs average prompting ability for all queries in one task. In particular, we\nadopt the average accuracy of the prompt for all test questions as the task-level reward for both zero-shot\nand few-shot settings,\n                                      N\n                                              1\n                                  Rtask(q(i), p(j)) = X 1{ÀÜy(i,j) = y‚àó(i)}.                               (2)\n                            N\n                                                       i=1\n\n\n                                              4\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\nFor query-level rewards, the design differs between zero-shot and few-shot scenarios due to the variations\nin evaluating approaches. For zero-shot evaluation, we utilize the prediction correctness as a coarse-grained\nmeasure for the prompt-query pair and additionally introduce the output perplexity (PPL) as a fine-grained\npenalty, which can be easily calculated by the Log-likelihood from the black-box API or directly obtained\nthrough the open-source LLM. During few-shot evaluation, the result of a query is obtained by voting or\naveraging across K different in-context demonstration combinations to eliminate the inherent instability\nand randomness of few-shot setup (Gao et al., 2020; Ajith et al., 2023). We use the average correctness\nacross demonstration combinations of the prompt-query pair as the fine-grained query-level reward in few-\nshot setting, which is more stable than PPL. As the query-level rewards are supposed to accurately reflect\nthe distinctions in prompting effectiveness for various prompts across different queries, both the perplexity\npenalty in zero-shot setting and the averaged correctness in few-shot setting, which are decimals rather\nthan binary integers, exhibits greater variability across different query-prompt pairs compared to the binary\ncorrectness, thus enable a more fine-grained alignment:\n\n\n                   Ô£±                                                    1{ÀÜy(i,j) = y‚àó(i)} ‚àí1              Zero-Shot,                                                               10PPL(ÀÜy(i,j)),                   Ô£¥Ô£¥Ô£¥Ô£≤                    Rquery(q(i), p(j)) =    K                                                           (3)                                     1\n             X 1{ÀÜy(i,j,k) = y‚àó(i)},          Few-Shot.\n                      K                   Ô£¥Ô£¥Ô£¥Ô£≥                                        k=1\n\nThe overall reward is the sum of the query-level and task-level reward. Due to significant differences in\nreward scales across different tasks, we normalize the overall reward using the Min-Max Normalization\nmethod (Patro, 2015) for convenience, mapping it to the range of 0-100.\n\n                            R(q(i), p(j)) = Rquery(q(i), p(j)) + Rtask(q(i), p(j)).                          (4)\n\n\nTraining Objective. To enable the model to output appropriate prompts based on expected rewards and\ninput questions, we maximize the log-likelihood with teacher forcing through DT training paradigm:\n\n                                 Lprompt = ‚àíE(q,p,r)‚àºD log p(p|r, q),                                  (5)\n\nwhere we input reward as return-to-go and the complete query as state, and allow the policy model to\nautoregressively predict the entire prompt‚Äôs tokens as an action.\n\nTo further enhance the model to distinguish the prompting abilities of different prompts for the various\nqueries, we introduce the reward prediction loss. We directly predict the expected reward based on the\noutput of the transformer at the first token, regarding the model as an autoencoder (Liou et al., 2014)\nby encoding an reconstructing the input reward.  This approach can extract more information from the\ngiven reward, impose constraints on the model weights, and significantly reduce training difficulty compared\nto predict reward based on the output prompt at the last token.  Further discussion on implementing\nautoencoder functionality is illustrated in Appendix A.4.\n\n                                  Lr = E(q,p,r)‚àºD(ÀÜr ‚àír‚àó)2                                        (6)\n\nWe define the overall objective function by combining the above objectives with a balancing hyperparameter:\n\n                            L = Lprompt + ŒªLr                                           (7)\n\n\nModel Architecture. To achieve the aforementioned functionality, enabling the model to fully leverage\nreward information and temporal dynamics in reinforcement learning, we also refine the architecture of\nthe pre-trained language model, elevating it to serve as our policy model. We introduce a distinct reward\nembedding layer, separate from the natural language token embeddings, to facilitate a deeper comprehension\nof reward signals by the model. Furthermore, during training, we incorporate an additional reward prediction\nlayer to implement the reward prediction loss, thereby imposing constraints on the optimization of the\nmodel‚Äôs primary network. The combined enhancements in both algorithm and model structure culminate in\nthe successful implementation of offline prompt optimization.\n\n\n                                              5\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\n2.3 QPO Framework for Multi-Loop Augmentation\n\n2.3.1   Initial Demonstration Construction\n\nDataset Construction. We start by emphasizing the presence and significance of prompt demonstrations\ngenerated by previous research of prompt optimization.  In the domain of prompt engineering, abundant\nprompts have been proposed to improve the LLMs‚Äô performance on downstream tasks.  Generally, their\nefficacy is assessed at the task level.  However, different queries exhibit varied preferences for different\nprompts. Query-level score, obtained before calculating task-level score but ignored by previous research,\ncan play a great role in teaching the query-dependent prompting ability. As the prompts have been evaluated\non open-sourced datasets, the following query-level demonstrations can be constructed as by-products. The\nprocess for constructing the offline dataset is as follows: first, we sample queries from the task dataset, then\ncombine them with the collected prompts and input the query-prompt pairs into the target LLM to obtain\nanswers. Through the designed reward function, we calculate the reward value for each query-prompt pair.\nThese query-prompt-reward triplets form the samples in our offline dataset:\n\n                D = {q(i), p(j), y‚àó(i), ÀÜy(i,j), R(y‚àó(i), ÀÜy(i,j))}i‚àà[N],j‚àà[M],\n\nwhere M, N denote the number of collected prompts and queries, respectively. To enrich the initial explo-\nration of the prompt space and simplify prompt acquisition, we leverage ChatGPT-3.5 to rewrite prompts,\nresulting in a diverse vocabulary and yielding a substantial number of prompts in our dataset. Details of\nrewriting prompts are demonstrated in Appendix B.2. While as each query requires testing with all prompts,\nwe only utilize a small collection of queries as the initial dataset to strike a balance between algorithm per-\nformance and computational cost.\n\nData Filtering. Similar to popular offline reinforcement learning‚Äôs need to differentiate expert, medium,\nand random datasets, in this work, we simply remove the less-quality examples with rewards below the\nexpert threshold, defined as the 66.7th percentile of the reward range in the dataset. Notably, this approach\ndoes not discard two-thirds of the data since we use rewards as the filtering metric rather than data volume,\nand most samples fall within the expert interval. Almost all prompts are retained without wastage; even if a\nprompt performs poorly on average, it may still enable the LLM to answer correctly on specific questions, and\nit is retained at this query. The specific data wastage resulting from data filtering is detailed in Section 3.3.\n\n\n2.3.2  Multi-Loop Augmentation\n\nAfter fine-tuning with the initial dataset, the policy model is supposed to generate prompts that are better\nsuited for specific problems than any existing prompts in the dataset.  Therefore, a natural and efficient\napproach is to use these prompts to enrich the dataset as a form of data augmentation.  Moreover, as\nthe model generates high-quality prompts tailored to specific queries, there is little concern about filtering\nout evaluation samples and wasting data, which presents a great opportunity to extensively assess various\nqueries, expanding exploration of the query space.\n\nQuery Augmentation. As our initial dataset covers only a limited number of queries, we randomly collect\nboth overlapped and uncovered queries from the task‚Äôs training set, simultaneously focusing on optimizing\nprompts on existing problems and exploring unknown ones. Since only one specifically generated prompt\nneeds to be evaluated for each problem, rather than testing all prompts, it significantly reduces computational\noverhead caused by the number of queries and candidate prompts. Hence, it enables large-scale exploration\nof new problems, avoiding the test performance degradation caused by the distribution shift of the limited\ninitial training queries.\n\nPrompt Augmentation. Based on the newly collected queries, we utilize the maximum expected reward\nto instruct the fine-tuned model to generate novel and superior prompts. In order to guarantee the quality\nand diversity of prompt exploration, we adopt the sampling generation approach with the policy model in\nthis phase.\n\nDataset Augmentation. For augmented query-prompt pairs, the calculation of the query-level rewards\nremains consistent with the original formula (3), while task-level rewards are determined by the model‚Äôs\n\n\n                                              6\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\noverall generation capability in the current loop, that is, the average accuracy of all prompts generated in\nthe current augmentation stage. Then, we obtain the new training examples:\n                         DA = {q(i‚Ä≤), p(i‚Ä≤), r(i‚Ä≤)}i‚àà[N ‚Ä≤],\nwhere N ‚Ä≤ denotes the number of new queries. This dataset is finally supplemented into the original dataset\nfor the next loop‚Äôs training.\n\nAfter data augmentation, the updated dataset is enriched with a greater variety of queries and higher-quality\nprompts, which can be utilized to fine-tune the model to further improve its performance. We structure these\nprocesses into a continuous bootstrapping loop, where both offline RL and data augmentation cyclically\nfeedback to each other. The algorithm stops when the number of iterations reaches a predefined value.\nOur method achieves incremental improvements utilizing offline optimization with significantly minimal\ninteractions with LLMs. The detailed algorithm of QPO are outlined in Algorithm 1 in Appendix A.5.\n\n3  Experiments\n\n3.1  Experimental Setup\n\nTasks. We perform experiments on 6 language understanding tasks and 2 math reasoning tasks to validate\nour methods, including topic classification (AG‚Äôs News (Zhang et al., 2015)), natural language inference\n(BoolQ (Clark et al., 2019)), sentiment classification (IMDB (Maas et al., 2011), TweetEval Emotion (Mo-\nhammad et al., 2018)), multi-choice QA (CosmosQA (Huang et al., 2019), HellaSwag (Zellers et al., 2019)),\nand math reasoning (GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021)). These tasks are widely\nstudied in prompting settings, and hence many expert-crafted and machine-generated prompts are available,\nwhich facilitates our offline data collection procedure.\n\nBaselines.  We compare our method with three types of baselines, including manual prompt engineer-\ning (PromptSource (Bach et al., 2022), Chain-of-Thought (Wei et al., 2022)), online prompt optimization\n(Low Perplexity (Gonen et al., 2022), RLPrompt (Deng et al., 2022), APE (Zhou et al., 2022)) and offline\nprompting approach (Prompt-OIRL (Sun et al., 2023)). We also compare the prompts rewritten by Chat-\nGPT. We reproduce the online methods based on InstructEval (Ajith et al., 2023). Rather than making a\ndirect while unfair comparison between the performance of QPO and newer online algorithms, our primary\nfocus is to demonstrate that QPO can further enhance performance by optimizing prompts at query-level\nbased on the prompts obtained through online optimization. Notably, for fair comparison, we use a larger\ndataset in Prompt-OIRL than ours after final loop data augmentation to eliminate our advances in additional\ninteractions.\n\nLLMs.  Our method is model-agnostic for policy model, and we use GPT-2 (Radford et al., 2019) in this\npaper, which is compact while has sufficient text generation capability. For the target LLMs, we use publicly\navailable Llama2-7b-chat (Touvron et al., 2023) for natural language understanding tasks, while for the more\nchallenging math reasoning tasks, we opt for GPT-3.5-turbo (OpenAI, 2023) and GPT-4o (OpenAI, 2024).\nMoreover, to evaluate the cross-model generalization ability of our trained policy model, we employ models\nat different abilities, scaling from the GPTNeo-1.3b (Black et al., 2021) to Llama2-13b-chat (Touvron et al.,\n2023) and Vicuna-13b (Zheng et al., 2024).\n\nImplementation Details.  For the initial data collection, we utilize 30 expert prompts obtained from\nInstructEval and 120 prompts rewritten from ChatGPT-3.5 to construct the dataset. We set the QPO with\n3 loops. For all the tasks, we use a unified hyperparameter set and do not need complex hyperparameter\ndesign for specific task. We do not cherry-pick checkpoints and directly use the final checkpoint in each\nloop for evaluation and next loop‚Äôs training. For NLU tasks, we evaluate our method on both zero-shot\nand few-shot settings, while for math reasoning tasks for GPT-3.5 and GPT-4o, we only test its zero-shot\nperformance due to budget limitation. Both training and testing are conducted on 3 seeds. We set the\nmaximum expected reward as 100 and pick the model with the highest score on the development set and\nreport its score on the test set. For data augmentation, we adopt sampling generation with a top-k of 2 and\ntop-p of 0.9, while for evaluation, we adopt greedy generation. Few-shot evaluations are conducted separately\nfor 3-shot and 6-shot, where the few-shot demonstrations are randomly sampled from the training set of the\n\n\n                                              7\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\n            Table 1: Main results (accuracy) of QPO and baselines on Llama2 on NLU tasks.\n\n\n  Method      AG News   BoolQ     IMDB      Emotion    CosmosQA  HellaSwag   Avg.\n\n                                          Zero-shot Accuracy\n\n  PromptSource    68.3         59.0         86.3         57.3         42.9         68.8         63.8\n ChatGPT         64.2(0.84)     62.0(2.41)     85.0(2.19)     62.4(2.68)     42.6(0.51)     68.8(0.51)    64.2\n Low Perplexity   70.3(1.04)     61.0(2.65)     78.7(2.21)     57.6(1.71)     42.0(0.67)     69.6(0.69)    63.2\n RLPrompt        47.8(4.67)     64.9(2.27)     86.4(2.04)     49.8(1.17)     44.7(0.33)     68.6(0.19)    60.4\n APE              66.5(1.12)     64.4(1.50)     77.0(3.84)     58.3(2.73)     43.2(1.17)     68.3(0.84)    62.9\n  Prompt-OIRL    78.3         65.0         89.0         68.3         42.3         68.3         68.5\n QPO             79.0(0.44)    71.0(1.02)    91.2(2.12)    68.7(1.07)    45.3(0.42)    69.7(0.71)   70.9\n\n                                        Few-shot Accuracy\n\n  PromptSource    83.3         67.1         89.4         66.4         45.6         70.3         70.4\n ChatGPT         83.8(0.75)     68.1(0.39)     89.2(0.25)     67.2(1.78)     45.5(0.62)     69.8(0.52)    70.6\n Low Perplexity   82.9(0.20)     68.5(0.47)     89.1(0.09)     67.6(1.49)     45.4(0.41)     70.4(0.13)    70.7\n RLPrompt        82.8(0.24)     71.2(0.50)     89.1(0.54)     65.4(0.70)     46.2(0.32)     70.2(0.57)    70.8\n APE              83.1(0.33)     68.4(0.79)     89.2(0.45)     67.9(0.58)     45.8(0.70)     70.2(0.12)    70.8\n  Prompt-OIRL    83.8         73.3         89.5         67.6         48.1         70.4         72.1\n QPO             85.6(0.29)    76.5(0.62)    91.4(0.16)    69.1(1.13)    51.2(0.41)    71.0(0.29)   74.2\n\n\n   Table 2: Main results (accuracy) of QPO and baselines on GPT-3.5 on math reasoning (MR) tasks.\n\n\n                Method       GSM8K       SVAMP          Avg.\n\n              CoT                88.0(0.49)           81.0(0.52)          84.5\n              ChatGPT          82.1(1.07)           79.5(2.24)          80.8\n            APE                85.8(0.74)           81.3(1.62)          83.6\n                Prompt-OIRL     87.5               83.7               85.6\n            QPO              90.3(0.68)          86.8(0.41)        88.6\n\n\n\ntasks. We measure the algorithm‚Äôs performance using accuracy, defined as the ratio of correctly answered\nqueries by the LLM to the total number of tested queries. More details are demonstrated in Appendix B.1.\nThe code is available at here.\n\n\n3.2  Main Results\n\nNatural Understanding tasks. Table 1 presents a comprehensive comparison of query-dependent opti-\nmized prompts generated by QPO against human prompts, query-agnostic prompt optimization methods,\nand SOTA offline query-dependent prompt selection method on Llama2-7b-chat on zero-shot and 6-shot set-\ntings. The results show that: (1) QPO delivers significantly better results on zero-shot metric (avg. +7.2%\nover other baselines) and also outperforms other baselines on few-shot setting (avg. +3.3%), where the con-\ntextual demonstrations slightly diminish the critical role of prompts, as evidenced by the standard deviation\nin few-shot scenarios being significantly smaller than in zero-shot scenarios. The 3-shot results in Table 11\nfurther illustrate that our algorithm‚Äôs advantage becomes more pronounced as the information provided by\ncontextual demonstrations decreases (avg. +6.1%). (2) Based on the dataset constructed from these online\nalgorithms, both QPO and Prompt-OIRL exhibit better performance to those optimized at the task level on\nall three settings (avg. +7.2%, +2.3%, +4.9%, respectively), which underscores the effectiveness of optimiz-\ning prompts at the query level. (3) Despite the comparative strength of Prompt-OIRL as a discriminative\nmodel over our generative model, our algorithm outperforms Prompt-OIRL, demonstrating that learning to\ngenerate optimal prompts for specific queries is superior to merely selecting the most suitable ones.\n\n\n                                              8\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\nMath Reasoning tasks. The experimental results of GPT-3.5 on zero-shot math reasoning tasks are\npresented in Table 2. When applied to more powerful models for solving complex tasks, QPO exhibits the\nsame leading capabilities.  Specifically, QPO achieves up to 7.8% improvement with an average of 5.0%\nover other baselines, which demonstrates that QPO is a good method for prompting fancy LLMs for these\nchallenging tasks. The results of GPT-4o are shown in Appendix C.2.\n\n\n3.3  Analysis\n\nMulti-loop augmentation  can  bootstrap  to\nimprove performance  in a  interaction-cost-\nefficient way. Multi-Loop Augmentation (MLA) in\nour method is supposed to improve the exploration of\nthe query space and prompt space efficiently, so that\nthe augmented dataset can enhance the policy model\nin turn. The average number of queries in datasets\nincreases from 283 in original dataset to 673 in the\nfinal augmented dataset, and the average number of\nprompts rises from 150 to 829, which obtains 138%\nand 453% increases in question coverage and prompt\ndiversity with only a 17.1% increase in total data vol-\nume. We limit the amount of data augmented in each\nloop, as research (Wang et al., 2024c) indicates that\ngenerated data comprising 10% of the real data pro-\nvides the maximum training benefit, whereas exces-                                                     Figure 3: Comparison of QPO and QPO w/o MLA.\nsive generated data can be counterproductive. To fur-\nther quantify the effect of MLA, we conduct ablation\nexperiments that finetune the model the same iterations without data augmentation. As shown in Figure 3,\nin 3 NLU tasks (AG News, BoolQ, IMDB) and 2 math reasoning (MR) tasks (GSM8K and SVAMP), MLA\ncan consistently improve the performance in all settings, while training with more epochs without\ndata augmentation results in significant drops due to overfitting. To be specific, multi-loop augmentation\nand training lead to a performance improvement of 6.7% and 2.3% for Llama2 in zero-shot and few-shot set-\ntings in NLU tasks, respectively, while in the zero-shot math reasoning tasks for GPT-3.5, the performance\nimproved by 3.5%.\n\nDespite the introduction of online LLM interactions in MLA, our                                                             Table 3: Interaction costs of QPO and APE\nmethod:  (1) only engages new interactions to supplement the  on Llama2-7b-chat on AG News.\ndataset after the training convergence in each loop, rather than\nrequiring interactions at each step to provide real-time guidance                                                     Method  Model Cost (GPU Hour)\nfor optimization direction; (2) requires significantly fewer inter-\n                                           APE              1.761actions compared to other online methods.  Therefore, QPO is\n                                                          Ours              0.291considered an offline approach. To demonstrate this, we com-\npared our method with the classic online algorithm APE, which\nis a paradigm for many online prompt optimization methods.  Table 3 presents the LLM inference time\ncosts, which is operated locally on NVIDIA V100 GPU. To be specific, for 4 loops of training, 3 data aug-\nmentation processes are employed, resulting in each MLA requiring approximately 0.097h of computational\nresources. Given an average performance improvement of around 2% per loop, this expense\nfor interaction is relatively justified and valuable. Thus, employing QPO for prompt optimization is\nsubstantially more cost-efficient than methods reliant on LLMs as critics. We analyse the specific number of\nrequired interactions for these methods in Appendix C.3.\n\nReward Matters. To validate the effects of RL and our proposed reward prediction loss, we conduct\nablation experiments comparing supervised fine-tuning (SFT), RL, and QPO (RL+Reward Loss). Table 4\npresents that RL+Reward Loss consistently surpasses others in all evaluation tasks, which confirms\nthe effects of the additional reward loss. It instruct the model focus more on the rewards, enabling it to de-\ntermine which query-prompt pairs can achieve higher rewards, allowing the policy model to generate optimal\n\n\n                                              9\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n                 Table 4: Comparison between SFT, RL and QPO (RL+Reward Loss).\n\n\n           AG News            BoolQ           IMDB                Avg.\n   Method\n              zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot\n\n   SFT      76.0        84.2       70.7        74.9       90.7        89.6       79.1        82.9\n  RL       78.7        84.3       65.0        75.2       91.0        89.6       78.2        83.1\n  QPO     79.0      85.6     71.0      76.5     91.2      91.4     80.4      84.5\n\n\nprompts given the maximum expected reward. Compared with SFT, RL also achieves better performance\n(5/6) because of the high data utilization, which is also thanks to the introduction of the reward. Detailed\nreason is analysed in Appendix A.1. The reason for the drop between QPO and RL can be attributed to\nthe underestimation of rewards. As the reward and each query token have equal weight contributing to the\nprompt generation, excessively long queries may cause the model to overlook the single-token reward. No-\ntably, SFT also yields strong results, indicating that fine-tuning PLMs to generate optimal prompts can serve\nas a general paradigm, regardless of the specific fine-tuning method. The success of SFT also implies that\neach query does not necessarily need multiple prompts in the dataset; as our data augmentation approach, to\nbalance interaction frequency with query exploration, each new query might only be accessed minimal times,\nresulting in a limited number of different corresponding prompts generated by sampling. Such augmented\ndata can still positively contribute to the training process. For more details on ablation experiments related\nto reward design, including the contributions of Rquery and Rtask, the importance of fine-grained perplexity\npenalty in Rquery, and the difference between reward predicting positions, please refer to Appendix C.4.\n\n                Table 5: Performance comparison between Nearest Neighbor and QPO.\n\n\n            AG News            BoolQ           IMDB                Avg.\n   Method\n               zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot\n\n  NN       70.3        82.4       64.3        76.7       83.7        89.1       72.8        82.7\n  QPO     79.0      85.6     71.0      76.5     91.2      91.4     80.4      84.5\n\n\nAblation on Prompt Generation. To further validate the rationality of using transformer for query\nencoding and prompt generation simultaneously, we additionally employ a direct query-dependent baseline\nfor comparison.  Specifically, we ablate the text generation component of the policy model, only utilize\nit to encode queries into query embeddings, select the training query most similar to the testing query\nusing Nearest Neighbor (NN) criterion, and directly choose the prompt with the highest reward associated\nwith the matched training query. The results are depicted in Table 5. Generating specific prompts (QPO)\nsignificantly outperforms merely selecting suitable ones (NN), indicating the effectiveness of the main training\nobjective. Notably, NN also achieves higher scores than those query-agnostic prompts shown in Table 1, which\nillustrates that the trained policy model can accurately obtain the input query embeddings, even without the\nspecific training objective for encoding. And query-specific prompts, whether selected or generated, indeed\noutperform query-agnostic prompts.\n\nAblation on Dataset. We investigate the effect of initial  Table 6: Performance under different prompt\nprompt quantity and overall data quality in the offline dataset on   availability. \"# prompts\" demonstrates the\nQPO. Firstly, we compare the performance with different num-  number of initial prompts used for training.\nbers of prompts shown in Table 6 , where the 30 prompts are\nfrom online algorithms without ChatGPT rewriting and the 5                        Avg.\nprompts are randomly selected from the 30 prompts. The results   # prompt\n                                                                             zero-shot   few-shot\nreveal an overall trend that more prompts lead to better fi-\nnal performance, while an exceptionally large number of initial    scarce (5)     72.8        83.1\nprompts is not necessary, as a moderate amount can already    middle (30)   77.8        84.2\nyield good results. This is because once the dataset contains a     rich (150)    80.4      84.5\nsufficient number of high-quality prompts, subsequent data aug-\nmentation phase can also effectively enhance prompt diversity.\n\n\n                                              10\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\nIt also validates the effectiveness of supplementing prompts with lower-quality GPT-rewritten prompts, as\na prompt with a lower average accuracy can still perform well on specific queries. This experiment demon-\nstrates QPO‚Äôs potential to perform well with a smaller number of prompts. Detailed results are shown in\nTable 16.\n\nNext, we discuss the impact of data quality on our method to\nillustrate the importance of data filtering. We view the filtered  Table  7:  Performance under  different data\ndataset as expert dataset and the original dataset as medium-   quality. QPO with medium-expert dataset can\nexpert one, for all the prompts within the dataset are capable  already achieve comparable or better results to\nof achieving at least moderate performances. Table 7 demon-  SOTA Prompt-OIRL.\nstrates that QPO on expert dataset after data filtering out-\nperforms that on medium-expert dataset. By filtering 34.4%                           Avg.\n                                                             Datasetof the lower-quality data, our method achieves a 2.1% per-                                                                                 zero-shot   few-shot\nformance improvement. While when trained on an unfiltered\nmedium-expert dataset, our algorithm still achieves compara-    expert          80.4      84.5\nble or better results to the SOTA Prompt-OIRL, which demon-    medium-expert   77.9        82.6\nstrates that QPO exhibits strong robustness to prompt\nquality and data filtering can further enhance its performance. The detailed results are presented\nin Table 18.\n\n                       Table 8: Cross-model generalization on different models.\n\n\n                          AG News            BoolQ           IMDB\n  Model             Method                                       # wins\n                                       zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot\n\n                  PromptSource   58.5        63.0      50.3      49.8      80.9        82.9         2\n  GPTNeo-1.3b     Prompt-OIRL  65.3      65.1      48.3        49.0       78.3        84.0         2\n           QPO           61.7        63.4      50.3       49.1      83.3      87.5        3\n\n                  PromptSource   75.1        83.7       79.1        82.8       89.8       92.2        1\n  LlaMa2-13b-chat  Prompt-OIRL  80.3       82.4       78.0        83.0       88.3        91.2         1\n           QPO          80.3      84.4     80.7      83.9     90.3       92.1         5\n\n                  PromptSource   74.3        80.3       79.4        80.8      89.5       92.2         1\n  Vicuna-13b      Prompt-OIRL  76.0       81.4       82.7        84.1       88.3        92.5         1\n           QPO           75.0       82.6     86.0      84.6      87.3       92.9        4\n\n\nCross-Model Generalization. We investigate whether QPO can be used to steer the target LLMs which\nare not involved in the dataset collection and augmentation. In this section, the policy model are trained\non datasets collected by Llama2-7b-chat. As shown in Table 8, QPO demonstrates excellent cross-model\ntransferability compared with human designed prompts, which are generally considered high-quality and can\nbe employed on any LLMs. Compared to the results with GPTNeo-1.3B, our algorithm demonstrates greater\nadvantages on two larger LLMs (3 wins v.s. 5 wins & 4 wins). This cross-model generalization significantly\nenhances the value of our approach: We can train the policy model using existing data tested on different\nmodels and then apply it to the target model. Additionally, we can collect data using a cost-effective smaller\nLLM and train the policy model, then directly apply it to a more expensive larger LLM.\n\nCase Study. The prompts generated by QPO in NLU tasks are presented in Table 19, which showcase the\nversatility of our approach. Notably, while their individual accuracies on all testing questions are relatively\nlow, their collective application significantly enhances task-level performance when aligned with specific\nqueries. Although a single generated prompt may not be suitable for all queries, it is certainly\nthe best for the specific query.\n\n4  Related Works\n\nPrompt Optimization. Automatic prompt optimization has become a pivotal issue in domain of LLMs.\nRecently, there has been a growing interest in this area. Soft prompts (Li & Liang, 2021; Zhang et al., 2021;\n\n\n                                              11\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\nLester et al., 2021; Liu et al., 2023b) achieve effective performance by tuning the parameters of the input\ntokens, while exhibit two significant drawbacks. They require parameters of the target LLM, which are\ninaccessible for black-box APIs, and their vector representation lacks interpretability for humans. Discrete\nprompts, also known as prompt engineering, eliminate the shortcomings and also achieve great success. Most\nof these methods are query-agnostic.  Automatic Prompt Engineering (APE)(Zhou et al., 2022) induces\nprompts by instructing a LLM to describe the given task and refines the set of generated prompts. Low\nPerplexity (SPELL)(Gonen et al., 2022) finds the perplexity is negatively correlated with the performance\nand select the least perplexity instructions. RLPrompt (Deng et al., 2022) employs RL to optimize a prompt\npolicy. Recently, more studies focus on leveraging LLM as prompt optimizer using black-box optimization\nmethod for swift prompt engineering, such as evolution algorithm (Guo et al., 2023; Fernando et al., 2023),\nalternate sampling (Zhou et al., 2022; Xu et al., 2023; Pryzant et al., 2023; Kong et al., 2023), and RL (Kong\net al., 2024; Wang et al., 2023b). However, studies (Wu et al., 2022; Jiang et al., 2022; Zhang et al., 2022;\nSun et al., 2023) have shown that query-dependent prompts can obtain better results. TEMPERA (Zhang\net al., 2022) designs the action space to be editing operations to achieve test-time query-dependent edit.\nPrompt-OIRL (Sun et al., 2023) utilize inverse RL to learn a proxy reward model and thus select the best\nprompt based on the query from a candidate prompt set. Another advantage of Prompt-OIRL is it makes full\nuse of offline dataset, significantly reducing the inference cost by interactions with LLMs. Promptist (Hao\net al., 2023) also collect an offline dataset and employ SFT to fine-tune the policy model, while the main\nperformance improvements are from the subsequent online RL.\n\nLearning from Human Feedback. Our method is related to the research on reinforcement learning from\nhuman feedback, which has shown promising results on a wide range of tasks, including NLP (Rajpurkar\net al., 2018; Wang et al., 2023a; Ouyang et al., 2022; Stiennon et al., 2020; Klie et al., 2020; Wang et al.,\n2024a) and classical RL (Christiano et al., 2017; Ibarz et al., 2018; Xia et al., 2024). Another similar category\nis reinforcement learning form AI feedback (RLAIF) (Bai et al., 2022; Lee et al., 2023), which leverages the\nfeedback typically from LLMs to facilitate RL. Differently, we directly use the feedback from LLM to shape\nrewards rather than learning a reward model. Our method is akin to imitation learning (Torabi et al., 2018;\nChen et al., 2021; Janner et al., 2021), directly learning a policy from a batch of behavioral demonstrations\nand their corresponding rewards. This distinctive approach sets our method apart from existing prompt\noptimization techniques.\n\n\n5  Conclusion and Future Work\n\n\nWe propose QPO, a novel query-dependent prompt optimization method through a multi-loop offline RL\nframework, which is cost-efficient.  This method leverages offline datasets from existing evaluations and\nemploys offline RL to fine-tune a small-scale language model as the policy model, generating query-level\noptimal prompts. Experimental results demonstrate the superiority and cross-model generalization ability\nof QPO, significantly enhancing the value of our method.  Besides, we conduct extensive ablation studies\nto analyse our method‚Äôs insights and rationale. While our method has substantially reduced the required\nonline interactions, we can entirely eliminate these interactions if necessary. For instance, in certain business\nscenarios where only a fixed amount of reward-labeled data is available and the reward labeling rules are\nunknown, it is impossible to use LLMs to obtain new labeled data as our exploration. We can employ inverse\nreinforcement learning to derive a reward model from the original dataset to score the new query-prompt\npairs obtained through the multiple-loop data augmentation. This approach completely eliminates the need\nfor online interactions with LLMs, which we consider for future work.  Furthermore, in the domains of\ntext-to-image and text-to-video generation, where LLMs have slower inference speeds, interactions are more\nexpensive, and user queries are more unique, our offline query-dependent prompt optimization methods will\nshow greater potential and advantages.\n\n\n6  Broader Impact Statements\n\n\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential\nsocietal consequences of our work, none which we feel must be specifically highlighted here.\n\n\n                                              12\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\nReferences\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\n  Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.  Gpt-4 technical report.  arXiv\n  preprint arXiv:2303.08774, 2023.\n\nAnirudh Ajith, Mengzhou Xia, Ameet Deshpande, and Karthik R Narasimhan.  Instructeval: Systematic\n  evaluation of instruction selection methods. In R0-FoMo: Robustness of Few-shot and Zero-shot Learning\n  in Large Foundation Models, 2023.\n\nStephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht\n  Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An integrated development\n  environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\n  Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from\n   ai feedback. arXiv preprint arXiv:2212.08073, 2022.\n\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale autoregressive\n  language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata, 58:\n   2, 2021.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Ar-\n  avind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.\n  Advances in neural information processing systems, 34:15084‚Äì15097, 2021.\n\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforce-\n  ment learning from human preferences. Advances in neural information processing systems, 30, 2017.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael  Collins, and Kristina\n  Toutanova.  Boolq:  Exploring the surprising difficulty of natural yes/no questions.  arXiv preprint\n  arXiv:1905.10044, 2019.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\n  Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.  Training verifiers to solve math word\n  problems. arXiv preprint arXiv:2110.14168, 2021.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric\n  Xing, and Zhiting Hu.  Rlprompt: Optimizing discrete text prompts with reinforcement learning.  In\n  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3369‚Äì3391,\n  2022.\n\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt√§schel. Prompt-\n  breeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.\n  arXiv preprint arXiv:2012.15723, 2020.\n\nHila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer.  Demystifying prompts in\n  language models via perplexity estimation. arXiv preprint arXiv:2212.04037, 2022.\n\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. News summarization and evaluation in the era of gpt-3.\n  arXiv preprint arXiv:2209.12356, 2022.\n\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu\n  Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers.\n  arXiv preprint arXiv:2309.08532, 2023.\n\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. In Thirty-\n  seventh Conference on Neural Information Processing Systems, 2023.\n\n\n                                              13\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading compre-\n  hension with contextual commonsense reasoning. arXiv preprint arXiv:1909.00277, 2019.\n\nBorja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning\n  from human preferences and demonstrations in atari. Advances in neural information processing systems,\n  31, 2018.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling\n  problem. Advances in neural information processing systems, 34:1273‚Äì1286, 2021.\n\nYuezihan Jiang, Hao Yang, Junyang Lin, Hanyu Zhao, An Yang, Chang Zhou, Hongxia Yang, Zhi Yang, and\n  Bin Cui. Instance-wise prompt tuning for pretrained language models. arXiv preprint arXiv:2206.01958,\n  2022.\n\nJan-Christoph Klie, Richard Eckart De Castilho, and Iryna Gurevych. From zero to hero: Human-in-the-\n  loop entity linking in low resource domains. In Proceedings of the 58th annual meeting of the association\n  for computational linguistics, pp. 6982‚Äì6993, 2020.\n\nWeize Kong, Spurthi Amba Hombaiah, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Prewrite:\n  Prompt rewriting with reinforcement learning. arXiv preprint arXiv:2401.08189, 2024.\n\nYilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru\n  Hu, Hangyu Mao, Ziyue Li, et al.  Tptu-v2: Boosting task planning and tool usage of large language\n  model-based agents in real-world systems. arXiv preprint arXiv:2311.11315, 2023.\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Car-\n  bune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback.\n  arXiv preprint arXiv:2309.00267, 2023.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.\n  arXiv preprint arXiv:2104.08691, 2021.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,\n  and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint\n  arXiv:2101.00190, 2021.\n\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\n  Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.  Holistic evaluation of language models. arXiv\n  preprint arXiv:2211.09110, 2022.\n\nCheng-Yuan Liou, Wei-Chen Cheng, Jiun-Wei Liou, and Daw-Ran Liou. Autoencoder for words. Neurocom-\n  puting, 139:84‚Äì96, 2014.\n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.  Pre-train,\n  prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM\n  Computing Surveys, 55(9):1‚Äì35, 2023a.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands,\n  too. AI Open, 2023b.\n\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning\n  word vectors for sentiment analysis.  In Proceedings of the 49th annual meeting of the association for\n  computational linguistics: Human language technologies, pp. 142‚Äì150, 2011.\n\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. Semeval-2018 task\n   1: Affect in tweets. In Proceedings of the 12th international workshop on semantic evaluation, pp. 1‚Äì17,\n  2018.\n\n\n                                              14\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\nOpenAI. Gpt-3.5-turbo. https://platform.openai.com/docs/models/gpt-3-5-turbo#gpt-3-5-turbo,\n  2023.\n\nOpenAI. Gpt-4o. https://platform.openai.com/docs/models/gpt-4o#gpt-4o, 2024.\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n  Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\n  human feedback. Advances in neural information processing systems, 35:27730‚Äì27744, 2022.\n\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word\n  problems? arXiv preprint arXiv:2103.07191, 2021.\n\nS Patro. Normalization: A preprocessing stage. arXiv preprint arXiv:1503.06462, 2015.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng.  Automatic prompt\n  optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\n  are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don‚Äôt know: Unanswerable questions for\n  squad. arXiv preprint arXiv:1806.03822, 2018.\n\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario\n  Amodei, and Paul F Christiano.  Learning to summarize with human feedback.  Advances in Neural\n  Information Processing Systems, 33:3008‚Äì3021, 2020.\n\nHao Sun, Alihan H√ºy√ºk, and Mihaela van der Schaar. Query-dependent prompt evaluation and optimization\n  with offline inverse rl. In The Twelfth International Conference on Learning Representations, 2023.\n\nFaraz Torabi, Garrett Warnell, and Peter Stone.  Behavioral cloning from observation.  arXiv preprint\n  arXiv:1805.01954, 2018.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\n  Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\n  language models. arXiv preprint arXiv:2302.13971, 2023.\n\nHaoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang,\n  Sen Zhang, Li Shen, et al. Are large language models really robust to word-level perturbations?  arXiv\n  preprint arXiv:2309.11166, 2023a.\n\nHaoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao\n  Bian, Tingyang Xu, et al. Step-on-feet tuning: Scaling self-alignment of llms via bootstrapping. arXiv\n  preprint arXiv:2402.07610, 2024a.\n\nRuochen Wang, Sohyun An, Minhao Cheng, Tianyi Zhou, Sung Ju Hwang, and Cho-Jui Hsieh. One prompt\n   is not enough: Automated construction of a mixture-of-expert prompts. arXiv preprint arXiv:2407.00256,\n  2024b.\n\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing,\n  and Zhiting Hu.  Promptagent:  Strategic planning with language models enables expert-level prompt\n  optimization. In The Twelfth International Conference on Learning Representations, 2023b.\n\nYifei Wang, Jizhe Zhang, and Yisen Wang. Do generated data always help contrastive learning?  arXiv\n  preprint arXiv:2403.12448, 2024c.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\n  Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information\n  processing systems, 35:24824‚Äì24837, 2022.\n\n\n                                              15\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Rui Hou, Yuxiao Dong, VG Vydiswaran, and Hao Ma. Idpg: An\n  instance-dependent prompt generation method. arXiv preprint arXiv:2204.04497, 2022.\n\nBo Xia, Yilun Kong, Yongzhe Chang, Bo Yuan, Zhiheng Li, Xueqian Wang, and Bin Liang. Deer: A delay-\n   resilient framework for reinforcement learning with variable delays.  arXiv preprint arXiv:2406.03102,\n  2024.\n\nWeijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. Reprompting: Automated chain-of-thought prompt\n  inference through gibbs sampling. arXiv preprint arXiv:2305.09993, 2023.\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\n   finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun\n  Chen. Differentiable prompt makes pre-trained language models better few-shot learners. arXiv preprint\n  arXiv:2108.13161, 2021.\n\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera: Test-\n  time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning\n  Representations, 2022.\n\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification.\n  Advances in neural information processing systems, 28, 2015.\n\nBoyuan Zheng, Sunny Verma, Jianlong Zhou, Ivor W Tsang, and Fang Chen. Imitation learning: Progress,\n  taxonomies and challenges. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\n  Zhuohan Li, Dacheng Li, Eric Xing, et al.  Judging llm-as-a-judge with mt-bench and chatbot arena.\n  Advances in Neural Information Processing Systems, 36, 2024.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference\n  on Learning Representations, 2022.\n\n\n\n\n\n                                              16\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\nA  Extended Discussion on QPO\n\nA.1  Advantages of offline RL compared to SFT\n\nSupervised fine-tuning (SFT) can be regarded as a form of imitation learning (Zheng et al., 2022), similar\nto behavior cloning (Torabi et al., 2018), where it mimics expert actions (i.e., the query-specific optimal\nprompts) based on given states (i.e., input questions). While offline reinforcement learning introduces reward\nvalues, guiding the generation of prompts based on the given queries and associated rewards. In prompt\ngeneration task, directly employing SFT results in low data utilization because it is unknown in advance\nthat which prompt is optimal for specific problems. This necessitates massive prior evaluation of all prompts\nbefore selecting the best one as the expert behavior to construct the dataset, leading to significant resource\nwastage. Specifically, in the final SFT dataset, each query should ideally have only one prompt label. All M\nprompts must be evaluated but only one optimal prompt is retained as the label for SFT ultimately, wasting\nthe remaining (M ‚àí1)/M evaluation data. While for offline RL, the prediction of prompt is based on both\nreward and query, leading to a much more diversity in input. Since different prompts have various rewards,\nmost of them can be utilized for training. Offline RL offers significant advantages through the introduction\nof the rewards, which allows each query to be associated with multiple prompts and their corresponding\nrewards in the dataset. On one hand, the rewards improves the utilization of collected suboptimal prompts\nfor training, significantly boosting exploration of the prompt space while avoiding resource wastage caused\nby prompt selection. On the other hand, rewards in RL enable the policy model to distinguish the prompting\nabilities of different prompts for various queries at the token level, leading to the generation of novel prompts\nfor unknown queries.\n\n\nA.2  Prompt Generation as Single-step Decision Process\n\nIn the reinforcement learning setup, prompt generation is a sparse reward task. The complete prompt is\nevaluated with the target LLM to obtain a reward value only after it is fully generated. Given the availability\nof substantial amounts of such test data, offline reinforcement learning is an effective approach for prompt\noptimization. For offline RL, there are two methods for collecting token-wise rewards which can be used for\nmulti-step decision-making processes:\n\n  (1) Manually designing token-level rewards, which not only requires extensive prior expert knowledge but\n       also introduces additional errors, making it unreliable.\n\n  (2) Iteratively deleting the prompt‚Äôs tokens from end to beginning and re-evaluating to obtain rewards\n     from sm‚àí1 = (q, p1, ..., pm‚àí1) to sm = (q, p1, ..., pm), where pm denotes the m-th token in the complete\n      prompt.  However, this approach significantly increases the cost of collecting datasets and is thus\n      impractical.\n\nTherefore, we do not introduce additional proxy rewards and directly utilize the sparse rewards. At this point,\ninstead of modeling the prompt generation task as a multi-step decision-making process {r, s0, p1, r, s1, p2, ...},\nwhere each r  is the same sparse reward and additional methods are required to encode long texts\n(q1, ..., qn, p1, ..., pi) into the single token state si, we simplify the task modeling based on the principle\nof Occam‚Äôs Razor. Thus, the prompt generation is formulated into a single-step decision process (r, q, p).\nNotably, the single-step decision-making process refers to the reward and state, taking only one action (i.e.,\ngenerating the prompt), while the inner prompt generation process remains an autoregressive prediction\np ‚àºQt œÄ(pt|r, q, p<t). The single-step decision process also resembles the offline contextual bandit setup,\nas actions do not interact with the environment.\n\n\nA.3  The Architecture of Policy Model\n\nCompared to the standard GPT-2 model with 124M parameters, we make three modifications to the model\narchitecture: a reward embedding layer, a reward prediction layer, and a timestep embedding layer. Both\nthe reward embedding layer and reward prediction layer are single-layer MLPs at the first token of the policy\nmodel. The reward is encoder into a reward encoding through the reward embedding layer, then input as the\nfirst token into the transformer to obtain the output at the first token position, which is then passed through\n\n\n                                              17\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\nthe reward prediction layer to generate the predicted reward. This process can be viewed as an autoencoder\ndesigned to extract more information from the reward. The additional timestep embedding layer encoding\nthree timesteps into three embeddings, which are added with reward embedding, query token embedding, and\nprompt token embedding, respectively. This helps the policy model better learn the positional relationships\nand distinctions between reward, question and prompt.\n\n\nA.4  Autoencoder with reward prediction loss\n\nAutoencoder encodes the input into a low-dimensional latent state through an encoder and subsequently\nlearns to reconstruct the original input from this low-dimensional latent state via a decoder. In this process,\nthe latent state can effectively represent the original input. In our approach, the reward embedding layer\nand the transformer map the reward value into a hidden state. During training, the additionally introduced\nreward prediction layer acts as a decoder to predict the original input reward based on this hidden state,\nand the encoder-decoder is updated through the reward prediction loss. In the inference phase, we no longer\nutilize the reward prediction layer (decoder); instead, we leverage the hidden state (related to the reward)\nobtained from the encoder to perform another task, that is, predicting the prompt. This process mirrors\nthe autoencoder‚Äôs principle, thus introducing the reward prediction loss and the reward prediction layer into\nthe PLM essentially makes it serve as an autoencoder. Notably, the reward prediction loss does not directly\ncontribute to reinforcement learning process, as its optimization objective is not to generate actions.\n\n\nA.5  Detailed Algorithm\n\n\nAlgorithm 1 Query-Dependent Prompt Optimization\nRequire: Initial dataset D0, a collection set for task queries Q, the number of loops T, a pre-trained\n    language model œÄ0, ORL(¬∑) denotes the offline RL fine-tuning process, R(¬∑) presents evaluating the new\n   query-prompt pairs on the target LLM and calculating the reward.\n  1: Offline RL: œÄ1 ‚ÜêORL(œÄ0, D0)\n  2: for t = 1 to T ‚àí1 do\n  3:   Query Augmentation: q ‚Üêrandom_sample(Q)\n  4:   Prompt Augmentation: p ‚ÜêœÄt(q)\n  5:    Dataset Augmentation: DA ‚Üê{q, p, R(q, p)}\n  6:                           Dt ‚Üê{Dt‚àí1, DA}\n  7:    Offline RL: œÄt+1 ‚ÜêORL(œÄt, Dt)\n  8: end for\n  9: return the policy model œÄT , which can generate optimal query-dependent prompts during testing.\n\n\nB  Supplemental Experiment Detials\n\nB.1  Hyperparameters and Details\n\nFor the task datasets with default testing or development set, we use their original split to obtain our\ntesting set.   If there is no official training/development/testing split, we randomly sample a reasonably\nlarge set for stable evaluating and testing.  Additionally, for all tasks, we split 10% training samples as\ncollection set for initial query collection and subsequent query augmentation, ensuring that the collected\nqueries for training the policy model do not appear in the in-context examples during few-shot evaluation,\nand simultaneously simulating a scenario where very few questions are available.  In each loop in query\naugmentation, 1,000 queries are sampled, which may include repeated queries. For repeated queries, the\nsampling prompt generation approach ensures different prompts are generated, allowing for the exploration\nof varying prompt effectiveness for each query. We do not explicitly aim for uniqueness in the sampled 1,000\nqueries per loop, opting instead for a fully random sampling approach.\n\nThe parameters for the experiments are shown in Table 9. Notably, QPO consists of 4 loops, indicating there\nare 4 times of offline reinforcement learning fine-tuning stage and 3 times of data augmentation between\n\n\n                                              18\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\neach fine-tuning stage. Since we introduce additional networks into the policy model, the number of training\nepochs and the learning rate required for the first loop of training are relatively high.\n\n                             Table 9: Hyperparameter settings of QPO\n\n\n                    Hyperparameters                      Values\n\n                   Loops                                 4\n                      Batchsize                             128\n                     Learning Rate           1e-3 for the 1st loop, 1e-4 for others\n                     Train Epochs            100 for the 1st loop, 20 for others\n                    Optimizer                  AdamW\n                   Weight Decay                            1e-4\n                     Balancing Parameter Œª                    0.1\n\n\nB.2   Offline Dataset Collection\n\nFor the initial dataset collection, testing 150 prompts on too many queries incurs excessive computational\ncosts.  Therefore, we test 100 queries for every group of 40 prompts (with the last group containing 30\nprompts). Each group sampled different queries, achieving a balance between query coverage and compu-\ntational cost. Since queries are randomly sampled for each test group, there are overlapped queries across\ndifferent groups. In the end, our dataset consists of 15,000 paired examples, comprising 150 prompts and ap-\nproximately 350 queries. This dataset size is significantly lower than that required by other offline algorithms,\nsuch as Prompt-OIRL for averages 56,900 examples and Promptist for averages 360,000 examples.\n\nFor the specific prompts, the initial 30 prompts are derived from various previous approaches, including\nPromptSource, ChatGPT Rewrite, Low Perplexity, RLPrompt and APE, which are all produced by In-\nstructEval (Ajith et al., 2023), and we leverage ChatGPT-3.5 chat box to rewrite 120 new prompts based on\nthese 30 prompts through in-context learning. Specifically, we provide 5 existing prompts to ChatGPT-3.5,\nand ask it to generate other 30 effective prompts for 4 times. The prompt we use for rewriting is demonstrated\nin Table 10.\n\n              Table 10: Template used for ChatGPT rewriting prompts (Sun et al., 2023)).\n\n\n\n In practice, people find the following prompts help improving the classification abilities of language models like\n LlaMa-7b-chat on the <TASK> dataset. Rewrite the following instructions via rephrasing and/or adding specific\n requirements. Use illustrative description if needed.\n\n #########\n Prompts:\n <PROMPT1>\n <PROMPT2>\n <PROMPT3>\n <PROMPT4>\n <PROMPT5>\n\n Please provide 30 more prompts different from those prompts that may improve the classification ability of language\n models. Diversity is much appriciated!\n\n\n\n\nB.3  Codes and Hardware\n\nOur code, as well as the offline datasets, is now publicly available at here. All experiments are conducted on\na single NVIDIA V100 32g GPU.\n\n\n                                              19\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\nC  Additional Results\n\nC.1  Main Results on 3-shot Evaluation\n\n             Table 11: Main results of QPO and baselines on NLU tasks on 3-shot settings\n\n\n         Method    AG News  BoolQ  IMDB  Emotion  CosmosQA  HellaSwag  Avg.\n\n       PromptSource    76.0        58.5     90.3     42.2       39.3          69.8         62.7\n      Low Perplexity   53.5        58.0     90.1     48.0       39.0          70.0         59.8\n      RLPrompt       76.2        69.4     89.7     56.1       41.1          69.9         67.1\n     APE             58.6        59.9     89.9     57.6       40.2          70.2         62.7\n      Prompt-OIRL    76.9        67.1     89.2     52.7       40.6          70.4         66.2\n     QPO           82.3      71.1    90.5    62.7      41.5        70.7       69.8\n\n\n\nC.2  Main Results on GPT-4o.\n\nAs demonstrated in Table 12, our method also outperforms other baselines on most updated models.\n\n\nC.3  Cost Analysis\n\nIn this section, we analyse the specific number of interactions required by different algorithms. APE requires\na total of M ‚àó|D|‚àóT interactions with LLMs to obtain the feedback for optimization, while QPO only requires\n|C|‚àóT ‚Ä≤ interactions for data augmentation, where M is the candidate population under evaluation, |D| is the\nsize of development set, |C| is the size of collection set, and T and T ‚Ä≤ are the number of iterations needed.\nThough |C| is slightly larger than |D| for better exploration, QPO significantly reduces computational\noverhead due to its minimal iteration T ‚Ä≤ and the absence of the need for evaluate M candidate prompts.\n\n\nC.4  Analysis on reward design\n\nBoth Rquery and Rtask contribute to the performance. By separately removing Rquery and Rtask and\ncompare the results with the full reward design(Rquery+Rtask), results in Table 13 indicate that for most\nof tasks, the combined design of Rquery+Rtask yields superior performance. Thus, both Rquery and Rtask\ncontribute to the performance. Notably, the use of either Rquery or Rtask alone also demonstrates commend-\nable results, which underscores the rationality of employing these two metrics as rewards. Furthermore, the\ncomparison between using Rquery and Rtask individually highlights the advantage of Rquery, suggesting that\nquery-based prompt optimization is more effective than task-level prompt optimization.\n\nPerplexity penalty is important. We compare the performance of rewards with and without perplexity\n(PPL) to validate the importance of the fine-grained penalty in Rquery.  The experimental results are\npresented in Table 14.  For all tasks, incorporating PPL consistently yields better performance.  This is\nbecause the introduction of PPL transforms the Rquery from a binary value into a fractional value, enhancing\nthe discrimination among different query-prompt pairs.  Consequently, the policy model can better learn\nwhich prompts are more effective for specific queries.\n\nPredicting reward at the first token helps learn better. We introduce the reward prediction loss\nto enhance the model‚Äôs ability to discern the reward values across different query-prompt pairs, while si-\nmultaneously preventing the policy model from overemphasizing rewards to the extent that it neglects the\ndistinctions between queries and prompts, potentially leading to overfitting on the prompt with the highest\nreward value across all samples in the dataset.  Predicting the reward at the first token can simplify the\nattention calculation process and mitigate excessive updates to the query and prompt embedding layers,\nthereby reducing the risk of overfitting. Our experimental validation in Table 15 confirms the effectiveness of\npredicting the reward value at the first token, as evidenced by its better performance compared to predicting\nat the last token.\n\n\n                                              20\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\n                 Table 12: Main results of QPO and baselines on GPT-4o on GSM8K.\n\n\n                            Method       GSM8K\n\n                        CoT              95.8\n                         ChatGPT         94.7\n                       APE              95.0\n                            Prompt-OIRL     93.5\n                     QPO            96.5\n\n\n                   Table 13: Comparisons between Rquery, Rtask and Rquery+Rtask.\n\n\n              AG News            BoolQ           IMDB                Avg.\n    Method\n                  zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot\n\n Rquery          78.3        84.5      71.4      77.2      89.0        90.9       79.6        84.2\n Rtask           76.3        81.6       71.0        75.3       84.7        91.2       77.3        82.7\n Rquery+Rtask  79.0      85.6      71.0        76.5      91.2      91.4     80.4      84.5\n\n\n\nC.5  Results on Different Prompt Quantity\n\nThe detailed results are shown in Table 16\n\n\nC.6  Results on Different Prompt Quality\n\nWe conduct experiments on the dataset where all prompts were rewritten by ChatGPT to validate the\nrobustness of QPO to prompt quality within the dataset. The results are presented in Table 17. The\nabsence of initial high-quality prompts has minimal impact on QPO‚Äôs performance, as the method focuses\non the score of each query-prompt pair rather than the performance of individual prompts. Consequently,\neven without expert prompts, there are still other prompts perform well for each specific query.\n\nThe detailed results on different query-prompt pair quality is shown in Table 18.\n\n\nC.7  Case Study\n\n\n\n\n\n                                              21\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\n                  Table 14: Comparisons between with and without perplexity (PPL).\n\n\n Method       AG News   BoolQ     IMDB      Emotion    CosmosQA  HellaSwag   Avg.\n\n QPO (w/ PPL)    79.0         71.0         91.2         68.7         45.3         69.7         70.9\n QPO (w/o PPL)   75.0         65.7         90.3         62.7         42.7         69.0         67.6\n\n\n\n\n\n                     Table 15: Comparisons between reward predicting positions.\n\n\n                AG News            BoolQ           IMDB                Avg.\n     Method\n                      zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot\n\n QPO (first token)   79.0        85.6       71.0        76.5       91.2        91.4       80.4        84.5\n QPO (last token)   79.3        81.2       69.7        76.1       89.3        90.9       79.4        82.7\n\n\n\n\n\n                      Table 16: Performance under different prompt availability.\n\n\n             AG News            BoolQ           IMDB                Avg.\n Prompt Num\n                 zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot\n\n scarce          65.3        83.3       65.7        76.3       87.3        89.6       72.8        83.1\n middle         70.7        83.9       70.7       78.4     91.5      89.7      77.6        84.0\n rich           79.0      85.6     71.0       76.5       91.2       91.4     80.4      84.5\n\n\n\n\n\n                   Table 17: Performance under different prompt quality in dataset.\n\n\n                   AG News            BoolQ           IMDB                Avg.\n        Datase\n                           zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot\n\n QPO                  79.0       85.6      71.0      76.5     91.2      91.4     80.4      84.5\n only ChatGPT prompt   74.3       85.9      67.0        76.4       90.7        90.9       77.3        84.4\n\n\n\n\n\n                   Table 18: Performance under different query-prompt pair quality.\n\n\n              AG News            BoolQ           IMDB                Avg.\n     Datase\n                   zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot   zero-shot   few-shot\n\n expert          79.0      85.6     71.0      76.5     91.2      91.4     80.4      84.5\n medium-expert   75.0        83.1       68.3        75.9       90.3        88.9       77.9        82.6\n\n\n\n\n                                              22\n\nPublished in Transactions on Machine Learning Research (02/2025)\n\n\n\n\n\n            Table 19: Prompts generated by QPO. \"I.Acc.\" denotes the individual accuracy.\n\n\n                        Generated Query-level Prompt                                       I. Acc.  QPO Acc.\n\n                             AG News\n\n  In which part of a newspaper, World News, Sports, Business, or Science and\n                                                                                       69.0\n  Technology? Innov., Sports.., or., or.,.,., or.,\n                                                                                                  79.0  Assign this news report to the most suitable newspaper segment: World News,\n                                                                                       70.0\n  Sports, Business, or Technology. and Sports, or Sports, or\n\n                                                                   ...                                                                        ...\n\n                                        BoolQ\n\n  Analyze if the passage validates the true/false claim in the question.                  64.0\n\n                                                                                                  71.0\n  Confirm the correctness of a true/false question using the text excerpt.......            63.0\n\n                                                                   ...                                                                        ...\n\n                                 IMDB\n\n  Decipher the overall sentiment conveyed by the reviewer‚Äôs critique of the movie.\n                                                                                       89.3\n  or or negative?\n                                                                                                  91.2\n  Interpret the reviewer‚Äôs emotional stance towards the film: positive or negative???    87.7\n\n                                                                   ...                                                                        ...\n\n                                         Emotion\n\n  receive full credit, choose the emotion that best fits the sentiment expressed in the\n                                                                                       49.7\n  tweet from the following options: anger, joy, optimism, or sadness. sadness.\n                                                                                                  68.7  From the given options, determine the emotion that best captures the essence of\n                                                                                       60.0\n  the tweet‚Äôs sentiment: anger, joy, optimism, or sadness... tweet. best\n\n                                                                   ...                                                                        ...\n\n                                   CosmosQA\n\n  Analyze the contextual details to select the response that offers the most compre-\n                                                                                       43.3\n  hensive understanding. the.\n                                                                                                  45.3  Synthesize the information provided below to formulate a well-supported response\n  to the upcoming question. question.                                                  42.3\n\n                                                                   ...                                                                        ...\n\n                                           HellaSwag\n\n  Explore the passage‚Äôs thematic depth and devise an ending that explores those\n                                                                                       68.0\n  themes further.........\n                                                                                                  69.7  EnIm with the passage and them and incorporate them into the ending. engaged.\n                                                                                       67.7\n  engagedates. their comple their expectations.\n\n                                                                   ...                                                                        ...\n\n\n\n\n\n                                              23",
"headers": [
"arXiv:2408.10504v2  [cs.AI]  30 May 2025",
"QPO: Query-dependent Prompt Optimization via",
"Multi-Loop Offline Reinforcement Learning",
"Abstract",
"1",
"Introduction",
"2",
"Methodology",
"3",
"Experiments",
"4",
"Related Works",
"5",
"Conclusion and Future Work",
"6",
"Broader Impact Statements",
"References",
"A",
"Extended Discussion on QPO",
"B",
"Supplemental Experiment Detials",
"C",
"Additional Results"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2408.10504v2.pdf"
}