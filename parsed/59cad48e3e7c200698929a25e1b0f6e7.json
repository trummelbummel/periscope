{
"text": "MULTIMODAL PROMPT OPTIMIZATION: WHY NOT\n          LEVERAGE MULTIPLE MODALITIES FOR MLLMS\n\n\n                  Yumin Choi1‚àó  Dongki Kim1‚àó  Jinheon Baek1  Sung Ju Hwang1,2\n                   1KAIST   2DeepAuto.ai\n                   {yuminchoi, cleverki, jinheon.baek, sungju.hwang}@kaist.ac.kr\n\n\n\n                                       ABSTRACT\n\n                             Large Language Models (LLMs) have shown remarkable success, and their multi-\n                           modal expansions (MLLMs) further unlock capabilities spanning images, videos,\n                            and other modalities beyond text. However, despite this shift, prompt optimization2025\n                               approaches, designed to reduce the burden of manual prompt crafting while maxi-\n                             mizing performance, remain confined to text, ultimately limiting the full potential\n                                of MLLMs. Motivated by this gap, we introduce the new problem of multimodalOct                            prompt optimization, which expands the prior definition of prompt optimization to\n10                          thetacklemultimodalthis problem,spacewedefinedthen proposeby the pairsthe Multimodalof textual andPromptnon-textualOptimizerprompts.(MPO),To\n                              a unified framework that not only performs the joint optimization of multimodal\n                            prompts through alignment-preserving updates but also guides the selection pro-\n                                cess of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-\n                             based selection strategy. Through extensive experiments across diverse modalities\n                                    that go beyond text, such as images, videos, and even molecules, we demonstrate\n                                    that MPO outperforms leading text-only optimization methods, establishing mul-[cs.LG]\n                               timodal prompt optimization as a crucial step to realizing the potential of MLLMs.\n\n\n                1  INTRODUCTION\n\n                     Large Language Models (LLMs) have demonstrated outstanding capabilities across a diverse range\n                       of tasks and domains (OpenAI, 2024; Grattafiori et al., 2024; Yang et al., 2025). We note that a\n                         central factor in unlocking their full potential lies in the design of prompts, which directly influence\n                   model performance. However, crafting high-quality prompts is often a labor-intensive and iterative\n                       process that requires substantial human intervention. To address this limitation, the field of Auto-\n                      matic Prompt Optimization (APO) has emerged, whose goal is to automate the discovery of effective\n                     prompts (Zhou et al., 2023; Pryzant et al., 2023; Yang et al., 2024; Fernando et al., 2024). For exam-\n                          ple, one representative approach (APE) frames this challenge as an iterative search problem, where\n                           at each step, a set of new candidate prompts is generated or updated, evaluated on a target task, and\n                        the best-performing prompts are selected to guide the next round of generation.arXiv:2510.09201v1\n                       Recently, on top of the LLMs, Multimodal Large Language Models (MLLMs) have been proposed,\n                    which process not only text but also images, videos, and other modalities (such as molecules) (Ope-\n                      nAI, 2023; Zhu et al., 2025; Bai et al., 2025; Gemini, 2025). Yet, despite these advances and their\n                      wide-ranging applications, existing prompt optimization methods remain restricted to the textual\n                      modality (Pryzant et al., 2023; Guo et al., 2024; Cui et al., 2025), and overlook the richer expressive\n                       capacity afforded by multimodal inputs (that text alone cannot capture). For instance, as illustrated\n                         in Figure 1, describing the distinct characteristics of a specific bird may require long and potentially\n                    ambiguous text, while a single image can convey the same information far more directly. By limit-\n                       ing optimization to text, existing methods are prone to generating less effective, suboptimal prompts\n                          that fail to fully exploit the multimodal space that MLLMs are inherently capable of leveraging.\n\n                      Motivated by this limitation, we first define the novel problem of multimodal prompt optimization,\n                    which expands the prompt optimization space beyond text to incorporate multiple modalities. How-\n                          ever, while this expanded space opens new opportunities, it also introduces a couple of challenges\n\n                          ‚àóEqual contribution; Code is available at https://github.com/Dozi01/MPO.\n\n\n                                                           1\n\n(A) Text Prompt Optimization                   (B) Multimodal Prompt Optimization (Ours)\n\n                                MLLM                                               MLLM\n      Optimization Process          Optimized Text Prompt:       Query:                                                Optimized Multimodal Prompt             Query:\n\n                                             Classify the bird in the image.      Choose one of the                                            (Optimized) Image   (Optimized) Text:     Choose one of the\n                                                                             following classes ‚Ä¶                                                                                                                                                                                                 following classes ‚Ä¶                                                                                      Textual           Non-Textual                                                  Classify the bird in                                            All are North Pacific ‚Ä¶\n                                                 Specifically,                                                                                                                         the image.\n                                                                                                                                                                                provided,                                                                                                                                                                         Black-                                        Laysan                                                  has                                                      a white body,                                                                               As    Unimodal (Textual) Space        -\n                                                                                                                                                                    footed                                                                                                                                                                                                                                    is identified                                                        -                                         Sooty has                       ‚Ä¶\n                                                        - Black-footed has ‚Ä¶                                     Multimodal Space                              by its pale bill, ‚Ä¶\n\nFigure 1: Concept Figure. (A) Existing prompt optimization approaches restrict the optimization\nto the textual space, leaving MLLMs underutilized by failing to provide rich contextual signals. (B)\nOur multimodal prompt optimization expands the optimization space into multimodality, allowing\nthe discovery of salient multimodal context and fully leveraging the expressive capacity of MLLMs.\n\n\nfor automatic optimization. First, exploring the larger, combinatorial space of multimodal prompts\nrequires a prompt update strategy that can efficiently navigate candidate prompts while maintaining\ncross-modal consistency. Furthermore, selecting promising candidates becomes substantially more\ndifficult, as the enlarged search space makes optimal prompts increasingly sparse, given the need to\naccount for both the effectiveness within each modality and the alignment across modalities, which,\nin turn, calls for evaluation strategies that are both efficient and accurate.\n\nTo address these challenges, we propose Multimodal Prompt Optimizer (MPO), a unified frame-\nwork for optimizing prompts across both the textual and non-textual modalities, which consists of\nthe two key components: (i) alignment-preserving exploration and (ii) prior-inheritance-based selec-\ntion. Specifically, for exploration, the proposed MPO jointly updates the textual prompt, as well as\nits associated non-textual counterparts by generating instructions to create (or revise) the non-textual\ncomponents of the multimodal prompt (unlike prior approaches that refine only text), and notably,\ntheir updates are guided by the single semantic gradient (i.e., feedback) to ensure their alignment\nderived from the failure analysis of the current prompt. Moreover, these updates are further diversi-\nfied through complementary operations, namely generation, editing, and mixing, to ensure the broad\nand expressive exploration of the multimodal prompt space. Then, building on this exploration with\nmultiple candidate prompts updated, MPO leverages the prior-inherited Bayesian-UCB as a prompt\nselection strategy, which utilizes the performance score of parent prompts as a prior (unlike conven-\ntional approaches that treat each candidate independently), to reliably identify the high-performing\nprompts by biasing the selection process toward more promising regions of the multimodal space.\n\nTo validate MPO, we conduct extensive experiments benchmarking it against leading text-only opti-\nmization methods across 10 datasets, and our evaluation suite spans not only images and videos but\nalso molecular structures, ensuring broad coverage of diverse modalities. Across all domains, MPO\ndemonstrate consistent and significant performance gains, empirically confirming our core hypothe-\nsis: expanding the prompt search space into the multimodal domain is crucial to exploit the expanded\ncapacity of MLLMs. Further analyses show the efficacy of MPO components: alignment-preserving\nexploration with complementary operators facilitates the discovery of optimal multimodal prompts\nby not only ensuring cross-modal consistency but also thoroughly probing the search space; and the\nprior-inherited Bayesian-UCB accurately and efficiently selects high-performing prompts, reducing\nevaluation budget by 42% compared with a prior-free baseline. These results highlight MPO as an\neffective framework for optimizing multimodal prompts, unlocking the full capabilities of MLLMs.\n\n\n2  RELATED WORK\n\nMultimodal Large Language Models  The development of MLLMs has significantly extended\nthe capabilities of traditional LLMs by enabling them to process and reason over diverse non-textual\nmodalities, including images, videos, audio, and more (Liu et al., 2023; OpenAI, 2023; Chu et al.,\n2023). In particular, these models are typically trained through large-scale multimodal pre-training,\nwhich aligns modality-specific encoders (e.g., vision or audio) with LLM backbones, followed by\npost-training stages such as supervised fine-tuning and preference optimization to endow them with\nmultimodal instruction-following abilities (Gemini, 2025; Bai et al., 2025; Zhu et al., 2025). More-\nover, leveraging these capabilities, MLLMs have achieved strong performance on a broad range\nof tasks, from foundational ones such as classification and captioning, to domain-specific, high-\nstakes applications such as medical image question answering and pharmacological property pre-\ndiction (Martin et al., 2019; Liu et al., 2021; Corbi`ere et al., 2025; Huang et al., 2021).\n\n\n                                       2\n\n(A) Alignment-Preserving Exploration                           (B) Prior-Inherited Bayesian UCB Selection\n     Textual Prompt ùíïùíï                Textual Prompt ùíïùíï               (A.1) Generation Operator                Informative Prior            Uninformative Prior\n                                                                                                                             ùíÑùíÑùíàùíàùíàùíàùíàùíà             ùíéùíéùíé           Parent Prompt            ùë©ùë©ùë©ùë©ùë©ùë©ùë©ùë©ùú∂ùú∂, ùú∑ùú∑     Non-Textual Prompt ùíéùíé          Non-Textual Prompt ùíéùíé\n                                                                          (A.2) Edit Operator                                     (ùíïùíï, ùíéùíé)                                                                          ùë©ùë©ùë©ùë©ùë©ùë©ùíÇùíÇ1, 1\n     Failure Set ùìïùìï                   Textual Feedback (ùúµùúµùíïùíï, ùúµùúµùíéùíé)\n                                                                ùíéùíé\n                                                                                           ùíéùíéùíé                 Inherit Prior from Parent‚Äôs Performance\n                                                                                                                                                      ùíÑùíÑùíÜùíÜùíÜùíÜùíÜùíÜùíÜùíÜ\n                                                                               Warm Start                   Cold Start\n                                                                          (A.3) Mix Operator\n                                                                ùíéùíé\n                                Updated Textual Prompt ùíïùíïùíï       ùíéùíé              ùíéùíéùíé          Updated Textual Prompt ùíïùíïùíï                   High-Performing\n     Textual Feedback (ùúµùúµùíïùíï, ùúµùúµùíéùíé)                                                                       ùíÑùíÑùíéùíéùíéùíéùíéùíé                                                             Prompt (ùíïùíïùíï, ùíéùíéùíé)\n                                                                                        Updated                                                                                                        Non-Textual                                     Textual Condition ùíÑùíÑ                        Modality-specific\n                                                                                           Prompt                                                                                    Generator ùíàùíà                                                                                                                                                ùíéùíéùíé             Bayesian\n                                                                                                           x N Different Candidate Prompts     UCB\n     Cohesive Backpropagation                      Joint Multimodal Update                              Prior Initialization                          Select Top-k\n\nFigure 2: Overview of MPO, consisting of two components. (A) Alignment-preserving exploration\nanalyzes a failure set to generate feedback, which is then used both to refine the textual prompt and to\nguide a modality-specific generator to create a new non-textual prompt with one of three operators.\n(B) Prior-Inherited Bayesian UCB Selection leverages the parent‚Äôs performance as an informative\nprior, warm-starting the search to effectively identify high-performing prompts among candidates.\n\n\nAutomatic Prompt Optimization  To reduce the burden of manual prompt engineering and sys-\ntematically uncover the effective prompts, the field of Automatic Prompt Optimization (APO) has\nemerged. Existing works can be broadly categorized into two paradigms. The first is gradient-based\noptimization, which learns continuous embedding vectors (i.e., soft prompts) that are prepended to\nmodel inputs to steer behavior (Khattak et al., 2023; Zeng et al., 2024; Wang et al., 2024). Yet, while\neffective, they are computationally costly, yield uninterpretable numerical vectors, and are restricted\nto open-source models with accessible parameters. To overcome these drawbacks, gradient-free\napproaches have been proposed, which iteratively generate, evaluate, and refine candidate prompts\nusing LLMs themselves (Zhou et al., 2023; Yang et al., 2024). Also, some recent works enhance this\nprocess by analyzing prompt failures to guide improvements (Pryzant et al., 2023; Ye et al., 2024;\nCui et al., 2025; Yuksekgonul et al., 2025), while others borrow ideas from evolutionary algorithms\n(e.g., mutation and crossover) to explore the prompt space (Guo et al., 2024; Fernando et al., 2024).\nDespite this progress, current APO techniques are limited to text-only settings, restricting optimiza-\ntion to purely linguistic information. In contrast, our work expands prompt optimization into the\nmultimodal domain, enabling the prompt discovery that fully exploits the capabilities of MLLMs.\n\nInstance-Specific Prompting and Optimization   Distinct from task-level prompt optimization,\nanother line of research focuses on instance-specific prompting strategies that operate at inference\ntime to enhance reasoning on a per-query basis.  For example, MM-CoT (Zhang et al., 2024b)\nguides the model to generate an intermediate textual rationale before producing the final answer.\nAlso, other methods augment visual inputs with query-dependent signals, such as bounding boxes\nor points, to guide attention toward relevant regions of an image (Zhou et al., 2024; Jiang et al.,\n2024; Lin et al., 2024). Similar ideas have been explored in text-to-image and text-to-video gener-\nation, where prompts are crafted and refined to produce outputs more faithfully aligned with user\nintent (MaÀúnas et al., 2024; Mo et al., 2024; Gao et al., 2025). However, these techniques are query-\nspecific, designed to improve model performance for a single instance at a time. By contrast, APO\npursues a different objective: discovering a single, reusable prompt that boosts performance across\nan entire task, and our work advances this paradigm by extending it into the multimodal domain.\n\n3  METHODOLOGY: MULTIMODAL PROMPT OPTIMIZER\n\nWe present Multimodal Prompt Optimizer (MPO), composed of two modules: alignment-preserving\nexploration of multimodal prompt space and prompt selection with prior-inherited Bayesian UCB.\n\n3.1  PROBLEM DEFINITION\n\nWe begin by formally describing MLLMs and then proposing a novel problem of multimodal prompt\noptimization, which redefines and expands the notion of existing prompt optimization beyond text.\n\nMultimodal Large Language Models  Multimodal Large Language Models (MLLMs) extend the\ncapabilities of LLMs by processing inputs that combine text with non-textual modalities. Formally,\nan MLLM can be represented as a parametric function MLLM : (T ‚à™M)‚àó‚ÜíT , where T denotes\nthe textual input space, M denotes the non-textual input space, and ‚àódenotes the Kleene Star (rep-\nresenting a finite sequence over the combined spaces). In other words, given a multimodal query q\n\n\n                                       3\n\nand a prompt p (each potentially containing both textual and non-textual components), the model\ngenerates a textual output y = MLLM(p, q). It is worth noting that prior work on prompt optimization\nhas generally restricted p to a purely textual form (p = t ‚ààT ), leaving the non-textual dimensions\nof M unused. This restriction underutilizes the expressive capacity of MLLMs and fails to provide\nricher contextual signals that are often crucial for real-world multimodal tasks (See Figure 1).\n\nMultimodal Prompt Optimization  Building on the expanded space of MLLMs, we extend the\nnotion of a prompt for optimization from text-only to multimodal. Specifically, we define a multi-\nmodal prompt as a pair p = (t, m) ‚ààT √óM, where t is the textual prompt and m is the non-textual\nprompt. Then, given a task dataset D consisting of query‚Äìanswer pairs (q, a), the objective of multi-\nmodal prompt optimization is to discover the optimal prompt (t‚àó, m‚àó) that maximizes performance:\n                    (t‚àó, m‚àó) =  argmax   E(q,a)‚àºD h f MLLM(t, m, q), a i ,\n                              (t,m)‚ààT √óM\n\nwhere f is a function for a task-specific evaluation metric, such as accuracy or F1 scores.\n\nNotably, compared to optimizing only textual prompts, the joint search space T √ó M introduces an\nentirely new axis of non-textual information, which in turn raises two fundamental challenges. First,\nmultimodal prompts must maintain cross-modal consistency: textual and non-textual components\nshould provide complementary, not conflicting signals; however, expanding to the combinatorial\nspace greatly increases the risk of semantic misalignment. Second, the enlarged space amplifies\nthe difficulty of candidate selection: high-quality prompts become sparse, and low-quality prompts\ndominate, making it harder to efficiently identify promising candidates. To overcome these, we now\nexplain the proposed multimodal prompt optimizer, designed to navigate this enlarged space below.\n\n3.2  ALIGNMENT-PRESERVING EXPLORATION OF MULTIMODAL PROMPT SPACE\n\nThe first challenge in multimodal prompt optimization lies in exploring the enlarged search space\nwhile preserving semantic consistency across modalities; thus, a naive approach that independently\nupdates textual and non-textual components risks producing misaligned prompts, where one modal-\nity contradicts the other. To tackle this, we introduce an exploration framework that couples the\nupdate of textual and non-textual prompts while supporting diverse operations (Figure 2).\n\nJoint Optimization of Multimodal Prompt  Our MPO jointly updates the textual and non-textual\nprompts to ensure that both evolve coherently, achieved through the following two mechanisms:\n\n‚Ä¢ Cohesive Backpropagation. We begin by identifying a failure set F = {(q, a, y) | y Ã∏= a} for\n  a multimodal prompt p = (t, m). Instead of treating errors separately for text and non-textual\n  inputs, we then generate a unified feedback ‚àáp = (‚àát, ‚àám) = MLLM(t, m; F), which encodes\n  cross-modal weaknesses in textual form. By doing so, we obtain the single supervisory signal that\n  guides both modalities simultaneously, mitigating the risk of overfitting updates to one modality.\n\n‚Ä¢ Joint Multimodal Update. Using the feedback, MPO jointly refines the textual prompt while de-\n  riving modality-specific conditions (in the textual form) that direct non-textual revisions. Specifi-\n  cally, the MLLM produces an updated textual prompt t‚Ä≤ and further a modality-specific condition\n  c describing how the non-textual prompt should adapt: (t‚Ä≤, c) = MLLM(t, m; F, ‚àáp). The condi-\n  tion c is then passed to modality-specific generators g (such as text-to-image or text-to-molecule\n  modules), which yield updated non-textual prompts m‚Ä≤ = g(c). This guarantees that updates to\n m remain consistent with the revised textual prompt t‚Ä≤, rather than being optimized in isolation.\n\nExploration Operators  Ensuring that generated outputs remain consistent with the guiding tex-\ntual conditions is a necessary baseline, and effective optimization further requires g that actively ex-\nplores diverse regions of the multimodal space. To achieve this, we design three operators (namely,\ngeneration, edit, and mix), which systematically expand, refine, and recombine non-textual prompts.\n\n‚Ä¢ Generation operator. This operator explores entirely new non-textual prompts, e.g., novel spatial\n  arrangements in visual inputs or unique substructures in molecules. Specifically, conditioned only\n  on the generation signal cgen, it creates a prompt from scratch without referencing prior candidates:\n              m‚Ä≤ = g(cgen, ‚àÖ), where (cgen, t‚Ä≤) = MLLM(t, m; ‚àáp, F).\n By decoupling from past candidates, it explores unexplored regions and avoids local optima, espe-\n  cially in early stages (where initial prompts are unavailable) or when the candidate pool is biased.\n\n\n                                       4\n\n‚Ä¢ Edit operator. This operator performs fine-grained refinements of non-textual prompts (e.g., tex-\n  tures) while retaining useful structures from the prior prompt. Specifically, given the edit condition\n   cedit, the update is performed by conditioning on the prior non-textual prompt:\n             m‚Ä≤ = g(cedit, {m}), where (cedit, t‚Ä≤) = MLLM(t, m; ‚àáp, F).\n  This enables targeted, incremental refinements, making it particularly effective when a prompt is\n  already strong but requires adjustment on specific attributes rather than a complete redesign.\n\n‚Ä¢ Mix operator. This operator blends the complementary strengths of multiple multimodal prompts.\n  Specifically, it first leverages feedback from multiple prompts to generate a mixing condition cmix,\n  which is then used by the generator to combine non-textual prompts as follows:\n         m‚Ä≤ = g(cmix, {mi}Ki=1), where (cmix, t‚Ä≤) = MLLM({ti, mi; ‚àápi, Fi}Ki=1).\n By synthesizing multiple candidates, it yields balanced compositions, avoids over-reliance on a\n  single candidate, and enables exploration of intermediate solutions better than individual ones.\n\n3.3  EFFECTIVE PROMPT SELECTION BY PRIOR-INHERITED BAYESIAN UCB\n\nAnother challenge in multimodal prompt optimization is to identify which candidates should be pri-\noritized for evaluation and carried forward. Yet, this step is non-trivial with the enlarged multimodal\nspace, since high-quality prompts become relatively sparse, and a large portion of the evaluation\nbudget risks being wasted on low-potential candidates. Existing approaches typically adopt either\n(i) uniform allocation, where each candidate is evaluated equally regardless of its prior likelihood\nof success (Zhou et al., 2023; Cui et al., 2025), or (ii) bandit-based allocation, such as UCB (Auer,\n2002; Pryzant et al., 2023), which adaptively balances exploration and exploitation. However, both\nparadigms suffer from an inefficient cold-start problem: newly generated prompts are treated as in-\ndependent arms with no prior information, leading to unproductive evaluations in the early rounds.\n\n\n\n                                                                                                                   = 0.88Parent-Child Correlation  We address this cold-start inefficiency by             1.0   CorrR2 = 0.78\nintroducing informative priors that warm-start the evaluation process. In              Score 0.8   P-value = 3.33e-170\nparticular, our hypothesis is that the performance of a parent prompt is                 Prompt 0.6\npositively correlated with that of its children. To test this, we analyze the             0.4                                                                                                                                                                                                                                                             Child\noptimization trajectory, measuring the correlation between the perfor-             0.2                                                                                                                                                                                                          Avg.\nmance of parent prompts and the average performance of their children.             0.0\nAs shown in Figure 3, we observe a strong positive correlation (Pear-                       0.2Parent0.4Prompt0.6 Score0.8    1.0\nson‚Äôs r = 0.88), providing concrete evidence that parent scores could  Figure 3: Correlation of\nserve as highly informative priors for estimating child performance.       parent and child scores.\n\nPrior-Inherited Bayesian UCB  Motivated by this finding, we propose prior-inherited Bayesian\nUCB, a selection strategy that initializes the score distribution of a new child prompt based on the\nposterior of its parent (rather than uniform).  Specifically, we model the expected score of each\nmultimodal prompt pi as a Beta distribution, Beta(Œ±i, Œ≤i), where Œ±i and Œ≤i correspond to (pseudo-)\ncounts of successful and failure outcomes, respectively. Then, for a child prompt pi originated from\na parent prompt ppar(i), we initialize its prior proportionally to the posterior mean performance of\nthe parent ÀÜ¬µpar(i), scaled by a prior strength hyperparameter S > 0, formalized as follows:\n                                                                                Œ±par(i)\n  Œ±i = ÀÜ¬µpar(i) ¬∑ S + 1,  Œ≤i = (1 ‚àíÀÜ¬µpar(i)) ¬∑ S + 1,  where   ÀÜ¬µpar(i) =                           .   (1)\n                                                                           Œ±par(i) + Œ≤par(i)\n\nThis prior-inherited mechanism provides S pseudo-observations to newly generated child prompts,\neffectively warm-starting the evaluation process. With a fixed total budget, it then proceeds itera-\ntively: at each round, we select the prompt with the highest UCB score (an upper quantile of its Beta\nposterior), evaluate it on a small batch of data, and update its posterior parameters Œ±i and Œ≤i. Once\nthe budget is exhausted, the candidate prompt with the highest expected score is selected as the new\nparent for the next iteration of optimization. Please refer to Algorithm 2 for the complete procedure.\nThe following proposition guarantees that our proposed selection strategy leverages an informative\nparent prior (better than random chance) to accelerate the selection of the best-promising prompt.\nProposition 3.1. (Fewer Pulls via Prior-Inherited Bayesian UCB) With the prior of Equation 1,\nand if the prior is more informative than uniform (Ei d(¬µi, ÀÜ¬µpar(i)) ‚àíd(¬µi, 12) ‚â§0), the best-arm\nidentification cost of Bayesian UCB is nonincreasing, where d(p, q) is the Bernoulli KL divergence.\n\n\n                                       5\n\nTable 1: Main Results. Comparison of MPO with manual prompting, few-shot prompting, and\ntext-only APO baselines on diverse benchmarks across image, video, and molecular modalities.\nResults are averaged over three independent runs. * denotes the average performance across multiple\nsubtasks within the benchmark. Avg. denotes the average accuracy over all datasets except F1.\n\n                                  Image                              Video                      Molecule\n\n                  PlantVillage*  CUB*  SLAKE*  DrivingVQA  RSVQA   Drive&Act  VANE.   Absorption*    BBBP    CYP Inhibit.*\n\n Methods          Acc.       Acc.     Acc.        Acc.       Acc.       Acc.      Acc.   Acc.   F1   Acc.   F1   Acc.   F1    Avg.\n\n Human            42.2       47.9     35.2        49.7        51.0       47.3       47.0    38.5   36.3   39.4   38.6   43.1    37.1    44.1\n CoT               43.1       49.0     30.8        52.9        49.6       37.2       31.6    39.6   36.7   33.6   32.5   40.1    32.3    40.8\n\n 1-Shot             39.7       54.7     31.4        54.5        48.5       50.4       62.4    37.8   35.7   36.1   34.8   56.2    48.3    47.2\n 3-Shot             48.2       58.8     30.6        53.9        52.2       54.2       56.0    46.1   44.2   42.7   42.6   51.9    47.3    49.5\n 5-Shot             46.5       58.1     28.0        45.9        49.2       54.3       61.4    48.1   45.5   49.3   49.3   52.0    47.0    49.3\n\n APE               55.8       67.3     34.3        52.8        54.4       50.3       64.3    45.7   40.4   36.0   34.7   52.3    50.9    51.3\n OPRO             54.1       59.7     33.9        52.7        51.0       46.4       51.0    37.6   35.4   39.2   38.3   43.0    37.1    46.9\n EvoPrompt         56.1       59.6     34.8        52.9        50.5       46.7       56.5    48.2   46.5   38.7   37.7   51.1    49.7    49.5\n PE2               67.9       71.6     35.8        53.7        55.2       50.8       63.0    64.5   56.8   61.3   58.2   58.5    55.1    58.2\n ProTeGi           64.4       70.0     35.4        54.4        54.2       53.0       65.5    71.1   58.2   72.1   65.7   59.8    57.0    60.0\n SEE               69.0       71.6     35.0        52.2        53.4       51.7       57.9    71.4   60.0   67.0   62.3   61.4    56.7    59.1\n MPO (Ours)       76.4       78.6     38.2        56.0        55.9       58.3       71.2    76.7   64.5   75.3   67.6   64.3    60.2    65.1\n\n\nThe proof and detailed analysis are provided in Appendix B. Intuitively, this guarantee demonstrates\nthat informative parent priors accelerate the discovery of high-quality prompts by reducing wasted\nevaluations on low-potential candidates, which is particularly beneficial for multimodal prompt op-\ntimization, where the combinatorial search space is far larger than text-only settings. In other words,\nby rapidly eliminating unpromising candidates and reallocating the budget toward more promising\nregions, our method enables efficient exploration of the vast multimodal prompt landscape.\n\n4  EXPERIMENTS\n\n4.1  EXPERIMENTAL SETUP\n\nDatasets  We conduct an extensive evaluation on MPO across a diverse set of modalities, including\nimages, videos, and molecules. For the image modality, we consider both image classification and\nvisual question answering (VQA) tasks. Specifically, we use PlantVillage (Mohanty et al., 2016) for\ndiseased leaf identification and CUB-200-2011 (Wah et al., 2011) for fine-grained bird classification;\nmeanwhile, for VQA, we evaluate on SLAKE (Liu et al., 2021), RSVQA (Lobry et al., 2020), and\nDrivingVQA (Corbi`ere et al., 2025), which cover radiology, remote sensing, and dynamic driving\nscenes, respectively. For the video modality, we evaluate on Drive&Act (Martin et al., 2019) for\ndriver action recognition and VANE-Bench (Gani et al., 2025) for abnormality detection in video-\nbased VQA. Finally, for the molecular modality, we include three different property prediction tasks\nfrom TDC (Huang et al., 2021), namely, Absorption (Hou et al., 2007; Ma et al., 2008; Broccatelli\net al., 2011; Siramshetty et al., 2021), BBBP (Martins et al., 2012), and CYP inhibition tasks (Veith\net al., 2009). Detailed configurations for each dataset are provided in Appendix A.1.\n\nBaselines  We benchmark MPO against both manually designed prompts and representative auto-\nmatic prompt optimization methods. For manual prompting, we include Human, a simple hand-\ncrafted prompt, Chain-of-Thought (CoT) (Wei et al., 2022), which uses the widely adopted phrase\n‚ÄúLet‚Äôs think step by step,‚Äù and Few-Shot, which supplies in-context examples drawn from the train-\ning data. For automatic methods, we compare against leading LLM-based text-only optimizers,\nincluding APE (Zhou et al., 2023), OPRO (Yang et al., 2024), EvoPrompt (Guo et al., 2024),\nPE2 (Ye et al., 2024), ProTeGi (Pryzant et al., 2023), and SEE (Cui et al., 2025). Detailed descrip-\ntions of all baselines are provided in Appendix A.2.\n\nImplementation Details  For answer generation, we use Qwen2.5-VL (7B) (Bai et al., 2025) as\nthe base model for image and video tasks, and Qwen3 (8B) (Yang et al., 2025) for molecular tasks.\nDuring optimization, GPT-4o mini (OpenAI, 2024) serves as the prompt optimizer, responsible for\nanalyzing failures and refining multimodal prompts. For modality-specific generation, we employ\nGPT-Image (OpenAI, 2025) for images, Wan2.1 (1.3B) (Wan et al., 2025) for videos, and again\nGPT-4o mini for molecules. For the implementation of the iterative optimization loop, we use the\nbeam search (Pryzant et al., 2023) with the beam size of b = 3 and the number of iterations of\nT = 13. Also, at each iteration (except the first), b2 child prompts are produced by evenly applying\nthe generation, edit, and mix operators, after which the top-b prompts are selected via prior-inherited\nBayesian-UCB. Meanwhile, in the first iteration, only the generation operator is used to initialize\n\n\n                                       6\n\nTable 2: Generalizability results of MPO across components                                                                                   (%)\nwith different backbones: (Top) base models; (Bottom Left) op-     60\ntimizer models; (Bottom Right) modality-specific generators.\n         Qwen2.5-VL (72B)  Gemma3 (12B)   InternVL-3.5 (14B)  GPT-4.1 nano       50\n\n Human           55.7               45.6                51.6               46.8 1-shot            66.8               56.7                34.7               46.1                               Improvement 40\n 3-shot            69.6               64.6                36.5               37.7\n 5-shot            72.3               68.9                34.9               42.6                                                                               30            MPO (Ours)\n ProTeGi          74.1               68.2                71.9               61.0                                     Sequential\n SEE              73.6               68.1                70.8               61.6                             Random Image Prompt\n                                                                                                                                                      In-Distribution Image Query MPO            80.4               73.1                73.2               65.9                               Performance 20            OOD Image Query\n Optimizer Model  SEE  MPO      T2I Generator     PlantVillage*                  0.2         0.4         0.6                                                                               Alignment Score (DSG)\n                                SEE (Text-only)         69.0\n Qwen2.5-VL (7B)   65.2    69.1                                 Figure 4: Relationship between\n                                                     (1.6B)                                                                    71.8  Gemini 2.5 Flash   68.2    74.8      SANA1.5                                                        cross-modal alignment and per-                                    Nano Banana                                                                    72.9\n   GPT-4o mini     69.0    76.4      GPT-Image-Low         76.4     formance gain. We report me-\n     GPT-4o        69.2    78.0     GPT-Image-Medium       76.6      dian values alongside Q1 and Q3.\n\n\nmultimodal prompts, since no non-textual prompts exist yet. The complete optimization process is\nsummarized in Algorithm 1. To ensure fairness, we keep the number of explored prompts consistent\nacross all methods. In our case, each candidate prompt is allocated an evaluation budget of 100, and\nthe prior strength for our prior inheritance is set to 10% of this budget (S = 10). Reported results\nare averaged over three independent runs. Please see Appendix A.3 for additional details.\n\n4.2  EXPERIMENTAL RESULTS AND ANALYSES\n\nMain Results  As shown in Table 1, MPO consistently outperforms all baselines across image,\nvideo, and molecular domains, confirming its effectiveness in discovering prompts that more effec-\ntively harness the capabilities of MLLMs. Specifically, compared to existing text-only optimization\nmethods, MPO achieves substantial gains, demonstrating that incorporating non-textual signals into\nprompts provides stronger contextual grounding and enhances task-specific reasoning. Moreover,\nMPO outperforms exemplar-based Few-Shot prompting, showing that it can capture richer cross-\nmodal information and its underlying dependencies beyond simple query‚Äìanswer demonstrations.\nIn both image and video domains, MPO performs strongly on classification and QA tasks, underscor-\ning its robustness across diverse real-world scenarios. Likewise, on molecular tasks, MPO surpasses\nall baselines, highlighting its effectiveness in highly specialized applications.\n\nGeneralizability to Diverse Backbone Models  We further validate the generalizability of MPO\nby varying the backbone models used in each component, namely, base models, optimizer models,\nand modality-specific generators, and assessing its robustness under these variations. First, as shown\nin Table 2 (Top), MPO maintains strong performance across different architectures and exhibits even\ngreater effectiveness as model size increases, for example, with Qwen2.5-VL (72B). Also, Table 2\n(Bottom Left) further shows that MPO remains effective regardless of the optimizer model, surpass-\ning state-of-the-art text-only methods (e.g., SEE) under diverse backbone models for optimization.\nFinally, Table 2 (Bottom Right) demonstrates that MPO generalizes well to modality-specific gen-\nerators, including lightweight open-source models such as SANA1.5 (1.6B), where it continues to\noutperform textual optimization methods. These results highlight MPO as a broadly generalizable\nand robust framework, effective across a wide variety of base models and practical scenarios.\n\nAnalysis on Cross-Modal Alignment  Recall that MPO uses the alignment-preserving exploration\nto jointly refine textual and non-textual components of multimodal prompts, and we further analyze\nhow this cross-modal alignment strategy contributes to performance gains. To isolate this effect, we\nconsider four variants: (1) Sequential, where the textual prompt is optimized first and the non-textual\nprompt is refined afterward; (2) Random Image Prompt, where the image component is replaced\nwith another optimized image prompt (i.e., not jointly optimized with the text); (3) In-Distribution\nImage Query, where it is replaced with an image sampled from the same task; and (4) OOD Image\nQuery, where it is replaced with an image sampled from a different task. After that, we measure\nthe relationship between performance gain over the Human baseline and the DSG score designed to\nquantify alignment (Cho et al., 2024). As shown in Figure 4, MPO achieves both the highest align-\nment score and the largest performance gains, followed by Sequential optimization and Random\nImage Prompt, while In-Distribution and OOD Image Query lag significantly behind. These results\n\n\n                                       7\n\nTable 3:  Ablation on the con-  Table 4: Ablation on three explo-            MPO (Ours)\n                                                                                           76       w/o Priortribution of each modality in the  ration operators,  utilizing each            UCB\n                                                                                                                     Uniform     42% savingsoptimized multimodal prompt.   one of them individually.                      Score 75                                                                                                   52% savings\n   Text  Image PlantVillage* CUB*              Apple Corn Grape Potato Avg.      74                                                                                                                                                                                                                                 Average             70% savings\n                                  SEE        76.4  75.9  48.0   75.7  69.0\n Human     -        42.2      47.9                                                    73\n                                             Generation  76.9  77.9  53.7   83.6  73.3\n Human MPO      50.4      58.2                                                       25       50       75       100\n                                                Edit         77.2  76.3  56.2   80.1  72.5          Evaluation Budget per Prompt\n  MPO      -        55.6      64.2    Mix         74.0  77.9  65.1   79.8  74.8                                                               Figure 5: Efficiency compari-\n  MPO MPO      76.4      78.6   MPO (Full)  77.7  78.2  65.9   84.0  76.4  son of selection strategies.\n\n\n\n                     Gen                      Mix                         Mix                          Edit\n\n\n\n                                                                                                                 Task Classes\n\n\n              Edit                           Mix                Gen\n\n\n\nFigure 6: Image prompt optimization process of the best-performing multimodal prompt on a\nsubtask (i.e., grosbeak species classification) of CUB. ‚ÄúTask Classes‚Äù box contains the examples of\nfour species: Rose Breasted Grosbeak, Pine Grosbeak, Blue Grosbeak, and Evening Grosbeak.\n\n\nconfirm that stronger cross-modal alignment directly translates to better task performance, and that\nalignment-preserving updates (included in MPO) are crucial in promoting modality consistency.\n\n\nAblation on Modality Contributions in Prompts  To examine the contribution of each modality\nwithin optimized prompts, we ablate the textual and non-textual components from the final multi-\nmodal prompt. As shown in Table 3, using only a single modality (either MPO text without image\nor human text combined with MPO image) already surpasses the Human baseline, confirming that\nboth modalities independently provide useful signals. However, the full multimodal prompt yields\nsubstantially higher performance, demonstrating that the two modalities are not merely additive but\nmutually reinforcing, which underscores the importance of jointly leveraging textual and non-textual\ninformation to achieve performance gains beyond what either modality can deliver alone.\n\n\nEffect of Exploration Operators  To assess the contribution of the proposed exploration operators\n(such as generation, edit, and mix), we conduct both qualitative and ablation analyses. Qualitatively,\nas illustrated in Figure 6, we observe that each operator serves a distinct role: the generation opera-\ntor introduces novel visual compositions, the edit operator fine-tunes local features such as textures\nor visual characters, and the mix operator blends broader attributes such as background or spatial\nlayout. In addition to this, the ablation study in Table 4 further confirms their complementary ef-\nfects: while each operator individually improves over the baseline, combining all three within MPO\nleads to the best performance. This demonstrates that the proposed operators jointly enable a more\ncomprehensive exploration of the multimodal prompt space, facilitating the discovery of the optimal\nprompts. We observe a similar pattern in molecular prompt optimization, shown in Figure 14, with\nconcrete examples of operator-driven updates (including textual conditions) provided in Table 8.\n\n\nSelection Strategies  We evaluate the effectiveness of our prior-inherited Bayesian UCB strategy\nfor candidate prompt selection by comparing it against three alternatives: Uniform, which distributes\nthe evaluation budget evenly across candidates; UCB (Auer, 2002), a standard bandit algorithm; and\nan ablated variant of ours w/o Prior. As shown in Figure 5, MPO achieves the same performance\nas the Uniform strategy while using only 30% of the evaluation budget, yielding a 70% reduction\nin resource cost. Moreover, MPO consistently outperforms both UCB and w/o Prior, reaching their\nperformance levels with 52% and 42% less budget, respectively. These results confirm that the warm\nstart enabled by prior inheritance is crucial for both efficiency and accuracy, allowing MPO to scale\neffectively over the enlarged multimodal search space and reliably identify high-quality prompts.\n\n\n                                       8\n\n80             2  10                                     77\n                                              5 Score\n                                                                                    76    75                                              0                                                              Score Test                            +6.4                                                                                                                                      Component\n                                              5                                     75    70                                                                                                                                                                                                               Average                               +1.1          10     MPOMPO (T+I)(T)         APEOPRO(T)(T)\n                    MPO                                    (Ours) Average                                                           SEE                                                                                            (T)                                                                                   EvoPrompt                                                                                                                         (T)         74    65                             ProTeGi                                                                                                                                      Principal  15                                                                        ProTeGi                                                                                                (T)                                                                             PE2 (T)\n\n        0        4        8       12                   10     0      10     20           0     5    10    15    20    25\n         Number of Iterations                     Principal Component 1                     Prior Strength (S)\nFigure 7: Train Curve of MPO  Figure 8: Visualization of hid-  Figure 9: Analysis of the prior\ncompared to ProTeGi on CUB.  den states in MLLMs by PCA.  strength (S) on performance.\n\n\nTrain Dynamics of MPO  To better understand how MPO improves over the course of optimiza-\ntion, we analyze its training dynamics in comparison to ProTeGi by tracking the test performance\nof the top-1 prompt on the CUB dataset. As shown in Figure 7, both methods improve during the\nearly iterations; however, ProTeGi quickly plateaus after the third iteration, with only a marginal\nadditional gain of 1.1 points. In contrast, MPO continues to improve steadily, ultimately achieving\na much higher final score, including an additional 6.4-point gain beyond the third iteration. This\ncomparison result highlights that MPO effectively overcomes the performance ceiling of text-only\noptimization methods by effectively navigating the multimodal prompt space, enabling it to escape\nlocal optima (imposed by the text-only strategy) and discover prompts closer to the global optimum.\n\nHidden State Visualization  To gain deeper insight into why optimized multimodal prompts yield\ngreater performance improvements than text-only prompts, we visualize the hidden state of MLLMs\nby averaging intermediate-layer embeddings, following Zhang et al. (2024a). As shown in Figure 8,\nhidden states obtained from text-only methods (including the text-only component of MPO) cluster\ntogether, suggesting that they guide the reasoning of MLLMs within a similar yet limited semantic\nspace. In contrast, the full multimodal prompt from MPO shifts the hidden states into a distinct re-\ngion, indicating that the non-textual component introduces information unavailable from text alone.\nIn other words, the multimodal prompt alters the internal representation space of models, enabling\nricher reasoning pathways and ultimately leading to superior task performance.\n\nAnalysis of Prior Strength  Recall that in our prior-inherited selection strategy, the prior strength\nS determines the number of pseudo-observations used to initialize the score distributions of child\nprompts, and we study its effect by varying S and reporting the resulting performance. As shown in\nFigure 9, we first observe that a small S under-utilizes the parent prior, resulting in weaker guidance\nand suboptimal performance. In contrast, an excessively large S causes the model to over-rely on the\nparent prior, limiting its ability to adapt to the actual performance of child prompts. Consequently,\nthe performance is maximized at an intermediate S, where inherited knowledge provides a strong\nwarm start while still allowing sufficient flexibility to incorporate new observations.\n\nQualitative Result  We provide qualitative examples for the optimized multimodal prompts for\nthe image modality in Table 9 of Appendix. From this, we observe that the optimized multimodal\nprompts consistently supply task-critical context in both textual and visual forms. Also, more impor-\ntantly, the textual prompts explicitly instruct the model to leverage non-textual signals (e.g., Use the\nhybrid reference image for guidance), thereby unlocking the full multimodal capacity of MLLMs.\nAdditional examples for the video and molecular modalities are presented in Tables 10, 11, and 12.\n\n5  CONCLUSION\n\nWe introduced the novel problem of multimodal prompt optimization, extending the optimization\nspace beyond text to fully leverage the capability of MLLMs. To tackle this, we proposed the Mul-\ntimodal Prompt Optimizer (MPO), a unified framework that jointly refines textual and non-textual\ncomponents through alignment-preserving exploration with multiple generation operations and ef-\nficiently identifies high-quality prompts via a prior-inherited Bayesian UCB strategy. Experiments\nacross diverse modalities (including images, videos, and molecules) demonstrate that MPO consis-\ntently surpasses leading text-only prompt optimization methods, validating its efficacy in diverse\nreal-world multimodal problems. We believe our work establishes multimodal prompt optimization\nas a key direction for advancing the use of MLLMs, moving beyond text-only prompting paradigms.\n\n\n                                       9\n\nETHICS STATEMENT\n\nOur study does not involve human subjects, personally identifiable data, or sensitive information.\nAll experiments were conducted on public datasets and models under research-permissive licenses.\n\n\nREPRODUCIBILITY STATEMENT\n\nWe attach the code to ensure the reproducibility of our work in the supplementary materials. Addi-\ntionally, we provide a detailed description of the experimental setup in Section 4.1. We further pro-\nvide additional implementation details in Appendix A.3, the dataset configuration in Appendix A.1,\nthe meta prompts to operationalize MPO in Appendix A.4, and the full algorithms in Appendix A.5.\n\n\nREFERENCES\n\nPeter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine\n  Learning Research, 2002.\n\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\n   Shijie Wang, et al. Qwen2.5-vl technical report. ArXiv, 2025.\n\nFabio Broccatelli, Emanuele Carosati, Annalisa Neri, Maria Frosini, Laura Goracci, Tudor I Oprea,\n  and Gabriele Cruciani. A novel approach for predicting p-glycoprotein (abcb1) inhibition using\n  molecular interaction fields. Journal of medicinal chemistry, 2011.\n\nJaemin Cho, Yushi Hu, Jason M. Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mo-\n   hit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in\n  fine-grained evaluation for text-to-image generation. In The Twelfth International Conference on\n  Learning Representations, ICLR, 2024.\n\nYunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and\n  Jingren Zhou.  Qwen-audio: Advancing universal audio understanding via unified large-scale\n  audio-language models. ArXiv, 2023.\n\nCharles Corbi`ere, Simon Roburin, Syrielle Montariol, Antoine Bosselut, and Alexandre Alahi.\n  DRIVINGVQA: analyzing visual chain-of-thought reasoning of vision language models in real-\n  world scenarios with driving theory tests. ArXiv, 2025.\n\nWendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley A. Malin,\n  and Kumar Sricharan. SEE: strategic exploration and exploitation for cohesive in-context prompt\n  optimization. In Proceedings of the 63rd Annual Meeting of the Association for Computational\n   Linguistics ACL 2025, 2025.\n\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt¬®aschel.\n  Promptbreeder: Self-referential self-improvement via prompt evolution.  In Forty-first Interna-\n   tional Conference on Machine Learning, ICML, 2024.\n\nHanan Gani, Rohit Bharadwaj, Muzammal Naseer, Fahad Shahbaz Khan, and Salman Khan. Vane-\n  bench: Video anomaly evaluation benchmark for conversational lmms. In Findings of the Associ-\n  ation for Computational Linguistics: NAACL 2025, 2025.\n\nBingjie Gao, Xinyu Gao, Xiaoxue Wu, Yujie Zhou, Yu Qiao, Li Niu, Xinyuan Chen, and Yaohui\n  Wang. The devil is in the prompts: Retrieval-augmented prompt optimization for text-to-video\n  generation. In Conference on Computer Vision and Pattern Recognition, 2025.\n\nTeam Gemini.  Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long\n   context, and next generation agentic capabilities. ArXiv, 2025.\n\nAaron Grattafiori et al. The llama 3 herd of models. ArXiv, 2024.\n\n\n                                       10\n\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,\n  and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\n  prompt optimizers. In The Twelfth International Conference on Learning Representations, ICLR,\n  2024.\n\nTingjun Hou, Junmei Wang, Wei Zhang, and Xiaojie Xu. Adme evaluation in drug discovery. 7.\n  prediction of oral absorption by correlation and classification. Journal of chemical information\n  and modeling, 2007.\n\nKexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Co-\n   ley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning\n   datasets and tasks for drug discovery and development. Proceedings of Neural Information Pro-\n  cessing Systems, NeurIPS Datasets and Benchmarks, 2021.\n\nSongtao Jiang, Yan Zhang, Chenyi Zhou, Yeying Jin, Yang Feng, Jian Wu, and Zuozhu Liu. Joint\n   visual and text prompting for improved object-centric perception with multimodal large language\n  models. ArXiv, 2024.\n\nMuhammad Uzair Khattak, Hanoona Abdul Rasheed, Muhammad Maaz, Salman H. Khan, and\n  Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Conference on Computer Vision\n  and Pattern Recognition, 2023.\n\nYuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, and Lu Yuan.\n  Rethinking visual prompting for multimodal large language models with external knowledge.\n  ArXiv, 2024.\n\nBo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.  Slake: A semantically-\n  labeled knowledge-enhanced dataset for medical visual question answering. In 18th IEEE Inter-\n  national Symposium on Biomedical Imaging, ISBI 2021, 2021.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Annual\n  Conference on Neural Information Processing Systems 2023, NeurIPS, 2023.\n\nSylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. RSVQA: visual question answering\n   for remote sensing data. Transactions on Geoscience and Remote Sensing, 2020.\n\nChang-Ying Ma, Sheng-Yong Yang, Hui Zhang, Ming-Li Xiang, Qi Huang, and Yu-Quan Wei.\n  Prediction models of human plasma protein binding rate and oral bioavailability derived by using\n  ga‚Äìcg‚Äìsvm method. Journal of pharmaceutical and biomedical analysis, 2008.\n\nOscar MaÀúnas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aish-\n  warya Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving text-to-image con-\n  sistency via automatic prompt optimization. Transactions on Machine Learning Research, 2024.\n\nManuel Martin, Alina Roitberg, Monica Haurilet, Matthias Horne, Simon Rei√ü, Michael Voit, and\n  Rainer Stiefelhagen. Drive&act: A multi-modal dataset for fine-grained driver behavior recogni-\n   tion in autonomous vehicles. In International Conference on Computer Vision, ICCV, 2019.\n\nInes Filipa Martins, Ana L Teixeira, Luis Pinheiro, and Andre O Falcao. A bayesian approach to in\n   silico blood-brain barrier penetration modeling. Journal of chemical information and modeling,\n  2012.\n\nWenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, and Qing Yang. Dynamic prompt\n  optimizing for text-to-image generation. In Conference on Computer Vision and Pattern Recog-\n   nition, 2024.\n\nSharada P. Mohanty, David P. Hughes, and Marcel Salath¬¥e. Using deep learning for image-based\n   plant disease detection. Frontiers in Plant Science, 2016.\n\nOpenAI.  Gpt-4v(ision) system card.  2023. URL https://cdn.openai.com/papers/\n  GPTV_System_Card.pdf.\n\nOpenAI. Gpt-4o system card. ArXiv, 2024.\n\n\n                                       11\n\nOpenAI.  Introducing 4o image generation, 2025.  URL https://openai.com/index/\n  introducing-4o-image-generation/.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\n  optimization with ‚Äúgradient descent‚Äù and beam search. In Proceedings of the 2023 Conference\n  on Empirical Methods in Natural Language Processing, EMNLP. Association for Computational\n   Linguistics, 2023.\n\nVishal Siramshetty, Jordan Williams, Dac-Trung Nguyen, Jorge Neyra, Noel Southall, Ewy Math¬¥e,\n  Xin Xu, and Pranav Shah. Validating adme qsar models using marketed drugs. SLAS DISCOV-\n  ERY: Advancing the Science of Drug Discovery, 2021.\n\nHenrike Veith, Noel Southall, Ruili Huang, Tim James, Darren Fayne, Natalia Artemenko, Min\n  Shen, James Inglese, Christopher P Austin, David G Lloyd, et al. Comprehensive characterization\n  of cytochrome p450 isozyme selectivity across chemical libraries. Nature biotechnology, 2009.\n\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds 200. Technical\n   report, California Institute of Technology, 2011.\n\nTeam Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu,\n  Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai\n  Wang, et al. Wan: Open and advanced large-scale video generative models. ArXiv, 2025.\n\nTaowen Wang, Yiyang Liu, James Chenhao Liang, Junhan Zhao, Yiming Cui, Yuning Mao, Shao-\n   liang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, and Dongfang\n  Liu. M2PT: Multimodal prompt tuning for zero-shot instruction learning. In Proceedings of the\n  2024 Conference on Empirical Methods in Natural Language Processing, EMNLP, 2024.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\n  Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\n  models. In Advances in Neural Information Processing Systems 35, NeurIPS, 2022.\n\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\n  Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. ArXiv, 2025.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun\n  Chen. Large language models as optimizers. In The Twelfth International Conference on Learning\n  Representations, ICLR, 2024.\n\nQinyuan Ye, Mohamed Ahmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt\n  engineer. In Findings of the Association for Computational Linguistics, ACL 2024, 2024.\n\nMert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin,\n  and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature,\n  639:609‚Äì616, 2025.\n\nFanhu Zeng, Fei Zhu, Haiyang Guo, Xu-Yao Zhang, and Cheng-Lin Liu.  Modalprompt:dual-\n  modality guided prompt for continual learning of large multimodal models. ArXiv, 2024.\n\nLechen Zhang, Tolga Ergen, Lajanugen Logeswaran, Moontae Lee, and David Jurgens. SPRIG:\n  improving large language model performance by system prompt optimization. ArXiv, 2024a.\n\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal\n  chain-of-thought reasoning in language models.  Transactions on Machine Learning Research,\n  2024b.\n\nQiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-thought\n  prompting for visual reasoning refinement in multimodal large language models. ArXiv, 2024.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\n  Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh Interna-\n   tional Conference on Learning Representations, ICLR, 2023.\n\nJinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen\n  Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for\n  open-source multimodal models. 2025.\n\n\n                                       12\n\nA  ADDITIONAL EXPERIMENTAL DETAILS\n\n\nA.1  DETAILS ON DATASETS\n\nWe provide a detailed description of the datasets used in our experiments. To conduct a comprehen-\nsive evaluation, we compile a diverse set of benchmarks for classification and question-answering\ntasks across various modalities, including images, videos, and molecules. We use the official train-\ning/test splits where available, and if not, we create our own splits. For the image and video modality\ntasks, we sample 300 test examples, whereas for the molecule modality, we use the entire test set.\n\n\nPlantVillage  The PlantVillage dataset (Mohanty et al., 2016) contains 54,306 images of plant\nleaves, spanning 38 disease categories across 14 crop species. To construct a focused, fine-grained\nclassification task, we design subtasks by selecting four crop species, each having at least three\ndistinct classes (e.g., one healthy and two or more diseases). This setup allows for a more controlled\nevaluation of the model‚Äôs ability to identify specific plant diseases. Due to the lack of an official\nsplit, we split this subset using the 50/50 ratio for training and testing.\n\n\nCUB-200-2011  The CUB-200-2011 dataset (Wah et al., 2011) is a standard benchmark for fine-\ngrained bird species classification. To evaluate the capability of MLLMs in distinguishing between\nvisually similar species, we group birds that share a common family name (e.g., ‚ÄúHummingbird‚Äù),\nand select groups containing three or four distinct species to ensure a balanced level of difficulty,\nresulting in a total of 12 subtasks. Then, we divide the samples for each subtask using a 50/50 ratio,\ncurating them to contain at least 80 instances for both training and test.\n\n\nSLAKE  The SLAKE dataset (Liu et al., 2021) is an open-ended visual question answering bench-\nmark tailored for the medical domain from various radiological modalities. To assess the perfor-\nmance of MLLMs across these different modalities, we partition the dataset into distinct subsets\nbased on the modality, creating separate tasks for CT, MRI, and X-Ray images.\n\n\nDrivingVQA  The DrivingVQA dataset (Corbi`ere et al., 2025) is a closed-ended visual question\nanswering benchmark with 3,931 multiple-choice questions based on real-world driving scenarios.\nTo avoid ambiguity in the evaluation process, we filter the dataset to exclusively retain instances\nwith a single correct answer, resulting in a final dataset of 2,039 training and 521 test instances.\n\n\nRSVQA  We use the RSVQA dataset (Lobry et al., 2020) to evaluate performance on the open-\nended visual question answering task for remote sensing images. Notably, the questions are designed\nto evaluate a model‚Äôs understanding of various geospatial concepts, including land cover classifica-\ntion, object counting, and relational reasoning between objects. For our experiments, we utilize the\nlow-resolution image set from the benchmark.\n\n\nDrive&Act  For the video classification task, we use the Drive&Act dataset (Martin et al., 2019),\nwhich provides comprehensive labels for driver behaviors inside vehicles. We adhere to the official\nsplit of 6,642 training and 2,222 test instances, and preprocess the video clips by sampling frames\nat a rate of 1 frame per second (fps).\n\n\nVANE-Bench  The VANE-Bench dataset (Gani et al., 2025) is a closed-ended question answering\nbenchmark for video anomaly detection, whose samples (each with a 10-frame clip from synthetic\nor real-world videos) show various irregularities or distortions. We split the dataset into training and\ntest sets using the 60/40 ratio, resulting in 293 training and 263 test instances.\n\n\nAbsorption  The Absorption task (Huang et al., 2021) is categorized into molecular property pre-\ndiction, designed to evaluate a model‚Äôs ability to estimate pharmacokinetic characteristics related to\ndrug absorption. It is composed of four subtasks: PAMPA (Parallel Artificial Membrane Permeabil-\nity Assay), HIA (Human Intestinal Absorption), Pgp (P-glycoprotein substrate classification), and\nBioavailability, and we use the official random split.\n\n\n                                       13\n\nBBBP  BBBP (Martins et al., 2012) is a molecular classification task to predict whether the given\nmolecule can penetrate the blood-brain barrier (BBB), which is a highly selective system. We use\nthe official random split from Huang et al. (2021), consisting of 1,453 train and 382 test examples.\n\nCYP Inhibit  The CYP Inhibition task (Veith et al., 2009) involves classifying whether a molecule\ncan inhibit Cytochrome P450 (CYP) enzymes, which play key roles in metabolism.  It comprises\nfive subtasks: inhibition of CYP 2C19, CYP 2D6, CYP 3A4, CYP 1A2, and CYP 2C9. We adopt\nthe official random split provided in Huang et al. (2021).\n\n\nA.2  DETAILS ON BASELINES\n\nThis subsection details the baseline methods used in our experiments.\n\n‚Ä¢ APE (Zhou et al., 2023) generates candidate prompts by reverse-engineering instructions from\n  examples and by paraphrasing existing prompts.\n‚Ä¢ OPRO (Yang et al., 2024) leverages the LLM as an optimizer, guiding it with pairs of prompts\n  and their performance scores to generate progressively better instructions.\n‚Ä¢ EvoPrompt (Guo et al., 2024) utilizes an evolutionary algorithm where the LLM performs muta-\n  tion and crossover operations on a population of prompts.\n‚Ä¢ PE2 (Ye et al., 2024) focuses on optimizing the meta-prompt used to steer the LLM optimizer.\n   It provides guidance through a structured template containing detailed task descriptions, context\n  specification, and a step-by-step reasoning format.\n‚Ä¢ ProTeGi (Pryzant et al., 2023) simulates gradient descent for discrete prompts. It uses the LLM\n  to generate natural language critiques based on prompt failures (termed ‚Äútextual gradients‚Äù), and\n  subsequently edits the prompt in the opposite semantic direction.\n‚Ä¢ SEE (Cui et al., 2025) performs cohesive optimization of both the prompt instructions and the in-\n  context examples. The method follows a four-phase process that strategically alternates between\n  global exploration and local exploitation.\n\n\nA.3  ADDITIONAL IMPLEMENTATION DETAILS\n\nIn this subsection, we provide the additional implementation details in our experiments. Regarding\nmodel temperature, we use a temperature value of 0 for the base model to ensure consistency and\n0.7 for the optimizer model to encourage the generation of diverse candidate prompts. The failure\nset size in the cohesive backpropagation process is fixed at 3. While the evaluation budget is gen-\nerally set to 100, for CUB subtasks with fewer than 100 training samples, the budget for our MPO\nmethod is specifically set to one-third of the available instances. For modality-specific handling, we\nimplement several strategies. In the video task, when the video query is part of the failure set, we\nsample three representative frames (first, middle, and last) from queries. In video generation, to mit-\nigate the high complexity of video editing and mixing, we employ only the generation operator. We\ngenerate 5-second videos at 16 fps, then downsample them to 5 frames at 1 fps to construct the video\nprompt. For the molecule tasks, we represent chemical structures using the 1D representation (i.e.,\nSMILES) and utilize GPT-4o mini for the molecule generator. Regarding optimization objectives,\nwe use accuracy for image and video modalities, and F1 for the molecular modality to handle the\nclass imbalance. Finally, to measure answer correctness, we adopt task-specific evaluation criteria:\nthe final predefined label is extracted for standard classification, strict formatting rules are applied\nfor binary and closed-ended QA tasks, and exact match is used for open-ended QA tasks. We select\nthe best-performing prompts on the training set and report their performance on the test set. Our\nexperiments are conducted on NVIDIA H100 80GB GPUs.\n\n\n\n\n\n                                       14\n\nA.4  META PROMPTS TO IMPLEMENT MPO\n\nThis subsection details the meta-prompts to instantiate MPO, which include a cohesive backpropa-\ngation prompt and three operator prompts (generation, edit, mix) for update. We provide the meta\nprompt from image modality as a representative example. The prompts for other modalities, such as\nvideo and molecule, are based on this structure, with minor, modality-specific wordings adjusted.\n\n   Prompt for Cohesive Backpropagation\n\n    You are a Prompt Failure Analysis Agent specialized in multimodal prompt optimization. Your task is to\n     analyze the failure case of a Multimodal Large Language Model (MLLM) and identify the potential\n    reasons in the prompt for the model‚Äôs incorrect prediction. Based on the given input, output, and\n    ground truth, analyze both the Text Prompt and the Image Prompt used in the task.\n\n    ### Input Structure for MLLM:\n    - Text Prompt: A task-specific textual instruction for the MLLM.\n    - Image Prompt: A reference image that supports task understanding.\n    - Input Query: The actual target instance (text, image, or both) on which the MLLM must generate an\n    answer.\n\n    ### Prompts:\n    - Text Prompt : {text_prompt}\n    - Image Prompt : {modality_prompt}\n\n    ### Wrong Examples:\n    {wrong_examples}\n\n    ### Output Format:\n    Text Prompt Analysis:\n    - Identify missing information, vague instructions, or ambiguous wording that could have misled the\n    model.\n    - Explain how weaknesses in the Text Prompt may have contributed to the wrong output.\n    - Suggest specific improvements (e.g., clearer task definition, additional constraints, better\n    examples) to help the model produce the correct answer.\n\n    Image Prompt Analysis:\n    - If an image Prompt was used, analyze its effectiveness.\n    - Identify problems such as lack of clarity, poor composition, irrelevant details, or missing key\n    features.\n    - If no image Prompt was used, suggest what kind of image (visual content, attributes, composition)\n    would help correct the failure.\n\n\n                Figure 10: Meta Prompt for Cohesive Backpropagation in MPO.\n\n   Prompt for Generation Operator\n\n    You are a Prompt-Improvement Agent specializing in multimodal prompt optimization. Your task is to\n    design improved prompts for both image generation and text instruction, aimed at enhancing the\n    performance of Multimodal Large Language Model (MLLM).\n\n    ### Input Structure for MLLM:\n    - Text Prompt: A task-specific textual instruction for the MLLM.\n    - Image Prompt: A reference image that supports task understanding.\n    - Input Query: The actual target instance (text, image, or both) on which the MLLM must generate an\n    answer.\n\n    ### Provided Material\n    - Text Prompt: {text_prompt}\n    - Image Prompt: {modality_prompt}\n    - Wrong Examples: {wrong_examples}\n    - Failure Analysis: {analysis}\n\n    ### Your Task\n    Your task is to review the failure analysis carefully to understand the issues and create two improved\n     prompts that directly address the issues in the failure analysis:\n    1. Image Generation Prompt\n      - Write a detailed prompt for an image generator.\n      - Enhance or redesign the reference image to resolve issues found in the analysis.\n      - Ensure the image highlights critical visual features necessary for success.\n       - If no reference image is provided, suggest an appropriate one based on the failure analysis.\n\n    2. Improved Text Prompt\n      - Write a clear, concise, and unambiguous instruction for the MLLM.\n      - Resolve ambiguities found in the failure analysis.\n      - Elaborate on how the reference image should be interpreted.\n\n    ### Output Format\n    <image_generation_prompt>{image_generation_prompt}</image_generation_prompt>\n    <improved_text_prompt>{improved_text_prompt}</improved_text_prompt>\n\n\n                   Figure 11: Meta Prompt for Generation Operator in MPO.\n\n\n\n\n                                       15\n\nPrompt for Edit Operator\n\nYou are a Prompt-Improvement Agent specializing in multimodal prompt optimization, with a focus on\nprompt editing. Your task is to design improved prompts for both image editing and text instruction,\naimed at enhancing the performance of Multimodal Large Language Model (MLLM).\n\n### Input Structure for MLLM:\n- Text Prompt: A task-specific textual instruction for the MLLM.\n- Image Prompt: A reference image that supports task understanding.\n- Input Query: The actual target instance (text, image, or both) on which the MLLM must generate an\nanswer.\n\n### Provided Material\n- Text Prompt: {text_prompt}\n- Image Prompt: {modality_prompt}\n- Wrong Examples: {wrong_examples}\n- Failure Analysis: {analysis}\n\n### Your Task\nYour task is to review the failure analysis carefully to understand the issues and create two improved\n prompts that directly address the issues in the failure analysis:\n1. Image Editing Prompt:\n  - Write a precise and context-aware prompt instructing the image editor to modify the given\nreference image.\n  - Specify which visual components (e.g., objects, colors, textures, lighting, perspective,\ncomposition) should be added, removed, or replaced based on the failure analysis.\n  - Clearly identify any undesirable visual elements that led to the failure.\n  - Guide the editor on how to retain key features, proportions, or stylistic elements that are\ncritical to the intended outcome.\n\n2. Improved Text Prompt\n  - Write a clear, concise, and unambiguous instruction for the MLLM.\n  - Resolve ambiguities found in the failure analysis.\n  - Elaborate on how the reference image should be interpreted.\n\n### Output Format\n<image_edit_prompt>{image_edit_prompt}</image_edit_prompt>\n<improved_text_prompt>{improved_text_prompt}</improved_text_prompt>\n\n\n                   Figure 12: Meta Prompt for Edit Operator in MPO.\n\n Prompt for Mix Operator\n\nYou are a Prompt-Improvement Agent specializing in multimodal prompt optimization, with a focus on\ncross-prompt fusion. Your task is to create improved, mixed prompts for both image prompt and text\ninstruction, aimed at enhancing the performance of Multimodal Large Language Model (MLLM).\n\n### Input Structure for MLLM:\n- Text Prompt: A task-specific textual instruction for the MLLM.\n- Image Prompt: A reference image that supports task understanding.\n- Input Query: The actual target instance (text, image, or both) on which the MLLM must generate an\nanswer.\n\n### Provided Material\n#### Prompt A\n- Text Prompt A: {text_prompt_A}\n- Image Prompt A: {modality_prompt_A}\n- Wrong Examples from Prompt A: {wrong_examples_A}\n- Failure Analysis for Prompt A: {analysis_A}\n\n#### Prompt B\n- Text Prompt B: {text_prompt_B}\n- Image Prompt B: {modality_prompt_B}\n- Wrong Examples from Prompt B: {wrong_examples_B}\n- Failure Analysis for Prompt B: {analysis_B}\n\n### Your Task\nYour task is to review the failure analysis carefully to understand the issues and create two improved\n prompts that directly address the issues in the failure analysis:\n1. Image Mixing Prompt:\n   - Write a guidance for the image generator to combine and improve both reference images.\n   - Address visual issues identified in both failure analyses.\n   - Guide the model to create a new hybrid image that merges key beneficial visual features from both\n references while mitigating their weaknesses.\n   - Explicitly state which visual elements from each image should be retained, modified, or discarded\n to achieve task success.\n\n2. Improved Text Prompt\n  - Write a clear, concise, and unambiguous instruction for the MLLM.\n  - Incorporate key visual or task-relevant features identified in both failure analysis.\n  - Explain how the reference image should be used to assist the task.\n\n### Output Format\n<image_mixing_prompt>{image_mixing_prompt}</image_mixing_prompt>\n<mixed_text_prompt>{mixed_text_prompt}</mixed_text_prompt>\n\n\n                   Figure 13: Meta Prompt for Mix Operator in MPO.\n\n\n                                    16\n\nA.5  FULL ALGORITHM OF MPO\n\nWe provide the overall algorithm for MPO, with alignment-preserving exploration (including the\noperators) described in Algorithm 1 and the prior-inherited Bayesian UCB selection in Algorithm 2.\n\nAlgorithm 1 MPO: Multimodal Prompt Optimizer\nRequire: Initial prompt (t0, ‚àÖ), Number of iterations T, Beam size b\n    Train dataset Dtr, Metric function f\n 1: p ‚Üê(t0, ‚àÖ), P ‚Üê{p}, C ‚Üê‚àÖ, ÀÜ¬µ ‚ÜêE(q,a)‚àºDtr[f(MLLM(t0, ‚àÖ, q), a)]\n 2: for i = 1..b2 do\n 3:   Fp ‚Üê{(q, a, y) | (q, a)‚àºDtr, y = MLLM(p, q), y Ã∏= a}\n 4:  ‚àáp ‚ÜêMLLM.Feedback(t0, ‚àÖ; Fp)\n 5:    (t‚Ä≤, cgen) ‚ÜêMLLM.Generation(t0, ‚àÖ; ‚àáp, Fp); m‚Ä≤ ‚Üêg(cgen, ‚àÖ)\n 6:   C ‚ÜêC ‚à™{(t‚Ä≤, m‚Ä≤)}\n 7: end for\n 8: P ‚ÜêBayesianUCBSelect(P, C, b)                  ‚ñ∑Select b prompts for next step\n 9: for iter = 1..T do\n10:   C ‚Üê‚àÖ\n11:    for all p = (t, m) ‚ààP do\n12:      for i = 1..b do\n13:      Fp ‚Üê{(q, a, y) | (q, a)‚àºDtr, y = MLLM(p, q), y Ã∏= a}\n14:     ‚àáp ‚ÜêMLLM.Feedback(t, m; Fp)               ‚ñ∑Cohesive backpropagation\n15:      op ‚ÜêRandomSample({generation, edit, mix})        ‚ñ∑Joint multimodal update\n16:           if op = generation then\n17:             (t‚Ä≤, cgen) ‚ÜêMLLM.Generation(t, m; ‚àáp, Fp); m‚Ä≤ ‚Üêg(cgen, ‚àÖ)\n18:         else if op = edit then\n19:             (t‚Ä≤, cedit) ‚ÜêMLLM.Edit(t, m; ‚àáp, Fp); m‚Ä≤ ‚Üêg(cedit, {m})\n20:         else if op = mix then\n21:              Àúp ‚ÜêRandomSample(P \\ {p})\n22:             (t‚Ä≤, cmix) ‚ÜêMLLM.Mix((t, m; ‚àáp, Fp),  (Àút, Àúm; ‚àáÀúp, FÀúp)); m‚Ä≤ ‚Üêg(cmix, {m, Àúm})\n23:       end if\n24:       C ‚ÜêC ‚à™{(t‚Ä≤, m‚Ä≤)}\n25:     end for\n26:   end for\n27:  P ‚ÜêBayesianUCBSelect(P, C, b)                ‚ñ∑Select b prompts for next step\n28: end for\n                                                           Œ±p\n29: return p‚àó‚â°(t‚àó, m‚àó) where p‚àó= arg maxp‚ààP ÀÜ¬µp,   ÀÜ¬µp = Œ±p+Œ≤p\n\n\nAlgorithm 2 Prior-Inherited Bayesian UCB Selection\nRequire: Parent Prompts P, A set of k child prompts C = {pi}ki=1, Beam Size b\n    Parent‚Äôs performance {ÀÜ¬µpar(i)}ki=1, Train dataset Dtr, Metric function f, Batch size B\n    Total evaluation budget N, Prior strength S, Exploration parameter c\n 1: Initialize Beta priors for each child prompt pi ‚ààP:\n 2: for i = 1, . . . , k do\n 3:   Œ±i ‚ÜêÀÜ¬µpar(i) ¬∑ S + 1 ,   Œ≤i ‚Üê(1 ‚àíÀÜ¬µpar(i)) ¬∑ S + 1             ‚ñ∑Inherit prior from parent\n 4: end for\n 5: for t = 1, 2, . . . , (N/B) do\n 6:    qt ‚Üê1 ‚àí t(log1N)c\n 7:    j ‚Üêarg maxi‚àà{1,..,k} BetaQuantile(qt; Œ±i, Œ≤i)   ‚ñ∑Choose prompt with highest UCB\n 8:   Dmini ‚ÜêSample(Dtr, B)\n 9:    st ‚ÜêE(q,a)‚àºDmini[f(MLLM(t, m, q), a)]               ‚ñ∑Evaluate on small data batch\n10:   Œ±j ‚ÜêŒ±j + st ¬∑ B ,   Œ≤j ‚ÜêŒ≤j + (1 ‚àíst) ¬∑ B                   ‚ñ∑Update posterior\n11: end for\n12: Return top-b prompts from P ‚à™C sorted by posterior mean ÀÜ¬µi = Œ±i+Œ≤iŒ±i\n\n\n\n\n                                       17\n\nB  THEORETICAL ANALYSIS ON PRIOR-INHERITED BAYESIAN UCB\n\nIn this section, we provide the proof for Proposition 3.1, starting with the formal problem setting.\n\nSetting.  Let each arm i ‚àà{1, . . . , k} have an unknown Bernoulli mean reward ¬µi ‚àà(0, 1) and\nlet i‚ãÜ‚ààarg maxi ¬µi be an optimal arm. Write the suboptimality gap as ‚àÜi = ¬µi‚ãÜ‚àí¬µi > 0\nfor i Ã∏= i‚ãÜ. As shown in algorithm 2, the Bayesian UCB algorithm maintains a Beta posterior\ndistribution for each arm‚Äôs mean reward. At each round t, Bayesian UCB selects the arm with\nthe highest upper posterior quantile, qt, observes the resulting Bernoulli reward, and updates the\ncorresponding posterior.\n\nPrior inheritance  For each child arm i, we initialize a Beta prior using the parent‚Äôs posterior mean\nÀÜ¬µpar(i) ‚àà(0, 1) and a pseudo-count S > 0:\n                       Œ±0,i = ÀÜ¬µpar(i) S + 1,      Œ≤0,i = (1 ‚àíÀÜ¬µpar(i)) S + 1.                    (2)\nFor comparison, a uninformative (or uniform) prior is Beta(1, 1).  After Ni(t) pulls with Xi(t)\nsuccesses by time t, the posterior parameters are Œ±i(t) =  Œ±0,i + Xi(t) and Œ≤i(t) =  Œ≤0,i +\nNi(t) ‚àíXi(t). Denote the posterior mean ÀÜ¬µt,i = Œ±i(t)/(Œ±i(t) + Œ≤i(t)), the upper quantile qt,i =\nBetaQuantile(qt; Œ±i(t), Œ≤i(t)), and the lower quantile ‚Ñìt,i = BetaQuantile(1 ‚àíqt; Œ±i(t), Œ≤i(t)).\n\nAverage KL-closeness assumption.  Our analysis relies on the assumption that a parent‚Äôs posterior\nprovides a useful inductive bias for its children. We formalize this concept using the Kullback-\nLeibler (KL) divergence for Bernoulli distributions, defined as d(p, q) = p log pq + (1 ‚àíp) log 1‚àíp1‚àíq .\nLet I be the population of child arms produced during optimization. We assume the parent estimate\nis, on average over children, KL-closer to the truth than the mean of the uninformative prior:\n                Ei‚àºI d ¬µi, ÀÜ¬µpar(i) ‚àíd ¬µi, 12  ‚â§‚àíŒ≥   for some Œ≥ > 0.                 (3)\nThe assumption is empirically supported by the strong positive correlation observed between parent\nand child scores (Figure 3).\n\nB.1  TWO AUXILIARY LEMMAS\n\nLemma B.1 (Pseudo-counts shrink one-sided credible widths). There exists a universal constant\nc > 0 such that for all t ‚â•2 and all arms i,\n\n             s             s                                    log t                             log t\n                     qt,i ‚àíÀÜ¬µt,i ‚â§c                    ,        ÀÜ¬µt,i ‚àí‚Ñìt,i ‚â§c                    .              (4)\n                               Ni(t) + S                      Ni(t) + S\n\nThe key implication is that the credible interval width scales with 1/pNi(t) + S rather than\n1/ pNi(t). Thus, the prior strength S acts as an additive effective sample size, shrinking the interval\nas if we had S additional observations.\n\nproof sketch. The proof relies on standard concentration bounds for Beta posteriors. The conjugacy\nof the Beta-Binomial model makes the posterior tractable, allowing for the specific application of a\nChernoff tail bound. For any Œµ ‚àà(0, 1), the probability that the upper posterior quantile underesti-\nmates the true mean by at least\n               P{ qt,i ‚â§¬µi ‚àíŒµ } ‚â≤exp ‚àí(Ni(t) + S) d(¬µi ‚àíŒµ, ¬µi)  ,                  (5)\nwith a symmetric bound holding for the lower quantile ‚Ñìt,i. The result is obtained by using the\napproximation d(¬µi ‚àíŒµ, ¬µi) ‚â≥Œµ2 for small Œµ and selecting the quantile level 1‚àíqt = Œò(1/t) yields\nthe stated p log t/(Ni(t) + S) bounds.\n\nLemma B.2 (Effect of Informative Priors on Posterior Quantiles). Under equation 3, for any fixed\nreal counts (n, s) with s ‚àºBinom(n, ¬µi), the posterior under prior inheritance Beta(Œ±0,i+s, Œ≤0,i+\nn ‚àís) is, on average over i ‚àºI, better centered around ¬µi than the posterior under uninformative\nprior Beta(1 + s, 1 + n ‚àís). Consequently,\n          Ei‚àºI ‚Ñì(par)t,i‚ãÜ  ‚àí‚Ñì(unif)t,i‚ãÜ   ‚â•0,    Ei‚àºI q(par)t,i   ‚àíq(unif)t,i  ‚â§0   (i Ã∏= i‚ãÜ),          (6)\nwith strict inequalities whenever Œ≥ > 0 and S > 0.\n\n\n                                       18\n\nproof sketch. The posterior mean under prior inheritance is ÀÜ¬µ(par)n,i  = SÀÜ¬µpar(i)+s+1S+n+2     , while the pos-\nterior mean under uninformative prior is ÀÜ¬µ(unif)n,i  = n+2.1+s  Taking expectation over s and then over\ni ‚àºI yields convex combinations of ¬µi with ÀÜ¬µpar(i) versus 1/2. Our KL-closeness assumption\nequation 3 directly implies that the posterior mean under prior inheritance is, in expectation, a better\nestimate of ¬µi. Since the posterior quantiles are centered around this mean, Lemma B.1 ensures that\nan improvement in the mean‚Äôs centering translates to the stated shifts in the quantiles, holding in\nexpectation.\n\nB.2  SUFFICIENT CONDITION FOR OPTIMAL ARM IDENTIFICATION\n\nFor the algorithm to correctly identify the optimal arm i‚ãÜby the final round T, a sufficient condition\nis that the credible intervals for the optimal and suboptimal arms are well-separated. Formally, this\noccurs if the lower quantile of the optimal arm exceeds the upper quantile of every suboptimal arm.\nThis is the separation event:\n                             ‚ÑìT,i‚ãÜ> maxiÃ∏=i‚ãÜqT,i.                                      (7)\nIf this separation event fails, it implies that for some suboptimal arm i, the credible intervals overlap.\nThis allows us to bound the suboptimality gap ‚àÜi by the sum of the one-sided credible widths:\n                     s       s                                                       log T             log T\n         ‚àÜi ‚â§(¬µi‚ãÜ‚àí‚ÑìT,i‚ãÜ) + (qT,i ‚àí¬µi) ‚â≤         +                       ,          (8)\n                                           Ni‚ãÜ(T) + S      Ni(T) + S\nwhere the second inequality follows from Lemma B.1. This implies that to guarantee separation,\nthe credible interval widths must be sufficiently small relative to the gap. Therefore, a deterministic\nsufficient condition for equation 7 is: there exists a universal constant c‚Ä≤ > 0 such that,\n           s       s                               log T             log T\n                       +         <  c‚Ä≤ ‚àÜi.                         (9)\n                        Ni‚ãÜ(T) + S      Ni(T) + S\nCrucially, combining this condition with the quantile shift from Lemma B.2 reveals the benefit of\nour approach. Because the prior inheritance yields better quantile estimates, the sample allocation\nrequired to satisfy equation 9 is achieved no later than with an uninformative prior.\n\nB.3  PROOF OF PROPOSITION 3.1\n\nProof. The prior inheritance improves the performance of Bayesian UCB through two synergistic\nmechanisms:\n\n(i) Tighter credible intervals at fixed counts. For any given allocation of pulls, the prior strength\nS acts as an additive effective sample size. As established in Lemma B.1, this shrinks the credible\ninterval widths by effectively replacing the sample size Ni(T) with Ni(T)+S. This directly reduces\nthe left-hand side of the deterministic condition equation 9, making it easier to satisfy.\n\n(ii) More efficient sample allocation. The informative prior also leads to a better allocation of pull\nover time. Lemma B.2 shows that the quantiles are favorably shifted on average: the lower bound for\nthe optimal arm i‚ãÜincreases, while the upper bounds for suboptimal arms decrease. This improved\nestimation guides the UCB policy to allocate more pulls to i‚ãÜand waste fewer on suboptimal arms,\nparticularly in the early stages. Consequently, in expectation:\n Eh Ni‚ãÜ(par,S) (T) i ‚â•Eh Ni‚ãÜ(unif) (T) i ,   Eh Ni(par,S) (T) i ‚â§Eh Ni(unif) (T) i   (i Ã∏= i‚ãÜ),  (10)\nwith strict inequalities when the prior is strictly beneficial (Œ≥ > 0 and S > 0).\n\nTogether, these two mechanisms ensure that the separation condition equation 7 is met more effi-\nciently. The tighter intervals (i) make the condition easier to satisfy for any given sample allocation,\nand the improved allocation strategy (ii) finds a sufficient allocation faster. As a result, for a suffi-\nciently larger budget T, the total expected number of pulls on suboptimal arms is reduced:\n                Ô£Æ          Ô£π    Ô£Æ         Ô£π\n                                                                                           (11)               EÔ£∞X Ni(par,S) (T) Ô£ª‚â§EÔ£∞X Ni(unif) (T) Ô£ª,\n                                     iÃ∏=i‚ãÜ                             iÃ∏=i‚ãÜ\nThis is equivalent to stating that the expected cost of identifying the best arm is non-increasing, and\nstrictly decreases whenever the average KL-closeness assumption holds.\n\n\n                                       19\n\nC  ADDITIONAL EXPERIMENTAL RESULTS AND ANALYSIS\n\nC.1  COMPARISON OF COMPUTATIONAL COSTS\n\n\n   Table 5: Comparison of the number of model requests (or calls) for MPO and other baselines.\n\n\n                                 Model Requests (Calls)\n\n      Methods      Base Model   Optimizer Model   Modality-Specific Generator   Avg. Performance\n\n    APE            11.7k          117               N/A                    51.3\n      ProTeGi         11.7k          234               N/A                    60.0\n     SEE             11.7k          153               N/A                    59.1\n    MPO (Ours)      11.7k          234                  117                    65.1\n\nWe analyze the number of model requests (or model calls) as a proxy for computational cost, and\nreport the results in Table 5.  First, the base model call is the same for all methods, as we fix the\nnumber of explored prompts and the evaluation budget. For the optimizer model calls, APE uses the\none-step exploration (e.g., paraphrasing), requiring the number of calls to be equal to the generated\ncandidates. ProTeGi and our MPO utilize a two-step process (e.g., feedback generation and refine-\nment), requiring twice the number of calls. SEE combines both approaches and falls in between.\nNote that, although MPO incurs an additional computational cost by calling a modality-specific gen-\nerator to explore non-textual prompts, this cost is manageable, as this process can utilize lightweight,\nopen-source generators such as SANA1.5 (1.6B) to minimize the additional expense, while still out-\nperforming text-only prompt optimization methods as validated in Table 2 (Bottom Right). In other\nwords, despite the marginal increase in computation (which is also manageable), MPO achieves a\nsubstantial performance improvement unattainable by existing text-only optimization methods.\n\n\nC.2  FULL MAIN RESULTS\n\nWe provide the full results, including performance on individual subtasks. The results for the image\nmodality are presented in Table 6, and for the molecule modality in Table 7.\n\nTable 6:  Full experimental results on image modality benchmarks, including subtasks, with all\nscores reported as the average accuracy over three independent experiments.\n\n\n\n                            PlantVillage                                                 CUB                                                 SLAKE          DrivingVQA  RSVQA\n\n             Apple  Corn  Grape   Potato  Avg  hummingbird   albatross   bunting   jay   cuckoo  cormorant  swallow   blackbird   auklet  grosbeak   oriole   grebe  Avg  CT  MRI  X-Ray  Avg  DrivingVQA  RSVQA\n\n Human       47.4    40.5    29.1     51.7    42.2        50.4          42.8       74.0     90.4    12.2       35.0        50.0       32.5       31.8      68.3      40.1    46.9   47.9   35.7   30.0    39.9    35.2       49.7         51.0\n CoT          57.0    35.6    34.9     44.8    43.1        49.6          39.0       80.6     81.6    30.1       39.1        49.4       44.1       34.3      56.7      32.8    50.3   49.0   31.5   27.9    33.1    30.8       52.9         49.6\n\n 1-shot        48.6    35.4    27.1     47.8    39.7        56.3          49.6       68.2     87.0    48.4       44.0        33.6       69.3       32.3      72.5      46.8    47.8   54.7   32.6   22.8    38.9    31.4       54.5         48.5\n 3-shot        72.2    37.5    27.5     55.4    48.2        62.6          36.8       74.0     92.0    55.7       50.2        51.4       53.1       35.9      80.6      45.9    66.9   58.8   29.7   21.5    40.6    30.6       53.9         52.2\n 5-shot        69.8    38.8    23.2     54.3    46.5        67.0          46.2       78.3     95.1    56.9       48.5        53.4       36.8       45.5      71.7      46.8    51.4   58.1   26.3   16.2    41.3    28.0       45.9         49.2\n\n APE          70.7    66.0    33.8     52.9    55.8        80.4          56.4       89.2     96.9    46.1       56.0        54.4       80.0       41.9      87.5      45.7    73.1   67.3   35.9   28.9    38.1    34.3       52.8         54.4\n OPRO        68.2    63.1    31.2     53.9    54.1        57.4          47.7       87.6     90.2    34.6       53.9        56.7       75.1       34.3      77.8      45.1    55.6   59.7   35.2   28.0    38.3    33.9       52.7         51.0\n EvoPrompt    70.9    65.7    32.8     55.1    56.1        61.3          41.5       90.5     87.9    41.3       45.3        50.6       62.6       34.3      88.3      44.3    67.2   59.6   35.2   29.9    39.3    34.8       52.9         50.5\n PE2          74.0    74.8    43.7     79.2    67.9        78.5          60.6       94.6     94.6    46.8       54.3        67.2       89.6       45.5      98.9      53.2    75.8   71.6   36.8   31.9    38.9    35.8       53.7         55.2\n ProTeGi      75.4    71.0    38.4     72.6    64.4        83.3          51.9       91.5     97.7    60.6       53.5        62.5       81.4       42.4      98.6      48.5    68.6   70.0   36.2   33.2    36.9    35.4       54.4         54.2\n SEE          76.4    75.9    48.0     75.7    69.0        78.9          56.8       93.4     95.0    61.0       60.5        64.7       86.7       47.0      98.1      48.2    69.2   71.6   36.3   31.8    36.9    35.0       52.2         53.4\n\n MPO         77.7    78.2    65.9     84.0    76.4        82.2          61.4       94.6     97.3    68.3       58.4        71.1       85.5       73.7      98.6      68.4    84.2   78.6   36.1   37.5    41.0    38.2       56.0         55.9\n\n\n\nTable 7: Full experimental results on molecule modality benchmarks, including subtasks, with all\nscores reported as the average accuracy over three independent experiments.\n\n\n                                     Absorption                     BBBP                       CYP Inhibition\n\n          PAMPA     HIA       Pgp        Bioavail.      Avg      BBBP   CYP 2C19  CYP 2D6   CYP 3A4   CYP 1A2   CYP 2C9      Avg\n\n            Acc   F1   Acc   F1   Acc   F1   Acc   F1   Acc   F1   Acc   F1   Acc   F1   Acc   F1   Acc   F1   Acc   F1   Acc   F1   Acc   F1\n\n Human      18.2   16.8   41.1   40.7   55.6   49.4   39.1   38.2   38.5   36.3   39.4   38.6   52.6   46.2   25.8   25.1   43.6   33.5   52.9   43.7   40.5   37.1   43.1   37.1\n CoT         30.2   31.1   40.2   40.8   51.3   36.1   36.7   38.9   39.6   36.7   33.6   32.5   48.7   39.1   22.8   21.5   42.8   32.7   50.0   38.7   36.2   29.5   40.1   32.3\n\n 1-shot       16.0   14.0   40.2   39.7   58.6   55.4   36.2   33.6   37.8   35.7   36.1   34.8   56.8   50.9   61.4   37.6   54.9   52.5   56.3   50.4   51.8   50.4   56.2   48.3\n 3-shot       23.3   22.7   56.9   52.7   58.2   56.0   46.1   45.6   46.1   44.2   42.7   42.6   53.1   48.1   48.5   40.0   47.1   40.3   60.4   58.6   50.7   49.2   51.9   47.3\n 5-shot       23.3   23.1   66.7   59.2   58.8   56.3   43.8   43.5   48.1   45.5   49.3   49.3   55.7   53.1   44.2   38.3   48.3   42.8   55.3   48.3   56.4   52.7   52.0   47.0\n\n APE         17.7   16.2   73.6   55.2   52.4   51.7   39.1   38.6   45.7   40.4   36.0   34.7   54.3   53.6   49.4   44.7   49.3   49.1   56.2   56.2   52.3   50.9   52.3   50.9\n OPRO       18.0   16.6   40.2   39.9   56.3   50.0   35.9   34.9   37.6   35.4   39.2   38.3   52.6   46.3   25.7   25.0   43.5   33.4   52.9   43.7   40.5   37.1   43.0   37.1\n EvoPrompt   36.4   35.8   55.8   51.8   52.4   50.6   48.4   47.6   48.2   46.5   38.7   37.7   52.8   52.1   46.3   42.8   48.8   46.9   57.7   57.2   49.8   49.4   51.1   49.7\n PE2         52.7   45.9   82.5   65.8   63.1   62.0   59.6   53.3   64.5   56.8   61.3   58.2   57.6   57.4   57.9   46.1   58.0   56.7   60.6   60.3   58.6   55.1   58.5   55.1\n ProTeGi     74.8   54.1   84.5   64.4   59.9   59.5   65.1   54.8   71.1   58.2   72.1   65.7   58.8   58.3   57.6   48.6   60.3   57.2   61.7   61.5   60.4   59.3   59.8   57.0\n SEE         68.8   52.5   85.1   69.7   65.4   65.1   66.4   52.6   71.4   60.0   67.0   62.3   56.4   56.4   70.7   51.1   57.7   57.7   62.1   61.7   59.8   56.8   61.4   56.7\n\n MPO        78.6   56.1   89.1   76.3   71.0   70.6   68.2   55.1   76.7   64.5   75.3   67.6   60.2   59.2   67.6   51.9   64.2   63.5   64.1   63.6   65.4   62.5   64.3   60.2\n\n\n\n                                       20\n\nC.3  QUALITATIVE RESULTS\n\nIn this section, we provide qualitative results for MPO. Figure 14 illustrates the optimization process\nin the molecular domain, and Table 8 shows examples of textual conditions for the modality-specific\ngenerator and the resulting image prompts. The optimized multimodal prompts from MPO are\npresented for the image (Table 9), video (Table 10), and molecular domains (Table 11 and Table 12).\n\n\n                                     Gen\n\n\n\n                                                           Mix             Gen                                Edit\n\n\n\n                  Mix                     Edit\n\n\n\nFigure 14: The optimization process for the best multimodal prompt on the BBBP task. Inherited\nsubstructures from the parent molecule are marked with the same colored circles.\n\n\nD  USE OF LARGE LANGUAGE MODELS (LLMS)\n\nWe use large language models merely as a writing assistant. Its role is confined to improving gram-\nmar and paraphrasing sentences for clarity, and all the core ideas regarding problem definition, MPO\nframework, experimental design, and interpretation of results are entirely our own.\n\n\n\n\n\n                                       21\n\nGen\n\n\n\n\n                                Parent Image Prompt                              Child Image Prompt\n\n  cgen: Textual Condition for Generation Operator\n\n  Create a detailed reference image showcasing various species of grosbeaks, ensuring each bird is clearly labeled with its common name\n  and distinctive features. Highlight key visual characteristics such as:\n      - Beak shape (e.g., wide, pointed)\n      - Color patterns (e.g., primary colors, unique markings)\n      - Body size (e.g., small, medium, large)\n  Use arrows or circles to emphasize these features, especially where they differ from other species. The composition should be organized\n  in a grid format for easy comparison, with a neutral background to avoid distractions from the birds themselves.\n\n\n\n                                                     Edit\n\n\n\n\n                                Parent Image Prompt                              Child Image Prompt\n\n   cedit: Textual Condition for Edit Operator\n\n  Enhance the reference image of grosbeaks by implementing the following modifications to improve clarity and usability for classifica-\n  tion tasks:\n  1. Increase Resolution and Clarity: Use a higher-resolution image to ensure that all details of the birds are crisp and easily identifiable.\n  2. Labeling: Clearly label each grosbeak species with bold, legible text. Ensure that the labels are positioned close to the respective\n  birds and distinguishable from the background.\n  3. Consistent Postures: Arrange the birds in similar postures and angles to facilitate direct visual comparisons. Consider a uniform\n  side view to best showcase the beak shapes and body sizes.\n  4. Highlight Distinctive Features: Add visual cues such as arrows or circles that point to unique characteristics (e.g., beak shape,\n  wing colors, and markings). Include brief descriptions of these features near the labels.\n  5. Simplify Background: Remove any distracting elements from the background, opting for a neutral color that allows the birds to\n  stand out more prominently.\n  6. Maintain Proportions: Ensure that the proportions of the birds remain accurate and consistent with their actual sizes to aid in the\n  visual comparison.\n  These changes aim to create a more effective reference image that enhances the classification of grosbeaks by making distinguishing\n  features more accessible.\n\n\n\n                                                   Mix\n\n\n\n\n                  Parent Image Prompt A    Parent Image Prompt B                             Child Image Prompt\n\n  cmix: Textual Condition for Mix Operator\n\n  Create a new reference image that combines the best visual elements from both provided grosbeak images. Retain the clear anatomical\n  labeling from Reference Image B, ensuring each grosbeak species is distinctly identified. Highlight key features such as color patterns\n  and beak shapes using arrows and concise annotations.\n  From Reference Image A, incorporate the variety of grosbeak species but arrange them in a less cluttered format, allowing for a clearer\n  comparison of unique characteristics. Focus on using high-resolution images that showcase the birds in similar poses and angles. Ensure\n  that the primary color, distinctive markings, and beak shapes are clearly visible and easily comparable to aid in accurate classification.\n  Discard any elements that create visual confusion or do not add value to the identification process.\n\nTable 8: Operation examples for the image prompt update, including parent image prompts, resulting\nchild image prompts, and the textual condition c to the modality-specific generator, i.e., GPT-Image.\n\n\n\n\n                                       22\n\nPlantVillage: Plant Leaf Images\n                   Analyze the provided grape leaf image and classify it into one of the following categories: [‚ÄòHealthy‚Äô,\n                    ‚ÄòLeaf Blight‚Äô, ‚ÄòBlack Rot‚Äô, ‚ÄòEsca‚Äô]. Use the hybrid reference image for guidance, focusing on the\n                     following critical visual features:\n                        1. Healthy: Look for a vibrant, uniform green color and a smooth texture without blemishes.\n                        2. Leaf Blight: Identify distinct yellowing edges along with well-defined small dark spots that are\n                       clearly visible.\n                        3. Black Rot: Check for sharply defined, dark, sunken lesions that are prominent on the leaf surface,\n                      often accompanied by slight shriveling.\n                        4. Esca: Look for distinct irregular brown patches, significant necrosis, and curling of the leaf edges.\n                     In cases where symptoms overlap, prioritize the most severe characteristics. For example, if both dark\n                      spots and sunken lesions are present, classify based on the prominence of the lesions. Ensure that you\n                      assess each feature carefully, referencing the hybrid image to visualize these distinctions accurately.\n\n             CUB: Bird Images\n                      Classify the bird in the target image by comparing it with the hybrid reference image of grosbeaks.\n                   Follow these refined steps for accurate classification.\n                        1. Identify the Grosbeak Group: Refer to the hybrid image that displays the Rose Breasted Grosbeak,\n                    Pine Grosbeak, Blue Grosbeak, and Evening Grosbeak. Familiarize yourself with the specific traits of\n                   each species, including color patterns and markings.\n                        2. Analyze Visual Features: Focus on these critical features of the target bird:\n                    ‚Äì Dominant Color and Markings: Note the primary color and any distinctive patterns, such as\n                       throat colors or wing designs.\n                    ‚Äì Beak Characteristics: Compare the shape and size of the beak with those in the reference image,\n                     as these can vary significantly among species.\n                    ‚Äì Body Size Comparison: Assess the body size of the target bird relative to the reference birds,\n                     ensuring accurate size comparisons.\n                        3. Feature Prioritization:\n                    ‚Äì Prioritize color patterns first, as they are often the most telling feature.\n                    ‚Äì If colors are similar, evaluate beak shape and size next.\n                    ‚Äì Finally, consider body size. If the target bird does not closely match any reference species, provide\n                      the name of the closest match or indicate ‚Äòunknown‚Äô, based on the following criteria:\n                      ‚Äì Closeness is determined by the degree of similarity across all analyzed features, with color being\n                      the primary factor, followed by beak shape and size.\n                     After analyzing these features, provide the name of the bird species that most closely matches the\n                      visual characteristics observed in the target image, supported by specific observations from the hybrid\n                     reference image.\n\n             SLAKE: Radiological Images\n                  Given the MRI scan image, identify and list all visible abnormalities present in the brain. Your response\n                    should include specific conditions, such as edema and tumors, along with their locations (e.g., ‚ÄúRight\n                  Lobe‚Äù for edema). Refer to the labeled markers in the image to assist with your analysis. Aim to provide\n                    a comprehensive answer that covers all relevant conditions without omitting any visible feature. Ensure\n                   your response is clear and concise.\n\n\n\n\n               DrivingVQA: Driving Images\n                 Examine the provided image of a road scenario and determine the most appropriate action based on\n                       specific visual cues. Pay close attention to the following details:\n                        1. Lane Markings: Identify the lane markings; solid lines indicate a no-overtaking zone, while dashed\n                       lines indicate a safe area for overtaking. Clearly explain how these markings influence your decision.\n                        2. Position of Vehicles: Assess the positions and distances of the vehicles. Determine if there is enough\n                    space and time to safely execute an overtaking maneuver based on their speeds and proximity.\n                        3. Traffic Signs: Observe all visible traffic signs, particularly their meanings. For example, a triangular\n                     sign may indicate a hazard ahead, while a circular sign specifies speed limits. Explain how each sign\n                      influences your decision.\n                  Based on your observations, decide whether you would (A) continue the overtaking maneuver or (B)\n                move to the right.  Justify your choice with specific details from the image, ensuring clarity in your\n                     reasoning.  Conclude your response with ‚ÄúThe answer is [answer].‚Äù Use the image as a reference\n                       to support your analysis of lane markings, vehicle positions, and relevant traffic signs in this driving\n                      scenario.\n\n             RSVQA: Remote Sensing Images\n                   Analyze the provided neighborhood map and respond to the following questions with accurate counts\n                  and concise answers:\n                        1. Count the total number of small roads (less than 5 feet wide), medium roads (5-10 feet wide), and\n                      large roads (greater than 10 feet wide). Indicate which category has the highest count. Use the legend\n                    provided to classify each road accurately.\n                        2. Identify if there is a commercial building (a structure used for business purposes, such as shops or\n                        offices) located to the left of any farmland area. Ensure you consider the top-down perspective of the\n               map when determining placement.\n                        3. For any presence or absence questions, provide a direct ‚ÄúYes‚Äù or ‚ÄúNo‚Äù response.\n                    Refer to the maps‚Äôs colors and labels, ensuring you utilize the legend for accurate identification of each\n                      category. Pay special attention to spatial relationships as defined in the map to avoid misinterpretations.\n\nTable 9: Qualitative examples of the optimized multimodal (image and text) prompts.\n\n                                 23\n\nDrive&Act: Driver‚Äôs Action Videos\n\n\n\n\n\nClassify the primary action being performed in the video, focusing specifically on interactions with objects or devices. If multiple actions\nare present, prioritize the action that is most visually prominent or contextually relevant. For example, if a person is both eating and using\na phone, classify the action of using the phone. Use the definitions provided for each action to guide your decision. Consider visual cues\nsuch as hand movements, object handling, and the overall context of the scene to help determine the primary action. If two actions appear\nequally relevant, choose the one that is visually dominant or crucial to understanding the situation.\n\nVANEBench: Abnormal Videos\n\n\n\n\n\nAnalyze the provided video to identify and describe specific actions or behaviors depicted. Focus particularly on actions that diverge from\ncommon social norms or expectations. For instance, typical actions might include greeting someone or making eye contact, while atypical\nactions could involve unexpected emotional reactions, erratic movements, or interactions that seem out of place. Consider the following\nexamples:\n- A) A person suddenly laughing in a serious situation.\n- B) Someone avoiding eye contact in a social setting.\nPlease select the most striking anomaly from the provided options and present your answer in the format: ‚ÄúThe answer is [answer].‚Äù\n\n     Table 10: Qualitative examples of the optimized multimodal (video and text) prompts.\n\n\n\n\n\n                       Absorption: Drug Absorption to Human\n                      You are a drug discovery assistant tasked with predicting the human intestinal absorption (HIA) of a newly\n                           designed hybrid molecule. Your analysis should focus on the following physicochemical properties, taking into\n                           account both the strengths and limitations of previous reference molecules:\n                                1. Molecular Weight (MW): Calculate the molecular weight of the hybrid molecule. A molecular weight\n                         below 500 Da is generally favorable for absorption.\n                                2. Lipophilicity (LogP): Estimate the LogP value of the hybrid molecule. Aim for a value between -2 and 5,\n                            ensuring that it balances contributions from both polar and non-polar functional groups.\n                                3.  Polarity and Solubility: Analyze the overall polarity of the molecule.  While polar functional groups\n                          can enhance solubility, ensure that their presence does not excessively hinder absorption through lipid-rich\n                            environments.\n 4. Functional Groups: Identify and describe key functional groups present in the hybrid molecule. Focus on ionizable groups that can\n enhance solubility while ensuring that non-polar groups are balanced to facilitate membrane permeability. Discuss how these groups interact\n to affect absorption.\n 5. Stereochemistry: Note any chiral centers present in the molecule. Different enantiomers may exhibit varying absorption profiles, so\n explain how stereochemistry could influence absorption.\n At the end of your analysis, provide a conclusion formatted as either ‚ÄòFinal answer: Absorbed‚Äô or ‚ÄòFinal answer: Not absorbed.‚Äô Ensure\n that your evaluation is comprehensive and considers the combined properties derived from the reference molecules to accurately predict the\n absorption potential of the hybrid molecule.\n Utilization of the Reference Molecule: This hybrid molecule is designed to improve predictions for human intestinal absorption (HIA) by\n integrating key features that enhance solubility and membrane permeability. The presence of two carboxylic acid groups increases the likeli-\n hood of ionization, which can improve solubility in the gastrointestinal tract, while the tertiary amine enhances interaction with transporters,\n facilitating absorption into the bloodstream.\n The molecular weight is kept below 500 Da, aligning with favorable absorption criteria, and the LogP is balanced to ensure optimal lipophilic-\n ity. This design allows for a comprehensive analysis of the hybrid molecule‚Äôs physicochemical properties, which can be used to inform\n predictive models for HIA. By leveraging the strengths of both reference molecules, the hybrid is expected to yield more accurate predictions\n regarding absorption potential, thereby aiding in drug discovery efforts. The combination of polar and non-polar functional groups ensures\n that the molecule can effectively navigate the lipid-rich environments of the intestinal membrane while maintaining sufficient solubility for\n absorption.\n\n    Table 11: Qualitative examples of the optimized multimodal (molecule and text) prompts.\n\n\n\n\n\n                                      24\n\nBBBP: Penetration to Blood-Brain Barrier\n                      You are a drug discovery assistant responsible for predicting the blood-brain barrier (BBB) penetration capa-\n                                 bility of a new hybrid molecule based on its physicochemical properties. Follow these detailed instructions to\n                          conduct your analysis effectively:\n                       ### Key Considerations for BBB Penetration:\n                               1. Lipophilicity (LogP): Estimate the lipophilicity of the hybrid molecule. Aim for a LogP between 1 and\n                               5, which is optimal for BBB crossing. Use computational tools like ALOGPS or ChemDraw to report the\n                           estimated LogP value.\n                               2. Molecular Weight: Check the molecular weight of the hybrid molecule. It should be below 450 Da. Provide\n                             the exact molecular weight in your analysis.\n3. Hydrogen Bonding: Assess the number of hydrogen bond donors (HBDs) and acceptors (HBAs). Aim for 1-2 HBDs and 3-5 HBAs to\nenhance the likelihood of BBB penetration. If the counts exceed these ranges, note how this may affect permeability.\n4. Ionization State: Evaluate if the hybrid molecule is neutral or charged at physiological pH (Àú7.4). Clearly state the estimated ionization\nstate and include pKa values for relevant groups.\n5. Presence of Polar Groups: Identify polar functional groups and assess their overall impact on BBB permeability. Ensure a balanced\npresence to avoid excessive hydrophilicity.\n### Analysis Instructions:\n   - Analyze the provided hybrid molecular structure and evaluate how these properties collectively influence its ability to cross the BBB.\nProvide specific quantitative measures where applicable.\n   - Compare the hybrid molecule‚Äôs characteristics with those of a well-characterized reference molecule known to cross the BBB, noting key\ndifferences in properties that may influence permeability.\n   - Summarize your findings clearly, stating the implications of the physicochemical properties on BBB crossing capability.\n### Final Answer Format:\nConclude your analysis with a clear statement formatted as either: ‚ÄòFinal answer: Can cross BBB‚Äô or ‚ÄòFinal answer: Cannot cross BBB.‚Äô\nEnsure that your analysis is thorough and based on the specific physicochemical properties outlined above to enhance the accuracy of your\npredictions.\nUtilization of the Reference Molecule: This modified molecule enhances predictions for blood-brain barrier (BBB) penetration by addressing\nseveral key physicochemical properties.\n1. Lipophilicity (LogP): The addition of a methyl group (C) at the terminal position increases the hydrophobic character of the molecule,\nwhich can improve its LogP value, making it more favorable for BBB crossing. This change is aimed at achieving a LogP within the optimal\nrange of 1-5.\n2. Molecular Weight: The modified molecule maintains a molecular weight below 450 Da, ensuring compliance with a critical criterion for\nBBB penetration. This is essential as larger molecules often struggle to cross the barrier.\n3. Hydrogen Bonding: The modification retains a balanced number of hydrogen bond donors (HBDs) and acceptors (HBAs). By keeping the\nHBDs to a minimum (1-2) and ensuring HBAs are within the optimal range (3-5), the likelihood of effective BBB penetration is increased.\n4. Ionization State: The structural modifications aim to maintain the molecule in a neutral or partially ionized state at physiological pH,\nwhich is critical for enhancing lipophilicity and reducing the likelihood of charge-related hindrances to BBB penetration.\n5. Presence of Polar Groups: The molecule has been adjusted to balance polar functional groups, reducing excessive hydrophilicity while\nretaining necessary polar characteristics for biological activity.\nBy focusing on these modifications, the molecule is better positioned for predictive modeling of BBB penetration capabilities, as it aligns\nwith established physicochemical parameters known to influence permeability. This can inform computational predictions and improve the\naccuracy of models assessing BBB crossing potential.\n\n               CYP Inhibition: Inhibitory Effect to Cytochrome P450 (CYP) Enzymes\n                      You are a drug discovery assistant tasked with predicting the CYP2C19 inhibition potential of a target\n                           molecule. Your analysis should focus on identifying and evaluating specific structural features that correlate\n                          with CYP2C19 inhibition. Address the following key characteristics:\n                               1. Aromatic Rings: Identify the number and type of aromatic rings present in the target molecule. Multiple\n                           aromatic rings are important for œÄ-œÄ stacking interactions with the CYP2C19 enzyme, enhancing inhibition\n                               potential.\n                               2. Functional Groups: Assess for the presence of functional groups known to enhance binding, such as:\n                                      - Sulfonamide groups (‚ÄìS(=O)2‚ÄìN‚Äì) and amide groups (‚ÄìC(=O)N‚Äì), which facilitate hydrogen bonding\n                        and electrostatic interactions.\n                                      - Highlight any other functional groups that may support or hinder binding.\n3. Basic Nitrogen Atoms or Heterocycles: Determine if the molecule includes basic nitrogen atoms or heterocycles that could enhance\nbinding affinity through electrostatic interactions.\n4. Comparison with Known Inhibitors: Compare the structural features of the target molecule with those of known CYP2C19 inhibitors\nlike Omeprazole or Voriconazole. Pay close attention to similarities and differences in aromaticity, functional groups, and other relevant\ncharacteristics.\nConclude your analysis with a clear statement regarding the inhibition status of the target molecule, formatted as follows: ‚ÄòFinal answer:\nInhibits CYP2C19‚Äô or ‚ÄòFinal answer: Does not inhibit CYP2C19.‚Äô Ensure your comparisons and conclusions are supported by your structural\nanalysis.\nUtilization of the Reference Molecule: This generated molecule, which contains multiple aromatic rings, a sulfonamide group, and an\namide group, can significantly improve predictions for CYP2C19 inhibition potential. The presence of two aromatic rings enhances œÄ-œÄ\nstacking interactions with the CYP2C19 enzyme, which is crucial for binding affinity. The sulfonamide group (‚ÄìS(=O)2‚ÄìN‚Äì) is known to\nfacilitate hydrogen bonding, while the amide group (‚ÄìC(=O)N‚Äì) can participate in additional hydrogen bonding interactions, both of which\nare important for stabilizing the enzyme-inhibitor complex.\nFurthermore, the molecule incorporates basic nitrogen atoms in the amide and sulfonamide groups, which can engage in electrostatic interac-\ntions with the enzyme, further enhancing binding affinity. This structural design aligns with the characteristics observed in known CYP2C19\ninhibitors like Omeprazole and Voriconazole, which also feature multiple aromatic systems and functional groups that promote hydrogen\nbonding.\nBy comparing the generated molecule‚Äôs structural features with those of established inhibitors, we can derive insights into the molecular\ndescriptors that correlate with inhibition potential. This molecule serves as a reference point for evaluating new compounds in terms of their\npredicted CYP2C19 inhibition, helping to refine predictive models and improve the accuracy of virtual screening processes. The combination\nof aromaticity, functional groups, and basic nitrogen atoms in this molecule provides a robust framework for understanding and predicting\nCYP2C19 inhibition.\n\n   Table 12: Qualitative examples of the optimized multimodal (molecule and text) prompts.\n\n\n\n                                     25",
"headers": [
"arXiv:2510.09201v1  [cs.LG]  10 Oct 2025",
"M",
"P",
"O",
": W",
"N",
"L",
"MLLM",
"ULTIMODAL",
"ROMPT",
"PTIMIZATION",
"HY",
"OT",
"EVERAGE",
"ULTIPLE",
"ODALITIES FOR",
"S",
"A",
"1",
"I",
"2",
"R",
"W",
"3",
": M",
"4",
"E",
"5",
"C",
"D",
"B",
"T",
"-I",
"UCB",
"U",
"(LLM",
")"
],
"tables": [
"|e R2 = 0.78|= 0.78|Col3|Col4|Col5|\n|---|---|---|---|---|\n|0.2<br>0.4<br>0.6<br>0.8<br>1.<br>Pt Pt S<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br> <br>R = 0.78<br>~~P-value = 3.33e-170~~|= 0.78<br>~~alue = 3~~|~~  .33e-1~~|~~  70~~||\n|0.2<br>0.4<br>0.6<br>0.8<br>1.<br>Pt Pt S<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br> <br>R = 0.78<br>~~P-value = 3.33e-170~~|||||\n|0.2<br>0.4<br>0.6<br>0.8<br>1.<br>Pt Pt S<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br> <br>R = 0.78<br>~~P-value = 3.33e-170~~|||||\n|0.2<br>0.4<br>0.6<br>0.8<br>1.<br>Pt Pt S<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br> <br>R = 0.78<br>~~P-value = 3.33e-170~~|||||",
"|PlantVillage* CUB* SLAKE* DrivingVQA RSVQA Drive&Act VANE. Absorption* BBBP CYP Inhibit.*|Col2|\n|---|---|\n|**Methods**<br>Acc.<br>Acc.<br>Acc.<br>Acc.<br>Acc.<br>Acc.<br>Acc.<br>Acc.<br>F1<br>Acc.<br>F1<br>Acc.<br>F1|**Avg.**|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|44.1|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|40.8|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|47.2|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|49.5|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|49.3|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|51.3|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|46.9|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|49.5|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|58.2|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|60.0|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|59.1|\n|Human<br>42.2<br>47.9<br>35.2<br>49.7<br>51.0<br>47.3<br>47.0<br>38.5<br>36.3<br>39.4<br>38.6<br>43.1<br>37.1<br>CoT<br>43.1<br>49.0<br>30.8<br>52.9<br>49.6<br>37.2<br>31.6<br>39.6<br>36.7<br>33.6<br>32.5<br>40.1<br>32.3<br>1-Shot<br>39.7<br>54.7<br>31.4<br>54.5<br>48.5<br>50.4<br>62.4<br>37.8<br>35.7<br>36.1<br>34.8<br>56.2<br>48.3<br>3-Shot<br>48.2<br>58.8<br>30.6<br>53.9<br>52.2<br>54.2<br>56.0<br>46.1<br>44.2<br>42.7<br>42.6<br>51.9<br>47.3<br>5-Shot<br>46.5<br>58.1<br>28.0<br>45.9<br>49.2<br>54.3<br>61.4<br>48.1<br>45.5<br>49.3<br>49.3<br>52.0<br>47.0<br>APE<br>55.8<br>67.3<br>34.3<br>52.8<br>54.4<br>50.3<br>64.3<br>45.7<br>40.4<br>36.0<br>34.7<br>52.3<br>50.9<br>OPRO<br>54.1<br>59.7<br>33.9<br>52.7<br>51.0<br>46.4<br>51.0<br>37.6<br>35.4<br>39.2<br>38.3<br>43.0<br>37.1<br>EvoPrompt<br>56.1<br>59.6<br>34.8<br>52.9<br>50.5<br>46.7<br>56.5<br>48.2<br>46.5<br>38.7<br>37.7<br>51.1<br>49.7<br>PE2<br>67.9<br>71.6<br>35.8<br>53.7<br>55.2<br>50.8<br>63.0<br>64.5<br>56.8<br>61.3<br>58.2<br>58.5<br>55.1<br>ProTeGi<br>64.4<br>70.0<br>35.4<br>54.4<br>54.2<br>53.0<br>65.5<br>71.1<br>58.2<br>72.1<br>65.7<br>59.8<br>57.0<br>SEE<br>69.0<br>71.6<br>35.0<br>52.2<br>53.4<br>51.7<br>57.9<br>71.4<br>60.0<br>67.0<br>62.3<br>61.4<br>56.7<br>**MPO (Ours)**<br>**76.4**<br>**78.6**<br>**38.2**<br>**56.0**<br>**55.9**<br>**58.3**<br>**71.2**<br>**76.7**<br>**64.5**<br>**75.3**<br>**67.6**<br>**64.3**<br>**60.2**|**65.1**|",
"|SEE 76.4 75.9 48.0 75.7<br>Generation 76.9 77.9 53.7 83.6<br>Edit 77.2 76.3 56.2 80.1<br>Mix 74.0 77.9 65.1 79.8<br>MPO (Full) 77.7 78.2 65.9 84.0|69.0|\n|---|---|\n|SEE<br>76.4<br>75.9<br>48.0<br>75.7<br>Generation<br>76.9<br>77.9<br>53.7<br>83.6<br>Edit<br>77.2<br>76.3<br>56.2<br>80.1<br>Mix<br>74.0<br>77.9<br>65.1<br>79.8<br>MPO (Full)** 77.7**<br>**78.2**<br>**65.9**<br>**84.0**|73.3<br>72.5|\n|SEE<br>76.4<br>75.9<br>48.0<br>75.7<br>Generation<br>76.9<br>77.9<br>53.7<br>83.6<br>Edit<br>77.2<br>76.3<br>56.2<br>80.1<br>Mix<br>74.0<br>77.9<br>65.1<br>79.8<br>MPO (Full)** 77.7**<br>**78.2**<br>**65.9**<br>**84.0**|74.8|\n|SEE<br>76.4<br>75.9<br>48.0<br>75.7<br>Generation<br>76.9<br>77.9<br>53.7<br>83.6<br>Edit<br>77.2<br>76.3<br>56.2<br>80.1<br>Mix<br>74.0<br>77.9<br>65.1<br>79.8<br>MPO (Full)** 77.7**<br>**78.2**<br>**65.9**<br>**84.0**|**76.4**|",
"|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|MP|(T+I)|APE (T)||",
"|Apple Corn Grape Potato|Avg|hummingbird albatross bunting jay cuckoo cormorant swallow blackbird auklet grosbeak oriole grebe|Avg|CT MRI X-Ray|Avg|DrivingVQA|RSVQA|\n|---|---|---|---|---|---|---|---|\n|Human<br>47.4<br>40.5<br>29.1<br>51.7<br>CoT<br>57.0<br>35.6<br>34.9<br>44.8|42.2|50.4<br>42.8<br>74.0<br>90.4<br>12.2<br>35.0<br>50.0<br>32.5<br>31.8<br>68.3<br>40.1<br>46.9<br>49.6<br>39.0<br>80.6<br>81.6<br>30.1<br>39.1<br>49.4<br>44.1<br>34.3<br>56.7<br>32.8<br>50.3|47.9|35.7<br>30.0<br>39.9<br>31.5<br>27.9<br>33.1|35.2|49.7|51.0|\n|Human<br>47.4<br>40.5<br>29.1<br>51.7<br>CoT<br>57.0<br>35.6<br>34.9<br>44.8|43.1|43.1|49.0|49.0|30.8|52.9|49.6|\n|1-shot<br>48.6<br>35.4<br>27.1<br>47.8<br>3-shot<br>72.2<br>37.5<br>27.5<br>55.4<br>5-shot<br>69.8<br>38.8<br>23.2<br>54.3|39.7|56.3<br>49.6<br>68.2<br>87.0<br>48.4<br>44.0<br>33.6<br>69.3<br>32.3<br>72.5<br>46.8<br>47.8<br>62.6<br>36.8<br>74.0<br>92.0<br>55.7<br>50.2<br>51.4<br>53.1<br>35.9<br>80.6<br>45.9<br>66.9<br>67.0<br>46.2<br>78.3<br>95.1<br>56.9<br>48.5<br>53.4<br>36.8<br>45.5<br>71.7<br>46.8<br>51.4|54.7|32.6<br>22.8<br>38.9<br>29.7<br>21.5<br>40.6<br>26.3<br>16.2<br>**41.3**|31.4|54.5|48.5|\n|1-shot<br>48.6<br>35.4<br>27.1<br>47.8<br>3-shot<br>72.2<br>37.5<br>27.5<br>55.4<br>5-shot<br>69.8<br>38.8<br>23.2<br>54.3|48.2|48.2|58.8|58.8|30.6|53.9|52.2|\n|1-shot<br>48.6<br>35.4<br>27.1<br>47.8<br>3-shot<br>72.2<br>37.5<br>27.5<br>55.4<br>5-shot<br>69.8<br>38.8<br>23.2<br>54.3|46.5|46.5|58.1|58.1|28.0|45.9|49.2|\n|APE<br>70.7<br>66.0<br>33.8<br>52.9<br>OPRO<br>68.2<br>63.1<br>31.2<br>53.9<br>EvoPrompt<br>70.9<br>65.7<br>32.8<br>55.1<br>PE2<br>74.0<br>74.8<br>43.7<br>79.2<br>ProTeGi<br>75.4<br>71.0<br>38.4<br>72.6<br>SEE<br>76.4<br>75.9<br>48.0<br>75.7|55.8|80.4<br>56.4<br>89.2<br>96.9<br>46.1<br>56.0<br>54.4<br>80.0<br>41.9<br>87.5<br>45.7<br>73.1<br>57.4<br>47.7<br>87.6<br>90.2<br>34.6<br>53.9<br>56.7<br>75.1<br>34.3<br>77.8<br>45.1<br>55.6<br>61.3<br>41.5<br>90.5<br>87.9<br>41.3<br>45.3<br>50.6<br>62.6<br>34.3<br>88.3<br>44.3<br>67.2<br>78.5<br>60.6<br>**94.6**<br>94.6<br>46.8<br>54.3<br>67.2<br>**89.6**<br>45.5<br>**98.9**<br>53.2<br>75.8<br>**83.3**<br>51.9<br>91.5<br>**97.7**<br>60.6<br>53.5<br>62.5<br>81.4<br>42.4<br>98.6<br>48.5<br>68.6<br>78.9<br>56.8<br>93.4<br>95.0<br>61.0<br>**60.5**<br>64.7<br>86.7<br>47.0<br>98.1<br>48.2<br>69.2|67.3|35.9<br>28.9<br>38.1<br>35.2<br>28.0<br>38.3<br>35.2<br>29.9<br>39.3<br>**36.8**<br>31.9<br>38.9<br>36.2<br>33.2<br>36.9<br>36.3<br>31.8<br>36.9|34.3|52.8|54.4|\n|APE<br>70.7<br>66.0<br>33.8<br>52.9<br>OPRO<br>68.2<br>63.1<br>31.2<br>53.9<br>EvoPrompt<br>70.9<br>65.7<br>32.8<br>55.1<br>PE2<br>74.0<br>74.8<br>43.7<br>79.2<br>ProTeGi<br>75.4<br>71.0<br>38.4<br>72.6<br>SEE<br>76.4<br>75.9<br>48.0<br>75.7|54.1|54.1|59.7|59.7|33.9|52.7|51.0|\n|APE<br>70.7<br>66.0<br>33.8<br>52.9<br>OPRO<br>68.2<br>63.1<br>31.2<br>53.9<br>EvoPrompt<br>70.9<br>65.7<br>32.8<br>55.1<br>PE2<br>74.0<br>74.8<br>43.7<br>79.2<br>ProTeGi<br>75.4<br>71.0<br>38.4<br>72.6<br>SEE<br>76.4<br>75.9<br>48.0<br>75.7|56.1|56.1|59.6|59.6|34.8|52.9|50.5|\n|APE<br>70.7<br>66.0<br>33.8<br>52.9<br>OPRO<br>68.2<br>63.1<br>31.2<br>53.9<br>EvoPrompt<br>70.9<br>65.7<br>32.8<br>55.1<br>PE2<br>74.0<br>74.8<br>43.7<br>79.2<br>ProTeGi<br>75.4<br>71.0<br>38.4<br>72.6<br>SEE<br>76.4<br>75.9<br>48.0<br>75.7|67.9|67.9|71.6|71.6|35.8|53.7|55.2|\n|APE<br>70.7<br>66.0<br>33.8<br>52.9<br>OPRO<br>68.2<br>63.1<br>31.2<br>53.9<br>EvoPrompt<br>70.9<br>65.7<br>32.8<br>55.1<br>PE2<br>74.0<br>74.8<br>43.7<br>79.2<br>ProTeGi<br>75.4<br>71.0<br>38.4<br>72.6<br>SEE<br>76.4<br>75.9<br>48.0<br>75.7|64.4|64.4|70.0|70.0|35.4|54.4|54.2|\n|APE<br>70.7<br>66.0<br>33.8<br>52.9<br>OPRO<br>68.2<br>63.1<br>31.2<br>53.9<br>EvoPrompt<br>70.9<br>65.7<br>32.8<br>55.1<br>PE2<br>74.0<br>74.8<br>43.7<br>79.2<br>ProTeGi<br>75.4<br>71.0<br>38.4<br>72.6<br>SEE<br>76.4<br>75.9<br>48.0<br>75.7|69.0|69.0|71.6|71.6|35.0|52.2|53.4|\n|MPO<br>**77.7**<br>**78.2**<br>**65.9**<br>**84.0**|**76.4**|82.2<br>**61.4**<br>**94.6**<br>97.3<br>**68.3**<br>58.4<br>**71.1**<br>85.5<br>**73.7**<br>98.6<br>**68.4**<br>**84.2**|**78.6**|36.1<br>**37.5**<br>41.0|**38.2**|**56.0**|**55.9**|",
"|Acc F1 Acc F1 Acc F1 Acc F1|Acc|F1|Acc|F1|Acc F1 Acc F1 Acc F1 Acc F1 Acc F1|Acc|F1|\n|---|---|---|---|---|---|---|---|\n|Human<br>18.2<br>16.8<br>41.1<br>40.7<br>55.6<br>49.4<br>39.1<br>38.2<br>CoT<br>30.2<br>31.1<br>40.2<br>40.8<br>51.3<br>36.1<br>36.7<br>38.9|38.5|36.3|39.4|38.6|52.6<br>46.2<br>25.8<br>25.1<br>43.6<br>33.5<br>52.9<br>43.7<br>40.5<br>37.1<br>48.7<br>39.1<br>22.8<br>21.5<br>42.8<br>32.7<br>50.0<br>38.7<br>36.2<br>29.5|43.1|37.1|\n|Human<br>18.2<br>16.8<br>41.1<br>40.7<br>55.6<br>49.4<br>39.1<br>38.2<br>CoT<br>30.2<br>31.1<br>40.2<br>40.8<br>51.3<br>36.1<br>36.7<br>38.9|39.6|36.7|33.6|32.5|32.5|40.1|32.3|\n|1-shot<br>16.0<br>14.0<br>40.2<br>39.7<br>58.6<br>55.4<br>36.2<br>33.6<br>3-shot<br>23.3<br>22.7<br>56.9<br>52.7<br>58.2<br>56.0<br>46.1<br>45.6<br>5-shot<br>23.3<br>23.1<br>66.7<br>59.2<br>58.8<br>56.3<br>43.8<br>43.5|37.8|35.7|36.1|34.8|56.8<br>50.9<br>61.4<br>37.6<br>54.9<br>52.5<br>56.3<br>50.4<br>51.8<br>50.4<br>53.1<br>48.1<br>48.5<br>40.0<br>47.1<br>40.3<br>60.4<br>58.6<br>50.7<br>49.2<br>55.7<br>53.1<br>44.2<br>38.3<br>48.3<br>42.8<br>55.3<br>48.3<br>56.4<br>52.7|56.2|48.3|\n|1-shot<br>16.0<br>14.0<br>40.2<br>39.7<br>58.6<br>55.4<br>36.2<br>33.6<br>3-shot<br>23.3<br>22.7<br>56.9<br>52.7<br>58.2<br>56.0<br>46.1<br>45.6<br>5-shot<br>23.3<br>23.1<br>66.7<br>59.2<br>58.8<br>56.3<br>43.8<br>43.5|46.1|44.2|42.7|42.6|42.6|51.9|47.3|\n|1-shot<br>16.0<br>14.0<br>40.2<br>39.7<br>58.6<br>55.4<br>36.2<br>33.6<br>3-shot<br>23.3<br>22.7<br>56.9<br>52.7<br>58.2<br>56.0<br>46.1<br>45.6<br>5-shot<br>23.3<br>23.1<br>66.7<br>59.2<br>58.8<br>56.3<br>43.8<br>43.5|48.1|45.5|49.3|49.3|49.3|52.0|47.0|\n|APE<br>17.7<br>16.2<br>73.6<br>55.2<br>52.4<br>51.7<br>39.1<br>38.6<br>OPRO<br>18.0<br>16.6<br>40.2<br>39.9<br>56.3<br>50.0<br>35.9<br>34.9<br>EvoPrompt<br>36.4<br>35.8<br>55.8<br>51.8<br>52.4<br>50.6<br>48.4<br>47.6<br>PE2<br>52.7<br>45.9<br>82.5<br>65.8<br>63.1<br>62.0<br>59.6<br>53.3<br>ProTeGi<br>74.8<br>54.1<br>84.5<br>64.4<br>59.9<br>59.5<br>65.1<br>54.8<br>SEE<br>68.8<br>52.5<br>85.1<br>69.7<br>65.4<br>65.1<br>66.4<br>52.6|45.7|40.4|36.0|34.7|54.3<br>53.6<br>49.4<br>44.7<br>49.3<br>49.1<br>56.2<br>56.2<br>52.3<br>50.9<br>52.6<br>46.3<br>25.7<br>25.0<br>43.5<br>33.4<br>52.9<br>43.7<br>40.5<br>37.1<br>52.8<br>52.1<br>46.3<br>42.8<br>48.8<br>46.9<br>57.7<br>57.2<br>49.8<br>49.4<br>57.6<br>57.4<br>57.9<br>46.1<br>58.0<br>56.7<br>60.6<br>60.3<br>58.6<br>55.1<br>58.8<br>58.3<br>57.6<br>48.6<br>60.3<br>57.2<br>61.7<br>61.5<br>60.4<br>59.3<br>56.4<br>56.4<br>**70.7**<br>51.1<br>57.7<br>57.7<br>62.1<br>61.7<br>59.8<br>56.8|52.3|50.9|\n|APE<br>17.7<br>16.2<br>73.6<br>55.2<br>52.4<br>51.7<br>39.1<br>38.6<br>OPRO<br>18.0<br>16.6<br>40.2<br>39.9<br>56.3<br>50.0<br>35.9<br>34.9<br>EvoPrompt<br>36.4<br>35.8<br>55.8<br>51.8<br>52.4<br>50.6<br>48.4<br>47.6<br>PE2<br>52.7<br>45.9<br>82.5<br>65.8<br>63.1<br>62.0<br>59.6<br>53.3<br>ProTeGi<br>74.8<br>54.1<br>84.5<br>64.4<br>59.9<br>59.5<br>65.1<br>54.8<br>SEE<br>68.8<br>52.5<br>85.1<br>69.7<br>65.4<br>65.1<br>66.4<br>52.6|37.6|35.4|39.2|38.3|38.3|43.0|37.1|\n|APE<br>17.7<br>16.2<br>73.6<br>55.2<br>52.4<br>51.7<br>39.1<br>38.6<br>OPRO<br>18.0<br>16.6<br>40.2<br>39.9<br>56.3<br>50.0<br>35.9<br>34.9<br>EvoPrompt<br>36.4<br>35.8<br>55.8<br>51.8<br>52.4<br>50.6<br>48.4<br>47.6<br>PE2<br>52.7<br>45.9<br>82.5<br>65.8<br>63.1<br>62.0<br>59.6<br>53.3<br>ProTeGi<br>74.8<br>54.1<br>84.5<br>64.4<br>59.9<br>59.5<br>65.1<br>54.8<br>SEE<br>68.8<br>52.5<br>85.1<br>69.7<br>65.4<br>65.1<br>66.4<br>52.6|48.2|46.5|38.7|37.7|37.7|51.1|49.7|\n|APE<br>17.7<br>16.2<br>73.6<br>55.2<br>52.4<br>51.7<br>39.1<br>38.6<br>OPRO<br>18.0<br>16.6<br>40.2<br>39.9<br>56.3<br>50.0<br>35.9<br>34.9<br>EvoPrompt<br>36.4<br>35.8<br>55.8<br>51.8<br>52.4<br>50.6<br>48.4<br>47.6<br>PE2<br>52.7<br>45.9<br>82.5<br>65.8<br>63.1<br>62.0<br>59.6<br>53.3<br>ProTeGi<br>74.8<br>54.1<br>84.5<br>64.4<br>59.9<br>59.5<br>65.1<br>54.8<br>SEE<br>68.8<br>52.5<br>85.1<br>69.7<br>65.4<br>65.1<br>66.4<br>52.6|64.5|56.8|61.3|58.2|58.2|58.5|55.1|\n|APE<br>17.7<br>16.2<br>73.6<br>55.2<br>52.4<br>51.7<br>39.1<br>38.6<br>OPRO<br>18.0<br>16.6<br>40.2<br>39.9<br>56.3<br>50.0<br>35.9<br>34.9<br>EvoPrompt<br>36.4<br>35.8<br>55.8<br>51.8<br>52.4<br>50.6<br>48.4<br>47.6<br>PE2<br>52.7<br>45.9<br>82.5<br>65.8<br>63.1<br>62.0<br>59.6<br>53.3<br>ProTeGi<br>74.8<br>54.1<br>84.5<br>64.4<br>59.9<br>59.5<br>65.1<br>54.8<br>SEE<br>68.8<br>52.5<br>85.1<br>69.7<br>65.4<br>65.1<br>66.4<br>52.6|71.1|58.2|72.1|65.7|65.7|59.8|57.0|\n|APE<br>17.7<br>16.2<br>73.6<br>55.2<br>52.4<br>51.7<br>39.1<br>38.6<br>OPRO<br>18.0<br>16.6<br>40.2<br>39.9<br>56.3<br>50.0<br>35.9<br>34.9<br>EvoPrompt<br>36.4<br>35.8<br>55.8<br>51.8<br>52.4<br>50.6<br>48.4<br>47.6<br>PE2<br>52.7<br>45.9<br>82.5<br>65.8<br>63.1<br>62.0<br>59.6<br>53.3<br>ProTeGi<br>74.8<br>54.1<br>84.5<br>64.4<br>59.9<br>59.5<br>65.1<br>54.8<br>SEE<br>68.8<br>52.5<br>85.1<br>69.7<br>65.4<br>65.1<br>66.4<br>52.6|71.4|60.0|67.0|62.3|62.3|61.4|56.7|\n|MPO<br>**78.6**<br>**56.1**<br>**89.1**<br>**76.3**<br>**71.0**<br>**70.6**<br>**68.2**<br>**55.1**|**76.7**|**64.5**|**75.3**|**67.6**|**60.2**<br>**59.2**<br>67.6<br>**51.9**<br>**64.2**<br>**63.5**<br>**64.1**<br>**63.6**<br>**65.4**<br>**62.5**|**64.3**|**60.2**|",
"|Col1|Gen<br>Parent Image Prompt Child Image Prompt|\n|---|---|",
"|Col1|Edit<br>Parent Image Prompt Child Image Prompt|\n|---|---|",
"|Col1|Mix<br>Parent Image Prompt A Parent Image Prompt B Child Image Prompt|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2510.09201v1.pdf"
}