{
"text": "Published as a conference paper at COLM 2024\n\n\n\n            FUSE-ing Language Models: Zero-Shot Adapter Discovery\n               for Prompt Optimization Across Tokenizers\n\n\n                    Joshua Nathaniel Williams                       J. Zico Kolter\n                   Department of Computer Science          Department of Machine Learning\n                     Carnegie Mellon University               Carnegie Mellon University\n                       Pittsburgh, PA 15213, USA                  Pittsburgh, PA 15213, USA\n                      jnwillia@cs.cmu.edu                      zkolter@cs.cmu.edu\n\n\n                                                Abstract\n2024                   The widespread use of large language models has resulted in a multi-\n                          tude of tokenizers and embedding spaces, making knowledge transfer in\n                        prompt discovery tasks difficult. In this work, we propose FUSE (Flex-\n                                 ible Unification of Semantic Embeddings)1, an inexpensive approach toAug\n                          approximating an adapter layer that maps from one model’s textual em-\n9                         bedding space to another, even across different tokenizers. We introduce\n                           a third-order tensor-based representation of a model’s embedding space\n                               that aligns semantic embeddings that have been split apart by different\n                              tokenizers, and use this representation to derive an approximation of the\n                             gradient of one model’s outputs with respect to another model’s embed-\n                          ding space. We show the efficacy of our approach via multi-objective op-[cs.CL]                        timization over vision-language and causal language models for image\n                            captioning and sentiment-based image captioning.\n\n\n                1  Introduction\n\n                 The current popularity of large language models (LLMs) has led to many individuals and\n                     organizations training and fine-tuning models for their own needs, resulting in a myriad\n                      of models with unique ways of processing, tokenizing, and embedding text. This diversity\n                       creates a challenge for knowledge transfer and interoperability across models, effectively\n                       siloing the insights and capabilities of any single model. One popular way of enabling in-\n                       teroperability is through prompting strategies. These approaches leverage the ability for text\n                      to be passed across models, by converting tasks into formats that LLMs can solve. How-\n                       ever, the uniqueness of different models’ token and embedding spaces creates difficulties\n                      in automated methods for prompt discovery.arXiv:2408.04816v1             While prompting strategies have found success across a variety of tasks including adver-\n                        sarial text generation (Zou et al., 2023), text summarization (Zhang et al., 2022), and prompt\n                    discovery for generative models (Wen et al., 2024), the non-differentiable nature of text re-\n                  mains a limitation. One way of addressing this challenge is by encouraging a standard-\n                     ized tokenization and embedding strategy, where every new model or architecture uses\n                     the same tokenizer and embedding space. Despite the potential for fostering cooperation\n                      across models, it is unlikely that model developers will converge on a single tokenization.\n                        Yet, such a standardized representation may not be necessary if we can freely compute\n                   forward and backward passes across models, regardless of their tokenization.\n\n                     In our work, we propose one such method of computing gradients across different models’\n                       discrete embedding spaces, even if these spaces are defined in terms of different tokenizers.\n                Our approach, which we call FUSE (Flexible Unification of Semantic Embeddings) inserts a\n                    simple module that approximates the functionality of an adapter layer that maps between\n                     the embeddings of multiple models without finetuning. We find that rather than focusing\n                 on individual tokens, if we instead focus on groups of tokens separated by whitespace, then\n\n                        1https://github.com/jnwilliams/FUSE prompt inversion.git\n\n\n                                                           1\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\nwe can track how a full word is represented in the embeddings space and create a necessary\nequivalence among tokenizers that can be leveraged to map from one model to another.\nWe then derive a strategy to compute one such differentiable map, and find that we can\napproximate the gradient of a language model’s output with respect to another model’s\nembedding space solely in terms of the first model’s embedding and a precomputed tensor.\n\nWe show the effectiveness of our approach through a zero-shot captioning task and zero-\nshot captioning with sentiment, where each task is solved via a multi-objective optimiza-\ntion over the sum of the models’ losses. Our contributions are as follows: 1) We introduce\na new framing for optimizing problems across embedding spaces that focus on groups of\nwhitespace-separated tokens, rather than individual tokens. 2) We show how to compute\nan approximate gradient through the input embedding spaces of different models, even\nin the case where the tokenizer and vocabulary for each model varies significantly. 3) We\nshow how we can enable zero-shot tasks by composing multiple specialized models with\nno additional finetuning, through image captioning tasks.\n\n\n2  Background and Related Work\n\n2.1  Prompt Engineering and Discovery\n\nPrompting has become a very useful tool for unlocking the knowledge of large, pretrained\nlanguage models. By carefully crafting prompts to LLMs, we can find simple strategies\nthat can be used to guide the model toward a specific task. For example, Radford et al.\n(2019) have framed the task of text summarization as an LLM task by appending “TLDR:”\nat the end of an article and then having the model generate the text that best follows.\n\nPrompt engineering and discovery (Chen et al., 2023; Gu et al., 2023) focus on finding ef-\nfective prompts for a variety of tasks. Early approaches, such as AutoPrompt (Shin et al.,\n2020) used a gradient-based search strategy to discover appendable suffixes for a prompt\nthat guide the model to act as a sentiment classifier on its original input. FluentPrompt (Shi\net al., 2022) improved upon this by applying a language prior on the suffix, better aligning\nprompt discovery with more human-like prompts. This approach has also been successful\nas an adversarial attack on aligned models. Zou et al. (2023) have introduced Greedy Co-\nordinate Gradients (GCG), to discovers suffixes that can be appended to a harmful prompt\nin order to circumvent safety measures in the LLM and generate harmful responses. Ad-\nditional work (Zhu et al., 2023; Chao et al., 2023) further built on these attacks in order to\nefficiently find adversarial prompts to elicit harmful behaviors in the models.\n\nThese strategies extend to generative image models. Wen et al. (2024) leverage CLIP (Rad-\nford et al., 2021), to find prompts that align well with an image, enabling them to “invert”\nthe image generation process. In contrast to other search methods (Zou et al., 2023; Shin\net al., 2020), the authors introduce an approach that finds prompts via a form of projected\ngradient descent. Mahajan et al. (2023) use similar projected gradient descent methods to\noptimize prompts directly through the diffusion process of a diffusion model. Their ap-\nproach has found prompts more closely tailored to a specific generative process. ClipCAP\n(Mokady et al., 2021) and ZeroCAP (Tewel et al., 2022), have found additional success in\nimage captioning by finetuning and optimizing aspects of pretrained language models in\norder to better align their output with the CLIP similarity of the prompt with the image.\n\n\n2.2  Prompt Discovery with Knowledge Transfer\n\nWhile automated prompt discovery for a single model is powerful, we can both broaden\nthe number of applicable tasks and improve upon existing methods by allowing multiple\nsystems to exchange information (Geraci, 1991; Nilsson, 2019; Hu et al., 2023). For example,\nChao et al. (2023) have found that using a discriminator to determine the degree of success\nfor an attack in tandem with other prompt discovery approaches can find successful natu-\nral language prompts that elicit harmful behavior faster than alternative apporaches.\n\nWe focus our work on knowledge transfer from one model to another by centering our at-\ntention on the text embedding layers of language models. In contrast to the above methods,\n\n\n                                       2\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\nour work aligns with prior work on adapter models, in which a new layer is inserted be-\ntween layers of pretrained models (He et al., 2021; Houlsby et al., 2019; Bapna et al., 2019).\nThese layers are then finetuned, adapting the model to new tasks, including knowledge\ntransfer between multiple models (Wang et al., 2020). By inserting a new layer just before\nthe embedding layer of a model that approximates the behavior of an adapter from one\nmodel’s embedding space to another, we can make use of a weighted combination models\nto solve tasks without requiring a specific architecture nor requiring additional fine-tuning.\nBy allowing the gradients to flow freely across models with different tokenizers and em-\nbedding spaces, we can optimize prompts for zero-shot tasks leveraging the knowledge\nwithin multiple models with relatively little overhead.\n\n\n3  Methodology\n\nBefore going into detail on our approach, we first introduce key concepts and notation that\nwill be necessary for understanding our method. Throughout this work, scalars, vectors,\nmatrices, and tensors are denoted as lowercase, a, bolded lowercase a, uppercase A, and as\nuppercase with a tilde ˜A, respectively.\n\n\n3.1  Language Model Embeddings\n\nGiven a string, a tokenizer maps it to a set of tokens, t ∈{0, ..., |V|}s, where s is the length\nof the tokenized string and |V| is the number of unique tokens in the tokenizer. The model\nthen applies a mapping E  : {0, ..., |V|}s →Rs×d which indexes these tokens across a dis-\ncrete set, mapping each to a unique d-dimensional embedding, E ∈Rs×d.\n\nAlternatively, we can represent the embedding function (E) itself as a matrix, V ∈R|V|xd,\nwhere each row corresponds to the embedding vector for a specific token in the vocabulary.\nBy representing the tokens as one-hot encodings over the vocabulary, X ∈{0, 1}s×|V|, we\ncan express the embedding vectors with a lookup operation E = XV. In this framing, V is\nboth a matrix and denotes the set of discrete embedding vectors for a model.\n\nWith this set of preliminary information in hand, we proceed to outline our approach, start-\ning from the simple case in which models share a tokenizer, but have different embeddings\n(i.e., strings will always be tokenized to the same t, but the embedding mapping, E(t) dif-\nfers between models. We then build on this case and extend it to the case in which models\ntokenize words differently and have different embedding mappings (i.e., words may be\nseparated arbitrarily, the model vocabularies have different lengths, and each embedding\nmay have a different dimensionality across models).\n\nFor the latter case, understanding how to multiply tensors is crucial for our approach.\nWhen working with tensors of order greater than 2, their multiplication has been well-\ndefined in terms of the t-product operator, ∗(Kilmer & Martin, 2011). The t-product defines\nan associative and left/right distributive multiplication operation of ˜A ∈Rm×k×p1×···×pn\nand ˜B ∈Rk×n×p1×···×pn, where ˜A ∗˜B ∈Rm×n×p1×···pn. We also make use of the folding\nand unfolding operation introduced alongside the t-product that reshapes an Rd1×d2×···×dn\ntensor into a partitioned tensor in Rd1dn×···×dn−1 tensor and back,\n\n             unfold( ˜X) =  ˜X1   ˜X2    · · ·   ˜Xn T    fold(unfold( ˜X)) = ˜X.\n\nNote that Kilmer & Martin (2011) require, ˜A and ˜B to have their first two dimensions of the\nappropriate shape for matrix multiplication and each of the remaining dimensions must be\nthe same size, however this product can also be generalized to arbitrary tensor sizes as long\nas the first two dimensions are appropriate sizes for matrix multiplication. See Appendix\nA for a further primer on the t-product and this generalization.\n\nThe key idea in our work is that while current tokenizers may split the same word arbi-\ntrarily, they always respect white-space separation. We can build shared representations\nacross embedding spaces by focusing on groups of white-space separated tokens and their\n\n\n                                       3\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\nembeddings, represented as third order tensors, rather than individual tokens and embed-\ndings represented by matrices. In doing so, we find that we can approximate the gradient\nof a language model’s output with respect to another model’s embedding space solely in\nterms of the first model’s embedding of a string and a precomputed tensor.\n\n\n3.2  Shared Tokenizers\n\nRecall that the embedding of a set of tokens for model i, can be represented as, Ei = XVi,\nwhere X is a one-hot encoding across the vocabulary, Vi 2. Our goal is to solve a multi-\nobjective optimization over K models, in which each model is solving a different task\nwhose loss is computed with a differentiable Li(Ei).\n\n                                      K\n                                arg min ∑ Li(XVi).                                     (1)\n                               X    i=1\n\nAs each model uses the same tokenizer, X is shared for each model. This problem can\nclearly be solved via any off-the-shelf optimizer. However, consider the pedagogical case\nin which we want to directly optimize the embedding vectors, Ei, instead of the one-hot\nencodings. Solving equation (1) becomes less clear. One approach is to choose one model\nto be the primary model, and use its embeddings as input to all other models by introducing\nan adapter Ti:j  : Vi →Vj that maps from model i’s vocabulary to model j’s vocabulary.\nWith the introduction of Ti:j, we only optimize in the primary model’s embedding space\nand our objective becomes,\n                           arg min Li(Ei) + ∑ Lj(Ti:j(Ei)).                              (2)\n                                             Ei                     j̸=i\n\nWith a differentiable representation of Ti:j, then this equation can be solved via gradient-\nbased optimization. However, as the vocabulary matrices are not square, they are not\ninvertible; we cannot directly map from the embedding space to back to token space. We\ninstead approximate a linear map for Ti:j using the Moore-Penrose inverse (pseudoinverse)\nof the model’s vocabulary, V+i = VTi (ViVTi )−1. By using the pseudoinverse, EiV+i ≈X, we\ncan substitute EiV+i  for every instance of X in Equation (1) and set Ti:j(Ei) = Ej ≈EiV+i  Vj.\nThe gradient of Equation (2), is then a simple application of the chain rule,\n\n                          ∇EiLj(Ti:j(Ei)) ≈  ∇EjLj(Ej) V+i  Vj.                           (3)\n\nPay particular attention to the fact that the approximate gradient is no longer dependent\non the embedding of the model that we want to map from, only on the embedding that we\nwant to map to. We can thus map Ei to Ej in a non-differentiable way (e.g., convert back to\ntext and retokenize), compute the gradient of the loss for model j, with respect to its own\nembeddings, and then multiply this gradient by V+i Vj to approximate the gradient of the\nloss of any secondary model with respect to the embeddings of the primary model. This\nenables us to freely have access to noisy descent methods across a variety of models and\nzero-shot tasks, while only keeping track of a single di × dj matrix per additional model.\n\n\n3.3  Different Tokenizers\n\nWhile the previous section enables gradient-based methods directly on the embedding\nspace, it relies on models tokenizing words in the same way. For example, if we tokenize\nthe word “Happy”, equation (3) assumes that the k-th token in every model’s vocabulary\nis the embedding for “Happy”. But when using different tokenizers, this is no longer true.\nIf one model tokenizes the word “Happy” as {‘Ha’,‘ppy’} and another as a single token,\n{‘Happy’}, equation (3) gives incompatibly sized gradients in R2×d and R1×d. The primary\nquestion becomes: “How do we reconcile these incompatible gradients?”\n\n   2Note that Vi uses the subscript i to denote the vocabulary of a particular model, not the token\nindex within a model’s vocabulary.\n\n\n                                       4\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\n          [    the     ]      ,    [   Ø       ]                     [     the       ]      ,    [  Ø      ]  \n                  [   qui     ]      ,    [    ck       ]                                           [    q        ]      ,    [   uick    ]\n                  [    br      ]      ,    [   own     ]                                           [  brown    ]      ,    [  Ø      ]\n                  [    fox     ]      ,    [   Ø       ]                                           [     fox       ]      ,    [  Ø      ]\n  ˜Vi =       [       j       ]      ,    [  umps    ]                         ˜Vj =       [   jump     ]      ,    [     s       ]\n                  [   over    ]      ,    [   Ø       ]                                           [    ov       ]      ,    [    er      ]\n                  [    the     ]      ,    [   Ø       ]                                           [     the       ]      ,    [  Ø      ]                                                                   [       l       ]      ,    [    azy      ]                                                                                                                                                                                                  [    lazy      ]      ,    [  Ø      ]                        \n                  [   dog     ]      ,    [   Ø       ]                                           [    d        ]      ,    [   og      ]\n\n\nFigure 1: An R9×d×2 tensor vocabulary over words: “the quick brown fox jumps over the\nlazy dog”. Each plain-text word represents its corresponding Rd embedding, and each Ø\nis a 0 vector. We approximate the gradient for a mapping from model Mi’s embeddings to\nMj’s embeddings by computing the t-product ˜V+i  ∗˜Vj, where ˜V+i   .\n\n\nConsider a case in which we split a string into a batch of its component white-space sepa-\nrated words and then compute the gradient of some function over each word in the batch.\nEven if words are tokenized differently, the total derivative with respect to a word’s multi-\ntoken representation still provides information on a loss-minimizing direction.\n\nWe therefore propose an embedding representation that focuses on batches of words, rather\nthan individual tokens, by introducing split and merge 3 operations analogous to the fold\nand unfold operations used by Kilmer & Martin (2011) when defining the t-product.\n                  split(E) =  ˜E1   ˜E2    · · ·   ˜Ek     merge(split(E)) = E,\nwhere ˜Ei ∈R1×d×li is the third-order tensor representation of the a set of tokens in E, and\nli are the number of tokens that make up the word represented by ˜Ei. The split operation\ndoes not return a tensor (denoted by the change from brackets to parenthesis) but a list of\ntensors where each element is a whitespace-separated set of tokens in the original string\nthat can have variable length, li 4. The merge operation stacks these tensors back into their\noriginal shape. Using the limited vocabulary in Figure 1 (and denoting each embedding\nvector in Rd as the plain-text token that it represents), calling ‘split’ on an embedding,\nϵ ∈R6×d that represents the phrase: “the quick brown fox”, gives\n\n                                    qui       br\n                    split(ϵ) =    the                          fox       .\n                                    ck     own\nUsing this lens, we extend the second-order vocabulary tensor to a third-order tensor, ˜V ∈\nRw×l×d, where w are the number of words that that can be represented by the original\nvocabulary V using at most l tokens. Any set of tokens that requires fewer than l tokens to\nrepresent is assumed zero-padded. See Figure 1 for an example of ˜V across two models.\n\nImportantly, Jin et al. (2017) have shown the Moore-Penrose inverse still exists for arbitrary\ntensors under the t-product. We can therefore reuse the ideas in section 3.2, however, rather\nthan matrix multiplication, we instead use the tensor t-product.  If the embedding for a\nword is represented as\n                                   ˜E = ˜X ∗˜V      ˜X = fold([X  0   · · ·   0]),\nwhere ˜E ∈Rs×d×l, ˜X ∈{0, 1}s×|V|×l is the one-hot tensor encoding for the t-product and\nX ∈{0, 1}s×|V| is the matrix one-hot encoding. We can construct ˜Ei and ˜Ej with a sys-\ntem of equations and follow the same process from Section 3.2 to compute a differentiable\n\n   3For clarity, we simplify the split and merge operations throughout this section. Each split and\nmerge are specific to a model and both have access to the original string that the embeddings repre-\nsent. A more formal notation may be, splitiS(E), however this may introduce unnecessary confusion\nfor the reader. Throughout 3.3, assume that split and merge have all necessary information to shape\ntensors into their appropriate shapes for each operation.\n   4For convenience, we also define the split operation to be distributive for any arbitrary function,\nexcept for the merge function that acts as an inverse.  f (split(E)) =   f ( ˜E1)     f ( ˜E2)    · · ·     f ( ˜Ek)  .\n\n\n                                       5\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\n                                                                          Model 1\n     Input Embedding\n\n\n\n\n    Shifted Embedding\n\n\n          Fuse Adapter\n\n              Forward Pass\n                                                                                                                                              Model 2\n                                                   Retokenize With                   Input Embedding                  Convert to Plain Text\n                                                      Tokenizer 2\n\n\n\n                                                                                          Shifted Embedding\n\n                                                              Backward Pass\n\n\nFigure 2: The FUSE adapter connecting two transformer models for parallel inference. In-\nputs from Model 1 flow through the adapter by converting to text, retokenizing with Model\n2’s tokenizer, and embedding into Model 2’s input space. The backward pass receives the\ngradient from Model 2, and multiplies it by the precomputed ˜V+1  ∗˜V2.\n\napproximation to ˜X that can be reused across the models i and j, ˜Ej ≈˜Ei ∗˜V+i  ∗˜Vj. In\nthis case, we overload notation from Ti:j and allow T to be a differentiable map between\ntensors of words, rather than tokens. Equation (2), can then be rephrased in terms of sets\nof whitespace-separated tokens, where ‘merge(Ti:j(split(Ei)))’ is simply a mapping of an\nembedding from model i to model j in terms of our tensor-based vocabulary,\n\n                    arg min Li(Ei) + ∑ Lj  merge(Ti:j(split(Ei))   .                     (4)\n                                   Ei                     j̸=i\n\nEvery ˜E in split(E) =   ˜E1   ˜E2    · · ·   ˜Ek may have a potentially different length l, so\nif ˜E1 is the embedding for a model that tokenizes the word “Happy” with two tokens,\n{‘Ha’,‘ppy’} and ˜E2 has been constructed from a model that tokenizes it as a single to-\nken, {‘Happy’}, we still need to ensure ˜V+i  ∗˜Vj are of appropriate sizes to compute the\nproduct. We can accomplish this by conditioning the mapping ˜V+i  ∗˜Vj on the length, l of\n˜E ∈Rw×d×l, and keep track of specific ˜V+i  ∗˜Vj maps across ‘sub’-vocabularies in which\nVj is comprised only of words that require l tokens to represent. When computing the\ngradients, we simply check how many tokens each word requires and use the appropriate\n˜V+i  ∗˜Vj. See Algorithm 2 in Appendix B for a full description.\n\nDuring a backward pass, we split the gradient from model j into a set of tensors that have\nthe same shape as calling ‘split’ on the original embeddings. We compute a final, approx-\nimate gradient by first converting model i’s embedding to text and then to model j’s em-\nbedding space, before computing the gradient of model j’s loss with respect to the correct\nembeddings. This gradient is then split apart and separated using the split operation and\neach piece is multiplied by the appropriate ˜V+i  ∗˜Vj based on its token length, before being\nmerged back together into the appropriate gradient size for Ei (see Figure 2 for a visualiza-\ntion and Algorithm 1 for pseudocode),\n\n       ∇EiLj  merge(Ti:j(split(Ei)))  ≈merge  ( ˜V+i  ∗˜Vj) ∗split(∇EjLj(Ej))   .       (5)\n\n\nJust as in the case where we have the same tokenizer across models, this allows us to\napproximate the gradient across the tokenizers, enabling us to freely use gradient-based\noptimizers, while needing to store a set of parameters of size di × dj ×  ∑li=1 i  tensor. In\ntheory this l could be very large, however, in practice we limit l to a reasonable number,\nl = 4 as we expect the number of words that require more than 4 tokens to be fairly rare.\nFor example, the Llama Tokenizer (Touvron et al., 2023) requires only 4 tokens to represent\n97.6% of the text in the BookCorpus (Zhu et al., 2015) dataset.\n\n\n                                       6\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\n  Algorithm 1: Pseudocode for computing the FUSE Adapter backward pass.\n  Input: Gradient from model j: ∇xj f (xj)\n  Input: List of (V+i  ∗Vj). List index corresponds to size of third tensor dimension\n  Output: Gradient w.r.t. model i’s embedding\n1 L ←split(∇xj f (xk))                           // Split gradient wrt each word\n2 G ←empty list\n\n3\n  // For each word’s gradient\n4 for k ←length(L) do\n5   m ←Sequence Length(L[k])                          // Tokens in this word\n6   T ←(V+i  ∗Vj)[m]                   // Index (V+i  ∗Vj) based on token count\n7    G[k] ←L[k] ∗T                                   // Compute Tensor Product\n\n8  ∇xi f (Ti:j(xi)) = merge(G)                                   // Stack to matrix\n9 return ∇xi f (Ti:j(xi))\n\n\n 4  Experiments\n\n  4.1  Datasets\n\n We show that our approach effectively transfers knowledge across multiple models by\n  focusing on two tasks: image captioning and image captioning with sentiment using the\n  following datasets:\n\n MS-COCO (Karpathy Test Split) (Lin et al., 2014) COCO provides 5000 images each with\n  5 human-annotated captions, allowing for the evaluation of image captioning quality.\n\n NoCaps-Val (Agrawal et al., 2019) This dataset seeks to provide a more varied set of ob-\n  jects and concepts than included in MS-COCO. This dataset consists of 10600 test and 4500\n  validation images sourced from the Open Images (). Each image is accompanied by 10\n human-annoted captions. The dataset is separated into an “in-domain”, “near-domain”,\n and “out-domain” splits that describe the degree to which the subset contains object classes\n common to MS-COCO images. Here we caption all images in the validation set.\n\n  SentiCap (Mathews et al., 2016) This dataset consists of 2360 images from the COCO\n  Karpathy validation split, each with 6 new captions for each image, 3 positive sentiment\n  captions and 3 negative sentiment captions. We use this dataset to investigate the ability to\n  control the sentiment of a caption via a pretrained sentiment classifier.\n\n\n  4.2  Implementation Details\n\n  For the above datasets, we construct a simple captioner via a multi-objective optimization:\n   E∗= arg min LCE( fθ(E), E) + α1 · CLIPθ(T f :CLIP(E), I) + α2 · LCE(g(T f :g(E)), s)    (6)\n               E\n\n  This equation minimizes the sum of the clip similarity between an image and the em-\n  bedding, the cross entropy between this embedding and an arbitrary language model’s\n  output, and the correctness of the sentiment as determined by a BERT-based sentiment-\n  classifier.  Here  f  is a pretrained language model (e.g., GPT2-Medium Radford et al.\n  (2019)), g is a sentiment classifier, LCE is the cross-entropy loss, T f :CLIP is the mapping\n from the language model’s embeddings to CLIP’s embeddings, T f :g is the found map-\n  ping from the language model’s embeddings to the sentiment classifier’s embeddings,\n  s ∈{positive, neutral, negative} is the desired sentiment, and αi is a scalar weight. When\n  captioning without sentiment, we set α2 = 0. In order to better compare with prior zero-\n  shot methods, we use GPT2-Medium as our language model, and VIT-B/32 for CLIP and\n  a Bert-based sentiment classifier5.\n\n     5cardiffnlp/twitter-roberta-base-sentiment-latest\n\n\n                                         7\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\n  No    A typical pizza from the  A cow grazing on a patch  A flower pot in the gar-  A player hitting a home\nSentiment    Norfolk website.             of bush close to where she   den of the terrace house.     run.  Photo:  Sierra Vista\n                                               lived.                                                     College.\n\n Positive   A pizza made with or-  A cow grazing on a hedge   One of the flowers stands   The ball hitting the back\n             ganic ingredients. Photo:    in front of the village.      on a pot in a garden out-    of the built-in sliding bat.\n              Fairfax                                                   side a house in            Note the\n\n Negative   A pizza being served to a  A cow in front of a ditch  A white bucket with a red  A man hitting and stomp-\n            group of students demon-    in the southeast country-    flower on it has been        ing on a college senior\n              strated how widespread    side.\n               this behaviour is.\n\nFigure 3: Example Captions that using a FUSE Adapter to minimize the sum of GPT2-\nMedium, CLIP-VIT-B/32, and a Bert-based Sentiment Classifier via AutoDAN (Zhu et al.,\n2023).  This combination of models controls through synonyms that indicate tone or\nthrough creating additional context for each image to denote tone. Note that AutoDAN\ndoes not have a clear stopping condition, a caption may stop in the middle of a sentence.\n\n\nWhen fitting FUSE, we limit it to computing gradients of words that require 4 or fewer\ntokens. We fit the adapter using 16384 random words from the Wiki-Text dataset for each\ncase where words require less than 4 tokens as described in Section 3.3 and Algorithm 2. If a\nword uses more than 4 tokens to represent, we treat the Jacobian used by FUSE as a random\nmatrix, expecting further optimization steps to insert a token with white-spacing, reverting\nto the setting that the adapter is fit to. Fitting the adapter for the models considered in\nour experiments requires only 4 minutes and 22 seconds on a standard workstation with\n32GB of memory. As shown in Figure 2, during optimization, the forward pass consists\nof a mapping from embeddings to text and back again, limited only by the time required\nto perform this mapping. During the backward pass we only require a single t-product,\nwhich consists of the sum of m2 matrix multiplications, where m is the number of tokens\nthat make up each word.\n\nWe then use the discrete optimizer AutoDAN Zhu et al. (2023) to optimize the objective.\nIn contrast to methods like, (Zou et al., 2023) and (Wen et al., 2024), AutoDAN optimizes\na prompt one token at-a-time by first computing the log probabilities of the next token\nusing our given language model and some prefix, and adds these logits to the negative\ngradient of the objective. This sum returns a set of scores that describe an estimate of the\nimprovement in the loss for each token. We choose the top 512 candidates and compute\nthe true error to determine the best token update. Unlike AutoDAN, which performs this\nsearch greedily, we also use a beam search with a beam width of 5 when searching through\nthe space of token updates. All captions use the prefix ”An image of” at initialization.\n\nWe assess the FUSE Adapter’s performance for image captioning using standard super-\nvised metrics: BLEU-N (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), CIDEr\n(Vedantam et al., 2015), SPICE (Anderson et al., 2016) that measure caption quality against\nhuman-written references, evaluating captions for n-gram overlap (BLEU-N), semantic\nsimilarity (METEOR), content alignment (CIDEr), and grammatical coherence (SPICE).\n\n5  Results\n\n5.1  Image Captioning\n\nIn Table 1, we show our results on MS-COCO and NoCaps-Val. As with other zero-shot\ncaptioning methods, without domain bias for human captions, we do not expect that we\nwill be able to achieve the same level of performance as models that have been finetuned\nfor captioning. However, among zero-shot methods, our approach significantly improves\n\n\n                                       8\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\n                                        MS-COCO            NoCaps-Val (Overall)\n                Metrics\n                                               B-4   M     C       S     C          S\n\n                                              Supervised Methods\n\n                BLIP-2 (Li et al., 2023)            43.7        -      145.8         -      119.7         15.40\n           mPLUG (Li et al., 2022)          46.5    32.0     155.1     26.0    114.8         14.8\n           OFA (Wang et al., 2022)          44.9    32.5     154.9     26.6         -                 -\n              CLIP-VL (Tewel et al., 2021)      40.2    29.7     130.3     23.8         -                 -\n              VinVL (Zhang et al., 2021)        40.9    30.9     140.4     25.1     90.4         13.07\n           LEMON-B (Hu et al., 2022)       40.3    30.2     133.3     23.3     79.0          12.3\n               ClipCap (Mokady et al., 2021)    32.2    27.1    108.35    20.12     65.7          11.1\n\n                                             Zero Shot Methods\n\n              ZeroCap (Tewel et al., 2022)      2.60    11.50    14.60     5.50         -                 -\n             ConZIC (Zeng et al., 2023)        1.29    11.23    13.26     5.01         -                 -\n              Ours ( GPT2-M + VIT-B/32 )     1.59    14.72    15.93     9.15    20.65         6.64\n\n                Table 1: Comparison of SOA image captioning methods.\n\n\n\namong most of our metrics. Moreover, we see a significantly larger increase in the SPICE\nscore over our zero-shot comparison methods; our caption generation process returns more\ngrammatically consistent text as the comparisons. This is likely due to using AutoDAN as\nour discrete optimizer, which places weight on not just the objective but the direct prob-\nabilities of each new token before computing the cross-entropy over GPT2-M’s logits. As\nour discrete optimizer determines candidates based on the gradient of Equation (6), the ob-\nserved performance necessitates that the gradient of the CLIP similarity between the image\nand the CLIP’s text embeddings, with respect to GPT2-M’s text embedding is meaningful.\n\n\n5.2  Captioning with Sentiment\n\nTable 2 shows our method’s performance on image captioning with sentiment. As in the\nstandard captioning task above, we see that combining CLIP-VIT-B/32, GPT2-M, and a\nBert-based sentiment classifier, successfully finds a caption that aligns well with the se-\nmantic content of the reference. But, we are less accurate in the found sentiment than the\ncomparison methods. While most methods insert descriptive adjectives that denote senti-\nment, at every step we are trying to minimize both the image similarity and the sentiment.\nAs a result, our approach finds synonyms that connote the sentiment. For example, in\nFigure 3, a negative caption replaces the “flower pot” with “bucket”. In the context of a\nreplacement word for ‘flower pot” bucket carries a more negative sentiment, however, at\nface value, “a bucket with a red flower” is a neutral statement. Again, our results are not\nfocused on improving over other methods in terms of performance on such datasets, but\nshowing that the FUSE Adapter provides meaningful gradients in its backward pass. The\nchanges to the standard captions elicited by the BERT-based sentiment classifier also neces-\nsitate that each gradient step is carrying information from both the image and sentiment.\n\n\n6  Conclusion and Future Work\n\n\nIn this work, we propose a novel approach for approximating gradients across models\nand tokenizers during prompt optimization. We introduce an adapter that precisely maps\nacross token and embedding spaces in the forward pass. By leveraging a precomputed lin-\near transformation, we efficiently approximate the behavior of a true differentiable map-\nping between embedding spaces during the backward pass. This adapter not only im-\nproves accessibility for knowledge transfer tasks for prompt optimization, but also unlocks\npotential new tasks by allowing for easy compositions of distinct models.\n\nWe demonstrate the potential of our approach on zero-shot image classification tasks,\nwhere combining a language model, a vision-language model, and a Bert-based sentiment\nclassifier in a multi-objective optimization, we achieve superior results to prior zero-shot\nimage captioning methods. This suggests that despite being an approximation our gradi-\nent carries meaningful information.\n\n\n                                       9\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\n                                                                Positive                  Negative\n                Metrics\n                                                      B-3(↑)   M(↑)    Acc(↑)    B-3(↑)   M(↑)    Acc(↑)\n\n                                                  Supervised\n\n                StyleNet (Gan et al., 2017)             12.1      12.1      45.2      10.6     10.9      56.6\n           MSCap (Guo et al., 2019)              16.2      16.8      92.5      15.4     16.2      93.4\n           MemCap (Zhao et al., 2020)           17.0      16.6      96.1      18.1     15.7      98.9\n            ADS-CAP (Cheng et al., 2022)         18.9      18.5      99.7      21.0     18.0      98.2\n\n                                                  Zero Shot\n\n             ConZIC (Zeng et al., 2023)             1.89      5.39      97.2      1.78     5.54      99.1\n              Ours ( GPT2-M + VIT-B/32 + Roberta)     1.91     10.40     83.8      2.29     7.42      85.6\n\n        Table 2: Comparison of SOA sentiment-based image captioning methods.\n\n\n\nWhile this work introduces a simple adapter, researchers and organizations may prefer\nlearning an actual mapping through supervised learning of a transformer to translate from\none embedding space to another.  Yet, the compute necessary for such a task may not\nbe universally available. We believe that FUSE may serve a valuable purpose in low-\nresource/low-compute settings, in which researchers may want to do inference across\nmodels, yet be unable to train a true adapter.  Additionally, this approach may be use-\nful in fast-paced environments, where FUSE can be used as a low-cost preliminary test for\nmore involved methods requiring a well-trained adapter.\n\nOur work presents an initial step to making prompt optimization more accessible and scal-\nable. Future research may explore more memory and storage-efficient approaches while\nimproving upon the accuracy of our proposed method. Since this work approximates a\ndifferentiable map from one discrete space to another, it is important to note that the tradi-\ntional concept of a gradient does not apply, as such traditional ways of validating gradient\napproximations were unavailable. Future work may introduce comprehensive validation\nmethods for mappings and gradients from one discrete embedding space to another. Our\nwork also opens the door for further investigations of techniques that mitigate the storage\ncosts associated with longer sequences and integrating more advanced mapping approxi-\nmations. While there remain areas to build on, our approach holds promise for improving\nmethods of prompt optimization, particularly in resource-constrained settings, and lays\nthe groundwork for future innovations in cross-model interactions.\n\n\nReferences\n\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv\n  Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at\n   scale. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 8948–\n  8957, 2019.\n\nPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould.  Spice: Semantic\n  propositional image caption evaluation. In Computer Vision–ECCV 2016: 14th European\n  Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14, pp.\n  382–398. Springer, 2016.\n\nBassam Bamieh. Discovering transforms: A tutorial on circulant matrices, circular convo-\n  lution, and the discrete fourier transform. arXiv preprint arXiv:1805.05533, 2018.\n\nSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with\n  improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic\n  and extrinsic evaluation measures for machine translation and/or summarization, pp. 65–72,\n  2005.\n\nAnkur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neu-\n   ral machine translation. arXiv preprint arXiv:1909.08478, 2019.\n\n\n                                       10\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\nPatrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and\n  Eric Wong.  Jailbreaking black box large language models in twenty queries.  arXiv\n  preprint arXiv:2310.08419, 2023.\n\nBanghao Chen, Zhaofeng Zhang, Nicolas Langren´e, and Shengxin Zhu. Unleashing the po-\n   tential of prompt engineering in large language models: a comprehensive review. arXiv\n  preprint arXiv:2310.14735, 2023.\n\nKanzhi Cheng, Zheng Ma, Shi Zong, Jianbing Zhang, Xinyu Dai, and Jiajun Chen. Ads-\n  cap: A framework for accurate and diverse stylized captioning with unpaired stylistic\n  corpora. In CCF International Conference on Natural Language Processing and Chinese Com-\n  puting, pp. 736–748. Springer, 2022.\n\nChuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, and Li Deng.  Stylenet: Generating\n  attractive visual captions with styles.  In Proceedings of the IEEE conference on computer\n  vision and pattern recognition, pp. 3137–3146, 2017.\n\nAnne Geraci. IEEE standard computer dictionary: Compilation of IEEE standard computer glos-\n   saries. IEEE Press, 1991.\n\nJindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruo-\n  tong Liao, Yao Qin, Volker Tresp, and Philip Torr. A systematic survey of prompt engi-\n  neering on vision-language foundation models. arXiv preprint arXiv:2307.12980, 2023.\n\nLongteng Guo, Jing Liu, Peng Yao, Jiangwei Li, and Hanqing Lu. Mscap: Multi-style im-\n  age captioning with unpaired stylized text. In Proceedings of the IEEE/CVF Conference on\n  Computer Vision and Pattern Recognition, pp. 4204–4213, 2019.\n\nRuidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low,\n  Lidong Bing, and Luo Si. On the effectiveness of adapter-based tuning for pretrained\n  language model adaptation. arXiv preprint arXiv:2106.03164, 2021.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Larous-\n   silhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient trans-\n  fer learning for nlp. In International conference on machine learning, pp. 2790–2799. PMLR,\n  2019.\n\nLinmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. A survey of\n  knowledge enhanced pre-trained language models. IEEE Transactions on Knowledge and\n  Data Engineering, 2023.\n\nXiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and\n  Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceed-\n  ings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 17980–17989,\n  2022.\n\nHongwei Jin, Minru Bai, Julio Ben´ıtez, and Xiaoji Liu. The generalized inverses of tensors\n  and an application to linear models. Computers & Mathematics with Applications, 74(3):\n  385–397, 2017.\n\nMisha E Kilmer and Carla D Martin. Factorization strategies for third-order tensors. Linear\n  Algebra and its Applications, 435(3):641–658, 2011.\n\nTamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review,\n  51(3):455–500, 2009.\n\nChenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong\n  Chen, Guohai Xu, Zheng Cao, et al. mplug: Effective and efficient vision-language learn-\n  ing by cross-modal skip-connections. arXiv preprint arXiv:2205.12005, 2022.\n\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\n  image pre-training with frozen image encoders and large language models. In Interna-\n   tional conference on machine learning, pp. 19730–19742. PMLR, 2023.\n\n\n                                       11\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\n  Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In\n  Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-\n  12, 2014, Proceedings, Part V 13, pp. 740–755. Springer, 2014.\n\nShweta Mahajan, Tanzila Rahman, Kwang Moo Yi, and Leonid Sigal. Prompting hard or\n  hardly prompting: Prompt inversion for text-to-image diffusion models. arXiv preprint\n  arXiv:2312.12416, 2023.\n\nAlexander Mathews, Lexing Xie, and Xuming He. Senticap: Generating image descriptions\n  with sentiments. In Proceedings of the AAAI conference on artificial intelligence, volume 30,\n  2016.\n\nRon Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning.\n  arXiv preprint arXiv:2111.09734, 2021.\n\nJacob Nilsson. System of systems interoperability machine learning model. PhD thesis, Lule˚a\n  University of Technology, 2019.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for auto-\n  matic evaluation of machine translation. In Proceedings of the 40th annual meeting of the\n  Association for Computational Linguistics, pp. 311–318, 2002.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\n  Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\n  wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning trans-\n  ferable visual models from natural language supervision. In International conference on\n  machine learning, pp. 8748–8763. PMLR, 2021.\n\nWeijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettle-\n  moyer. Toward human readable prompt tuning: Kubrick’s the shining is a good movie,\n  and a good prompt too? arXiv preprint arXiv:2212.10539, 2022.\n\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Au-\n  toprompt:  Eliciting knowledge from language models with automatically generated\n  prompts. arXiv preprint arXiv:2010.15980, 2020.\n\nYoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. Zero-shot image-to-text generation\n  for visual-semantic arithmetic. arXiv preprint arXiv:2111.14447, 2, 2021.\n\nYoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. Zerocap: Zero-shot image-to-text\n  generation for visual-semantic arithmetic. In Proceedings of the IEEE/CVF Conference on\n  Computer Vision and Pattern Recognition, pp. 17918–17928, 2022.\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\n  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\n  Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n  2023.\n\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based\n  image description evaluation. In Proceedings of the IEEE conference on computer vision and\n  pattern recognition, pp. 4566–4575, 2015.\n\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang\n  Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modali-\n   ties through a simple sequence-to-sequence learning framework. In International Confer-\n  ence on Machine Learning, pp. 23318–23340. PMLR, 2022.\n\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin\n  Jiang, Ming Zhou, et al. K-adapter: Infusing knowledge into pre-trained models with\n  adapters. arXiv preprint arXiv:2002.01808, 2020.\n\n\n                                       12\n\nPublished as a conference paper at COLM 2024\n\n\n\n\n\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Gold-\n   stein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning\n  and discovery. Advances in Neural Information Processing Systems, 36, 2024.\n\nZequn Zeng, Hao Zhang, Ruiying Lu, Dongsheng Wang, Bo Chen, and Zhengjue Wang.\n  Conzic: Controllable zero-shot image captioning by sampling-based polishing. In Pro-\n  ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 23465–\n  23476, 2023.\n\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin\n  Choi, and Jianfeng Gao.  Vinvl: Revisiting visual representations in vision-language\n  models.  In Proceedings of the IEEE/CVF conference on computer vision and pattern recog-\n   nition, pp. 5579–5588, 2021.\n\nYubo Zhang, Xingxing Zhang, Xun Wang, Si-qing Chen, and Furu Wei.  Latent prompt\n  tuning for text summarization. arXiv preprint arXiv:2211.01837, 2022.\n\nWentian Zhao, Xinxiao Wu, and Xiaoxun Zhang. Memcap: Memorizing style knowledge\n  for image captioning. In Proceedings of the AAAI Conference on Artificial Intelligence, vol-\n  ume 34, pp. 12984–12992, 2020.\n\nSicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang,\n  Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable adversarial attacks\n  on large language models. arXiv preprint arXiv:2310.15140, 2023.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\n  ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual expla-\n  nations by watching movies and reading books. In The IEEE International Conference on\n  Computer Vision (ICCV), December 2015.\n\nAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable\n  adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.\n\n\n\n\n\n                                       13\n\nPublished as a conference paper at COLM 2024\n\n\n\n\nA  Tensor Products\n\nRecall that the order (aka. modes or ways) of a tensor is the number of dimensions that\nmake it up. Kolda & Bader (2009) have used one dimensional fibers or two dimensional\nslices to define tensors, where a third-order rank one tensor is defined as,\n                                               ˜A = a ◦b ◦c,\nwhere ◦denotes the outer product operation between vectors a and b, defined as\n              a0b0    · · ·  a0bn \n                      a1b0    · · ·  a1bn\n            a ◦b =           ...      ...                        ...       A ◦b = [A0b0  A1b1    · · ·  Anbn]\n                     anb0    · · ·  anbn\n\nMultiplication between tensors has been introduced in Kilmer & Martin (2011), in terms of\nthe ciruclant matrix, where,\n                                  a = [a0   a1   a2   a3]T\nthen\n                            a0   a3   a2   a1 \n                                            a1   a0   a3   a2\n                                 circ(a) =  a2   a1   a0   a3 .\n                                            a3   a2   a1   a0\n\nIn order to multiply tensors, we first, we define an unfolding operation that reshapes an\nRd1×d2×···×dn tensor into a partitioned tensor in Rd1dn×···×dn−1 tensor and we conversely\ndefine a fold operation to reshape the tensor back into its original shape,\n             unfold( ˜X) =  ˜X1   ˜X2    · · ·   ˜Xn T    fold(unfold( ˜X)) = ˜X.             (7)\n\nUsing this notation, Kilmer & Martin (2011) defines the t-product between tensors recur-\nsively as,\n                             ˜A ∗˜B = fold(circ(unfold( ˜A))) ∗unfold( ˜B))\n                                             ˜A0   ˜A1                = fold(         ∗      )                                             ˜A1   ˜A0             ˜B0˜B1\n                                             ˜A0 ∗˜B0 + ˜A1 ∗˜B1                = fold(                          ),\n                                             ˜A1 ∗˜B0 + ˜A0 ∗˜B1\nwhere circ is the circulant matrix.  It is well known that the circulant matrix has a strong\nconnection to circular convolutions as shown in Bamieh (2018). We can thus think of the\nt-product as a convolution with circular padding,\n                                 ˜A ∗˜B =  ˜A0   ˜A1 ⊗  ˜B0   ˜B1   ˜B0   ,\nwhere ⊗denotes a convolution of ˜A across ˜B, using the t-product instead of the matrix\nmultiplication. In this way, we can express a generalization of the t-product. Kilmer &\nMartin (2011) defined the t-product in terms of, ˜A ∈Rm×k×p1×···pn and ˜B ∈Rk×n×p1×···pn,\nwhere ˜A and ˜B must have their first two dimensions of the appropriate shape for matrix\nmultiplication and each of the remaining dimensions must be the same size for both ten-\nsors.\n\nAs a circular convolution, we can allow arbitrary tensor products as long as the tensors\nare of the same order by applying circular padding. For example, if ˜A ∈Rm×k×2 and\n˜B ∈Rk×n×4, we can express the product as,\n                     ˜A ∗˜B =  ˜A0   ˜A1 ⊗  ˜B0   ˜B1   ˜B2   ˜B3   ˜B0 ∈Rm×n×4\nNote that this product is equivalent to that described in Kilmer & Martin (2011) when ˜A\nand ˜B have the same sized dimensions after dimension 2. Moreover, it is easy to verify\nthat this generalization still follows the same rules of distributivity and associativity as the\nstandard t-product.\n\n\n                                       14\n\nPublished as a conference paper at COLM 2024\n\n\n\n B  Precomputing the Gradient V+                                                                             i Vj\n\n  Algorithm 2: Precomputing the Gradient V+i Vj for words that are tokenized to l tokens\n  Input: Text Corpus C, Language models Mi and Mj\n  Output: Gradient V+i Vj\n 1  l ←only consider words that require l tokens in Mj;\n 2 Ti, Tj ←Tokenizer of model Mi, Mj;\n 3 Ei, Ej ←Mapping from token to embedding of Mi, Mj;\n 4 di, dj ←Dimensionality of Mi, Mj embeddings;\n 5 W ←∅;                                          // Initialize an empty list\n 6 k ←0 ;                          // keep track of max size to tokenize with Ti\n 7 foreach word in C do\n 8      if word /∈W then\n 9            tj ←Tj(word) ;                                // Tokenize a single word\n10        k ←max(k, |Ti(word)|) ;                                      // update k\n11          if  |tj| = l then\n12     W ←W ∪{word} ;            // Add to list if exactly  l tokens in  j\n\n13 Vi ←initialized zero tensor of |W| rows, di columns, and depth l;\n14 Vj ←initialized zero tensor of |W| rows, dj columns, and depth k;\n15 for m ←1 to |W| do\n16       tj ←Tj(W[m]) ;                     // Tokenize word W[m] with tokenizer  j\n17       ti ←Ti(W[m]);\n18    for n ←1 to |tj| do\n19         (Vj)w,:,m ←(Vj)[m, :, n] + Ej((tj)[n]) ;     // Add the embedding of  tj to Vj\n20    for n ←1 to |ti| do\n21         (Vi)w,:,m ←(Vi)[m, :, n] + Ei((ti)[n]) ;      // Add the embedding of  ti to Vi\n22 V+i  ←Pseudoinverse( Vi ) ;                   // According to Jin et al. (2017)\n\n23   ∇EiTi:j(Ei) ←V+i  ∗Vj ;                                // Compute the t-product\n24 return ∇EiTi:j(Ei)\n\n\n\n\n\n                                         15",
"headers": [
"arXiv:2408.04816v1  [cs.CL]  9 Aug 2024",
"FUSE-ing Language Models: Zero-Shot Adapter Discovery",
"for Prompt Optimization Across Tokenizers",
"∑",
"Abstract",
"1",
"Introduction",
"2",
"Background and Related Work",
"3",
"Methodology",
"4",
"Experiments",
"5",
"Results",
"6",
"Conclusion and Future Work",
"References",
"A",
"Tensor Products",
"B",
"Precomputing the Gradient",
"V"
],
"tables": [
"|B-4 M C S|C S|\n|---|---|",
"|BLIP-2 (Li et al., 2023) 43.7 - 145.8 -<br>mPLUG (Li et al., 2022) 46.5 32.0 155.1 26.0<br>OFA (Wang et al., 2022) 44.9 32.5 154.9 26.6<br>CLIP-VL (Tewel et al., 2021) 40.2 29.7 130.3 23.8<br>VinVL (Zhang et al., 2021) 40.9 30.9 140.4 25.1<br>LEMON-B (Hu et al., 2022) 40.3 30.2 133.3 23.3<br>ClipCap (Mokady et al., 2021) 32.2 27.1 108.35 20.12|119.7 15.40<br>114.8 14.8<br>- -<br>- -<br>90.4 13.07<br>79.0 12.3<br>65.7 11.1|\n|---|---|",
"|ZeroCap (Tewel et al., 2022) 2.60 11.50 14.60 5.50<br>ConZIC (Zeng et al., 2023) 1.29 11.23 13.26 5.01<br>Ours ( GPT2-M + VIT-B/32 ) 1.59 14.72 15.93 9.15|- -<br>- -<br>20.65 6.64|\n|---|---|",
"|B-3(↑) M(↑) Acc(↑)|B-3(↑) M(↑) Acc(↑)|\n|---|---|",
"|StyleNet (Gan et al., 2017) 12.1 12.1 45.2<br>MSCap (Guo et al., 2019) 16.2 16.8 92.5<br>MemCap (Zhao et al., 2020) 17.0 16.6 96.1<br>ADS-CAP (Cheng et al., 2022) 18.9 18.5 99.7|10.6 10.9 56.6<br>15.4 16.2 93.4<br>18.1 15.7 98.9<br>21.0 18.0 98.2|\n|---|---|",
"|ConZIC (Zeng et al., 2023) 1.89 5.39 97.2<br>Ours ( GPT2-M + VIT-B/32 + Roberta) 1.91 10.40 83.8|1.78 5.54 99.1<br>2.29 7.42 85.6|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2408.04816v1.pdf"
}