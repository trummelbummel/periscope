{
"text": "LLM Prompt Duel Optimizer: Efficient Label-Free Prompt\n                               Optimization\n\n\n           Yuanchen Wu1∗†   Saurabh Verma2   Justin Lee2   Fangzhou Xiong2\n      Poppy Zhang2  Amel Awadelkarim2  Xu Chen2  Yubai Yuan1  Shawndra Hill2\n                  1Department of Statistics, The Pennsylvania State University  2Meta\n\n\n\n\n                      Abstract                      et al., 2023) rely on reference-based super-\n                                                       vision (e.g., ground-truth labels, gold refer-\n                Large language models (LLMs) are highly\n                                                              ences, or label-derived rewards) to score can-\n                   sensitive to prompts, but most automatic\n                                                            didates on validation sets. In many practical              prompt optimization (APO) methods as-2026         sume access to ground-truth references (e.g.,       settings, however, obtaining such supervision\n                 labeled validation data) that are costly to       at scale is costly and slow (Ratner et al., 2017).\n                  obtain. We propose the Prompt Duel Op-        Still, prompt optimization remains essential;Jan\n               timizer (PDO), a sample-efficient frame-        for example, in industrial LLM-based text clas-\n              work for label-free prompt optimization                                                                      sification, practitioners need reasonably good28                based on pairwise preference feedback from\n                                                   prompts to initiate deployment before large\n              an LLM judge. PDO casts prompt selec-\n                                                        human-labeled datasets are available (Wagner,                  tion as a dueling-bandit problem and com-\n                 bines (i) Double Thompson Sampling to       2024). Such challenges raise a central question:\n                    prioritize informative comparisons under a\n                   fixed judge budget, with (ii) top-performer        • Can we optimize prompts without ground-[cs.CL]\n                guided mutation to expand the candidate            truth label references?\n                pool while pruning weak prompts. Experi-\n               ments on BIG-bench Hard (BBH) and MS                                              One direction is to reduce reliance on hu-\n          MARCO show that PDO consistently iden-\n                                            man annotation by adopting automatic evalua-\n                       tifies stronger prompts than label-free base-\n                                                              tion of model outputs. Recent work suggests                     lines, while offering favorable quality–cost\n                   trade-offs under constrained comparison       that LLMs can serve as judges of model out-\n                 budgets.                                       puts, including in reference-free settings (Zheng\n                                                                et al., 2023; Liu et al., 2023; Gu et al., 2024).\n         1  Introduction                                              As prompt quality is reflected in generated\n                                                             responses, evaluating prompts with an LLM       LLM  performance  hinges  on  well-crafted\n                                                        judge is a natural approach. In this setting,          prompts that unlock their capabilities (Sun\n                                                              rather than scoring each output independently,             et  al., 2023).   Creating  effective prompts\n                                                                                   it is often preferable to rely on pairwise pref-             typically requires extensive trial-and-error orarXiv:2510.13907v2\n                                                  erence feedback: a judge compares outputs              task-specific techniques (e.g., chain-of-thought\n                                                    from two prompts on the same input and se-           prompting for reasoning tasks; Wei et al., 2022),\n                                                                     lects the preferred one. Pairwise comparisons           which often do not transfer across tasks or do-\n                                                                  typically yield a more reliable signal than direct           mains. This limitation motivates Automatic\n                                                             pointwise scoring, which is prone to calibration          Prompt Optimization (APO) (Ramnath et al.,\n                                                                  issues (Liu et al., 2024).             2025), which iteratively generates and evaluates\n            candidate prompts to discover high-performing   LLM preference feedback introduces two\n             instructions.                                      challenges. First, LLM judges are noisy: calls\n              Despite encouraging results across diverse    are non-deterministic (He and Lab, 2025), judg-\n             tasks, most APO methods (Zhou et al., 2022;   ments can exhibit position and verbosity biases\n          Yang et al., 2024; Fernando et al., 2023; Pryzant   (Zheng et al., 2023), and task complexity may\n                                                           amplify these effects. Second, preference-based\n              ∗Work done during an internship at Meta.\n                                                            evaluation scales quadratically with the num-               †Code   is   available   at  https://github.com/\n             meta-llama/prompt-ops                          ber of candidates; each comparison requires\n\n\n                                                    1\n\nan LLM API call and incurs monetary cost,   the Copeland criterion: the Copeland score of\nmaking exhaustive evaluation infeasible.       arm i counts the number of opponents it beats\n  To address these issues, we introduce the   with probability greater than 1/2:\nPrompt Duel Optimizer (PDO), which\ncasts  label-free prompt optimization  as a           C(i) = { j ̸= i : µ(i, j) > 12 } .\ndueling-bandit problem where the cost is the\n                            A Copeland winner  is then an arm  i⋆ ∈\nnumber of judge comparisons.  PDO com-\n                                               arg maxi C(i).bines Double Thompson Sampling (D-TS) to\nfocus comparisons on informative prompt pairs   2.2  Prompt Optimization Through the\nwith a top-performer mutate-and-prune strat-       Lens of Dueling Bandits\negy that expands the pool while discarding\n                                              Building on these definitions, we formalize\nweak candidates. We further assess judge reli-\n                                               preference-based prompt optimization in the\nability via cross-family re-evaluation to probe\n                                                dueling bandits setting.\ncircularity and show that PDO’s gains are ro-\nbust across judge choices.                 Problem Setup.  Let X denote the input\n We summarize our contributions as follows:   space and P = {p1, . . . , pK} a finite set of can-\n                                                 didate prompts. Let DX be a distribution over\n  1. We formalize label-free prompt optimiza-\n                                      X. For p ∈P and x ∼DX , let fp(x) denote\n     tion with LLM-judged pairwise compar-                                                the LLM output when applying prompt p to in-\n     isons as a dueling-bandit problem.\n                                            put x. To compare the two prompts pi, pj ∈P\n                                         on the same input x, we query an LLM judge  2. We propose PDO, integrating D-TS for\n                                        and record a binary preference:     sample-efficient prompt selection with top-\n    performer guided mutation that steers                                     (                                                                               1,   if fpi(x) ≻fpj(x),\n     search toward stronger regions and yields      Judgex(pi, pj) =\n     better candidates.                                                      0,  otherwise.\n\n  3. We show empirically on BBH and MS   Given a set of unlabeled examples {xi}ni=1, the\n   MARCO that PDO outperforms label-free   empirical estimate of µ(pi, pj) becomes\n     baselines under fixed judge budgets, and\n                                                               n\n    analyze quality–cost trade-offs via budget                1\n                                                          pj) = X 1 Judgexk(pi, pj) = 1 .    and runtime comparisons.                           bµ(pi,      n\n                                                              k=1\n\n  4. We analyze judge noise and potential circu-                                 Prompt Optimization Objective.  The\n      larity, and demonstrate that PDO’s gains                                                goal of prompt optimization is to identify a\n     persist under alternative judge families.                                        prompt that maximizes task performance. In\n                                              the absence of ground-truth references, we2  Preliminaries and Problem Setup\n                                              use pairwise preferences as a practical proxy\nIn this section, we establish the connection    for selecting high-quality prompts. Using em-\nbetween preference-based prompt optimization    pirical estimates bµ(pi, pj), we select the Con-and dueling bandits.                            dorcet winner when it exists, or otherwise the\n                                         Copeland winner.  In practice, limited API\n2.1  Background: Dueling Bandits\n                                             budgets make exhaustive estimation (O(n|P|2)\nIn the K-armed dueling bandit problem (Bengs   comparisons) infeasible, motivating sample-\net al., 2021), at each round the decision-maker    efficient methods that adaptively target in-\nselects two arms i and j to duel, and the com-   formative pairs while aligning with the Con-\nparison yields a stochastic preference outcome.   dorcet/Copeland objectives.\nSpecifically, the probability that i is preferred\nto j is                               3  Prompt Duel Optimizer (PDO)\n               µ(i, j) = Pr(i ≻j).\n                                     To address the above challenge, we propose\nA Condorcet winner  is an arm i∗such that   the Prompt Duel Optimizer (PDO), an\nµ(i∗, j) > 0.5 for all j ̸= i∗. When no Con-   algorithm designed to identify high-performing\ndorcet winner exists, a standard choice rule is   prompts under limited comparison budgets.\n\n\n                                         2\n\nPer-round Procedure (t = 1, 2, . . . , T):\n\n                                                          1. First Prompt Selection. Compute an opti-\n                                                      mistic Copeland score for each prompt:\n\n                                                                                   bζi(t) = K−11 X 1{uij(t) ≥0.5}\n                                                                                                       j̸=i\n\n                                          Keep only prompts with the maximum\n                                                     score  in a  set  ζ(t) =  { i   |   bζi(t) =\n                                          maxk bζk(t) }. For each i ∈ζ(t), draw inde-\n                                               pendent samples θ(1)ij  and count\n\n                                                                            si = X 1{θ(1)ij ≥0.5}\n                                                                                                   j̸=i\n\nFigure 1: Workflow of the Prompt Duel Optimizer.        Select prompt i⋆= arg maxi∈ζ(t) si.\n                                                          2. Second Prompt Selection. Restrict to only\n                                                    uncertain opponents for i⋆,\nPDO combines two complementary compo-\nnents:  (1) Double Thompson Sampling (D-              Si⋆(t) = {j ̸= i⋆:  li⋆j(t) ≤0.5}\nTS) (Wu and Liu, 2016), a Bayesian strat-\negy for efficient pairwise evaluation that tar-      Draw independent samples θ(2)ji⋆and select\ngets Copeland-optimal winners; and (2) Top-                                    j⋆= arg maxj∈Si⋆(t) θ(2)ji⋆Performer Guided Mutation, which adaptively\n                                                          3. Duel and update. Judge prompts (pi⋆, pj⋆),\nmutates top-performing prompts to expand\n                                                    record the winner, and update Wi⋆j⋆.\ncoverage of the search space.\n                                            3.2   Efficient Prompt Discovery via\n3.1   Efficient Prompt Selection via                                           Top-Performer Guided Mutation\n     Double Thompson Sampling\n                                         While D-TS identifies the Copeland-optimal\nD-TS extends Thompson Sampling (Agrawal   prompt within a fixed pool, the broader goal of\nand Goyal, 2012) to the dueling bandit set-  PDO is to locate the global optimum p∗over\nting, where feedback comes from pairwise com-   a combinatorially large prompt space. Since\nparisons rather than scalar rewards.  Each   exhaustive search is infeasible, PDO incremen-\nround uses two independent Thompson draws    tally expands the candidate pool through mu-\nto concentrate queries on informative duels and    tation of top-performing prompts.\nguides the search toward a Copeland-optimal\nprompt.                              Mutation Procedure.  At round t, let the\n                                               current pool be Pt = {p1, . . . , pKt} with the\nNotations.  For each pair (pi, pj), let Wij de-   empirical Copeland scores bCt :=  bCt(p) p∈Pt.\nnote the current number of wins of pi over   The procedure is:\npj, and let Nij = Wij + Wji be the total\nnumber of duels. The Bayesian posterior for      1. Selection:  Choose the top-performing\nthe probability that pi beats pj  is modeled       prompt p∗t = arg maxp∈Pt bCt.\nas θij ∼Beta(Wij + 1, Wji + 1). For α > 0,      2. Mutation: Generate a variant pnew of p∗t\nthe corresponding upper and lower confidence        via template edits, text-gradient guided\nbounds are defined as                               changes, or LLM-assisted rewrites.\n                                                          3. Expansion:  Update pool Pt+1 = Pt ∪\n          s                    α log t                   {pnew}.              Wij\n         uij(t) =   +                Nij     max{1, Nij},                                            3.3  Theoretical insights of PDO\n          s              Wij      α log t         PDO can be viewed as a two-level optimization\n          lij(t) =   −                Nij     max{1, Nij}.         scheme that alternates selection and discovery.\n\n\n                                         3\n\nWithin any fixed candidate pool, D-TS pro-   Algorithm 1: Prompt Duel Optimizer\nvides a principled way to allocate pairwise com-                                        Require : P0 (initial pool); judge J ;\nparisons and is known to achieve O(K2 log T)                                                        batch size m; total rounds T;\nexpected Copeland regret in general settings,                                                       mutation period M\nimplying vanishing average regret and conver-                                             1 P ←P0; initialize W, N ∈R|P|×|P| to\ngence to the Copeland-winner set as the com-                                                         zeros;\nparison budget grows (Wu and Liu, 2016).                                             2 for t ←1 to T do\n  To move beyond a fixed pool, PDO period-   3     (pi, pj) ←D-TS(P, W, N) ;\nically expands the candidate set by mutating                                               // prompt selection via D-TS\nthe current Copeland leader. Under a stan-   4     (wi, wj) ←Duel(pi, pj, m, J ) ;\ndard smoothness assumption that prompt util-                                               // get win counts\nity varies gradually under local edits (e.g., L-   5    W[i, j] ←W[i, j] + wi;\nLipschitz with respect to a prompt distance),   6    W[j, i] ←W[j, i] + wj;\nthis strategy concentrates new candidates in   7    N[i, j] ←N[i, j] + m;\nincreasingly competitive neighborhoods, anal-   8    N[j, i] ←N[j, i] + m;\nogous to the “zooming-in” intuition in Lips-   9      if t mod M = 0 then\nchitz/metric bandits (Kleinberg et al., 2008).   10        p⋆t ←CurrentBest(P, W, N) ;\n  Taken together, PDO separates prompt opti-                                                  // Copeland rank\nmization into (i) sample-efficient identification  11        pnew ←Mutate(p⋆t ) ;\nof the best prompt among current candidates                                                  // generate new prompts\n(via D-TS) and (ii) targeted exploration that                                                  via top-performer guided\nincreases coverage of near-optimal regions (via                                                  mutation\ntop-performer mutation), which helps explain  12     P ←P ∪{pnew};\nits empirical efficiency under limited judge bud-  13      Expand W, N with zero\ngets.                                              row/column for pnew;\n                                            14 return CurrentBest(P, W, N) ;\n4  Experiment and Results\n                                            // final Copeland winner\n4.1  Experimental Setup\n\nDatasets.  In this section, we conduct ex-\n                          LLM Judge Design.  For multiple-choice\nperiments to evaluate PDO on both closed-\n                                                   tasks, we require the LLM to produce both\nended multiple-choice tasks and open-ended\n                                        an answer and the accompanying reasoning\nQA tasks. For the multiple-choice setting, we\n                                               given the instruction prompt. We then apply\nselect 16 tasks from Big-Bench-Hard (Suzgun\n                                           a dual-judge approach:  if two prompts yield\net al., 2022), using accuracy as the evaluation\n                                                     different answers, the LLM judge selects the\nmetric. For the open-ended QA setting, we con-\n                                        prompt with the correct answer; if they yield\nsider four task categories from MS-MARCO\n                                              the same answer, the judge decides based on\n(Bajaj et al., 2016), where the final evaluation\n                                                the quality of reasoning. For open-ended tasks,\nmetric is an integer score between 1 and 5, as-\n                                                the LLM judge design is more straightforward:\nsigned by an LLM judge comparing the model’s\n                                              each pair of responses is evaluated on accuracy,\noutput with the ground-truth answer provided\n                                                completeness, relevance, and clarity, consistent\nby the original dataset.\n                                            with the criteria of the final evaluation met-\n  Across both task types, for the results re-\n                                                               ric. To mitigate position bias, the order of the\nported in this section, we randomly split the\n                                           two outputs in each prompt pair is randomized\ndata into development and test sets with a\n                                                before being fed into the judge template. A\n50/50 split ratio. We report the test set perfor-\n                                                   detailed rationale and analysis of the LLM pref-\nmance averaged over 10 runs. The Llama-3.3-\n                                               erence judge design are provided in Section 5\n70B-Instruct model is used for prompt genera-\n                                        and Appendix E.\ntion, preference judging, and final evaluation.\nDetailed descriptions of the datasets and the   Baselines. PDO is designed for prompt opti-\nexperimental setup of PDO are provided in   mization without access to ground-truth labels\nAppendix C.                                   or external references. A directly comparable\n\n\n                                         4\n\nMethod      Causal     Date    DisambigQA   Formal   Geometric Hyperbaton  Logical-5    Logical-7\n No prompt   0.661±0.044  0.854±0.024    0.698±0.047    0.739±0.031  0.434±0.036   0.900±0.020   0.785±0.018   0.739±0.033\n CoT         0.653±0.042  0.877±0.019    0.720±0.039    0.725±0.027  0.422±0.028   0.891±0.023   0.761±0.027   0.726±0.025\n PoS          0.652±0.037  0.878±0.019    0.698±0.043    0.739±0.027  0.403±0.030   0.896±0.024   0.798±0.032   0.750±0.026\n SPO         0.655±0.033  0.884±0.017    0.725±0.057    0.738±0.018  0.650±0.069   0.886±0.031   0.787±0.031   0.721±0.026\n PDO (ours)  0.681±0.040  0.918±0.014   0.738±0.050   0.744±0.030  0.598±0.073   0.910±0.029  0.804±0.034   0.711±0.019\n Method     Navigate   Penguins     Salient      Snarks   Tracking-5  Tracking-7  Tracking-3 Web of Lies\n No prompt   0.869±0.013  0.915±0.018    0.698±0.020    0.823±0.027  0.695±0.033   0.499±0.020   0.890±0.019   0.766±0.020\n CoT         0.878±0.016  0.915±0.023   0.709±0.021   0.833±0.023  0.724±0.046   0.532±0.025   0.904±0.019   0.796±0.022\n PoS          0.866±0.019  0.910±0.027    0.693±0.025    0.816±0.026  0.725±0.030   0.538±0.034   0.888±0.019   0.861±0.019\n SPO         0.874±0.035  0.934±0.025    0.662±0.038    0.820±0.046  0.692±0.046   0.543±0.026   0.826±0.087   0.818±0.043\n PDO (ours)  0.900±0.023  0.937±0.034   0.681±0.032   0.840±0.039  0.796±0.084  0.641±0.089  0.930±0.046  0.942±0.040\n\nTable 1: Test results on 16 BBH tasks averaged over 10 runs. For PDO, we report the test accuracy of\nthe prompt selected by the highest Copeland score. PDO is compared with baselines that assume no\naccess to ground-truth labels. The best performance is shown in bold, and the second-best is underlined.\n\n\n\n\n\n      (a) Description                (b) Entity                   (c) Numeric                (d) Location\n\nFigure 2: Test performance of the winning prompt on the four MS-MARCO tasks. Each curve shows\nthe mean score of the current Copeland leader over rounds, with 50 duels per round. PDO with D-TS\nconsistently outperforms RUCB and Random sampling, and surpasses the SPO baseline within a few\nrounds. Gray lines indicate the median test score across all prompts generated by PDO.\n\n\nbaseline is SPO (Xiang et al., 2025), which   4.2  Benchmark Results\nsimilarly uses an LLM judge to iteratively com-                                          Multiple-choice Tasks Performance.  As\npare the outputs of two prompts and select                                         shown in Table 1, using only preference signals\nthe winning prompt. We also include classi-                                          from the LLM judge and selecting the winner\ncal prompting techniques including chain-of-                                         prompt by Copeland scores, PDO achieves the\nthought COT (Wei et al., 2022) and plan-and-                                                highest evaluation accuracy on 13/16 tasks.\nsolve PoS (Wang et al., 2023).                                           Notable gains include Tracking-7 (0.641 vs.\n                                                  0.543, +9.8pp) and Web of Lies (0.942 vs.\n                                                   0.861, +8.1pp).\n\n  For the Big-Bench-Hard (BBH) dataset, we   Open-ended QA Tasks. We evaluate four\nalso evaluate PDO under a supervised set-  MS-MARCO tasks (Description, Entity, Nu-\nting to enable comparison with popular super-   meric, and Location), starting from a pool\nvised APO methods, including APE (Zhou    of |P| = 50 instructions.  At each round\net  al., 2022), OPRO (Yang et  al., 2024),    t, we snapshot the win matrix Wt, compute\nand Breeder (Fernando et al., 2023). The   Copeland scores, and report the test perfor-\noptimization itself remains label-free; labels   mance of the current Copeland winner. Fig-\nare introduced only in the final stage to se-   ure 2 reports averages over 30 independent\nlect the prompt that achieves the highest    runs, comparing D-TS with the dueling-bandit\ndevelopment-set accuracy among candidates    alternative Relative Upper Confidence Bound\ngenerated through initial proposals and prompt  (RUCB; Zoghi et al. 2013), uniform Random\nmutations. Results for the without-label and   sampling, and the SPO baseline. Across all\nwith-label settings are reported in Tables 1 and    tasks, D-TS achieves the highest scores and\n6, respectively.                                 converges more quickly than RUCB and Ran-\n\n\n                                         5\n\ndom. The horizontal reference lines mark the\nmedian score of all prompts generated by PDO\n(gray) and the SPO baseline (red). Within just\na few rounds, D-TS surpasses both reference\nlines and maintains this advantage over RUCB\nand Random. These results highlight the sam-\nple efficiency of D-TS: it consistently identifies\nstronger prompts faster and more reliably than\nrandom sampling or alternative dueling-bandit    (a) Judge noise across dif-  (b) Noise reduction on Ge-\n                                                              ferent BBH tasks.         ometric using Claude 4.5.\nmethods.\n\n                                                   Figure 3: (a) The accuracy of the LLM judge varies\n5 LLM Judge Analysis                     across tasks, with some tasks (e.g., Geometric)\n                                                 showing persistent misjudgments compared to an or-\n                                                          acle judge. (b) Using a frontier LLM judge (ClaudeIn the experiments reported above, we use two\n                                                         4.5) helps reduce the impact of noisy judgements.\ntypes of LLM judges. The first is the prefer-\nence judge used during LLM prompt optimiza-\ntion to compare outputs from two candidate   Reducing Judge Noise using frontier\nprompts. The second is the evaluation judge,  model. We hypothesize that a key source\nwhich provides the final score for the open-   of judge noise on BBH tasks arises from het-\nended MS-MARCO task. In this section, we   erogeneous task difficulty: when two prompts\npresent additional experiments to analyze the   produce different answers, the preference judge\nnoise and potential bias introduced by both   must determine which output is correct, and\njudge types.                                       this decision becomes unreliable on harder\n                                                tasks for judges with limited reasoning abil-\n5.1  Preference Judge                               ity. A natural remedy is to replace the current\n                                            Llama-3.3 preference judge with a stronger\nCorrelation Between Judge Noise and\n                                                     frontier model that better approximates an\nTask Performance. We proxy preference\n                                                      oracle, therefore reducing decision noise.\njudge effectiveness for the 16 BBH tasks by\n                                                Table 2 reports the selection accuracy of the\nthe test accuracy gap between Table 1 (prompt\n                                                preference judge on examples where the two\nchosen by Copeland score, label-free) and Ta-\n                                               candidate answers differ. With Llama-3.3-70B\nble 6 (prompt chosen by development-set accu-\n                                                as the judge, accuracy is high on Tracking-7\nracy). In our results, Tracking-7 and Web of\n                                        and Web of Lies but only marginally above\nLies show small gaps and achieve top perfor-\n                                              chance on Geometric, consistent with the trend\nmance under both selection criteria, whereas\n                                                   in Figure 3a. Substituting Claude-4.5-Sonnet\nGeometric exhibits a large discrepancy of 11.4\n                                                 as the preference judge substantially improves\npercentage points.\n                                                   selection accuracy on Geometric and yields\n  To investigate further, for each task we fix                                                   near-oracle performance on the other two tasks.\n|P| = 20 instruction prompts with varying ac-                                     As a result, when replacing the preference\ncuracy, run D-TS with the LLM judge, and                                               judge with Claude 4.5, the test accuracy of the\ncompare it against an oracle judge that always                                          prompts on Geometric found by PDO exceeds\nselects the higher-accuracy prompt in each duel.                                                that of SPO across LLM judge API budgets as\nAt round t, we record the ground-truth ac-                                              reported in Figure 3b. We report additional\ncuracy rank of the current Copeland leader.                                                     results on three other BBH tasks in Figure 7.\nFigure 3a shows that the oracle converges to\nthe best prompt by round 4; with the LLM     Judge Model   Geometric Tracking-7 Web of Lies\njudge, Tracking-7 steadily improves to rank 2      Llama-3.3-70B        0.59         0.89         0.85\n                                                                 Claude-4.5-Sonnet     0.87         0.98         0.99\nand Web of Lies approaches rank ≈2.5, while\nGeometric remains around ranks 6–8 across   Table 2:  Preference-judge selection accuracy on\nrounds. These trends confirm that judge re-  BBH instances where two candidate prompts pro-\nliability is closely related to the performance   duce different answers (1 = always selects the cor-\ngaps observed in Tables 1 and 6.                   rect answer; 0.5 = random guessing).\n\n\n                                         6\n\n5.2  Evaluation Judge                     with its default 20 rounds and for PDO early-\n                                           stopped at 10 rounds (Table 4). We use 10Cross-family  Evaluation.  For  the MS\n                                           rounds for PDO because Figure 2 indicatesMARCO results in Section 4, we use the same\n                                                that by round 10, the prompt selected by PDOLlama 3.3 model both to guide prompt opti-\n                                               already surpasses SPO in test accuracy.mization and to score the final prompt, which\n                                  SPO is faster per run largely because it com-may introduce judge circularity (i.e., the op-\n                                               pares only two prompts per round and imme-timized prompt is favored by the same model\n                                                   diately eliminates the loser. However, withoutused for evaluation). To probe this issue be-\n                                        an explicit exploration–exploitation strategy,yond a single prompt pair, we form two prompt\n                                                      this procedure can be brittle under noisy com-sets by sampling uniformly at random:  10\n                                                    parisons, which helps explain its weaker perfor-prompts produced by PDO and 10 prompts pro-\n                                          mance.duced by SPO (all with their original Llama 3.3\n                                                      Practically, as discussed in Section 5.1, wejudge scores recorded during optimization). To\n                                                 control judge noise by using a strong butavoid additional inference variability, we use\n                                              expensive model (e.g., Claude-4.5) to guidecached model outputs for each prompt on the\n                                              the search, and then deploying the optimizedsame evaluation set and re-score these fixed\n                                         prompt on a lower-cost model (e.g., Llama-3.3)outputs against the reference answers using\n                                              that serves production traffic. In this setting,judges from different model families.\n                                     D-TS is especially effective because it achieves  Table 3 reports cross-family re-scoring re-\n                                                  better prompts with fewer expensive judge calls.sults  across the 10 sampled prompts per\nmethod. PDO remains higher than SPO under\nevery judge, with consistently positive margins,      Method Description Entity Numeric Location\nsuggesting that the observed improvement is      PDO        4.39      3.39    3.83     3.10\nnot merely an artifact of circularity with the       SPO          6.78       6.74     6.17       7.45\noriginal Llama 3.3 judge.\n                                                Table 4: Mean wall-clock time (minutes) on MS\n  In Appendix A, we include a small-scale hu-                               MARCO across four prompt tasks.\nman agreement study testing whether the LLM\njudge evaluation on MS-MARCO aligns with\nhuman preferences on examples where PDO\nand SPO outputs receive different scores.\n\n\n  Judge Model      PDO     SPO    ∆\n  Llama 3.3 (original)   4.68 ± 0.06   4.50 ± 0.07   0.18\n   Mistral-Large          4.61 ± 0.07   4.49 ± 0.07   0.12\n  GPT-4o               4.60 ± 0.06   4.41 ± 0.06   0.19\n   Claude 3.5 Sonnet     4.45 ± 0.08   4.25 ± 0.08   0.20\n   Claude 4.5 Sonnet     4.58 ± 0.06   4.49 ± 0.06   0.09\n\nTable 3: Cross-family LLM-judge re-evaluation on\nMS MARCO (Description).\n                                                   Figure 4: Effect of top-performer–guided mutation\n                                             on Web of Lies (left) and Tracking-7 (right).\n6  Ablation Study\n\nCost Analysis. We make two kinds of LLM  Prompt Mutation. PDO incrementally ex-\ncalls in prompt optimization: inference calls to   pands the prompt pool by mutating top-\ngenerate a prompt’s outputs and judge calls to   performing prompts. We visualize its effect\ncompare outputs pairwise. Because our evalua-   on Web of Lies and Tracking-7. In the mu-\ntion is empirical, we assess sample efficiency by   tate variant, at rounds 10 and 20 we prune\ncomparing methods under the same judge-call   the 10 lowest–Copeland-score prompts and\nbudget.                                         replace them with 10 new candidates gener-\n  Figure 2 shows that D-TS consistently finds   ated by mutating the current Copeland leader.\nbetter prompts than random sampling given   The non mutate baseline keeps the original 20\nthe same number of judge calls. We also report   prompts fixed throughout. At each round, we\nmean wall-clock time on MS MARCO for SPO    report the ground-truth accuracy of the current\n\n\n                                         7\n\nCopeland leader. As shown in Figure 4, both   7  Related Work\nmethods behave similarly before round 10, but\n                         APO Methods:  from Supervised tomutation yields consistently higher accuracy af-\n                                             Label-free.  Traditional prompt optimizationterward, surpassing the limits of the fixed pool\n                                        methods rely heavily on supervised signalsand navigating toward better prompt regions.\n                                          from labeled validation sets, where the typi-\n We additionally report results across all 16\n                                                      cal pipeline involves generating, mutating, and\nBBH tasks in Table 7. In total, 8 out of 16\n                                                 scoring prompts (Zhou et al., 2022; Fernando\ntasks show statistically significant gains under\n                                                   et al., 2023; Pryzant et al., 2023; Guo et al.,\nmutation, and mutation never underperforms\n                                              2023; Schulhoff et al., 2024). More recently,\nrelative to the non-mutation baseline.\n                                                   studies have reduced supervision requirements\n                                            (Xiang et al., 2025; Chen et al., 2024; Madaan\nPairwise Preference vs. Pointwise Scor-                                                   et al., 2023; Cheng et al., 2023; Dong et al.,\ning. PDO relies on the pairwise preferences                                                2022). Notably, Self-Supervised Prompt Opti-\nof an LLM judge to select the winning prompt.                                               mization (SPO) (Xiang et al., 2025) eliminates\nTo highlight the benefit of this formulation, we                                                 external references by using output-vs-output\ncompare it against a pointwise scoring judge,                                              comparisons, iteratively generating, executing,\nwhere each prompt is evaluated independently                                         and comparing prompts through pairwise LLM\non the development set by assigning a numeric                                            judgments.  However, SPO follows a greedy\nscore (1–5) without access to ground-truth                                                   hill-climbing loop without a principled explo-\nanswers. The prompt with the highest aver-                                                  ration–exploitation strategy, limiting its ability\nage score is then selected. We evaluate both                                                to allocate comparisons efficiently and to ro-\nstrategies on the same fixed pool of 50 can-                                                bustly identify high-performing prompts.\ndidates from MS-MARCO in 4.2 and report\nthe test mean score of the selected prompt.  Bandit-Based  Prompt  Optimization.\nFigure 5 shows that preference judgement con-  The connection between prompt optimiza-\nsistently outperforms pointwise scoring in 7 of    tion and multi-armed bandits (MAB) has re-\n8 model–task combinations across two judge    cently gained attention, as prompts can nat-\nmodels. Overall, these results are consistent    urally be viewed as arms. OPTS (Ashizawa\nwith the LLM-as-judge literature:  pairwise    et al., 2025) formulates prompt strategy se-\ncomparisons yield more stable decisions than di-   lection as a bandit problem with Thompson\nrect pointwise scoring by reducing dependence   sampling. TRIPLE (Shi et al., 2024) casts\non calibration (Liu et al., 2024).              prompt optimization as fixed-budget best-arm\n                                                     identification, assuming a predefined set of\n                                         prompts with known scores. However, both\n                                            approaches require labeled validation sets for\n                                                   scoring. APOHF (Lin et al., 2025) connects\n                                               preference-based prompt optimization to du-\n                                                    eling bandits, but assumes human-annotated\n                                                pairwise preferences that are impractical at\n                                                       scale. PDO retains the dueling-bandit formu-\n                                                    lation while replacing human preferences with\n                                        LLM-judge comparisons to align with realistic\n                                        prompt optimization settings.\n\n                                   8  Conclusion\n\nFigure 5: Comparison of pairwise preference used in   In this paper, we introduce PDO, a prompt op-\nPDO and pointwise scoring for prompt selection on    timization method that reduces dependence on\nMS-MARCO. Results indicate that pairwise pref-   costly supervision by formulating label-free op-\nerence leads to higher-performing prompts across    timization as a dueling bandit problem guided\nmodel sizes (7 out of 8 cases).                                        by LLM preference feedback. By combining\n                                          Double Thompson Sampling (D-TS) with top-\n\n\n                                         8\n\nperformer–guided mutation, PDO adaptively      2025, pages 20799–20817, Vienna, Austria. Asso-\nsearches for informative prompt comparisons       ciation for Computational Linguistics.\nto identify high-performing prompts. Experi-\n                                                Payal  Bajaj,  Daniel Campos,  Nick  Craswell,\nments on BBH and MS-MARCO datasets show                                                      Li Deng, Jianfeng Gao, Xiaodong Liu, Ran-\nthat PDO consistently achieves competitive     gan Majumder, Andrew McNamara, Bhaskar\nperformance against baseline methods across      Mitra, Tri Nguyen, Mir Rosenberg, Xia Song,\n                                                    Alina Stoica, Saurabh Tiwary, and Tong Wang.tasks. We analyze judge noise and potential\n                                                     2016. Ms marco: A human generated machine\ncircularity, and demonstrate that PDO’s gains                                                     reading comprehension dataset. arXiv preprint\npersist under alternative judge families.            arXiv:1611.09268.\n\nLimitation                                    Viktor  Bengs,  R´obert  Busa-Fekete,  Adil  El\n                                                  Mesaoudi-Paul, and Eyke H¨ullermeier. 2021.\n                                                     Preference-based online learning with duelingWhile our dueling-bandit framework provides\n                                                          bandits: A survey. Journal of Machine Learning\na practical and sample-efficient approach to                                                      Research, 22(7):1–108. Open access.\nprompt optimization without ground-truth la-\nbels, it also has inherent limitations. The ca-   Yongchao Chen, Jacob Arkin, Yilun Hao, Yang\npability of the preference judge plays a critical     Zhang, Nicholas Roy, and Chuchu Fan. 2024.\n                                       PRompt  optimization  in  multi-step  tasks\nrole: depending on task complexity, the judge                                        (PROMST): Integrating human feedback and\nmay be noisy. Stronger foundation models tend      heuristic-based sampling. In Proceedings of the\nto yield more reliable preferences but incur sub-     2024 Conference on Empirical Methods in Nat-\nstantially higher cost, whereas smaller or less       ural Language Processing, pages 3859–3920, Mi-\n                                                   ami, Florida, USA. Association for Computa-specialized models may introduce greater noise,\n                                                           tional Linguistics.\nparticularly on domain-specific tasks. Because\nthe algorithm optimizes for the judge’s notion    Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke,\nof quality rather than the true task metric, it     Hongning Wang, Yuxiao Dong, Jie Tang, and\n                                                       Minlie Huang. 2023. Black-box prompt optimiza-may favor stylistic patterns that align with the\n                                                             tion: Aligning large language models without\njudge’s preferences.                                               model training. ArXiv, abs/2311.04155.\n  To mitigate these concerns, we include a de-\ntailed discussion and ablation studies on LLM   Yihong Dong, Kangcheng Luo, Xue Jiang, Zhi Jin,\n                                              and Ge Li. 2022. Pace: Improving prompt withjudge design, along with a small-scale human\n                                                              actor-critic editing for large language model. In\nevaluation, in Section 5 and Appendix E. How-                                                   arXiv preprint arXiv:2212.XXXX.\never, we were unable to conduct broader exper-\niments across a wider range of tasks. Despite   Chrisantha  Fernando,  Dylan  Banarse,  Hen-\n                                                  ryk Michalewski, Simon Osindero, and Timthese challenges, our formulation provides a\n                                                          Rockt¨aschel.  2023.    Promptbreeder:    Self-\nprincipled and scalable starting point for label-                                                             referential self-improvement via prompt evolu-\nfree prompt optimization using preference feed-       tion. arXiv preprint arXiv:2309.16797.\nback, and we hope it motivates future work on\nimproving the alignment between LLM judges    Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang\n                                                 Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Ying-\nand task objectives.                                             han Shen, Shengjie Ma, Honghao Liu, Saizhuo\n                                            Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao,\n                                                       Lionel M. Ni, and Jian Guo. 2024. A survey on\nReferences                                            llm-as-a-judge. arXiv preprint arXiv:2411.15594.\n\nShipra Agrawal and Navin Goyal. 2012.  Analy-   Qingyan Guo, Rui Wang, Junliang Guo, Bei Li,\n   sis of thompson sampling for the multi-armed      Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,\n  bandit problem. In Proceedings of the 25th An-     and Yujiu Yang. 2023. Evoprompt: Connect-\n  nual Conference on Learning Theory, volume 23      ing llms with evolutionary algorithms yields\n   of Proceedings of Machine Learning Research,      powerful prompt optimizers.  arXiv preprint\n  pages 39.1–39.26, Edinburgh, Scotland. PMLR.      arXiv:2309.08532.\n\nRin Ashizawa, Yoichi Hirose, Nozomu Yoshinari,   Horace He and Thinking Machines Lab. 2025.\n  Kento Uchida, and Shinichi Shirakawa. 2025.     Defeating  nondeterminism  in  llm  inference.\n  Bandit-based prompt design strategy selection     Thinking  Machines  Lab:    Connectionism.\n  improves prompt optimizers. In Findings of the      Https://thinkingmachines.ai/blog/defeating-\n  Association for Computational Linguistics: ACL      nondeterminism-in-llm-inference/.\n\n\n                                         9\n\nRobert Kleinberg, Aleksandrs Slivkins, and Eli Up-    beam search. In Proceedings of the 2023 Con-\n   fal. 2008. Multi-armed bandits in metric spaces.      ference on Empirical Methods in Natural Lan-\n  In Proceedings of the 40th Annual ACM Sympo-     guage Processing, pages 7957–7968. Association\n  sium on Theory of Computing (STOC), pages       for Computational Linguistics.\n  681–690. ACM.\n                                              Kiran  Ramnath,  Kang  Zhou,  Sheng  Guan,\n                                           Soumya Smruti Mishra, Xuan Qi, ZhengyuanJunpei Komiyama, Junya Honda, Hisashi Kashima,\n                                                   Shen,  Shuai Wang, Sangmin Woo,  Sullam  and Hiroshi Nakagawa. 2015. Regret lower bound\n                                                   Jeoung, Yawei Wang, Haozhu Wang, Han Ding,  and optimal algorithm in dueling bandit prob-\n                                             Yuzhe Lu, Zhichao Xu, Yun Zhou, Balasubra-  lem. In Proceedings of The 28th Conference on\n                                           maniam Srinivasan, Qiaojing Yan, Yueyan Chen,  Learning Theory (COLT), pages 1141–1154.\n                                              Haibo Ding, and 2 others. 2025. A systematic\n                                                    survey of automatic prompt optimization tech-Xiaoqiang Lin, Zhongxiang Dai, Arun Verma,\n                                                         niques. arXiv preprint arXiv:2502.16923v2.  See-Kiong  Ng,  Patrick  Jaillet,  and Bryan\n  Kian Hsiang Low. 2025. Prompt optimization                                                Alexander Ratner, Stephen H. Bach, Henry Ehren-\n  with human feedback.                                                        berg, Jason Fries, Sen Wu, and Christopher R´e.\n                                                       2017. Snorkel: Rapid training data creation with\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,                                              weak supervision. Proceedings of the VLDB En-\n  Ruochen Xu, and Chenguang Zhu. 2023. G-eval:                                                dowment, 11(3):269–282.\n  NLG evaluation using gpt-4 with better human\n  alignment. In Proceedings of the 2023 Conference   Samuel Schulhoff and 1 others. 2024. A systematic\n  on Empirical Methods in Natural Language Pro-      survey of prompting techniques. arXiv preprint\n   cessing, pages 2511–2522, Singapore. Association      arXiv:2406.06608.\n   for Computational Linguistics.\n                                              Chengshuai Shi, Kun Yang, Jing Yang, and Cong\nYinhong Liu, Han Zhou, Zhijiang Guo, Ehsan      Shen. 2024.    Efficient prompt optimization\n  Shareghi, Ivan Vuli´c, Anna Korhonen, and Nigel      through the lens of best arm identification. In\n   Collier. 2024. Aligning with human judgement:     Neural Information Processing Systems.\n  The role of pairwise preference in large language\n                                           Yanan Sui, Vincent Zhuang, Joel W. Burdick, and\n  model evaluators.  In Proceedings of the First\n                                                 Yisong Yue. 2017. Multi-dueling bandits with\n  Conference on Language Modeling (COLM 2024),\n                                                  dependent arms. In Proceedings of the 33rd Con-\n  page –.\n                                                         ference on Uncertainty in Artificial Intelligence\n                                                  (UAI).\nAman Madaan, Niket Tandon, Prakhar Gupta,\n  Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri    Jiuding Sun, Chantal Shaib, and Byron C. Wal-\n  Alon, Nouha Dziri, Shrimai Prabhumoye, Yim-       lace. 2023. Evaluating the zero-shot robustness\n  ing Yang, Sean Welleck, Bodhisattwa Prasad       of instruction-tuned language models.  arXiv\n  Majumder, Shashank Gupta, Amir Yazdan-      preprint arXiv:2306.11270.\n  bakhsh, and Peter Clark. 2023.  Self-refine: It-\n   erative refinement with self-feedback.  ArXiv,   Mirac Suzgun, Nathan Scales, Nathanael Sch¨arli,\n  abs/2303.17651.                                     Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\n                                              Aakanksha Chowdhery, Quoc V Le, Ed H Chi,\nKrista Opsahl-Ong, Michael J. Ryan, Josh Purtell,     Denny Zhou,  , and Jason Wei. 2022.  Chal-\n  David Broman, Christopher Potts, Matei Za-      lenging big-bench tasks and whether chain-\n   haria, and Omar Khattab. 2024.  Optimizing      of-thought can solve them.   arXiv preprint\n   instructions and demonstrations for multi-stage      arXiv:2210.09261.\n  language model programs.  In Proceedings of\n                                                    Patrick Wagner. 2024. Using llms for text classifi-  the 2024 Conference on Empirical Methods in\n                                                           cation. Medium.  Natural Language Processing, pages 9340–9366,\n  Miami, Florida, USA. Association for Computa-                                                    Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\n   tional Linguistics.                                                  Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n                                                     2023.  Plan-and-solve prompting:  Improving\nKonstantina Palla, Jos´e Luis Redondo Garc´ıa, Clau-                                                       zero-shot chain-of-thought reasoning by large\n  dia Hauff, Francesco Fabbri, Henrik Lindstr¨om,                                                   language models.  In Proceedings of the 61st\n  Daniel R. Taber, Andreas Damianou, and Mou-                                              Annual Meeting of the Association for Compu-\n   nia Lalmas. 2025. Policy-as-prompt: Rethinking                                                          tational Linguistics (Volume 1: Long Papers),\n  content moderation in the age of large language                                                    pages 2609–2634, Toronto, Canada. Association\n  models. In Proceedings of the ACM Conference                                                              for Computational Linguistics.\n  on Fairness, Accountability, and Transparency\n  (FAccT 2025), pages 840–854.                   Jason Wei,  Xuezhi Wang,  Dale Schuurmans,\n                                              Maarten Bosma, Brian Ichter, Fei Xia, Ed H.\nReid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chen-      Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-\n  guang Zhu, and Michael Zeng. 2023. Automatic      of-thought prompting elicits reasoning in large\n  prompt optimization with “gradient descent” and      language models. In NeurIPS.\n\n\n                                         10\n\nHuasen Wu and Xin Liu. 2016. Double thompson\n  sampling for dueling bandits. Advances in neural\n  information processing systems, 29.\n\nJinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Xinbing\n  Liang, Fengwei Teng, Jinhao Tu, Fashen Ren, Xi-\n  angru Tang, Sirui Hong, Chenglin Wu, and Yuyu\n  Luo. 2025. Self-supervised prompt optimization.\n  arXiv preprint arXiv:2502.06855.  Version v3,\n   revised 21 August 2025.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao\n   Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.\n  2024. Large language models as optimizers. In\n  ICLR.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng,\n  Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\n  Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing,\n  Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\n  2023. Judging llm-as-a-judge with mt-bench and\n  chatbot arena. arXiv preprint arXiv:2306.05685.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\n  Keiran Paster, Silviu Pitis, Harris Chan, and\n  Jimmy Ba. 2022.  Large language models are\n  human-level prompt engineers. arXiv preprint\n  arXiv:2211.01910.\n\nMasrour Zoghi, Shimon Whiteson, and Maarten\n  de Rijke. 2015. Mergerucb: A method for large-\n   scale online ranker evaluation. In Proceedings\n  of the Eighth ACM International Conference on\n Web Search and Data Mining (WSDM), pages\n  17–26. ACM.\n\nMasrour Zoghi, Shimon Whiteson, R´emi Munos,\n  and Maarten de Rijke. 2013. Relative upper con-\n   fidence bound for the k-armed dueling bandit\n  problem. arXiv preprint arXiv:1312.3393. Ver-\n   sion 2, submitted December 2013.\n\n\n\n\n\n                                         11\n\nA  Small-scale Human Agreement        is between non-stochastic index policies, which\n   Check                                         select the next duel deterministically based on\n                                                 confidence or divergence indices, and stochastic\nIn addition to the cross-judge evaluation in\n                                                         policies, which randomize comparisons via pos-\nTable 3, we examine whether the original\n                                                      terior sampling. To justify the use of D-TS as\nLlama 3.3 MS-MARCO judge aligns with hu-\n                                              the exploration strategy in PDO, we conduct\nman preferences. We select 20 evaluation ex-\n                                                additional experiments comparing two UCB-\namples (5 from each of the four MS-MARCO\n                                                       style index methods (RUCB (Zoghi et al., 2013)\ntasks) for which the cached PDO and SPO out-\n                                       and MergeRUCB (Zoghi et al., 2015)), a KL-\nputs receive different Llama 3.3 scores under\n                                                divergence index method (RMED (Komiyama\nthe 1–5 rubric (e.g., 5 vs. 4). We then re-\n                                                   et  al., 2015)), and two posterior-sampling\ncruit 10 non-author volunteer raters (graduate-\n                                        methods (D-TS (Wu and Liu, 2016) and Self-\nstudent participants from a university popula-\n                                              Sparring (Sui et al., 2017)).\ntion) to perform a blinded pairwise comparison:\n                                                Table 5 shows monotonic improvement from\nfor each example, raters are shown the input\n                                      Round 5 to 15 for all methods. Under the same\nand two anonymized outputs (PDO vs. SPO),\n                                             judge budget, posterior-sampling approaches\nwith left/right order randomized and prompt\n                                                are consistently strongest: D-TS achieves the\nidentities hidden, and are asked to choose the\n                                               best performance in most (round, task) cells,\noutput they prefer under the same criteria as\n                                              while Self-Sparring remains highly competi-\nthe Llama judge (accuracy, completeness, rele-\n                                                      tive and is the top-performing method on En-\nvance, and clarity).\n                                        tity at every reported round. RMED con-\n  For each example, we compute an alignment\n                                                     sistently falls between the TS-based methods\nrate as the fraction of raters whose preferred\n                                       and the UCB-style baselines—outperforming\noutput matches the direction implied by the\n                               RUCB and MergeRUCB but remaining slightly\nLlama scores, and we summarize the distribu-\n                                            below posterior sampling—while random selec-\ntion of alignment rates across the 20 examples\n                                                  tion performs worst overall.\nin Figure 6.  Overall, this distribution sug-\n                                We  conjecture  that  this  performancegests that the Llama-based evaluation signal is\n                                         gap  reflects  characteristics  of the prompt-broadly consistent with human preferences on\n                                              optimization setting:  a short optimizationthe subset of cases most relevant to distinguish-\n                                               horizon with noisy preference feedback, whereing PDO from SPO, with a mean alignment\n                                     many prompt pairs are difficult to reliably sep-ratio of 0.87.\n                                                arate and the effective notion of “best” aligns\n                                       more closely with overall win frequency than\n                                             with a clearly dominant Condorcet winner. In\n                                            such regimes, posterior sampling can act as\n                                        an anytime, uncertainty-aware heuristic that\n                                                continues to explore multiple plausibly strong\n                                         prompts without over-investing in resolving\n                                                     near-ties. By contrast, index-based methods\n                                            (both UCB- and divergence-based) may allo-\n                                                cate a larger fraction of a limited budget to\n                                               shrinking confidence sets or satisfying infor-\nFigure 6: Histogram of per-example human–Llama   mation constraints, which can be less effective\nalignment rates on 20 MS MARCO examples, where   when the horizon is small and the judge signal\nPDO and SPO outputs receive different Llama 3.3     is noisy.\nevaluation scores.\n                        C  Experiment Details\n\nB  Analysis of Exploration            C.1  Hyperparameters\n    Strategies\n                                           For the main experiments in Section 4.2, we\nThe dueling-bandit literature spans several al-    initialize with 20 candidate prompts for BBH\ngorithmic families. A useful high-level division   and 50 candidate prompts for MS-MARCO.\n\n\n                                         12\n\nRound  Method      Description   Entity  Numeric   Location\n                            D-TS             4.52         4.38       4.00       4.32\n                                      Self-Sparring        4.50        4.40      4.00         4.30\n                      RMED              4.47         4.37        3.94         4.29\n                          5\n                       RUCB              4.46         4.32        3.92         4.26\n                           MergeRUCB        4.44         4.34        3.88         4.28\n                           Random            4.40         4.36        3.89         4.20\n                            D-TS             4.57         4.44       4.07       4.41\n                                      Self-Sparring        4.56        4.46       4.06         4.40\n                      RMED              4.53         4.42        4.02         4.39\n                          10\n                       RUCB              4.52         4.38        4.00         4.36\n                           MergeRUCB        4.50         4.40        3.95         4.38\n                           Random            4.45         4.41        3.97         4.29\n                            D-TS             4.61         4.48       4.11       4.45\n                                      Self-Sparring        4.59        4.50      4.11         4.43\n                      RMED              4.57         4.45        4.05         4.42\n                          15\n                       RUCB              4.55         4.40        4.03         4.39\n                           MergeRUCB        4.53         4.42        3.98         4.41\n                           Random            4.48         4.44        4.00         4.33\n\nTable 5: Comparison of exploration strategies on MS-MARCO across optimization rounds in PDO. We\nreport the mean test score of the current Copeland leader over rounds, selected by different exploration\nstrategies. For each {round, dataset}, the best score is shown in bold and the second best is underlined.\n\n\nEach experiment runs for 30 rounds, with 25   evaluates the current prompt by running one\nduels per round. At rounds 10 and 20, we   answer-generation LLM call per example. A\napply prompt mutation by selecting the top-3    revised prompt is then proposed using a single\nprompts ranked by Copeland scores and gen-  LLM call conditioned on the observed cases.\nerating 10 new prompts. At the same time,   Both the current and revised prompts are\nwe prune the 10 lowest-ranked prompts by   evaluated on the same set of examples via an\nCopeland scores. We always select the prompt   additional nexamples answer-generation  calls.\nwith the highest Copeland score as the winner,  The two sets of outputs are subsequently com-\nusing the average win rate as a tiebreaker when   pared using njudge trials pairwise LLM judge\nmultiple prompts share the same score. The    calls, where each judge observes both outputs\nD-TS parameter is fixed at α = 1.2 throughout   with randomized order to mitigate positional\nall experiments.                                 bias and votes for the preferred prompt. The\n                                                 revised prompt  is accepted  if  it receives a\nC.2  Prompt generation strategies                                             majority of the judge votes; otherwise, the\nTo generate the initial candidate prompts, we   current prompt  is retained.   Overall, each\nfollow the approach in MiPROV2 (Opsahl-Ong   round incurs 2nexamples + njudge trials + 1 LLM\net al., 2024), which first uses an LLM to con-    calls.\nstruct a dataset summary from demonstra-    With the default setting nexamples = 3 and\ntion examples. The prompt generation process    njudge trials = 3, this corresponds to 10 calls\nthen incorporates this dataset summary, addi-   per round, and we run 20 rounds to obtain the\ntional unlabelled demonstration examples, and   main results reported in Section 4.\nrandomly selected prompt tips from a prede-\nfined set to encourage diversity. In the muta-   Supervised methods  For supervised APO\ntion stage, we create new variants of the top-   methods, the typical pipeline involves gener-\nperforming prompt by applying prompt tips    ating, mutating, and scoring prompts. APE\nto guide local edits, such as changing the tone,   frames prompt design as black-box optimiza-\nadjusting the wording, or appending synthetic    tion, generating candidate instructions from in-\nfew-shot examples.                           put–output demonstrations and refining them\n                                           through iterative search. OPRO treats the\nC.3  Baselines                             LLM itself as the optimizer: a meta-prompt\nUnsupervised   (label-free)  methods   with task descriptions and prior results guides\nSPO follows an iterative optimize–execute–   the model to propose and evaluate new candi-\nevaluate loop.  In each round, the method   dates in an iterative loop. Breeder applies an\nfirst samples  nexamples  fresh examples and   evolutionary approach, jointly evolving task-\n\n\n                                         13\n\nprompts and mutation-prompts through LLM-\ndriven mutation and selection.\n\nC.4  Datasets\n\nBIG-bench Hard (BBH). BBH (Suzgun\net al., 2022) is a curated subset of BIG-bench\nconsisting of 23 reasoning-intensive tasks.  It\nis commonly used to stress multi-step reason-\ning and symbolic manipulation. In our experi-\nments, we evaluate on 16 BBH multiple-choice\ntasks, where LLaMA-3.3-70B shows non-trivial\nsensitivity to instruction prompts.\n\nMS MARCO. MS MARCO (Bajaj et al.,\n2016)  is a  large-scale  question  answering\ndataset built from real Bing search queries,\npaired with human-written answers and linked\npassages.  It supports QA, passage ranking,\nand related IR/NLP tasks. In our setting, we\nfocus on four task categories—Description, En-\ntity, Numeric, and Location—and adopt a 1–5\ninteger scoring scheme from an LLM judge that\ncompares model outputs against the dataset’s\nground-truth answers.\n\nD  Discussion on Evaluation Scope\n\nWe evaluate PDO on BBH and MS MARCO,\nwhich  capture  two  practically  important\nprompt-optimization  regimes.  BBH  cov-\ners a broad set of real-world, prompt-based\ninstruction-following classification tasks in a\ncold-start setting, where labeled data are scarce\nand costly to obtain at the outset. This setting\nis increasingly relevant in industry workflows\nsuch as content moderation (Palla et al., 2025),\nwhere many systems are transitioning from\nfully supervised ML/DL pipelines to prompt-\nbased solutions whose performance is largely\ndetermined by the underlying policy prompt.\n  In contrast, MS MARCO represents a single-\nturn instruction/QA setting, mirroring sce-\nnarios where practitioners optimize a system\nprompt for a locally deployed LLM application.\nIn such cases, tasks are often open-ended and\nevaluation criteria can be subjective, making\nan LLM judge a natural and scalable choice\nfor preference-based comparisons.\n  We admit that extending PDO to multi-turn\ninteractions, as well as jointly optimizing sets\nof prompts (e.g., for multi-agent systems), is\na valuable and important direction for future\nwork.\n\n\n                                         14\n\nMethod      Causal     Date    DisambigQA   Formal   Geometric Hyperbaton  Logical-5    Logical-7\n APE         0.680±0.044  0.892±0.019    0.730±0.043    0.747±0.022  0.670±0.072   0.940±0.013   0.822±0.041   0.721±0.035\n OPRO       0.682±0.044  0.910±0.022    0.734±0.038    0.728±0.026  0.569±0.048   0.932±0.021   0.774±0.030   0.718±0.031\n Breeder     0.683±0.027  0.898±0.026   0.745±0.038   0.746±0.027  0.684±0.048   0.932±0.021   0.778±0.022   0.724±0.018\n PDO (ours)  0.680±0.033  0.915±0.024   0.740±0.040   0.754±0.022  0.712±0.058  0.948±0.017   0.809±0.028   0.733±0.024\n Method     Navigate   Penguins     Salient      Snarks   Tracking-5  Tracking-7  Tracking-3 Web of Lies\n APE         0.899±0.024  0.925±0.025    0.686±0.033    0.875±0.033  0.803±0.041   0.600±0.050   0.911±0.041   0.948±0.020\n OPRO       0.882±0.032  0.919±0.025    0.687±0.023    0.888±0.021  0.833±0.083   0.662±0.073   0.924±0.035   0.938±0.045\n Breeder     0.908±0.016  0.927±0.018   0.698±0.024   0.903±0.019  0.859±0.049   0.600±0.042   0.927±0.028   0.963±0.019\n PDO (ours)  0.890±0.010  0.933±0.027   0.695±0.022    0.874±0.036  0.847±0.062   0.662±0.084  0.946±0.015  0.979±0.024\n\nTable 6: We report test results for PDO when selecting the prompt with the highest development-set\naccuracy from the same candidate pools as in Table 1. This setting serves as a proxy for replacing the\nLLM preference judge with an oracle. We compare these results against supervised APO methods and\nfind that PDO remains competitive with state-of-the-art prompt optimization baselines, achieving the\nbest performance on 9 out of 16 tasks.\n\n\n\n\n\n             (a) Causal                         (b) Tracking-7                         (c) Web of Lies\n\nFigure 7: Test performance when using Claude 4.5 vs. Llama 3.3 as the LLM preference judge on BBH\ntasks under the same judge-call budget. Using Claude 4.5 more reliably identifies higher-performing\nprompts; the improvement is largest on tasks where the Llama 3.3 judge is noisier (e.g., Causal in Figure\n7a and Geometric in Figure 3b).\n\n\n\n\n\n                                         15\n\nDataset      Non-mutate   Mutate    Diff.     p-value\n                    Causal              0.709          0.703      -0.006   > 0.05\n                   Date                0.927          0.938      0.011      0.0291\n                  DisambigQA        0.766          0.760      -0.006   > 0.05\n                   Formal             0.759          0.767      0.008   > 0.05\n                    Geometric          0.611          0.672      0.061    9.7 × 10−5\n                   Hyperbaton         0.935          0.943      0.008   > 0.05\n                       Logical-5            0.821          0.831      0.010   > 0.05\n                       Logical-7            0.755          0.750      -0.005   > 0.05\n                     Navigate            0.903          0.917      0.014      0.0146\n                    Penguins            0.943          0.955      0.012      0.0236\n                       Salient              0.719          0.711      -0.008   > 0.05\n                    Snarks              0.863          0.876      0.013   > 0.05\n                     Tracking-5          0.793          0.833      0.040      0.0252\n                     Tracking-7          0.585          0.647      0.062      0.0040\n                     Tracking-3          0.949          0.957      0.008      0.0491\n               Web of Lies         0.919          0.979      0.060    1.5 × 10−6\n\nTable 7: Effect of prompt mutation on BBH. We compare the final Copeland-winner prompt selected\nby PDO when mutation is disabled (Non-mutate) versus when mutation is enabled (Mutate). For each\nBBH task, we report the mean test accuracy over 10 independent runs. We perform a two-sample t-test\nper task to assess whether mutation yields a statistically significant accuracy change. Overall, 8 out\nof 16 tasks show significant improvement at p < 0.05. We visualize representative tasks (Web of Lies\nand Tracking-7) in Figure 4. Tasks with small negative differences show only minor changes and are\nlikely attributable to run-to-run noise; across all tasks considered, mutation does not materially degrade\nperformance.\n\n\n\n\n\n                                         16\n\nE  Further Discussion on LLM Judge   Result Analysis.  For most BBH tasks, we\n    Design for Closed-Ended Tasks       find strong evidence that the LLM judge reli-\n                                               ably selects the higher-accuracy prompt when\nThe design of an LLM preference judge can  A is correct and B is wrong. We also observe\nvary depending on the type of task PDO tackles.   moderate evidence that when both prompts\nIn this section, we present the rationale and   produce the same answer, the judge prefers A\nprovide a detailed analysis of our judge design   based on the quality of its reasoning. Datasets\nfor multiple-choice BBH tasks.                where this preference is consistent under both\n  Since outputs in BBH are tied to fixed  Answer and Reasoning conditions (Hyperba-\nmultiple-choice answers, the judging process    ton, Tracking-5, Tracking-7, and Web of Lies)\nnaturally splits into two cases. (i) When the   show superior performance in Table 1. On the\ntwo prompts produce different answers, the   other hand, when the judge is inconsistent in\njudge’s role is straightforward: a capable judge    identifying the higher-accuracy prompt A in\nshould be able to identify the correct answer    either condition, performance degrades (Geo-\nand select the corresponding prompt.   (ii)   metric, Salient).\nWhen the two prompts produce the same an-\n                                       E.2  Weighted Preference Matrix\nswer, the situation is less clear. In this case,\n                                       Update\nthe answer alone cannot distinguish between\nthe prompts, so we instead ask the judge to   Motivated by the above analysis in Figure 8, we\ncompare their reasoning chains. This design is    consider a modification of the PDO algorithm\nwell suited to BBH tasks, which are explicitly    for BBH tasks in which reasoning-based deci-\nconstructed to stress reasoning (Suzgun et al.,    sions, which are noisier than answer-based ones,\n2022): the datasets involve multi-step deduc-   are down-weighted so that they contribute less\ntion, object tracking, logical rules, or numerical   to the preference counts. Let Wij and Wji be\nreasoning, where strong performance correlates   the win counts used to form the Beta posteriors\nwith the quality of intermediate steps. Our hy-   Beta(Wij+1, Wji+1) in D-TS. For a duel de-\npothesis is that examining the reasoning trace   cided by source y ∈{Answer, Reasoning}, we\ncan reveal whether the model reached the cor-   introduce a margin parameter γy ∈[0, 0.5] and\nrect answer through a coherent process or by   update\nshortcuts or luck.\n                                                (Wij, Wji) ←(Wij + 0.5+γy, Wji + 0.5 −γy)\n  This formalizes our dual-judge approach, as\ndiscussed in Section 5.1, where the judge evalu-                                            whenever i is preferred, with the symmetric rule\nates either the answer or the reasoning depend-  when j is preferred. This preserves one effec-\ning on the case. To enable this, we prompt                                                      tive comparison per duel (sum increment = 1)\nthe model to always produce both a reasoning   while shrinking the winner–loser gap to 2γy\nchain and a final answer using JSON-guided                                           under noisier judgments. We fix γAnswer = 0.5\ndecoding.                                         and ablate γReasoning ∈{0.0, 0.2, 0.5}. Figure 9\n                                                  reports, for each dataset, the ground-truth\nE.1  Validating the Dual-Judge           accuracy rank  of the Copeland leader over\n     Approach                               rounds. The results suggest that smaller values\n                                                      of γReasoning (e.g., 0.2) often yield more stable\nTo further probe the dual-judge approach, we\n                                                progress than the full update γReasoning = 0.5\nconduct the following experiments. For each\n                                        on tasks with noisier reasoning judgments,\ndataset, we select two prompts with differ-\n                                               while tasks with consistent reasoning judg-\nent accuracy levels: prompt A as the higher-\n                                          ments remain robust across settings.\naccuracy prompt and prompt B as the lower-\naccuracy prompt. We then test the judge in   Takeaway.  For BBH  tasks, we use  dis-\ntwo scenarios:  (i) whether prompt A is pre-   counted updates as a simple mechanism to\nferred when A is correct and B is wrong, and   handle differences in judge reliability between\n(ii) whether prompt A is preferred when both   answer-based and reasoning-based decisions in\nA and B produce correct answers. The results   preference-based prompt optimization. More\nare reported in Figure 8.                           generally, for other tasks, LLM judges could be\n\n\n                                         17\n\nextended with adaptive adjustments that re-\nflect reliability or with additional mechanisms\nto better capture uncertainty in judgments.\nExploring such extensions is an interesting di-\nrection for future work.\n\nE.3  Case Study\n\nWe conduct a case study of the judge’s decisions\nin Table 8 on the BBH Web of Lies task by\ncomparing a lower-accuracy Prompt A (0.852)\nwith a higher-accuracy Prompt B (1.000). Two\nrepresentative scenarios explain why the judge\ntends to prefer B: (i) when both prompts pro-\nduce the correct answer, the reasoning-based\njudge compares the reasoning chains and fa-\nvors B’s more concise and internally consis-\ntent rationale; (ii) when the prompts disagree\nand B’s answer is correct, the answer-based\njudge correctly selects B.\n\n\n\n\n\n                                         18\n\nFigure 8: For each BBH dataset, we construct a prompt pair (A, B), where A is the higher-accuracy\nprompt and B is the lower-accuracy prompt. We report the proportion of comparisons in which the\njudge prefers A. Answer (green) covers cases where A is correct and B is wrong; the judge evaluates by\nchecking answers. Reasoning (blue) covers cases where both prompts produce the correct answer; the\njudge compares their reasoning chains. Aggregate pools all examples across datasets.\n\n\n\n\n\n            (a) Logical-5                       (b) Hyperbation                         (c) Tracking-7\n\n\n\n\n\n          (d) Web of lies                           (e) Formal                                 (f) Date\n\nFigure 9: Effect of the reasoning-discount γ in the D-TS update across six BBH datasets. Each plot\nshows the ground-truth accuracy rank (lower is better) of the current Copeland leader over rounds. We\nfix γAnswer = 0.5 and ablate γReasoning ∈{0.0, 0.2, 0.5}. Results indicate that introducing a mild discount\nat γReasoning = 0.2 generally accelerates convergence and produces better final ranks overall compared to\nthe undiscounted case γReasoning = 0.5.\n\n\n                                         19\n\nTable 8: Case study of Reasoning-based and Answer-based judging on Prompt A (lower accuracy) and\nPrompt B (higher accuracy) for selected questions from the Web of Lies dataset in BBH.\n\n Item                    Content\n\n Prompt definitions\n Prompt A (accuracy = 0.852)  Analyze the given statements about individuals telling the\n                               truth or lying, and determine the truthfulness of a specific\n                              person by iteratively applying logical deductions, stating as-\n                             sumptions before solving, and considering the chain of state-\n                            ments to arrive at a conclusion.\n Prompt B (accuracy = 1.000)  Analyze the given statements about individuals telling the\n                                truth or lying, and determine the truthfulness of the person\n                                    in question by following the chain of statements and applying\n                                    logical reasoning. For example, if we have the statements:\n                                  ‘Fidel lies. Fletcher says Fidel tells the truth. Yoland says\n                                 Fletcher lies. Raymond says Yoland lies. Leda says Raymond\n                                            lies,’ we can conclude that Leda tells the truth by tracing\n                               the statements:  since Fidel lies, Fletcher’s statement that\n                                Fidel tells the truth is false, meaning Yoland’s statement\n                               that Fletcher lies is true, which in turn means Raymond’s\n                               statement that Yoland lies is false, and thus Leda’s statement\n                                that Raymond lies is true. Another example is: ‘Willian tells\n                               the truth. Phoebe says Willian lies. Alejandro says Phoebe\n                                             lies. Lorine says Alejandro tells the truth. Christie says Lorine\n                                          tells the truth,’ where we can deduce Christie tells the truth by\n                                  following the chain of statements in a similar manner. Apply\n                                    this reasoning process to the given question to determine the\n                                  truthfulness of the person in question.\n\n Case I: Both prompts output the same correct answer →Reasoning-based judge\n Question                       Kristian lies. Sherrie says Kristian lies. Delbert says Sherrie\n                                           lies. Jerry says Delbert tells the truth. Shalonda says Jerry\n                                         tells the truth. Does Shalonda tell the truth?\n Correct answer            No\n Prompt A reasoning            Since Kristian lies and Sherrie says Kristian lies, Sherrie tells\n                               the truth. Delbert says Sherrie lies, so Delbert lies, which\n                          means Jerry lies when saying Delbert tells the truth, and\n                                 consequently, Shalonda lies when saying Jerry tells the truth.\n Prompt A answer          No\n Prompt B reasoning           Since Kristian lies, Sherrie’s statement that Kristian lies is\n                                    true. This means Delbert’s statement that Sherrie lies is false,\n                                 so Jerry’s statement that Delbert tells the truth is false, and\n                               thus Shalonda’s statement that Jerry tells the truth is false.\n Prompt B answer          No\n Judge reasoning             Prompt B’s reasoning is more concise and directly addresses\n                                the truthfulness of each statement in a straightforward manner.\n                                      It clearly states the implications of each person’s statement\n                              without introducing unnecessary complexity, making it easier\n                                 to follow and understand. The logical flow from Kristian’s lie\n                                 to Shalonda’s falsehood is more streamlined in B, enhancing\n                                     clarity and coherence.\n Judge decision        B\n\n\n                                         20\n\nCase II: Prompts output different answers (B correct, A incorrect)\n       →Answer-based judge\n\nQuestion            Vina tells the truth. Helene says Vina lies. Kandi says Helene tells the\n                       truth. Jamey says Kandi lies. Ka says Jamey lies. Does Ka tell the\n                      truth?\nCorrect answer     No\nPrompt A reasoning   If Ka lies, then Jamey tells the truth, which means Kandi lies, and thus\n                    Helene lies, contradicting Vina’s truthfulness. Therefore, Ka must tell\n                      the truth to maintain the consistency of the statements.\nPrompt A answer    Yes\nPrompt B reasoning  Since Vina tells the truth, Helene’s statement that Vina lies is false.\n                    This makes Kandi’s statement that Helene tells the truth also false,\n                    which in turn makes Jamey’s statement that Kandi lies true, and thus\n                    Ka’s statement that Jamey lies is false, meaning Ka lies.\nPrompt B answer    No\nJudge reasoning     The correct answer is determined by tracing the statements back to\n                     Vina’s truthfulness. Response B correctly identifies the chain: Vina\n                      true →Helene false →Kandi false →Jamey true →Ka’s statement\n                    about Jamey false (Ka lies). This matches the task requirement.\nJudge decision   B\n\n\n\n\n\n                                        21\n\nBBH: Answer-based Preference Judge Template\n\n\n## Role ##\nYou are a specialized judge focused on evaluating answer correctness when two responses give\n     different answers.\n## Task ##\n{question}\n## Response from Prompt X ##\n**Reasoning:** {reasoning_X}\n**Answer:** {answer_X}\n## Response from Prompt Y ##\n**Reasoning:** {reasoning_Y}\n**Answer:** {answer_Y}\n## Your Task ##\nThe responses above give different answers: \"{answer_X}\" vs \"{answer_Y}\".\nYour job is to determine which answer is more correct for the given task.\n## Evaluation Criteria ##\nFocus primarily on:\n1. **Factual Accuracy** - Which answer better matches reality and task requirements?\n2. **Task Alignment** - Which answer better fulfills the specific question asked?\n## Output Format ##\n{{\n  \"reasoning\": \"Your detailed justification explaining why prompt X or Y provided the more\n    correct answer (~100 words).\",\n  \"winner\": \"X or Y\"\n}}\n\n\n\n\n\nBBH: Reasoning-based Preference Judge Template\n\n\n## Role ##\nYou are a specialized judge focused on evaluating reasoning quality when two responses give\n    the same answer.\n## Task ##\n{question}\n## Response from Prompt X ##\n**Reasoning:** {reasoning_X}\n**Answer:** {answer_X}\n## Response from Prompt Y ##\n**Reasoning:** {reasoning_Y}\n**Answer:** {answer_Y}\n## Your Task ##\nThe responses above give the same answer: \"{answer_X}\".\nSince both arrive at the same conclusion, your job is to determine which reasoning process\n    is better.\n## Evaluation Criteria ##\nFocus primarily on:\n1. **Logical Coherence** - Is the reasoning chain clear and well-structured?\n2. **Completeness** - Does the reasoning address all key aspects of the problem?\n3. **Clarity** - Is the reasoning easy to follow and understand?\n4. **Accuracy** - Are the intermediate steps and assumptions correct?\n## Output Format ##\n{{\n  \"reasoning\": \"Your detailed justification explaining why prompt X or Y provided better\n    reasoning (~100 words).\",\n  \"winner\": \"X or Y\"\n}}\n\n\n\n\n\n                                       22\n\nMS-MARCO: Preference Judge Template\n\n\n## Role ##\nYou are a meticulous, impartial referee evaluating two competing answers to determine which\n    better answers the given question based on the provided context.\n## Query ##\n{query}\n## Context ##\n{context}\n## Answer from Prompt X ##\n{answer_X}\n## Answer from Prompt Y ##\n{answer_Y}\n## Evaluation Criteria ##\nCompare both answers based on:\n1. **Accuracy** - How factually correct is each answer based on the context?\n2. **Completeness** - Does the answer address all aspects of the question?\n3. **Relevance** - How well does the answer stay focused on answering the question?\n4. **Clarity** - How clear and well-articulated is the answer?\n## Output Format ##\n{{\n    \"reasoning\": \"Your detailed justification explaining why answer X or Y is better (~100\n    words).\",\n    \"winner\": \"X or Y\"\n}}\n\n\n\n\n\nMS-MARCO: final evaluations with ground-truth references\n\n\n\"\"\"\nBegin your evaluation by carefully comparing the AI-generated answer with the reference\n    solution. Identify any outputs that are not true, as well as omissions or deviations,\n    and simulate human judgments in explaining how these impact the overall quality of the\n    response. Ensure that your assessment is objective and consistent.\nAt the end of your evaluation, assign a score from 1 to 5 based on the following scale:\n\n- 1: Very poor - does not meet the requirements or is significantly incorrect.\n- 2: Poor - contains major errors or omissions.\n- 3: Fair - adequate but with notable flaws.\n- 4: Good - meets the requirements with minor errors.\n- 5: Excellent - fully accurate and well-articulated.\n\n[User Question]\n{question}\n\n[Reference Solution]\n{ground_truth}\n\n[AI-Generated Answer]\n{prediction}\n\nYour response should be a valid JSON string (no backticks) following this schema:\n{{ \"explanation\": \"{{Detailed reasoning based on the comparison}}\"\n    \"score\": {{1-5}}\n}}\n\"\"\"\n\n\n\n\n\n                                       23\n\nTemplate for generating initial prompts\n\n\n\"\"\"\nYou are an expert prompt-engineer. Your task is to generate **exactly 1** *high-quality* **\n    system-level instruction** for the target reasoning task.\n\n# Dataset Snapshot\nBelow is an *LM-written summary* of the unlabeled question pool:\n{dataset_summary}\n\n# Sample Inputs (do NOT answer them)\n{questions}\n\n# Prompt-Engineering Tip\n{tip}\n\n# Output Format (STRICT)\nReturn **exactly** 1 item in a JSON array, *and nothing else*:\n[ \"Your single high-quality instruction here\" ]\n\"\"\"\n\n\n\n\n\nTemplate for mutating on top-performing prompts\n\n\n\"\"\"\nYou are an expert prompt-engineer specializing in prompt optimization.\nYour task is to generate 1 *diverse, high-quality* **mutation** of the currently **BEST\n    PERFORMING** instruction for the target reasoning task.\n\n# BEST PERFORMING Instruction (Current Champion)\n{instructions}\n\n# CRITICAL: Follow This Prompt-Engineering Tip\n{tip}\n\n# Output Format\nReturn **exactly** 1 mutated instruction in a JSON object, *and nothing else*:\n{ \"mutated_prompt\": \"Your mutated instruction here, following the tip.\" }\n\"\"\"\n\n\n\n\n\n                                       24\n\nPrompt engineering tips for prompt generations\n\n\n# Mutation tips for top-performer-guided promp mutations\nMUTATION_TIPS = {\n    \"expansion\": \"Keep the current champion instruction exactly as is, but expand on it by\n    adding additional helpful guidance or clarifications. The result should be the original\n    instruction plus new supplementary content.\",\n    \"minimal\": \"Make very minimal changes to the current champion instruction. Keep it\n    around the same length and modify only a few words through paraphrasing while preserving\n     the core meaning.\",\n    \"few_shot\": \"Add a few concrete examples to the current champion instruction to\n    demonstrate the expected reasoning process or output format. Include 1-3 brief example\n    cases that show how to apply the instruction.\",\n    \"emphasis\": \"Adjust the tone, emphasis, or directional focus of the current champion\n    instruction to create different reasoning patterns.\",\n}\n\n# Initial instruction generation tips\nINITIAL_INSTRUCTION_TIPS = {\n    \"framing\": \"Set the context for the task by framing it as a concrete creative scenario\n    .\",\n    \"simple\": \"Keep the instruction clear and concise.\",\n    \"description\": \"Make sure your instruction is very informative and descriptive.\",\n    \"persona\": \"Provide the LM with a creative persona that is relevant to the task.\",\n    \"edge_cases\": \"List tricky cases the instruction must handle.\",\n    \"assumptions\": \"Have the model state assumptions before solving.\",\n}\n\n\n\n\n\n                                       25",
"headers": [
"arXiv:2510.13907v2  [cs.CL]  28 Jan 2026",
"LLM Prompt Duel Optimizer: Efficient Label-Free Prompt",
"Optimization",
"Yuanchen Wu",
"Saurabh Verma",
"Justin Lee",
"Fangzhou Xiong",
"Poppy Zhang",
"Amel Awadelkarim",
"Xu Chen",
"Yubai Yuan",
"Shawndra Hill",
"Department of Statistics, The Pennsylvania State University",
"Meta",
"Abstract",
"1",
"Introduction",
"2",
"Preliminaries and Problem Setup",
"3",
"Prompt Duel Optimizer (PDO)",
"4",
"Experiment and Results",
"5",
"LLM Judge Analysis",
"6",
"Ablation Study",
"7",
"Related Work",
"8",
"Conclusion",
"Limitation",
"References",
"A",
"Small-scale Human Agreement",
"Check",
"C",
"Experiment Details",
"B",
"Analysis of Exploration",
"Strategies",
"D",
"Discussion on Evaluation Scope",
"E",
"Design for Closed-Ended Tasks",
"Further Discussion on LLM Judge"
],
"tables": [
"|Method|Causal Date DisambigQA Formal Geometric Hyperbaton Logical-5 Logical-7|\n|---|---|\n|No prompt<br>CoT<br>PoS<br>SPO<br>PDO(ours)|0_._661~~_±_~~0_._044<br>0_._854_±_0_._024<br>0_._698_±_0_._047<br>0_._739~~_±_~~0_._031<br>0_._434_±_0_._036<br>0_._900~~_±_~~0_._020<br>0_._785_±_0_._018<br>0_._739~~_±_~~0_._033<br>0_._653_±_0_._042<br>0_._877_±_0_._019<br>0_._720_±_0_._039<br>0_._725_±_0_._027<br>0_._422_±_0_._028<br>0_._891_±_0_._023<br>0_._761_±_0_._027<br>0_._726_±_0_._025<br>0_._652_±_0_._037<br>0_._878_±_0_._019<br>0_._698_±_0_._043<br>0_._739~~_±_~~0_._027<br>0_._403_±_0_._030<br>0_._896_±_0_._024<br>0_._798~~_±_~~0_._032<br>**0**_._**750**_±_0_._026<br>0_._655_±_0_._033<br>0_._884_±_0_._017<br>0_._725~~_±_~~0_._057<br>0_._738_±_0_._018<br>**0**_._**650**_±_0_._069<br>0_._886_±_0_._031<br>0_._787_±_0_._031<br>0_._721_±_0_._026<br>**0**_._**681**_±_0_._040<br>**0**_._**918**_±_0_._014<br>**0**_._**738**_±_0_._050<br>**0**_._**744**_±_0_._030<br>0_._598~~_±_~~0_._073<br>**0**_._**910**_±_0_._029<br>**0**_._**804**_±_0_._034<br>0_._711_±_0_._019|\n|**Method**|**Navigate**<br>**Penguins**<br>**Salient**<br>**Snarks**<br>**Tracking-5**<br>**Tracking-7**<br>**Tracking-3**<br>**Web of Lies**|\n|No prompt<br>CoT<br>PoS<br>SPO<br>PDO(ours)|0_._869_±_0_._013<br>0_._915_±_0_._018<br>0_._698~~_±_~~0_._020<br>0_._823_±_0_._027<br>0_._695_±_0_._033<br>0_._499_±_0_._020<br>0_._890_±_0_._019<br>0_._766_±_0_._020<br>0_._878~~_±_~~0_._016<br>0_._915_±_0_._023<br>**0**_._**709**_±_0_._021<br>0_._833~~_±_~~0_._023<br>0_._724_±_0_._046<br>0_._532_±_0_._025<br>0_._904~~_±_~~0_._019<br>0_._796_±_0_._022<br>0_._866_±_0_._019<br>0_._910_±_0_._027<br>0_._693_±_0_._025<br>0_._816_±_0_._026<br>0_._725~~_±_~~0_._030<br>0_._538_±_0_._034<br>0_._888_±_0_._019<br>0_._861~~_±_~~0_._019<br>0_._874_±_0_._035<br>0_._934_±_0_._025<br>0_._662_±_0_._038<br>0_._820_±_0_._046<br>0_._692_±_0_._046<br>0_._543~~_±_~~0_._026<br>0_._826_±_0_._087<br>0_._818_±_0_._043<br>**0**_._**900**_±_0_._023<br>**0**_._**937**_±_0_._034<br>0_._681_±_0_._032<br>**0**_._**840**_±_0_._039<br>**0**_._**796**_±_0_._084<br>**0**_._**641**_±_0_._089<br>**0**_._**930**_±_0_._046<br>**0**_._**942**_±_0_._040|",
"|Judge Model|PDO SPO|∆|\n|---|---|---|\n|Llama 3.3 (original)<br>Mistral-Large<br>GPT-4o<br>Claude 3.5 Sonnet<br>Claude 4.5 Sonnet|4_._68_ ±_ 0_._06<br>4_._50_ ±_ 0_._07<br>4_._61_ ±_ 0_._07<br>4_._49_ ±_ 0_._07<br>4_._60_ ±_ 0_._06<br>4_._41_ ±_ 0_._06<br>4_._45_ ±_ 0_._08<br>4_._25_ ±_ 0_._08<br>4_._58_ ±_ 0_._06<br>4_._49_ ±_ 0_._06|0_._18<br>0_._12<br>0_._19<br>0_._20<br>0_._09|",
"|Method|Causal Date DisambigQA Formal Geometric Hyperbaton Logical-5 Logical-7|\n|---|---|\n|APE<br>OPRO<br>Breeder<br>PDO(ours)|0_._680_±_0_._044<br>0_._892_±_0_._019<br>0_._730_±_0_._043<br>0_._747~~_±_~~0_._022<br>0_._670_±_0_._072<br>0_._940~~_±_~~0_._013<br>**0**_._**822**_±_0_._041<br>0_._721_±_0_._035<br>0_._682~~_±_~~0_._044<br>0_._910_±_0_._022<br>0_._734_±_0_._038<br>0_._728_±_0_._026<br>0_._569_±_0_._048<br>0_._932_±_0_._021<br>0_._774_±_0_._030<br>0_._718_±_0_._031<br>**0**_._**683**_±_0_._027<br>0_._898_±_0_._026<br>**0**_._**745**_±_0_._038<br>0_._746_±_0_._027<br>0_._684~~_±_~~0_._048<br>0_._932_±_0_._021<br>0_._778_±_0_._022<br>0_._724~~_±_~~0_._018<br>0_._680_±_0_._033<br>**0**_._**915**_±_0_._024<br>0_._740~~_±_~~0_._040<br>**0**_._**754**_±_0_._022<br>**0**_._**712**_±_0_._058<br>**0**_._**948**_±_0_._017<br>0_._809~~_±_~~0_._028<br>**0**_._**733**_±_0_._024|\n|**Method**|**Navigate**<br>**Penguins**<br>**Salient**<br>**Snarks**<br>**Tracking-5**<br>**Tracking-7**<br>**Tracking-3**<br>**Web of Lies**|\n|APE<br>OPRO<br>Breeder<br>PDO(ours)|0_._899~~_±_~~0_._024<br>0_._925_±_0_._025<br>0_._686_±_0_._033<br>0_._875_±_0_._033<br>0_._803_±_0_._041<br>0_._600~~_±_~~0_._050<br>0_._911_±_0_._041<br>0_._948_±_0_._020<br>0_._882_±_0_._032<br>0_._919_±_0_._025<br>0_._687_±_0_._023<br>0_._888~~_±_~~0_._021<br>0_._833_±_0_._083<br>**0**_._**662**_±_0_._073<br>0_._924_±_0_._035<br>0_._938_±_0_._045<br>**0**_._**908**_±_0_._016<br>0_._927_±_0_._018<br>**0**_._**698**_±_0_._024<br>**0**_._**903**_±_0_._019<br>**0**_._**859**_±_0_._049<br>0_._600~~_±_~~0_._042<br>0_._927~~_±_~~0_._028<br>0_._963~~_±_~~0_._019<br>0_._890_±_0_._010<br>**0**_._**933**_±_0_._027<br>0_._695~~_±_~~0_._022<br>0_._874_±_0_._036<br>0_._847~~_±_~~0_._062<br>**0**_._**662**_±_0_._084<br>**0**_._**946**_±_0_._015<br>**0**_._**979**_±_0_._024|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2510.13907v2.pdf"
}