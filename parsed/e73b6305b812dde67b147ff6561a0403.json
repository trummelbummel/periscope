{
"text": "POEM: Interactive Prompt Optimization for Enhancing Multimodal\n                 Reasoning of Large Language Models\n\n\n                        Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, and Huamin Qu\n\n\n2024\nSep\n30\n\n\n\n\n\n                     Fig. 1: The POEM interface consists of three major panels. The Prompt Panel (A) offers versatile operations for users to efficiently craft[cs.HC]               and edit prompt content, such as importing various principles and demonstration examples, to support an effortless prompt engineering\n                  experience. The Reasoning Panel (B) facilitates a comprehensive multi-level investigation of the model’s multimodal reasoning\n                 performance, ranging from the global modality interaction level to the local instance level. The Evaluation Panel (C) supports both\n                   global and local evaluation of prompts, coupled with detailed documentation of modifications during prompt iterations for continuous\n                  monitoring and comparison.\n\n               Abstract— Large language models (LLMs) have exhibited impressive abilities for multimodal content comprehension and reasoning\n                  with proper prompting in zero- or few-shot settings. Despite the proliferation of interactive systems developed to support prompt\n                  engineering for LLMs across various tasks, most have primarily focused on textual or visual inputs, thus neglecting the complex interplay\n                between modalities within multimodal inputs. This oversight hinders the development of effective prompts that guide models’ multimodal\n                 reasoning processes by fully exploiting the rich context provided by multiple modalities. In this paper, we present POEM, a visual\n                   analytics system to facilitate efficient prompt engineering for steering the multimodal reasoning performance of LLMs. The system\n                 enables users to explore the interaction patterns across modalities at varying levels of detail for a comprehensive understanding of the\n                 multimodal knowledge elicited by various prompts. Through diverse recommendations of demonstration examples and instructional\n                     principles, POEM supports users in iteratively crafting and refining prompts to better align and enhance model knowledge with human\n                     insights. The effectiveness and efficiency of our system are validated through two case studies and interviews with experts.\n\n                Index Terms—prompt engineering, multimodal reasoning, multimodal large language modelsarXiv:2406.03843v3\n\n\n         1  INTRODUCTION\n          Large Language Models (LLMs), pre-trained on massive data with   spectrum of multimodal tasks (e.g., multimodal scene understanding\n             billions of parameters, have become a cornerstone for natural language   and question answering). By using text as the universal representation,\n            processing. They encode extensive knowledge about the world in their   these works aim to leverage LLMs to integrate and analyze knowledge\n           parameter space, exhibiting impressive capabilities in text understand-   distilled from diverse modalities (e.g., audio and images) in text format\n             ing, reasoning, and generation across various downstream tasks [8,62].  and provide a holistic understanding of multimodal content. These\n           Building on the strength of LLMs, there are an increasing number   models are also known as multimodal LLMs [49, 65]. Comprehend-\n           of works [5, 15, 24, 55, 55, 70] exploring their applications in a wide   ing multimodal content necessitates extensive multimodal knowledge,\n                                                                    where models not only need to understand the information presented\n                                                                                      in each individual modality but also have to correctly infer how the\n                                                                               information combines to inform accurate reasoning [63].\n            Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication      Recently, prompting has emerged as a data-efficient and user-friendly\n            xx xxx. 201x; date of current version xx xxx. 201x. For information on         paradigm for steering and improving LLM’s performance on complex\n             obtaining reprints of this article, please send e-mail to: reprints@ieee.org.      reasoning tasks. Relying on extensive knowledge acquired during pre-\n             Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx                            training, the models can instantly adapt to new downstream tasks in the\n                                                                               few-shot or even zero-shot settings without the need for model retrain-\n\ning [33]. Moreover, the LLMs can be prompted to generate free-text       prompt engineering for multimodal reasoning tasks.\nrationales emulating human thought processes in the chain-of-thought        • We conduct two case studies and expert interviews to demonstrate\n(CoT) manner [5]. For example, they can provide step-by-step deriva-        the usefulness and efficiency of POEM.\ntions that lead to the final answer of a math problem or substantiate their\nanalysis process with supported evidence such as emotive words for  2  RELATED WORKS\nsentiment prediction [73]. These human-readable rationales enhance   The research studies related to the design of POEM include prompt\nthe accuracy and transparency of the model’s reasoning process [39,42].   engineering, multimodal reasoning, and visual analytics for model\n  While multimodal LLMs exhibit remarkable performance in var-   understanding and steering.\nious tasks with prompting, their reasoning performance is notably\nsensitive to prompt variations. Moreover, inadequate or ill-designed   2.1  Prompt Engineering\nprompts may elicit erroneous knowledge, resulting in biased and unre-  Equipped with extensive knowledge acquired during pre-training,\nliable reasoning. Devising well-performing prompts that can guide and  LLMs (e.g., GPT- [8] and LLaMA [51] series models) exhibit remark-\nimprove the multimodal reasoning performance of LLMs remains a   able adaptability to specialized downstream tasks such as question an-\npersistent challenge. Prompting is inherently a process requiring exper-   swering, content retrieval, and complex reasoning, given precise instruc-\ntise and trial-and-error, where users need to meticulously craft prompts,   tions and proper demonstration examples. This emerging paradigm,\nscrutinize the outputs to identify flaws requiring improvement, and   known as prompting or prompt engineering, offers a user-friendly and\niteratively refine the prompts to reach the intended outcomes [12,48].   data-efficient way for non-expert users to interact and steer large mod-\nDuring the process, users first face the challenge of systematically un-    els. Prompting generally includes instruction-based and example-based\nderstanding and examining the multimodal reasoning performance of   prompts [4]. Instruction-based prompts include system prompts that\ndifferent prompts. Manually inspecting each instance is not only time-   provide general guidelines and task prompts that deliver direct and\nconsuming but also fails to provide a holistic understanding. Therefore,   task-specific instructions. Example-based prompts utilize a small set\nsummarizing and presenting generated rationales at varying detail levels   of examples to showcase the desired input-output patterns for models\nis non-trivial for users to fully verify outputs and pinpoint problematic   to follow. Numerous studies [21,59,69] have highlighted two major\naspects. However, in the multimodal context, the complex interplay   challenges in prompting: the formulation of effective prompts, and the\namong different modalities, coupled with the unstructured and gen-   assessment of prompt efficacy alongside strategies for enhancement.\nerative nature of free-text rationales, makes interpreting multimodal    Many studies have been conducted to address the prompting chal-\nLLMs’ reasoning process particularly challenging. Furthermore, users   lenges. Strobelt et al. introduced PromptIDE [48] as a tool for rapid\nalso struggle to revise their prompts in a way that effectively incorpo-   exploration and assessment of variations in prompt templates. Knowl-\nrates and elicits the desired multimodal reasoning knowledge from the   edgeVis [10] compared multiple fill-in-the-blank prompts to probe\nmodel [21]. Well-clarified task instructions (e.g., format, phrasing, and   the input-output associations in BERT-based models. Beyond expe-\ncontent) and informative demonstration examples are both imperative   diting the wording and phrase structure refinement, ScatterShot [59]\nfor enabling LLMs to grasp the intended input-output relationships and   proposed a slice-based sampling strategy to identify the most informa-\ngenerate consistent outputs with correct rationales [43]. Considering    tive data patterns for human annotation. PromptAid [43] combined\nthe huge space of possible task instructions and the difficulty of select-   multiple prompt perturbation strategies to find satisfactory prompts\ning and annotating demonstration examples from high-dimensional and   for text classification tasks. In addition, PromptChainer [60] and AI\ninformation-complex multimodal data, it is important yet challenging   Chains [61] have been developed to support more sophisticated tasks by\nto facilitate users to craft and refine prompts in an efficient manner.     decomposing them into manageable sub-tasks, and supported prompt\n  To tackle the above challenges, we present POEM, a visual analytics   chain prototyping and authoring to enhance controllability. Kim et al.\napproach designed to streamline the process of prompt engineering for   proposed EvaLM [22] for iterative prompt evaluation according to user-\nmodel practitioners, including model developers and model users, to   defined criteria, while ContitutionMaker [44] converted users’ natural\nsystematically probe and steer the multimodal reasoning performance   language feedback into a principle for chatbot behavior customization.\nof LLMs for targeted downstream tasks. To build a comprehensive   Besides text-to-text generative tasks, several works facilitated prompt\nunderstanding of LLMs’ knowledge and reasoning on multimodal tasks,   refinement for text-to-image generation by keywords [13] and style de-\nwe develop computational methods to decompose and summarize cross-   scription recommendation [7], structured search of visual concepts [35],\nmodal interactions captured by LLMs in various levels of detail. At the   and rubric-based adjustment for precise emotion expression [54].\nmodality level, we adopt a three-layer augmented Sankey diagram to     However, existing interactive prompt engineering systems are lim-\ncontextualize model performance with complement and conflict interac-   ited to text-to-text or text-to-image generation tasks, failing to deal with\ntions between modalities. Then, drilling down into specific interactions,   the complexity of multimodal inputs for more sophisticated reasoning\nwe distill and summarize the linguistic and visual evidence from individ-   tasks. In this paper, we develop POEM to optimize the prompt engi-\nual instances to reflect model reasoning patterns. These visualizations   neering process for adapting and steering the multimodal reasoning\nhelp align the model’s knowledge and reasoning processes with the   performance of LLMs. POEM facilitates a comprehensive investiga-\nhuman understanding at scale. Based on the multi-level model under-   tion of prompt effects and provides diverse support for users to iterate\nstanding, POEM allows users to conduct both top-down and bottom-up   prompts with reduced cognitive burden and increased efficiency.\napproaches to build and refine prompts that guide LLM’s multimodal\nreasoning. Specifically, we employ an effective sampling strategy for   2.2  Multimodal Reasoning\ndemonstration examples, ensuring a balance between relevance and   Reasoning generally refers to the process of drawing on evidence to\ndiversity to provide varied and informative input-output mappings for  make logical inferences based on existing knowledge for prediction\ninductive model learning. On the other side, drawing on human innate   and decision-making [49]. In the multimodal context, it is imperative\ncapabilities for summarization and generalization, we incorporate an   for models to not only grasp evidence derived from single modalities\nLLM-assisted module that distills principles at both instance-specific   but also to comprehend how evidence from different modalities relates\nand agnostic levels. This approach facilitates users to precisely articu-   to each other. This comprehension could lead to the generation of new\nlate and apply their domain-specific knowledge and expertise to guide   insights that the models must capture to achieve accurate reasoning.\nthe model deductively.                                                 Recently, LLMs have demonstrated the capability to generate coherent\n  Our contributions are summarized as follows:                         rationales through Chain of Thought (CoT) prompting [57], where\n    • We propose an effective human-in-the-loop workflow that facil-  LLMs provide the intermediate reasoning steps in natural language that\n      itates systematic investigation and guidance of the multimodal   lead to the final answer [73]. These generated free-text rationales have\n     reasoning performance of LLMs.                             been increasingly explored for model interpretability, as they provide\n    • We develop a visual analytics system POEM, equipped with care-   an explicit and transparent way to communicate the decision-making\n      fully designed visualizations and interactions to support efficient   process of models to end-users in a human-like manner [42].\n\nA growing number of benchmark datasets [14, 36, 68] have been   conference publications in the areas of multimodal machine learning\nproposed to evaluate the capabilities of LLMs in multimodal reason-  and multimodal LLMs.\ning tasks, with a primary focus on visual content understanding like      All experts concurred that there is a lack of tools for systemati-\nVisual Question Answering (i.e., answering text questions based solely   cally analyzing the multimodal reasoning performance of LLMs. The\non visual content). However, the comprehension of how LLMs in-   current practice typically begins with observing the model’s overall\ntegrate and coordinate information from various modalities (visual +   performance metrics, followed by randomly sampling instances to\nlanguage, or additional modalities) in the given context for question   examine reasoning correctness. While few datasets [27, 36] provide\nanswering and reasoning remains under-explored. This includes tasks   expert-written rationales as ground truth, the intricate interplay across\nthat necessitate a nuanced understanding of multimodal contexts, such   modalities and extensive variability in free-text expressions makes it\nas multimodal scene comprehension and multimodal sentiment analy-   challenging to systematically understand the knowledge models use\nsis [63]. Moreover, the metrics on these benchmarks fail to capture the   for reasoning and pinpoint their weaknesses. Moreover, crafting and\ndetailed reasoning process of models for in-depth model understanding    refining prompts to effectively elicit the desired knowledge from mod-\nand diagnosis. Besides evaluation, many works [37, 46,55, 70] have    els for specific tasks often require labor-intensive and tedious prompt\ntried to steer multimodal LLMs’ reasoning abilities. Compared with    iterations. Consequently, an integrated tool is desired to facilitate sys-\nthe labor-intensive fine-tuning approaches involving curating specific   tematic investigation of model behaviors at various levels and support\ndatasets with additional reasoning chain annotation, the training-free   well-informed prompt iterations with less cognitive effort. The design\nprompting-based methods [71,74] have become prevalent. However,   requirements are summarized as follows:\nthese automatic techniques fall short of providing fine-grained prompt\n                                                 R1 Summarize the impact of prompts on multimodal reasoningevaluation and flexible prompt refinement. Instead, we present a human-\n                                                              performance across varying levels of detail When evaluatingin-the-loop approach where users can interactively examine, evaluate,\n                                                                             the reasoning performance of different prompts, users focus notand refine prompts to guide and steer model performance in a more\n                                                                      only on overall statistics but also on how well the model’s rea-interpretable and controllable manner.\n                                                                      soning aligns with established knowledge at group and instance\n2.3  Visual Analytics for model understanding and steering          level. Therefore, it is crucial for the system to support multi-level\n                                                                and multi-faceted investigation of the model’s multimodal rea-\nVisual Analytics has proved to be an effective approach to help users                                                                        soning performance. Initially, the system should present a global\nunderstand and steer machine learning models [62, 67]. Prior works                                                                   overview of model performance. As E1 noted, “understanding\naimed to disclose the functionalities of neurons and layers of di-                                                       how different modalities interact is crucial for interpreting the\nverse neural network models like RNNs [17,41,47]. Recently, many                                                                      model’s behavior in the context of multimodal reasoning.” Users\nworks [11, 18, 20, 25, 34, 56, 64] have sought to elucidate the atten-                                                                  need to recognize the modalities the model relies on for its de-\ntion mechanism to understand the inner workings of transformer-based                                                                              cisions and how the model behaves when different modalities\nmodels in reasoning and decision-making process. Beyond visualizing                                                                             present complementary or contradictory information. After gain-\nmodel internals, numerous studies [9,26,31,50,52,58,72] have tried to                                                                         ing a global understanding, users also need insights into how\nprobe model knowledge through analyzing post-hoc model behaviors                                                                      evidence from distinct modalities and their combinations influ-\nwith input variations. For example, M2Lens [52] and MultiViz [31]                                                                      ence the model. For example, E3 expressed interest in identifying\ncharacterized intra- and inter-modal interactions with aggregated fea-                                                                which types of visual cues or spoken words the model interprets\nture importance for multimodal model diagnosis. The What-If Tool [58]                                                                             as key indicators during reasoning. Besides, users need to inspect\nand SliceTeller [72] identified specific data slices to understand model                                                                             the model’s output at the instance level to intuitively understand\nfailures. Integrated tools [9,26,50] have also been developed for unified                                                                  and verify the alignment of rationales with the original data.\nlanguage model evaluation.                                                 R2 Provide comparative analysis of different prompt perfor-\n  Beyond mere understanding, recent works [6,16,19,53] have pro-      mance Multiple aspects of prompts influence model reasoning\ngressed to align model behavior with human knowledge, thereby adapt-                                                                        performance, including the structure and content of task-specific\ning and steering models to generate desired outcomes for specific tasks.         instructions, as well as the choice and order of demonstration\nSharedInterest [6] designed quantitative metrics using saliency methods        examples. Navigating and exploring the evolving dynamics of\nto compare human and model reasoning for identifying recurring model        prompts is necessary for users to “identify influential factors for\nbehavior patterns. Hoque et al. [19] and He et al. [16] employed data                                                                     improvement”, as E2 commented. Therefore, it is imperative for\nprogramming concepts to inject human knowledge at scale for model                                                                              the system to document prompt alternations, support streamlined\nimprovement. CommonsenseVis [53] constructed knowledge graphs                                                                prompt testing, and assist users in tracking and comparing the\nwith external knowledge bases to contextualize model reasoning behav-                                                                                     effects of diverse prompt modifications throughout the refinement\niors and allow interactive model editing to enhance specific knowledge                                                                               process. This process facilitates an understanding of how different\nfor poorly behaved areas. Our work expands on these ideas to examine        modifications impact model reasoning performance, thereby of-\npost-hoc model behaviors with varied prompt inputs for comprehending                                                                                fering valuable insights for users to provide appropriate feedback\nhow different prompts affect model performance.  It further enables                                                                  and make informed decisions regarding subsequent iterations.\nmodel practitioners to provide feedback and align model performance   R3 Facilitate effective prompt refinement in diverse and efficient\nwith their knowledge and expertise through iterative prompting.           manner After pinpointing areas of underperformance, users can\n                                                                                align and elicit model knowledge through refining prompts. This\n3  DESIGN REQUIREMENTS                                                                        refinement includes providing more precise task and scenario\nOur goal is to develop a visual analytics approach that streamlines         descriptions, clear outlining principles for the model to follow,\nprompt engineering, empowering model practitioners to efficiently        and supplementing with informative demonstration examples that\nadapt and steer the multimodal reasoning performance of LLMs for tar-        help the model grasp the intended relationships. However, given\ngeted downstream tasks. By systematically understanding how models         the vast range of potential feedback options, it imposes a huge\nintegrate multimodal information for reasoning, users can evaluate and         cognitive burden on users to manually revise task articulation,\nenhance knowledge in underperforming areas through proper prompt        formulate principles from scratch, and source the most informa-\ndesign informed by domain expertise. To better understand users’ re-          tive examples for learning. Furthermore, since the feedback users\nquirements for system design, we worked closely with four experts in        intend to provide often stems from their intuition and expertise,\nNLP and multimodal machine learning (E1-E4, E1 is the coauthor).       encompassing both inductive and deductive reasoning [44], the\nE1 is a researcher specializing in developing interactive systems for        system ought to assist in translating these intuitive insights into\nNLP and multimodal model analysis. E2 is an industry researcher        concrete prompt content in an efficient and user-friendly manner.\nresponsible for applying and developing multimodal models for real-       For instance, E4 suggested providing diverse prompt templates\nworld applications. E3 and E4 are Ph.D. candidates with multiple top         for easy selection. E1 emphasized the need for a feature that con-\n\nFig. 2: The POEM system framework comprises four primary modules. (A) The visual and language modality information from the multimodal video\ndataset is processed by expert models, which are then fused and fed into multimodal LLMs. (B) The multimodal reasoning understanding module\nsummarized the nuanced modality interactions and patterns at global and group levels. (C) The prompt iteration strategy recommendation panel\nprovides diverse support for prompt refinement with semi-automatic k-shot example construction and instructional principle generation. (D) The\nPOEM interface facilitates efficient prompt performance examination, prompt refinement assistance, and prompt monitoring and comparison. sys\n\n\n\n      verts users’ fragmented feedback into systematic principles for   Evaluation Panel, users can evaluate and compare the effect of each\n      the model to follow, while E3 highlighted the importance of auto-  prompt iteration on both global model performance and individual\n     matically sourcing informative examples to provide high-quality   instances. Additionally, they can monitor and track detailed changes\n      rationales for knowledge alignment.                              across various prompt versions and iteratively refine the prompt to\n                                                                    achieve satisfactory multimodal reasoning performance.\n4  SYSTEM & METHODS\n                                                               4.2  Dataset and Model\nWe designed POEM based on the distilled design requirements in\nSec. 3. In this section, we first introduce the overall system frame-  We demonstrate the effectiveness of our system on two different datasets\nwork. Then we illustrate the methods for data processing, multimodal   for multimodal content comprehension tasks: CMU-MOSEI [2] for\nrationale understanding, and prompt iteration strategy recommendation.   multimodal sentiment analysis, and WTaG [3] for user intent under-\n                                                                        standing. The CMU-MOSEI [2] dataset consists of monologue video\n4.1  System Framework                                               clips in which speakers express their sentiments about a specific topic.\n                                                       The WTaG dataset [3] comprises egocentric video clips of users per-\nFigure 2 demonstrates the overarching workflow of the system. The                                                              forming cooking tasks under the guidance of an instructor within an\nmultimodal video dataset, processed into image frames (visual modal-                                                           augmented reality setting. The videos within both datasets contain\nity) with spoken narratives (language modality), along with the prompt,                                                                    information from two primary modalities: the language modality, rep-\nserves as input for the multimodal LLM. The multimodal LLM then per-                                                                      resented by spoken content, and the visual modality, characterized by\nforms reasoning and generates free-text answers for each input instance.                                                                        the scenes and user behaviors depicted in the videos. Both datasets in-\n(Fig. 2A). Subsequently, the Multimodal Reasoning Understanding                                                                     clude ground-truth labels for evaluation. Following the practice in prior\nmodule (Fig. 2B) provides a multi-level analysis of the generated free-                                                             works [43,59], we split each dataset into three subsets: a validation set,\ntext answers for a systematic understanding of the model’s reasoning                                                                   a demonstration example set, and a test set. In the splitting process, we\nbehavior. Initially, it characterizes different interaction types between                                                                     ensure that the label distribution remains consistent across these subsets.\nmodalities. Then, a multimodal reasoning pattern mining algorithm                                                       The validation set serves the purpose of prompt iteration evaluation.\nis employed to identify intricate and fine-grained reasoning patterns.                                                       The demonstration example set facilitates the construction of k-shot\nConcurrently, the Prompt Iteration Strategy Recommendation module                                                                 examples, and the test set provides additional instances beyond the\n(Fig. 2C) offers varied support, including bottom-up k-shot example                                                                           validation set for a more comprehensive assessment of prompt efficacy.\nrecommendations that balance similarity and diversity, and top-down                                                       The size of the validation set needs to be moderate so that users can\ninstructional principle summarization at both instance-specific and ag-                                                                       get timely feedback during the prompt iteration while also covering\nnostics levels aided by an auxiliary LLM. This module is designed                                                                        diverse data patterns for comprehensive model reasoning performance\nto facilitate efficient prompt refinement, aiming to elicit and enhance                                                                       diagnosis. Based on our preliminary experiment, we maintain a dis-\nspecific knowledge to guide and improve model performance                                                                            tribution ratio of 1:2:1 for the validation, demonstration, and test sets,\n   In the POEM interface (Fig. 2D), users have the option to either                                                                           respectively. We also implemented batch processing to improve the\ninput their own prompts or choose from available templates in the                                                                    system’s response speed.\nPrompt Panel. Subsequently, they can inspect the model’s multimodal                                                                  Regarding the model setting, we employ the LLaVA [32] and GPT-\nreasoning performance from different levels of detail. Specifically, at a              1                                                                    4V(ision)  model to perform multimodal reasoning considering their\nglobal level, users can inspect the model’s overall performance in the                                                                     strong reasoning and instruction-following abilities. We specifically\nEvaluation Panel and the interaction between and within modalities                                                                                 utilize the “llava-v1.5-13b” and “gpt-4-vision-preview” version. It’s\nin the Reasoning Panel. At the group level, users are able to scru-                                                                   important to note that our approach is designed to be model-agnostic,\ntinize the model’s reasoning patterns concerning different concepts                                                          meaning other multimodal LLMs that support multimodal content\nspanning across modalities. At the instance level, users can examine                               2                                                                        reasoning, such as Gemini  and LLaMA series [51], can be easily inte-\nindividual instances in detail for verification. Users can then revise                                                                        grated into the system. For each video clip, we followed the commonly\nand incorporate principles and/or k-shot examples into prompts based                                                                adopted practice [14,63], sampling frames per second to compose an\non automatic recommendations and insights obtained from the current\nmodel and prompt performance examinations. The refined prompt can       1https://openai.com/index/gpt-4v-system-card/\nthen be sent to the model for evaluation in the Prompt Panel. In the       2https://deepmind.google/technologies/gemini/\n\nimage sequence from the visual modality, which is then combined with    their combinations within and across individual modalities the model\nthe corresponding spoken content from the language modality as input    utilizes for reasoning. As shown in Fig. 2B, we parse the generated\nof the multimodal LLM.                                                   rationale into a list of intermediate evidence along with their associated\n  The input prompt for the multimodal LLM reasoning follows the   inferences that contribute to the final answer. For example, within\ngeneral prompt structure (I,{xi,yi}ki=1 ,xt) [22,57,60]. Here, I is the   a free-text rationale generated by the LLM, “The serious expression\ntask-specific instructions elaborating on the targeted scenarios, tasks   suggests a neutral sentiment, while in the spoken content, the phrase\nto be finished, and expected output structure (e.g., return your answer   ’incredible command’ conveys a positive sentiment. The visual evidence\nin a JSON object). To propel the LLM to perform CoT reasoning,   “serious expression” infers “neutral” sentiment and the language evi-\nThe prompt could include instructions like “Please provide a step-   dence “incredible command” implies “positive” sentiment. Given the\n                                                      LLM’s generative characteristics resulting in the variability of evidenceby-step analysis”. {xi,yi}ki=1 is the demonstration example set. Each                                                                      across different rationales, we employed the text-embedding-3-small3demonstration example includes input xi and output yi, where xi follows\n                                                            model to calculate embeddings for all extracted evidence (e.g., ”seriousthe same format as the validation set and yi includes the correct rational\n                                                                      expression” and ”incredible command” et al). Subsequently, we utilizedand final answer as provided by the ground-truth labels. We also support\n                                                                         the HDBSCAN algorithm [40] to cluster visual and language evidencethe zero-shot setting where demonstration examples are not provided.\n                                                                             respectively. We identified the evidence located closest to the cluster’sFinally, for each test input xt, the LLM is expected to generate the\n                                                                      centroid as the representative concept for each cluster. Subsequently,output yt, containing a free-text rationale and a final answer.\n                                                 we utilized the Apriori [1] algorithm to identify frequent patterns of\n                                                                 concept co-occurrence within and across different modalities in the4.3  Multimodal Rationale Understanding\n                                                                    generated rationales for validation set. This approach enables users to\n4.3.1  Modality Interaction Characterization                      conduct a more structured and comprehensive analysis of the patterns\nUnderstanding how multimodal models utilize information from dis-   within the generated rationales, allowing them to identify potential\ntinct modalities and integrate it to make cross-modal inferences is cru-   recurring biases or errors made by the model.\ncial for gaining insight into the model’s reasoning performance. Several\nworks [31,52] have tried to characterize the interaction between differ-   4.4  Prompt Iteration Strategy Recommendation\nent modalities based on aggregated feature attribution values [38,45].  As mentioned in Sec. 2, the content and phrasing of task instructions,\nThere are also works [28–30,66] trying to quantify the degree of inter-   along with the choice of demonstration examples, can greatly influence\nactions between modalities with a partial information decomposition   the model’s reasoning performance. Our preliminary experiment and\nframework. Building on the foundation of these works, we character-   expert interview results suggested that the instruction content (e.g., task\nize the modality interaction in the context of our targeted multimodal    specifications) and the choice of demonstration examples exert a more\nreasoning tasks as follows:                                        pronounced effect on model performance than the precise wording used\n   Considering the labeled multimodal dataset with two modality X1    for our targeted multimodal reasoning tasks. Therefore, in this paper,\nand X2, the unimodal data Di = {(xi,y) : Xi ×Y} where i ∈{1,2} , and  we mainly focus on facilitating users in instruction content refinement\nthe multimodal data DM = {(x1,x2,y) : X1 ×X2 ×Y}. When perform-  and demonstration example construction.\ning chain-of-thought reasoning, for each input data point, the output of\nthe multimodal LLM includes a free-text rationale and a final answer.   4.4.1  K-shot Example Recommendation\nHere, we denote the sample space where the multimodal LLM performs\n                                                                 Few-shot prompting has been a data-efficient strategy to adapt LLMs for\nreasoning using information from a single modality as  fi : Xi →∆Yi                                                                             specific downstream tasks using merely a handful of illustrative input-\nand the sample space where the multimodal LLM performs reason-\n                                                                   output pairs. However, the effectiveness heavily relies on the choice\ning with information from both modalities as fM : X1 × X2 →∆YM.                                                                     of examples to inform the model about the desired mapping [21,59].\nwhere ∆Y denotes the probability simplex of final output answer. For                                                                          Identifying informative examples for effectively guiding the model can\n fa and fb where a,b ∈{1,2,M}, The distance function can be defined                                                               be challenging for users. Moreover, beyond simply pairing inputs with\nas d (fa, fb) = ∥∆a −∆b∥to measure the distance between fa and fb.                                                                                 final answers, reasoning necessitates providing a rationale for each\nBased on the distance function, we can define two basic interaction\n                                                                 example, which is equally difficult for users to craft on their own.\ntypes between two modalities. When d (f1, f2) < θ, where θ is a pre-                                                          To enhance the efficiency of sourcing the demonstration example\ndefined threshold, the interaction type is complement, indicating these\n                                                                                       set, we first employed the k-nearest neighbors algorithm to sample the\ntwo modalities contribute to the final answer in the same direction. Con-\n                                                                     candidate k-shot example set considering both relevancy and diversity.\nversely, when d (f1, f2) > θ, the interaction type is conflict, indicating                                                                 For each instance in both the validation set and demonstration example\nthese two modalities provide discrepant information for reasoning. By\n                                                                                  sets, we computed embeddings for the visual (images) and language\nfurther considering how the final answer will change when analyzing\n                                                                                 (text transcript) modalities separately and then concatenated these em-\ninformation from each modality independently, and combining infor-\n                                                                beddings to represent each instance. Specifically, we utilized the pre-\nmation from two modalities jointly for reasoning, i.e., the distance                                                                         trained CLIP4 model, which maps text and images to a shared vector\nfunction d (f1, fM) and d (f2, fM), we can define subdivided interaction                                                                 space for embedding computation. Further details are provided in the\ntype [28,30] as shown in Figure 2B:\n                                                                 supplementary material. For each validation instance, we identified its\n    •  Complement-Redundant: when fM aligns with  f1 and  f2,                                                                       k-nearest neighbors as potential candidates based on their embedding\n     where d (f1, fM) < θ, and d (f2, fM) < θ                                                                    cosine similarity. These candidates were then ranked in descending\n    •  Complement-Distinct: when fM distinct from f1 and f2, where                                                                     order of similarity. To select the final k-shot examples, we prioritized\n     d (f1, fM) > θ, and d (f2, fM) > θ                                                                  both ranking and label diversity, ensuring the inclusion of all possible\n    •  Conflict-Dominant: when fM aligns with  f1 or  f2, where                                                                           labels in the final set to prevent model bias. To streamline the process\n     d (f1, fM) < θ and d (f2, fM) > θ or switch the f1 and f2                                                                                  5                                                                      of crafting rationales for users, we integrated the gpt-4-turbo model\n    •  Conflict-Distinct: when fM distinct from  f1 or  f2, where                                                                            to automatically generate structured rationales for each demonstration\n     d (f1, fM) < θ, and d (f2, fM) < θ                                                             example based on its ground truth labels. This approach offers users a\nIn this paper, we primarily focus on the visual and language modalities,                                                                     preliminary basis for refinement, sparing them the need to begin from\nwhich are the main subjects of investigation in current multimodal                                                                           scratch. Furthermore, we utilized the refinements operated by users\nLLM research. For more modalities, the interaction characterization                                                                          to iteratively enhance the quality of the generated rationales. These\nframework can be extended by pairwise comparison.\n                                                                    demonstration examples are then combined into the sequence{xi,yi}ki=1\n4.3.2  Multimodal Reasoning Pattern Mining                                                                                 3https://platform.openai.com/docs/guides/embeddings\nUpon gaining insight into the model’s reasoning process at the modality       4https://huggingface.co/sentence-transformers/clip-ViT-B-16\ninteraction level, it becomes crucial to identify the specific concepts or       5https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4\n\nfor inclusion in the prompt, where yi is the rationale and final answer   summary of K-shot examples with user-annotated rationales, waiting\nprovided by users for xi (Figure 2C).                                      for further editing or inclusion into the prompt.\n\n4.4.2  Instructional Principle Generation                        5.2  Reasoning Panel\n\nWhile k-shot examples aim to inductively teach the model the correct   The Reasoning Panel (Fig. 1B) facilitates a thorough investigation of the\nmappings between input-output pairs, providing explicit principles re-   model’s multimodal reasoning behaviors, from global and sub-group\ngarding proper practices or clarifying potential errors has also proven to   patterns down to specific individual instances (R1).\nbe an effective strategy for drawing out desired knowledge and guiding   A three-layer Sankey diagram-based design (Fig. 1B-1) is adopted\nmodel performance [44,71]. Humans generally formulate principles in   to portray interactions among modalities at the global level. The first\ntwo ways. One involves directly leveraging their existing knowledge.   layer demonstrates the overall distribution of prediction classes and\nFor example, the principle for identifying sarcasm could be to “pay   errors with two vertically stacked barcode charts. The horizontal length\nattention to the inconsistency between a word’s literal interpretation   encodes the number of instances, and the color encodes the correspond-\nand its contextual meaning.” The other is that individuals derive lessons   ing class and error. Instances belonging to the same class are positioned\nfrom specific instances and subsequently aggregate these instance-level   close together to enable easy exploration both within and between\ninsights into higher-level principles in a bottom-up manner. However,   classes. The second intermediate layer summarizes the conflict and\nusers may find it difficult to immediately generate principles from   complement relationship between visual and language modalities and\nscratch, derive insights by manually examining instances one at a time,   adopts the same encoding as the first layer. The third layer delves into\nand fully articulate their principles considering the complexity of mul-   the fine-grained four types of modality interactions. While retaining\ntimodal reasoning. For this purpose, we employed an auxiliary LLM   the same visual encoding for the prediction class and error distribution\nto facilitate the summarization and recommendation of principles. We   in each interaction type, this layer introduces two additional barcode\nselected the gpt-4-turbo model for its strong capabilities in text under-   charts to delineate the prediction result of the single visual and language\nstanding and summarization, and it can be replaced by more advanced   modality, thus illustrating the detailed distributions of the two modali-\nmodels in the future.                                                            ties across various types of interactions. Besides, two adjacent layers\n   Specifically, we instructed the gpt-4-turbo model to produce princi-   are interconnected through flows, the width of which is proportional\nples at both instance-specific and instance-agnostic levels (Figure 2C).   to the number of instances they encompass. Hovering over the flows\nAt the instance-specific level, the model is tasked with analyzing dis-   will highlight the related instances across all three layers. Users can\ncrepancies between generated reasoning and ground truth answers for   also brush the barcode chart in each layer to select an interested group\neach instance, summarizing potential error causes, and further deriving   of instances for further investigation. The selected instances will be\nprinciples to avoid similar mistakes. At the instance-agnostic level, We   highlighted with a grey background. The corresponding mined patterns\ninstruct the model to condense the generated instance-specific princi-  and instances will be displayed on the right and below respectively.\nples into more generic principles tailored to the specific targeted task.     After selecting the interested group of instances, the multimodal\nIt is important to acknowledge that the generated principles may not   reasoning pattern mining algorithm in Sec. 4.3 is applied, with the\nalways be accurate and should not be treated as golden rules. Their   extracted patterns displayed in the table on the right (Fig. 1B-2). Each\nprimary purpose is to provoke thought and inspire users to conceive   row exhibits one distinct pattern with its representative visual and\nnew ideas or enhance existing ones rather than initiate from zero. Thus,   language concepts, support (i.e., contained instance numbers), and error\nusers are empowered to either input and create their principles or choose    statistics. They can sort and filter the patterns based on these statistics\nto amend and revise principles that have already been generated accord-  by clicking on the corresponding column. The representative language\ning to their preferences. Details regarding the prompt used for principle   and visual concepts are shown for intuitive pattern understanding by\ngeneration are provided in the supplementary material.                   users.  Adjacent to each concept, a stacked bar chart presents the\n                                                                           distribution of its associated class. Users can expand each row to\n5  INTERFACE DESIGN                                       view the detailed distribution of evidence in a word cloud, where\n                                                                each phrase’s size represents its frequency of occurrence and its colorThe POEM interface (Fig. 1) consists of three coordinated views to\n                                                                   denotes the proportion of associated classes. Users can select patternsassist users in seamlessly evaluating the impact of different prompts,\n                                                                     or evidence of interest by clicking, and the corresponding instancesrefining prompts through semi-automatic suggestions, and conducting\n                                                                           will be displayed in the instance view below.iterative testing of prompts. In this section, we introduce the design of\n                                                          The instance view below (Fig. 1B-3) is designed to expedite theeach view and the interactions that connect them in detail.\n                                                                  examination and verification of individual instances by showcasing the\n                                                                          original multimodal video content along with its detailed reasoning.5.1  Prompt Panel\n                                                          The raw data column exhibits the video’s keyframe image sequence and\nThe Prompt Panel (Fig. 1A) provides flexible prompt operations to   spoken content to enable quick visual and language content digestion\nsupport smooth prompt engineering experience (R3). Upon selecting   and validation. Users can hover over these frames for an enlarged\nthe dataset and model, users can craft the prompt on their own or initiate   view and playback the original video for rapid verification. Subsequent\nby selecting from a list of prompt templates collected from state-of-  columns present the ground truth labels, the model’s predictions, and\nthe-art benchmarks [14, 63] in the Prompt Editor (Fig. 1A-1). The   the generated free-text rationales. To enhance readability and quick\nprompt is organized into distinct sections, as introduced in Sec. 4.2, to    text comprehension, evidence is highlighted with the corresponding\nfacilitate a clear and straightforward editing experience. Users can also   color of its associated class. The ground truth and prediction columns\nswitch to the plain text editing mode for editing and format checking   are also colored for easy comparison. Users can select instances for\nbefore submission. The Principle Recommendation view (Fig. 1A-2)   principle generation by clicking the “Generate   ” button, after which\ndisplays an organized summary of principles for user validation. The   the generated principles will be listed in the Principle Recommendation\ngenerated instance-specific and agnostic principles are differentiated by   view. In addition to displaying selected validation instances for review,\nbackground colors: gray for instance-specific principles and green for   users can toggle to the K-shot Example Mode( Figure 6A). Within\ninstance-agnostic principles. Newly generated principles are marked    this mode, the interface presents the ranked list of k-shot examples\nwith red dots at the top right for highlighting. Users can modify any   recommended by the proposed sampling strategy in Sec. 4.4. For each\nexisting principle by utilizing the editing function or articulate their   example, the interface details its raw data (i.e., keyframe sequence and\nprinciples via the principle input box. Furthermore, users are allowed to   spoken narratives in the raw video), its ground truth, and the rationales.\ndelete any principles deemed inappropriate or redundant. Subsequently,   Users can modify the content in the corresponding column directly\nupon selecting the desired principles, users can integrate them into the   to provide high-quality rationales. Moreover, users can source more\ncurrent prompt within the prompt editor by clicking the “Import   ”   k-shot examples by clicking the “Retrieve   ” button and save the\nbutton. The K-shot Example List (Fig. 1A-3) below provides a concise   selected ones to the K-shot Example List with the “Save   ” button.\n\n5.3  Evaluation Panel                                          while the visual concept “smile” comprised instances such as “small\n                                                                       smile”, “slight smile”, and “smiling” marked in red (Fig. 3B-2). E5The Evaluation Panel (Fig. 1C) offers comprehensive insights into\n                                                                  thought these inferences for each modality reasonable but wonderedboth global and local performance of prompts, along with the prompt\n                                                   how the correctly deduced evidence led to the final error. Therefore,iteration history for efficient monitoring and comparison of prompt\n                                                                    she proceeded to inspect the detailed reasonings of individual instancesperformance (R2).\n                                                                       exhibiting this pattern in the instance view below (Fig. 3C). Upon  The Prompt History view (Fig. 1C-1) archives previous prompts\n                                                              examining the raw data and the rationales generated by the LLM, E5regarding their content and performance. Each row represents a prompt\n                                                                          figured out that the model correctly reasoned about individual modality,version with its accuracy and modifications are summarized using intu-\n                                                                     as in these instances, the speakers had explicitly stated their negativeitive icons. This design enables users to easily compare performance\n                                                                    opinions verbally while showing mild positive facial expressions likeand trace alterations in different sections of the prompts. Users can\n                                                                       gentle smiles. However, the LLM was biased by the positive visualexpand and collapse each row for a hierarchical examination of modifi-\n                                                                          cues, allowing them to overshadow and dominate its reasoning, despitecations within each prompt section. Detailed additions and deletions in\n                                                                        the explicit negative sentiment conveyed through language.the content are distinctly marked and highlighted through varied colors\nand line styles. The line chart below shows the model accuracy change.\nThe Overall Model Performance view (Fig. 1C-2) records the global\nperformance statistics of each prompt iteration. Users can expand each\nrow to inspect the detailed confusion matrix. The Instance Test view\n(Fig. 1C-3) exhibits the performance of prompts on individual instances\nthat are of particular interest to users. Users can select instances from\nReasoning Panel and save them to observe their performance change\nduring prompt iterations with the “Save   ” button. They can also\nsource additional unseen test instances with the “Retrieve   ” function.\n\n6  EVALUATION\nIn this section, we showcase the efficacy and efficiency of POEM via\ntwo case studies and feedback gathered from expert interviews. The\nprimary objective of the two case studies is to help users obtain well-\nperforming prompts utilizing their domain expertise and knowledge to\nguide LLM’s multimodal reasoning performance with minimal effort.\n\n6.1  Case One: Improving multimodal sentiment reasoning\n     with CMU-MOSEI dataset\nE5, a sentiment analysis expert, seeks to generate effective prompts\nfor steering LLM’s multimodal sentiment reasoning performance with\nthe CMU-MOSEI dataset. The LLM is tasked with interpreting the\nspeakers’ verbal and visual signals to determine their sentiment as\n“positive”, “negative”, or “neutral”.\n   After loading the dataset and model, E5 initially selected and sub-\nmitted a provided prompt template in the Prompt Panel to evaluate\nits performance (R2), which yielded an accuracy of 70%. To gain an\noverview of the interactions between the visual and language modali-\n                                                                              Fig. 3: (A) Identified dense error areas in conflict-dominant modality\nties in the LLM’s reasoning process (R1), E5 began by examining the\n                                                                                interaction.  (B) The multimodal pattern “didn’t like” and “smile” and\nSankey diagram in the Reasoning Panel. Through observing the length                                                                                   their associated evidence group.  (C) The error cases where “smile”\nand error distribution of the barcode charts in the first and second layers,                                                                 predominated and biased the reasoning process.\nE5 noticed that the model tended to interpret sentiments as “neutral” or\n“positive” rather than “negative”. Furthermore, in a large proportion of\ninstances, the visual and language modalities provided complementary     Following this discovery, E5 decided to derive principles from these\ninformation, while in others, they presented conflicting information   erroneous cases to guide the LLM toward correct reasoning in this\nwith increased errors. E5 was particularly interested in how the LLM   situation (R3).  Therefore, E5 selected these instances and clicked\nreasoned in scenarios where the two modalities presented conflicting   the “Generate   ” button to generate principles. She also saved these\ninformation and how errors occurred. So, she explored the third layer   instances of interest to the right test panel for further validation. In the\nfor a more fine-grained examination. At the third layer, she identi-   Principle Recommendation View, E5 reviewed the generated principles\nfied a dense cluster of errors within the conflict-dominant relationship   and identified well-articulated general principles that underscored the\n(Fig. 3A), where the visual modality implied a positive influence, while   importance of interpreting visual cues alongside the corresponding\nthe language modality suggested a negative one. The ultimate com-   verbal content with careful consideration of specific context (Fig. 4A).\nbined effect was positive, indicating that the visual modality dominated   To ensure generalizability and avoid introducing new bias, E5 revised\nthe reasoning process.                                                   the last sentence as “It is crucial to avoid overemphasizing one modality\n   Following this, E5 brushed this group of instances to further inspect   over another when the latter carries clear indications of opinions or\ntheir contained reasoning patterns in the table on the right. When    explicit expressions of sentiment.” Then, E5 imported this principle into\ngoing through the patterns sorted in descending order of error rate,   the prompt editor and submitted it for testing. In the Model Performance\nE5 discovered the combination of the language concept “didn’t like”   View, she found a slight improvement in the overall accuracy from\nwith the visual concept “smile” yielded high error rates (Fig. 3B). The  70% to 74%. Meanwhile, in the Test Panel View, she checked the\nadjacent bar charts, predominantly colored in blue for “didn’t like”   performance of the new prompt on previously saved instances, the\nand red for “smile”, indicated that the LLM consistently interpreted   majority of which were now correctly reasoned (Figure 1C). This\nlanguage evidence concluded with “didn’t like” as a negative signal   indicated that the incorporated principle had effectively guided the\nand visual evidence featuring “smile” as positive during the reasoning  LLM to use the correct knowledge for reasoning in this scenario.\nprocess. She further explored this pattern by unfolding the row, where      Subsequently, E5 sought to enhance the model’s reasoning stability\nevidence under the language concept “didn’t like” included phrases   and its ability to recognize varied patterns in sentiment analysis by\nlike “arduous”, “boring”, and “hate” highlighted in blue (Fig. 3B-1),   incorporating some k-shot examples (R3). Thus, she switched to K-shot\n\nExample Mode, where recommended K-shot examples with reasonings   the model. (R3). Thus, he revised the prompt to add the clarification\ncrafted by the auxiliary LLM were listed. E5 selected the top three   such as “Self Description refers to scenarios where the user narrates\ninstances spanning distinct classes and refined the provided reasoning   or explains what they are doing, intend to do, or their thought process\nleveraging his knowledge and expertise. Upon completing the rationale   regarding the task at hand.” While submitting this prompt for testing,\nannotations, E5 appended these examples to the K-shot example list on  E6 also thought that, besides providing explicit rationales, he could\nthe left side and imported them into the prompt. After running the test,   also include concrete k-shot examples to help the model learn (R3).\nthe overall accuracy increased to 82% (Fig. 4B).                 He navigated to the K-shot Example Mode in the Reasoning Panel and\n                                                                        selected five K-shot examples, each representing a distinct class from\n                                                                          the top recommended ones (Fig. 6A). E6 also noticed that the rationales\n                                                                   generated by the more advanced auxiliary LLM also contained errors\n                                                                          for the “Self Description” class, indicating that this category might\n                                                              be challenging for LLMs to grasp and reason about, underscoring the\n                                                             need for providing additional guidance in the prompt. Following the\n                                                                   refinement of rationales for the k-shot examples, E6 imported these\n                                                                    annotated examples and submitted this prompt version for testing. E6\n                                                                     then examined the updated test outcomes in the Evaluation Panel (R2).\n                                                        The increased performance statistics proved that providing either ex-\n                                                                                    plicit explanations or k-shot examples can help improve the LLM’s\n                                                                    reasoning performance ( Figure 5).\nFig. 4: (A) The recommended principles for alleviating errors in case one.\n(B) The recorded prompt iteration history in case one.\n\n\n\n6.2  Case Two: Enhancing Multimodal User Intention Un-\n     derstanding with WTaG dataset\n\nE6 is an engineer tasked with building an intelligent virtual assistant\nto help users perform complex tasks within augmented reality envi-\nronments. Building such an assistant necessitates comprehending user\nintentions. E6 thus wanted to steer the GPT-4V(ision) model using\nPOEM to finish this task. E6 experimented with the WTaG dataset [3],\nwhere the video clips were recorded from the user’s egocentric per-\nspective. These clips included user-instructor dialogues captured by\nmicrophones and visual context encompassing the scene and user be-\nhaviors from head-mounted cameras. The multimodal LLM needs to    Fig. 6: (A) The selected and annotated k-shot examples from distinct\ndeduce the user’s intention based on this multimodal context and catego-   classes. (B) The “uh” pattern influenced the “Hesitation” class reasoning.\nrize it into one of five classes: “Question”, “Answer”, “Confirmation”,   (C) The recommended principles to guide “Hesitation” class reasoning.\n“Hesitation” and “Self Description”.                                    (D) The test results of added out-of-distribution instances.\n   After initializing the dataset and model, E6 first chose to use the\nprompt provided in the original dataset repository for validation (R2).\n                                                   E6 further explored the performance specifics of the latest promptThe Evaluation Panel revealed that this prompt achieved only 53%\n                                                                       version (enhanced with k-shot examples) in the Reasoning Panel (R1).\naccuracy in a zero-shot setting. While this result is higher than what\n                                                   He identified a cluster of errors in the first layer of the Sankey diagramwas reported in the paper using gpt-3.5-turbo model [3], it is still in-\n                                                                         associated with the predicted “Hesitation” class. The consistent yellowsufficient for the task. E6 next examined the confusion matrix and\n                                                                      color of the language modality and the overall prediction suggestednoticed that the model’s predictions were heavily biased towards the\n                                                                             that language modality predominated the reasoning process, and all“ Confirmation” and “Answer” classes ( Figure 3A). Upon randomly\n                                                                      these instances were misclassified as the “Hesitation” class. In theinspecting the model-generated rationales alongside the raw data in-\n                                                                         pattern table on the right, he identified a frequent language pattern,correctly classified in the Reasoning Panel, E6 observed that while the\n                                                                    “uh”, associated with a high error rate, with its bar chart fully coloredmodel could adequately describe and analyze both visual and spoken\n                                                                          in yellow (Fig. 6B). Upon expanding the row, he found it containedcontent, it struggled to comprehend the meaning of designated pre-\n                                                                 evidence like “uh” and “oh” that indicated “Hesitation”. Therefore,diction classes, especially “Self Description.” This resulted in scarce\n                                                 E6 clicked the row to examine the specific instances it included. He\npredictions for this class and a bias towards more familiar classes such\n                                                              found that whenever the spoken content contained modal words like\nas “Confirmation” and “Answer”.\n                                                             “uh” and “oh”, the model interpreted these as indicators of unwilling-\n                                                                     ness to continue, thereby predicting the user’s intention as “Hesitation”\n                                                                    without considering any other factors. Consequently, E6 selected these\n                                                                        instances for the auxiliary LLM to summarize principles for avoiding\n                                                               such error (R3). He then refined and incorporated these principles\n                                                                              (Fig. 6C) into the prompt and saved these instances in the Instance Test\n                                                                    view. Additionally, he added multiple instances from his project into\n                                                                       the Instance Test view to evaluate the prompt robustness (R2). The\n                                                                                     test results showed that the accuracy reached 77%, with the added test\n                                                                          instances correctly predicted (Fig. 6D). E6 was satisfied with this result\n                                                             and planned to use the prompt for his project.\nFig. 5: (A) The confusion matrix showing the model’s prediction bias\ntowards the “Confirmation” and “Answer” classes.  (B) The recorded   6.3  Expert Interviews\nprompt iteration history in case two.\n                                            We further conducted semi-structured interviews with two academic\n                                                                       researchers and one industry research scientist (P1-P3) to verify the\n  To address this problem, E6 decided to include more explicit ratio-   effectiveness and usability of POEM. All participants had experience in\nnales of each prediction class within the prompt instructions to guide   prompt engineering and the training or adaption of multimodal LLMs\n\nfor downstream tasks, while none had previously tried the POEM be-   ancing both uniqueness and generability. Identifying and crafting an\nfore the interviews. Each interview began with the research background    effective set of principles with suitable granularity for different tasks\nintroduction, followed by the system workflow and function demon-   remains an open question. Moreover, current users can only articulate\nstration with examples. Experts were then invited to freely explore the   principles in natural language where more diverse interactions (e.g.,\nsystem using real datasets, voicing their thoughts in a think-aloud man-   clicking in SAM [23]) can be integrated to enable users to provide\nner. We also collected feedback from E5 and E6 during case studies.  more nuanced and precise feedback. Meanwhile, managing the accu-\nThe gathered feedback is summarized below:                        mulated principles is non-trivial due to conflict and forgetting issues.\n  System workflow All experts concurred that the workflow of POEM   Users may also struggle to grasp the influence of varying principles on\nis thoughtfully designed, significantly improving the efficiency of   model performance. Utilizing LLMs to condense and differentiate the\nprompt iteration compared to their current practices, which rely solely   patterns and impacts of principles could serve as a potential solution.\non performance statistics for evaluating prompt effects and laborious  On the other side, while principles are most effective for large models\nmanual experiments to search for better-performing prompts. As P2   possessing robust instruction-following capacities, they can also benefit\nnoted, “I think POEM offers a more systematic and comprehensive way   smaller models by guiding dataset retrieval and generation for model\nto analyze the model’s complex reasoning behaviors.” P1 highlighted   fine-tuning. Furthermore, as demonstration examples and principles\nthat the varied strategies and streamlined process provided by POEM   represent two distinct approaches of injecting and eliciting knowledge\nnotably “reduce the pain for prompt writing and testing ” which are   for reasoning in a bottom-up and top-down manner respectively, how\nchallenging tasks for them. E5 commented that the recommended   to collocate k-shot examples with principles to maximize information\nprinciples and K-shot examples “serve as good starting points to bring   gain in prompt engineering remains a compelling question.\nnew perspectives and inspire thoughts”.                             System generalizability and scalability In this paper, we mainly fo-\n  System designs and interactions All experts remarked that the   cus on the interaction between the two most-studied visual and language\nvisual and interaction design of POEM is intuitive and easy to learn   modalities. However, our system can be extended to investigate the\nand use. P3 expressed particular favor for the Prompt History design,   interactions between multiple modalities by pair-wise comparison. Be-\nwhich makes it effortless to track every detail of changes, “as I usually   sides the analysis tasks evaluated in this paper, the proposed framework\nget lost after several rounds of prompt iteration. Now I can start with    is readily to be utilized for other multimodal content comprehension\nany version at ease.” P1 valued the convenient one-click generate and   and reasoning tasks such as multimodal hate or sarcasm recognition,\nimport function, which saves tons of time in manually editing and   and multimodal context question answering [63], where the modality\nformatting the prompts. E5 appreciated the ability to examine and   interaction relationship persistently exist. This prompting-based sys-\nevaluate at the instance level with reference to raw data, stating, “Since   tem can also serve as a testing tool to uncover weaknesses in model\nhallucinations can happen inevitably, having access to instance-specific   multimodal reasoning performance and identify example types and\ndetails for validation significantly increased my trust for the system   principles to inform larger-scale data collection for model fine-tuning.\nand confidence in the prompts I developed”. Meanwhile, experts E6   Moreover, the design of the system can be extended for other applica-\nand P2 mentioned that it took some time to understand and proficiently    tions. For example, the Reasoning Panel design can be used for other\nuse the Sankey diagram, yet they acknowledged that the complexity of   tasks that necessitate summarizing relationships across various informa-\nmultimodal reasoning performance necessitates such a design.           tion channels at multiple levels. The highlighted difference design in\n  Suggestions for improvement P1 proposed that the generated   Prompt History view can also help text summarization and comparison\ninstance-specific principles can be visually linked to their originat-   tasks in a structured and intuitive way. The system scalability is rooted\ning instances to offer a more intuitive and comprehensible reference.   in the algorithm and visual design. The bottleneck of the algorithm part\nP2 expressed a desire for a feature that allows the system to recommend    is the time cost of processing the video dataset and LLM’s generation\ninstances based on users’ high-level input criteria for further evaluation   speed. Currently, we have implemented batch processing to expedite\nor demonstration. E6 also thought it would be beneficial if the system   the data processing and generation process for a smooth prompting\ncould help summarize users’ annotated rationales to identify potential   experience. However, this approach may not suffice for handling the\nambiguities and conflicts. P3 thought it would be interesting and useful   data scale of thousands of instances, necessitating the exploration of\nto enable comparisons across multiple LLMs. Besides, step-by-step    strategies like parallel computing and data sampling to ensure instant\nguides are wanted during real-time exploration to reduce learning curve.   feedback. For the visual design, The Sankey diagram design in Rea-\n                                                                   soning Panel may become visually cluttered when dealing with a large\n7  DISCUSSION                                            number of prediction classes or complex modalities. For this situation,\n                                                 we can consider adopting a hierarchical visualization design coupled\nIn this section, we discuss the POEM regarding knowledge alignment\n                                                                  with interaction techniques to enhance visual scalability.\nwith principle, system generalizability, and scalability. We also pointed\n                                                                  Limitations & Future Work Current LLMs exhibit deficienciesout current limitations and potential directions for future work.\n                                                                           in producing hallucinated and inconsistent responses. Our system has\n  Human-AI knowledge alignment through principle Given the\n                                                                               tried to mitigate this issue by fixing hyperparameters and providing a\nemerging prompting paradigm that allows users to interact with LLMs\n                                                                          multi-level systematic analysis of outputs regarding different prompts,\nthrough natural language, there is a growing interest in harnessing ex-\n                                                                   allowing users to easily examine and identify outlying responses. Fu-\nplicitly stated principles for evaluating and guiding model performance\n                                                                             ture efforts can be directed towards developing techniques for reducing\nin downstream applications. While previous studies have explored the\n                                                                         hallucination occurrence in model outputs. Moreover, considering the\nassessment of models using human-input criteria [22] and the alignment\n                                                                            potential information loss or inaccuracies introduced by expert models\nof chatbot behaviors with user preferences through converting feedback\n                                                                       across different modalities, we plan to integrate more advanced expert\ninto principles [44], our research pioneers the use of data-derived prin-\n                                                            models and visualize potential uncertainties to increase user trust. In\nciples to direct and improve model multimodal reasoning performance.\n                                                                        the future, we consider enabling comparison across multiple LLMs to\nDrawing on the innate human capacity for both inductive and deduc-\n                                                                            further investigate effective prompt engineering strategies for different\ntive reasoning, POEM proposed an LLM-assisted module condensing\n                                                              models and tasks. Additionally, we plan to extend our work to study in-\nboth instance-specific and agnostic principles to encourage users to\n                                                                            teraction involving more modalities in increasingly complex scenarios\nefficiently express and externalize their domain-specific knowledge and\n                                                             and applications.\nexpertise for model steering. Despite the exhibited great potential for\neliciting desired knowledge, exploring how to design, manage, and\n                                                     8  CONCLUSIONapply principles more effectively across varied tasks and contexts re-\nmains a fertile area for research. As pointed out by prior works [44,71],   In this paper, we introduce POEM, a novel visual analytics tool de-\nthere is no one-size-fits-all principle granularity, as the effectiveness   signed to facilitate prompt engineering for enhancing multimodal rea-\nvaries with task complexity, dataset diversity, and principle quality. In   soning of LLMs with human insight and expertise. The system al-\nour work, we provide both specific and universal principles for bal-   lows users to thoroughly assess prompt effectiveness through well-\n\nsummarized multimodal reasoning patterns and offers varied strate-   [19] M. N. Hoque, W. He, A. K. Shekar, L. Gou, and L. Ren. Visual concept\ngies for prompt revision, enabling users to apply their knowledge for        programming: A visual analytics approach to injecting human intelligence\nefficient prompt iteration. The system’s efficacy and efficiency are          at scale. IEEE Transactions on Visualization and Computer Graphics,\nvalidated through two case studies and positive feedback from experts.        29(1):74–83, 2023. doi: 10.1109/TVCG.2022.3209466 3\n                                                                               [20]  T. Jaunet, C. Kervadec, R. Vuillemot, G. Antipov, M. Baccouche, and\n                                                                             C. Wolf. Visqa: X-raying vision and language reasoning in transformers.\n                                                                 IEEE Transactions on Visualization and Computer Graphics, 28(1):976–\nREFERENCES                                                                                 986, 2022. doi: 10.1109/TVCG.2021.3114683 3\n [1] R. Agrawal, R. Srikant, et al. Fast algorithms for mining association rules.   [21] E. Jiang, K. Olson, E. Toh, A. Molina, A. Donsbach, M. Terry, and C. J.\n      In Proc. VLDB, vol. 1215, pp. 487–499. Santiago, 1994. 5                        Cai. Promptmaker: Prompt-based prototyping with large language models.\n [2] A. Bagher Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency.         In Proc. CHI: Extended Abstracts, article no. 35, 8 pages. ACM, New\n     Multimodal language analysis in the wild: CMU-MOSEI dataset and         York, 2022. doi: 10.1145/3491101.3503564 2, 5\n      interpretable dynamic fusion graph.  In Proc. ACL (Volume 1: Long    [22]  T. S. Kim, Y. Lee, J. Shin, Y.-H. Kim, and J. Kim. Evallm: Interactive\n      Papers), pp. 2236–2246. ACL, 2018. doi: 10.18653/v1/P18-1208 4              evaluation of large language model prompts on user-defined criteria. In\n [3] Y. Bao, K. Yu, Y. Zhang, S. Storks, I. Bar-Yossef, A. de la Iglesia, M. Su,         Proc. CHI, article no. 306, 21 pages. ACM, 2024. 2, 5, 9\n     X. Zheng, and J. Chai. Can foundation models watch, talk and guide you    [23] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,\n      step by step to make a cake? In Findings of EMNLP, pp. 12325–12341.         S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick. Segment\n    ACL, 2023. doi: 10.18653/v1/2023.findings-emnlp.824 4, 8                     anything. In Proc. ICCV, pp. 4015–4026. IEEE Computer Society, Los\n [4] D. Bhattacharjya, J. Lee, D. J. Agravante, B. Ganesan, and R. Marinescu.        Alamitos, 2023. 9\n     Foundation model sherpas: Guiding foundation models through knowl-   [24]  J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-\n     edge and reasoning, 2024. doi: 10.48550/arXiv.2402.01602 2                 image pre-training with frozen image encoders and large language models.\n [5] A. Bhattacharyya, Y. K. Singla, B. Krishnamurthy, R. R. Shah, and         In Proc. ICML, vol. 202, pp. 19730–19742. PMLR, 2023. 1\n     C. Chen. A video is worth 4096 tokens: Verbalize videos to understand    [25] R. Li, W. Xiao, L. Wang, H. Jang, and G. Carenini. T3-vis: visual analytic\n     them in zero shot. In Proc. EMNLP, pp. 9822–9839. ACL, Singapore,          for training and fine-tuning transformers in NLP. In Proc. EMNLP: System\n     2023. doi: 10.18653/v1/2023.emnlp-main.608 1, 2                            Demonstrations, pp. 220–230. ACL, Singapore, 2021. doi: 10.18653/v1/\n [6] A. Boggust, B. Hoover, A. Satyanarayan, and H. Strobelt. Shared interest:        2021.emnlp-demo.26 3\n     Measuring human-ai alignment to identify recurring patterns in model    [26] Z. Li, X. Wang, W. Yang, J. Wu, Z. Zhang, Z. Liu, M. Sun, H. Zhang, and\n      behavior. In Proc. CHI, article no. 10, 17 pages. ACM, New York, 2022.         S. Liu. A unified understanding of deep nlp models for text classification.\n      doi: 10.1145/3491102.3501965 3                                    IEEE Transactions on Visualization and Computer Graphics, 28(12):4980–\n [7]  S. Brade, B. Wang, M. Sousa, S. Oore, and T. Grossman. Promptify: Text-        4994, 2022. doi: 10.1109/TVCG.2022.3184186 3\n     to-image generation through interactive prompt exploration with large    [27] Z. Lian, L. Sun, M. Xu, H. Sun, K. Xu, Z. Wen, S. Chen, B. Liu, and\n     language models. In Proc. UIST, article no. 96, 14 pages. ACM, New            J. Tao. Explainable multimodal emotion reasoning. arXiv, 2023. doi: 10.\n     York, 2023. doi: 10.1145/3586183.3606725 2                               48550/arXiv.2306.15401 3\n [8]  T. Brown, B. Mann, N. Ryder, and et al. Language models are few-shot    [28]  P. P. Liang, Y. Cheng, X. Fan, C. K. Ling, S. Nie, R. Chen, Z. Deng,\n      learners. In Proc. NeurIPS, vol. 33, pp. 1877–1901, 2020. 1, 2                N. Allen, R. Auerbach, F. Mahmood, R. R. Salakhutdinov, and L.-P.\n [9] A. A. Cabrera, E. Fu, D. Bertucci, K. Holstein, A. Talwalkar, J. I. Hong,        Morency. Quantifying and modeling multimodal interactions: An infor-\n     and A. Perer. Zeno: An interactive framework for behavioral evaluation        mation decomposition framework. In NeurIPS, vol. 36, pp. 27351–27393,\n      of machine learning. In Proc. CHI, article no. 419, 14 pages. ACM, New        2023. 5\n     York, 2023. doi: 10.1145/3544548.3581268 3                             [29]  P. P. Liang, Y. Cheng, R. Salakhutdinov, and L.-P. Morency. Multimodal\n[10] A. Coscia and A. Endert. Knowledgevis: Interpreting language models         fusion interactions: A study of human and automatic quantification. In\n     by comparing fill-in-the-blank prompts. IEEE Transactions on Visualiza-        Proc. ICMI, 11 pages, pp. 425—-435. ACM, New York, 2023. doi: 10.\n      tion and Computer Graphics, pp. 1–13, 2023. doi: 10.1109/TVCG.2023.       1145/3577190.3614151 5\n     3346713 2                                                               [30]  P. P. Liang, C. K. Ling, Y. Cheng, A. Obolenskiy, Y. Liu, R. Pandey,\n[11]  J. F. DeRose, J. Wang, and M. Berger. Attention flows: Analyzing and        A. Wilf, L.-P. Morency, and R. Salakhutdinov. Quantifying interactions\n     comparing attention mechanisms in language models. IEEE Transactions          in semi-supervised multimodal learning: Guarantees and applications. In\n     on Visualization and Computer Graphics, 27(2):1160–1170, 2021. doi: 10        ICLR, 2024. 5\n     .1109/TVCG.2020.3028976 3                                            [31]  P. P. Liang, Y. Lyu, G. Chhablani, N. Jain, Z. Deng, X. Wang, L.-P.\n[12] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and        Morency, and R. Salakhutdinov. Multiviz: Towards visualizing and under-\n     Z. Sui. A survey for in-context learning. arXiv, 2022. doi: 10.48550/arXiv         standing multimodal models. In ICLR, 2023. 3, 5\n     .2301.00234 2                                                           [32] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In Proc.\n[13] Y. Feng, X. Wang, K. K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang,        NeurIPS, vol. 36, pp. 34892–34916, 2023. 4\n     and W. Chen. Promptmagician: Interactive prompt engineering for text-   [33]  P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train,\n     to-image creation. IEEE Transactions on Visualization and Computer         prompt, and predict: A systematic survey of prompting methods in natural\n     Graphics, 30(1):295–305, 2024. doi: 10.1109/TVCG.2023.3327168 2          language processing. ACM Computing Surveys, 55(9), article no. 195, 35\n[14] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou,         pages, 2023. doi: 10.1145/3560815 2\n     Y. Shen, M. Zhang, et al. Video-mme: The first-ever comprehensive eval-   [34]  S. Liu, Z. Li, T. Li, V. Srikumar, V. Pascucci, and P.-T. Bremer. Nlize: A\n     uation benchmark of multi-modal llms in video analysis. arXiv preprint          perturbation-driven visual interrogation tool for analyzing and interpreting\n     arXiv:2405.21075, 2024. 3, 4, 6                                                  natural language inference models. IEEE Transactions on Visualization\n[15] L. Hanu, A. L. Ver˝o, and J. Thewlis. Language as the medium: Multimodal        and Computer Graphics, 25(1):651–660, 2019. doi: 10.1109/TVCG.2018.\n     video classification through text only. arXiv, 2023. doi: 10.48550/arXiv.       2865230 3\n     2309.10783 1                                                            [35] V. Liu, H. Qiao, and L. Chilton. Opal: Multimodal image generation for\n[16]  J. He, X. Wang, K. K. Wong, X. Huang, C. Chen, Z. Chen, F. Wang,       news illustration. In Proc. UIST, article no. 73, 17 pages. ACM, New York,\n    M. Zhu, and H. Qu. Videopro: A visual analytics approach for interactive         2022. doi: 10.1145/3526113.3545621 2\n     video programming. IEEE Transactions on Visualization and Computer    [36]  P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord,\n     Graphics, 30(1):87–97, 2024. doi: 10.1109/TVCG.2023.3326586 3                P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via\n[17]  F. Hohman, H. Park, C. Robinson, and D. H. Polo Chau. Summit: Scaling         thought chains for science question answering. In Proc. NeurIPS, vol. 35,\n     deep learning interpretability by visualizing activation and attribution sum-         pp. 2507–2521, 2022. 3\n      marizations. IEEE Transactions on Visualization and Computer Graphics,   [37]  P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu,\n     26(1):1096–1106, 2020. doi: 10.1109/TVCG.2019.2934659 3                 and J. Gao. Chameleon: Plug-and-play compositional reasoning with large\n[18] B. Hoover, H. Strobelt, and S. Gehrmann. exBERT: A Visual Analysis        language models. In Proc. NeurIPS, vol. 36, pp. 43447–43478, 2023. 3\n     Tool to Explore Learned Representations in Transformer Models. In Proc.   [38]  S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model\n    ACL: System Demonstrations, pp. 187–196. ACL, Online, 2020. doi: 10.         predictions. In Proc. NeurIPS, 10 pages, p. 4768–4777, 2017. 5\n     18653/v1/2020.acl-demos.22 3                                           [39] A. Madsen, S. Reddy, and S. Chandar. Post-hoc interpretability for neural\n\nnlp: A survey. ACM Computing Surveys, 55(8), article no. 155, 42 pages,         Interactive in-context example curation for text transformation. In Proc.\n     2022. doi: 10.1145/3546577 2                                            UIST, p. 353–367. ACM, 2023. doi: 10.1145/3581641.3584059 2, 4, 5\n[40] L. McInnes, J. Healy, S. Astels, et al. hdbscan: Hierarchical density based    [60]  T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J. Cai.\n      clustering. J. Open Source Softw., 2(11):205, 2017. 5                          Promptchainer: Chaining large language model prompts through visual\n[41]  Y. Ming, S. Cao, R. Zhang, Z. Li, Y. Chen, Y. Song, and H. Qu. Understand-        programming. In Proc. CHI: Extended Abstracts, article no. 359, 10 pages.\n     ing hidden memories of recurrent neural networks. In IEEE Conference      ACM, New York, 2022. 2, 5\n     on Visual Analytics Science and Technology (VAST), pp. 13–24, 2017. doi:   [61]  T. Wu, M. Terry, and C. J. Cai. Ai chains: Transparent and controllable\n     10.1109/VAST.2017.8585721 3                                           human-ai interaction by chaining large language model prompts. In Proc.\n[42] A. Mishra, S. Rahman, H. Kim, K. Mitra, and E. Hruschka. Characterizing        CHI, p. 385. ACM, New York, 2022. doi: 10.1145/3491102.3517582 2\n      large language models as rationalizers of knowledge-intensive tasks. arXiv,   [62] W. Yang, M. Liu, Z. Wang, and S. Liu. Foundation models meet visualiza-\n     2023. doi: 10.48550/arXiv.2311.05085 2                                           tions: Challenges and opportunities. Comp.Visual Media, 2024. doi: 10.\n[43] A. Mishra, U. Soni, A. Arunkumar, J. Huang, B. C. Kwon, and C. Bryan.        1007/s41095-023-0393-x 1, 3\n     Promptaid: Prompt exploration, perturbation, testing and iteration using    [63] X. Yang, W. Wu, S. Feng, M. Wang, D. Wang, Y. Li, Q. Sun, Y. Zhang,\n      visual analytics for large language models. arXiv, 2023. doi: 10.48550/        X. Fu, and S. Poria. Mm-bigbench: Evaluating multimodal models on\n     arXiv.2304.01964 2, 4                                                  multimodal content comprehension tasks. arXiv, 2023. doi: 10.48550/\n[44]  S. Petridis, B. D. Wedin, J. Wexler, M. Pushkarna, A. Donsbach, N. Goyal,        arXiv.2310.09036 1, 3, 4, 6, 9\n     C. J. Cai, and M. Terry. Constitutionmaker: Interactively critiquing large    [64] C. Yeh, Y. Chen, A. Wu, C. Chen, F. Viegas, and M. Wattenberg. Atten-\n     language models by converting feedback into principles. In Proc. IUI, 16          tionviz: A global view of transformer attention. IEEE Transactions on\n      pages, p. 853–868. ACM, 2024. doi: 10.1145/3640543.3645144 2, 3, 6, 9         Visualization and Computer Graphics, 30(01):262–272, 2024. doi: 10.\n[45] M. Ribeiro, S. Singh, and C. Guestrin. “why should I trust you?”: Explain-       1109/TVCG.2023.3327163 3\n     ing the predictions of any classifier. In Proc. ACM SIGKDD, p. 1135–1144.   [65]  S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen. A survey on\n    ACM, New York, 2016. doi: 10.1145/2939672.2939778 5                    multimodal large language models. arXiv, 2023. doi: 10.48550/arXiv.\n[46] Z. Shao, Z. Yu, M. Wang, and J. Yu. Prompting large language models        2306.13549 1\n     with answer heuristics for knowledge-based visual question answering. In    [66] H. Yu, P. P. Liang, R. Salakhutdinov, and L.-P. Morency. Mixture of multi-\n     Proc. CVPR, pp. 14974–14983, 2023. 3                                 modal interaction experts. In UniReps: the First Workshop on Unifying\n[47] H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush. Lstmvis: A tool         Representations in Neural Models, 2023. 5\n      for visual analysis of hidden state dynamics in recurrent neural networks.   [67]  J. Yuan, C. Chen, W. Yang, M. Liu, J. Xia, and S. Liu. A survey of visual\n    IEEE Transactions on Visualization and Computer Graphics, 24(1):667–         analytics techniques for machine learning. Computational Visual Media,\n     676, 2018. doi: 10.1109/TVCG.2017.2744158 3                                  7(1):2, 2021. doi: 10.1007/s41095-020-0191-7 3\n[48] H. Strobelt, A. Webson, V. Sanh, B. Hoover, J. Beyer, H. Pfister, and A. M.   [68] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang,\n     Rush. Interactive and visual prompt engineering for ad-hoc task adaptation       W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal\n     with large language models. IEEE Transactions on Visualization and         understanding and reasoning benchmark for expert agi. In Proc. CVPR,\n    Computer Graphics, 29(1):1146–1156, 2023. doi: 10.1109/TVCG.2022.         pp. 9556–9567, 2024. 3\n     3209479 2                                                               [69]  J. Zamfirescu-Pereira, R. Y. Wong, B. Hartmann, and Q. Yang. Why johnny\n[49]  J. Sun, C. Zheng, E. Xie, Z. Liu, R. Chu, J. Qiu, J. Xu, M. Ding, H. Li,         can’t prompt: How non-ai experts try (and fail) to design llm prompts. In\n    M. Geng, et al. A survey of reasoning with foundation models. arXiv,         Proc. CHI, article no. 437. ACM, 2023. doi: 10.1145/3544548.3581388 2\n     2023. doi: 10.48550/arXiv.2312.11562 1, 2                               [70] A. Zeng, M. Attarian, brian ichter, K. M. Choromanski, A. Wong,\n[50]  I. Tenney, J. Wexler, J. Bastings, T. Bolukbasi, A. Coenen, S. Gehrmann,         S. Welker, F. Tombari, A. Purohit, M. S. Ryoo, V. Sindhwani, J. Lee,\n     E. Jiang, M. Pushkarna, C. Radebaugh, E. Reif, and A. Yuan. The language         V. Vanhoucke, and P. Florence. Socratic models: Composing zero-shot\n      interpretability tool: Extensible, interactive visualizations and analysis        multimodal reasoning with language. In ICLR, 2023. 1, 3\n      for NLP models. In Proc. EMNLP: System Demonstrations, pp. 107–118.   [71]  T. Zhang, A. Madaan, L. Gao, S. Zhang, S. Mishra, Y. Yang, N. Tandon,\n    ACL, Online, 2020. doi: 10.18653/v1/2020.emnlp-demos.15 3                and U. Alon. In-context principle learning from mistakes. In ICML 2024\n[51] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,       Workshop on In-Context Learning, 2024. 3, 6, 9\n     B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and    [72] X. Zhang, J. P. Ono, H. Song, L. Gou, K.-L. Ma, and L. Ren. Sliceteller:\n      efficient foundation language models. arXiv, 2023. doi: 10.48550/arXiv.     A data slice-driven approach for machine learning model validation. IEEE\n     2302.13971 2, 4                                                            Transactions on Visualization and Computer Graphics, 29(1):842–852,\n[52] X. Wang, J. He, Z. Jin, M. Yang, Y. Wang, and H. Qu. M2lens: Visu-        2023. doi: 10.1109/TVCG.2022.3209465 3\n      alizing and explaining multimodal models for sentiment analysis. IEEE    [73] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\n     Transactions on Visualization and Computer Graphics, 28(1):802–812,        and M. Du. Explainability for large language models: A survey. ACM\n     2022. doi: 10.1109/TVCG.2021.3114794 3, 5                                Transactions on Intelligent Systems and Technology, 15(2):1–38, 2024.\n[53] X. Wang, R. Huang, Z. Jin, T. Fang, and H. Qu. Commonsensevis: Visual-         doi: 10.1145/3639372 2\n      izing and understanding commonsense reasoning capabilities of natural    [74] G. Zheng, B. Yang, J. Tang, H.-Y. Zhou, and S. Yang. Ddcot: Duty-distinct\n     language models. IEEE Transactions on Visualization and Computer         chain-of-thought prompting for multimodal reasoning in language models.\n     Graphics, 30(01):273–283, 2024. doi: 10.1109/TVCG.2023.3327153 3          In Proc. NeurIPS, vol. 36, pp. 5168–5191, 2023. 3\n[54]  Y. Wang, S. Shen, and B. Y. Lim. Reprompt: Automatic prompt editing to\n      refine ai-generative art towards precise expressions. In Proc. CHI, article\n     no. 22, p. 29. ACM, New York, 2023. doi: 10.1145/3544548.3581402 2\n[55] Z. Wang, M. Li, R. Xu, L. Zhou, J. Lei, X. Lin, S. Wang, Z. Yang, C. Zhu,\n     D. Hoiem, S.-F. Chang, M. Bansal, and H. Ji. Language models with\n     image descriptors are strong few-shot video-language learners. In Proc.\n     NeurIPS, vol. 35, pp. 8483–8497, 2022. 1, 3\n[56] Z. J. Wang, R. Turko, and D. H. Chau. Dodrio: Exploring transformer\n     models with interactive visualization. In Proc.ACL: System Demonstra-\n      tions, pp. 132–141. ACL, Online, 2021. doi: 10.18653/v1/2021.acl-demo.\n    16 3\n[57]  J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V.\n     Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large\n     language models. In Proc. NeurIPS, vol. 35, pp. 24824–24837, 2022. 2, 5\n[58]  J. Wexler, M. Pushkarna, T. Bolukbasi, M. Wattenberg, F. Viégas, and\n       J. Wilson. The what-if tool: Interactive probing of machine learning\n     models. IEEE Transactions on Visualization and Computer Graphics,\n     26(1):56–65, 2020. doi: 10.1109/TVCG.2019.2934619 3\n[59]  S. Wu, H. Shen, D. S. Weld, J. Heer, and M. T. Ribeiro.  Scattershot:",
"headers": [
"arXiv:2406.03843v3  [cs.HC]  30 Sep 2024",
"POEM: Interactive Prompt Optimization for Enhancing Multimodal",
"Reasoning of Large Language Models"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2406.03843v3.pdf"
}