{
"text": "Benchmarking Uncertainty Metrics for LLM\n                         Prompt Optimization\n\n\n                                       Pei-Fu Guo1,2, Yun-Da Tsai1,3, and Shou-De Lin1,4\n\n                                                                 1 National Taiwan University\n                                                                2 r12922217@csie.ntu.edu.tw\n                                                                3 f08946007@csie.ntu.edu.tw\n                                                                   4 sdlin@csie.ntu.edu.tw2024\n\n\n                               Abstract. Prompting methods for LLMs like Chain of Thought (CoT)Dec                         and Tree of Thought (ToT) improve reasoning through single- or multi-\n25                               stepsuch processes.as Monte CarloThese TreemethodsSearchcan(MCTS)be enhancedand Banditby searchmethods,algorithms,which\n                                        rely on accurate uncertainty estimation. However, current uncertainty\n                                    metrics for text generation—based on token-level likelihoods or verbal-\n                                      ized confidence—primarily focus on output variability and do not align\n                                  with the needs of optimization.\n                                    In this work, we first highlight the different requirements for uncertainty\n                                    metrics in prompt optimization versus text generation. We outline four[cs.LG]                                  key uncertainties—Answer, Correctness, Aleatoric, and Epistemic—that\n                                     are beneficial for prompt optimization and introduce a novel benchmark-\n                                     ing pipeline to evaluate how well current NLG uncertainty metrics esti-\n                              mate these target uncertainties.\n                             Our experiments using GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct\n                               on two reasoning datasets reveal a significant limitation in current un-\n                                      certainty metrics, as they predominantly capture Answer Uncertainty\n                                 but fail to effectively measure other types of uncertainty. This gap high-\n                                          lights the need for a broader range of optimization-aware uncertainty\n                                     estimators to effectively guide search in prompt optimization tasks with\n                                         different objectives. Our code and data are available at github link.\n\n                            Keywords: Uncertainty Metric Evaluation · Prompt Optimization\n\n\n                  1  IntroductionarXiv:2409.10044v2\n\n                        Prompting methods for large language models (LLMs) have gained significant\n                            attention for their ability to enhance reasoning capabilities through multi-step\n                             processes, such as Chain of Thought (CoT)[12], Tree of Thought(ToT)[15], and\n                           ReAct[16]. These approaches can be extended by incorporating search algo-\n                          rithms to optimize prompts, utilizing techniques like Monte Carlo Tree Search\n                       (LATS, STaR)[18,17], bandit algorithms (LongPO)[7], and gradient-style search\n                         (OPRO)[14]. A key element in these search and optimization algorithms is uncer-\n                             tainty estimation, which is vital for guiding decisions, balancing exploration and\n\n2      Guo. et al.\n\n\nexploitation, and improving algorithm efficiency. Uncertainty estimation tech-\nniques, such as those used in Bandit algorithms or Bayesian optimization, can\ndynamically adjust learning rates or hyperparameters. In combinatorial opti-\nmization (e.g., genetic algorithms or simulated annealing), uncertainty estima-\ntion informs heuristic decisions like mutation rates or temperature adjustments.\nHence, developing robust methods to quantify uncertainty in LLMs, particularly\nfor prompt optimization, is essential.\n   Previous approaches to measuring uncertainty in LLMs primarily rely on\ntoken-level or sentence-level generation likelihoods, often represented by metrics\nlike token disparity probability [11], predictive entropy [1] and reciprocal of per-\nplexity [3]. These techniques have been used for bias calibration [19], controllable\ndecoding [20], and LLM planning [9]. However, we argue that such token-level\nor sentence-level uncertainty measurements are more indicative of model output\nconfidence or output diversity, which may not align with the needs of prompt\noptimization tasks. In these contexts, uncertainty estimation should guide the\nsearch process itself. For example, in tree-based reasoning, the uncertainty at\neach node should help steer the search direction in line with the search objec-\ntives rather than simply reflecting model confidence or output variability.\n   Figure1a justify our hypothesis and illustrates the relationship between LLM\ncorrectness uncertainty, answer uncertainty (see section  2), and response accu-\nracy in the GSM8K dataset. In prompt optimization tasks focused on searching\ncorrect answer, a reliable uncertainty metric targeting this objective should ex-\nhibits 50% response accuracy(correct/wrong) when its value is at its highest.\nThis objective conflicts with answer uncertainty, which is designed to measure\ndiversity of responses but may reflect an incorrect majority answer.\n   In this work, we first highlight the differing requirements for uncertainty\nmetrics in prompt optimization versus text generation. We outline four key un-\ncertainties that are beneficial to prompt optimization—Answer, Correctness,\nAleatoric, Epistemic and propose a novel benchmarking pipeline designed to\nevaluate the effectiveness of current NLG uncertainty metrics in prompt opti-\nmization setting. By performing extensive sampling on LLMs, our pipeline con-\nstruct large, tree-structured reasoning traces from model outputs. Once these\ntraces are built, we can compute accurate estimation of the uncertainties, which\ncan serve as ground truth values for comparison with metric predictions. Our\nevaluation shows that current uncertainty metrics mainly capture Answer Un-\ncertainty and fail to measure other uncertainty types, emphasizing the need for\nmore diverse, optimization-aware estimators to guide prompt optimization for\ndifferent objectives.\n\n\n2  Different Uncertainties for Prompt Optimization\n\nUncertainty can arise from various aspects and express in different forms. For\ninstance, uncertainty about the input question might reflect ambiguity or lack\nof clarity in the prompt itself, whereas uncertainty about the output answer\nconcerns the model fidelity or diversity of the responses generated. Each type\n\nBenchmarking Uncertainty Metrics for LLM Prompt Optimization      3\n\n\n\n\n\n(a) Relationship between correctness un-   (b)  Overview   of  our  benchmarking\ncertainty,  answer  uncertainty  and  re-   pipeline. Detail of each step is shown at\nsponse accuracy. Each spot  is a node  Algorithm  1.\nwithin reasoning traces. See section  4.\n\n\n\nof uncertainty represents different dimensions of the problem and can guide the\noptimization process in unique ways. In this section, we outline four types of\nuncertainty, each playing a distinct role and offering unique benefits for prompt\noptimization across tasks.\n  Answer Uncertainty (AnsU) reflects the model’s confidence and the di-\nversity of possible answers. AnsU describes how consistently the model produces\nthe same answer after repeated sampling, but it does not guarantee correctness.\nIf the model lacks necessary knowledge to answer the given question, it is rea-\nsonable for the output answers to be incorrect, even if low AnsU is observed\nafter repeated sampling.\n  AnsU is measured as the entropy of the output answer distribution.\n                 AnsU(x) = − X p(yi|x) log p(yi|x)\n                                                          i\n\nwhere p(yi|x) is the probability of output answer yi obtain from {xj} given the\ninput x. AnsU can guide prompt algorithms to explore a richer solution space,\nincreasing the variability of generated content. This can be particularly beneficial\nfor tasks like creative writing, idea generation and open-ended problem-solving,\nwhere diversity and originality are highly valued.\n   Correctness Uncertainty (CU) provides insights into the likelihood of\nanswer correctness. For example, in a medical diagnosis system, high CU in-\ndicates that the model’s prediction may be unreliable, suggesting the need for\nadditional verification or consultation. Note that this differs from AnsU; CU is\ndirectly related to the accuracy of the diagnosis. When CU is low, the model’s\npredictions are less likely to include a majority of false positives.\n  CU is calculated as the entropy of the output correctness distribution.\n                 CU(x) = − X p(ci|x) log p(ci|x)\n                                                         i\n\n4      Guo. et al.\n\nwhere p(ci|x) represents the probability of correctness ci (whether an answer is\ncorrect or incorrect) obtain from {xj} given the input x. CU can guide prompt\nalgorithms to effectively narrow down the solution space and acquire the correct\nanswer. In question-answering tasks with a ground truth answer, CU helps the\nmodel focus on more accurate responses, reducing the likelihood of incorrect or\nirrelevant answers.\n   Aleatoric Uncertainty (AU) and Epistemic Uncertainty (EU) are two\ndistinct sources of model uncertainty. AU arises from the inherent vagueness or\nnoise in the data itself. For example,  if asked \"What time is the meeting?\",\neven  if the LLM has access to a detailed schedule, it may still be uncertain\nbecause the question doesn’t specify which meeting is being referred to, making\nit inherently ambiguous. EU, on the other hand, originates from the model’s\nlimitations and is related to its knowledge and understanding. This uncertainty\ncan be reduced by training the model with more data or refining its algorithms.\nFor instance, a language model trained on a limited dataset might show high\nEU in underrepresented domains or languages.\n  AU and EU are calculated using the Deep-Ensemble-Decomposition method\n[2], where total model uncertainty is the sum of AU and EU. In our context,\nθ, which represents model parameters in the original paper, corresponds to the\nperturbed question in our setting. EU captures the disagreement between differ-\nent perturbations, measured by the mutual information I(Y ; θ|X). AU reflects\nthe inherent data noise and is represented as Eq(θ|D)[H(q(Y |X, θ))], where the\nexpectation is taken over the perturbed questions θ.\n   The total model uncertainty is expressed as :\n\n               H(q(Y |X)) = I(Y ; θ|X) + Eq(θ|D)[H(q(Y |X, θ))]\n                                 |  EU{z   }   |      AU{z         }\n\nThe advantage of AU and EU is that they explain the underlying causes of\nuncertainty(due to data noise or model limitations). This understanding enables\nmore targeted improvements during prompt optimization, such as rephrasing the\nquestion or providing additional few-shot examples.\n\n\n3  Current NLG Uncertainty Metrics\n\n\nCurrent NLG uncertainty metrics measure the uncertainty in a model’s output\nand aim to improve correctness by favoring answers with lower uncertainty, under\nthe assumption that more model confident answers are more likely to be correct.\nIn this section, we outline four commonly used black-box metrics, primarily based\non decoding probabilities and verbalized confidence. We further evaluate their\neffectiveness in quantifying different uncertainties through our benchmarking\npipeline in section 4.\n   Normalized Predictive Entropy (NPE)[1] measures the uncertainty of\ngenerated text by calculating the average entropy of possible output sequences\ngiven a input context x.\n\nBenchmarking Uncertainty Metrics for LLM Prompt Optimization      5\n\n\n\n                               1\n                  NPE(x) = X X ln p(si|s<i)\n                   N\n                                  n    i\n\nwhere N is the number of generations and si is the i-th token of sentence s.\n   Length-Normalized Predictive Entropy (LNPE)[1] adjusts for sen-\ntence length by normalizing the entropy with the number of tokens. This ensures\nfair comparison across different sentence lengths Sn .\n                            1       1\n              LNPE(x) = X  X ln p(si|s<i)\n                 N     Sn                                n               i\n  TopK-Token Disparity (Top-DISP) is based on the concept introduced\nby [13], which suggests that a larger difference between the top-1 and top-2\ntoken probabilities correlates with higher confidence in the model answer. The\nmetric calculates the average difference in probability between the top-1 and\ntop-2 tokens for each token within the output sequence and further averages\nthem across multiple outputs.\n\n                                       1           p(si,top1|s<i)              Top-DISP(x) = −1 X  X ln\n                   N     Sn          p(si,top2|s<i)\n                                  n               i\n\n   Intra-Sample Similarity (Intra)[5] computes the average of the uncer-\ntainties discerned individually for each sample output. Following the approach\nused in SPUQ, we utilize verbalized uncertainty method [10] to obtain the uncer-\ntainty articulated by the LLM for each perturbed input and output pair c(xi, yi).\n\n                       Pki=0 c(xi, yi)                          Intra(x) = −\n                                     k + 1\n\n4  Benchmarking Pipeline\n\nA reliable uncertainty metric should serve as an accurate estimator of its target\nuncertainty. In this study, we introduce a novel benchmarking pipeline designed\nto assess the effectiveness of uncertainty metrics in estimating target uncertain-\nties within the context of prompt optimization. Our pipeline focuses on evaluat-\ning metrics in tree-structured reasoning traces, which represent a predominant\napproach in current prompting algorithms. The following sections outline the\nconceptual foundation and steps of our pipeline.\n\n\n4.1  Design Concept\n\nTo evaluate how well a metric quantifies its target uncertainty in prompt op-\ntimization, we first need to establish the ground truth values for the target\nuncertainty at each reasoning step. This requires constructing a comprehensive\nset of reasoning traces for input questions.\n\n6      Guo. et al.\n\n\n  We generate large tree-structured reasoning traces for each question by per-\nturbing input prompts and sampling outputs multiple times in each node of\ntraces. This servies two key purposes. First, these traces align with prompt op-\ntimization algorithms, as they emulate how such algorithms explore possible\nreasoning paths, making them highly relevant for evaluating metrics in realis-\ntic scenarios. Second, they enable a thorough exploration of the solution space\nby considering diverse reasoning paths. This approach allows the tree-structured\ntraces to approximate the complete solution space, facilitating robust estimation\nof target uncertainty ground truth values using Monte Carlo methods.\n   After obtaining the ground truth values, we calculate the metric estimates\nat each reasoning node based on the formula in section  3. We then evaluate the\nalignment between uncertainty metrics and ground truth values using statisti-\ncal methods. This involves measuring the correlation, bias, and variance of the\nmetric estimates relative to the ground truth. Figure 1b shows an overview of\nour benchmarking pipeline.\n\n\n4.2  Detailed Workflow\n\nGiven a dataset of questions, our pipeline builds a reasoning tree for each ques-\ntion, ultimately producing a large number of (Umetric, Utrue) pairs. Algorithm 1\noutlines the process in detail. Increasing M and K improves the approximation\nto the underlying solution space, yielding more accurate ground truth values and\nconsequently improving the quality of the evaluation.\n\n\n5  Experiments\n\nIn this section, we use our benchmarking pipeline to evaluate the uncertainty\nmetrics introduced in Section 3 against the target uncertainty defined in Sec-\ntion 2. Our evaluation focuses on the correlation map (see 2) and visualization\nplots (see B) between uncertainty metrics and ground truth uncertainty values.\nBy analyzing these relationships, we can determine which metrics serve as better\nestimators of the four target uncertainties, guiding prompt optimization more\neffectively. Prompt templates are shown in A.\n\n\n5.1  Dataset and LLMs\n\nWe conduct experiments on two reasoning datasets: GSM8K [4] and Strate-\ngyQA [6], which involve solving math problems and complex strategic reasoning,\nrespectively. We selected these datasets because they provide exact ground truth\nanswers and encompass diverse types of knowledge and problem-solving tasks.\nWhile both are reasoning datasets, GSM8K features an infinite range of pos-\nsible answers, whereas StrategyQA is constrained to binary answers: \"true\" or\n\"false\". We use two large language models, GPT-3.5-Turbo[8] and Meta-Llama-\n3.1-8B-Instruct, to showcase that our benchmarking pipeline is suitable for both\ncommercial and open-source LLMs of varying sizes.\n\nBenchmarking Uncertainty Metrics for LLM Prompt Optimization      7\n\n\n Algorithm 1: Benchmarking Pipeline Workflow\n\n   foreach input question Q do\n         Initialize a reasoning tree with a root node containing Q;\n      while the reasoning tree is not fully constructed do\n          foreach node in the tree that has not yet terminated do\n            Step 1: Input Perturbation\n             Generate M rephrased inputs {xj}Mj=1 from the current node’s\n               input x;\n            Step 2: Random Sampling\n              For each rephrased input xj, sample K responses {yjk}Kk=1;\n         Expand the tree by adding child nodes using the newly generated\n             responses;\n\n      Step 3: Ground Truth Uncertainty Calculation\n       For each node, calculate the ground truth uncertainty using the answers in\n           its subtree’s leaves, as described in Section 2;\n      Step 4: Uncertainty Metric Calculation\n       For each node, compute the estimated uncertainty metric based on its own\n        input and output, following the formulas in Section 3;\n\n   Step 5: Statistical Analysis\n    Collect all (Umetric, Utrue) pairs from every node across all trees.;\n   Compute Corr(Umetric, Utrue) and visualize these relationships to assess how\n     well the estimated metrics align with the ground truth.;\n\n\n\n5.2  Results and Analysis\n\nFigure 2 presents correlation maps illustrating the relationships between uncer-\ntainty metrics and target uncertainties. Each map is divided into three sections:\nthe upper-left shows correlations among the uncertainty metrics, the upper-right\nshows how well each metric aligns with the target uncertainties, and the lower-\nright shows correlations among the target uncertainties themselves. A strong\ncorrelation between a metric and a target uncertainty indicates that the metric\nserves as an effective estimator, which can be used to guide search in prompting\nalgorithms. Below, we summarize key findings:\n\n\n 1. Current metrics estimate AnsU well but struggle with CU. As\n   shown in Figure 2, the correlation maps demonstrate that evaluated met-\n     rics are more correlated with AnsU and AU. As for CU, we can see zero or\n    negative correlation in all the maps. This implies most uncertainty metrics\n   predominantly capture uncertainty of answer diversity and model confidence,\n   but fail to estimate the correctness, which is important in prompting algo-\n    rithms targeting answer correctness.\n 2. Uncertainty metrics show strong inter-correlation. The upper-left\n    section of each map shows that token-likelihood-based metrics (NPE, LNPE,\n   Top-DISP) are highly correlated, while INTRA, a verbalized confidence met-\n     ric, exhibits little correlation with them. This high correlation indicates that\n\n8      Guo. et al.\n\n\n\n\n\nFig. 2: Correlation Maps of uncertainty metrics and target uncertainty on differ-\nent datasets and models.\n\n\n\n    these estimators capture similar uncertainties, highlighting a lack of diversity\n    in metrics for other uncertainty types, such as CU.\n 3. AnsU’s weak link to CU reveals fundamental differences of the two.\n   The lower-right section of the correlation maps shows a strong relationship\n   between AnsU, AU, and EU, suggesting that these uncertainties can po-\n    tentially share common estimators in prompting algorithms. However, CU\n    displays a much weaker or even negative correlation with AnsU, especially\n    in the GSM8K dataset when solution space are really large. This highlights\n   a key distinction: AnsU reflects answer diversity, CU measures answer cor-\n    rectness. Metrics designed to estimate AnsU are not effective for capturing\n   CU. Therefore, tailored estimators for correctness uncertainty are needed to\n    better support tasks focused on accuracy.\n\n\n6  Conclusion\n\nIn this work, we emphasize the different requirements for uncertainty metrics in\nprompt optimization versus text generation. We propose a novel benchmarking\npipeline to assess how well current NLG uncertainty metrics estimate various\ntypes of uncertainty in prompt optimization settings. Through experiments on\ntwo datasets and multiple LLMs, we find that most NLG uncertainty metrics\n\nBenchmarking Uncertainty Metrics for LLM Prompt Optimization      9\n\n\nprimarily capture uncertainty related to answer diversity or model confidence,\nbut fail to estimate uncertainty related to correctness, which is an essential factor\nfor prompting algorithms targeting accurate answers. This gap underscores the\nneed for optimization-aware uncertainty metrics that can better guide prompt\noptimization in LLMs.\n\n\nReferences\n\n 1. Andrey Malinin, M.G.: Uncertainty estimation in autoregressive structured pre-\n    diction arXiv:2002.07650 (2020), http://arxiv.org/abs/2002.07650, version\n    5\n 2. Balaji Lakshminarayanan, Alexander Pritzel, C.B.: Simple and scalable predictive\n    uncertainty estimation using deep ensembles arXiv:1612.01474 (2016), http:\n    //arxiv.org/abs/1612.01474, version 3\n 3. Chen, S.F., Beeferman, D., Rosenfeld, R.: Evaluation metrics for language models\n    (1998)\n 4. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,\n    M., Tworek, J., Hilton, J., Nakano, R., et al.: Training verifiers to solve math word\n    problems. arXiv preprint arXiv:2110.14168 (2021)\n 5. Gao, X., Zhang, J., Mouatadid, L., Das, K.: Spuq: Perturbation-based uncer-\n    tainty quantification for large language models arXiv:2403.02509 (2024), http:\n    //arxiv.org/abs/2403.02509, version 1\n 6. Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., Berant, J.: Did aristotle\n    use a laptop? a question answering benchmark with implicit reasoning strategies.\n    Transactions of the Association for Computational Linguistics 9, 346–361 (2021)\n 7. Hsieh, C.J., Si, S., Yu, F.X., Dhillon, I.S.: Automatic engineering of long prompts.\n    arXiv preprint arXiv:2311.10117 (2023)\n 8. OpenAI: Openai models api (2023), https:platform.openai.com/docs/models\n 9. Ren, A.Z., Dixit, A., Bodrova, A., Singh, S., Tu, S., Brown, N., Xu, P., Takayama,\n     L., Xia, F., Varley, J., et al.: Robots that ask for help: Uncertainty alignment for\n    large language model planners. arXiv preprint arXiv:2307.01928 (2023)\n10. Stephanie Lin, Jacob Hilton, O.E.: Teaching models to express their uncertainty in\n    words. arXiv:2205.14334 (2022), http://arxiv.org/abs/2205.14334, version 2\n11. Wang,  X., Zhou,  D.: Chain-of-thought  reasoning without prompting. arXiv\n    preprint arXiv:2402.10200 (2024)\n12. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\n    D., et al.: Chain-of-thought prompting elicits reasoning in large language models.\n    Advances in neural information processing systems 35, 24824–24837 (2022)\n13. Xuezhi  Wang,   D.Z.:   Chain-of-thought   reasoning   without   prompting\n   arXiv:2402.10200 (2024), http://arxiv.org/abs/2402.10200, version 2\n14. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.V., Zhou, D., Chen, X.: Large\n    language models as optimizers. ArXiv abs/2309.03409 (2023), https://api.\n    semanticscholar.org/CorpusID:261582296\n15. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., Narasimhan, K.: Tree\n     of thoughts: Deliberate problem solving with large language models. Advances in\n    Neural Information Processing Systems 36 (2024)\n16. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y.: React: Syn-\n    ergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629\n    (2022)\n\n10     Guo. et al.\n\n\n17. Zelikman, E., Wu, Y., Mu, J., Goodman, N.: Star: Bootstrapping reasoning with\n    reasoning. Advances in Neural Information Processing Systems 35, 15476–15488\n    (2022)\n18. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., Wang, Y.X.: Language\n    agent tree search unifies reasoning acting and planning in language models. arXiv\n    preprint arXiv:2310.04406 (2023)\n19. Zhou, H., Wan, X., Proleev, L., Mincu, D., Chen, J., Heller, K., Roy, S.: Batch\n    calibration: Rethinking calibration for in-context learning and prompt engineering.\n    arXiv preprint arXiv:2309.17249 (2023)\n20. Zhu, Y., Li, J., Li, G., Zhao, Y., Jin, Z., Mei, H.: Hot or cold? adaptive temperature\n    sampling for code generation with large language models. In: Proceedings of the\n   AAAI Conference on Artificial Intelligence. vol. 38, pp. 437–445 (2024)\n\n\nA  Prompt Templates\n\n\n\n\n You will receive a question and your goal is to generate a new version of it that\n convey the same meaning as the original.\n Q1.Original Question: Would a dog respond to bell before Grey seal?\n New-Version: Would a dog react to a bell sooner than a grey seal?\n Q2.Original Question: The perimeter of a rectangle is the sum of all its sides.\n New-Version: A rectangle’s perimeter is obtained by summing the lengths of its\n  sides.\n Q3. Original Question: <Question>\n New-Version:\n\n\n               Fig. 3: Example prompt for question perturbation.\n\n\n\n\n\n Q: Was ethanol beneficial to Jack Kerouac’s health?\n A: Jack Kerouac died from internal bleeding due to long-term alcohol abuse.\n Thus, ethanol was not beneficial to Jack Kerouac’s health. So the answer is no.\n Q: If Goofy were a pet, would he need heartworm prevention?\n A: Goofy is a dog, and dogs require regular heartworm prevention. Thus, if Goofy\n were a pet, he would need heartworm prevention. So the answer is yes.\n Q : <Question>\n A :\n\n\nFig. 4: Example prompt for StrategyQA. We random pick 4 few shot samples\nfrom pool.\n\nBenchmarking Uncertainty Metrics for LLM Prompt Optimization     11\n\n\n\n\n\n Question 1: Mark has a garden with flowers. He planted plants of three different\n  colors in it. Ten of them are yellow, and there are 80% more of those in purple.\n There are only 25% as many green flowers as there are yellow and purple flowers.\n How many flowers does Mark have in his garden?\n Answer : There are 80% more purple flowers than yellow flowers, so there are 10\n * 1.8 = 18 purple flowers. There are 10 yellow flowers and 18 purple flowers, so\n  there are 10 + 18 = 28 yellow and purple flowers. There are 25% as many green\n  flowers as there are yellow and purple flowers, so there are 28 * 0.25 = 7 green\n  flowers. Mark has 10 yellow flowers, 18 purple flowers, and 7 green flowers, so he\n has 10 + 18 + 7 = 35 flowers in his garden. The answer to the question is 35.\n Question 2: Albert is wondering how much pizza he can eat in one day. He buys 2\n  large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza has 8\n  slices. If he eats it all, how many pieces does he eat that day?\n Answer : He buys 2 large pizzas, so he has 2 * 16 = 32 slices. He buys 2 small piz-\n  zas, so he has 2 * 8 = 16 slices. There are 32 slices from the large pizzas and 16\n  slices from the small pizzas, so he eats 32 + 16 = 48 pieces that day. The answer\n  to the question is 48.\n Question 3: <Question>\n Answer :\n\n\nFig. 5: Example prompt for GSM8K. We random pick 4 few shot samples from\npool.\n\n12     Guo. et al.\n\nB  Additional Results\n\n\n\n\n\nFig. 6: Scatter plots show the evaluation results of metrics on Llama (StrategyQA\nand GSM8K) and GPT-3.5-Turbo (GSM8K and StrategyQA), with each point\nrepresenting a reasoning node. The plots illustrate the relationship between CU,\nuncertainty metrics, and response accuracy. As shown, most metrics fail to es-\ntimate CU effectively, as there is no clear trend of higher metric values(x-axis)\ncorresponding to higher correctness uncertainty(y-axis).",
"headers": [
"arXiv:2409.10044v2  [cs.LG]  25 Dec 2024",
"Benchmarking Uncertainty Metrics for LLM",
"Prompt Optimization",
"1",
"Introduction",
"2",
"Different Uncertainties for Prompt Optimization",
"3",
"Current NLG Uncertainty Metrics",
"4",
"Benchmarking Pipeline",
"5",
"Experiments",
"6",
"Conclusion",
"References",
"A",
"Prompt Templates",
"B",
"Additional Results"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2409.10044v2.pdf"
}