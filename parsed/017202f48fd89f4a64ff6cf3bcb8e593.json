{
"text": "Automating Personalization: Prompt Optimization for\n                  Recommendation Reranking\n\n            Chen Wangâˆ—          Mingdai Yangâˆ—          Zhiwei Liuâ€              Pan Li\n                University of Illinois     The University of Chicago      Salesforce AI Research           Georgia Tech\n                    Chicago              Chicago, Illinois, USA      Palo Alto, California, USA      Atlanta, Georgia, USA\n               Chicago, Illinois, USA      frankyang@uchicago.edu    zhiweiliu@salesforce.com    pan.li@scheller.gatech.edu\n               cwang266@uic.edu\n\n                           Linsey Pang          Qingsong Wen             Philip Yu\n                               Salesforce AI Research         Squirrel Ai Learning     The University of Chicago\n                           Palo Alto, California, USA          California, USA           Chicago, Illinois, USA\n                          panglinsey@gmail.com     qingsongedu@gmail.com         psyu@uic.edu2025\n      ABSTRACT                                                    Recommendation Reranking. In Proceedings of Make sure to enter the correct\n                                                                                            conference title from your rights confirmation emai (SIGIRâ€™25). ACM, New         Modern recommender systems increasingly leverage large language\n                                                                                           York, NY, USA, 5 pages. https://doi.org/XXXXXXX.XXXXXXX\n          models (LLMs) for reranking to improve personalization. However,Apr\n            existing approaches face two key limitations: (1) heavy reliance on\n4    manually crafted prompts that are difficult to scale, and (2) inad-\n          equate handling of unstructured item metadata that complicates\n                                                       1  INTRODUCTION           preference inference. We present AGP (Auto-Guided Prompt Re-\n           finement), a novel framework that automatically optimizes user      Reranking, which refines initial recommendations to better align\n             profile generation prompts for personalized reranking. AGP intro-     with user preferences, plays a pivotal role in improving recom-\n          duces two key innovations: (1) position-aware feedback mecha-     mendation quality [8, 9, 12]. Recent advancements in large lan-[cs.IR]          nisms for precise ranking correction, and (2) batched training with     guage models (LLMs) have shown promise for reranking tasks\n           aggregated feedback to enhance generalization. Extensive experi-     by capturing complex user interests via contextual understand-\n          ments across three diverse datasets (Amazon Movies & TV, Yelp, and      ing [1, 11, 15, 16].\n           Goodreads) demonstrate AGPâ€™s effectiveness, achieving improve-        However, their effectiveness relies heavily on manually crafting\n         ments of 5.61â€“20.68% in NDCG@10 over baseline models with     prompts â€“ a labor-intensive process that demands significant do-\n             just 100 training users. Our results show AGPâ€™s particular strength     main expertise and limits scalability. Moreover, manually designed\n            in enhancing graph-based recommenders (9.36â€“20.68% gains for     prompts struggle to address the complexity and diversity of user\n          LightGCN) while maintaining strong performance with sequential       preferences. For instance, crafting prompts to capture nuanced user\n           models.                                                                     interests from user-item interactions, such as item titles or descrip-\n                                                                                          tions, often requires iterative trial-and-error. This process is not\n      CCS CONCEPTS                                              only time-consuming but also prone to suboptimal results due to its\n                                                                                       reliance on intuition rather than systematic optimization. Moreover,              â€¢ Information systems â†’Recommender systems.\n                                                                                              static prompts fail to adapt to dynamic datasets and evolving user\n      KEYWORDS                                                      behaviors, limiting their ability to deliver personalized recommen-\n                                                                                dations at scale.\n         Prompt Optimization, Recommender Systems, Large Language\n                                                                                   Existing research on prompt optimization largely focuses on\n          Models (LLMs), Collaborative Filtering, RerankingarXiv:2504.03965v1                                                                          tasks like question answering [13], mathematical reasoning [14],\n       ACM Reference Format:                                       and news recommendation [10]. RecPrompt [10] introduces a self-\n          Chen Wang, Mingdai Yang, Zhiwei Liu, Pan Li, Linsey Pang, Qingsong Wen,      tuning prompting framework incorporating TopicScore to enhance\n           and Philip Yu. 2025. Automating Personalization: Prompt Optimization for       explainability in news recommendations. However, this approach\n              âˆ—Equal Contributions                                                             relies on structured content and topical consistency, making it\n             â€ Corresponding author                                                          less applicable to heterogeneous item recommendation scenarios,\n                                                                   where item metadata can be inconsistent, unstructured, and user-\n             Permission to make digital or hard copies of all or part of this work for personal or\n              classroom use is granted without fee provided that copies are not made or distributed      generated. Current optimization methods [4, 10] usually rely on\n                for profit or commercial advantage and that copies bear this notice and the full citation      aggregated ranking metrics like AUC or NDCG, which are useful\n            on the first page. Copyrights for components of this work owned by others than ACM       for performance evaluation but insufficient for direct optimization\n            must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n               to post on servers or to redistribute to lists, requires prior specific permission and/or a      guidance. Input reranking approaches [2] reorder items based on\n                 fee. Request permissions from permissions@acm.org.                             relevance and exposure but do not offer structured feedback to re-\n               SIGIRâ€™25, July 13â€“18, 2025, Padova, Italy                                             fine user preference modeling. A more interpretable and systematic\n         Â© 2025 ACM.\n         ACM ISBN 978-1-4503-XXXX-X/18/06                                          strategy is needed to close the gap between LLM-based reranking\n            https://doi.org/XXXXXXX.XXXXXXX                                   and explicit user feedback.\n\nSIGIRâ€™25, July 13â€“18, 2025, Padova, Italy                                                                                        Chen Wang et al.\n\n\n  To address these challenges, in this paper we propose AGP:\n                                                                                  Batched Process\nAuto-Guided Prompt Refinement for Personalized Rerank-\ning, a novel framework that optimizes user profile generation prompts                                 User Poï¬le Generation                  Rerank\nrather than directly modifying reranking prompts. By refining user\nprofiles, AGP enables LLMs to better capture personalized interests,             User Interaction History with text                 User Proï¬le                Reranked List\nleading to more effective and explainable reranking. AGP improves\noptimization through a batched training with batched feedback\nmechanism. Instead of refining prompts on a per-user basis, AGP            User Proï¬le Generation Prompt              Ranked List            Evaluation\nevaluates multiple users simultaneously, preventing overfitting to\nindividual cases and enhancing generalization. During each iter-\nation, AGP systematically analyzes why a generated user profile\nfails to prioritize ground-truth items, ensuring that refinements\nare both meaningful and robust. To further enhance optimization,\n                                                                                                        Prompt Optimization            Feedback Summarization           Batched Feedback\nAGP introduces position-based feedback, which explicitly signals\nranking misalignment. Unlike traditional ranking metrics such as\nNDCG, which serve as indirect optimization objectives, position-\nbased feedback provides actionable instructions by identifying dis-     Figure 1: The pipeline of the AutoGuidePrompt (AGP) frame-\ncrepancies between predicted and ideal item rankings. For example,     work, illustrating the process from user interaction history\nif an item is ranked 3rd but should be 1st, AGP generates structured      to optimized reranked lists.\nfeedback to adjust the user profile generation prompt accordingly.\nThis feedback is then aggregated across batches, allowing learned\nrefinements to generalize across different users. AGPâ€™s optimiza-      interacted item. Unlike ID-based approaches, this representation\ntion process follows an iterative batch-based strategy, where      leverages semantic knowledge, enabling LLMs to infer preferences.\nfeedback is collected over multiple iterations. Each batch gener-     A recommender system provides a baseline ranking ğ‘…base(ğ‘¢) =\nates actionable instructions that refine the user profile prompt in a        {ğ‘–1,ğ‘–2, . . . ,ğ‘–ğ‘˜}, where items are ranked by predicted relevance. The\ngradient-like manner, ensuring continuous improvement without      goal is to refine ğ‘…base(ğ‘¢) into an optimized ranking ğ‘…LLM(ğ‘¢) that\noverfitting. By integrating structured user profile optimization with       better aligns with user preferences, and the reranking function is:\nposition-aware feedback, AGP enables a more interpretable and                      ğ‘“: (U, ğ‘…base) â†’ğ‘…LLM.                      (1)\nscalable approach to personalized reranking.\n                                                                          Challenges. Applying LLMs to item recommendations presents\n  Our Contributions.                                        two key challenges:\nâ€¢ User Profile Generation Prompt. We propose refining a user     â€¢ Disparate and Noisy Item Information: Unlike structured do-\n   profile generation promptâ€”as opposed to a single-step rerank       mains like news recommendation, item representations in recom-\n  promptâ€”to more effectively capture user preferences from noisy       mender systems vary widely, with metadata that can be incom-\n  textual data. This design enables more nuanced and robust per-        plete, inconsistent, or redundant (e.g., short titles, varying\n  sonalization.                                                            descriptions, or noisy user-generated content). This variability\nâ€¢ Position-Based Feedback. We introduce a position-based feed-       makes it difficult for LLMs to infer structured user preferences.\n  back mechanism that pinpoints item-level ranking misalign-     â€¢ Lack of Direct Optimization Signals: Existing methods (e.g.,\n  ments, providing interpretable signals for iterative refinement.       RecPrompt) optimize prompts using aggregated ranking metrics\n  This approach proves more actionable than relying solely on       (AUC, NDCG), which measure performance but do not directly\n  aggregated metrics like NDCG.                                      provide optimization guidance. A more interpretable strategy is\nâ€¢ Batched Training and Summarized Feedback. We devise a       needed to refine prompts using explicit ranking signals.\n  batched optimization framework that aggregates feedback across                                                          To address these, we propose AGP, which optimizes user pro-\n  multiple users, mitigating overfitting to individual quirks and                                                                                                file generation prompts instead of reranking prompts, incorporates\n  ensuring updates remain broadly applicable.                                                                     structured feedback, and iteratively refines prompts to improve\n                                                                      personalization and ranking quality.\n2 METHODOLOGY\nIn this section, we formalize the reranking task, introduce our Auto     2.2  User Profile Generation with a Learned\nPrompt Optimization Framework, and detail each of its components.        Prompt\nWe first define the problem setup and notation, then elaborate on                                                AGP optimizes a shared profile-generation prompt ğ‘gen to construct\nthe user profile generation process, the evaluation and feedback                                                                      personalized user profiles. Unlike manually crafted prompts, ğ‘gen is\nmechanism, and finally the iterative prompt optimization strategy.\n                                                                              iteratively refined through batch training to capture generalizable\n                                                                      patterns across users.\n2.1  Problem Setup and Notation                           For each user ğ‘¢, AGP generates a profile based on two inputs: (1)\nGiven a set of users U, each user ğ‘¢âˆˆU has an interaction his-      the userâ€™s text-based interaction history ğ»(ğ‘¢) and (2) the current\ntory ğ»(ğ‘¢) = {ğ‘¡1,ğ‘¡2, . . . ,ğ‘¡ğ‘š}, where each ğ‘¡ğ‘—is the textual title of an      version of ğ‘gen. The LLM synthesizes these inputs to generate a\n\nAGP                                                                                                                               SIGIRâ€™25, July 13â€“18, 2025, Padova, Italy\n\n\nstructured profile:                                               improvements:\n\n                  ğ‘(ğ‘¢) = LLM ğ»(ğ‘¢), ğ‘gen  .                    (2)                   Fsum(Uğ‘) = âˆ‘ï¸ ğ‘¤(ğ‘¢)F (ğ‘¢),                  (5)\n                                                                               ğ‘¢âˆˆUğ‘\nSince ğ»(ğ‘¢) consists of item titles, the LLM extracts thematic prefer-\nences (e.g., science fiction, deep learning) while ğ‘gen provides struc-     where ğ‘¤(ğ‘¢) = avgPos(ğ‘¢)1     assigns a higher weight to users whose\nture and emphasis.                                                 ground-truth items rank lower on average.\n  The generated profile ğ‘(ğ‘¢) is then used to rerank the baseline       The prompt is iteratively updated as:\nrecommendation list ğ‘…base(ğ‘¢), where the LLM refines rankings as:\n                                                                              ğ‘gen â†ğ‘gen âˆ’ğ‘¤(Uğ‘) Â· âˆ‡text Fsum(Uğ‘)  ,          (6)\n                    Ë†ğ‘…(ğ‘¢) = LLM ğ‘(ğ‘¢), ğ‘…base(ğ‘¢)  .                  (3)                               1                                                         where ğ‘¤(Uğ‘) =           ensures stronger updates when rank-                                                                                       avgPos(Uğ‘)\nThis two-step process enhances LLM reasoning by summarizing      ing errors are larger.\nuser interests before reranking, allowing informed and context-        This iterative process refines ğ‘gen by adjusting user profile de-\naware ranking decisions. Since AGP refines profile generation       scriptions based on ranking misalignment. After each update, new\nrather than modifying raw rankings, it preserves user-specific in-      user profiles ğ‘(ğ‘¢) and reranked lists Ë†ğ‘…(ğ‘¢) are generated, and feed-\nsights while generalizing effectively across different users.            back is recalculated until convergence. By summarizing batch-wide\n                                                                         trends, AGP prevents overfitting to specific user preferences and\n2.3  Position-Based Evaluation and Feedback          ensures adaptability across diverse user populations.\nTo refine reranking, we introduce position-based feedback, a more\ninterpretable alternative to aggregated metrics like NDCG. Given a    3  EXPERIMENTS\nuser ğ‘¢, we define a set of ground-truth relevant items ğº(ğ‘¢) âŠ†Ë†ğ‘…(ğ‘¢)                                                         3.1  Experimental Settings\nin the reranked list Ë†ğ‘…(ğ‘¢). For each itemğ‘–âˆˆğº(ğ‘¢), we record its actual\n                                             We use three public datasets: Amazon Movies & TV (Movies), Yelp,\nposition ğ‘ŸË†ğ‘…(ğ‘–,ğ‘¢) and compare it to its target position ğ‘Ÿtarget(ğ‘–,ğ‘¢). If                                                            and Goodreads. The Amazon dataset has 95,593 users, 43,117 items,\nan item should ideally be ranked in the top-3 but appears at position\n                                                          and 750,081 interactions; Yelp has 65,348 users, 33,626 items, and\n5, the LLM receives a correction signal.\n                                                                         1,041,540 interactions; and Goodreads has 13,100 users, 25,434 items,\n   This process generates feedback signals:\n                                                            and 856,280 interactions. Using a leave-one-out (LOO) strategy, we\n       F (ğ‘¢) =   ğ‘ŸË†ğ‘…(ğ‘–,ğ‘¢), ğ‘Ÿtarget(ğ‘–,ğ‘¢)  ğ‘–âˆˆğº(ğ‘¢)   .          (4)      designate the most recent user interaction as the test item and\n                                                                   the second-most recent as validation. To establish a baseline, we\nThese signals specify ranking deviations, providing direct and in-      train two recommender models: (1) LightGCN [5], a graph-based\nterpretable instructions for refinement. Unlike aggregated ranking      collaborative filtering method, and (2) SASRec [6], a sequential\nmetrics, which provide an overall evaluation score, position-based      transformer-based recommender. Each model generates top-10 pre-\nfeedback delivers explicit corrections for each item, making adjust-      dictions per user, from which we randomly select 300 users for\nments more targeted.                                                  evaluation, ensuring each ground-truth item is included. On this\n   Position-based feedback offers two key advantages. First, it im-      subset, we apply our AGP framework to rerank the top-10 list.\nproves interpretability by providing explicit positional corrections    AGP is trained on 100 randomly selected user interaction histo-\nrather than relying on abstract scores. Second, it enables fine-       ries with a maximum of 10 epochs. We explore user history se-\ngrained ranking adjustments, ensuring that high-relevance items     quence lengths of 5, 10, and 20 as a hyperparameter, along with\nreceive stronger refinements while less critical items undergo minor      batch size values of 5, 10, and 20. Evaluation is conducted using\ntweaks. This process naturally integrates with AGPâ€™s optimization    NDCG@10, which measures ranking quality (higher is better). We\nstrategy by offering direct ranking signals that guide systematic       also compare against two non-iterative baselines: LLM-Dir (single-\nimprovements.                                                    pass prompt for reranking) and LLM-CoT (single-pass chain-of-\n                                                              thought prompt [7] for reranking). Additionally, we include four\n2.4  Batch Formation and Optimization             LLMs for performance comparison: GPT-4o (4o), GPT-4o-Mini (4o-\nAGP optimizes the profile-generation prompt ğ‘gen using batch       Mini), GPT-o3-Mini (o3-Mini), and DeepSeek-V3 (DeepSeek) [3].\ntraining. In each iteration, a batch Uğ‘of users is processed, where\neach user ğ‘¢âˆˆUğ‘generates a profile ğ‘(ğ‘¢) = LLM ğ»(ğ‘¢), ğ‘gen  ,     3.2  Results and Discussion\nwhich is then used to compute the reranked list Ë†ğ‘…(ğ‘¢). Feedback     The results are shown in Table 1, highlighting key findings. AGP\nF (ğ‘¢), derived from position-based evaluation (Sec. 2.3), guides      effectiveness in item recommendation: AGP proves effective\nprompt refinement.                                                     for reranking tasks, demonstrating that LLMs can self-optimize\n   Batch-based training has been previously explored for improving      prompts within our framework. This adaptability reduces the need\nranking robustness [2], where partitioned input evaluation helps       for manual prompt tuning and enhances ranking quality, particu-\nmitigate bias. Inspired by this, AGP extends batch training to struc-       larly in personalized recommendation scenarios. LLM reranker\ntured prompt optimization by aggregating feedback across users,     performance on SASRec vs. LightGCN: LLM rerankers show\nensuring refinements are driven by explicit ranking misalignments       greater improvements on SASRec rankings than on LightGCN. This\nrather than indirect scoring metrics. Given individual position-       is likely due to SASRec and LLMs both leveraging time-series\nbased signals F (ğ‘¢), AGP generates a summarized set of batch-level     modeling, making them more effective in capturing sequential\n\nSIGIRâ€™25, July 13â€“18, 2025, Padova, Italy                                                                                        Chen Wang et al.\n\n\nuser behaviors. LightGCN, a graph-based collaborative filter-\ning model, focuses on global user-item interactions, which limits\nLLMsâ€™ ability to enhance its ranking list. CoT effectiveness: The\nbest performance on the Yelp dataset is achieved by o3-Mini, indi-\ncating that in scenarios with insufficient textual context and\nnoisy data, multi-step reasoning is beneficial. This suggests that\nstructured thinking models like o3-Mini can extract better signals\nin complex and sparse text environments, improving reranking\neffectiveness. Yelp dataset challenges: The Yelp dataset sees mi-\n                                                             Figure 2: Ablation study on summarization (left) and batch\nnor improvements due to the lack of rich textual features and\n                                                                size/sequence length impact (right) using GPT-4o on the AMZ\npresence of noisy text. Many business descriptions are sparse or\n                                                                     dataset. Summarization reduces overfitting, while batch size\ninconsistent, making it difficult for LLMs to infer meaningful item\n                                                           10 and sequence length 5 yield optimal ranking performance.\nrelationships. Despite these challenges, AGP remains effective by\ndynamically refining prompts, allowing LLMs to better interpret\navailable textual information and improve reranking performance.\n\n\nTable 1: Reranking performance (N@10) across datasets.\nâ€œT100â€ denotes AGP trained on 100 users. Bold values in-\ndicate the best performance within the same LLM model.\n\n                                                                                          (a) Amazon Dataset      (b) Yelp Dataset     (c) Goodreads Dataset\n                                N@10\n     Model    Method\n                         AMZ  YELP  GR              Figure 3: Ablation study on Position-Based Feedback.\n\n                 Base                      0.513   0.501   0.474\n\n              + LLM-Dir (4o)           0.547   0.491   0.555\n                                                        Summarization Impact on Overfitting: We analyze the ef-              + LLM-CoT (4o)           0.551   0.491   0.560\n              + AGP-T100 (4o)         0.553  0.502  0.572             fect of summarization by comparing training and test performance\n                                                                with and without summarization. As shown in Figure (2, left), sum-              + LLM-Dir (4o-Mini)      0.553   0.484   0.548\n              + LLM-CoT (4o-Mini)     0.553   0.481   0.547           marization prevents excessive fitting to the training data while\n     LightGCN              + AGP-T100 (4o-Mini)    0.561  0.493  0.553           improving test performance, enhancing generalization by filtering\n              + LLM-Dir (o3-Mini)      0.542   0.538   0.540           redundant information and refining prompt optimization. Effec-\n              + LLM-CoT (o3-Mini)     0.543   0.531   0.542           tiveness of Batch Size and Sequence Length: We examine how\n              + AGP-T100 (o3-Mini)    0.558  0.541  0.548           batch size and sequence length impact reranking effectiveness. Fig-\n              + LLM-Dir (DeepSeek)    0.546   0.496   0.533           ure (2, middle) shows that the best performance is achieved with\n              + LLM-CoT (DeepSeek)   0.547   0.498   0.544           a batch size of 10 and a sequence length of 5, suggesting an opti-\n              + AGP-T100 (DeepSeek)  0.551  0.504  0.564          mal balance between convergence and generalization. Larger batch\n                 Base                      0.659   0.528   0.599             sizes stabilize training, while shorter sequences reduce noise from\n              + LLM-Dir (4o)           0.671   0.510   0.624           long interaction histories, highlighting the importance of careful\n              + LLM-CoT (4o)           0.664   0.521   0.610           hyperparameter selection for improving LLM reranking. Effective-\n              + AGP-T100 (4o)         0.696  0.530  0.636          ness of Position-Based Feedback (PBF): We further investigate\n              + LLM-Dir (4o-Mini)      0.654   0.502   0.631           the impact of PBF on reranking quality across datasets. Figure 3\n              + LLM-CoT (4o-Mini)     0.656   0.507   0.615           demonstrates that incorporating PBF improves both NDCG@10 and      SASRec              + AGP-T100 (4o-Mini)    0.658  0.517   0.627                                                                average ranking position across all datasets. The gains are more\n              + LLM-Dir (o3-Mini)      0.658   0.527   0.587             significant in datasets with high variability in ranking scores, indi-\n              + LLM-CoT (o3-Mini)     0.663   0.528   0.585                                                                        cating that PBF helps LLMs better capture relative item importance.\n              + AGP-T100 (o3-Mini)    0.683  0.541  0.595\n                                                                 This result highlights its potential in enhancing ranking stability\n              + LLM-Dir (DeepSeek)    0.648   0.512   0.622          and improving personalization in LLM-based reranking systems.\n              + LLM-CoT (DeepSeek)   0.655   0.519   0.610\n              + AGP-T100 (DeepSeek)  0.687  0.523  0.636\n                                                         3.4  Training Efficiency\n                                            We evaluate the efficiency of AGP training by analyzing the API\n                                                                                  call calculations and performance scaling. The total API calls per\n3.3  Ablation Study                                             training stage follow the formula:\nTo evaluate the impact of design choices in our framework, we con-\n                                                                                                          100\nduct an ablation study using GPT-4o on the AMZ dataset, focusing             API Calls = (batch_size Ã— 3 + 2) Ã—                      (7)\n                                                                                                                 batch_size\non three key aspects: the effect of summarization in reducing over-\nfitting, the influence of batch size and sequence length on reranking     where 3 corresponds to generating a user profile, reranking, and\nperformance, and the effectiveness of position-based feedback.        computing the loss per user, while 2 accounts for summarization\n\nAGP                                                                                                                               SIGIRâ€™25, July 13â€“18, 2025, Padova, Italy\n\n\nand prompt optimization per batch. To further assess AGPâ€™s effi-         [6] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nciency, we trained it on the AMZ dataset with GPT-4o using 700            mendation. In 2018 IEEE International Conference on Data Mining (ICDM). IEEE,\n                                                                                          197â€“206.\nusers instead of 100. The NDCG@10 increased marginally from          [7] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\n0.696 to 0.705 (a 1.29% increase), demonstrating that AGP maintains            Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in\ncompetitive performance with significantly fewer training samples.             neural information processing systems 35 (2022), 22199â€“22213.\n                                                                                                  [8] Xinyi Li, Yifan Chen, Benjamin Pettit, and Maarten De Rijke. 2019. Personalised\nThis result underscores AGPâ€™s effectiveness and efficiency, reducing            reranking of paper recommendations using paper content and user behavior.\ncomputational costs while preserving high reranking quality.            ACM Transactions on Information Systems (TOIS) 37, 3 (2019), 1â€“23.\n                                                                                                  [9] Xiao Lin, Xiaokai Chen, Chenyang Wang, Hantao Shu, Linfeng Song, Biao Li, and\n                                                                                 Peng Jiang. 2024. Discrete conditional diffusion for reranking in recommendation.\n4  CONCLUSION                                                                  In Companion Proceedings of the ACM on Web Conference 2024. 161â€“169.\n                                                                                            [10] Dairui Liu, Boming Yang, Honghui Du, Derek Greene, Neil Hurley, Aonghus\nWe propose AGP, a framework optimizing user profile generation             Lawlor, Ruihai Dong, and Irene Li. 2024. RecPrompt: A Self-tuning Prompting\nprompts for better LLM-based reranking. reduce one word: AGP           Framework for News Recommendation Using Large Language Models. In Pro-\nemploys batched feedback and position-based feedback for              ceedings of the 33rd ACM International Conference on Information and Knowledge\n                                                                               Management (Boise, ID, USA) (CIKM â€™24). Association for Computing Machinery,\nimproved generalization and ranking accuracy. Experiments con-         New York, NY, USA, 3902â€“3906.  https://doi.org/10.1145/3627673.3679987\nfirm its effectiveness, with future work exploring reinforcement        [11] Sichun Luo, Bowei He, Haohan Zhao, Wei Shao, Yanlin Qi, Yinya Huang, Aojun\nlearning and broader use.                                                         Zhou, Yuxuan Yao, Zongpeng Li, Yuanzhang Xiao, et al. 2024. Recranker: Instruc-                                                                                                tion tuning large language model as ranker for top-k recommendation. ACM\n                                                                                                Transactions on Information Systems (2024).\nREFERENCES                                                                    [12] Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao Sun, Jian\n                                                                      Wu, Peng Jiang, Junfeng Ge, Wenwu Ou, et al. 2019. Personalized re-ranking\n [1] Diego Carraro and Derek Bridge. 2024. Enhancing recommendation diversity\n                                                                                                      for recommendation. In Proceedings of the 13th ACM conference on recommender\n    by re-ranking with large language models. ACM Transactions on Recommender\n                                                                                                  systems. 3â€“11.\n     Systems (2024).\n                                                                                            [13] Antonio Sabbatella, Andrea Ponti, Ilaria Giordani, Antonio Candelieri, and\n [2] Mohsen Dehghankar and Abolfazl Asudeh. 2024. Rank It, Then Ask It: Input\n                                                                                       Francesco Archetti. 2024.  Prompt Optimization in Large Language Models.\n     Reranking for Maximizing the Performance of LLMs on Symmetric Tasks. arXiv\n                                                                                      Mathematics 12, 6 (2024), 929.\n      preprint arXiv:2412.00546 (2024).\n                                                                                            [14] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan\n [3] DeepSeek-AI et al. 2024. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL]\n                                                                                      Zhang, YK Li, Yu Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of\n      https://arxiv.org/abs/2412.19437\n                                                                                         mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300\n [4] Jingtong Gao, Bo Chen, Xiangyu Zhao, Weiwen Liu, Xiangyang Li, Yichao Wang,\n                                                                                                         (2024).\n      Zijian Zhang, Wanyu Wang, Yuyang Ye, Shanru Lin, et al. 2024. LLM-enhanced\n                                                                                            [15] Haobo Zhang, Qiannan Zhu, and Zhicheng Dou. 2025. Enhancing Reranking for\n     Reranking in Recommender Systems. arXiv preprint arXiv:2406.12433 (2024).\n                                                                              Recommendation with LLMs through User Preference Retrieval. In Proceedings\n [5] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\n                                                                                                          of the 31st International Conference on Computational Linguistics. 658â€“671.\n    Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for\n                                                                                            [16] Yongfeng Zhang, Zhiwei Liu, Qingsong Wen, Linsey Pang, Wei Liu, and Philip S\n     recommendation. In Proceedings of the 43rd International ACM SIGIR conference\n                                                                                            Yu. 2024. AI Agent for Information Retrieval: Generating and Ranking. In Pro-\n     on research and development in Information Retrieval. 639â€“648.\n                                                                                                  ceedings of the 33rd ACM International Conference on Information and Knowledge\n                                                                                  Management. 5605â€“5607.",
"headers": [
"arXiv:2504.03965v1  [cs.IR]  4 Apr 2025",
"Automating Personalization: Prompt Optimization for",
"Recommendation Reranking",
"Zhiwei Liu",
"Chen Wang",
"Mingdai Yang",
"Pan Li",
"Linsey Pang",
"Qingsong Wen",
"Philip Yu",
"ABSTRACT",
"1",
"INTRODUCTION",
"CCS CONCEPTS",
"KEYWORDS",
"2",
"METHODOLOGY",
"2.2",
"User Profile Generation with a Learned",
"Prompt",
"2.1",
"Problem Setup and Notation",
"2.3",
"Position-Based Evaluation and Feedback",
"3",
"EXPERIMENTS",
"3.1",
"Experimental Settings",
"2.4",
"Batch Formation and Optimization",
"3.2",
"Results and Discussion",
"3.4",
"Training Efficiency",
"3.3",
"Ablation Study",
"4",
"CONCLUSION",
"REFERENCES"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2504.03965v1.pdf"
}