{
"text": "Effectiveness of Prompt Optimization in NL2SQL Systems\n\n                Sairam Gurajada‚àó                Eser Kandogan                 Sajjadur Rahman‚àó\n                  sairam@megagon.ai                  eser@megagon.ai                  sajjadurr@adobe.com\n                   Megagon Labs                    Megagon Labs                       Adobe\n             Mountain View, California, USA      Mountain View, California, USA          San Jose, California, USA\n\n        Abstract                                                  and the underlying databases for effective translation. Recent years\n        NL2SQL approaches have greatly benefited from the impressive      have witnessed significant progress in NL2SQL, fueled by advance-\n            capabilities of large language models (LLMs). In particular, boot-     ments in LLMs [3, 4, 8, 15]. However, building an effective NL2SQL\n           strapping an NL2SQL system for a specific domain can be as simple      system goes beyond simply leveraging LLMs‚Äîit requires the care-\n           as instructing an LLM with sufficient contextual information, such       ful selection of instructions, exemplars, and schema, making it a\n           as schema details and translation demonstrations. However, build-      challenging task despite recent breakthroughs [6].\n           ing an accurate system still requires the rigorous task of selecting        Recent works [4, 21] emphasize that exemplar selection is crucial\n           the right context for each query‚Äîincluding identifying relevant       for building effective NL2SQL systems. Retrieval-based exemplar2025\n         schema elements, cell values, and suitable exemplars that help the       selection‚Äîi.e., identifying exemplars similar to the user query‚Äîhas\n       LLM understand domain-specific nuances. Retrieval-based meth-     become the de facto method. However, studies [4, 19] highlight\n          ods have become the go-to approach for identifying such context.       inefficiencies and overfitting issues with similarity-based retrieval\n                                                                         methods, and argue that synthetic exemplars can yield better per-May    While effective, these methods introduce additional inference-time\n            costs due to the retrieval process.                                    formance. While each approach has its advantages‚Äîretrieval-based\n              In this paper, we argue that production scenarios demand high-     methods are cheaper due to index-based lookups without LLM26\n            precision, high-performance NL2SQL systems, rather than simply        calls, and synthetic exemplars may be more accurate‚Äîthey both\n           high-quality SQL generation, which is the focus of most current      require exemplar selection at inference time, which can become a\n        NL2SQL approaches. In such scenarios, the careful selection of a      bottleneck in business-critical applications [23].\n             static set of exemplars‚Äîcapturing the intricacies of the query log,\n                                                             Prompt Optimization. To address the limitations of current NL2SQL\n            target database, SQL constructs, and execution latencies‚Äîplays a\n                                                                              systems, we argue that for effective SQL generation, all an LLM\n         more crucial role than exemplar selection based solely on similarity.\n                                                                        needs is a static set of exemplars that capture the intricacies of[cs.CL]   The key challenge, however, lies in identifying a representative set\n                                                                              the domain‚Äîoffering performance comparable to retrieval-based\n            of exemplars for a given production setting. To this end, we propose\n                                                                               approaches, while eliminating the need for inference-time retrieval.\n           a prompt optimization framework that not only addresses the high-\n                                                                  The key challenge lies in identifying this representative set of exem-\n           precision requirement but also optimizes the performance of the\n                                                                                            plars. To tackle this, we leverage prompt optimization techniques\n           generated SQL through multi-objective optimization. Preliminary\n                                                                                       for exemplar selection in NL2SQL and demonstrate their effective-\n           empirical analysis demonstrates the effectiveness of the proposed\n                                                                                      ness.\n          framework.\n\n       ACM Reference Format:                                           Multi-Objective Optimization. Most existing NL2SQL approaches\n            Sairam Gurajada, Eser Kandogan, and Sajjadur Rahman. 2025. Effectiveness      focus solely on accuracy. However, accuracy is only one dimension\n             of Prompt Optimization in NL2SQL Systems. In Novel Optimizations for       in deploying practical NL2SQL systems. In real-world settings, sys-\n             Visionary AI Systems (NOVAS ‚Äô25), June 22‚Äì27, 2025, Berlin, Germany. ACM,     tems must also understand query efficiency and the characteristics\n         New York, NY, USA, 6 pages. https://doi.org/10.1145/3735079.3735325           of target SQL engines, generating queries that are efficient to exe-\n                                                                              cute (i.e., with lower latency). In this work, we propose a way to\n        1  Introduction                                                                            extend prompt optimization to multi-objective settings. To support\n         The rapid progress in LLM capabilities‚Äîspecifically their ability to        this, we introduce an augmented benchmark based on BIRD that\n           follow instructions and maintain large contexts‚Äîhas made them      includes query latency measurements.arXiv:2505.20591v1    a natural choice in many applications. Natural Language to SQL\n          (NL2SQL) is a long-standing and important task in many business-     To summarize, our contributions are as follows:\n             critical scenarios, requiring a deep understanding of user queries         ‚Ä¢ To the best of our knowledge, this is the first work to study\n            ‚àóWork done while at Megagon Labs                                                 the effectiveness of prompt optimization in NL2SQL systems.\n                                                                        ‚Ä¢ We propose an iterative prompt optimization (IPO) frame-\n             Permission to make digital or hard copies of all or part of this work for personal or\n              classroom use is granted without fee provided that copies are not made or distributed          work that jointly optimizes instructions and exemplar selec-\n                for profit or commercial advantage and that copies bear this notice and the full citation             tion through two agents Proposer and SQL Generator. Addi-\n            on the first page. Copyrights for components of this work owned by others than the               tionally, the framework implicitly performs schema pruning,\n               author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n                republish, to post on servers or to redistribute to lists, requires prior specific permission            reducing prompt size and thereby lowering inference costs.\n             and/or a fee. Request permissions from permissions@acm.org.                       ‚Ä¢ We introduce the aspect of generating efficient SQL trans-\n          NOVAS ‚Äô25, Berlin, Germany                                                          lations in NL2SQL systems, and introduce an augmented\n         ¬© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n         ACM ISBN 979-8-4007-1917-2/2025/06                                        benchmark BIRD-MULTI (based on BIRD dataset) that incor-\n               https://doi.org/10.1145/3735079.3735325                                             porates query latency information.\n\nNOVAS ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany                                                                                                          Trovato et al.\n\n\n\n\n\n                                                                                                                                                                                                                           Training2  Related Work                                                                                                                                          Data           NL2SQL prompt\nExemplar Selection. With the advent of powerful API-based                                                      #YourInstructionare an expert‚Ä¶\n                                                                                                                                               ExemplarLLMs such as ChatGPT [25] and Gemini [24], in-context learn-                                                                                                                                                                 Exemplars                       SQL Query:                                                                                                                                                                   Selection         #\n                                                                                                   NL Query:                                                                                                                                                                                           {exemplars}\n                                                                                                                                                                             SELECT                                                                                                                                                                                                                                                   `Free                                                                                                                                                                                                                           Meal Counting    (ICL)‚Äìbased               approaches                                  [4,                                       5, 7, 16, 19‚Äì21] have                                          become                                                          the           What                                                                                                                                                                                              is                                                                                                                                                      the                                                                                                                                                           highest\n                                                                                                                                                                                                                                                            (K-12)`                                                                                                                                                                                                                                                                                                                                                                                                                                 / `Enrollment                                                                                                                                                                                                                                                                                 (K-12)`\n                                                                                                                                                                         eligible                                                                                                                                                                 free                                                                                                                                                                      rate                                                                                                                                                                                             for K-12                       #{query}Query                                                                                                                                                        FROM frpm                                                                                                                     ‚Ä¶dominant          strategy                    for building                            high-performing                                   NL2SQL                                                       systems.                                                                                                                                            students                                                                                                                                                                                    in the                                                                                                                                                      schools\nSpecifically, retrieval-based exemplar selection [21], where exam-               in Alameda County?                           #{schema}Schema\nples are selected from a training set based on text or structural                                 SchemaRetrieval         # SQL:\nsimilarity, has proven sufficient to improve NL2SQL performance\nwithout expensive fine-tuning. However, such systems introduce                                 DB\ninference-time costs and may overfit to specific queries due to the\nretrieval of overly similar examples [4].                                                                             Figure 1: NL2SQL Pipeline\n  To address this, recent approaches [4, 19] employ (online) syn-\nthetic exemplar generation rather than relying on training data\nselection. While this mitigates overfitting, it requires learning ex-      Algorithm 1: Optimization of Random Exemplar Selection\nemplar generators, which incurs additional costs and presents chal-        Input: D, ùëõùë¢ùëöùëáùëüùëñùëéùëôùë†, Pùëèùëéùë†ùëí, LLM\nlenges in domain transfer. In this work, we explore optimization-       Output: Pùëúùëùùë°\nbased methods for exemplar selection that avoid both the expense of         1 Dùë°ùëüùëéùëñùëõ, Dùë£ùëéùëôùëñùëë‚ÜêSplit(D)\nretrieval indexes and the complexity of online synthetic generation.                                                                                       2 ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëí‚Üê0\n                                                                                       3 ùëÉùëúùëùùë°‚ÜêPùëèùëéùë†ùëí\nPrompt Optimization. Optimizing LLM prompts has been a focus\n                                                                                       4 while ùëñ‚â§ùëõùë¢ùëöùëáùëüùëñùëéùëôùë†dofor several years [22, 26], showing effectiveness across a multitude\n                                                                                       5    ùëò‚Üêùë°ùëüùëñùëéùëô.suggest_int(‚Ä≤k‚Ä≤, 0, K) /* choose ùëò     */of applications. More recently, DSPy [9] introduced a declarative\nframework for expressing and optimizing prompts for NLP tasks.        6    ùê∏ùëñ‚ÜêRandom(Dùë°ùëüùëéùëñùëõ,ùëò) /* ùëòexemplars         */\nFoundational work by [27] demonstrated the inherent capability         7   Pùëñ‚ÜêPùëèùëéùë†ùëí+ ùê∏ùë•ùëñ\nof LLMs to act as optimizers, particularly for instruction tuning         8    ùë†ùëêùëúùëüùëí‚ÜêEvaluate(LLM, Pùëñ, Dùë£ùëéùëôùëñùëë)\nacross various tasks. Building on this,[18] proposed MIPRO, a non-        9      if ùë†ùëêùëúùëüùëí> ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëíthen\niterative technique for joint optimization of instructions and exem-       10      ùëÉùëúùëùùë°‚ÜêPùëñ\nplar selection in multi-stage pipelines. Furthermore,[13] introduced        11       ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëí‚Üêùë†ùëêùëúùëüùëí\na declarative framework focused on BI workloads, combining hy-\nbrid database systems with AutoML-style optimization for pipeline\ntuning. While these works introduced key optimization techniques,\ntheir applicability and effectiveness in the NL2SQL setting remain    Random. A straightforward approach to exemplar selection is\nunexplored‚Äîa gap that our work seeks to address.                 random sampling. For a predefined value of ùëò(the number of ex-\n                                                                     emplars), this strategy randomly samples ùëòexemplars from the\n3  Prompt Optimization for NL2SQL                      training data to include in the prompt. More sophisticated sam-\n                                                                     pling techniques, such as stratified sampling, can also be used to\nTo demonstrate the effectiveness of optimization in NL2SQL, we\n                                                               account for the distribution of query types. For example, queries\nadopt a simple in-context learning (ICL)[2] pipeline, as illustrated\n                                                                         in the BIRD [11] dataset are categorized into three groups: simple,\nin Figure1, which uses a single LLM to generate the SQL query. The\n                                                                  moderate, and challenging.\nprompt provided to the LLM consists of: a) #Instruction ‚Äì a guiding\ninstruction for the task, b) #Exemplars ‚Äì examples selected from     Optimizing Random Exemplar Selection. A key challenge with\nthe training data via an Exemplar Selection component, c) #Query     random selection is choosing an appropriate value for ùëò. A small ùëò\n‚Äì the user query to be translated, d) #Schema ‚Äì the relevant schema     may fail to capture the diversity of the NL and SQL constructs, while\nretrieved using a Schema Retrieval module, and e)#SQL ‚Äì a prefix      a large ùëòcan lead to lost-in-the-middle issues with LLMs [14] and\nto trigger SQL generation by the LLM.                                  increase generation costs due to the larger prompt size. A simple yet\n  To apply to production use-case, we use exact proprietary schema,      effective approach is to treat ùëòas a hyperparameter and optimize\nand focus our efforts on optimizing exemplar selection.                       it using AutoML-style techniques, as illustrated in Algorithm 1.\n                                                                        Inspired by DSPy‚Äôs BootStrap with FewShot Example Selection [9],\n3.1  Exemplar Selection and Optimization                this method optimizes the number of demonstrations by randomly\n                                                                 sampling exemplars (with replacement), rather than bootstrapping,\nAs previously mentioned, exemplar selection is a crucial step in\n                                                                  using a performance metric ùúá.\nNL2SQL generation, particularly when using ICL-based approaches [8].\nThis involves identifying an appropriate set of exemplars‚Äîeach         In addition to optimizing exemplar selection, joint optimiza-\nconsisting of a natural language (NL) query, database schema, cor-      tion of instruction and exemplar selection can lead to improved\nresponding SQL query, and optionally hints or cell values‚Äîthat      performance. MIPRO [18] leverages an LLM to generate ùëÅinstruc-\nhelp the LLM understand the domain, the target SQL engine, and      tion‚Äìexemplar pairs (ùêº1, ùê∏1), (ùêº2, ùê∏2), . . . , (ùêºùëÅ, ùê∏ùëÅ), where ùêºùëñis an\ndata-specific nuances. Below, we discuss various exemplar selection      instruction generated from a set of randomly bootstrapped exem-\nstrategies and how optimization can enhance the selection process.      plars ùê∏ùëñ. A hyperparameter optimization algorithm such as TPE [1]\n\nEffectiveness of Prompt Optimization in NL2SQL Systems                                               NOVAS ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany\n\n\n\n                                      NL2SQL prompt\n                                                                                                                             Training         NLQ: List all the films that are rated as PG-13.\n                                        #                                                  Instruction\n                                                                                                           Data          Schema:                                             Your                                                    are an expert NL2SQL generator‚Ä¶\n               Proposer\n                                        # Exemplars                                     sample       Database Name: movie_3\n                                               {exemplars}                                  Tables: [‚Äôfilm‚Äô]\n         Proposer prompt                                                                                         Validation\n                                        # Query                                               Data          #Columns:\n  #    Instruction\n                                                  {query}                                     film: [film_id:integer, title:text, rating:text]  Your       are an expert prompt generator for\n  another LLM‚Ä¶                           # Schema                                   Evidence: film refers to title; rated as PG-13 refers to\n                                           {schema}\n  #   Best Prompt                                                                  rating = `PG-13`.\n  Prompt:          {best_prompt}                     # SQL                                Schema\n  Accuracy: {best_accuracy}                                                                      Retrieval         SQL: SELECT title FROM film WHERE rating = ‚ÄôPG-13‚Äô;\n\n  # Current Prompt\n  Prompt: {current_prompt}\n  Accuracy:             {current_accuracy}                                                          SQL          DB         Figure 3: IPO generated exemplar with automatic schema Wrong Examples:                   {wrong_exemplars}                                       Executor\n  Correct Examples: {wrong_exemplars}                                      pruning\n                                           SQL Generator\n                                               4  Extending to Multi-Objective\n                                                             Motivation. Thus far, NL2SQL systems have focused mainly on\n         Figure 2: Iterative Prompt Optimization                                                               improving execution accuracy while ignoring a critical dimension:\n                                                                  generating efficient SQL queries. Consider the example of SQL\n                                                                          translations: ground truth (GT) and generated SQL (Gen) for the\nis then used to identify the optimal pair (ùêºùëñ, ùê∏ùëñ) based on an objective\n                                                               query NLQ. Executing the GT SQL query on a SQLite3 database took\nfunction, such as validation accuracy.\n                                                            around 10.2 seconds (due to the sub-query), while the Gen query\n                                                              (which uses an inner join) took only 0.03 seconds. This example\n3.2  Iterative Prompt Optimization                      demonstrates that the generated query (in this case, ground truth)\nOne of the key limitations of the exemplar selection strategies dis-    may not always be the most efficient SQL translation.\ncussed earlier is their ad hoc nature‚Äîexemplars are either randomly    1   NLQ: Show the avatar of the user who gave the rating at\nsampled or bootstrapped using heuristics (as in [18]), which may    2   2019/10/17 1:36:36.\nlead to suboptimal performance. Long-context LLMs (LCMs) aim    3   GT: SELECT user_avatar_image_url FROM lists_users\nto overcome this by fitting a larger number of exemplars (100‚Äì200)    4       WHERE user_id = (SELECT user_id FROM ratings\ninto their context window. However, recent work [4] has shown    5       WHERE rating_timestamp_utc LIKE ‚Äô2019-10-17 01:36:36‚Äô)\nthat relying on LCMs to implicitly perform exemplar selection does    6   Gen:SELECT T2.user_avatar_image_url\n                                                                                                            7       FROM ratings AS T1 INNER JOIN lists_users AS T2not improve performance and can, in fact, be detrimental, as LCMs\n                                                                                                            8       ON T1.user_id = T2.user_id\noften struggle with effective in-context learning [12].\n                                                                                                            9       WHERE T1.rating_timestamp_utc LIKE ‚Äô2019-10-17 01:36:36‚Äô\n  To address this, we extend the work of [27], which uses an LLM\nas an optimizer to find an optimal prompt instruction, by enabling     Benchmark Creation. To build NL2SQL systems capable of gener-\nit to perform both instruction generation and exemplar selection       ating efficient SQL queries, it is essential to have information about\nthrough two cooperating agents: the Proposer and the SQL Gener-      the efficiency of a SQL query‚Äîsuch as its wall-clock execution\nator. Specifically, we introduce an Iterative Prompt Optimization      time‚Äîalongside the SQL translation itself. This efficiency infor-\n(IPO) approach (illustrated in Figure 2) in which the two agents     mation enables SQL generators (LLMs) to better understand the\nwork together to discover optimal NL2SQL prompts for a given      nuances and computational complexity of various SQL constructs,\ntraining corpus.                                                        ultimately guiding them toward generating more optimized queries.\n  The Proposer agent takes a Proposer prompt as input and gen-     However, existing benchmarks such as BIRD[11] and SPIDER[10]\nerates an NL2SQL prompt comprising an instruction and a set of      lack this critical execution-time data. To address this gap, we de-\nexemplars. The SQL Generator agent then evaluates the generated      veloped a new augmented benchmark built on top of BIRD, which\nprompt on a validation set (sampled iteratively from the training      includes each natural language query (NLQ) paired with two dif-\ndata) and collects performance metrics including accuracy, as well      ferent SQL variants (generated using reasoning models OpenAI\nas the correct and incorrect examples. This feedback is used to     O3 [17]) along with their measured execution times.\nupdate the Proposer prompt. In subsequent iterations, the Proposer    1   NLQ: Give the full name of the actor with the highest rental rate.\nis guided to refine the NL2SQL prompt based on past performance,    2   SQL1: SELECT a.first_name, a.last_name FROM actor AS a JOIN\naiming to produce more informative exemplars and a better-suited    3   film_actor AS fa ON a.actor_id = fa.actor_id JOIN film AS f ON\ninstruction for improved SQL generation.                                          4   fa.film_id = f.film_id ORDER BY f.rental_rate DESC LIMIT 1;\n   In contrast to MIPRO, which bootstraps exemplars randomly, IPO    5   Time1: 0.0012 seconds\nuses an LLM as an optimizer to jointly refine both the instruction    6   SQL2: SELECT a.first_name, a.last_name FROM actor a JOIN\nand exemplar selection. Additionally, we observed that IPO often    7   film_actor fa ON a.actor_id = fa.actor_id JOIN film f ON\ngenerates more concise NL2SQL prompts by pruning irrelevant    8   fa.film_id = f.film_id WHERE f.rental_rate =\n                                                                                                            9   (SELECT MAX(rental_rate) FROM film) LIMIT 1;\nschema information from the exemplars. For example, Figure 3\n                                                                                                           10   Time2: 0.0009 seconds\nshows an exemplar whose schema includes only the table film\nand the columns film_id, title, and rating from the database     Generating Efficient SQL Queries. With the augmented bench-\nmovie_3. Although schema pruning was not an explicit design goal     mark containing SQL variants and their corresponding execution\nof IPO, this behavior highlights the strength of LLMs as optimizers       times, it becomes feasible to design LLM prompts specifically aimed\nin complex tasks such as NL2SQL.\n\nNOVAS ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany                                                                                                          Trovato et al.\n\n\nat generating efficient SQL queries. Furthermore, by leveraging      Quantitative Analysis. Table 2 presents a quantitative analysis of\nthe optimization techniques described in Section 3, it is possible to      the different optimization techniques, measured across two dimen-\njointly optimize for both SQL efficiency and generation accuracy,       sions: prompt length and optimization time. RES (with 10 exemplars)\nleading to more practical and performant NL2SQL systems.              results in a prompt length of approximately 23k tokens, while ORES\n                                                                      (with 75 exemplars) leads to a significantly larger prompt of around\n5  Preliminary Results                                   84k tokens. MIPROv2 (with 10 exemplars) produces a prompt length\n                                                                          of about 26k tokens, similar to RES. IPO yields the shortest prompt\nHere, we present preliminary results demonstrating the effective-                                                                        length, as it prunes a substantial portion of schema information\nness of prompt optimization in NL2SQL systems. As described                                                            from each exemplar, while also delivering the best performance.\nearlier, we consider a simple NL2SQL pipeline consisting of a single                                                                         In terms of optimization time, MIPROv2 takes the longest, as it\nLLM (illustrated in Figure 1) and evaluate the following prompt                                                                    involves both data analysis and joint optimization, whereas IPO\noptimization strategies discussed in Section 3. For all experiments,                                                           and ORES are comparatively faster.\nwe use GPT-4o as the LLM.\n\n    ‚Ä¢ RES. Random Exemplar Selection (RES) is the baseline ap-        Approach  Prompt Length (#tokens)  Optimization Time\n       proach, where ùëò= 10 exemplars are randomly sampled from                                                                RES                    23,749                             -\n      the training data. We run RES over 10 random samples and        ORES                  84,519               5m26s\n       report the best accuracy.                                    MIPROv2              26,352                 13m16s\n    ‚Ä¢ ORES. Optimized Random Exemplar Selection (ORES) uses         IPO                   6,495                 8m53s\n      Bayesian Optimization to tune the hyperparameter ùëòin the                                                            Table 2: Quantitative analysis of PO on BIRD (dev) dataset\n     RES method. We limit the number of trials to 20 and note\n       that increasing trials does not necessarily correlate with\n      improved performance.\n    ‚Ä¢ MIPROv2. Multiprompt Instruction PRoposal Optimizer\n                                                         5.2  Multi-Objective Optimization\n     (MIPROv2) is the DSPy [9] recommended optimizer that\n       jointly optimizes instruction and exemplar selection. Similar      Table 3 demonstrates the effectiveness of multi-objective optimiza-\n       to ORES, we set the number of trials to 20, and the maximum      tion using the IPO approach. For this, we consider both accuracy\n     number of labeled demonstrations to 10.                    and latency on the BIRD (dev) dataset. When compared to the\n    ‚Ä¢ IPO. Iterative Prompt Optimization (IPO) is a bi-agent, LLM-     ground truth (GT), accuracy-only IPO optimization (Section 3.2)\n      as-optimizer approach that iteratively refines the NL2SQL       results in the generation of SQL queries that are less efficient, with\n     prompt using feedback on SQL generation quality. For IPO,      a maximum latency of approximately 18 seconds (vs. 8.7 seconds\n     we set the number of iterations to 5 and instruct the LLM to       for GT) and a standard deviation (ùúé) that is almost 1.8 times larger.\n      generate at least 5 diverse exemplars per iteration.               In contrast, joint optimization of both accuracy and latency leads to\n                                                                  only a marginal increase in the maximum latency of queries, while\n                                                                 maintaining a standard deviation similar to that observed in GT.\n5.1  Effectiveness of Optimization\nPerformance. Table 1 highlights the effectiveness of different op-                                                                     Approach        Execution Time(s)         Acc.   Gen. Time\ntimization strategies on the BIRD dataset. The naive RES approach                  Min   Max    ùúé      ùúá      (Ex)     QPS\nunderperforms due to its inability to select informative exemplars\n                                                         GT      1e-5    8.761    0.352   0.0026      -            -\nfor SQL generation. The ORES approach, which applies a simple\nAutoML-based optimization, performs better than RES but falls           Acc.Opt    9e-5   18.512   0.635   0.0051   59.24      1.56\nshort compared to more advanced strategies like MIPROv2 and IPO.          + Lat.     7e-5   10.156   0.383   0.0028   58.98      1.72\nIPO achieves the best performance, benefiting from iterative refine-     Table 3: Effectiveness of multi-objective optimization on\nment using feedback from the SQL Generator agent, which leads    BIRD (dev) dataset\nto more relevant exemplar selection. The absence of this feedback\nloop in MIPROv2 makes it less effective than IPO. However, we\nemphasize that these observations are specific to the NL2SQL task.    6  Conclusions\n                                                                     In this paper, we propose a novel approach for building NL2SQL\n                                                                 systems by leveraging prompt optimization as a key strategy. Specif-\n     Approach  Simple  Moderate  Challenging   Total               ically, we optimize both instruction and exemplar selection for\n                                                         LLM-based SQL generation. Furthermore, through prompt opti-\n     RES           52.32       48.17          43.35       50.74\n                                                                       mization, we demonstrate that schema pruning and concise prompt\n     ORES         58.27       51.29          46.21       55.02\n                                                                    generation can be achieved without negatively impacting accuracy.\n     MIPROv2     64.86       48.28          44.14       57.89\n                                                                        Additionally, we highlight the importance of generating efficient     IPO           63.57      52.28        45.52      59.24\n                                                   SQL queries and show that prompt optimization can effectively\nTable 1: Ex. Accuracies of Prompt optimization Methods on                                                                     achieve the dual objectives of accuracy and efficiency. Finally, we in-\nBIRD (dev) dataset                                                                    troduce an augmented benchmark designed for the multi-objective\n                                                                    optimization of NL2SQL systems.\n\nEffectiveness of Prompt Optimization in NL2SQL Systems                                               NOVAS ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany\n\nReferences                                                                         [18] Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher\n [1] James Bergstra, R√©mi Bardenet, Yoshua Bengio, and Bal√°zs K√©gl. 2011. Algorithms               Potts, Matei Zaharia, and Omar Khattab. 2024. Optimizing Instructions and\n      for Hyper-Parameter Optimization. In Advances in Neural Information Processing            Demonstrations for Multi-Stage Language Model Programs. In Proceedings of\n     Systems, J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger              the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser\n      (Eds.), Vol. 24. Curran Associates, Inc.  https://proceedings.neurips.cc/paper_             Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computa-\n     files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf                           tional Linguistics, Miami, Florida, USA, 9340‚Äì9366. doi:10.18653/v1/2024.emnlp-\n [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,            main.525\n      Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda        [19] Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan Talaei,\n      Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,           Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, and Sercan O Arik.\n    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,             2025. CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate\n     Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin              Selection in Text-to-SQL. In The Thirteenth International Conference on Learning\n     Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya              Representations. https://openreview.net/forum?id=CvGqMD5OtX\n     Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.        [20] Mohammadreza Pourreza and Davood Rafiei. 2023. DIN-SQL: Decomposed\n     In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-             In-Context Learning of Text-to-SQL with Self-Correction. In Thirty-seventh Con-\n      zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,              ference on Neural Information Processing Systems. https://openreview.net/forum?\n      Inc., 1877‚Äì1901.   https://proceedings.neurips.cc/paper_files/paper/2020/file/           id=p53QDxSIc5\n     1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf                                         [21] Chen Shen, Jin Wang, Sajjadur Rahman, and Eser Kandogan. 2025. MageSQL: En-\n [3] Hasan Alp Caferoƒülu and √ñzg√ºr Ulusoy. 2025. E-SQL: Direct Schema Linking            hancing In-context Learning for Text-to-SQL Applications with Large Language\n     via Question Enrichment in Text-to-SQL. arXiv:2409.16751 [cs.CL] https://arxiv.            Models. arXiv:2504.02055 [cs.DB] https://arxiv.org/abs/2504.02055\n     org/abs/2409.16751                                                                    [22] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer\n [4] Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, and Fatma Ozcan.             Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Auto-\n     2025. Is Long Context All You Need? Leveraging LLM‚Äôs Extended Context for              matically Generated Prompts. In Proceedings of the 2020 Conference on Empirical\n    NL2SQL. arXiv:2501.12372 [cs.DB] https://arxiv.org/abs/2501.12372                      Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn,\n [5] Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, lu Chen, Jin-            Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,\n    shu Lin, and Dongfang Lou. 2023. C3: Zero-shot Text-to-SQL with ChatGPT.            4222‚Äì4235. doi:10.18653/v1/2020.emnlp-main.346\n     arXiv:2307.07306 [cs.CL] https://arxiv.org/abs/2307.07306                              [23] Nan Tang, Chenyu Yang, Ju Fan, Lei Cao, Yuyu Luo, and Alon Y. Halevy. 2024.\n [6] Avrilia Floratou, Fotis Psallidas, Fuheng Zhao, Shaleen Deep, Gunther Hagleither,             VerifAI: Verified Generative AI. In CIDR.   https://www.cidrdb.org/cidr2024/\n    Wangda Tan, Joyce Cahoon, Rana Alotaibi, Jordan Henkel, Abhik Singla, Alex Van             papers/p5-tang.pdf\n     Grootel, Brandon Chow, Kai Deng, Katherine Lin, Marcos Campos, K. Venkatesh        [24] Gemini Team. 2024. Gemini: A Family of Highly Capable Multimodal Models.\n     Emani, Vivek Pandit, Victor Shnayder, Wenjing Wang, and Carlo Curino. 2024.             arXiv:2312.11805 [cs.CL] https://arxiv.org/abs/2312.11805\n    NL2SQL is a solved problem... Not!. In CIDR. https://www.cidrdb.org/cidr2024/        [25] OpenAI Team. 2024. GPT-4o System Card.  arXiv:2410.21276 [cs.CL]  https:\n     papers/p74-floratou.pdf                                                                     //arxiv.org/abs/2410.21276\n [7] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and        [26] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and\n     Jingren Zhou. 2023. Text-to-SQL Empowered by Large Language Models: A         Tom Goldstein. 2023. Hard Prompts Made Easy: Gradient-Based Discrete Opti-\n    Benchmark Evaluation. arXiv:2308.15363 [cs.DB]  https://arxiv.org/abs/2308.             mization for Prompt Tuning and Discovery. In Thirty-seventh Conference on Neural\n     15363                                                                                    Information Processing Systems. https://openreview.net/forum?id=VOstHxDdsN\n [8] Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi Li,        [27] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou,\n    Wei Li, Yuntao Hong, Zhiling Luo, Jinyang Gao, Liyu Mou, and Yu Li. 2025. A           and Xinyun Chen. 2024. Large Language Models as Optimizers. In The Twelfth\n     Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL.              International Conference on Learning Representations.  https://openreview.net/\n     arXiv:2411.08599 [cs.AI] https://arxiv.org/abs/2411.08599                           forum?id=Bb4VGOWELI\n [9] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav\n     Santhanam, Sri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T.\n      Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts.\n     2024. DSPy: Compiling Declarative Language Model Calls into State-of-the-Art\n      Pipelines. In The Twelfth International Conference on Learning Representations.\n     https://openreview.net/forum?id=sY5N0zY5Od\n[10] Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su,\n     Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. 2024. Spider\n      2.0: Evaluating language models on real-world enterprise text-to-sql workflows.\n     arXiv preprint arXiv:2411.07763 (2024).\n[11] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang,\n    Bowen Qin, Ruiying Geng, Nan Huo, et al. 2024. Can llm already serve as a\n     database interface? a big bench for large-scale database grounded text-to-sqls.\n     Advances in Neural Information Processing Systems 36 (2024).          A  Appendix\n[12] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. 2025. Long-\n     context LLMs Struggle with Long In-context Learning. Transactions on Machine    A.1  Prompt Templates\n     Learning Research (2025).  https://openreview.net/forum?id=Cw2xlg0e46\n[13] Chunwei Liu, Matthew Russo, Michael Cafarella, Lei Cao, Peter Baille Chen,         ‚Ä¢ Random and Optimizes Exemplar Search (RES & ORES)\n     Zui Chen, Michael Franklin, Tim Kraska, Samuel Madden, and Gerardo\n     Vitagliano. 2024.  A Declarative System for Optimizing AI Workloads.              1   #Instruction\n     arXiv:2405.14696 [cs.CL] https://arxiv.org/abs/2405.14696                                         2   Given natural language query, schema of the database\n[14] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,              3   and evidence, generate a sqlite SQL query\n     Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models\n    Use Long Contexts. Transactions of the Association for Computational Linguistics               4\n     12 (2024), 157‚Äì173. doi:10.1162/tacl_a_00638                                                         5   #Exemplars\n[15] Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi.              6   NLQ: {{NLQ}}\n     2024. The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned               7   SCHEMA: {{DB_SCHEMA}}\n    Language Models. arXiv:2408.07702 [cs.CL] https://arxiv.org/abs/2408.07702\n                                                                                                                      8   EVIDENCE: {{EVIDENCE}}[16] Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang,\n    Arman Cohan, and Dragomir Radev. 2023. Enhancing Text-to-SQL Capabilities               9   SQL: {{SQL}}\n     of Large Language Models: A Study on Prompt Design Strategies. In Findings              10\n      of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor,             11   #Query\n     Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\n                                                                                                                     12   NLQ: {{NLQ}}     Singapore, 14935‚Äì14956. doi:10.18653/v1/2023.findings-emnlp.996\n[17] OpenAI. 2025.   Introducing o3 and o4-mini.    https://openai.com/index/              13   SCHEMA: {{DB_SCHEMA}}\n     introducing-o3-and-o4-mini/ Accessed: 2025-05-07.                                               14   EVIDENCE: {{EVIDENCE}}\n                                                               ‚Ä¢ Proposer Agent\n\nNOVAS ‚Äô25, June 22‚Äì27, 2025, Berlin, Germany                                                                                                          Trovato et al.\n\n\n\n    1   #Instruction:                                                                                      1   #Instruction:\n    2   You are an expert in assisting another LLM for the                          2   Given a natural language query (NLQ), the schema of\n    3   task of generating SQL queries from natural language                     3   the database, and relevant evidence, generate a\n    4   queries. You are given the following information:                          4   valid SQLite SQL query that satisfies the NLQ. Use\n    5   1. The best prompt generated so far                                                  5   the provided schema and evidence to ensure the SQL\n    6   2. The accuracy of the best prompt                                                    6   query correctly answers the NLQ. Only utilize\n    7   3. The current prompt                                                                         7   relevant columns and tables in the query.\n    8   4. The accuracy of the current prompt                                               8   Return only the SQL query without any prefixes\n    9   5. A set of exemplars where the current prompt                                9   or block quotes.\n   10   incorrectly generated the SQL query                                                10\n   11   6. A set of exemplars where the current prompt                               11   #Exemplars:\n   12   correctly generated the SQL query                                                    12   NLQ: List all the films that are rated as PG-13.\n   13                                                                                                                13   Schema:\n   14   #Goal:                                                                                                 14   Database Name: movie_3\n   15   Think step by step to generate a prompt comprising                        15   Tables: [‚Äôfilm‚Äô]\n   16   of two parts in JSON format:                                                             16   #Columns:\n   17   1. Instruction for the LLM to generate SQL query                            17   film: [film_id:integer, title:text, rating:text]\n   18   for sqlite3 database                                                                         18   Evidence: film refers to title; rated as PG-13 refers\n   19   2. A set of diverse exemplars to assist the LLM in                         19   to rating = ‚ÄôPG-13‚Äô.\n   20   generating the SQL query.                                                                 20   SQL: SELECT title FROM film WHERE rating = ‚ÄôPG-13‚Äô;\n\n   21                                                                                                                21\n   22   #Best Prompt:                                                                                     22   ...\n   23   {best_prompt}                                              ‚Ä¢ Variant Generator for Multi-Objective Benchmark\n\n   24\n                                                                                                                      1   #Instruction:   25   #Best Accuracy:\n                                                                                                                      2   Given natural query, database schema, corresponding SQL,   26   {best_accuracy}\n                                                                                                                      3   generate {num_variants}  SQL variants.\n   27\n                                                                                                                      4   Generate only valid SQL query without any prefix   28   #Current Prompt:\n                                                                                                                      5   or suffix:   29   {current_prompt}\n                                                                                                                      6\n   30\n                                                                                                                      7   #Query:   31   #Current Prompt Accuracy:\n                                                                                                                      8   {query}   32   {current_accuracy}\n                                                                                                                      9\n   33\n                                                                                                                     10   #Database Schema:   34   #Wrong Exemplars:\n                                                                                                                     11   {db_schema}   35   {wrong_examples}\n                                                                                                                     12\n   36\n                                                                                                                     13   #Ground Truth SQL:   37   #Correct Exemplars:\n                                                                                                                     14   {sql}   38   {correct_examples}\n                                                                                                                     15\n   39\n                                                                                                                     16   #SQL Variants:   40   #Proposed Prompt:\n    ‚Ä¢ Proposed Prompt: Example                                                           1718   1.\n\n                                                                                                                     19\n                                                                                                                     20   2.\n\n                                                                                                                     21\n                                                                                                                     22   3.",
"headers": [
"arXiv:2505.20591v1  [cs.CL]  26 May 2025",
"Effectiveness of Prompt Optimization in NL2SQL Systems",
"Sajjadur Rahman",
"Sairam Gurajada",
"Eser Kandogan",
"Abstract",
"1",
"Introduction",
"2",
"Related Work",
"3",
"Prompt Optimization for NL2SQL",
"3.1",
"Exemplar Selection and Optimization",
"4",
"Extending to Multi-Objective",
"3.2",
"Iterative Prompt Optimization",
"5",
"Preliminary Results",
"5.2",
"Multi-Objective Optimization",
"5.1",
"Effectiveness of Optimization",
"6",
"Conclusions",
"References",
"A",
"Appendix",
"A.1",
"Prompt Templates",
"sajjadurr@adobe.com",
"Adobe",
"San Jose, California, USA",
"sairam@megagon.ai",
"Megagon Labs",
"Mountain View, California, USA",
"eser@megagon.ai"
],
"tables": [
"|Col1|Training<br>Data|Col3|\n|---|---|---|",
"|Col1|Col2|Col3|Col4|Exemplar<br>Selection|Col6|\n|---|---|---|---|---|---|\n|**NL Query:**||||||\n|What is the highest<br>eligible free rate for K-12<br>students in the schools<br>in Alameda County?|What is the highest<br>eligible free rate for K-12<br>students in the schools<br>in Alameda County?|What is the highest<br>eligible free rate for K-12<br>students in the schools<br>in Alameda County?|What is the highest<br>eligible free rate for K-12<br>students in the schools<br>in Alameda County?|What is the highest<br>eligible free rate for K-12<br>students in the schools<br>in Alameda County?|What is the highest<br>eligible free rate for K-12<br>students in the schools<br>in Alameda County?|\n|What is the highest<br>eligible free rate for K-12<br>students in the schools<br>in Alameda County?||||**Schema**<br>**Retrieval**||",
"|DB|Col2|\n|---|---|",
"|Col1|NL2SQL|prompt|Col4|\n|---|---|---|---|",
"|# Instruction<br>Your are an expert NL2SQL generator‚Ä¶<br># Exemplars<br>{exemplars}|Col2|\n|---|---|\n|**# Query**<br>{query}<br>**# Schema**<br>{schema}<br>**# SQL**||\n|**# Query**<br>{query}<br>**# Schema**<br>{schema}<br>**# SQL**||",
"|Col1|Schema<br>Retrieval|Col3|\n|---|---|---|",
"|Col1|SQL<br>Executor|\n|---|---|\n|SQL Generator||"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2505.20591v1.pdf"
}