{
"text": "Preprint. Under review.\n\n\n\n           REVISITING PROMPT OPTIMIZATION WITH LARGE REA-\n           SONING MODELS—A CASE STUDY ON EVENT EXTRAC-\n           TION\n\n\n                   Saurabh Srivastava & Ziyu Yao\n                     George Mason University\n                          Fairfax, VA 22030, USA\n                 {ssrivas6,ziyuyao}@gmu.edu\n\n\n\n                                       ABSTRACT\n2025                       Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have\n                              demonstrated remarkable capabilities in various reasoning tasks. Their strong capa-\n                                      bility to generate and reason over intermediate thoughts has also led to argumentsOct                            that they may no longer require extensive prompt engineering or optimization to\n                                    interpret human instructions and produce accurate outputs. In this work, we aim to\n16                           systematically study this open question, using the structured task of event extraction\n                                   for a case study. We experimented with two LRMs (DeepSeek-R1 and o1) and two\n                              general-purpose Large Language Models (LLMs) (GPT-4o and GPT-4.5), when\n                                they were used as task models or prompt optimizers. Our results show that on tasks\n                                as complicated as event extraction, LRMs as task models still benefit from prompt\n                                optimization, and that using LRMs as prompt optimizers yields more effective\n                              prompts. Our finding also generalizes to tasks beyond event extraction. Finally,[cs.CL]                       we provide an error analysis of common errors made by LRMs and highlight the\n                                      stability and consistency of LRMs in refining task instructions and event guidelines.\n\n\n                1  INTRODUCTION\n\n                       In  recent  years,  Large Language Models\n                  (LLMs) have demonstrated remarkable capabil-                                                                 40.84\n                             ities across various natural language processing               Average LRM performance as    opt                   LRMsoptimizersas\n                                                                                                                                                              32.51               yeild\n                         tasks. However, their proficiency in complex               Average LLM performance as    opt                          significantimprovement\n                      reasoning tasks has often been limited (Zhou                                                          36.91  Optimization\n                                                                                                                                                                                         helps                                                                                                                                                                   and                                                                                                                                                              LRMs                            et al., 2022). To address this, a new class of mod-              Average LRM performance as    task                                                                                                                                                                                                     benefit                                                                                                                                                                                            the                                                                                                                                                                        most                                                                                                                                                            31.25\n                             els, known as Large Reasoning Models (LRMs),              Average LLM performance as    task\n                      has emerged, focusing on enhancing reasoning                  16.47\n                           abilities through advanced training methodolo-               Best LRM, DeepSeek-R1 (No Optimization)                                                                                                                               16.45\n                           gies. Two prominent examples are DeepSeek-R1               Best LLM, GPT-4.5 (No Optimization)arXiv:2504.07357v2                   (Guo et al., 2025) and OpenAI’s o1 (Zhong et al.,          15       20       25       30       35       40\n                                                                                                                          AC F1-score\n                       2024), both setting new standards in various rea-\n                      soning tasks.                                                                   Figure 1: Summary of our main results, where\n                  The advent of these advanced reasoning mod- LRMs and LLMs are used as either the task model\n                           els has sparked discussions (Wang et al., 2024a;  (Mtask) or the optimizer (Mopt) in prompt opti-\n                   OpenAI, 2025; Mantaras, 2025; Together AI,  mization, and we observed a strong advantage of\n                      2025; Menendez et al., 2025) about the necessity  LRMs over LLMs.\n                        of prompt optimization—the process of refining\n                       input prompts to guide model outputs effectively (Zhou et al., 2022; Yang et al., 2024; Srivastava\n                           et al., 2024; Agarwal et al., 2024; Guo et al., 2024; Fernando et al., 2024; Li et al., 2025). Tradition-\n                             ally, prompt optimization has been crucial for enhancing LLM performance, with frameworks like\n                    PromptAgent (Wang et al., 2024b) and OPRO (Yang et al., 2024) automating the creation and refine-\n                    ment of prompts through iterative feedback and strategic planning. However, the inherent reasoning\n                          capabilities of LRMs raise questions about whether such prompt optimization techniques are equally\n                         beneficial for these models. While previous studies have demonstrated the effectiveness of prompt\n\n\n                                                           1\n\nPreprint. Under review.\n\n\n\n\n\noptimization in improving LLM performance, there is a notable gap in research focusing on its impact\non LRMs. Moreover, many existing prompt optimization studies focus on tasks where zero-shot\nbaselines already perform well, whereas recent work, such as Gao et al. (2024), demonstrates that\neven powerful models like GPT-4 struggle with Information Extraction tasks, underscoring the need\nfor more targeted and optimized prompting strategies.\n\nTo fill this gap, we conduct the first systematic study of prompt optimization with LRMs and compare\ntheir performance with LLMs. In particular, we experimented with these models on a challenging\ntask, i.e., end-to-end event extraction (EE), a structured prediction task of information extraction\nthat requires identifying and classifying event triggers and arguments within text. EE poses unique\nchallenges: models must follow schema constraints, handle coreference, and balance precision with\nrecall, all of which demand nuanced reasoning. We evaluated four models, two LRMs (DeepSeek-R1,\no1) and two LLMs (GPT-4.5, GPT-4o) as both task models and prompt optimizers within a Monte\nCarlo Tree Search (MCTS) framework (Wang et al., 2024b). This setup allows us to examine both\ntask performance and prompt optimization quality under a consistent setting.\n\nOur experimental results (Fig. 1) show that LRMs benefit substantially from prompt optimization,\neven when the training set for optimization is small, and they outperform LLMs in both task\nperformance (as a task model) and optimization effectiveness (as a prompt optimizer). When used\nas optimizers, LRMs produce more precise prompts that align with human annotation heuristics,\nleading to faster convergence and lower variance in MCTS. Our error analysis further shows that these\noptimized prompts reduce common mistakes such as implicit trigger overgeneration or argument\nspan drift. While DeepSeek-R1 as a prompt optimizer yields the most effective and concise prompts,\nprompt length alone is not predictive, i.e., different task models prefer different prompt styles. To\ntest generality, we apply the same optimization framework to two tasks beyond EE, i.e., Geometric\nShapes (Suzgun et al., 2022) and NCBI Disease NER (Do˘gan et al., 2014). In both, LRMs again\nshow the largest gains, confirming that our findings extend beyond schema-based tasks.\n\n\n2  RELATED WORKS\n\n\nPrompt optimization has become an essential direction in adapting LLMs for downstream tasks with-\nout modifying their weights. For models with accessible internal states, such as open-source LLMs,\nprior work has explored soft prompt tuning (Li & Liang, 2021; Lester et al., 2021; Wang et al., 2023b;\nHu et al., 2022) and gradient-based search methods that directly adjust prompt embeddings (Shin\net al., 2020; Wen et al., 2023). Reinforcement learning has also been applied to optimize prompts\nthrough interaction-based feedback (Deng et al., 2022; Zhang et al., 2023).\n\nHowever, these approaches are not applicable to closed-source LLMs accessed via APIs, where\ngradients and internal representations are unavailable. As such, research has focused on black-box,\ngradient-free techniques that rely on prompt perturbation and scoring. Many of these methods operate\nin an iterative loop: starting from an initial prompt, they generate variants, evaluate them on held-out\nexamples, and retain the best one for the next round. Variants can be created through phrase-level\nedits (Prasad et al., 2023), back-translation (Xu et al., 2022), evolutionary operations (Guo et al.,\n2024; Fernando et al., 2024), or by prompting another LLM to rewrite the prompt based on model\nerrors (Zhou et al., 2022; Pryzant et al., 2023; Srivastava et al., 2024; Wang et al., 2024b). Structured\nstrategies such as Monte Carlo search (Zhou et al., 2022), Gibbs sampling (Xu et al., 2024), and\nbeam search (Pryzant et al., 2023) have been explored to improve the efficiency of exploration.\n\nMore recent efforts have proposed structured prompt optimization. APE (Zhou et al., 2022) uses\nMonte Carlo Tree Search (MCTS) to explore the prompt space, while PromptBreeder (Fernando\net al., 2024) and EvoPrompt (Guo et al., 2024) evolve prompts using feedback-driven mutation\nstrategies. OPRO (Yang et al., 2024) employs mutation-based search guided by model performance.\nOther systems, such as PromptWizard (Agarwal et al., 2024) and G¨odel Machine (Yin et al., 2025),\nincorporate self-evolving mechanisms in which the LLM iteratively generates, critiques, and refines\nits own prompts and examples.\n\nWhile these approaches are promising, they have so far been applied exclusively to large, general-\npurpose LLMs. To the best of our knowledge, our work is the first to investigate prompt optimization\nfor LRMs. Furthermore, we introduce and study this framework in the context of a structured\n\n\n                                       2\n\nPreprint. Under review.\n\n\n\n\n\n                # This is an event extraction task where the goal is to extract structured events from the text                Task\n                     following structured event definitions in Python. (complete instruction omitted)                                Instruction\n                # Here are the event definitions:\n                 @dataclass\n                   class PhoneWrite(ContactEvent):\n                       \"\"\"A PhoneWrite Event occurs when two or more people directly engage in discussion which does not\n                  take place 'face-to-face'. To make this Event less open-ended, we limit it to written or telephone\n                 communication where at least two parties are specified.\"\"\"\n                    mention: str   # The text span that most clearly expresses (triggers) the event\n                          entity: List[str] # The communicating agent\n                       time: List[str]  # When the communication takes place\n                                                                                                                    Event                                         Schema  @dataclass                                                                                              Guidelines\n                   class Meet(ContactEvent):\n                       \"\"\"A Meet Event occurs whenever two or more Entities come together at a single location and interact                                  Event\n                    with one another face-to-face. Meet Events include talks, summits, conferences, meetings, visits, and any\n                   other Event where two or more parties get together at some location.\"\"\"\n                    mention: str    # The text span that most clearly expresses (triggers) the event\n                          entity: List[str]  # The agents who are meeting\n                       time: List[str]  # When the meeting takes place\n                      place: List[str] # Where the meeting takes place\n                  (Other event definitions omitted)\n\n              # These are the texts to analyze\n                   text = \"They met yesterday to discuss the plans before the next attack.\"                                (Input Text)\n\n                   result = [Meet(mention = \"met\", entity = [\"they\"], \"time\" = [\"yesterday\"], \"place\" = [])]                     (Output)\n\nFigure 2: An example prompt for end-to-end Event Extraction (EE) used in our experiments,\nconsisting of a task instruction and an event schema. The event schema contains information about\nthe labels that are represented as Python classes and event guidelines defining both the event classes\nand the arguments. In prompt optimization, we refine both the task instruction and event guidelines\n(shown for two events; others omitted due to space limits) to generate more effective prompts for the\ntask model.\n\nprediction task, event extraction, which poses distinct challenges compared to typical mathematical\nor reasoning tasks explored in prior work (Zhou et al., 2022; Srivastava et al., 2024).\n\n\n3  METHODOLOGY\n\n3.1  PROBLEM SETUP\n\nDiscrete prompt optimization aims to refine task-specific prompts for a task LLM Mtask to improve\nits performance without modifying the model weights. In this study, we analyze whether LRMs benefit\nfrom prompt optimization in the context of end-to-end EE. The task consists of trigger extraction,\nwhich involves identifying event trigger spans and classifying their event types, and argument\nextraction, which requires identifying argument spans within the extracted event instance with a\npre-defined role. To prompt a task model, Mtask, we adopted a Python code-based representation\nfor both the input and the output of the model, which was shown to be effective by prior work (Wang\net al., 2023a; Sainz et al., 2024; Li et al., 2023; 2024; Srivastava et al., 2025). As shown in Fig. 2, the\ninitial prompt, P0 consists of two main parts: the task instruction and the event schema annotated by\nguidelines. Task instruction PI forms the initial segment of input to introduce the task and specify\ninstructions such as the desired output format. The event schema contains information about the\nlabels, such as event names and argument roles, that are represented as Python classes. The argument\nroles (e.g., time and place) are defined as attributes of event classes. All the events and arguments in\na schema are annotated using human-written event guidelines PE. The output is represented as a\nlist of instances of the classes defined in the event schema. In this paper, we refine both PI and PE\nwhich is represented as the concatenation P0 = [PI||PE], where || represents the concatenation.\nGiven a training set Dtrain = {(Qi, Ai)}Ni=1, where each Qi denotes an input text and Ai its\ncorresponding event instance, the objective of prompt optimization is to discover an optimal prompt\nP∗that maximizes a task-specific evaluation function R, such as the F-score for EE. Event guidelines\ntypically contain a combination of explicit schema constraints and implicit domain-specific rules that\nannotators follow during data labeling. However, not all of these rules are fully documented or easily\ntranslatable into a single static prompt. As a result, the initial prompt P0 may lack critical structural\nor interpretive cues required for high-quality extraction. We employ an optimizer LLM Mopt to\nrefine P0 to discover such rules and constraints through strategic planning for superior, expert-level\nprompt optimization. Note that we do not modify the event schema defined by the original EE task,\nbut only the human-written task instruction and the guidelines.\n\n\n                                       3\n\nPreprint. Under review.\n\n\n\n\n\n                                          Task Instruction    Answer Generation              3 Feedback Generation\n                                          Event Guidelines                      2\n                                  1                Set of incorrect           Feedback Instruction             4 Optimization                                           Concatenation\n                                         Python Interpreter    Q1         A1   examples\n                                  Q2         A2     Q2   A2\n                                                                      Q2  A2  Q4  A4  Q6  A6  Q7  A7\n                                  Q3         A3     Q4   A4\n                                  Q4         A4\n                                                                         Q6   A6\n                                  Q5         A5\n\n                                  Q6         A6     Q7   A7\n                                                                                                                                       Optimization\n                                  Q7         A7                                                                             Instruction\n\n                                             Optimized Task Instruction\n                                           and Event Definitions\n\nFigure 3: Overview of our prompt optimization framework. At each iteration, a zero-shot task LLM\ngenerates outputs, while a separate optimizer LLM analyzes the errors and updates the prompt,\nincluding task instructions and event guidelines, accordingly. This process continues over batches\nof training samples Dtrain, and the final optimized prompt is evaluated on the development set to\ndetermine the node reward rt.\n\n3.2  PROMPT OPTIMIZATION FRAMEWORK\n\nWe frame prompt optimization as a discrete search over a large natural-language prompt space S.\nSince S is too large for exhaustive search, we adopt Monte Carlo Tree Search (MCTS) to explore it\nefficiently, balancing exploration of new prompts with exploitation of promising ones, as in Wang\net al. (2024b). We model the process as a Markov Decision Process (MDP) where each state st is a\nprompt Pt and each action is formulated to make edits to the current prompt (e.g., adding constraints\nor clarifying rules).\n\nPrompt optimization assumes a training set Dtrain. As illustrated in Fig. 3, each MCTS node holds\na prompt Pt and a batch of queries Qbatch from the training set. In Step 1, the task model Mtask\nis first employed to generate answers for queries in Qbatch. The incorrect outputs generated by\nthe task model are then extracted and passed through a Python interpreter to identify issues such\nas parsing errors, missing event types, and invalid spans (Step 2). Following it, in Step 3, we\nprompt a prompt optimizer LLM Mopt with an instruction mfb to analyze the model errors and\ngenerate structured feedback ft, including pinpointing unclear role definitions, proposing fixes, and\nsummarizing recurring issues. In doing so, the generated feedback can be leveraged to produce\ntargeted, actionable edits to improve clarity, coverage, and consistency of the task instruction and\nevent guidelines. Next, in Step 4, Mopt is instructed by another instruction mopt to generate the\nupdated prompt Pt+1 in a single pass, based on the distribution pMopt(st+1 | st, ft, mopt). We also\npass the history of previous prompts to discourage redundant edits. Only event types involved in the\nerror batch are updated; others are inherited unchanged.\n\nTo evaluate each new prompt, we compute a reward rt = R(st, ft) based on averaged F1 scores\nacross EE subtasks (TI, TC, AI, AC, described in Section 4.1) on a held-out development (dev) set\nafter editing Pt with feedback ft. The best prompt is selected based on dev-set performance. We\nprovide additional details, the full algorithm, and the settings in Appendix A.\n\n\n4  EXPERIMENTS\n\n\n4.1  EXPERIMENTAL SETUP\n\nDataset.  To evaluate the impact of prompt optimization on LRMs, we conduct experiments on the\nwidely used ACE05 dataset (Doddington et al., 2004), a standard benchmark for EE that provides\nfine-grained event distinctions. We used the “split 1” preprocessed by Huang et al. (2024) and\nfurther processed it into the Python code format. The original ACE05 dataset includes 33 event types.\nHowever, our preliminary exploration found that including all 33 event types for prompt optimization\ncould lead to overly long prompts, which both LLMs and LRMs cannot properly handle. To eliminate\nthe impact of this confounding factor while assessing whether LRMs require and facilitate prompt\noptimization, we downsampled a subset of 10 event types in our experiments and left the issue of\nlong-context processing as future work.\n\n\n                                       4\n\nPreprint. Under review.\n\n\n\n\nWe utilize two smaller versions of ACE05 training set in our experiments as Dtrain. To simulate\nlow-resource conditions, we construct ACElow of 15 samples, where we select one instance per\nevent type, prioritizing those with higher densities of event and argument annotations (i.e., training\nexamples annotated with multiple event instances); the remaining samples are non-event instances.\nTo examine the effect of scaling up the training size, we also construct a medium-scale dataset,\nACEmed, comprising 120 examples—ten per event type—with the remaining being non-event\ninstances. For both settings, we use a consistent development set of 100 examples randomly sampled\nfrom the ACE05 development set and focus our discussions about various task and optimizer models’\nperformance on this set. For the full MCTS, we additionally report the model performance on a test\nset consisting of 250 examples randomly sampled from the ACE05 test set. Dataset statistics for\nACElow and ACEmed are summarized in Table 4 (Appendix A).\n\nTo test generalization beyond EE, we additionally include two tasks: Geometric Shapes (Suzgun\net al., 2022), a symbolic reasoning benchmark, and NCBI Disease NER (Do˘gan et al., 2014), a\nbiomedical named entity recognition task.\n\nEvaluation.  Following Huang et al. (2024), on EE, we evaluate models using four F1-based metrics:\n(1) Trigger Identification (TI), which measures the correct extraction of trigger spans; (2) Trigger\nClassification (TC), which additionally requires predicting the correct event type; (3) Argument\nIdentification (AI), which assesses the correct extraction of arguments and their association with\nthe predicted trigger; and (4) Argument Classification (AC), which further requires correct role\nlabeling and serves as the most comprehensive measure of overall end-to-end EE performance. For\nanalysis, we primarily report AC scores, which are widely regarded as a precise metric for evaluating\nboth argument and trigger quality (Huang et al., 2024). Full results for all EE metrics are provided\nin Appendix B. For Geometric Shapes, we report test accuracy; for NCBI Disease NER, we report\nmicro-F1 on strict disease spans.\n\nExperimental Settings and Baselines.  Our experiments involve two LRMs, DeepSeek-R1 and\nOpenAI-o1, and two general-purpose LLMs, GPT-4.5 and GPT-4o, used both as Mopt and Mtask.\nWe conduct two sets of experiments. First, we evaluate all models trained on ACElow and ACEmed\nusing shallow MCTS (depth 1) to examine whether LRMs benefit from prompt optimization. We\nstarted with this design choice owing to its reduced complexity and computational costs. Next, we\nthen perform full MCTS (depth 5) optimization on ACEmed to investigate the deeper dynamics of\noptimization; ACElow is excluded from full-scale search due to its limited size. In each depth of\nrollout, we expand the parent node by three child expansions. For all our experiments, we report\nresults only from the best-performing prompt nodes in each model’s search trajectory. To reduce the\ninference cost, we followed Cheng et al. (2023) to employ “batch prompting” when querying Mtask\nfor answer generation (Step 1 in Fig. 3). Interestingly, we observed a performance gain than querying\nthe task model for one question at a time. Due to policy restrictions, we were not allowed to access\nDeepSeek-R1 through API calls and thus deployed it locally on our own server. Because of our\ncompute limit, we quantize DeepSeek-R1 to 2.5 bits using the UnSloth framework, which has shown\nminimal degradation in reasoning tasks even at lower precisions when rigorously benchmarked to\n1.58 bits (Daniel Han & team, 2023). Additional details on batch prompting and hyperparameter\nconfigurations are provided in Appendix A.\n\n\n4.2  EXPERIMENTAL RESULTS\n\nOur main results are presented in Table 1. We discuss the following research questions (RQs).\n\nRQ1: Do LRMs benefit from prompt optimization in EE?  We first study whether the models\ncan gain from prompt optimization by performing MCTS at depth 1. We observe consistent gains\nfrom prompt optimization across all models, with LRMs showing especially strong improvements\nover their non-optimized counterparts (around +8% on ACElow and +23% on ACEmed). LLMs also\nbenefit from optimization, though to a lesser extent: GPT-4o and GPT-4.5 improve by around +7%\nand +5% on ACElow, and by +14% and +20% on ACEmed, respectively. Overall, the performance\ngains from prompt optimization are more pronounced in LRMs than in LLMs.\n\nSimilarly, in cross-model comparisons using optimized prompts, LRMs remain highly competitive.\nOn ACElow, GPT-4.5 slightly outperforms o1 by about +1% AC but trails behind DeepSeek-R1 by\n\n\n                                       5\n\nPreprint. Under review.\n\n\n\n\n\n                                      Optimizer LLMs/LRMs (Mopt)               #Output\n          Mtask        No Opt.    GPT-4o    GPT-4.5       o1      DS-R1     Tokens\n                   MCTS at depth 1 trained on ACElow (Development Set)\n\n          GPT-4o          12.68     18.18 +5.50   16.67 +3.99   18.83 +6.15   20.15 +7.47      15.31\n           GPT-4.5         16.47     19.33 +2.86   16.47 00.00   19.32 +2.85   22.31 +5.84      24.57\n           o1               13.94     18.96 +5.02   18.57 +4.63   20.29 +6.35   21.92 +7.98     489.67\n          DS-R1           16.45     18.67 +2.22   18.57 +2.12   21.83 +5.38   24.66 +8.21:::     217.71\n                   MCTS at depth 1 trained on ACEmed (Development Set)\n\n          GPT-4o          12.68     22.32 +9.64   27.54 +14.86  26.30 +13.62  25.10 +12.42     17.31\n           GPT-4.5         16.47     29.63 +13.16  35.94 +19.47  36.51 +20.04  35.42 +18.95     28.75\n           o1               13.94     30.19 +16.25  36.67 +22.73  36.98 +23.04  36.96 +23.02    543.45\n          DS-R1           16.45     32.20 +15.75  37.14 +20.69  38.77 +22.32  40.00 ::::+23.55    277.11\n                   MCTS at depth 5 trained on ACEmed (Development Set)\n\n          GPT-4o          12.68     28.04 +15.36  27.03 +14.35  28.57 +15.89  27.31 +14.63     17.55\n           GPT-4.5         16.47     32.35 +15.88  37.58 +21.11  36.22 +19.75  37.74 +21.27     32.65\n           o1               13.94     33.52 +19.58  37.78 +23.84  38.71 +24.77  39.81 +25.87    575.36\n          DS-R1           16.45     37.97 +21.52  38.40 +21.95  40.58 +24.13  44.26 ::::+27.81    301.45\n                      MCTS at depth 5 trained on ACEmed (Test Set)\n\n          GPT-4o          13.33     26.94 +13.61  34.75 +21.42  30.59 +17.26   35.79+22.46     27.00\n           GPT-4.5         14.29     27.31 +13.02  35.29 +21.00  36.59 +22.30  36.69 +22.40     35.56\n           o1               15.38     28.57 +13.19  36.73 +21.35  38.71 +23.33  37.86 +22.48    526.43\n          DS-R1           16.00     31.93 +15.93  41.98 +25.98  42.06 +26.06  43.75 ::::+27.75    211.43\n\nTable 1: AC (F1) scores using different Mtask and Mopt. #Output Tokens delineates the average\nnumber of output tokens from the task model, including reasoning and non-reasoning contents. The\nbackground shades indicate the choice of prompt optimizers, i.e., LRMs, LLMs, or no optimization.\nThe best optimization result is in bold for each task model, while the highest relative improvement\nover the no-optimization baseline is::::::::::underlined. We observe that LRMs not only benefit significantly\nfrom prompt optimization but also serve as strong prompt optimizers for other models.\n\n\nroughly +2%. On ACEmed, both LRMs outperform LLMs: o1 surpasses GPT-4.5 by +0.5% AC,\nand DeepSeek-R1 gains over approximately +3.5%. These findings suggest that LRMs are not only\nmore responsive to prompt optimization but also more capable in zero-shot EE settings. As we show\nin RQ2, this gap widens further when using the full-depth MCTS-based optimization strategy.\n\n   Insight 1: Prompt optimization benefits all models, but LRMs gain more, no matter whether\n   small and medium-sized training data is present.\n\n\nRQ2: How do LRMs perform under full-scale MCTS prompt optimization?  To assess whether\nthe advantages of LRMs persist at scale, we perform MCTS with a search depth of 5 across all\nmodels on ACEmed. While performance improves overall, we observe that the gains from full-scale\noptimization are incremental rather than dramatic when compared to the improvements observed\nwith a single roll-out (i.e., depth 1) of MCTS. LRMs, however, still exhibit relatively stronger\nimprovements. DeepSeek-R1, for instance, gains an additional +4.26% AC over its previous best\n(40.00 7→44.26). Similarly, o1 improves by +2.83% (36.98 7→39.81) when selecting the best\noptimizer across depths. In contrast, LLMs GPT-4.5 and GPT-4o show modest gains of only +1.23%\n(36.51 7→37.74) and +1.03% (27.54 7→28.57), respectively. Finally, we report each task model’s\nperformance on the test set, using the same best prompt searched on ACEmed. Consistently, we\nobserved that LRMs benefit more from full MSTC prompt optimization than LLMs.\n\n   Insight 2: Full-scale MCTS optimization yields non-dramatic gains over single-step optimiza-\n    tion, but LRMs benefit more.\n\nRQ3: Do LRMs make better prompt optimizers?  We evaluate each task model’s performance\nwhen optimized using various LRMs and LLMs to investigate the quality of optimized prompts. In the\nlow-resource setting (ACElow, Depth 1), DeepSeek-R1 consistently outperforms all other optimizers\nacross all task models. Compared to the best-performing LLM optimizer (GPT-4o), DeepSeek-R1\nyields substantial gains: about +2% AC for optimizing GPT-4o (18.187→20.15), +3% for GPT-4.5\n(19.337→22.31) and o1 (18.967→21.92), and +6% when optimizing itself (18.67→24.66). Notably,\n\n\n                                       6\n\nPreprint. Under review.\n\n\n\n\n\n                               Examples of Task Instructions Optimized by Different Models\n\n  NO OPTI-   # This is an event extraction task where the goal is to extract structured events from the text following structured event definitions\n  MIZATION    in Python. (...) For each different event type, please output the extracted information from the text into a python list format (...)\n  Best Scores    you should always output in a valid pydantic format: result = [EventName(”mention” = ”trigger”, ”arg1 key” = ”arg1 span”, ...),\n   TI - 39.29\n  TC - 33.93    EventName(”mention” = ”trigger”, ”arg1 key” = ”arg1 span”, ...)]. (...)\n  AI - 16.47\n AC - 16.47\n\n  GPT-4O    # This is an event extraction task where the goal is to extract structured events (...)\n  Best Scores    # Task Instructions: 1. For each different event type, output the extracted information from the text (...)\n   TI - 48.28      2. Structure the output in a valid Pydantic format: ‘result = [EventName(”mention” = ”trigger”, (...).\n  TC - 48.28\n  AI - 40.51      3. Adhere strictly to the described event descriptions (...).\n AC - 37.97      4. Address special cases:- Appeals: Consider involved parties from prior related events as “prosecutor”.\n                     - Multiple roles may apply contextually; ensure complete information extraction.\n                     - Implicit indications: If mentions like ”filed”, ”concluded”, etc.,(...) use context to clarify them.(...)\n\n  GPT-4.5    # This is an event extraction task for identifying and structuring events from text using Python-defined event classes. Each\n  Best Scores    structured event consists of an event trigger word, an event type (...)\n   TI - 46.15    ## Instructions:\n  TC - 46.15\n  AI - 40.80      1. Span Extraction:\n AC - 38.40      - Extract precise and concise spans for mentions and arguments, conveying the event or argument role clearly (...)\n                    - Accurately identify roles using contextual cues, effectively resolving ambiguities while prioritizing explicit spans. If roles are\n               unmentioned, leave them empty. (...)\n                   3. Output Format: Please follow the Python-format(...)\n                   4. Clarifications and Exceptions:- Note explicitly when roles have exceptions based on role definitions.\n                    - Manage overlapping roles by following specific guidelines for span clarity and precision, (...)\n\n DEEPSEEK-  # Event Extraction Task: Extract structured events from text using Python class definitions.(...):\n   R1          1. Span Extraction:- Triggers: Minimal contiguous spans (verbs/nouns) directly expressing the event. Include both verbal and\n  Best Scores    nominal forms (”death” = Die, ”killings” = Die).(...)\n   TI - 56.60\n  TC - 56.60      - Arguments: - Remove articles (”a/an/the”) and possessive pronouns EXCEPT when part of official names or temporal phrases\n  AI - 44.26    (”The Hague”, ”the past year”)\n AC - 44.26      - Resolve pronouns AND POSSESSIVE NOUNS to named entities immediately using same-sentence antecedents (”airline’s\n               plan” →[”airline”])\n                     - Strip role/location/age descriptors from arguments (”Philadelphia lawyers” →”lawyers”) (...)\n                     - Keep FULL spans for crimes/money including sources/amounts (”stereo worth $1,750 from family”) unless legal terms (...)\n                   2. Special Handling:- Bankruptcy Triggers: ”went bust” →EndOrg(...)\n                     - Crime Spans: Retain full contextual clauses (”If convicted of killings...”) without truncation\n                     - Temporal Phrases: Keep original spans with articles when part of phrase (”the early 90’s”)\n                   3. Output Rules: Always output in Python-format as (...)\n                   4. Critical Exceptions:-(...)\n\n   O1        # This is an event extraction task where the goal is to extract structured events from the text following structured event definitions\n  Best Scores     in Python. (...)\n   TI - 66.67    Keep argument references minimal by removing articles, possessives, or descriptive words unless they are crucial identifiers (e.g.,\n  TC - 66.67\n  AI - 44.93     ”the retailer” →”retailer”, ”my uncle” →”uncle”).\n AC - 40.58    # Important guidelines to address prior errors:\n              # 1. For each event trigger, use the single most relevant word (e.g., ”bankruptcy” rather than ”file for bankruptcy”).\n              # 2. For argument roles, also use minimal spans (e.g., ”soldier” instead of ”a soldier,” ”woman” instead of ”a woman”).(...)\n              # 4. For justice events (Sue, Appeal, Convict, SentenceAct, etc.): (...)\n              # 5. For transfers of money, watch for direct or indirect references to donations, (...)\n              # 6. Do not skip events implied by synonyms or indirect wording (e.g., ”shutting down” →EndOrg, (...).\n              # 7. If there is more than one event in a single text, output each in a separate entry.(...)\n\nTable 2: Example task instructions optimized by different optimizers when Mtask = DeepSeek-R1,\nwhich yielded the best performance for each optimizer. LRMs tend to emphasize actionable extraction\nrules and exception handling, while paying minimal attention to the task instruction and output format.\nAdditionally, they often include illustrative examples (in bold) to facilitate span extraction.\n\namong LLMs, GPT-4o performs better than GPT-4.5 as an optimizer in all task model settings, despite\nbeing weaker as a task model.\n\nOn the other hand, when a larger training set is available (ACEmed, Depth 1), we observe a shift.\nWhile LRM optimizers remain strong—achieving over +23% AC gain while optimizing themselves—\nGPT-4.5 shows a significant boost in effectiveness. It consistently outperforms GPT-4o as an optimizer\nand in some cases narrows the gap with LRMs, reaching 35.94 when optimizing itself and 36.67 when\noptimizing o1.  Qualitatively, as shown in Table 2, DeepSeek-R1 enhances the optimized prompt P∗\nby adding precise extraction rules, such as removing articles (“a/an/the”) and possessive pronouns\n(highlighted in blue), as well as critical exception cases for handling specific triggers (highlighted\nin pink). In contrast, o1 tends to generate a larger number of extraction rules, resulting in longer\nprompts. Both LRMs also include specific examples to guide extraction. LLMs, by comparison,\nfocus more on task instructions and output formatting, typically generating shorter prompts with\nfewer examples. Among them, GPT-4.5 occasionally adds exception handling, though this behavior\nis less consistent than in LRMs. We provide additional examples of optimized task instruction and\nevent guidelines in Appendix C, and include an additional analysis of the prompt quality in Section 5.\n\n\n                                       7\n\nPreprint. Under review.\n\n\n\n\n\n                                                                               40\n                                40\n                                                                               30\n                                30\n                      AC                                 AC20\n                                20                             DeepSeek-R1                                     DeepSeek-R1\n                                                        O1                                       O1\n                                10                                GPT-4.5          10                                GPT-4.5\n                                                                    GPT-4o                                            GPT-4o\n                                 0  0      1      2      3      4      5      0  0      1      2      3      4      5\n                                                             Tree Depth                                              Tree Depth\n\n                           (a) Mopt =DeepSeek-R1           (b) Mopt =GPT-4.5\n\nFigure 4: Convergence analysis of prompt optimization across different task models with two\noptimizers—DeepSeek-R1 (left) and GPT-4.5 (right). Task models converge faster with minimal\nvariance when their prompts are optimized by LRMs.\n\n   Insight 3: LRMs serve as highly effective optimizers, especially in low-resource settings\n   where DeepSeek-R1 consistently outperforms all others as a prompt optimizer.\n\nRQ4: Can LRMs act as efficient and stable optimizers in prompt optimization?  In Fig. 4a,\nwe observe that with DeepSeek-R1 as an optimizer, DeepSeek-R1 and GPT-4o demonstrate faster\nconvergence compared to when GPT-4.5 is used as an optimizer (Fig. 4b), suggesting that it generates\na higher quality of prompts. For DeepSeek-R1 and GPT-4.5 as task models, it also exhibits a smaller\nperformance variance, which shows that R1 not only generates high-quality prompts but also does\nso reliably. In contrast, with GPT-4.5 as an optimizer, convergence tends to be slower. Under this\nsetup, both LRMs reach their peak at depth 3, while GPT-4.5 and GPT-4o converge at depths 4 and\n5, respectively. For GPT-4.5, the optimization process is visibly less stable than optimizing with\nDeepSeek-R1. Finally, we notice that most models begin to plateau, or slightly decline, beyond their\noptimal depth (marked using half-filled markers), reinforcing the presence of diminishing returns,\nwhere additional optimization yields increasingly smaller or no performance gains.\n\n   Insight 4: DeepSeek-R1 (LRM) as an optimizer yields faster and more stable convergence\n   than GPT-4.5 (LLM).\n\n\nRQ5: Do the optimization gains with LRMs    Model    No Opt. (Test)      Depth 1 (Dev)      Depth 5 (Dev)      Depth 5 (Test)\ngeneralize beyond schema-based tasks?                         (a) Symbolic Reasoning — Geometric Shapes (Accuracy)\nWe further experimented on two tasks: Ge-    GPT-4oGPT-4.5        53.4069.96             61.2072.90 +7.80+2.94:::          68.6775.33 +15.27+5.37::::         67.5074.20 +14.10+4.24::::\nometric Shapes and NCBI, and reported each     o1DS-R1          70.0769.67             73.5073.80 +3.43+4.13           78.0078.67 +7.93+9.00           77.8078.40 +7.73+8.73\ntask model’s performance when we use the                          (b) Biomedical IE — NCBI Disease NER (Micro-F1)\nsame model as an optimizer. As shown in Ta-    GPT-4oGPT-4.5        43.7556.25             49.5058.67 +5.75+2.42          54.3765.56 +10.62+9.31           52.6364.56 +8.88+8.31\n                                                                                                                                                                                                 +18.33                                                                                                                                                           70.15                                                                                                              ::::         71.46                                                                                                                                                                                                                            +17.02::::\n                                                                                                                                                                                                 +17.20ble 3, on both tasks, we observe that prompt     o1DS-R1          53.1354.20            66.4666.00 +13.33+11.80                                                                                                                                       71.40                                                                                                                                                           69.96 +15.76\noptimization consistently improves all mod-\nels. On Geometric Shapes, o1 and DeepSeek-  Table 3: Results on symbolic reasoning and biomed-\nR1 reach test accuracies of 77.80 and 78.40,  ical NER tasks. Overall, LRMs benefit most from\noutperforming GPT-4.5 (74.20) and GPT-4o  prompt optimization.\n(67.50). While GPT-4o achieves a larger rela-\ntive gain (+14.1), LRMs still achieve higher absolute performance. In NCBI, LRMs show strong gains\nand high final performance: o1 and DeepSeek-R1 improve by +17.0% and +15.8% F1, respectively,\nreaching 70.15 and 69.96, well above the LLM performance. These results mirror our findings on EE,\nreinforcing that LRMs not only serve as strong task models post-optimization but also generalize\neffectively as optimizers beyond schema-based tasks.\n\n   Insight 5: Prompt optimization benefits transfer across tasks: LRMs gain benefit on both\n   symbolic reasoning and biomedical NER.\n\n\n5  FURTHER ANALYSIS\n\nPrompt Quality Across Optimizers  In addition to our qualitative analysis about Table 2 in RQ3,\nwe also analyze the distribution of prompt effectiveness using a survival plot with DeepSeek-R1 as\nMtask. The x-axis represents increasing AC thresholds, while the y-axis indicates the percentage\nof prompts that achieve at least that threshold. A higher survival curve indicates that an optimizer\nmore consistently produces high-performing prompts. As shown in Fig. 5a, prompts optimized via\n\n\n                                       8\n\nPreprint. Under review.\n\n\n\n\n\n                                                                  50\n                       DeepSeek-R1        o1         GPT-4.5        GPT-4o                                                                                            GPT-4o\n                                                                  45                                                   80\n           100\n                                                                  40\n            80     AC                                                                                                                    70                                                                                                                                                                 GPT-4.5                                                                                                                                                                                                                                                               Errors                                                                  35\n                                                                                     of                                                                                                                     O1\n         with                                                                  30            60                                                                                           AC-F1\n                                                                                                                    60 DS-R1                                  Parsing\n                                                                                                                                                                                                              Hallucinations            40                                                   25                                 O1                        Number\n                                                                                                                                                                                                                    Total50                                            MultipleLabel NoiseEvents                Prompts 20                                                   20                                      DeepSeek-R1GPT-4o                                                                      Errors\n  %  0                                                   15                                           GPT-4.5                                                       Coreferences\n                                                                                                                                                                     Span                                                                                                                                                                                                         Overprediction               10                        20                                  30                                           40       50         1500  1750  2000  2250                                                                                            2500  2750  3000  3250       40                                                                                                                                                                                                                                  Implicit                                                                                                                                                                                               Events               Argument                               Classification                                       Threshold                                                                                                    Length\n\n                      (a)                                (b)                                 (c)\n\nFigure 5: (a) A survival plot showing the % of prompts (y-axis) that achieve at least a given AC score\n(x-axis) for DeepSeek-R1 across different optimizers. (b) Prompt length vs. AC score across the\nbest-performing full MCTS configuration for each task model on dev set. (c) Error categorization for\nDeepSeek-R1 as the task model with various optimizers.\n\nDeepSeek-R1 exhibit the strongest survival curve, maintaining high-performance density even at\nstricter AC cutoffs (≥35% AC). In contrast, GPT-4o’s curve decays rapidly, showing that while\nit occasionally generates effective prompts, its output quality is inconsistent. Interestingly, o1 and\nGPT-4.5 fall in between, with o1 slightly outperforming GPT-4.5 in the mid-range thresholds but\ntrailing DeepSeek-R1 significantly at higher cutoffs. These trends reinforce our earlier findings:\nreasoning models are not only capable of producing better peak performance but also generate a\ngreater density of usable prompts.\n\n\nPrompt Length vs. Task Model Performance  To better understand how much instruction is\nneeded for different task models to reach their peak performance, we analyze the relationship between\nprompt length and model accuracy across full MCTS search trees. For each model, we select its\nbest-performing search trajectory (i.e., o1 as optimizer for GPT-4o and DeepSeek-R1 as optimizer for\nthe other task models) and plot the corresponding full prompt lengths (including inherited definitions)\nagainst their AC scores in Fig. 5b. DeepSeek-R1 achieves its highest performance utilizing the\nshortest prompt (∼1750 tokens) in the search space, suggesting a preference for more concise task\ninstructions. In contrast, both LLMs (GPT-4o and GPT-4.5) and the reasoning model o1 tend to rely\non significantly longer prompts to achieve comparable accuracy.\n\n\nError Analysis  To better understand the types of errors introduced by different optimizers, we\nconduct a fine-grained analysis of all development examples where DeepSeek-R1 fails on prompts\ngenerated by different optimizers. As shown in Fig. 5c, LRMs notably reduce event-related errors,\nparticularly those involving multiple or implicit events. Argument-related issues, such as coreference\nerrors and span overprediction, are also slightly reduced. In some cases, all models produce non-\nparsable outputs or hallucinated argument spans. The remaining errors are primarily attributed to\nlabel noise in the dataset. We provide an example for each error category in Appendix B.\n\n   Insight 6: LRM-optimized prompts are enriched with new extraction rules absent from the\n    original task instruction, directly addressing frequent errors. DeepSeek-R1 achieves its highest\n   performance using the shortest prompt.\n\n\n6  CONCLUSION\n\n\nWe present the first systematic study of prompt optimization for LRMs, evaluating their roles as\nboth task models and optimizers in a unified MCTS framework. On the structured task of event\nextraction, we find that LRMs benefit more from prompt optimization than LLMs and serve as stronger\noptimizers. They produce higher-quality prompts, converge faster, and generalize more reliably\nacross models, highlighting their effectiveness in both prompt consumption and generation. Our error\nanalysis further reveals that prompts optimized by LRMs reduce overprediction, hallucination, and\nparsing errors, contributing to more faithful and structured outputs. These trends generalize beyond\nevent extraction: on Geometric Shapes and NCBI Disease NER, optimization improves all models,\nwith LRMs outperforming LLMs when serving as their own optimizers. This strengthens our claim\nthat LRMs both profit from and serve as strong agents for prompt optimization across diverse tasks.\n\n\n                                       9\n\nPreprint. Under review.\n\n\n\n\nREFERENCES\n\nEshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav Magazine, Tanuja Ganu, and Akshay Nambi.\n  Promptwizard: Task-aware prompt optimization framework, 2024. URL https://arxiv.\n  org/abs/2405.18369.\n\nZhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language\n  model APIs. In Mingxuan Wang and Imed Zitouni (eds.), Proceedings of the 2023 Conference on\n  Empirical Methods in Natural Language Processing: Industry Track, pp. 792–810, Singapore, De-\n  cember 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-industry.\n  74. URL https://aclanthology.org/2023.emnlp-industry.74/.\n\nMichael Han Daniel Han and Unsloth team.  Unsloth, 2023. URL http://github.com/\n  unslothai/unsloth.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song,\n  Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement\n   learning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Con-\n  ference on Empirical Methods in Natural Language Processing, pp. 3369–3391, Abu Dhabi, United\n  Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n  2022.emnlp-main.222. URL https://aclanthology.org/2022.emnlp-main.222/.\n\nGeorge Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and\n  Ralph Weischedel. The automatic content extraction (ACE) program – tasks, data, and evaluation.\n  In Maria Teresa Lino, Maria Francisca Xavier, F´atima Ferreira, Rute Costa, and Raquel Silva\n   (eds.), Proceedings of the Fourth International Conference on Language Resources and Evaluation\n  (LREC‘04), Lisbon, Portugal, May 2004. European Language Resources Association (ELRA).\n  URL https://aclanthology.org/L04-1011/.\n\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong Lu. Ncbi disease corpus: a resource for disease\n  name recognition and concept normalization. Journal of biomedical informatics, 47:1–10, 2014.\n\nChrisantha Fernando, Dylan Sunil Banarse, Henryk Michalewski, Simon Osindero, and Tim\n  Rockt¨aschel.  Promptbreeder:  Self-referential self-improvement via prompt evolution, 2024.\n  URL https://openreview.net/forum?id=HKkiX32Zw1.\n\nJun Gao, Huan Zhao, Wei Wang, Changlong Yu, and Ruifeng Xu. Eventrl: Enhancing event extraction\n  with outcome supervision for large language models. 2024.\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\n  Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\n  via reinforcement learning. 2025.\n\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,\n  and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\n  prompt optimizers. In The Twelfth International Conference on Learning Representations, 2024.\n  URL https://openreview.net/forum?id=ZG3RaNIsO8.\n\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n  and Weizhu Chen. LoRA: Low-rank adaptation of large language models.  In International\n  Conference on Learning Representations, 2022. URL https://openreview.net/forum?\n  id=nZeVKeeFYf9.\n\nKuan-Hao Huang, I-Hung Hsu, Tanmay Parekh, Zhiyu Xie, Zixuan Zhang, Prem Natarajan, Kai-Wei\n  Chang, Nanyun Peng, and Heng Ji. Textee: Benchmark, reevaluation, reflections, and future\n  challenges in event extraction. In Findings of the Association for Computational Linguistics ACL\n  2024, pp. 12804–12825, 2024.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant.  The power of scale for parameter-efficient\n  prompt tuning.  In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-\n  tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-\n  guage Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November\n  2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL\n  https://aclanthology.org/2021.emnlp-main.243/.\n\n\n                                       10\n\nPreprint. Under review.\n\n\n\n\n\nPeng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, and Xipeng Qiu.\n  CodeIE: Large code generation models are better few-shot information extractors. In Anna Rogers,\n  Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pp. 15339–15353, Toronto,\n  Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.\n  855. URL https://aclanthology.org/2023.acl-long.855/.\n\nWenwu Li, Xiangfeng Wang, Wenhao Li, and Bo Jin. A survey of automatic prompt engineering: An\n  optimization perspective, 2025. URL https://arxiv.org/abs/2502.11560.\n\nXiang Lisa Li and Percy Liang.  Prefix-tuning: Optimizing continuous prompts for generation.\n  In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th\n  Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\n  Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, Online,\n  August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353.\n  URL https://aclanthology.org/2021.acl-long.353/.\n\nZixuan Li, Yutao Zeng, Yuxin Zuo, Weicheng Ren, Wenxuan Liu, Miao Su, Yucan Guo, Yantao Liu,\n  Lixiang Lixiang, Zhilei Hu, Long Bai, Wei Li, Yidan Liu, Pan Yang, Xiaolong Jin, Jiafeng Guo,\n  and Xueqi Cheng. KnowCoder: Coding structured knowledge into LLMs for universal information\n   extraction. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd\n  Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n  8758–8779, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi:\n  10.18653/v1/2024.acl-long.475. URL https://aclanthology.org/2024.acl-long.\n  475/.\n\nAgustin Mantaras. Prompt engineering for openai’s o1 and o3-mini reasoning models. Microsoft\n  Tech Community Blog, February 2025. URL https://techcommunity.microsoft.\n  com/blog/azure-ai-services-blog/prompt-engineering-for-openai%\n  E2%80%99s-o1-and-o3-mini-reasoning-models/4374010.\n\nHector D. Menendez, Gema Bello-Orgaz, and Cristian Ram´ırez Atencia. Deepstableyolo: Deepseek-\n  driven prompt engineering and search-based optimization for AI image generation.  In XVI\n  Congreso Espa˜nol de Metaheur´ısticas, Algoritmos Evolutivos y Bioinspirados, 2025. URL https:\n  //openreview.net/forum?id=hZucDPawRu.\n\nOpenAI. Reasoning best practices. OpenAI Platform Documentation, April 2025. URL https:\n  //platform.openai.com/docs/guides/reasoning-best-practices#\n  how-to-prompt-reasoning-models-effectively.\n\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal.  GrIPS: Gradient-free, edit-based\n   instruction search for prompting large language models. In Andreas Vlachos and Isabelle Au-\n  genstein (eds.), Proceedings of the 17th Conference of the European Chapter of the Associ-\n  ation for Computational Linguistics, pp. 3845–3864, Dubrovnik, Croatia, May 2023. Asso-\n   ciation for Computational Linguistics.  doi: 10.18653/v1/2023.eacl-main.277. URL https:\n  //aclanthology.org/2023.eacl-main.277/.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\n  optimization with “gradient descent” and beam search. In Houda Bouamor, Juan Pino, and Kalika\n  Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\n  Processing, pp. 7957–7968, Singapore, December 2023. Association for Computational Linguis-\n   tics. doi: 10.18653/v1/2023.emnlp-main.494. URL https://aclanthology.org/2023.\n  emnlp-main.494/.\n\nOscar Sainz, Iker Garc´ıa-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, and Eneko\n  Agirre. GoLLIE: Annotation guidelines improve zero-shot information-extraction. In The Twelfth\n  International Conference on Learning Representations, 2024. URL https://openreview.\n  net/forum?id=Y3wpuxd7u9.\n\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt:\n   Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Bonnie\n  Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on\n\n\n                                       11\n\nPreprint. Under review.\n\n\n\n\n\n  Empirical Methods in Natural Language Processing (EMNLP), pp. 4222–4235, Online, November\n  2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL\n  https://aclanthology.org/2020.emnlp-main.346/.\n\nSaurabh Srivastava, Chengyue Huang, Weiguo Fan, and Ziyu Yao.  Instances need more care:\n  Rewriting prompts for instances with LLMs in the loop yields better zero-shot performance.\n  In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for\n  Computational Linguistics: ACL 2024, pp. 6211–6232, Bangkok, Thailand, August 2024.\n  Association for Computational Linguistics.  doi:  10.18653/v1/2024.findings-acl.371. URL\n  https://aclanthology.org/2024.findings-acl.371/.\n\nSaurabh Srivastava, Sweta Pati, and Ziyu Yao. Instruction-tuning llms for event extraction with\n  annotation guidelines, 2025. URL https://arxiv.org/abs/2502.16377.\n\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\n  Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\n  and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\n\nTogether AI. Prompting deepseek-r1. Together AI Documentation, February 2025. URL https:\n  //docs.together.ai/docs/prompting-deepseek-r1.\n\nGuoqing Wang, Zeyu Sun, Zhihao Gong, Sixiang Ye, Yizhou Chen, Yifan Zhao, Qingyuan Liang, and\n  Dan Hao. Do advanced language models eliminate the need for prompt engineering in software\n  engineering? 2024a.\n\nXingyao Wang, Sha Li, and Heng Ji. Code4struct: Code generation for few-shot event structure\n   prediction. In Proceedings of the 61st Annual Meeting of the Association for Computational\n  Linguistics (Volume 1: Long Papers), pp. 3640–3663, 2023a.\n\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric\n  Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level\n  prompt optimization. In The Twelfth International Conference on Learning Representations, 2024b.\n  URL https://openreview.net/forum?id=22pyNMuIoa.\n\nZhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. Multitask\n  prompt tuning enables parameter-efficient transfer learning. In The Eleventh International Confer-\n  ence on Learning Representations, 2023b. URL https://openreview.net/forum?id=\n  Nk2pDtuhTq.\n\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein.\n  Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.\n  In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:\n  //openreview.net/forum?id=VOstHxDdsN.\n\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang Yanggang, Haiyu Li, and Zhilin Yang. GPS:\n  Genetic prompt search for efficient few-shot learning. In Yoav Goldberg, Zornitsa Kozareva,\n  and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natu-\n   ral Language Processing, pp. 8162–8171, Abu Dhabi, United Arab Emirates, December 2022.\n  Association for Computational Linguistics.  doi: 10.18653/v1/2022.emnlp-main.559. URL\n  https://aclanthology.org/2022.emnlp-main.559/.\n\nWeijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. Reprompting: Automated chain-of-thought\n  prompt inference through gibbs sampling, 2024. URL https://arxiv.org/abs/2305.\n  09993.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\n  Chen. Large language models as optimizers. In The Twelfth International Conference on Learning\n  Representations, 2024. URL https://openreview.net/forum?id=Bb4VGOWELI.\n\nXunjian Yin, Xinyi Wang, Liangming Pan, Xiaojun Wan, and William Yang Wang. G¨odel agent: A\n   self-referential agent framework for recursive self-improvement, 2025. URL https://arxiv.\n  org/abs/2410.04444.\n\n\n                                       12\n\nPreprint. Under review.\n\n\n\n\n\n Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E. Gonzalez. TEMPERA:\n   Test-time prompt editing via reinforcement learning.  In The Eleventh International Confer-\n   ence on Learning Representations, 2023. URL https://openreview.net/forum?id=\n   gSHyqBijPFO.\n\n\n Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu,\n   Yanjun Lyu, Peng Shu, Xiaowei Yu, et al. Evaluation of openai o1: Opportunities and challenges\n   of agi. 2024.\n\n\n Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\n   Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International\n   Conference on Learning Representations, 2022.\n\n\n\n A  ADDITIONAL DETAILS\n\n\n A.1  MORE IMPLEMENTATION DETAILS\n\n To effectively optimize prompts for task-specific performance, we adopt a Monte Carlo Tree Search\n (MCTS) framework that iteratively explores and refines prompts based on model feedback and reward\n signals. The proposed algorithm, outlined in Algorithm 1, combines structured exploration with\n guided optimization by leveraging a task model, a feedback-generating optimizer, and a reward\n function. At each iteration, the algorithm performs selection, expansion, simulation, and back-\n propagation steps, progressively improving the prompt to maximize task performance across sampled\n batches.\n\n\n Algorithm 1 Algorithm for MCTS-based Prompt Optimization\n   Inputs:\n         Initial prompt s0 = P0, task model Mtask, optimizer Mopt, reward function R, batch size k, depth limit\n      L, iterations τ, exploration weight c\n    Initialize:\n        State-action mapping A : S 7→F, children mapping ch : S × F 7→S, rewards r : S × F 7→R,\n       Q-values Q : S × F 7→R, visit count N  : S 7→N\n    for n ←0, . . . , τ −1 do\n      Sample batch (Qbatch, Abatch) from training data\n       for t ←0, . . . , L −1 do\n               if A(st) is not empty then                                                          ▷selection\n                     q  ln N (st)\n               ft ←arg maxf∈A(st)  Q(st, f) + c ·  N (ch(st,f))\n             st+1 ←ch(st, ft), rt ←r(st, ft), N(st) ←N(st) + 1\n            else                                                         ▷expansion and simulation\n              (Step 1) Answer Gen: ˆQbatch ∼Mtask(Qbatch, st)\n          (Step 2) Error Extract: Identify errors using interpreter on ˆAbatch\n          (Step 3) Feedback Gen: ft ∼Mopt(feedback|st, errors)\n          (Step 4) Prompt Update: st+1 ∼Mopt(s|st, ft)\n         Update A(st) ←{ft}, ch(st, ft) ←st+1, r(st, ft) ←R(ˆAbatch, Abatch)\n            rt ←r(st, ft), N(st) ←N(st) + 1\n      end if\n          if st+1 is an early-stopping state then\n         break\n      end if\n   end for\n  T ←number of steps\n    for t ←T −1, . . . , 0 do                                                       ▷back-propagation\n      Update Q(st, ft) with rollout rewards {rt, . . . , rL}\n   end for\nend for\n\n\n\n                                        13\n\nPreprint. Under review.\n\n\n\n\n\n                                            Train     Train\n                                                       Dev\n                                     ACElow  ACEmed\n\n                       TransferMoney       3        13      29\n                      Meet                2        15      13\n                       PhoneWrite          1        11       1\n                         SentenceAct         6        25       4\n                        Appeal              2        16       4\n                         Convict             5        11       5\n                       Sue                 3        13       8\n                    EndOrg             1        11       1\n                         Die                 2        26      15\n                         DeclareBankruptcy   1        11       1\n                     None               5        20      30\n\n                          Table 4: Data distribution for selected ETs.\n\nA.2  BATCH PROMPTING\n\nSince querying LLMs individually for each input incurs substantial computational costs, a na¨ıve\napproach that treats each input separately is inefficient. To mitigate this, we employ batch prompt-\ning (Cheng et al., 2023), which enables the combination of multiple queries into a single structured\nprompt. Given a batch of inputs {Q1, Q2, ..., Qn} that share the same task instruction PI, batch\nprompting constructs a concatenated input string in the form [P0||Q1||Q2|| . . . ||Qn]. Each query is\nuniquely labeled (e.g., ”text1”) to maintain order and structure. The model processes this batch and\ngenerates a structured response in the form [A1||A2||...||An], where each Ai corresponds to the output\nfor Qi. These responses are parsed to extract individual predictions while preserving alignment. By\nreducing the number of API calls while maintaining high task accuracy, batch prompting improves\nefficiency, making large-scale prompt optimization feasible.\n\n\nA.3  PROMPT OPTIMIZATION AS A SEARCH PROBLEM\n\nWhile batch prompting enhances efficiency, it does not inherently improve task performance. To\naddress this, we formulate prompt optimization as a search problem over an expansive, intractable\nspace of possible natural language prompts, denoted as S. The objective is to discover an optimal\nprompt P∗that maximizes a task-specific evaluation function R, such as the F-score for event\nextraction, formally defined as: P∗= arg maxP∈S R(pMtask(Abatch|Qbatch, P)) where Qbatch\nand Abatch denote the batched queries and responses, respectively. Since this space is too large to\nexhaustively explore, we introduce a secondary LLM, Mopt, which iteratively refines P0 based on\nerrors observed in the output of Mtask. As shown in Fig. 3, this iterative refinement continues\nuntil a predefined stopping criterion is met, such as performance convergence or a fixed number of\noptimization steps. Once optimization concludes, the final optimized prompt P∗is used for inference\non unseen test data.\n\n\nA.4  DATA SPLIT\n\nWe utilized two shorter versions of ACE05, ACElow and ACEmed. Their detailed descriptions are\nprovided in Section 4.1. Table 4 presents the distribution of selected event types (ETs) across ACElow,\nACEmed, and the development (Dev) set. These subsets were curated to simulate both low-resource\nand medium-resource scenarios. Frequent ETs such as SentenceAct and Die contrast with rarer ones\nlike PhoneWrite and DeclareBankruptcy, allowing for a diverse evaluation spectrum. The None class\nincludes instances without any annotated events, preserving a realistic class distribution.\n\nA.5  META-PROMPTS FOR FEEDBACK (mfb) AND OPTIMIZATION (mopt)\n\nFeedback Collection Prompt.  Below we present the prompt mfb to collect structured feedback\nfrom Mopt.\n\nI am writing event guidelines and prompt (or task instructions) for a\n    language model designed for an event extraction task.\n\n\n\n                                       14\n\nPreprint. Under review.\n\n\n\n\n\nMy current prompt is:\n<START>\n{cur_prompt}\n<END>\n\nThe event guideline in Python format is as following:\n<START>\n{event_definitions}\n<END>\n\nThe task involves:\n1. Extracting structured events (triggers, event type, arguments, and\n    their roles) from the text.\n2. Adhering to strict Python syntax for output (a Python list of event\n    instances).\n3. Handling all event definitions accurately, including mandatory roles\n    and edge cases.\n\nBut this prompt gets the following examples wrong:\n<START>\n{example_string}\n<END>\n\nFor each example, perform the following step-by-step analysis:\n1. Error Type Classification: Identify the specific type(s) of error for\n    each example (e.g., incorrect span extraction, missing roles,\n    spurious arguments, format violations, etc.).\n2. Root Cause Analysis:\n    a. Did the current guideline fail to explain key extraction rules\n        clearly?\n    b. Are the instructions after ‘#‘ in the event definitions (\n        guidelines) ambiguous, inconsistent, or insufficient?\n    c. Were there ambiguities or overlaps in roles (e.g., ‘agent‘ vs. ‘\n        person‘) that caused confusion?\n3. Example-Specific Recommendations:\n    - Suggest precise changes to the guidelines (comments after ‘#‘ in\n        event guidelines) to fix the errors for the given example.\n    - Include explicit \"what to do\" and \"what not to do\" instructions for\n         ambiguous roles or edge cases.\n    - Provide a simple example and counterexample to illustrate each\n        guideline.\n4. General Trends: Identify recurring issues in guidelines across all\n    examples.\n\nExpected Output:\n1. For all the examples, summarize and list all actionable changes to\n    improve the event definitions for all the classes, including:\n    - Improved clarity for event/role definitions.\n    - Enhanced handling of ambiguous or overlapping roles.\n    - Guidelines for precise span extraction.\n\n2. Provide an output pointing out the mistakes in the current guidelines\n    and propose refinements for all the classes. Each refinement should\n    include:\n    - For an event, updated guidelines for \"what to do\" and \"what not to\n        do.\"\n    - Examples and counterexamples for each role.\n\n\nTask Instruction and Guidelines Optimization Prompt.  Below we present the prompt mopt to\noptimize task instruction and event guidelines.\n\nI am optimizing prompts for a language model designed for an event\n    extraction task.\n\n\n\n                                       15\n\nPreprint. Under review.\n\n\n\n\n\nMy current prompt (or task instructions) is:\n<START>\n{cur_prompt}\n<END>\n\nThe event guideline in Python format is as following:\n<START>\n{event_definitions}\n<END>\n\nBut this prompt gets the following examples wrong:\n<START>\n{example_string}\n<END>\n\nBased on these errors, the problems with the event guideline and the\n    reasons are:\n<START>\n{feedback}\n<END>\n\nThere are a list of former event guidelines including the current one,\n    and each guideline is modified from its former prompts:\n<START>\n{trajectory_prompts}\n<END>\n\nGuidelines given to me for optimization of event classes:\n1. Refine the prompt (or the task instructions) to address the issues\n    mentioned previously. Focus on:\n    - Clearer instructions for span extraction and role definitions along\n         with any exceptions.\n    - Handling ambiguous or overlapping roles effectively.\n    - Strict adherence to Python-parsable output format.\n2. Refine the guidelines for event definitions (the instructions after ‘#\n    ‘) based on the identified mistakes. Ensure the refined guidelines\n    addresses the concerns mentioned in the above.\n3. Maintain backward compatibility: Ensure previously correct examples\n    remain valid.\n4. DO NOT change the ontology (Python classes). Instead, provide the\n    refined guidelines in the format given at the end.\n5. Ensure outputs follow these formats:\n    - Optimized prompt (or the task instructions) wrapped with <START>\n        and <END>.\n    - Refined guidelines wrapped with <CLASS_START> and <CLASS_END>.\n\nOutput Requirements:\n1. I have to provide the optimized prompt (or the task instructions) that\n     evolves incrementally from the current one.\n2. I also have to provide an output containing the fully optimized\n    guidelines for each event definitions following the structure below:\nclass Event_Name(Parent_Event):\n    \\\"\\\"\\\"\n    # Updated guidelines here consulting the problems given to me\n    \\\"\\\"\\\"\n    mention: str # refined comments or extraction rules for event\n    triggers. Include what/who can play the role with examples.\n    {{role1}}: List # do the same for all roles including \"mention\",\n    refining the comments after \"#\". Include what/who can play the role\n    with examples and span extraction rule.\n\n\nMy response is:\n\n\n\n\n                                       16\n\nPreprint. Under review.\n\n\n\n\n                                GPT-4o       O1         GPT-4.5       DeepSeekR1\n                    BS-1                  BS-5                 BS-10                BS-15                BS-20\n                                                                                                                                                        67.5        70                          70                                           70                                                                                                                               64.4  70                                                                                                                                                             60.9                                                                                                                          60.1                                                                                           63.2  59.7  70                          56.4       58.4                                                        55.8  58.3  55.8\n                                                  50.4                                                                                                                                             49.6  51.8                                                                                47.3  50.0                                                                                                               45.5\n                               41.1\n   TI35   33.3            35                35                35        36.4        35\n\n         0                 0                 0                 0                 0\n                                                                                                                                                        65.0        70                          70                                           70                                                             70                                                                              70\n                                                                                                                          57.5  59.1                                                                                           59.2  57.1                                                             55.6                                                                                                                                                             55.6                                     54.7                          51.0                                                                   50.6                                                  45.9  49.0                                                                                41.2  44.3                                                                                                               40.9                                                                                                                                             40.6  43.4                               38.4\n   TC35   31.7            35                35                35        31.8        35\n\n         0                 0                 0                 0                 0\n                                                                                                                                                        35.1                                                                                           34.5\n                                                             31.9                                                                                                                                                             31.5        35                                     31.8  35   30.4                                           35                                                                                                                               33.9  35                                                                                                 29.4  35                                                                                                                          29.4                                                                                                                                             28.0                                                                                26.7  28.0                                                        26.5                                                                   25.2                          25.5  24.8                                                                                                                                                  24.1                                                                                                               22.8\n                    19.8                                                                                            19.6\n   AI\n        15                15                15                15                15\n\n         0                 0                 0                 0                 0\n\n                                                                                           32.2                                                                                                                                                        31.5        35                                           35                                                             35                                                                                                                               30.7  35                                     29.8  35   28.8                                                             28.3                                                                                                                                                             27.6                                                                                                                                             27.1                                                                                                 25.7                                                                                                                          25.7                                                        25.0                                                                                25.0  26.4                                                                   23.0                                                                                                                                                  22.5                          23.4  22.1                                                                                                               21.9                    19.8                                                                                                                    19.6\n   AC\n        15                15                15                15                15\n\n         0                 0                 0                 0                 0\n\n                              Figure 6: Batch-wise performance.\n\n\n\n\nA.6  ADDITIONAL HYPERPARAMETER AND MCTS CONFIGURATION\n\n\nSimilar to Wang et al. (2024b), we provide the details of hyperparameters and Monte Carlo Tree\nSearch (MCTS) configurations used in our experiments. For all runs, we fix the depth limit L\nof the search tree to 5 and the number of MCTS iterations τ to 12, unless stated otherwise. The\nexploration-exploitation trade-off is controlled by the exploration weight c, which we set to 2.5\nfollowing prior work. The batch size k for each rollout is set to 15.\n\nWe use greedy decoding for the task model Mtask to simulate deterministic predictions, and temper-\nature sampling with T = 0.7 for the optimizer model Mopt to promote diverse feedback generation.\nEarly stopping in MCTS is triggered if a prompt leads to zero errors across two consecutive rollouts.\n\n\n\nA.7  PRELIMINARY EXPERIMENTS AND MODEL SELECTION\n\n\nGrowing a full MCTS tree for prompt optimization can be computationally expensive, as noted in\nprior work Wang et al. (2024b). To establish a foundation before scaling up, we conducted initial\nexperiments to analyze the impact of batch size on performance and computational efficiency. Since\nbatch prompting reduces the number of API calls, we experimented with different batch sizes for\nconstructing Qbatch by varying the number of queries Qi and corresponding outputs Ai. However,\nwe found that determining an optimal batch size for any LLM is highly model-dependent and lacks\na universal heuristic (Fig.  6). Given this ambiguity, we set the batch size to 15, as it provides\na straightforward 15-fold reduction in API calls while maintaining response quality. This choice\nensured computational feasibility while allowing prompt optimization to operate effectively within\nour budget constraints. To further refine our experimental setup before scaling to a full MCTS search,\nwe conducted an initial trial using a single iteration of MCTS. In this controlled setup, we instantiated\na root node corresponding to the initial task prompt and generated three child nodes representing\ndifferent prompt refinements. This limited exploration allowed us to assess the effect of prompt\noptimization for event extraction under different model settings.\n\n\n                                       17\n\nPreprint. Under review.\n\n\n\n\nB  ADDITIONAL RESULTS AND ANALYSIS\n\nB.1 HOW DO OPTIMIZERS FOLLOW (OR IGNORE) FEEDBACK?\n\nAs mentioned in Section 5, optimizers exhibit different behaviors in how they apply feedback.\nFor instance, we observed that in the majority of cases, DeepSeek-R1 refines only the event def-\ninitions that are explicitly mentioned in the feedback generated for the refinement of the task\ninstruction and guidelines, leaving the remaining event definitions untouched. An example is shown\nin Figure 7, where DeepSeek-R1 reasons that the incorrect argument extraction for the Attack\nevent likely stems from limitations of Mtask rather than the guideline itself, and consequently\nrefuses to modify it. In such cases, the unchanged definitions are inherited from the parent node.\nTo quantify this behavior, we measure the aver-\nage number of edited guidelines and their av-\n                                                             (truncated) The model's prediction didn't include this,\nerage token length across all optimizers, un-     possibly because the coref wasn't resolved. The\nder each model’s best-performing configuration      current guideline says pronouns require coref\n(based on AC score), in Figure 8. Notably, the      resolution before extraction. So if \"his\" is unresolved,\ntoken counts in this analysis differ from those in         it's omitted. But in the correct answer, \"his\" is resolved\nFigure 5b because we consider only the edited      to Arafat. So the model might not have access to\n                                                             coref resolution, leading to omission. However, this is\nguidelines here—unedited ones are inherited                                                a model limitation, but the guideline says pronouns\nfrom prior states—whereas the earlier analysis     must be coref resolved. So maybe the guideline is\nincludes the full prompt content at each node.      clear, but the model isn't performing coref properly.\nAs shown in the figure, DeepSeek-R1 edits the     Hence, I will skip the Attack event. (truncated)\nfewest event types’ guidelines (6.7 on average)\nand produces the shortest guidelines (approxi-  Figure 7: Example reasoning when DeepSeek-R1\nmately 1.5k tokens for guidelines edited in one  refuses to edit the Attack event.\noptimization step), reflecting a more feedback-\nsensitive and token-efficient strategy. In con-    10                                 3000\ntrast, GPT-o1 and GPT-4.5 modify nearly all                                       2500inten guidelines (9.8 and 8.5 on average), regard-      Guidelines 8less of feedback specificity, resulting in much                                       2000Tokens\n                                                 6                           oflonger outputs (2.9k and 2k tokens, respectively).    Edited                                                                                   1500     GuidelinesWhile GPT-4o also appears restrained (7.6 edits  of\n                                                 4on average), qualitative analysis suggests this is                                       1000Numberdue to feedback overflow: when many sugges-                                                                                                    Edited\n                                                 2tions are provided, GPT-4o often fails to address     Number                                                                                   500 Avg.\nthem all. These findings highlight DeepSeek-   Avg. 0                                 0R1’s more specific and efficient editing behav-       DeepSeek GPT-4.5   O1    GPT-4o\n                                                   R1ior, further reinforcing its strength as a prompt\noptimizer.\n                                              Figure 8: Average number of guidelines edited by\nIn this section, we present a comprehensive eval-  each model and the average number of tokens in\nuation of various task models optimized through   the edited guidelines for different optimizers when\nMonte Carlo Tree Search (MCTS) guided by dif-  Mtask=DeepSeek-R1.\nferent optimizer models. We analyze performance across multiple configurations, including varying\ndataset sizes (ACElow, ACEmed, and ACE test set) and MCTS depths. Our analysis highlights how\nthe interplay between task and optimizer models, as well as the depth of the optimization process,\naffects performance on trigger and argument prediction metrics.\n\nB.2  FULL RESULTS\n\nTable 5 compares the performance of four task models—DeepSeek-R1, o1, GPT-4.5, and GPT-\n4o—when optimized by different optimizer models across four key metrics: Trigger Identification\n(TI), Trigger Classification (TC), Argument Identification (AI), and Argument Classification (AC).\nEach row corresponds to a task model, and each column group corresponds to a specific optimizer\nguiding the prompt updates during MCTS. This layout allows us to analyze both the robustness of\ntask models and the relative effectiveness of various optimizers under a shallow MCTS setup.\n\nWe further evaluate our method on the ACEmed dataset using the same MCTS configuration with\ndepth 1. Table 6 reports the performance of four task models under different optimizer models\nacross the four standard evaluation metrics. Compared to ACElow, this medium-resource setup\n\n\n                                       18\n\nPreprint. Under review.\n\n\n\n\n\n             DeepSeek-R1 (Optimizer)      o1 (Optimizer)        GPT-4.5 (Optimizer)      GPT-4o (Optimizer)\n Models\n              TI  TC   AI  AC   TI  TC   AI  AC   TI  TC   AI  AC   TI  TC   AI  AC\n\n DeepSeek-R1  37.5  33.93  25.57  24.66  27.72  25.74  18.67  18.67  36.89  34.95  22.91  22.91  32.78  32.78  21.83  21.83\n o1            31.54  31.54  21.92  21.92  29.33  29.33  18.96  18.96  31.91  31.91  18.57  18.57  29.24  29.24  21.74  20.29\n GPT-4.5      36.04  34.23  23.14  22.31  34.78  33.04  20.07  19.33  31.37  31.37  19.32  19.32  30.29  30.29  20.97  20.19\n GPT-4o       35.29  35.29  22.07  20.15  28.28  28.28  18.18  18.18  30.61  30.61  16.67  16.67  31.67  31.67  19.57  18.83\n\n  Table 5: Complete results of training on ACElow with MCTS depth 1 and tested on the dev set.\n\n\nenables deeper insights into the generalizability and adaptability of both task and optimizer models.\nThe results reveal notable variance in model-optimizer synergy, with certain combinations (e.g., o1\noptimized by itself) yielding significantly stronger trigger performance, while others show more\nbalanced gains across argument-level metrics.\n\n\n             DeepSeek-R1 (Optimizer)      o1 (Optimizer)        GPT-4.5 (Optimizer)      GPT-4o (Optimizer)\n Models\n              TI  TC   AI  AC   TI  TC   AI  AC   TI  TC   AI  AC   TI  TC   AI  AC\n\n DeepSeek-R1  63.16  63.16  40.00  40.00  65.45  65.45  32.2   32.2  56.25  56.25  37.14  37.14  62.7   62.7  40.06  38.77\n o1            78.95  78.95  39.13  36.96  54.78  54.78  33.96  30.19  59.26  59.26  36.67  36.67  57.14  57.14  36.98  36.98\n GPT-4.5      64.71  64.71  35.42  35.42  46.15  46.15  29.63  29.63  63.57  63.57  35.94  35.94  59.21  59.21  38.1  36.51\n GPT-4o       30.00  30.00  25.88  25.1  28.57  28.57  22.32  22.32  34.55  34.55  27.54  27.54  29.38  29.38  26.99  26.3\n\n  Table 6: Complete results of training on ACEmed with MCTS depth 1 and tested on the dev set.\n\nWe now report results on the ACEmed dataset using a deeper MCTS configuration with depth 5.\nTable 7 summarizes the performance of each task model under four different optimizers. Compared\nto the shallower setup, this deeper search allows for more extensive prompt refinement, which can\nlead to either improved generalization or potential overfitting, depending on the optimizer-task model\ncombination. Notably, certain models like o1 exhibit strong trigger-level performance when paired\nwith GPT-4.5 as an optimizer, while others demonstrate more balanced gains across argument metrics.\nThese results highlight the sensitivity of the optimization process to both the depth of MCTS and the\nchoice of optimizer.\n\n\n             DeepSeek-R1 (Optimizer)      o1 (Optimizer)        GPT-4.5 (Optimizer)      GPT-4o (Optimizer)\n Models\n              TI  TC   AI  AC   TI  TC   AI  AC   TI  TC   AI  AC   TI  TC   AI  AC\n\n DeepSeek-R1  56.6   56.6  44.26  44.26  66.67  66.67  44.93  40.58  46.15  46.15  40.8   38.4  48.28  48.28  40.51  37.97\n o1            48.08  48.08  40.74  39.81  42.86  42.86  38.71  38.71  84.68  84.68  41.48  37.78  48.28  48.28  34.64  33.52\n GPT-4.5      45.68  45.68  38.36  37.74  51.24  51.24  36.22  36.22  59.26  59.26  36.24  37.58  41.18  41.18  32.35  32.35\n GPT-4o       49.09  49.09  28.11  27.31  61.11  61.11  28.57  28.57  52.00  52.00  27.03  27.03  61.54  61.54  29.91  28.04\n\n  Table 7: Complete results of training on ACEmed with MCTS depth 5 and tested on the dev set.\n\nTo assess the generalization capability of the optimized prompts, we evaluate all model-optimizer\npairs on the ACE test set using an MCTS depth of 5. Table 8 presents the performance. This setup\nrepresents the final evaluation phase, where models are tested on unseen examples after undergoing\ndeeper exploration-driven prompt optimization. Overall, the results show that performance trends\nremain consistent with those observed on the development set, though certain combinations—such as\nDeepSeek-R1 with itself as optimizer—demonstrate stronger stability, while others exhibit slight\nperformance drops, especially in argument-level metrics. These observations reinforce the impact of\nboth optimizer choice and MCTS depth on downstream generalization.\n\n\nB.3  ERROR CATEGORIES AND EXAMPLES\n\nTo better understand the limitations of our approach and the nature of model failures during prompt\noptimization, we conduct a qualitative error analysis by categorizing common mistakes observed in\nmodel outputs. Table 9 summarizes the key error categories encountered across multiple evaluation\nruns, along with representative examples and detailed descriptions. These categories—ranging from\nparsing issues and hallucinations to deeper linguistic challenges such as coreference and implicit\n\n\n                                       19\n\nPreprint. Under review.\n\n\n\n\n\n             DeepSeek-R1 (Optimizer)      o1 (Optimizer)        GPT-4.5 (Optimizer)      GPT-4o (Optimizer)\n Models\n              TI  TC   AI  AC   TI  TC   AI  AC   TI  TC   AI  AC   TI  TC   AI  AC\n\n DeepSeek-R1  69.23  67.69  44.33  43.75  54.12  54.12  42.06  42.06  52.8   52.8  41.98  41.98  47.27  47.27  33.61  31.93\n o1            68.28  67.76  38.44  37.86  67.86  67.86  38.71  38.71  58.29  58.29  36.73  36.73  41.11  41.11  28.57  28.57\n GPT-4.5      68.31  68.31  38.44  36.69  64.71  64.71  39.02  36.59  56.45  56.45  35.29  35.29  49.09  49.09  28.11  27.31\n GPT-4o       59.44  59.44  36.99  35.71  64.52  64.52  30.59  30.59  56.57  56.57  34.75  34.75  48.19  48.19  26.94  26.94\n\n  Table 8: Complete results of training on ACEmed with MCTS depth 5 and tested on the test set.\n\n\n\n\n\nevent detection—highlight areas where models tend to struggle, particularly under batch prompting\nand complex event structures.\n\n\n\n\n\n Error Category\n\n                         Description: Parsing errors occur when the model’s output is not in the expected format (e.g., JSON or structured\n                                   list), often due to extra reasoning or verbose responses in batch prompts. These make the output unusable for\n Parsing Errors          evaluation pipelines.\n\n                     Example: Prompts that return extra text or commentary instead of a valid Python structure, causing non-parsable\n                           output.\n\n                         Description: Hallucinations occur when the model generates arguments or events that are not supported by the\n Hallucinations           input. This usually happens due to biases learned during training or lexical overlaps with known labels.\n\n                     Example: Text: “Different parts of the strip saw conflicts today.” →Model incorrectly predicts a ‘Conflict‘\n                         event based solely on the word “conflict”.\n\n                         Description: Multiple event errors happen when the model detects only a single event in a sentence that contains\n Multiple Events         multiple, usually defaulting to the most salient or final event.\n\n                     Example: Text: “...went home and his father-in-law killed him.” →Model only predicts the ‘Die‘ event,\n                          ignoring the ‘Transport‘ event.\n\n                        Description: Label noise refers to inconsistencies or ambiguities in the dataset annotations, such as differing\n Label Noise             treatment of coreferences or unclear event boundaries, which confuse both training and evaluation.\n\n                     Example: Text: “Our president has repeatedly... relied on a man... Hussein Kamel... leader of the Iraq arms\n                      program who defected...” →Label uses ‘person=[”leader”]‘; model uses ‘person=[”Hussein Kamel”]‘.\n\n                        Description: Coreference errors arise when the model fails to resolve references like pronouns or role-based\n Coreferences            descriptors to their actual entities, leading to incorrect or incomplete argument spans.\n\n                     Example: Text: “...Hussein Kamel, leader of the Iraq arms program who defected...” →Label uses “leader”;\n                      model uses “Hussein Kamel”, highlighting coreference resolution challenges.\n\n                         Description: Span overprediction occurs when the model predicts more detailed argument spans than necessary,\n Span Overprediction   often including modifiers or descriptors not required by the task’s minimal span rules.\n\n                     Example: Text: “Orders went out today to deploy 17,000 U.S. Army soldiers in the Persian Gulf region.” →\n                         Label: “soldiers”; Prediction: “17,000 U.S. Army soldiers” – includes extra modifiers.\n\n                         Description: Implicit events are those not directly triggered by verbs but inferred through adjectives, nouns, or\n Implicit Events         other context (e.g., “former”). These are often missed by models unless explicitly instructed.\n\n                     Example: Text: “...with former Congressman Tom Andrews...” →Trigger “former” implies ‘EndPosition‘, but\n                                is often missed by models lacking rules for implicit event detection.\n\n                    Table 9: Description of error categories with examples.\n\n\n\n\n\n                                       20\n\nPreprint. Under review.\n\n\n\n\nC  OPTIMIZED TASK INSTRUCTION AND GUIDELINES\n\nIn this section, we present fully optimized task instruction and event guidelines generated by\nDeepSeek-R1, o1, GPT-4.5, and GPT-4o.\n\nC.1  EXAMPLE OF OPTIMAL TASK INSTRUCTION AND EVENT GUIDELINES GENERATED BY\n     DEEPSEEK-R1\n\n# Event Extraction Task: Extract structured events from text using Python\n     class definitions. Follow these rules:\n\n1. **Span Extraction**:\n   - **Triggers**: Minimal contiguous spans (verbs/nouns) directly\n       expressing the event. Include both verbal and nominal forms (\"\n       death\" = Die, \"killings\" = Die). Add new triggers like \"converge\"\n       for Meet and \"is no more\" for EndOrg\n   - **Arguments**:\n     - Remove articles (\"a/an/the\") and possessive pronouns EXCEPT when\n         part of official names or temporal phrases (\"The Hague\", \"the\n         past year\")\n     - Resolve pronouns AND POSSESSIVE NOUNS to named entities **\n         immediately** using same-sentence antecedents (\"airline’s plan\"\n        →[\"airline\"])\n     - Strip role/location/age descriptors from arguments (\"Philadelphia\n         lawyers\" →\"lawyers\") unless part of multi-word crime\n     - Keep FULL spans for crimes/money including sources/amounts (\"\n         stereo worth $1,750 from family\") unless legal terms\n     - Detect beneficiaries via ownership markers (\"for X’s project\"),\n         direct \"to X\" transfers go to recipient\n\n2. **Special Handling**:\n   - **Bankruptcy Triggers**: \"went bust\" →EndOrg unless explicit\n       bankruptcy context\n   - **Meet Entities**: Include ALL resolvable participants (subject +\n       object)\n   - **Crime Spans**: Retain full contextual clauses (\"If convicted of\n       killings...\") without truncation\n   - **Temporal Phrases**: Keep original spans with articles when part of\n        phrase (\"the early 90’s\")\n\n3. **Output Rules**:\n   - Always output in Python-format as [EventName(\"mention\" = \"trigger\",\n       \"arg1_key\" = \"arg1_span\", ...), EventName(\"mention\" = \"trigger\", \"\n       arg1_key\" = \"arg1_span\", ...)]\n   - Include ALL role fields with empty lists where applicable\n   - Output separate events for each trigger (no merging) even for\n       identical event types\n   - Strict pydantic syntax: [EventName(mention=\"span\", role=[\"span\"],\n       ...)]\n   - Preserve original casing for locations unless explicitly proper\n       nouns\n\n4. **Critical Exceptions**:\n   - **EndOrg Triggers**: Add \"collapse\", \"drive out\", \"went bust\" with\n       explicit org mentions\n   - **Appeal Roles**: defendant = opposing party (state), prosecutor =\n       appellant\n   - **TransferMoney**: \"for X\" →recipient unless ownership marker (\"for\n        X’s Y\" →beneficiary)\n   - **PhoneWrite Entities**: Strip ALL role descriptors (\"Secretary\n       Powell\" →[\"Powell\"])\n\n\n\n\n\n                                       21\n\nPreprint. Under review.\n\n\n\n\n\n# Here are the event definitions:\n\nclass Convict(JusticeEvent):\n    \"\"\"Extract convictions where entity is found guilty of crime.\n    Key Updates:\n    - crime: Retain FULL spans including amounts/sources (\"received\n    stereo worth $1,750 from family\")\n\n    Example: \"convicted of taking bribes worth $1M\" →crime=[\"taking\n    bribes worth $1M\"]\n    Counterexample: Truncating to [\"taking bribes\"] →error\n    \"\"\"\n    mention: str  # Triggers: \"convicted\", \"conviction\"\n    defendant: List[str]  # [\"Vang\"] (resolved pronouns, strip\n        descriptors)\n    adjudicator: List[str]  # [\"court\"] (official names only)\n    crime: List[str]  # Full offense span without legal terms\n    time: List[str]  # [\"last Wednesday\"] (exact temporal phrases)\n    place: List[str]  # [\"Minnesota\"] (geopolitical entities from context\n        )\n\nclass TransferMoney(TransactionEvent):\n    \"\"\"Money transfers without goods exchange.\n    Key Updates:\n    - recipient: Direct receiver (\"to X\" OR \"for X\" if X is endpoint)\n    - beneficiary: Only for ownership (\"for X’s project\") or indirect\n    benefit\n\n    Example: \"donated $5 for Tim Kaine\" →recipient=[\"Tim Kaine\"]\n    Example: \"funds for Kaine’s campaign\" →beneficiary=[\"Kaine\"]\n    \"\"\"\n    mention: str  # Triggers: \"provided money\", \"donation\"\n    giver: List[str]  # [\"foundation\"] (strip descriptors)\n    recipient: List[str]  # [\"charity\"] (direct receiver from \"to/for X\")\n    beneficiary: List[str]  # [\"Suha\"] (from ownership markers)\n    money: List[str]  # [\"$15M\"] (keep symbols/approximations)\n    time: List[str]  # [\"two years\"] (full temporal span)\n    place: List[str]  # [\"Swiss\"] (origin locations, strip prepositions)\n\nclass Meet(ContactEvent):\n    \"\"\"Face-to-face interactions.\n    Key Updates:\n    - entity: Include ALL resolvable participants (subject + object)\n\n    Example: \"Annan met Al-Douri\" →entity=[\"Annan\", \"Al-Douri\"]\n    Counterexample: Omitting subject →error\n    \"\"\"\n    mention: str  # Triggers: \"meet\", \"summit\", \"talks\"\n    entity: List[str]  # [\"delegates\"] (all participants)\n    time: List[str]  # [\"today\"] (exact temporal span)\n    place: List[str]  # [\"Dallas\"] (resolved location noun)\n\n\nclass PhoneWrite(ContactEvent):\n    \"\"\"Non face-to-face communication.\n    Key Updates:\n    - entity: Strip ALL role descriptors unless part of compound name\n\n    Example: \"e-mail from Secretary Powell\" →entity=[\"Powell\"]\n    Counterexample: Retaining \"Secretary\" →error\n    \"\"\"\n    mention: str  # Triggers: \"called\", \"e-mail\" with transmission\n        context\n    entity: List[str]  # [\"we\", \"them\"] (bare names, resolved pronouns)\n    time: List[str]  # [\"during meeting\"] (exact time phrase)\n\n\n                                       22\n\nPreprint. Under review.\n\n\n\n\n\n    place: List[str]  # [\"office\"] (specific location if present)\n\n\nclass DeclareBankruptcy(BusinessEvent):\n    \"\"\"Formal bankruptcy declarations.\n    Key Rules:\n    - entity: Resolve org pronouns AND possessive nouns (\"airline’s\n    bankruptcy\" →[\"airline\"])\n    - Triggers: \"bankruptcy\", \"Chapter 11\" (exclude \"collapse\"/\"went bust\n    \" without explicit bankruptcy context)\n\n    Example: \"airline’s bankruptcy filing\" →mention=\"bankruptcy\", org=[\"\n    airline\"]\n    Counterexample: \"near-collapse\" →EndOrg\n    \"\"\"\n    mention: str  # Triggers indicating financial collapse: \"bankruptcy\",\n         \"Chapter 11\"\n    entity: List[str]  # [\"Enron Corp\"] (resolved orgs from pronouns/\n        possessives in same sentence)\n    time: List[str]  # [\"2003\"] (declaration time phrase)\n    place: List[str]  # [\"Texas\"] (jurisdiction noun if specified)\n\nclass EndOrg(BusinessEvent):\n    \"\"\"Organization termination events.\n    Key Rules:\n    - Triggers: \"ceased\", \"is no more\", \"collapse\", \"drive out\", \"went\n    bust\"\n    - org: Require explicit organizational mention (\"casinos\" in \"casinos\n     faced collapse\")\n\n    Example: \"company went bust\" →mention=\"went bust\", org=[\"company\"]\n    Counterexample: \"facing collapse\" (no explicit org) →ignore\n    \"\"\"\n    mention: str  # Triggers must indicate actual termination\n    org: List[str]  # [\"plant\"] (direct object or possessive noun)\n    time: List[str]  # [\"the past year\"] (with articles when part of\n        phrase)\n    place: List[str]  # [\"Eugene\"] (specific location noun)\n\nclass Die(LifeEvent):\n    \"\"\"Death events.\n    Key Updates:\n    - mention: Include nominal forms (\"killings\", \"casualties\") as valid\n    triggers\n\n    Example: \"massacre casualties\" →mention=\"casualties\"\n    Counterexample: \"death penalty\" →ignore\n    \"\"\"\n    mention: str  # Triggers: \"died\", \"killings\", \"casualties\"\n    agent: List[str]  # [\"shooter\"] (intentional actors only)\n    victim: List[str]  # [\"patient\"] (without quantifiers/possessives)\n    instrument: List[str]  # [\"knife\"] (specific tools/weapons)\n    time: List[str]  # [\"last night\"] (exact span)\n    place: List[str]  # [\"hospital\"] (death location noun)\n\nclass SentenceAct(JusticeEvent):\n    \"\"\"Punishment issuance events.\n    Key Updates:\n    - crime: Retain original crime from conditional clauses (\"If\n    convicted of killings...\" →[\"killings\"])\n\n    Example: \"faces life for fraud\" →crime=[\"fraud\"]\n    Counterexample: \"could face penalty\" →ignore\n    \"\"\"\n\n\n\n                                       23\n\nPreprint. Under review.\n\n\n\n\n\n    mention: str  # Triggers: \"sentenced\", \"faces\". Must reference actual\n         punishment\n    defendant: List[str]  # [\"activist\"] (strip role descriptors)\n    adjudicator: List[str]  # [\"jury\"] (bare roles unless official title)\n    crime: List[str]  # [\"illegally attending meeting\"] (full contextual\n        span)\n    sentence: List[str]  # [\"life in prison\"] (exact punishment phrase)\n    time: List[str]  # [\"Thursday\"] (exact temporal expression)\n    place: List[str]  # [\"district court\"] (decision location noun)\n\nclass Sue(JusticeEvent):\n    \"\"\"Legal action initiations.\n    Key Updates:\n    - adjudicator: Include \"judge\" if overseeing case approval (\"approved\n     by judge\" →[\"judge\"])\n\n    Example: \"suit against Gateway approved by judge\" →adjudicator=[\"\n    judge\"]\n    Counterexample: \"lawsuit documents\" →adjudicator=[]\n    \"\"\"\n    mention: str  # Triggers: \"suit\", \"lawsuit\". Must reference legal\n        filing\n    plaintiff: List[str]  # [\"patients\"] (strip locations/roles unless\n        critical)\n    defendant: List[str]  # [\"Gateway\"] (explicitly sued entities)\n    adjudicator: List[str]  # [\"judge\"] (if directly involved)\n    crime: List[str]  # [\"malpractice\"] (explicit offense without legal\n        terms)\n    time: List[str]  # [\"last month\"] (keep articles in temporal phrases)\n    place: List[str]  # [\"South Florida\"] (specific noun phrases)\n\nclass Appeal(JusticeEvent):\n    \"\"\"Court decision appeals.\n    Key Updates:\n    - defendant: Opposing party (state/prosecution), NOT appellant\n    - prosecutor: Entity filing appeal (resolved from subject/pronouns)\n\n    Example: \"appeal by Anwar against conviction\" →prosecutor=[\"Anwar\"],\n     defendant=[]\n    Counterexample: Assigning appellant as defendant →error\n    \"\"\"\n    mention: str  # Triggers: \"appeal\", \"appeals\"\n    defendant: List[str]  # [\"state\"] (opposing party in original case)\n    prosecutor: List[str]  # [\"Pasko\"] (appellant, bare name without\n        roles)\n    adjudicator: List[str]  # [\"court\"] (original court name)\n    crime: List[str]  # [\"espionage\"] (original charge)\n    time: List[str]  # [\"last week\"] (exact temporal phrase)\n    place: List[str]  # [\"Malaysia\"] (country from court description)\n\n\nC.2  EXAMPLE OF OPTIMAL TASK INSTRUCTION AND EVENT GUIDELINES GENERATED BY O1\n\n# This is an event extraction task where the goal is to extract\n    structured events from the text following structured event\n    definitions in Python.\n# A structured event contains:\n#   (1) an event trigger word (mention) -- always use the minimal lexical\n     span (e.g., \"appeal\" rather than \"filed an appeal\"),\n#   (2) an event type, and\n#   (3) the arguments participating in the event (with their roles).\n\n# Keep argument references minimal by removing articles, possessives, or\n    descriptive words unless they are crucial identifiers (e.g., \"the\n    retailer\" -> \"retailer\", \"my uncle\" -> \"uncle\").\n\n\n                                       24\n\nPreprint. Under review.\n\n\n\n\n\n# Important guidelines to address prior errors:\n#   1. For each event trigger, use the single most relevant word (e.g., \"\n    bankruptcy\" rather than \"file for bankruptcy\").\n#   2. For argument roles, also use minimal spans (e.g., \"soldier\"\n    instead of \"a soldier,\" \"woman\" instead of \"a woman\").\n#   3. Output a separate event for each distinct trigger or implied event\n     (e.g., a conviction and a subsequent sentencing should be two events\n    ).\n#   4. For justice events (Sue, Appeal, Convict, SentenceAct, etc.):\n#       - \"defendant\" is the party or entity accused or found guilty.\n#       - \"plaintiff\" or \"prosecutor\" is the party initiating legal\n    action or bringing an appeal. If the text does not specify who is\n    accused, leave \"defendant\" empty.\n#       - If the text refers to a punishment or sentencing (e.g., \"faces\n    the death penalty\"), include a separate SentenceAct event referencing\n     the same \"defendant.\"\n#   5. For transfers of money, watch for direct or indirect references to\n     donations, funding, or contributions and label them as TransferMoney\n     events.\n#   6. Do not skip events implied by synonyms or indirect wording (e.g.,\n    \"shutting down\" →EndOrg, \"emerged from bankruptcy\" →\n    DeclareBankruptcy).\n#   7. If there is more than one event in a single text, output each in a\n     separate entry.\n#   8. Always produce valid Python list format exactly as:\n#       result = [\n#         EventName(\"mention\" = \"trigger\", \"role1\" = [...], \"role2\" =\n    [...], ...),\n#         EventName(\"mention\" = \"trigger\", \"role1\" = [...], \"role2\" =\n    [...], ...),\n#       ]\n#   9. Do not output anything else except this parsable Python structured\n     format (no extra text or explanation).\n\n# The event class definitions remain the same, but refer to the following\n     refined docstrings for usage examples, minimal spans, and role\n    clarifications.\n\n\n# Here are the event definitions:\n\nclass Convict(JusticeEvent):\n    \"\"\"\n    A Convict Event occurs whenever a Try Event ends with a successful\n    prosecution of the Defendant.\n    In other words, a Person, Organization or GPE Entity is convicted\n    whenever that Entity has been\n    found guilty of a Crime.\n\n    Refined Guidelines:\n       • mention: Use the minimal trigger word referring to the conviction\n     (e.g., \"guilty\", \"convicted\").\n       • defendant: The entity/ies found guilty. Remove articles or\n    possessives (\"the man\" →\"man\").\n       • adjudicator: The court or judge that issued the guilty verdict,\n    if explicitly given.\n       • crime: The wrongdoing for which the defendant was found guilty (e\n    .g., \"murdering X\").\n       • time: Any explicit time references (e.g., \"last week\").\n       • place: Any explicit location references (e.g., \"in Boston\").\n\n    What to do:\n      - Include \"crime\" if stated: e.g., \"convicted of murdering his wife\n    \" →crime=[\"murdering his wife\"].\n\n\n                                       25\n\nPreprint. Under review.\n\n\n\n\n\n      - Keep the defendant arg minimal: \"Scott Peterson\" →[\"Scott\n    Peterson\"], not [\"Mr. Scott Peterson\"].\n\n    What not to do:\n      - Do not guess or infer the crime if not stated.\n      - Do not prepend articles or descriptive words (e.g., \"the\n    defendant\" →\"defendant\" if used generically).\n\n    Example:\n      Text: \"John was found guilty of fraud.\"\n     →Convict(mention=’guilty’, defendant=[’John’], crime=[’fraud’],\n    time=[], place=[])\n    \"\"\"\n    mention: str  # minimal word expressing the conviction event\n    defendant: List[str]  # who is found guilty\n    adjudicator: List[str]  # the judge or court, if stated\n    crime: List[str]  # the wrongdoing for which the defendant is\n        convicted\n    time: List[str]  # when the conviction takes place\n    place: List[str]  # where the conviction takes place\n\n\nclass TransferMoney(TransactionEvent):\n    \"\"\"\n    TransferMoney Events refer to giving, receiving, borrowing, or\n    lending money\n    when not purchasing goods or services in return.\n\n    Refined Guidelines:\n       • mention: Single word that triggers the transfer event (e.g., \"\n    donated\", \"loaned\").\n       • giver: The agent who provides funds. Remove determiners (\"the\", \"\n    a\") unless part of a name.\n       • recipient: The agent who receives the funds.\n       • beneficiary: Any additional agent that benefits, if separate from\n     recipient.\n       • money: The amount of funds (if any mention like \"$3,000\", \"large\n    sum\").\n       • time: When the event takes place (e.g., \"today\", \"last year\").\n       • place: Where the transaction or transfer occurs.\n\n    What to do:\n      - Label intangible references (e.g., \"contributed\", \"had\n    contributors\") as TransferMoney if it implies funds.\n      - Use minimal references for all money roles.\n\n    What not to do:\n      - Do not label intangible help (e.g., \"emotional support\") as\n    TransferMoney.\n      - Avoid listing indefinite articles or extraneous descriptors in\n    the agent spans.\n\n    Example:\n      Text: \"He donated $5,000 to Red Cross last week.\"\n     →TransferMoney(mention=’donated’, giver=[’He’], recipient=[’Red\n    Cross’], money=[’$5,000’], time=[’last week’], place=[])\n    \"\"\"\n    mention: str  # minimal word triggering the money transfer\n    giver: List[str]  # who provides the money\n    recipient: List[str]  # who receives the money\n    beneficiary: List[str]  # who additionally benefits, if any\n    money: List[str]  # the sum or amount\n    time: List[str]  # when the transfer happens\n    place: List[str]  # where the transfer event occurs\n\n\n\n                                       26\n\nPreprint. Under review.\n\n\n\n\n\nclass Meet(ContactEvent):\n    \"\"\"\n    A Meet Event occurs when two or more Entities come together face-to-\n    face\n    at a single location and interact with one another.\n\n    Refined Guidelines:\n       • mention: The single best word for the meeting (e.g., \"met\", \"\n    summit\", \"conference\").\n       • entity: All participants, stripped of articles or descriptors. If\n     multiple, list them all.\n       • time: Any temporal phrase referencing when the event took place.\n       • place: The location of the meeting.\n\n    What to do:\n      - Use triggers for in-person gatherings (e.g., \"met\", \"conference\",\n     \"summit\").\n      - Keep participant references minimal: \"President\", \"Vice-President\n    \" instead of \"the US President\".\n\n    What not to do:\n      - Do not treat phone calls or written communication as Meet (use\n    PhoneWrite).\n\n    Example:\n      Text: \"The leaders met in Paris yesterday.\"\n     →Meet(mention=’met’, entity=[’leaders’], time=[’yesterday’], place\n    =[’Paris’])\n    \"\"\"\n    mention: str  # minimal word or short phrase for the meeting\n    entity: List[str]  # who met face-to-face\n    time: List[str]  # when the meeting happened\n    place: List[str]  # where the meeting occurred\n\n\nclass PhoneWrite(ContactEvent):\n    \"\"\"\n    A PhoneWrite Event occurs when two or more people communicate\n    without meeting face-to-face. This includes phone calls, email,\n    texting, etc.\n\n    Refined Guidelines:\n       • mention: The minimal expression of communication (e.g., \"called\",\n     \"emailed\", \"texted\").\n       • entity: The agents communicating. Strip out articles, determiners\n    , or extra descriptors.\n       • time: When the communication took place (e.g., \"this morning\", \"\n    yesterday\").\n\n    What to do:\n      - Common triggers: \"phoned\", \"emailed\", \"talked by phone\", \"texted\"\n    , \"messaged\".\n      - Keep roles minimal (e.g., entity=[’John’, ’Mary’]).\n\n    What not to do:\n      - Do not mark in-person discussions as PhoneWrite (use Meet).\n\n    Example:\n      Text: \"They emailed each other last night.\"\n     →PhoneWrite(mention=’emailed’, entity=[’They’], time=[’last night\n    ’])\n    \"\"\"\n    mention: str  # minimal communication trigger\n    entity: List[str]  # communicating parties\n\n\n                                       27\n\nPreprint. Under review.\n\n\n\n\n\n    time: List[str]  # when the communication happened\n\n\nclass DeclareBankruptcy(BusinessEvent):\n    \"\"\"\n    A DeclareBankruptcy Event occurs whenever an Entity officially seeks\n    legal protection\n    from debt collection due to severe financial distress.\n\n    Refined Guidelines:\n       • mention: Short trigger related to bankruptcy (e.g., \"bankruptcy\",\n     \"filed\", \"declared\").\n       • org: The organization or person who declares bankruptcy. Remove \"\n    the\", \"my\", etc.\n       • time: When the bankruptcy is declared (e.g., \"in 2003\", \"today\").\n       • place: Where the declaration is made, if mentioned (e.g., \"in\n    court\", \"in New York\").\n\n    What to do:\n      - Recognize synonyms or indirect references like \"emerged from\n    bankruptcy\" or \"bankruptcy protection\" as triggers.\n\n    What not to do:\n      - Do not guess an org if not specified.\n\n    Example:\n      Text: \"My uncle declared bankruptcy in 2003.\"\n     →DeclareBankruptcy(mention=’bankruptcy’, org=[’uncle’], time\n    =[’2003’], place=[])\n    \"\"\"\n    mention: str  # minimal expression for bankruptcy\n    org: List[str]  # the party declaring bankruptcy\n    time: List[str]  # when the declaration takes place\n    place: List[str]  # where it is declared\n\n\nclass EndOrg(BusinessEvent):\n    \"\"\"\n    An EndOrg Event occurs when an Organization ceases to exist or\n    \"goes out of business.\"\n\n    Refined Guidelines:\n       • mention: Minimal trigger (e.g., \"shutting down\", \"closing\").\n       • org: The organization or sub-unit that ends. E.g., \"plant\", \"\n    branch\".\n       • time: When this closure or end is stated to happen.\n       • place: Where the organization is located or ended.\n\n    What to do:\n      - Consider references such as \"closing its plant\" →\"plant\" in org.\n      - Identify synonyms like \"shutting down,\" \"ceasing operations.\"\n\n    What not to do:\n      - Do not skip it if the text explicitly says the org ended.\n\n    Example:\n      Text: \"Hewlett Packard is shutting down its plant in Eugene.\"\n     →EndOrg(mention=’shutting down’, org=[’plant’], time=[], place=[’\n    Eugene’])\n    \"\"\"\n    mention: str  # minimal expression for the organizational end\n    org: List[str]  # the ended organization\n    time: List[str]  # when the end occurs\n    place: List[str]  # where this event happens\n\n\n\n                                       28\n\nPreprint. Under review.\n\n\n\n\n\nclass Die(LifeEvent):\n    \"\"\"\n    A Die Event occurs whenever a Person loses their life, whether\n    accidental,\n    intentional, or self-inflicted.\n\n    Refined Guidelines:\n       • mention: The short trigger referencing the death (e.g., \"killed\",\n     \"died\", \"murdered\").\n       • agent: The killer or cause if identified (e.g., \"gunman\", \"regime\n     \")---remove articles.\n       • victim: Who died, again with minimal references (e.g., \"soldier\"\n    instead of \"a soldier\").\n       • instrument: The device or method used, if any (e.g., \"gun\", \"bomb\n    \").\n       • time: When the death occurred.\n       • place: Where it took place.\n\n    What to do:\n      - Create separate Die events for each death trigger in the text.\n      - If the text references homicide: agent is the killer, victim is\n    the deceased.\n\n    What not to do:\n      - Do not combine multiple victims into one string if they appear as\n     separate triggers.\n\n    Example:\n      Text: \"He killed the soldier in Iraq.\"\n     →Die(mention=’killed’, agent=[’He’], victim=[’soldier’],\n    instrument=[], time=[], place=[’Iraq’])\n    \"\"\"\n    mention: str  # minimal word referencing the death\n    agent: List[str]  # optional killer or cause\n    victim: List[str]  # who died\n    instrument: List[str]  # how they were killed (weapon, etc.)\n    time: List[str]  # when the death happened\n    place: List[str]  # where the death happened\n\n\nclass SentenceAct(JusticeEvent):\n    \"\"\"\n    A SentenceAct Event occurs whenever a punishment for the Defendant is\n     issued,\n    e.g., a prison term or another legal penalty.\n\n    Refined Guidelines:\n       • mention: A trigger referencing sentencing or punishment (e.g., \"\n    sentenced\", \"faces [penalty]\").\n       • defendant: The same party convicted or found guilty, if known.\n       • adjudicator: The entity delivering the sentence, if stated (e.g.,\n     \"judge\", \"court\").\n       • crime: The wrongdoing for which the defendant is sentenced (e.g.,\n     \"murder\", \"embezzlement\").\n       • sentence: The specific punishment (e.g., \"death penalty\", \"life\n    in prison\").\n       • time: When the sentencing occurs.\n       • place: Where the sentencing occurs.\n\n    What to do:\n      - Look for words like \"faces the death penalty,\" \"was sentenced to\n    ten years.\"\n\n    What not to do:\n\n\n                                       29\n\nPreprint. Under review.\n\n\n\n\n\n      - Do not omit a SentenceAct if there’s explicit mention of\n    punishment.\n\n    Example:\n      Text: \"He now faces the death penalty for murdering his wife.\"\n     →SentenceAct(mention=’faces’, defendant=[’He’], crime=[’murdering\n    his wife’], sentence=[’death penalty’], time=[], place=[])\n    \"\"\"\n    mention: str  # minimal expression for the sentencing event\n    defendant: List[str]  # who is sentenced\n    adjudicator: List[str]  # judge or court\n    crime: List[str]  # the wrongdoing or offense\n    sentence: List[str]  # the punishment\n    time: List[str]  # when the sentencing happens\n    place: List[str]  # where it happens\n\n\nclass Sue(JusticeEvent):\n    \"\"\"\n    A Sue Event occurs whenever a court proceeding is initiated to\n    determine\n    the liability of a Person, Organization, or GPE.\n\n    Refined Guidelines:\n       • mention: The minimal trigger (e.g., \"sued\", \"suing\", \"filed a\n    lawsuit\", \"suit\").\n       • plaintiff: The party bringing the suit. Strip out any articles or\n     adjectives.\n       • defendant: The party being sued. Again, keep references minimal.\n       • adjudicator: The judge or court if one is explicitly named.\n       • crime: If a wrongdoing is stated (e.g., \"for fraud\", \"for breach\n    of contract\").\n       • time: When the suit is filed or mentioned.\n       • place: Where the suit is taking place.\n\n    What to do:\n      - Label the party initiating the lawsuit as \"plaintiff.\"\n\n    What not to do:\n      - Do not confuse \"plaintiff\" with \"defendant\" if the text clearly\n    states who is suing whom.\n\n    Example:\n      Text: \"A nurse sued Dell for bait and switch.\"\n     →Sue(mention=’sued’, plaintiff=[’nurse’], defendant=[’Dell’],\n    crime=[’bait and switch’], time=[], place=[])\n    \"\"\"\n    mention: str  # minimal expression for the lawsuit event\n    plaintiff: List[str]  # who brings the suit\n    defendant: List[str]  # who is being sued\n    adjudicator: List[str]  # the judge or court, if stated\n    crime: List[str]  # the wrongdoing for which the suit is filed\n    time: List[str]  # when the suit took place\n    place: List[str]  # where the suit took place\n\n\nclass Appeal(JusticeEvent):\n    \"\"\"\n    An Appeal Event occurs whenever a court decision is taken to a higher\n     court\n    for review.\n\n    Refined Guidelines:\n       • mention: The short trigger for the appeal (e.g., \"appeal\", \"\n    appealed\").\n\n\n                                       30\n\nPreprint. Under review.\n\n\n\n\n\n       • defendant: The party accused or found guilty, if the text states\n    so.\n       • prosecutor: The party bringing the appeal (i.e., the appellant).\n    This might be the same individual who was a defendant in a prior\n    trial but is now appealing.\n       • adjudicator: The higher court or judge handling the appeal, if\n    given.\n       • crime: The wrongdoing for which the appeal is made (if stated).\n       • time: When the appeal is filed or heard.\n       • place: Where the appeal is taking place.\n\n    What to do:\n      - If text says someone \"filed an appeal,\" that entity is the \"\n    prosecutor\" if no other roles are specified.\n      - If the text does not identify an accused, keep defendant=[].\n\n    What not to do:\n      - Do not automatically fill \"defendant\" if it’s unclear who was\n    accused.\n\n    Example:\n      Text: \"He appealed the verdict last week.\"\n     →Appeal(mention=’appealed’, defendant=[], prosecutor=[’He’], crime\n    =[], time=[’last week’], place=[])\n    \"\"\"\n    mention: str  # minimal word for the appeal event\n    defendant: List[str]  # the accused, if stated\n    prosecutor: List[str]  # who is bringing the appeal\n    adjudicator: List[str]  # the judge or court for the appeal\n    crime: List[str]  # the crime or issue being appealed\n    time: List[str]  # when the appeal occurs\n    place: List[str]  # where the appeal is heard\n\n\n\nC.3  EXAMPLE OF TASK INSTRUCTION AND OPTIMAL EVENT GUIDELINES GENERATED BY\n     GPT-4.5\n\n# This is an event extraction task for identifying and structuring events\n     from text using Python-defined event classes. Each structured event\n    consists of an event trigger word, an event type, participant\n    arguments, and their roles. Your objective is to output this\n    information in a Python list of events, ensuring it is Python-\n    parsable and strictly follows the event definitions provided below.\n\n## Instructions:\n\n1. **Span Extraction**:\n    - Extract precise and concise spans for mentions and participant\n        arguments, conveying the event or argument role clearly without\n        unnecessary context.\n    - For extracts involving titles or specifics, use general terms\n        unless details are crucial to the events integrity.\n    - When identifying entity roles in events, prioritize the core\n        identifiers over accompanying descriptors.\n\n2. **Role Identification**:\n    - Accurately identify roles using contextual cues, effectively\n        resolving ambiguities while prioritizing explicit spans. If roles\n         are unmentioned, leave them empty.\n    - Maintain consistency, particularly with distinctions like plaintiff\n         vs. defendant, based on contextual evidence.\n    - Clarify roles in complex transactions, such as distinguishing\n        between beneficiaries and direct recipients.\n\n\n\n                                       31\n\nPreprint. Under review.\n\n\n\n\n3. **Output Format**:\n    - Please follow the Python-format EventName(\"mention\" = \"trigger\", \"\n        role1\" = [...], \"role2\" = [...], ...) strictly.\n    - Ensure consistent output in the specified format for Python\n        compatibility, adhering strictly to event definitions.\n    - Represent unmentioned participants with an empty list rather than\n        assumptions or placeholders.\n\n4. **Clarifications and Exceptions**:\n    - Note explicitly when roles have exceptions based on role\n        definitions.\n    - Manage overlapping roles by following specific guidelines for span\n        clarity and precision, ensuring no crucial details are overlooked\n        .\n\n5. **Consistency**:\n    - Ensure consistency in role identification and event extraction\n        across similar scenarios.\n    - Address ambiguity and overlap by defining roles explicitly and\n        setting clear precedence for extraction guidelines.\n\nBelow are the structured event definitions:\n\n\n\n\n\n# Here are the event definitions:\n\nclass Convict(JusticeEvent):\n    \"\"\"\n    A Convict Event signifies the successful prosecution of a defendant.\n    This involves a person, organization, or geographical political\n    entity (GPE) being convicted for a crime.\n    \"\"\"\n    mention: str  # Focus on concise triggers like \"convicted\" or \"\n        conviction\", avoiding embellishments.\n    defendant: List[str]  # Name the convicted individuals or entities.\n        Use direct identifiers, example: \"John Doe\".\n    adjudicator: List[str]  # Reference the judicial entity, example: \"\n        court\" or \"judge\", unless specifics are critical.\n    crime: List[str]  # Provide short, precise descriptions of crimes, e.\n        g., \"fraud\".\n    time: List[str]  # Specify exact times if mentioned, e.g., \"Monday\".\n    place: List[str]  # Note locations if explicitly mentioned, avoid\n        assumptions.\n\nclass TransferMoney(TransactionEvent):\n    \"\"\"\n    Non-purchasing money transfers involving giver and recipient roles,\n    where transactions are more indirect or complex.\n    \"\"\"\n    mention: str  # Use explicit terms like \"donated\", staying concise.\n    giver: List[str]  # Identify the money source, example: \"Sheila C.\n        Johnson\".\n    recipient: List[str]  # Clearly name receiving entities.\n    beneficiary: List[str]  # Note additional beneficiaries unambiguously\n        .\n    money: List[str]  # Use exact figures, avoiding vague amounts.\n    time: List[str]  # Define occurrence times if clearly specified.\n    place: List[str]  # Mention the transaction location if detailed.\n\nclass Meet(ContactEvent):\n    \"\"\"\n    Events where entities gather face-to-face, e.g., meetings, summits,\n    or conferences.\n\n\n                                       32\n\nPreprint. Under review.\n\n\n\n\n\n    \"\"\"\n    mention: str  # Central meeting references like \"summit\", without\n        extra detail.\n    entity: List[str]  # List participants clearly, omitting superfluous\n        descriptions.\n    time: List[str]  # Specify times if explicitly provided.\n    place: List[str]  # Mention locations if available, avoiding\n        unsupported assumptions.\n\nclass PhoneWrite(ContactEvent):\n    \"\"\"\n    Non-face-to-face communications, covering written and phone-based\n    interactions.\n    \"\"\"\n    mention: str  # Terms indicating communication, e.g., \"called\",\n        succinctly.\n    entity: List[str]  # Capture the participants in the communication.\n    time: List[str]  # Specify times if mentioned, ensuring clarity.\n\nclass DeclareBankruptcy(BusinessEvent):\n    \"\"\"\n    Occurs when an organization requests legal protection from debt\n    collection.\n    \"\"\"\n    mention: str  # Use declarations like \"bankruptcy\", clearly.\n    org: List[str]  # Focus on the organizational name in question.\n    time: List[str]  # Mention when the declaration occurs if explicitly\n        stated.\n    place: List[str]  # Note the declaration’s location if outlined.\n\nclass EndOrg(BusinessEvent):\n    \"\"\"\n    An organization ceases operations, going out of business completely.\n    \"\"\"\n    mention: str  # Use terms like \"shut down\" to capture essence\n        effectively.\n    org: List[str]  # Succinctly list the organizations ending operations\n        .\n    time: List[str]  # Clearly mention when specifics are supplied.\n    place: List[str]  # Mention location details if clearly stated.\n\nclass Die(LifeEvent):\n    \"\"\"\n    Event marking the end of life, covering direct, accidental, and self-\n    inflicted cases.\n    \"\"\"\n    mention: str  # Specific terms like \"died\", excluding excess context.\n    agent: List[str]  # Cite any responsible party if indicated.\n    victim: List[str]  # Precisely identify the deceased without titles.\n    instrument: List[str]  # Specify instruments used if described.\n    time: List[str]  # Use accurate timing where provided.\n    place: List[str]  # Mention locations where explicitly noted.\n\nclass SentenceAct(JusticeEvent):\n    \"\"\"\n    Legal sentence issuance, often involving incarceration.\n    \"\"\"\n    mention: str  # Direct words like \"sentenced\", retaining clarity.\n    defendant: List[str]  # Identify the sentenced party succinctly.\n    adjudicator: List[str]  # State the authority issuing the sentence.\n    crime: List[str]  # Precisely include mentioned crimes.\n    sentence: List[str]  # Clearly outline the penalties involved.\n    time: List[str]  # Specific timing if explicitly declared.\n    place: List[str]  # Cite location details when supplied.\n\n\n\n                                       33\n\nPreprint. Under review.\n\n\n\n\n\nclass Sue(JusticeEvent):\n    \"\"\"\n    The initiation of legal proceedings against an entity to determine\n    liability.\n    \"\"\"\n    mention: str  # Specific terms like \"sued\".\n    plaintiff: List[str]  # Clearly identify the suing parties.\n    defendant: List[str]  # Identify the sued entities unambiguously.\n    adjudicator: List[str]  # Specify judicial role if expressed.\n    crime: List[str]  # Highlight alleged crimes if specified.\n    time: List[str]  # Reference explicit timing if detailed.\n    place: List[str]  # Extract the location details if outlined.\n\nclass Appeal(JusticeEvent):\n    \"\"\"\n    Represents decisions moved to higher courts for further review.\n    \"\"\"\n    mention: str  # Use terms like \"appealed\" directly.\n    defendant: List[str]  # Name the entity under review.\n    prosecutor: List[str]  # Name the initiating party of the appeal.\n    adjudicator: List[str]  # Reference the reviewing court.\n    crime: List[str]  # Clearly detail crimes if mentioned.\n    time: List[str]  # Capture filing times if explicit.\n    place: List[str]  # Mentioned locale of appeal if detailed.\n\n\n\nC.4  EXAMPLE OF OPTIMAL TASK INSTRUCTION AND EVENT GUIDELINES GENERATED BY\n    GPT-4O\n\n# This is an event extraction task where the goal is to extract\n    structured events from the text following structured event\n    definitions in Python. A structured event contains an event trigger\n    word, an event type, the arguments participating in the event, and\n    their roles in the event.\n\n# Task Instructions:\n1. For each different event type, output the extracted information from\n    the text into a Python list format where:\n   - The first key ’mention’ holds the value of the event trigger.\n   - Subsequent keys/values follow the class definitions below.\n\n2. Structure the output in a valid Pydantic format: ‘result = [EventName(\n    \"mention\" = \"trigger\", \"arg1_key\" = \"arg1_span\", ...)]‘.\n3. Adhere strictly to the described event descriptions and role\n    definitions, considering implicit contexts and indirect attributions.\n4. Address special cases:\n   - Appeals: Consider involved parties from prior related events as ‘‘\n       prosecutor’’.\n   - Multiple roles may apply contextually; ensure complete information\n       extraction.\n   - Implicit indications: If mentions like \"filed\", \"concluded\", etc.,\n       suggest indirect roles, use context to clarify them.\n\n5. Maintain backward compatibility where applicable. Do not output\n    anything else except parsable structured event format in Python.\n\n\n\n\n\n# Here are the event definitions:\n\nclass Convict(JusticeEvent):\n    \"\"\"\n\n\n                                       34\n\nPreprint. Under review.\n\n\n\n\n\n    A Convict Event occurs whenever a Try Event ends with a successful\n    prosecution of the Defendant.\n    There may not always be explicit mentions of crimes in the text; use\n    contextual clues.\n    \"\"\"\n    mention: str  # The text span that expresses the conviction (e.g., \"\n        convicted\").\n    defendant: List[str]  # The entity found guilty, search for adjacent\n        terms like \"defendant\".\n    adjudicator: List[str]  # The judge or court, often implicitly\n        understood from context.\n    crime: List[str]  # Crime references, even implied (e.g., \"guilty of\n        ...\").\n    time: List[str]  # When conviction happens, contextual or explicit\n        dates.\n    place: List[str]  # Where the conviction occurs, often a court or\n        city name nearby.\n\nclass TransferMoney(TransactionEvent):\n    \"\"\"\n    Refers to money transfer actions outside purchasing contexts.\n    Recognize givers and recipients even in indirect mentions.\n    \"\"\"\n    mention: str  # Turn of phrase indicating transfer (e.g., \"\n        transferred\", \"donated\").\n    giver: List[str]  # Entity initiating transfer (may be implied; use\n        context).\n    recipient: List[str]  # Direct receiver of money, often clearly\n        stated.\n    beneficiary: List[str]  # Can be implied; beneficiaries are often\n        indirect.\n    money: List[str]  # Described amounts; look for currency signs ($, e,\n         etc.).\n    time: List[str]  # Dates or relative times (e.g., \"two years ago\").\n    place: List[str]  # Locations of transaction, if specified.\n\nclass Meet(ContactEvent):\n    \"\"\"\n    Occurs when entities meet face-to-face; discern collective entity\n    mentions from individual roles.\n    \"\"\"\n    mention: str  # Trigger phrases (e.g., \"meet\", \"conference\").\n    entity: List[str]  # Entities, clarified through context or explicit\n        mentions.\n    time: List[str]  # When entities meet, even if future planned.\n    place: List[str]  # Meeting location, from nearby phrases.\n\nclass PhoneWrite(ContactEvent):\n    \"\"\"\n    Encompasses non-face-to-face communications; cover implied\n    interactors.\n    \"\"\"\n    mention: str  # Non-direct communication identified triggers (e.g., \"\n        called\", \"emailed\").\n    entity: List[str]  # Communicating entities, occasionally understood\n        indirectly.\n    time: List[str]  # Times derived from text, even if not very specific\n        .\n\nclass DeclareBankruptcy(BusinessEvent):\n    \"\"\"\n    An event signifying financial distress declarations; distinguish from\n     emergence narratives.\n    \"\"\"\n    mention: str  # Indicators like \"declared bankruptcy\".\n\n\n                                       35\n\nPreprint. Under review.\n\n\n\n\n\n    org: List[str]  # Company/entity that declared, directly mentioned.\n    time: List[str]  # Declaration date, often provided.\n    place: List[str]  # Geographical context of declaration.\n\nclass EndOrg(BusinessEvent):\n    \"\"\"\n    Concludes an organization’s operations; ensure specificity of\n    organization ceases.\n    \"\"\"\n    mention: str  # Marks of closure (e.g., \"dissolved\", \"shutdown\").\n    org: List[str]  # Organization ending, referenced in texts.\n    time: List[str]  # Date context around organization ending.\n    place: List[str]  # Location tied to organizational operations.\n\nclass Die(LifeEvent):\n    \"\"\"\n    Recognizes cessation of life events; determine involvements from\n    surrounding text.\n    \"\"\"\n    mention: str  # Triggering term showing death (e.g., \"died\", \"passed\n        away\").\n    agent: List[str]  # Agents causing death if deliberate; contextual\n        deductions.\n    victim: List[str]  # Deceased, named or implied victims.\n    instrument: List[str]  # Weapons or causes if mentioned.\n    time: List[str]  # Death-related timing, even metaphorical.\n    place: List[str]  # Place the death occurred, discerned from text.\n\nclass SentenceAct(JusticeEvent):\n    \"\"\"\n    Legal actions culminating in punishment; include implied authority\n    adjudication references.\n    \"\"\"\n    mention: str  # Verbs indicating sentencing (e.g., \"sentenced\").\n    defendant: List[str]  # Persons sentenced, more direct mentions.\n    adjudicator: List[str]  # State actor issuing punishment.\n    crime: List[str]  # Crimes specified can be explicit or by context\n        related.\n    sentence: List[str]  # Detailed punishments, commonly listed.\n    time: List[str]  # Contextual timing of legal processes.\n    place: List[str]  # Legal venues, stated or implicit.\n\nclass Sue(JusticeEvent):\n    \"\"\"\n    Legal actions initiation detections; interpreting mentions to detect\n    implicated parties.\n    \"\"\"\n    mention: str  # Lawsuit trigger terms (e.g., \"sued\").\n    plaintiff: List[str]  # Agents initiating, even implicit from context\n        .\n    defendant: List[str]  # Specific subjects of the lawsuit.\n    adjudicator: List[str]  # Legal bodies, typically explicit.\n    crime: List[str]  # Charges or offenses underpinning the suit.\n    time: List[str]  # Suit filing and related timings.\n    place: List[str]  # Locations cited, often courts.\n\nclass Appeal(JusticeEvent):\n    \"\"\"\n    Reviewal legal challenges; correctly attribute events around\n    appellate actions.\n    \"\"\"\n    mention: str  # Terms denoting appeals like \"appealed\".\n    defendant: List[str]  # Party whose case goes under review.\n    prosecutor: List[str]  # Original case actors initiating the appeal,\n        inferred.\n\n\n                                       36\n\nPreprint. Under review.\n\n\n\n\n\n    adjudicator: List[str]  # Higher court taking the over evaluation.\n    crime: List[str]  # Reviews’ subject offenses.\n    time: List[str]  # Appeal reference times, may not be given.\n    place: List[str]  # Court location details or broader judicial zones.\n\n\n\n\n\n                                       37",
"headers": [
"arXiv:2504.07357v2  [cs.CL]  16 Oct 2025",
"R",
"P",
"O",
"L",
"-",
"M",
"—A C",
"S",
"E",
"TION",
"EVISITING",
"ROMPT",
"PTIMIZATION WITH",
"ARGE",
"EA",
"SONING",
"ODELS",
"ASE",
"TUDY ON",
"VENT",
"XTRAC",
"A",
"1",
"I",
"2",
"W",
"3",
"4",
"5",
"F",
"6",
"C",
"D",
"B",
"T",
"G"
],
"tables": [
"|e Models|40.84<br>Average LRM performance as opt LRM|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|e Models<br>|40.84<br>Average LRM performance as<br>ot<br>LRM<br>|||||40.84<br>LRM<br>|\n|le capabil-<br>     rocessing||Average L|RM performance|as<br>ot|||\n|<br>    n complex||<br>Average L|<br> LM performance|32<br> <br>p<br>   as<br>t|35<br>4<br>.51<br>36.9<br><br>  )|0<br>1<br>Optimiz<br>helps a<br>benefit<br>opti<br>yeild<br>signi<br>impr|\n|<br>     ted (Zhou<br>||<br>Average L|<br> RM performance|<br>op<br>   as<br>task|<br>op<br>   as<br>task|<br>op<br>   as<br>task|\n|ss of mod-<br>     ls (LRMs),<br>     reasoning<br>||16.47<br> <br>Average L<br>Best LRM,|16.47<br> <br>Average L<br>Best LRM,|30<br><br>31.25<br> <br><br>   as<br>task<br> o Optimization<br>  mization)|30<br><br>31.25<br> <br><br>   as<br>task<br> o Optimization<br>  mization)|30<br><br>31.25<br> <br><br>   as<br>task<br> o Optimization<br>  mization)|\n|ethodolo-<br>     epSeek-R1<br>hong et al.,<br>|ethodolo-<br>     epSeek-R1<br>hong et al.,<br>|ethodolo-<br>     epSeek-R1<br>hong et al.,<br>|ethodolo-<br>     epSeek-R1<br>hong et al.,<br>|ethodolo-<br>     epSeek-R1<br>hong et al.,<br>|ethodolo-<br>     epSeek-R1<br>hong et al.,<br>|ethodolo-<br>     epSeek-R1<br>hong et al.,<br>|",
"|Step|Col2|3|\n|---|---|---|",
"|Step|4|\n|---|---|",
"|GPT-4o<br>GPT-4.5<br>o1<br>DS-R1|12.68|18.18 +5.50|16.67 +3.99|18.83 +6.15|20.15 +7.47|15.31|\n|---|---|---|---|---|---|---|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|16.47|19.33** +2.86**|16.47** 00.00**|19.32** +2.85**|**22.31 +5.84**|24.57|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|13.94|18.96** +5.02**|18.57** +4.63**|20.29** +6.35**|**21.92 +7.98**|489.67|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|16.45|18.67** +2.22**|18.57** +2.12**|21.83** +5.38**|**24.66**_ :::_<br>**+8.21**|217.71|",
"|GPT-4o<br>GPT-4.5<br>o1<br>DS-R1|12.68|22.32 +9.64|27.54 +14.86|26.30 +13.62|25.10 +12.42|17.31|\n|---|---|---|---|---|---|---|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|16.47|29.63** +13.16**|35.94** +19.47**|**36.51 +20.04**|35.42** +18.95**|28.75|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|13.94|30.19** +16.25**|36.67** +22.73**|**36.98 +23.04**|36.96** +23.02**|543.45|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|16.45|32.20** +15.75**|37.14** +20.69**|38.77** +22.32**|**40.00**_ ::::_<br>**+23.55**|277.11|",
"|GPT-4o<br>GPT-4.5<br>o1<br>DS-R1|12.68|28.04 +15.36|27.03 +14.35|28.57 +15.89|27.31 +14.63|17.55|\n|---|---|---|---|---|---|---|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|16.47|32.35** +15.88**|37.58** +21.11**|36.22** +19.75**|**37.74 +21.27**|32.65|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|13.94|33.52** +19.58**|37.78** +23.84**|38.71** +24.77**|**39.81 +25.87**|575.36|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|16.45|37.97** +21.52**|38.40** +21.95**|40.58** +24.13**|**44.26**_ ::::_<br>**+27.81**|301.45|",
"|GPT-4o<br>GPT-4.5<br>o1<br>DS-R1|13.33|26.94 +13.61|34.75 +21.42|30.59 +17.26|35.79+22.46|27.00|\n|---|---|---|---|---|---|---|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|14.29|27.31** +13.02**|35.29** +21.00**|36.59** +22.30**|**36.69 +22.40**|35.56|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|15.38|28.57** +13.19**|36.73** +21.35**|**38.71 +23.33**|37.86** +22.48**|526.43|\n|**GPT-4o**<br>**GPT-4.5**<br>**o1**<br>**DS-R1**|16.00|31.93** +15.93**|41.98** +25.98**|42.06** +26.06**|**43.75**_ ::::_<br>**+27.75**|211.43|",
"|NO OPTI-<br>MIZATION i<br>Best Scores y<br>TI - 39.29<br>TC - 33.93 E|# This is an event extraction task where the goal i|s to extract structured events from the text following structured event def|initions|\n|---|---|---|---|\n|**NO OPTI-**<br>**MIZATION**<br>Best Scores<br>TI - 39.29<br>TC - 33.93<br> <br>i<br>y<br>E|n Python.** (...)** For each different event type, plea|se output the extracted information from the text into a python list format** (...)**|se output the extracted information from the text into a python list format** (...)**|\n|**NO OPTI-**<br>**MIZATION**<br>Best Scores<br>TI - 39.29<br>TC - 33.93<br> <br>i<br>y<br>E|ou should always output in a valid pydantic form|at: result = [EventName(”mention” = ”trigger”, ”arg1~~ k~~ey” = ”arg1~~ s~~p|an”, ...),|\n|**NO OPTI-**<br>**MIZATION**<br>Best Scores<br>TI - 39.29<br>TC - 33.93<br> <br>i<br>y<br>E|ventName(”mention” = ”trigger”, ”arg1~~ k~~ey” =|”arg1~~ s~~pan”, ...)].** (...)**|”arg1~~ s~~pan”, ...)].** (...)**|",
"|GPT-4O # This is an event extraction task where the goal i<br>Best Scores # Task Instructions: 1. For each different event ty<br>TI - 48.28 2. Structure the output in a valid Pydantic format:<br>TC - 48.28|This is an event extraction task where the goal i|s to extract structured events (...)<br>pe, output the extracted information from the text (...)<br>‘result = [EventName(”mention” = ”trigger”, (...).|\n|---|---|---|\n|**GPT-4O**<br>Best Scores<br>TI - 48.28<br>TC - 48.28<br># This is an event extraction task where the goal i<br># Task Instructions: 1. For each different event ty<br>2. Structure the output in a valid Pydantic format:|. Structure the output in a valid Pydantic format:|. Structure the output in a valid Pydantic format:|",
"|AC - 37.97 4<br>-<br>-|. Address special cases:- Appeals: Consider invo|lved parties from prior related events as “prosecutor”.<br>omplete information extraction.<br>oncluded”, etc.,(...) use context to clarify them.(...)|Col4|\n|---|---|---|---|\n|<br>AC - 37.97<br>4<br>-<br>-|Multiple roles may apply contextually; ensure c|Multiple roles may apply contextually; ensure c|Multiple roles may apply contextually; ensure c|\n|<br>AC - 37.97<br>4<br>-<br>-|Implicit indications: If mentions like ”filed”, ”c|Implicit indications: If mentions like ”filed”, ”c|Implicit indications: If mentions like ”filed”, ”c|\n|**GPT-4.5**<br>Best Scores<br> <br>#<br>s|This is an event extraction task for identifyin|g and structuring events from text using Python-defined event classe|s. Each|\n|**GPT-4.5**<br>Best Scores<br> <br>#<br>s|tructured event consists of an event trigger word|, an event type** (...)**|, an event type** (...)**|",
"|DEEPSEEK- #<br>R1 1<br>Best Scores n<br>TI - 56.60<br>TC - 56.60 -<br>AI - 44.26 (<br>AC - 44.26 -<br>p<br>-<br>-<br>2<br>-<br>-<br>3<br>4|Event Extraction Task: Extract structured event|s from text using Python class definitions.(...):|Col4|\n|---|---|---|---|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|. Span Extraction:- Triggers: Minimal contiguo|us spans (verbs/nouns) directly expressing the event. Include both ver|bal and|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|ominal forms** (”death” = Die, ”killings” = Die)**|.**(...)**|.**(...)**|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|Arguments: - Remove articles** (”a/an/the”)** and|possessive pronouns EXCEPT when part of official names or temporal|phrases|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|**”The Hague”, ”the past year”)**|||\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|Resolve pronouns AND POSSESSIVE NOUN|S to named entities immediately using same-sentence antecedents** (”a**|** irline’s**|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|**lan” →[”airline”])**|nts** (”Philadelphia lawyers” →”lawyers”) (...)**<br>       ources/amounts** (”stereo worth $1,750 from family”)** unless legal terms** (...)**<br>** t bust” →EndOrg(...)**<br>**  convicted of killings...”)** without truncation<br>       cles when part of phrase** (”the early 90’s”)**<br>       as** (...)**|nts** (”Philadelphia lawyers” →”lawyers”) (...)**<br>       ources/amounts** (”stereo worth $1,750 from family”)** unless legal terms** (...)**<br>** t bust” →EndOrg(...)**<br>**  convicted of killings...”)** without truncation<br>       cles when part of phrase** (”the early 90’s”)**<br>       as** (...)**|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|Strip role/location/age descriptors from argume|Strip role/location/age descriptors from argume|Strip role/location/age descriptors from argume|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|Keep FULL spans for crimes/money including s|ources/amounts** (”stereo worth $1,750 from family”)** unless legal ter|ources/amounts** (”stereo worth $1,750 from family”)** unless legal ter|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|. Special Handling:- Bankruptcy Triggers:** ”wen**|. Special Handling:- Bankruptcy Triggers:** ”wen**|. Special Handling:- Bankruptcy Triggers:** ”wen**|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|Crime Spans: Retain full contextual clauses** (”If**|Crime Spans: Retain full contextual clauses** (”If**|Crime Spans: Retain full contextual clauses** (”If**|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|Temporal Phrases: Keep original spans with arti|Temporal Phrases: Keep original spans with arti|Temporal Phrases: Keep original spans with arti|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|. Output Rules: Always output in Python-format|. Output Rules: Always output in Python-format|. Output Rules: Always output in Python-format|\n|**DEEPSEEK-**<br>**R1**<br>Best Scores<br>TI - 56.60<br>TC - 56.60<br>AI - 44.26<br>AC - 44.26<br>#<br>1<br>n<br>-<br>**(**<br>-<br>**p**<br>-<br>-<br>2<br>-<br>-<br>3<br>4|. Critical Exceptions:**-(...)**|. Critical Exceptions:**-(...)**|. Critical Exceptions:**-(...)**|\n|**O1**<br>Best Scores<br>TI - 66.67<br>TC - 66.67<br>AI - 44.93<br>#<br>i<br>K<br>**”**|This is an event extraction task where the goal is|to extract structured events from the text following structured event def|initions|\n|**O1**<br>Best Scores<br>TI - 66.67<br>TC - 66.67<br>AI - 44.93<br>#<br>i<br>K<br>**”**|n Python.** (...)**|||\n|**O1**<br>Best Scores<br>TI - 66.67<br>TC - 66.67<br>AI - 44.93<br>#<br>i<br>K<br>**”**|eep argument references minimal by removing a|rticles, possessives, or descriptive words unless they are crucial identifie|rs (e.g.,|\n|**O1**<br>Best Scores<br>TI - 66.67<br>TC - 66.67<br>AI - 44.93<br>#<br>i<br>K<br>**”**|**the retailer”**_ →_**”retailer”**,** ”my uncle”**_ →_**”un**|**cle”**).|**cle”**).|",
"|AC - 40.58 #|# Important guidelines to address prior errors:|Col3|\n|---|---|---|\n|<br>#<br>#<br>#<br>#<br>#<br>#|1. For each event trigger, use the single most rel|evant word (e.g.,** ”bankruptcy”** rather than** ”file for bankruptcy”**).|\n|<br>#<br>#<br>#<br>#<br>#<br>#|2. For argument roles, also use minimal spans (|e.g.,** ”soldier”** instead of** ”a soldier,” ”woman”** instead of** ”a woman”**|\n|<br>#<br>#<br>#<br>#<br>#<br>#|4. For justice events (Sue, Appeal, Convict, Sen|4. For justice events (Sue, Appeal, Convict, Sen|\n|<br>#<br>#<br>#<br>#<br>#<br>#|5. For transfers of money, watch for direct or in|5. For transfers of money, watch for direct or in|\n|<br>#<br>#<br>#<br>#<br>#<br>#|6. Do not skip events implied by synonyms or i|6. Do not skip events implied by synonyms or i|\n|<br>#<br>#<br>#<br>#<br>#<br>#|7. If there is more than one event in a single tex|7. If there is more than one event in a single tex|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|40|Col10|Col11|Col12|Col13|Col14|Col15|Col16|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|0<br>1<br>2<br>3<br>4<br>5<br>Tree Depth<br>0<br>10<br>20<br>30<br>40<br><br>~~DeepSeek-R1~~<br>O1<br>~~GPT-4.5~~<br>GP~~T-~~4o||||||||0<br>1<br>2<br>3<br>4<br>5<br>Tree Depth<br>0<br>10<br>20<br>30<br>40<br>AC<br>DeepSeek~~-~~R1<br>O1<br>GP~~T-~~4.5<br>GP~~T-~~4o||||||||\n|0<br>1<br>2<br>3<br>4<br>5<br>Tree Depth<br>0<br>10<br>20<br>30<br>40<br><br>~~DeepSeek-R1~~<br>O1<br>~~GPT-4.5~~<br>GP~~T-~~4o||||||||||||||||\n|0<br>1<br>2<br>3<br>4<br>5<br>Tree Depth<br>0<br>10<br>20<br>30<br>40<br><br>~~DeepSeek-R1~~<br>O1<br>~~GPT-4.5~~<br>GP~~T-~~4o||||||||||||||||\n|0<br>1<br>2<br>3<br>4<br>5<br>Tree Depth<br>0<br>10<br>20<br>30<br>40<br><br>~~DeepSeek-R1~~<br>O1<br>~~GPT-4.5~~<br>GP~~T-~~4o||||||||||||||||\n|0<br>1<br>2<br>3<br>4<br>5<br>Tree Depth<br>0<br>10<br>20<br>30<br>40<br><br>~~DeepSeek-R1~~<br>O1<br>~~GPT-4.5~~<br>GP~~T-~~4o||||||~~De~~<br>O1<br>|~~pSeek-R1~~<br>|~~pSeek-R1~~<br>||||||De<br>O1<br>|pSeek~~-~~R<br>|\n|0<br>1<br>2<br>3<br>4<br>5<br>Tree Depth<br>0<br>10<br>20<br>30<br>40<br><br>~~DeepSeek-R1~~<br>O1<br>~~GPT-4.5~~<br>GP~~T-~~4o|||1<br>2<br>3<br><br>~~GPT~~<br>GP~~T~~|1<br>2<br>3<br><br>~~GPT~~<br>GP~~T~~|1<br>2<br>3<br><br>~~GPT~~<br>GP~~T~~|1<br>2<br>3<br><br>~~GPT~~<br>GP~~T~~|4<br>5<br>~~-4.5~~<br>~~-~~4o|4<br>5<br>~~-4.5~~<br>~~-~~4o|4<br>5<br>~~-4.5~~<br>~~-~~4o|4<br>5<br>~~-4.5~~<br>~~-~~4o|1<br>2<br>3<br><br>GP~~T~~<br>GP~~T~~|1<br>2<br>3<br><br>GP~~T~~<br>GP~~T~~|1<br>2<br>3<br><br>GP~~T~~<br>GP~~T~~|1<br>2<br>3<br><br>GP~~T~~<br>GP~~T~~|4<br>5<br>~~-~~4.5<br>~~-~~4o|\n|0<br>1<br>2<br>3<br>4<br>5<br>Tree Depth<br>0<br>10<br>20<br>30<br>40<br><br>~~DeepSeek-R1~~<br>O1<br>~~GPT-4.5~~<br>GP~~T-~~4o|||1<br>2<br>3<br><br>~~GPT~~<br>GP~~T~~|1<br>2<br>3<br><br>~~GPT~~<br>GP~~T~~|Dept|h|h|h|h|h|h|h|Dept|h|h|",
"|4|Col2|Col3|\n|---|---|---|",
"|47.3 50.|0|\n|---|---|",
"|49.6 51|.8|Col3|\n|---|---|---|",
"|55.6<br>49.0 50.6|Col2|Col3|\n|---|---|---|\n||||",
"|59.2 57|Col2|\n|---|---|\n|41.2<br>44.||",
"|65.0|Col2|55.6|\n|---|---|---|\n|40.6<br>43|.4||",
"|26.5|Col2|25.2|\n|---|---|---|\n||||",
"|26.7 28.|0|\n|---|---|\n|||",
"|28.0<br>24|.1|Col3|\n|---|---|---|\n||||",
"|25.0|Col2|23.0|\n|---|---|---|\n||||",
"|25.0 26.|4 25|\n|---|---|\n|||",
"|22|.5|Col3|\n|---|---|---|\n||||",
"|(truncated) The model's prediction didn't include this,<br>possibly because the coref wasn't resolved. The<br>current guideline says pronouns require coref|Col2|\n|---|---|\n|_(truncated) _The model's prediction didn't include this,<br>possibly because the coref wasn't resolved.The<br>current guideline says pronouns require coref|he|\n|current guideline says pronouns require core|current guideline says pronouns require core|\n|resolution before extraction. So if \"his\" is unresolved,|resolution before extraction. So if \"his\" is unresolved,|\n|it's omitted. But in the correct answer, \"his\" is resolve|it's omitted. But in the correct answer, \"his\" is resolve|\n|to Arafat. So the model might not have access to|to Arafat. So the model might not have access to|\n|coref resolution, leading to omission|coref resolution, leading to omission|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||",
"|DeepSeek-R1 (Optimizer) o1 (Optimizer) GPT-4.5 (Optimizer) GPT-4o (Optimizer)<br>Models<br>TI TC AI AC TI TC AI AC TI TC AI AC TI TC AI AC|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|**DeepSeek-R1**|37.5|33.93|25.57|24.66|27.72|25.74|18.67|18.67|36.89|34.95|22.91|22.91|32.78|32.78|21.83|21.83|\n|**o1**|31.54|31.54|21.92|21.92|29.33|29.33|18.96|18.96|31.91|31.91|18.57|18.57|29.24|29.24|21.74|20.29|\n|**GPT-4.5**|36.04|34.23|23.14|22.31|34.78|33.04|20.07|19.33|31.37|31.37|19.32|19.32|30.29|30.29|20.97|20.19|\n|**GPT-4o**|35.29|35.29|22.07|20.15|28.28|28.28|18.18|18.18|30.61|30.61|16.67|16.67|31.67|31.67|19.57|18.83|",
"|DeepSeek-R1 (Optimizer) o1 (Optimizer) GPT-4.5 (Optimizer) GPT-4o (Optimizer)<br>Models<br>TI TC AI AC TI TC AI AC TI TC AI AC TI TC AI AC|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|**DeepSeek-R1**|63.16|63.16|40.00|40.00|65.45|65.45|32.2|32.2|56.25|56.25|37.14|37.14|62.7|62.7|40.06|38.77|\n|**o1**|78.95|78.95|39.13|36.96|54.78|54.78|33.96|30.19|59.26|59.26|36.67|36.67|57.14|57.14|36.98|36.98|\n|**GPT-4.5**|64.71|64.71|35.42|35.42|46.15|46.15|29.63|29.63|63.57|63.57|35.94|35.94|59.21|59.21|38.1|36.51|\n|**GPT-4o**|30.00|30.00|25.88|25.1|28.57|28.57|22.32|22.32|34.55|34.55|27.54|27.54|29.38|29.38|26.99|26.3|",
"|DeepSeek-R1 (Optimizer) o1 (Optimizer) GPT-4.5 (Optimizer) GPT-4o (Optimizer)<br>Models<br>TI TC AI AC TI TC AI AC TI TC AI AC TI TC AI AC|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|**DeepSeek-R1**|56.6|56.6|44.26|44.26|66.67|66.67|44.93|40.58|46.15|46.15|40.8|38.4|48.28|48.28|40.51|37.97|\n|**o1**|48.08|48.08|40.74|39.81|42.86|42.86|38.71|38.71|84.68|84.68|41.48|37.78|48.28|48.28|34.64|33.52|\n|**GPT-4.5**|45.68|45.68|38.36|37.74|51.24|51.24|36.22|36.22|59.26|59.26|36.24|37.58|41.18|41.18|32.35|32.35|\n|**GPT-4o**|49.09|49.09|28.11|27.31|61.11|61.11|28.57|28.57|52.00|52.00|27.03|27.03|61.54|61.54|29.91|28.04|",
"|DeepSeek-R1 (Optimizer) o1 (Optimizer) GPT-4.5 (Optimizer) GPT-4o (Optimizer)<br>Models<br>TI TC AI AC TI TC AI AC TI TC AI AC TI TC AI AC|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|**DeepSeek-R1**|69.23|67.69|44.33|43.75|54.12|54.12|42.06|42.06|52.8|52.8|41.98|41.98|47.27|47.27|33.61|31.93|\n|**o1**|68.28|67.76|38.44|37.86|67.86|67.86|38.71|38.71|58.29|58.29|36.73|36.73|41.11|41.11|28.57|28.57|\n|**GPT-4.5**|68.31|68.31|38.44|36.69|64.71|64.71|39.02|36.59|56.45|56.45|35.29|35.29|49.09|49.09|28.11|27.31|\n|**GPT-4o**|59.44|59.44|36.99|35.71|64.52|64.52|30.59|30.59|56.57|56.57|34.75|34.75|48.19|48.19|26.94|26.94|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2504.07357v2.pdf"
}