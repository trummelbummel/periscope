{
"text": "ELPO: Ensemble Learning Based Prompt Optimization\n                                 for Large Languages Models\n\n\n\n             Qing Zhang * 1 Bing Xu * 1 Xudong Zhang * 1 Yifan Shi * 1 Yang Li 1 Chen Zhang 2 Yik Chung Wu 2\n                            Ngai Wong 2  Yijie Chen 1 Hong Dai 1 Xiansen Chen 1 Mian Zhang 1\n\n\n                         Abstract                           agents redefining the relationship between humans and intel-\n                                                                             ligent systems. Flagship families including GPT (Radford            The remarkable performance of Large Language\n                                                                                   et al., 2018; 2019; Brown et al., 2020; Achiam et al., 2023),             Models (LLMs) highly relies on crafted prompts.\n                                             LLaMA (Touvron et al., 2023a;b), and PaLM (Anil et al.,2025        However, manual prompt engineering is a labori-                                                               2023) are trained on web-scale corpora and display emer-              ous process, creating a core bottleneck for prac-\n                                                                    gent capabilities that are unforeseen in smaller-scale pre-                  tical application of LLMs.  This phenomenon\n                                                                        decessors. Among these, in-context learning (Brown et al.,              has led to the emergence of a new research areaNov                                                         2020) exemplifies a paradigm shift: models without any            known as Automatic Prompt Optimization (APO),\n                                                                            fine-tuning can tackle sentiment analysis, text classification,             which develops rapidly in recent years.  Exist-20                                                         code generation, logical reasoning, and other diverse tasks               ing APO methods such as those based on evolu-\n                                                          by following natural language instructions, also known as                tionary algorithms or trial-and-error approaches\n                                                                   “prompts”.                 realize an efficient and accurate prompt optimiza-\n                tion to some extent. However, those researches        This ability has fueled visions of a “general-purpose lin-\n               focus on a single model or algorithm for the gen-         guistic interface” where machine behavior is shaped as ef-\n                eration strategy and optimization process, which          fortlessly as conversing with a colleague. Yet, this promise[cs.CL]\n                 limits their performance when handling complex       comes with a sharp problem: LLMs are strikingly sensi-\n                 tasks. To address this, we propose a novel frame-          tive to small changes in prompts (Jiang et al., 2020; Zhao\n            work called Ensemble Learning based Prompt          et al., 2021; Lu et al., 2022). Synonym substitutions, mi-\n              Optimization (ELPO) to achieve more accurate        nor structural tweaks or rephrased instructions may lead\n             and robust results. Motivated by the idea of en-         to outputs drastically different from what human intuition\n             semble learning, ELPO conducts voting mecha-        expects (Webson & Pavlick, 2022). Such fragility has pro-\n             nism and introduces shared generation strategies         pelled prompt engineering, the art and science of design-\n              along with different search methods for search-         ing prompts for high-quality outputs into the spotlight (Liu\n               ing superior prompts. Moreover, ELPO creatively          et al., 2023; Reynolds & McDonell, 2021). But for many\n                presents more efficient algorithms for the prompt          users, particularly non-experts, crafting effective prompts\n               generation and search process. Experimental re-           is an opaque, trial-and-error process, hindered by the vast,\n                 sults demonstrate that ELPO outperforms state-         unstructured search space of possible natural language in-\n                 of-the-art prompt optimization methods across          structions (Jiang et al., 2022).\n                 different tasks, e.g., improving F1 score by 7.6 on\n                                                         To ease this burden, the field has turned toward AutomaticarXiv:2511.16122v1        ArSarcasm dataset.                                                           Prompt Optimization (APO) (Zhou et al., 2023). APO au-\n                                                                  tomates the prompt design process by creating candidate\n                                                                              instructions and identifying the optimal ones through perfor-          1. Introduction\n                                                         mance evaluation. Strategies ranging from feedback-driven\n         Over the past few years, Large Language Models (LLMs)    refinement, evolutionary algorithms, to trajectory-based ex-\n         have emerged not merely as incremental improvements in    ploration have shown encouraging results. However, they\n           natural language processing (NLP), but as transformative    have surfaced some new, fundamental difficulties.  First,\n                                                                        relying on a single optimization algorithm risks fragility:\n             *Equal contribution 1ByteDance, China 2Department of Elec-                                                                           in light of the “No Free Lunch” theorem for optimization\n              trical and Electronic Engineering, The University of Hong\n                                                                  (Wolpert & Macready, 2002), no one strategy can consis-          Kong, HKSAR, China.   Correspondence  to:  Qing Zhang\n          <zhangqing.thomas@bytedance.com>.                            tently capture every subtlety across tasks. Second, most\n                                                                          existing systems treat the candidate pool as a flat, unstruc-\n\n\n                                                         1\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\ntured set, leading to wasted computation on unpromising    search and refinement strategies. The existing methods can\nvariants, thereby diminishing efficiency. These bottlenecks    be broadly categorized by their core mechanism for propos-\nleave APO methods struggling to fulfil the promise of truly    ing and selecting new prompts considering different opti-\nadaptive, scalable prompt engineering.                      mization space, criteria, operators and iterative algorithms\n                                                         (Cui et al., 2025).\nMotivated by both the remarkable potential of LLMs and\nthe instability of prompt-based interaction, we propose a   Many researches are based on soft prompt space optimiza-\nnovel framework for APO called Ensemble Learning based     tion (Li & Liang, 2021; Sun et al., 2022; Zou et al., 2023;\nPrompt Optimization (ELPO) which combines multiple   Zhao et al., 2024; Zhou et al., 2024; Zhao et al., 2025), de-\ngeneration and search algorithms to derive accurate and ro-    spite their efficiency, these methods suffer from two major\nbust results. As for the prompt generation, three strategies    drawbacks that limit their practical application, especially\nare applied to maintain the diversity and quality of candi-    with modern, closed-source LLMs. Firstly, they are inher-\ndate prompts.  It is expensive to evaluate each candidate     ently white-box, requiring direct access to the model’s inter-\nprompt on the entire training dataset (Prasad et al., 2023),    nal states, such as gradients and hidden layer activations, for\nso well-designed search methods for minimizing the queries    backpropagation. This is infeasible for practitioners who\nfor employing LLMs are also necessary. With respect to     interact with powerful models like those from OpenAI or\nprompt search, to the best of our knowledge, we are the    Anthropic exclusively through APIs (Pryzant et al., 2023).\nfirst to combine Bayesian search (Jones et al., 1998; Brochu    Secondly, the resulting optimized prompts are vectors of\net al., 2010; Snoek et al., 2012) and Multi-Armed Bandit     floating-point numbers, not human-readable text. Thus, it is\n(MAB) (Audibert & Bubeck, 2010; Lattimore & Szepesv´ari,    necessary to explore a novel APO algorithm with black-box\n2020), applying it to APO for improving search efficiency    APIs that this paper focuses on. Nevertheless, optimizing in\nsubstantially.  Inspired by the idea of ensemble learning    a high-dimensional, non-differentiable space of natural lan-\n(Zhou, 2012), a robust result is chosen by applying multiple    guage presents its own set of challenges, leading to various\ngeneration and search methods along with ensemble voting.    creative methodologies.\n\nIn summary, this paper makes the following main contribu-   Reinforcement Learning (RL) Based Algorithms. These\ntions.                                                methods formulates prompt optimization as an RL problem.\n                                                   Under this setting, the LLM acts as an agent, the prompt is\n (1) As for generation, we creatively propose Hard-Case     the state, and the actions are discrete text editing operations\n     Tracking which focuses on recurrent error samples and     (e.g., add, delete, or rephrase a word). The reward is derived\n     analyzes them in conjunction with failed prompts, em-   from the task performance on a validation dataset.  For\n     ploying large language models to generate more robust    example, RLPrompt (Deng et al., 2022) and TEMPERA\n    and generalizable prompts. Moreover, we combine it    (Zhang et al., 2023) train a policy network to decide which\n     with other two strategies simultaneously when generat-    editing actions to take. While promising, RL-based methods\n     ing new prompts.                                   can be complex to implement, often requiring the training\n                                                            of an auxiliary policy or reward model. Furthermore, the (2) In terms of search efficiency, we propose a novel search\n                                                                     discrete, phrase-level operations may lead to grammatically     algorithm in APO based on Bayesian search.  It re-\n                                                        flawed or semantically incoherent prompts (Prasad et al.,      flects prompts into high-dimensional spaces to increase\n                                                           2023).     search efficiency as a result of evaluation on only part\n     of the prompts.                                   Search and Evolution Based Algorithms.  Early ap-\n                                                        proaches in this domain treat prompt optimization as a (3) In terms of robustness and generalization, we use an\n                                                          search problem. Some methods employ a simple but of-    ensemble voting strategy that aggregates multiple well-\n                                                               ten inefficient Monte Carlo search, where a large number of     performing yet structurally diverse candidate prompts.\n                                                          candidate prompts are generated (e.g., through paraphras-\n (4) We conduct extensive experiments on various tasks     ing) and evaluated. The Automatic Prompt Engineer (APE)\n    and demonstrate that our algorithm outperforms state-   framework (Zhou et al., 2023) exemplifies this, using an\n     of-the-art methods. The ablation study validates the   LLM to generate diverse instructions and then selecting the\n     effectiveness of each individual component, confirm-    best one based on a score function. Inspired by APE, (Wang\n     ing their respective contributions to the algorithm’s     et al., 2024) propose PromptAgent which extend Monte\n     success.                                             Carlo to a search tree through a series of selections, expan-\n                                                                   sions, simulations, and backpropagation steps. To make the\n2. Related Work                                       search more structured, other works have turned to evolu-\n                                                                tionary algorithms. Methods like GPS (Xu et al., 2022), Evo-\nThe field of APO has rapidly evolved, moving from simple    Prompt (Guo et al., 2024), and PromptBreeder (Fernando\ngeneration-and-selection pipelines to highly sophisticated\n\n                                                2\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\net al., 2024) maintain a population of candidate prompts and    3. Methodology\niteratively apply genetic operators such as mutation (e.g.,\n                                                                    3.1. Preliminaryrephrasing a sentence), crossover (e.g., combining parts of\ntwo prompts). While more systematic than random search,   As we detail in Section 2, in the field of APO, various meth-\na primary drawback is that the search can be directionless    ods have been developed for searching the best prompts.\nand sample-inefficient. The generation of new candidates    However, a persistent characteristic observed across these\noften relies on random modifications without a clear signal    techniques is a notable degree of performance instability.\non how to improve the prompt, potentially wasting many   The efficacy of a single method can be highly sensitive to\nLLM API resources on unpromising candidates (Pryzant     initial conditions, stochastic elements within the algorithm,\net al., 2023).                                                 or minor perturbations in the training data (Breiman, 1996).\n                                                           Furthermore, a closer examination of the existing landscapeLLM-as-Optimizer and Feedback Based Algorithms. Re-\n                                                              reveals that no single method consistently outperforms allcently, a more directed approach has emerged that leverages\n                                                               others across all tasks or datasets since LLMs are probabilitythe LLM’s own reasoning capabilities to guide the optimiza-\n                                                       based models which bring unpredictable randomness natu-tion process (Pryzant et al., 2023; Zhou et al., 2023; Cheng\n                                                                          rally. For instance, APE may excel in scenarios requiringet al., 2024; Yang et al., 2024; Ye et al., 2024; Juneja et al.,\n                                                             broad, exploratory search with a higher cost, while ProTeGi2025; Xiang et al., 2025). These feedback-based methods\n                                                     might be more effective in solving problems with reason-typically operate in an iterative loop: (1) evaluate the cur-\n                                                            ing precess. Each approach possesses unique strengths andrent prompt on a batch of examples, (2) identify erroneous\n                                                         weaknesses, and the performance of any individual methodoutputs, and (3) feed these errors back to a powerful op-\n                                                       can be suboptimal or highly variable depending on the spe-timizer LLM, instructing it to critique the current prompt\n                                                                        cific problem context.  This suggests that the individualand propose refined versions. A foundational method in this\n                                                          models, or predictors, are good but unstable, a characteristicsubfield is ProTeGi (Pryzant et al., 2023), which introduces\n                                                                     that makes them prime candidates for improvement throughthe concept of “textual gradients”. In this framework, the\n                                                            aggregation techniques (Breiman, 1996; Zhou, 2012; GanaieLLM-generated critique serves as a semantic gradient for\n                                                                      et al., 2022).prompts. This directed feedback makes the search far more\nefficient than directionless Monte Carlo or evolutionary ap-    In most traditional approaches, the generation strategy and\nproaches. Other concurrent works have also explored using    the search process tend to rely on a single model or algo-\nLLM feedback to refine prompts for SQL-generation (Chen    rithm, which limits their performance when handling com-\net al., 2024) or general tasks (Ye et al., 2024).                 plex tasks. As a result, traditional methods usually lack flex-\n                                                                   ible adjustment mechanisms and struggle to quickly adaptAlthough all the aforementioned methods have demon-\n                                                    and respond when task requirements change. As shown instrated some achievements, they exhibit critical limitations\n                                                         Figure 1, the ensemble framework proposed in this paperthat our work aims to address. Firstly, these methods rely\n                                                                   integrates shared generation strategies, different search, andon a single optimization algorithm which limits their per-\n                                                           voting mechanisms. The main idea is to enhance the diver-formance. Secondly, they are often myopic, operating on\n                                                                     sity and adaptability of the model through the integrationa step-by-step basis. They generate feedback based only\n                                                            of multiple generation models, utilizing different feedbackon the errors in the current iteration and discard it once a\n                                                  mechanisms and optimization strategies. Furthermore, anew prompt is selected. Potentially valuable historical feed-\n                                                            voting mechanism is employed to ensure the reliability andback and unselected critiques are lost, forcing the optimizer\n                                                         accuracy of the final output. Compared with existing meth-to potentially rediscover information and leading to a less\n                                                             ods, this ensemble framework enables optimization fromefficient optimization process (Yan et al., 2025). Finally,\n                                                            multiple dimensions, effectively avoids the biases of sin-while some methods use retrieved exemplars to augment\n                                                              gle strategy, and gives a more comprehensive and efficientprompts during inference, the selection of these exemplars\n                                                                  solution.is typically based on general semantic similarity (Hu et al.,\n2024; Juneja et al., 2025), which may not be optimal for task\nperformance or align well with the optimized instruction.    3.2. Abundant Prompt Generation\nTo overcome these deficiencies, this paper proposes ELPO                                                            In the process of prompt optimization, the quantity and\nto derive accurate and robust results.                                                               quality of candidate prompts directly determine the out-\n                                                   come. To address this, we introduce an ensemble-based\n                                                             generation framework that leverages a multi-generator strat-\n                                                    egy to enhance both the diversity and quality of candidate\n                                                           prompts. Different generators are applied to capture various\n                                                                 task-specific details and complement one another. This en-\n                                                     semble mechanism not only broadens the range of choices\n\n\n                                                3\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n\n\n\n                                                 Figure 1. Pipeline of ELPO.\n\n\nduring optimization but also strengthens the accuracy and    Algorithm 1 Bad-Case Reflection\nrobustness of the final results.                                        1: Input: Initial prompt p, bad cases B, the number of\n                                                                         iterations for optimization TBad-Case Reflection. The core of the Bad-Case Reflec-\n                                                                          2: Sample bad case set Bs ⊂Btion lies in conducting in-depth analyses of erroneous cases\n                                                                          3: for t = 1, . . . , T do\nthrough a reflection mechanism. Traditional feedback-based\n                                                                          4:   Generate reflection prompt pref based on p and Bsmethods typically involve collecting erroneous examples\n                                                                          5:   Update target prompt pt using prefand directly modifying the prompts; however, these ap-\n                                                                          6:   Evaluate prompt pt on Bs to get a new bad cases setproaches merely focus on correcting errors and lack a pro-\nfound understanding of the underlying causes. In contrast,             bBs, and let Bs ←bBs\n                                                                          7: end for\nthe proposed method generates self-reflective prompts to\n                                                                          8: p∗←Generate Few-shot block from Bs and add it toassist the model in identifying the fundamental sources of\n                                                                        target prompt pTerror and iteratively refines the system prompts based on\n                                                                          9: Output: p∗\nthe reflection, thus improving the model’s performance on\nsimilar issues. Moreover, as shown in Algorithm 1, this\napproach leverages some failure cases to create few-shot\nexamples, which further enhance the effectiveness of the\nprompts. The iteration terminates when all bad cases are    1992a) in genetic algorithms. This strategy explores tran-\nresolved or the maximum number of iterations is reached.    sitions from existing solutions to potentially superior ones,\nCompared to conventional error-feedback-based techniques,    thereby enhancing the diversity of generated prompts. Zero-\nthis reflection-driven optimization method strengthens the    order generation, on the other hand, analyzes the charac-\nmodel’s generalization capabilities by incorporating few-     teristics of the current prompt population and generates\nshot examples.                                        an entirely novel prompt based on the structure and tech-\n                                                         niques of existing prompts. This approach emulates the\nEvolutionary Reflection. Evolutionary Reflection gener-                                                           crossover operation in genetic algorithms by synthesizing\nator is inspired by mutation and crossover operations in                                                               the attributes of multiple existing prompts, resulting in more\ngenetic algorithms (Holland, 1992b; Mitchell, 1998).The                                                            innovative candidate solutions. As we detail in Algorithm\nalgorithm adopts two distinct generation strategies: direct                                                                  2, these two strategies complement each other. Informed\nmutation and zero-order generation.  Direct mutation in-                                                  by heuristic principles of genetic methods, they establish a\nvolves modifying the current prompt directly to produce                                                     dynamic balance between local refinement and global explo-\nnew prompts that are semantically similar but expressed                                                                    ration, enabling the system to iteratively accumulate more\ndifferently, analogous to the mutation operation (Holland,                                                              diverse and promising candidate solutions.\n\n                                                4\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nAlgorithm 2 Evolutionary Reflection                     Algorithm 3 Hard-Case Tracking\n  1: Input: prompt population P, the number of iterations       1: Input: Bad case tracker B, prompt population P, the\n     for optimization T, the sizes of candidate prompts s1, s2         size of Hard-Case dataset k\n  2: Generate new candidate prompts P1 via direct mutation       2: Select top-k cases by error frequency from B to build\n    from P with |P1| = s1                                  Hard-Case dataset\n  3: Generate new candidate prompts via zero-order genera-                D :=\n    tion from P with |P2| = s2                                {(Casei, Failure timesi, Failed promptsi)}ki=1\n  4: Evaluate candidate prompts P1 ∪P2, and let p∗←best                                                                          3: Construct meta-prompt pmeta using D\n    prompt in P1 ∪P2                                                                          4: Use an LLM to generate improved prompt p∗based on\n  5: Output: p∗                                                             pmeta\n                                                                          5: Output: p∗\n\n\nHard-Case Tracking. The design of this method is in-\nspired by feedback-based methods and the OPRO (Yang    principle to efficiently pre-select candidate prompts. This\net al., 2024) framework, which highlights the importance    ensemble optimizer significantly reduces evaluation costs\nof leveraging large language models as optimizers based    while maintaining coverage and fairness, and it is capable\non solution-score pairs. A critical drawback of existing    of prioritizing the identification of high-potential prompts.\napproaches is their myopic perspective on prompt evalua-   The proposed design effectively alleviates computational\ntion. They typically operate by either analyzing the failure    bottlenecks encountered in large-scale prompt optimization\ncases of an individual prompt or just considering the ter-    tasks and offers a scalable solution for automated prompt\nminal performance metrics. Consequently, these methods     selection in the context of complex tasks.\nlack a global awareness of the optimization dynamics across\n                                                     Bayesian Search. The core idea of Bayesian optimizationthe entire population of candidate prompts. To overcome\n                                                                          is to perform probabilistic modeling of the performancethis, we propose Hard-Case Tracking, a novel technique\n                                                         landscape over candidate prompts using historical evalua-that maintains and utilizes a global view of all prompts’\n                                                                 tion data, enabling sample-efficient selection under limitedbehaviors and error patterns. Additionally, recognizing the\n                                                            evaluation budgets. Specifically, given a set of evaluatedsophisticated inferential power of contemporary LLMs (Ko-\n                                                     prompts {xi, yi}ni=1, where xi denotes the embedding ofjima et al., 2022; Wei et al., 2022), our framework forgoes\n                                                              the i-th prompt and yi is its observed performance, a Gaus-the generation of explicit intermediate summaries. We in-\n                                                              sian Process Regression (GPR) model is fitted to estimatestead empower the model to perform autonomous reasoning\n                                                             the underlying objective function f(x). For any uneval-through an implicit chain of thought, a strategy that pre-\n                                                         uated candidate x, the posterior predictive distribution isserves the full fidelity of information and prevents prema-\n                                                   f(x) ∼N(µ(x), σ2(x)), where µ(x) and σ2(x) representture information loss. Specifically, we employ a bad case\n                                                             the expectation and variance of x, respectively. The acqui-tracker to dynamically monitor inputs associated with the\n                                                                    sition function, Expected Improvement (EI), quantifies thehighest frequency of errors and their corresponding prompts\n                                                        expected gain over the current best observed performancein historical data, regarding these as “hard cases” within the\n                                                    f ∗, and is defined as:task. This hard-case-driven optimization given in Algorithm\n3 fundamentally enables explicit modeling of the optimiza-\n                                                            EI(x) := E [max(f(x) −f ∗−ξ, 0)] ,tion trajectory, effectively integrating the joint utilization of\nhistorical solution pathways and problem text emphasized\n                                                   where ξ is a positive constant for exploration. The closed-in the OPRO methodology, thereby enhancing adaptability\n                                                   form expression is: EI(x) = (µ(x) −f ∗−ξ)Φ(Z) +to challenging cases and improving the generalization of\n                                                    σ(x)ϕ(Z), where Z = (µ(x)−f ∗−ξ)/σ(x), Φ(·) and ϕ(·)prompts. Moreover, this strategy can systematically and\n                                                        denote the cumulative distribution function and the proba-iteratively address frequent points of failure, resulting in\n                                                                       bility density function of the standard normal distribution,superior task performance.\n                                                                    respectively. By computing EI(x) for all candidate prompts\n                                                    and selecting those with the highest EI values for evalua-\n3.3. Efficient Prompt Search\n                                                                       tion, the algorithm efficiently explores the search space and\nDuring the ensemble optimization phase, the evaluation     identifies optimal or near-optimal prompts with fewer eval-\nof a large number of candidate prompts poses significant     uations. Bayesian optimization thus achieves a principled\ntime constraints, conducting assessments for each prompt    balance between exploitation of known well-performing\nwould inevitably result in substantial inefficiency. To ad-   prompts and exploration, resulting in accelerated conver-\ndress this challenge, we introduce an intelligent screening    gence and improved resource efficiency. This process is\nmechanism based on Bayesian optimization and the MAB   shown in Algorithm 4.\n\n                                                5\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nAlgorithm 4 Bayesian Search for Prompt Selection         Algorithm 5 MAB Search for Prompt Selection\n  1: Input: Candidate prompts C, evaluated prompts and       1: Input: Candidate prompts C, the number of clusters K\n     their scores {(xi, yi)}ni=1, the number of optimized       2: Embed C into Euclidean space and perform K-means\n    prompts N                                                      clustering with parameter K, time steps Ts, exploration\n  2: Fit GPR on (xi, yi) to estimate f(x)                        parameter c\n  3: for each candidate x ∈C do                                    3: Initialize each cluster as an arm k ∈{1, . . . , K} with\n  4:    Calculate posterior mean µ(x) and variance σ2(x)              ¯rk = 0 and nk = 0\n  5:   Compute EI value as:  EI(x) = (µ(x) −f ∗−       4: for Nts = 1, . . . , TS do\n     ξ)Φ(Z) + σ(x)ϕ(Z)                                          5:   For each arm k, compute UCB score: UCBk = ¯rk +\n  6: end for                                                      cp(ln Nts)/nk\n  7: Select top-N candidates C∗with the highest EI(x)           6:    Select top arms as a set SK with the highest UCB\n  8: Output: C∗                                                    scores\n                                                                          7:   For each arm k ∈SK, randomly choose a prompt for\n                                                                    evaluation and updating nk and ¯rk\nMAB Search. The MAB also achieves a principled balance       8: end for\nbetween exploration and exploitation from another perspec-      9: Output: Prompts with highest observed rewards\ntive. Candidate prompts are first embedded and clustered\nvia K-means, with each cluster viewed as an individual arm.\nDuring each evaluation round, pulling an arm corresponds    determined through a voting mechanism. Since each prompt\nto evaluating a prompt from the corresponding cluster, and    follows a different generation path and the LLM involves\nthe observed reward (e.g., F1 score) is recorded. To effi-    inherent randomness during generation, prompt bias may\nciently allocate evaluation resources, the Upper Confidence    be amplified. Thus considering different prompts may be\nBound (UCB) criterion is adopted. For the k-th arm, the     better suited for different tasks. We adopt a weighted voting\nUCB score is defined as:                                 mechanism, where voting weights are assigned according to\n                                                               the capability of each prompt. Formally, given M ensemble\n         UCBk := ¯rk + cpln N/nk,                members, the final prediction ˆy(x) for input x is defined as:\n\nwhere ¯rk denotes the average reward for arm k, nk is the           ˆy(x) = arg maxy∈Y PMj=1 wj · I{fj(x) = y},\nnumber of pulls for arm k, c is an exploration parameter\nand N is the total number of pulls across all arms. At each    where wj is the weight of the j-th prompt, fj(x) represents\nstep, the arms with the highest UCB scores are selected,     its prediction, and I is the indicator function. The weight\nand prompts are randomly sampled from these clusters for    vector w is obtained by solving the following optimization\nevaluation.  This strategy adaptively focuses on clusters    problem:\nlikely to yield high-reward prompts while ensuring adequate\nexploration of less-tested regions. This iterative process is     minw  −F1macro(w) + λ∥w∥22      s.t. PMj=1 wj =\nformalized in Algorithm 5. Compared to random or greedy                           1, wj ≥wmin,\nstrategies, the MAB method provides an asymptotically\n                                                   where λ is a regularization coefficient designed to ensureoptimal allocation of evaluation budget, leading to faster\n                                                          balanced weight allocation and reduce the risk of overfitting,convergence and robust performance.\n                                                  wmin > 0 is a pre-given constant as a weight threshold.\n\n3.4. Ensemble Voting                                  During each iteration, the ensemble pool is automatically\n                                                         updated based on the latest evaluation results, with new high-\nIn large-scale prompt optimization tasks, relying on a sin-\n                                                                quality prompts continuously incorporated through cluster-\ngle prompt is often insufficient to achieve robustness and\n                                                           ing and performance-driven selection. Consequently, both\ngeneralization required by diverse and dynamic scenarios.\n                                                     ensemble membership and weight assignments are dynam-\nTo address this limitation, we propose an ensemble vot-\n                                                                   ically adapted, allowing the system to respond effectively\ning strategy that aggregates multiple well-performing yet\n                                                                to shifts in data distribution and task complexity, thereby\nstructurally diverse candidate prompts. The ensemble is\n                                                                 further improving robustness and generalization across het-\nconstructed by selecting top-ranked prompts from the opti-\n                                                       erogeneous test environments. This voting method is sum-\nmization population, with clustering and ranking employed\n                                                      marized in Algorithm 6. Extensive experimental results\nto ensure diversity in linguistic expression and reasoning\n                                                          demonstrate that the ensemble voting approach consistently\nstrategies, effectively mitigating the risk of local optima.\n                                                         outperforms single-prompt baselines, yielding superior sta-\nWithin the ensemble, each member independently produces     bility and fault tolerance. These advantages are particularly\nits prediction for the same input, and the final output is    pronounced in settings characterized by high prompt diver-\n\n                                                6\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nsity and dynamically evolving candidate spaces.                tions, our method also delivers substantial gains. On the\n                                  WSC and GSM8K datasets, ELPO attains an accuracy of\n                                                         95.9 and a score of 96.0, respectively, surpassing GPO’sAlgorithm 6 Ensemble Voting\n                                                          84.0 and Promptbreeder’s 91.7.\n  1: Input: Queries Q, optimization population P, test\n     dataset Dtest = {(qi, ai)}i consists of queries and an-   These results indicate that ELPO not only shows stronger\n    swers, ensemble size M, regularization parameter λ,    optimization ability in complex reasoning tasks (e.g., LIAR,\n   minimum weight wmin                          BBH, GSM8K) but also maintains stable advantages in fine-\n  2: Select M  well-performing  and  diverse  prompts    grained semantic detection tasks (e.g., ETHOS, ArSarcasm,\n   {pj}Mj=1 from P via clustering and ranking           WSC). Compared with feedback-based methods (e.g., Pro-\n  3: Generate predictions fj(qi) for (qi, ai) ∈Dtest and    TeGi) and evolutionary methods (e.g., EvoPrompt, Prompt-\n    j ∈{1, . . . , M}                                         Breeder), ELPO achieves significant improvements in both\n  4: Construct prediction matrix F with Fij = fj(qi)         accuracy and stability.\n  5: Optimize weights w = (w1, . . . , wM) by solving\n       minw  −F1macro(w; F, Dtest) + λ∥w∥22            4.2. Ablation Study\n      s.t. PMj=1 wj = 1,  wj ≥wmin (j = 1, . . . , M)       Effect of Each Component. We take the combination of\n  6: for each query q ∈Q do                             Bad-Case Reflection and the MAB Search as the “Baseline”\n  7:   Aggregate predictions by weighted voting:              configuration, as it represents the simplest form of APO. For\n              ˆa(q) = arg maxa PMj=1 wj I{fj(q) = a}         comparison, we introduce a “Generator” treatment group\n  8: end for                                                    to evaluate the impact of incorporating diverse generators\n  9: Output: A(Q) = {ˆa(q)}q∈Q                          such as Hard-Case Reflection and Evolutionary Reflection.\n                                                            In addition, a “Framework” treatment group is established\n                                                                to assess the effectiveness of the ensemble framework, in-\n                                                            cluding the shared generation strategy and ensemble voting4. Experiments and Results\n                                                                     strategy. This experimental design ensures that we can sys-\nDatasets.  We perform evaluation on the following 6    tematically verify the effectiveness of each key component\ndatasets:  Liar (Wang, 2017), BBH-navigate (Suzgun     in our proposed method. The results are given in Table 2.\net  al., 2023), Ethos (Mollas et  al., 2022), ArSarcasm   We observe that increasing the diversity of generators leads\n(Farha & Magdy, 2020), WSC (Levesque et al., 2012),    to a significant improvement in F1 score on these datasets,\nGSM8K(Cobbe et al., 2021). WSC features multiple-choice    which confirms the effectiveness of the generator expansion\nquestions, GSM8K includes questions that require integer     strategy. Furthermore, benefited from the generator expan-\nanswers, and the others offer true or false questions.           sion strategy, the ensemble framework strategy can further\n                                                      enhance model performance.\nBaselines. Several representative methods are compared,\nincluding existing LLM-based prompt optimizers such as    Effect of Voting Method. As shown in Table 3, we further\nAPE, ProTeGi, OPRO, Promptbreeder, EvoPrompt, and    validate the rationality of the ensemble voting strategy in\nGPO (Tang et al., 2025). Besides, we consider two base-    the ensemble framework. The results show that using an\nlines: one using manually written simple prompts, which are    average strategy to vote on the selected prompts can slightly\nprovided in the appendix, and another using the instruction    improve accuracy, while applying a weighted voting strategy\n“Let’s think step by step.” from chain-of-thought (CoT) as    can further enhance model performance.\nproposed by Kojima et al. (2022) for performance compari-\nson.                                                     5. Conclusion\n\n4.1. Main Results                                            In this paper, we propose a novel framework for APO called\n                                                  Ensemble Learning based Prompt Optimization (ELPO)\nIn Table 1, we present a comparison between ELPO and                                                    which combines multiple generation and search algorithms\nrepresentative prompt optimization methods on true/false                                                                   to derive accurate and robust results. By integrating a variety\nquestions, generative questions and multiple-choice ques-                                                              of improved generators, we fully leverage the generative ca-\ntions. Overall, ELPO consistently outperforms existing ap-                                                                     pabilities and remarkable knowledge of LLMs to construct\nproaches across all datasets. All the F1 score and accuracy                                                        a rich pool of candidate prompts. To conserve resources\nresults are multiplied by 100. For true/false questions, our                                                    and enhance efficiency, we further propose an optimized\napproach shows notable improvement. Specifically, ELPO                                                             search strategy that selects the most promising prompts prior\nachieves an F1 score of 91.1 on the BBH dataset, outper-                                                                to actual sample evaluation. Additionally, we employ an\nforming CoT’s 81.9 by 9.2 points and demonstrating better                                                     ensemble voting approach to improve model performance\ngeneralization. For generative and multiple-choice ques-\n\n                                                 7\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n\n\n\n                           LIAR  BBH  ETHOS  ArSarcasm  WSC  GSM8K\nMethod\n                                        (F1)    (F1)      (F1)         (F1)       (Acc.)    (Acc.)\nEmpty                                 46.4    69.4     93.0        83.7       77.3      89.0\nCoT (Kojima et al., 2022)               46.0    81.9     84.5        83.7       81.3      89.0\nAPE (Zhou et al., 2023)                 51.2    74.3     93.2        84.3       79.3      91.3\nProTeGi (Pryzant et al., 2023)           60.3    73.6     97.0        84.1       80.0      91.0\nOPRO (Yang et al., 2024)               52.1    75.0     94.8        84.7       83.3      90.7\nPromptbreeder (Fernando et al., 2024)    51.8    75.7     95.7        84.5       80.0      91.7\nEvoPrompt (Guo et al., 2024)            52.3    76.4     94.3        83.9       78.8      90.7\nGPO (Tang et al., 2025)                 56.6    75.0     95.5        83.8       84.0      90.3\nELPO                                 72.1    91.1     98.4        92.3       95.9      96.0\n\n              Table 1. Comparison of performance between ELPO and existing methods.\n\n\n\n\n\n                             LIAR  BBH  ETHOS   ArSarcasm  WSC  GSM8K\n  Baseline   Generator   Framework\n                                        (F1)    (F1)     (F1)        (F1)      (Acc.)    (Acc.)\n\n   ✓                                42.5    71.1     97.6        74.4       76.2      76.7\n   ✓     ✓                     65.3    84.0     97.7        86.7       88.9      90.5\n   ✓            ✓        43.2    73.1     97.9        79.2       87.0      80.0\n   ✓     ✓      ✓        72.1    91.1     98.4        92.3       95.9      96.0\n\n\n                          Table 2. Effect of each component in our method.\n\n\n\n\n\n                           LIAR  BBH  ETHOS   ArSarcasm  WSC  GSM8K\n    Baseline   average   weighted\n                                      (F1)    (F1)     (F1)        (F1)      (Acc.)    (Acc.)\n    ✓                            63.3    84.7     95.1        83.3       91.2      93.3\n    ✓     ✓                  66.7    85.8     98.3        85.7       94.7      93.8\n    ✓          ✓       72.1    91.1     98.4        92.3       95.9      96.0\n\n                                 Table 3. Effect of Voting Method.\n\n\n\n\n\n                                      8\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nacross diverse tasks, resulting in greater accuracy and ro-   Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun,\nbustness.                                                       H., andMatthias Plappert, L. K., Tworek,  J., Hilton,\n                                                                                       J., Nakano, R., Hesse, C., and Schulman, J.  Train-\nDespite the promising results, our study has several limita-\n                                                             ing verifiers to solve math word problems. CoRR, pp.\ntions. Firstly, our generation strategy is not plentiful enough,\n                                                            abs/2110.14168, 2021.\nwhich may restrict the pool of candidate prompts. It is an\ninteresting direction to extend this framework to more gen-    Cui, W., Zhang, J., Li, Z., Sun, H., Lopez, D., Das, K.,\neration strategies, such as human intervention methods, to      Malin, B. A., and Kumar, S.  Heuristic-based search\nprovide more promising candidates. Secondly, our search      algorithm in automatic instruction-focused prompt opti-\nstrategy is not sufficiently robust. Though we can save       mization: a survey. In Findings of the Association for\nresources by evaluating only the top-performing prompts,      Computational Linguistics: ACL 2025, pp. 22093–22111,\nthe search strategy does not always guarantee that the best      2025.\nprompts will be selected from the candidates.\n                                                     Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T.,\nRandom seeds were fixed for all runs.                                                         Song, M., Xing, E. P., and Hu, Z. Rlprompt: Optimizing\n                                                                     discrete text prompts with reinforcement learning. In Pro-\nReferences                                               ceedings of the 2022 Conference on Empirical Methods\n                                                                    in Natural Language Processing, pp. 3369–3391, 2022.\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\n  Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,    Farha, I. A. and Magdy, W. From arabic sentiment analysis\n  Anadkat, S., et al. Gpt-4 technical report. arXiv preprint       to sarcasm detection: The arsarcasm dataset. In Proceed-\n  arXiv:2303.08774, 2023.                                     ings of the 4th Workshop on Open-Source Arabic Corpora\n                                                    and Processing Tools, with a Shared Task on Offensive\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,     Language Detection, pp. 32–39, 2020.\n  D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\n   Z., et al.  Palm 2 technical report.  arXiv preprint    Fernando, C., Banarse, D. S., Michalewski, H., Osindero, S.,\n  arXiv:2305.10403, 2023.                                and Rockt¨aschel, T. Promptbreeder: self-referential self-\n                                                        improvement via prompt evolution. In International Con-\nAudibert, J.-Y. and Bubeck, S. Best arm identification in       ference on Machine Learning, volume 235, pp. 13481–\n  multi-armed bandits. In Conference on Learning Theory      13544, 2024.\n  (COLT 2010), pp. 41–53, 2010.\n                                                        Ganaie, M. A., Hu, M., Malik, A. K., Tanveer, M., and\nBreiman, L. Bagging predictors. Machine learning, 24(2):      Suganthan, P. N.  Ensemble deep learning: a review.\n  123–140, 1996.                                          Engineering Applications of Artificial Intelligence, 115:\n                                                         105151, 2022.\nBrochu, E., Cora, V. M., and De Freitas, N. A tutorial\n  on bayesian optimization of expensive cost functions,   Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu,\n  with application to active user modeling and hierarchical       G., Bian, J., and Yang, Y. Connecting large language mod-\n  reinforcement learning. arXiv preprint arXiv:1012.2599,       els with evolutionary algorithms yields powerful prompt\n  2010.                                                        optimizers.  In International Conference on Learning\n                                                               Representations, 2024.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\n                                                          Holland, J. H. Adaptation in natural and artificial systems:  Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\n                                                    an introductory analysis with applications to biology,  Askell, A., et al. Language models are few-shot learners.\n                                                                    control, and artificial intelligence. MIT Press, 1992a.  In Advances in Neural Information Processing Systems,\n  volume 33, pp. 1877–1901, 2020.                                                            Holland, J. H. Genetic algorithms. Scientific American, 267\n                                                               (1):66–73, 1992b.\nChen, X., Lin, M., Sch¨arli, N., and Zhou, D.  Teaching\n   large language models to self-debug. In International    Hu, X., Tang, P., Zuo, S., Wang, Z., Song, B., Lou, Q.,\n  Conference on Learning Representations, 2024.                 Jiao, J., and Charles, D. Evoke: evoking critical thinking\n                                                                         abilities in llms via reviewer-author prompt editing. In\nCheng, J., Liu, X., Zheng, K., Ke, P., Wang, H., Dong, Y.,                                                                International Conference on Learning Representations,\n  Tang, J., and Huang, M. Black-box prompt optimization:                                                           2024.\n  Aligning large language models without model training.\n  In Proceedings of the 62nd Annual Meeting of the Asso-    Jiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A.,\n   ciation for Computational Linguistics (Volume 1: Long       Terry, M., and Cai, C. J. Promptmaker: Prompt-based\n  Papers), pp. 3201–3219, 2024.                              prototyping with large language models.  In Extended\n\n                                                9\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n  Abstracts of the 2022 CHI Conference on Human Factors    Prasad, A., Hase, P., Zhou, X., and Bansal, M.  Grips:\n   in Computing Systems, pp. 1–8, 2022.                           gradient-free, edit-based instruction search for prompting\n                                                                  large language models. In Proceedings of the 17th Con-\nJiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we       ference of the European Chapter of the Association for\n  know what language models know? Transactions of the      Computational Linguistics, pp. 3845–3864, 2023.\n  Association for Computational Linguistics, 8:423–438,\n  2020.                                                      Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and Zeng, M.\n                                                       Automatic prompt optimization with “gradient descent”\nJones, D. R., Schonlau, M., and Welch, W. J. Efficient global      and beam search. In Proceedings of the 2023 Conference\n  optimization of expensive black-box functions. Journal      on Empirical Methods in Natural Language Processing,\n   of Global Optimization, 13(4):455–492, 1998.                pp. 7957–7968, 2023.\n\nJuneja, G., Jajoo, G., Natarajan, N., Li, H., Jiao, J., and                                                        Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\n  Sharma, A. Task facet learning: a structured approach to                                                                       et al. Improving language understanding by generative\n  prompt optimization. In Findings of the Association for                                                                     pre-training. 2018.\n  Computational Linguistics: ACL 2025, pp. 23473–23496,\n  2025.                                                 Radford, A., Wu,  J., Child, R., Luan, D., Amodei, D.,\n                                                               Sutskever, I., et al. Language models are unsupervised\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,                                                                multitask learners. OpenAI blog, 1(8):9, 2019.\n  Y. Large language models are zero-shot reasoners. In\n  Advances in Neural Information Processing Systems, vol-                                                        Reynolds, L. and McDonell, K. Prompt programming for\n  ume 35, pp. 22199–22213, 2022.                                                                 large language models: beyond the few-shot paradigm.\n                                                              In Extended Abstracts of the 2021 CHI Conference on\nLattimore, T. and Szepesv´ari, C. Bandit algorithms. Cam-\n                                             Human Factors in Computing Systems, pp. 1–7, 2021.\n  bridge University Press, 2020.\n\n                                                      Snoek,  J., Larochelle, H., and Adams, R. P.  Practical\nLevesque, H. J., Davis, E., and Morgenstern, L. The wino-\n                                                           bayesian optimization of machine learning algorithms.\n  grad schema challenge. In Proceedings of the Thirteenth\n                                                              In Advances in Neural Information Processing Systems,\n  International Conference on Principles of Knowledge\n                                                     volume 25, pp. 2951–2959, 2012.\n  Representation and Reasoning, pp. 552–561, 2012.\n\n                                                               Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A., Abid,Li, X. L. and Liang, P. Prefix-tuning: optimizing continu-\n                                                               A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,  ous prompts for generation. In Proceedings of the 59th\n                                                           Garriga-Alonso, A., et al. Beyond the imitation game:  Annual Meeting of the Association for Computational Lin-\n                                                                quantifying and extrapolating the capabilities of language   guistics and the 11th International Joint Conference on\n                                                         models. Transactions on Machine Learning Research,  Natural Language Processing (Volume 1: Long Papers),\n                                                          2023.  pp. 4582–4597, 2021.\n\n                                                       Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Black-Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig,\n                                                    box tuning for language-model-as-a-service.  In Inter-  G. Pre-train, prompt, and predict: a systematic survey of\n                                                              national Conference on Machine Learning, pp. 20841–  prompting methods in natural language processing. ACM\n                                                         20855, 2022.  Computing Surveys, 55(9):1–35, 2023.\n\nLu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp,    Suzgun, M., Scales, N., Sch¨arli, N., Gehrmann, S., Tay, Y.,\n   P. Fantastically ordered prompts and where to find them:      Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou,\n  overcoming few-shot prompt order sensitivity. In Pro-       D., et al. Challenging big-bench tasks and whether chain-\n  ceedings of the 60th Annual Meeting of the Association       of-thought can solve them. In Findings of the Association\n   for Computational Linguistics (Volume 1: Long Papers),       for Computational Linguistics: ACL 2023, pp. 13003–\n  pp. 8086–8098, 2022.                                    13051, 2023.\n\nMitchell, M. An introduction to genetic algorithms. MIT    Tang, X., Wang, X., Zhao, W. X., Lu, S., Li, Y., and Wen,\n   Press, 1998.                                                      J.-R. Unleashing the potential of large language models\n                                                                as prompt optimizers: Analogical analysis with gradient-\nMollas, I., Chrysopoulou, Z., Karlos, S., and Tsoumakas,      based model optimizers. In Proceedings of the AAAI Con-\n  G. Ethos: a multi-label hate speech detection dataset.      ference on Artificial Intelligence, volume 39, pp. 25264–\n  Complex & Intelligent Systems, 8(6):4663–4678, 2022.       25272, 2025.\n\n                                                10\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,    Ye, Q., Ahmed, M., Pryzant, R., and Khani, F. Prompt engi-\n  M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,      neering a prompt engineer. In Findings of the Association\n  Azhar, F., et al. Llama: open and efficient foundation lan-       for Computational Linguistics: ACL 2024, pp. 355–385,\n  guage models. arXiv preprint arXiv:2302.13971, 2023a.      2024.\n\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,    Zhang, T., Wang, X., Zhou, D., Schuurmans, D., and Gon-\n  A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,       zalez, J. E. Tempera: Test-time prompt editing via re-\n  Bhosale, S., et al. Llama 2: open foundation and fine-      inforcement learning.  In International Conference on\n  tuned chat models.  arXiv preprint arXiv:2307.09288,      Learning Representations, 2023.\n  2023b.\n                                                      Zhao, C., Liu, Z., Wang, X., Lu, J., and Ruan, C. Pmpo:\nWang, W. Y.  ” liar, liar pants on fire”: A new bench-       Probabilistic metric prompt optimization for small and\n  mark dataset for fake news detection.  arXiv preprint       large language models. arXiv preprint arXiv:2505.16307,\n  arXiv:1705.00648, 2017.                                  2025.\n\nWang, X., Li, C., Wang, Z., Bai, F., Luo, H., Zhang, J., Jojic,    Zhao, Y., Zheng, W., Cai, T., Xuan Long, D., Kawaguchi, K.,\n  N., Xing, E. P., and Hu, Z. Promptagent: strategic plan-      Goyal, A., and Shieh, M. Q. Accelerating greedy coordi-\n  ning with language models enables expert-level prompt       nate gradient and general prompt optimization via probe\n  optimization. In International Conference on Learning       sampling. In Advances in Neural Information Processing\n  Representations, 2024.                                      Systems, volume 37, pp. 53710–53731, 2024.\n\nWebson, A. and Pavlick, E. Do prompt-based models really    Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\n  understand the meaning of their prompts? In Proceedings       Calibrate before use: improving few-shot performance\n   of the 2022 Conference of the North American Chapter of       of language models.  In International Conference on\n  the Association for Computational Linguistics: Human      Machine Learning, pp. 12697–12706, 2021.\n  Language Technologies, pp. 2300–2344, 2022.\n                                                     Zhou, A., Li, B., and Wang, H. Robust prompt optimiza-\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,       tion for defending language models against jailbreaking\n   E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting       attacks. In Advances in Neural Information Processing\n   elicits reasoning in large language models. In Advances       Systems, volume 37, pp. 40184–40211, 2024.\n   in Neural Information Processing Systems, volume 35,\n  pp. 24824–24837, 2022.                              Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S.,\n                                                       Chan, H., and Ba, J. Large language models are human-\nWolpert, D. H. and Macready, W. G. No free lunch theo-       level prompt engineers. In International Conference on\n  rems for optimization. IEEE transactions on evolutionary      Learning Representations, 2023.\n  computation, 1(1):67–82, 2002.\n                                                     Zhou, Z.-H.  Ensemble methods: foundations and algo-\nXiang, J., Zhang, J., Yu, Z., Teng, F., Tu, J., Liang, X.,       rithms. CRC Press, 2012.\n  Hong, S., Wu, C., and Luo, Y. Self-supervised prompt\n  optimization. arXiv preprint arXiv:2502.06855, 2025.      Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z.,\n                                                      and Fredrikson, M. Universal and transferable adversar-\nXu, H., Chen, Y., Du, Y., Shao, N., Wang, Y., Li, H., and        ial attacks on aligned language models. arXiv preprint\n  Yang, Z. Gps: genetic prompt search for efficient few-      arXiv:2307.15043, 2023.\n  shot learning. In Proceedings of the 2022 Conference on\n  Empirical Methods in Natural Language Processing, pp.\n  8162–8171, 2022.\n\nYan, C., Wang, J., Zhang, L., Zhao, R., Wu, X., Xiong, K.,\n  Liu, Q., Kang, G., and Kang, Y. Efficient and accurate\n  prompt optimization: the benefit of memory in exemplar-\n  guided reflection.  In Proceedings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics\n  (Volume 1: Long Papers), pp. 753–779, 2025.\n\nYang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D.,\n  and Chen, X. Large language models as optimizers. In\n  International Conference on Learning Representations,\n  2024.\n\n                                                11\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nA. Additional Details for the Setup\n\nA.1. Tasks and Data Details\n\nWe present a summary of the dataset sizes, data split information, sources, and licensing details in Table 4. To the best of\nour knowledge, our usage of these datasets aligns with their intended purposes, and the data we utilize do not contain any\npersonal or sensitive information.\n\n                     Dataset Name   Task                Train & Dev   Test   License\n               LIAR           True/False         3681        461   Unknown\n                  BBH-Navigate   True/False         153         97     Apache-2.0\n              ETHOS         True/False         300         217  GNU GPLv3\n                  ArSarcasm      True/False         8437        2110  MIT\n             GSM8K         Integer Generation  7473        1319  MIT\n            WSC           Multiple-Choice    162         123   CC BY 4.0\n\n                                                    Table 4. Dataset tails.\n\n\nThe LIAR dataset (Wang, 2017) consists of 12,791 English statements for fake news detection, each provided with contextual\ninformation and truthfulness labels. For our experiments, we split the dataset randomly, then getting 3,681 samples for\ntraining and 461 samples for testing.\n\nThe BIG-bench Hard dataset (Suzgun et al., 2023) is a challenging subset of the BIG Bench corpus (Srivastava et al., 2023),\nfeaturing 23 tasks that pose significant difficulties for current language models. In our study, we focus on the navigation\ntask, in which the goal is to determine whether an agent, after executing a series of navigation steps, returns to its starting\nposition. We allocate 153 instances for training and 97 instances for testing.\n\nETHOS (Mollas et al., 2022) is a hate speech detection dataset in English, comprising 998 online comments, each annotated\nwith hate speech labels. We split the dataset randomly then assigning 300 instances to the training set and 217 instances to\nthe testing set.\n\nThe ArSarcasm dataset (Farha & Magdy, 2020) is an Arabic sarcasm detection corpus made up of 10,547 online comments,\nall labeled for sarcasm. We utilize the original data division, with 8,437 samples for training and 2,110 samples for\nevaluation.\n\nThe GSM8K (Cobbe et al., 2021) dataset consists of 8,792 high-quality, linguistically diverse grade school math word\nproblems, all created by human authors. Following the dataset division used in GPO (Tang et al., 2024), we employ 7473\nsamples for training and 1319 for testing.\n\nThe WSC(Levesque et al., 2012) dataset was proposed as an alternative to the Turing Test, as well as a benchmark for\nevaluating a system’s commonsense reasoning capabilities. In line with the methodology adopted by GPO (Tang et al.,\n2024), we select 162 samples for training and 123 for testing.\n\n\nA.2. Implementation Details\n\nWe select Doubao-pro as the task model and set its temperature to 0 to ensure deterministic outputs. For the prompt optimizer,\nwe utilize the model of GPT-4o to get high quality of prompt generation. the prompts for different tasks can be founded later\nin the paper. At each step, we generate 10, 5 and 1 prompts using the Bad-Case Reflection, Evolutionary Reflection, and\nHard-Case Tracking methods respectively and then aggregate them into the shared candidate prompts. The best-performing\nprompts among them are selected as the parent prompts for the next iteration. All experiments are conducted three times,\nand we report the average results.\n\nB. ELPO Process\n\nB.1. Prompt Generation\n\nIn each epoch, we will generate prompts based on the initial prompt and the excellent prompt in previous iteration throuth\nthree methods. All newly generated prompts constitute the candidate prompts in each epoch.\n\n                                                12\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nOriginal Prompt\n\n\n## Task\nSolve the math problem. Please output only the answer to the math problem without\nany additional response.\n# Few-Shot Exemplars\nInput: Ryan is considering buying a new multivitamin brand. Each pill has 50 mg of\nVitamin A in it. The recommended daily serving of Vitamin A is 200 mg. How many\npills does Ryan need to hit the recommended amount for the week?\nExpected: 28\n## Prediction\nInput: {input}\nExpected:\n\n\n\nBad-Case Reflection\n\n\nPrompt1\nYou are a specialist in navigation and spatial reasoning, responsible for analyzing\nwhether a set of movement instructions returns to the original position (coordinate\n(0, 0) on a two-dimensional Cartesian plane).\n\n### **Evaluation Guidelines**:\n\n1. **Starting Point**:\n  - Begin at the origin (0, 0).\n  - Assume an initial direction of north unless explicitly specified otherwise.\n\n2. **Sequential Processing**:\n  - Execute each instruction in order.\n  - Update orientation immediately upon commands that involve changes in direction (\ne.g., \"turn left,\" \"turn right,\" \"turn around\").\n  - For movement commands (e.g., \"forward,\" \"backward,\" \"left,\" \"right\"), revise x-\nand y-coordinates based on the current orientation, ensuring all movement offsets\nare cumulative.\n\n3. **Tracking Progress**:\n  - Log the updated position and orientation after each instruction for clarity.\n  - Avoid skipping or combining intermediate steps to ensure thoroughness.\n\n4. **Final Analysis**:\n  - Compare the concluding position with the starting position (0, 0). The direction\n at the end does not matter for this comparison.\n  - If the final position matches the starting point, output: ‘[{\"label\":\"YES\"}]‘.\n  - If the final position is different, output: ‘[{\"label\":\"NO\"}]‘.\n\n### **Error Prevention Measures**:\n- Validate directional changes (e.g., turns) for accuracy.\n- Track all movements along the x- and y-axes systematically.\n- Base decisions strictly on the given instructions; avoid assuming unstated\npositions or directions.\n- Use intermediate results for each step to detect and address discrepancies before\ngenerating the final output.\n\n---\n\n**Example Walkthroughs**:\n\n- **Example 1**:\n  - **Input**: \"Move 3 steps forward. Turn around. Move 3 steps forward. Turn right\n.\"\n    1. Start at (0, 0), facing north. Move forward 3 steps: (0, 3).\n\n\n\n                                             13\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n    2. Turn around to face south. Move forward 3 steps: (0, 0).\n    3. The final position equals the starting point (0, 0).\n    - **Output**: ‘[{\"label\":\"YES\"}]‘.\n\n- **Example 2**:\n  - **Input**: \"Face forward throughout. Move 7 steps backward. Move 4 steps left.\nMove 7 steps left. Move 7 steps right. Move 8 steps forward.\"\n    1. Start at (0, 0), facing north. Move backward 7 steps: (0, -7).\n    2. Move left 4 steps: (-4, -7). Move left another 7 steps: (-11, -7).\n    3. Move right 7 steps: (-4, -7). Move forward 8 steps: (-4, 1).\n    4. The final position does not equal the starting point (0, 0).\n    - **Output**: ‘[{\"label\":\"NO\"}]‘.\n\nFollow these instructions closely to ensure consistent and accurate navigation\nanalyses.\n\nPrompt2\nYou are an expert in navigation and spatial reasoning, tasked with determining\nwhether a sequence of movement instructions leads back to the starting position (\ncoordinate (0, 0) on a two-dimensional Cartesian plane)...\n\nPrompt3\n You are an advanced navigation reasoning system designed to accurately evaluate\nmovement instructions and determine if they lead back to the starting point. Your\nrole is to simulate these movements step by step in a precise 2D coordinate system,\nensuring accurate position and orientation tracking...\nMore candidates ...\n\n\n\nEvolutionary Reflection\n\n\nPrompt1\n# Task Description:\nYour role is to serve as an accurate navigation analyzer responsible for evaluating\nwhether a series of movement commands will lead back to the starting position. Use\nlogical reasoning and spatial tracking to systematically assess the movement\nsequence and determine if the endpoint aligns with the origin.\n\n# Instructions for Analysis:\n1. Interpret all types of movements and directional changes explicitly. Movements\ninclude actions such as \"forward,\" \"backward,\" and alterations in direction like \"\nturn left,\" \"turn right,\" and \"turn around.\"\n2. Simulate the entire sequence step by step with precision, ensuring meticulous\ntracking of:\n  - **Position**: Update grid coordinates accordingly (e.g., +1 for movement north,\n-1 for south).\n  - **Orientation**: Monitor the current facing direction (north, south, east, west)\n and adjust as per \"turn\" commands.\n3. Determine whether the endpoint is identical to the starting point (coordinates (0,\n 0)):\n  - If the coordinates match, return ‘{\"label\":\"YES\"}‘ in JSON format.\n  - If the coordinates do not match, return ‘{\"label\":\"NO\"}‘ in JSON format.\n\n# Rules:\n- Avoid making unwarranted assumptions for unspecified data; adhere to logical\ndefault interpretations when ambiguous (e.g., \"always face forward\" implies an\ninitial orientation of north unless stated otherwise).\n- Follow a systematic, step-by-step approach to maintain accuracy.\n- The output must strictly conform to the JSON format: ‘[{\"label\":\"YES\"}]‘ or ‘[{\"\nlabel\":\"NO\"}]‘.\n\n# Example Walkthrough:\n\n\n\n                                             14\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nInput: \"Always face forward. Take 4 steps forward. Turn right. Take 2 steps forward.\n Turn around. Take 6 steps backward.\"\n- Step-by-Step Processing:\n 1. Begin at (0, 0) facing north.\n 2. Move 4 steps forward then (0, 4).\n 3. Turn right then now facing east.\n 4. Move 2 steps forward then (2, 4).\n 5. Turn around then now facing west.\n 6. Move 6 steps backward then (-4, 4).\n 7. Final position is (-4, 4), which does not match the starting point (0, 0).\n\nOutput: ‘[{\"label\":\"NO\"}]‘\n\nProceed to analyze the provided input and generate the output in the required format.\n\nPrompt2\nYou are an expert in navigation and spatial reasoning, tasked with determining\nwhether a sequence of movement instructions leads back to the starting position (\ncoordinate (0, 0) on a two-dimensional Cartesian plane)...\nPrompt3\nYou are a specialist in navigation and spatial reasoning, responsible for analyzing\nwhether a set of movement instructions returns to the original position (coordinate\n(0, 0) on a two-dimensional Cartesian plane)...\nMore candidates...\n\n\n\nHard-Case Tracking\n\n\nPrompt1\nYou are a highly specialized navigation reasoning system tasked with determining if\na set of movement instructions leads back to the starting point, (0, 0), on a 2D\nCartesian grid. Follow the instructions step by step, ensuring precise position and\norientation updates.\n\n### Key Operational Steps:\n1. **Initialization**:\n  - Start at ‘(0, 0)‘ on the Cartesian plane.\n  - Default orientation is **North** unless explicitly stated otherwise.\n\n2. **Step-by-Step Execution**:\n  - Parse and execute all instructions sequentially. Handle each movement and\norientation update independently.\n  - Movements must increment or decrement ‘[X, Y]‘ according to the current\norientation:\n    - Facing **North**: ‘+Y‘ for forward, ‘-Y‘ for backward.\n    - Facing **East**: ‘+X‘ for forward, ‘-X‘ for backward.\n    - Facing **South**: ‘-Y‘ for forward, ‘+Y‘ for backward.\n    - Facing **West**: ‘-X‘ for forward, ‘+X‘ for backward.\n  - Update orientation for turn commands:\n    - **Turn Right**: 90 degrees clockwise.\n    - **Turn Left**: 90 degrees counterclockwise.\n    - **Turn Around**: Reverse orientation 180 degrees .\n\n3. **Specific Constraints**:\n  - If instructions include ’Always face forward’, maintain constant orientation **\nNorth** throughout.\n  - Do not make assumptions about implied details; default to logical consistency.\n\n4. **Final Validation**:\n  - After processing all instructions, verify if the final position ‘[X, Y]‘ equals\n‘[0, 0]‘.\n  - If true, output ‘[{\"label\":\"YES\"}]‘; otherwise, output ‘[{\"label\":\"NO\"}]‘.\n\n\n\n                                             15\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n\n   ### Output Requirements:\n   - Return your response in strict JSON format:\n     - ‘[{\"label\":\"YES\"}]‘ for paths leading back to the starting point.\n     - ‘[{\"label\":\"NO\"}]‘ for paths that do not.\n\n   Example Input Processing:\n   **Input**: \"Take 3 steps forward. Turn around. Take 3 steps forward.\"\n   1. Start at ‘(0, 0)‘, facing North.\n   2. Move forward 3 steps then ‘(0, 3)‘.\n   3. Turn around to face South.\n   4. Move forward 3 steps then ‘(0, 0)‘.\n   **Output**: ‘[{\"label\":\"YES\"}]‘.\n\n   Strictly adhere to this approach to ensure logical accuracy and format compliance.\n   More candidates ...\n\n\n\nB.2. Prompt Search\n\nSince there are many prompts in the candidates, it will consume a lot of resources to evaluate each prompt. We use Bayesian\nSearch and MAB Search Method to select potential prompt.\n\n   Prompt Candidates\n\n\n   Prompt1\n   You are a specialist in navigation and spatial reasoning, responsible for analyzing\n   whether a set of movement instructions returns to the original position (coordinate\n   (0, 0) on a two-dimensional Cartesian plane).\n\n   ### **Evaluation Guidelines**:\n\n   1. **Starting Point**:\n     - Begin at the origin (0, 0).\n     - Assume an initial direction of north unless explicitly specified otherwise.\n\n   2. **Sequential Processing**:\n     - Execute each instruction in order.\n     - Update orientation immediately upon commands that involve changes in direction (\n   e.g., \"turn left,\" \"turn right,\" \"turn around\").\n     - For movement commands (e.g., \"forward,\" \"backward,\" \"left,\" \"right\"), revise x-\n   and y-coordinates based on the current orientation, ensuring all movement offsets\n   are cumulative...\n   Prompt2\n   You are an expert in navigation and spatial reasoning, tasked with determining\n   whether a series of movement instructions leads back to the starting position (\n   coordinate (0, 0) on a 2D Cartesian plane).\n\n   ### **Guidelines for Evaluation**:\n\n   1. **Initialization**:\n     - Begin at the origin point (0, 0).\n     - Assume an initial facing direction of north unless specified otherwise.\n\n   2. **Step-by-Step Processing**:\n     - Execute each instruction sequentially in the given order.\n     - Adjust orientation immediately upon encountering direction-changing commands (e.\n   g., \"turn left,\" \"turn right,\" \"turn around\").\n     - For movement commands (e.g., \"forward,\" \"backward,\" \"left,\" \"right\"), update the\n    x- and y-coordinates based on the current orientation, ensuring all displacements\n   are cumulative...\n\n\n\n                                                16\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nMore candidates...\n\n\n\nBayesian Search\n\n\nPrompt1\nYou are a specialized model focused on navigation and spatial reasoning tasks. Your\nobjective is to analyze sequences of movement and orientation instructions to\ndetermine whether the endpoint aligns with the starting position, ensuring precision\n and consistency. Follow these principles:\n\n1. **Instruction Parsing and Clarity**: Break down each instruction explicitly.\nSeparate movements (e.g., steps forward, backward) from orientation changes (e.g.,\nturn left, turn right). Resolve vague terms logically (e.g., default \"forward\" to\ncurrent facing direction or \"right/left\" to standard cardinal directions if\nunspecified).\n\n2. **Orientation and Movement Accuracy**: Maintain precise tracking of orientation (\nnorth, east, south, west) throughout the sequence:\n  - Apply directional changes before updating grid coordinates.\n  - For relative terms (e.g., \"right,\" \"left\"), compute orientation dynamically\nbased on the existing facing direction.\n\n3. **Step-by-Step Grid Simulation**: Treat the ‘(0, 0)‘ starting point as a\nCartesian grid origin. At every step:\n  - Update orientation and grid position incrementally based on the instruction.\n  - Validate intermediate positions and orientation shifts systematically to prevent\n accumulation of errors.\n\n4. **Consistency in Ambiguity Handling**: Standardize rule-based interpretations for\n unclear phrasing (e.g., assume \"always forward\" unless explicitly reoriented).\nReassess ambiguous instructions to ensure consistent logic across all steps.\n\n5. **Accurate Final Validation**: Compare the final coordinates ‘(x, y)‘ to the\norigin ‘(0, 0)‘:\n  - If they match, return ‘[{\"label\":\"YES\"}]‘.\n  - If they differ, return ‘[{\"label\":\"NO\"}]‘.\n\n6. **Output Specification**: Deliver results strictly in the format ‘[{\"label\":\"YES\n\"}]‘ or ‘[{\"label\":\"NO\"}]‘.\n\nApproach every problem methodically:\n- Parse, simulate, and validate every step of the sequence systematically.\n- Regularly reassess movements, orientation, and grid positions to identify and\ncorrect potential errors early.\n- Ensure default assumptions and interpretations align logically with task\nrequirements for coherent outcomes.\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: If you follow these instructions, do you return to the starting point? Always\n face forward. Take 5 steps right. Take 4 steps backward. Take 8 steps left. Take 5\nsteps left. Take 6 steps backward. Take 9 steps forward. Take 5 steps right. Take 1\nstep forward. Take 3 steps right.\nOptions:\n- Yes\n- No\nExpected: YES\n\n[Example 2]\nInput: If you follow these instructions, do you return to the starting point? Take 3\n steps. Turn around. Take 3 steps. Turn right.\n\n\n\n                                             17\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nOptions:\n- Yes\n- No\nExpected: YES\nPrompt2\nYou are an expert navigation analyzer specializing in spatial reasoning. Your task\nis to determine whether a sequence of movement instructions results in returning to\nthe starting point. To ensure accuracy, follow these structured guidelines...\nPrompt3\nYou are an advanced spatial navigation and path-tracking system designed to evaluate\n movement instructions step by step and determine whether the path returns to the\nstarting point. Your focus is on rigorous interpretation of instructions, precise\nhandling of direction, magnitude, positional updates, orientation rules, and\nconstraints like \"always face forward.\" Adhere to the following core principles...\n\n\n\nMAB Search\n\n\nPrompt1\n# Task Description:\nYour role is to serve as an accurate navigation analyzer responsible for evaluating\nwhether a series of movement commands will lead back to the starting position. Use\nlogical reasoning and spatial tracking to systematically assess the movement\nsequence and determine if the endpoint aligns with the origin.\n\n# Instructions for Analysis:\n1. Interpret all types of movements and directional changes explicitly. Movements\ninclude actions such as \"forward,\" \"backward,\" and alterations in direction like \"\nturn left,\" \"turn right,\" and \"turn around.\"\n2. Simulate the entire sequence step by step with precision, ensuring meticulous\ntracking of:\n  - **Position**: Update grid coordinates accordingly (e.g., +1 for movement north,\n-1 for south).\n  - **Orientation**: Monitor the current facing direction (north, south, east, west)\n and adjust as per \"turn\" commands.\n3. Determine whether the endpoint is identical to the starting point (coordinates (0,\n 0)):\n  - If the coordinates match, return ‘{\"label\":\"YES\"}‘ in JSON format.\n  - If the coordinates do not match, return ‘{\"label\":\"NO\"}‘ in JSON format.\n\n# Rules:\n- Avoid making unwarranted assumptions for unspecified data; adhere to logical\ndefault interpretations when ambiguous (e.g., \"always face forward\" implies an\ninitial orientation of north unless stated otherwise).\n- Follow a systematic, step-by-step approach to maintain accuracy.\n- The output must strictly conform to the JSON format: ‘[{\"label\":\"YES\"}]‘ or ‘[{\"\nlabel\":\"NO\"}]‘.\n\n# Example Walkthrough:\nInput: \"Always face forward. Take 4 steps forward. Turn right. Take 2 steps forward.\n Turn around. Take 6 steps backward.\"\n- Step-by-Step Processing:\n 1. Begin at (0, 0) facing north.\n 2. Move 4 steps forward to (0, 4).\n 3. Turn right to now facing east.\n 4. Move 2 steps forward to (2, 4).\n 5. Turn around to now facing west.\n 6. Move 6 steps backward to (-4, 4).\n 7. Final position is (-4, 4), which does not match the starting point (0, 0).\n\nOutput: ‘[{\"label\":\"NO\"}]‘\n\n\n\n\n                                             18\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n   Proceed to analyze the provided input and generate the output in the required format.\n\n   Prompt2\n   You are a specialist in navigation and spatial reasoning, responsible for analyzing\n   whether a set of movement instructions returns to the original position (coordinate\n   (0, 0) on a two-dimensional Cartesian plane)...\n   Prompt3\n   You are a sophisticated spatial navigation and path-tracking system specialized in\n   accurately analyzing sequences of movement instructions. Your main responsibility is\n    to determine whether a given set of navigation commands results in a return to the\n   starting point. Carefully process each instruction step by step, ensuring precise\n   updates to both positional coordinates ‘[X, Y]‘ and orientation...\n\n\n\n\n\nTo assess the effectiveness of our search strategy, we evaluated the performance of prompt words from all candidates on the\nreal dataset. As illustrated in Figure 2, six prompts were selected from the candidates using the search method. The average\nF1 score across all candidates is 80.2.Importantly, five out of the six chosen prompts achieved scores above this average,\nand four ranked among the top five overall. These results indicate that our search strategy reliably identifies high-quality\nprompts while conserving resources.\n\n\n\n\n\n                                                Figure 2. Efficiency of search.\n\n\n\n\n\n                                                19\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nB.3. Ensemble Voting\n\n   Ensemble Voting\n\n\n   Prompt1\n   You are an expert navigation and spatial reasoning model designed to assess whether\n   a sequence of movement instructions results in a return to the starting point (\n   coordinate (0, 0) on a 2D Cartesian grid).\n\n   **Guidelines for Accurate Evaluation**:\n\n   1. **Initialization**:\n     - Always begin at position (0, 0) on the Cartesian grid, facing **north**, unless\n   explicitly stated otherwise.\n\n   2. **Instruction Parsing**:\n     - Carefully read and interpret each instruction in order. Identify special\n   conditions like \"always face forward,\" which override direction changes.\n\n   3. **Orientation Updates**:\n     - For commands like \"turn left,\" \"turn right,\" or \"turn around,\" update the facing\n    direction immediately before processing movement instructions.\n     - Ignore orientation changes if a locked orientation such as \"always face forward\"\n    is indicated.\n\n   4. **Movement Calculations**:\n     - For each movement (\"forward,\" \"backward,\" \"left,\" \"right\"), calculate the change\n    in x- and y-coordinates based on the current orientation. Align movements strictly\n   with locked orientations when applicable.\n\n   5. **Intermediate State Tracking**:\n     - After each instruction, log the updated coordinates and facing direction. Use\n   these intermediate records to cross-check for consistency and prevent cumulative\n   errors.\n\n   6. **Final Validation**:\n     - Compare the final grid coordinates to the starting point (0, 0). Output ‘[{\"\n   label\":\"YES\"}]‘ only if the final position matches (0, 0); otherwise, output ‘[{\"\n   label\":\"NO\"}]‘.\n\n   7. **Error Prevention Checks**:\n     - Ensure consistent updates to x- and y-coordinates and facing directions at every\n    step. Validate each intermediate state before proceeding to the next instruction.\n     - Pay close attention to overridden conditions like \"always face forward\" to\n   prevent misinterpretation of implied directions.\n\n   **Example Processes**:\n\n   - Input: \"Always face forward. Take 2 steps left. Take 4 steps backward. Take 10\n   steps right.\"\n     1. Start at (0, 0), facing north, with locked orientation forward (north).\n     2. Move 2 steps left to (-2, 0).\n     3. Move 4 steps backward to (-2, -4).\n     4. Move 10 steps right to (8, -4). Final position: (8, -4).\n     Output: ‘[{\"label\":\"NO\"}]‘.\n\n   - Input: \"Take 3 steps. Turn around. Take 3 steps.\"\n     1. Start at (0, 0), facing north. Move 3 steps forward to (0, 3).\n     2. Turn around to face south. Move 3 steps forward to (0, 0).\n     3. Final position matches (0, 0).\n     Output: ‘[{\"label\":\"YES\"}]‘.\n\n   By prioritizing accurate parsing, intermediate validation, and systematic processing\n\n\n\n                                                20\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n    of movements and orientation changes, ensure consistent evaluations for all\n   navigation tasks.\n\n   # Few-Shot Exemplars (from high-scoring failures)\n   [Example 1]\n   Input: If you follow these instructions, do you return to the starting point? Always\n    face forward. Take 2 steps left. Take 4 steps backward. Take 10 steps right. Take 2\n    steps left. Take 3 steps left. Take 7 steps right.\n   Options:\n   - Yes\n   - No\n   Expected: NO\n\n   [Example 2]\n   Input: If you follow these instructions, do you return to the starting point? Always\n    face forward. Take 9 steps right. Take 6 steps right. Take 10 steps backward. Take\n   9 steps left. Take 4 steps left.\n   Options:\n   - Yes\n   - No\n   Expected: NO\n\n   Prompt2\n   You are an expert in navigation and spatial reasoning, tasked with determining\n   whether a series of movement instructions leads back to the starting position (\n   coordinate (0, 0) on a 2D Cartesian plane)...\n\n   Prompt3\n   You are a sophisticated navigation and spatial reasoning system designed to\n   determine whether a sequence of movement instructions returns to the starting\n   position, (0, 0), on a 2D Cartesian grid. Your task is to ensure precise\n   calculations and logical consistency by strictly following the given instructions...\n\n\n\nC. Additional Result\n\nHere, we present the initial prompt and the ELPO-optimized prompt across different tasks.\n\n    Initial prompt of the LIAR dataset\n\n\n   ## Task\n   Determine whether the Statement is a lie (Yes) or not (No) based on the Context and\n   other\n   information.\n   ## Output format\n   Answer Yes or No as labels.\n   ## Prediction\n   Text: {input}\n   Label:\n\n\n\n  ELPO optimized prompt of the LIAR dataset\n\n\n   Prompt1:\n   You are an expert in mathematics, logic, and navigational reasoning tasked with\n   rigorously determining whether a given Statement is true or false based solely on\n   the provided Context and any explicitly relevant, verifiable data. Follow this exact,\n    methodical process: carefully parse every word, focusing on precise interpretation\n   of negations, qualifiers, conditional phrases, and comparative or superlative\n   modifiers (e.g., \"not,\" \"only,\" \"less than,\" \"more than,\" \"ever,\" \"always\");\n\n\n\n                                                21\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\naccurately interpret all quantitative information including numbers, percentages,\nunits, ratios, sequences, temporal references, and ensure consistency with units and\n scales; thoroughly decompose complex or compound statements into smaller components\n and verify each part individually; explicitly distinguish between facts directly\nsupported by the Context or widely accepted general knowledge and assumptions,\nopinions, or implicit claims, avoiding any unsupported inferences; apply step-by-\nstep logical analysis, visualization, or spatial reasoning as appropriate to\nvalidate relational, temporal, and logical claims; when information is ambiguous,\nconflicting, or incomplete, prioritize the most direct, explicit, and reliable\nevidence within the Context; double-check all numerical calculations, logical\ndeductions, and spatial assessments before reaching a conclusion; remain vigilant\nagainst common errors including misreading negations or qualifiers, misinterpreting\npercentages or comparisons, overlooking units or contextual cues, misclassifying\nassumptions as facts, and neglecting nested conditionals or compound relationships.\nConclude with a clear, concise final answer: output only \"Yes\" if the Statement is\nverified as true by this rigorous analysis, or \"No\" if it is false. Do not provide\nexplanations or additional commentary.\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: Says Amanda Fritz manages less than 5 percent of city operations.\nExpected: YES\n\n[Example 2]\nInput: Many state and federal agencies have such navigators involved in helping\nfolks maneuver through the often complex processes associated with filing benefits\nclaims, for example -- even buying health insurance.\nExpected: YES\n\n[Example 3]\nInput: Says MAX carries 30 percent of evening rush-hour commuters traveling from\nDowntown on the Sunset and Banfield freeways.\nExpected: YES\n\n[Example 4]\nInput: The State of Texas is funding womens health services at historically high\nlevels; they just increased their level another 50 million for the next two years.\nExpected: YES\n\n[Example 5]\nInput: Ohio is not meeting its obligation to update voter registrations when voters\nchange their address with the BMV.\nExpected: YES\n\n[Example 6]\nInput: Says he was the only Republican to vote against creating a House panel to\ninvestigate Planned Parenthood.\nExpected: YES\n\nPrompt2:\n## Task\nAs an expert in math and navigation reasoning, determine whether the given Statement\n is true or false based solely on the provided Context and any explicitly verifiable,\n relevant information. Use precise, step-by-step logical, numerical, temporal, and\nspatial analysis without introducing unsupported assumptions.\n\n## Guidelines\n- Carefully parse every element of the Statement, paying special attention to\nnegations, conditionals, exclusive terms such as \"only,\" comparative phrases like \"\nmore than,\" \"less than,\" and nuanced modifiers.\n- Break down complex or compound Statements into smaller parts and verify each part\nindependently against the Context.\n\n\n\n\n                                             22\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n- Cross-check all quantitative data, units, percentages, ratios, dates, sequences,\ndirections, and spatial relationships against the Context, performing explicit\ncalculations or logical reasoning as needed.\n- Distinguish rigorously between explicit facts and opinions, assumptions, or\nimplicit claims; rely solely on verifiable information present in the Context or\nestablished, relevant general knowledge.\n- Avoid inferring or assuming information beyond the provided Context unless it is\nlogically necessary, explicitly justified, and clearly documented in your reasoning.\n- When confronted with ambiguity, contradiction, or incomplete information,\nprioritize the most direct, explicit, and reliably sourced evidence from the Context.\n\n- Employ mental visualization, mapping, or systematic logical and numerical checks\nto confirm temporal, spatial, conditional, and comparative relationships.\n- Double-check all calculations, logical deductions, qualifier interpretations, and\nspatial or temporal conclusions before finalizing your determination.\n- Vigilantly avoid common errors such as ignoring negations, misreading percentages\nor comparative data, conflating assumptions with facts, misinterpreting spatial or\nlogical relationships, or overlooking key qualifiers.\n- Maintain a rigorous, detailed, and cautious approach throughout the analysis to\nensure accuracy and reliability in your verification.\n\n## Output format\nRespond only with a single word: Yes if the Statement is true strictly based on the\nContext; otherwise, No.\n\n## Prediction\nText: {input}\nLabel:\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: This is the slowest job recovery since Hoover.\nExpected: NO\n\n[Example 2]\nInput: The State of Texas is funding womens health services at historically high\nlevels; they just increased their level another 50 million for the next two years.\nExpected: YES\n\n[Example 3]\nInput: Oregonians have an amazing no-cost way to fight abortion with free political\ndonations\nExpected: YES\n\n[Example 4]\nInput: Our pension system is the only one in the country thats 100 percent funded.\nExpected: YES\n\n[Example 5]\nInput: Says Rick Scott called education not a core function of the state.\nExpected: NO\n\n[Example 6]\nInput: When we took office, let me remind you, there was virtually no international\npressure on Iran.\nExpected: NO\n\n\nPrompt3:\n## Task\nAs an expert in mathematics and spatial reasoning, determine whether the given\nStatement is true or false solely based on the supplied Context and any directly\nrelevant, verifiable information, using thorough logical, numerical, and spatial\n\n\n\n                                             23\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nanalysis.\n\n## Guidelines\n- Perform a detailed and methodical review of all elements within the Context before\n forming your conclusion.\n- Accurately interpret every quantitative detail-including units, ratios,\npercentages, sequences, directions, and spatial relationships-without exception.\n- Pay close attention to all negations, qualifiers, conditionals, and implied\nmeanings, carefully considering modifiers such as \"not,\" \"only,\" \"less than,\" \"more\nthan,\" and comparative terms.\n- Distinguish clearly between facts, opinions, assumptions, and implied statements;\nverify facts exclusively through explicit contextual evidence or commonly accepted\nknowledge.\n- Refrain from making assumptions beyond the provided information unless they are\nstrictly necessary, explicitly justified, and clearly documented in your reasoning.\n- In cases of ambiguity, contradiction, or incomplete data, prioritize the most\ndirect, explicit, and trustworthy information available in the Context.\n- Avoid common pitfalls: do not ignore negations or qualifiers, misconstrue\npercentages or comparative data, or misunderstand spatial, logical, or conditional\nrelationships.\n- Employ mental visualization, mapping, or stepwise logical checks to confirm\nspatial and relational interpretations as needed.\n- Verify all calculations, logical inferences, and spatial evaluations carefully\nbefore delivering your final decision.\n- Analyze the Statement meticulously, focusing on each word and modifier in its\ncontext to ensure precise comprehension.\n\n## Output format\nRespond with a single word: Yes if the Statement is true; No if it is false.\n\n## Prediction\nText: {input}\nLabel:\n\n\n\n\nInitial prompt of the BBH-nevigate dataset\n\n\n# Role & Task\nYou are an expert in spatial navigation. Given a sequence of navigation instructions,\n determine if the path returns to the starting point.\nInstructions include directions (north, south, east, west) and actions (move forward,\n turn left, turn right).\nTrack the position and orientation carefully, considering each step and turn. Output\n only \"Yes\" if the path returns to the starting point, or \"No\" if it does not.\n\n# Output Requirement\nYou must output the result in strict JSON format, with no additional text. The JSON\nshould be an array containing a single object with the \"label\" key.\n- If the path returns to the starting point, output: [{\"label\":\"YES\"}]\n- If the path does NOT return to the starting point, output: [{\"label\":\"NO\"}]\n- Any output that deviates from this format (such as plain text, missing brackets,\nor other content) will be considered invalid.\n\n# Example\n- Input: \"Always face forward. Take 1 step backward. Take 4 steps left. Take 4 steps\n left.\"\n- Output: [{\"label\":\"NO\"}]\n\n\n\n\n\n                                             24\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\nELPO optimized prompt of the BBH-nevigate dataset\n\n\nPrompt1\nYou are an expert in spatial navigation path - return - to - start - point\ndetermination. Given a series of navigation instructions (which consist of\ndirections like north, south, east, west and actions like move forward, turn left,\nturn right), figure out whether the path goes back to the starting point.\n- **Orientation Tracking**: Start by initializing the orientation (assume facing\nnorth at the start). Maintain a turn - count (mod 4) and orientation mapping (\nturn_count: 0 to North, 1 to West, 2 to South, 3 to East). For each turn (left/right)\n, update the orientation using the cumulative 90 - degree turn rule. After every 4\nleft/right turns, reset the orientation. Keep a running count of cumulative turn\nangles.\n- **Movement Tracking**: For each forward/backward step, update the position in the\ncurrent orientation’s forward/backward axis (e.g., if facing north, forward is +y,\nbackward is -y in a Cartesian - like coordinate system. If facing east, forward is +\nx, backward is -x. If facing south, forward is -y, backward is +y. If facing west,\nforward is -x, backward is +x). When there is a left/right step (which changes the\ndirection of movement), first update the orientation as per the rules and then\nupdate the position. Track movements explicitly by writing down the movement in each\n axis (x and y) for each step (e.g., [orientation, x_change, y_change]).\n- **Sum Calculation**: Maintain separate sums for the x (east - west) and y (north -\n south) axes. After processing all the steps in the instruction, check if both sums\nare zero. Use a step - by - step table for x and y sums and double - check\narithmetic (e.g., - 5+3 = - 2, not + 2). Use intermediate checks (e.g., after every\n5 steps).\n- **Edge Case Handling**: Test edge cases like 4 consecutive turns (left or right)\nto ensure orientation reset. For ambiguous inputs (e.g., \"Turn around\" = 2 left/\nright turns), convert to standard turns. For movement without direction (e.g., \"Take\n N steps\"), assume forward unless context (like prior \"Turn around\") implies\nbackward. Pre - process input: replace \"Turn around\" with \"Turn left Turn left\" or \"\nTurn right Turn right\". For \"Take N steps\", default to \"Take N steps forward\" (\noverride if \"Turn around\" precedes).\n- **Error Prevention**: Avoid orientation miscalculation (especially orientation\nreset after 4 turns), movement axis error (correctly map movement to current\norientation’s axis), sum calculation arithmetic mistake, and edge case neglect. Use\na systematic approach (like writing down orientation and movement for each step) to\navoid confusion. Double - check all calculations, especially for edge cases. Pay\nclose attention to the order of operations (e.g., update orientation first when\nthere is a turn before updating movement). When handling input instructions, parse\nthem carefully to ensure all steps (turns and movements) are correctly identified\nand processed. When dealing with movement steps that have no direction specified (e.\ng., \"Take 10 steps\" without specifying forward or backward), assume forward movement\n unless context suggests otherwise. But also be aware of cases where \"turn around\"\nfollowed by \"Take steps\" implies backward movement.\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: If you follow these instructions, do you return to the starting point? Always\n face forward. Take 2 steps forward. Take 2 steps backward. Take 4 steps right. Take\n 7 steps right.\nOptions:\n- Yes\n- No\nExpected: NO\n\nPrompt2:\nYou are an expert in spatial navigation path - return - to - start - point\ndetermination. Given a series of navigation instructions (which consist of\ndirections like north, south, east, west and actions like move forward, turn left,\nturn right), figure out whether the path goes back to the starting point.\n- **Orientation Tracking**: Start by initializing the orientation (assume facing\nnorth at the start). Maintain a turn - count (mod 4) and orientation mapping (\n\n\n\n                                             25\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nturn_count: 0 to North, 1 to West, 2 to South, 3 to East). For each turn (left/right)\n, update the orientation using the cumulative 90 - degree turn rule. After every 4\nleft/right turns, reset the orientation. Keep a running count of cumulative turn\nangles. Replace \"Turn around\" with \"Turn left Turn left\" or \"Turn right Turn right\"\n(whichever is appropriate).\n- **Movement Tracking**: For each movement (e.g., \"Take N steps forward/backward\"):\n   - If there was a turn before the movement, update orientation first.\n   - Map movement to current orientation’s axis:\n      - North: forward = +y, backward = -y\n      - East: forward = +x, backward = -x\n      - South: forward = -y, backward = +y\n      - West: forward = -x, backward = +x\n   - Record x_change and y_change for each step (e.g., [orientation, x_change,\ny_change]).\n   - For movement without direction (e.g., \"Take N steps\"):\n      - Assume forward unless \"Turn around\" precedes (then assume backward).\n- **Sum Calculation**: Maintain separate sums for x and y axes. After each step,\nupdate the sums (e.g., x_sum += x_change, y_sum += y_change). Use intermediate\nchecks (e.g., after every 5 steps) to verify sums. Double - check arithmetic (e.g.,\n-5 + 3 = -2, not +2).\n- **Edge Case Handling**: Test edge cases like 4 consecutive turns (left or right)\nto ensure orientation reset. For ambiguous inputs (e.g., \"Turn around\"), convert to\nstandard turns (2 left/right turns). For movement without direction, use the default\n (forward) with context override (if \"Turn around\" precedes, use backward).\n- **Error Prevention**: Avoid orientation miscalculation (especially orientation\nreset after 4 turns), movement axis error (correctly map movement to current\norientation’s axis), sum calculation arithmetic mistake, and edge case neglect. Use\na systematic approach (like writing down orientation and movement for each step in a\n table: Step \\ Instruction \\ Orientation \\ x_change \\ y_change) to avoid confusion.\nDouble - check all calculations, especially for edge cases. Pay close attention to\nthe order of operations (e.g., update orientation first when there is a turn before\nupdating movement). When handling input instructions, parse them carefully to ensure\n all steps (turns and movements) are correctly identified and processed.\nAdditionally, always use a step - by - step table (as mentioned in the error\nprevention section) to avoid confusion. After calculating x_sum and y_sum, double -\ncheck the arithmetic and ensure that orientation was correctly updated before each\nmovement. Familiarize yourself with edge cases (4 turns, ambiguous instructions)\nthrough practice problems. Use the following additional guidelines:\n   - **Orientation Tracking**: Always start with initial orientation (north) and\nturn - count (0). For each turn (left/right), increment/decrement turn - count (mod\n4). Double - check orientation mapping (0 to North, 1 to West, 2 to South, 3 to East)\n. When 4 turns (left or right) occur consecutively, reset turn - count to 0 and\norientation to North. Replace \"Turn around\" with 2 left/right turns (whichever is\nappropriate) immediately.\n   - **Movement Tracking**: If a turn precedes a movement, update orientation first.\n Use the correct axis mapping: North (forward = +y, backward = -y), East (forward = +\nx, backward = -x), South (forward = -y, backward = +y), West (forward = -x, backward\n = +x). For movement without direction (e.g., \"Take N steps\"), assume forward unless\n \"Turn around\" precedes (then assume backward). Record [orientation, x_change,\ny_change] for each step in a table (Step \\ Instruction \\ Orientation \\ x_change \\\ny_change).\n   - **Sum Calculation**: Maintain separate x_sum and y_sum. After each step, update\n the sums (x_sum += x_change, y_sum += y_change). Use intermediate checks (e.g.,\nafter every 5 steps) to verify sums. Double - check all arithmetic operations (e.g.,\n - 5+3=-2, not +2).\n   - **Edge Case Handling**: Test 4 consecutive turns (left or right) in practice\nproblems to ensure orientation reset. For ambiguous \"Turn around\", convert to\nstandard turns (2 left/right turns) as per system prompt. For movement without\ndirection, use default (forward) with context override (if \"Turn around\" precedes,\nuse backward).\n   - **General Advice**: Use a systematic step - by - step table (as in the error\nprevention section of the system prompt) for every problem. This helps in\nvisualizing orientation and movement changes clearly. Double - check all\n\n\n\n                                             26\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\ncalculations (orientation update, movement - axis mapping, sum arithmetic) at each\nstep. Familiarize yourself with common edge cases (4 turns, ambiguous instructions)\nthrough continuous practice. When in doubt about an instruction (e.g., movement\ndirection ambiguity), refer back to the rules in the system prompt (e.g., default\nforward with \"Turn around\" context override for backward).\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: If you follow these instructions, do you return to the starting point? Always\n face forward. Take 6 steps backward. Take 8 steps left. Take 3 steps right. Take 7\nsteps forward. Take 3 steps right. Take 9 steps right. Take 1 step backward. Take 7\nsteps left.\nOptions:\n- Yes\n- No\nExpected: YES\n\n[Example 2]\nInput: If you follow these instructions, do you return to the starting point? Always\n face forward. Take 8 steps forward. Take 9 steps right. Take 2 steps forward. Take\n10 steps backward. Take 9 steps left.\nOptions:\n- Yes\n- No\nExpected: YES\n\n[Example 3]\nInput: If you follow these instructions, do you return to the starting point? Always\n face forward. Take 9 steps left. Take 10 steps forward. Take 9 steps right. Take 10\n steps backward.\nOptions:\n- Yes\n- No\nExpected: YES\n\n\n\nInitial prompt of the ETHOS dataset\n\n\n## Task\nIs the following text hate speech?\n## Output format\nAnswer Yes or No as labels.\n## Prediction\nText: {input}\nLabel:\n\n\n\nELPO optimized prompt of the ETHOS dataset\n\n\nPrompt1:\nYou are an expert in identifying hate speech in text. Hate speech is language that\nexplicitly or implicitly targets a protected group-defined by characteristics\nincluding race, ethnicity, religion, gender, sexual orientation, disability, mental\nor physical health status, or other identities-with clear hostility, hateful intent,\n or calls for hatred, discrimination, violence, or dehumanization against that group.\n\n\nTo determine whether the given text constitutes hate speech, follow these precise\nguidelines:\n\n\n\n\n                                             27\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n1. Confirm that the text targets a protected individual or group with explicit or\nimplicit hostility, hateful intent, or advocacy of hatred, discrimination, violence,\n or dehumanization. The hostility must be connected to the group’s identity; general\n criticism, negative opinions, or frustration without group-based hostility do not\nqualify as hate speech.\n\n2. Recognize that hate speech includes not only overt hateful statements but also\nsubtle, coded, metaphorical, sarcastic, or indirect language that clearly conveys\nhostility and intent to demean, devalue, or incite harm toward a protected group.\n\n3. Carefully distinguish between insults or negative remarks about individuals or\nbehaviors and hate speech. Individual insults count as hate speech only if they\ntarget the person explicitly because of their protected group identity or as a\nrepresentative symbol of that group.\n\n4. Apply a comprehensive and inclusive list of protected groups, explicitly\nincluding mental and physical health conditions as protected statuses. Statements\nattacking or dehumanizing people on the basis of mental or physical illness, or\nadvocating harm or discrimination toward them, constitute hate speech.\n\n5. Analyze the context, rhetorical devices, and implied intention behind language to\n identify if the text promotes hatred, discrimination, violence, or dehumanization\nagainst a protected group.\n\n6. Require clear, convincing, and direct or indirect evidence of hateful targeting\nand harmful intent before labeling text as hate speech. Avoid false positives by\nresponding ’No’ when evidence is ambiguous, lacks clear hostility toward a protected\n group, or only expresses criticism or negative opinion without hateful intent.\n\n7. When uncertain or lacking sufficient evidence of hateful intent toward a\nprotected group, always respond ’No.’ Respond only with ’Yes’ if the text clearly\nmeets all these criteria.\n\nRespond only with ’Yes’ if the text unequivocally constitutes hate speech as defined\n above; otherwise, respond with ’No.’\n\nText: {input}\nLabel:\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: To say that black lives matter, is not racist statement. But the B.L.M.\norganization itself is racist group. If they could just quit hating white people and\n turn that energy towards their own community, the biggest group of black oppressors\n there is, they might make a difference.\nExpected: NO\n\n[Example 2]\nInput: Do you know why he is smiling because there is no ’excretion law’ in New\nZealand! The max sentence he will receive from a judge is no more than 27 years in\nprison! Is this justice? Or because Muslims lives don’t matter!??? :(((\nExpected: NO\n\n[Example 3]\nInput: I’m very sad about this fucking situation with all this muslims\nExpected: NO\n\n[Example 4]\nInput: all mental illnesses are awful and must be treated\nExpected: YES\n\nPrompt2:\n\n\n\n\n                                             28\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nYou are an expert in identifying hate speech in text. Hate speech is language that\nexplicitly or implicitly targets a protected group-defined by characteristics\nincluding race, ethnicity, religion, gender, sexual orientation, disability, mental\nor physical health status, or other identities-with the intent to promote hatred,\ndiscrimination, violence, dehumanization, or to incite such actions.\n\nTo determine whether the given text constitutes hate speech, follow these precise\nsteps:\n\n1. Identify whether the text targets a protected group or individuals specifically\nidentified with a protected group with hostility, hatred, harmful intent, or calls\nfor harm, including explicit or implicit hateful language conveyed through direct,\nsubtle, coded, or indirect means.\n\n2. Recognize that hate speech may include aggressive insults, threats,\ndehumanization, calls for violence, or derogatory language linked to protected group\n identities, even if veiled by sarcasm, metaphor, or rhetorical devices.\n\n3. Distinguish hate speech from language expressing general anger, frustration,\ncriticism, or insults that do not specifically target protected groups or advocate\nharm against them; profanity or insults alone do not constitute hate speech without\ntargeted hateful intent.\n\n4. Carefully analyze context, underlying intent, rhetorical structures, and the\nrelationship between the language and protected group identities to detect advocacy,\n promotion, or encouragement of hatred, discrimination, violence, or dehumanization.\n\n5. Include all protected groups comprehensively, explicitly considering commonly\noverlooked identities such as mental and physical health conditions, religion,\nethnicity, and political or ideological groups when they are targeted with hateful\nintent.\n\n6. Treat insults or threats aimed solely at individuals as hate speech only if they\nreflect hostility toward the individual’s protected group identity or are directed\nat individuals as representatives of a protected group.\n\n7. Require clear, convincing evidence of hateful targeting and harmful intent before\n labeling text as hate speech, avoiding false positives by withholding judgment when\n such evidence is ambiguous or lacking.\n\n8. When uncertain, err on the side of caution by responding ’No’; respond only with ’\nYes’ if the text clearly meets all criteria of hate speech as defined above.\n\nRespond only with ’Yes’ if the text clearly meets these criteria; otherwise, respond\n with ’No.’\n\nText: {input}\nLabel:\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: That girl still need a good ass whooping, stupid ass bitch!!\nExpected: YES\n\n[Example 2]\nInput: This priest is being hated even by his stupid god.\nExpected: YES\n\n[Example 3]\nInput: If I had to meet one of the apostles I would fuck them for not killing him\nExpected: YES\n\n[Example 4]\n\n\n\n                                             29\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nInput: Ignorant feminist, she got roasted and still doesn’ understand what equality\nof outcome means even when Jordan explained data to her. What’s this means she’s\nignorant and emotionally biased.\nExpected: YES\n\n[Example 5]\nInput: Taliban and all people with this thought must hanged\nExpected: YES\n\nPrompt3:\nYou are tasked with determining whether a given text constitutes hate speech. Hate\nspeech is defined as language that explicitly or implicitly targets a protected\ngroup-such as those defined by race, ethnicity, religion, gender, sexual orientation,\n disability, mental or physical health status, or other identities-with the intent\nto promote hatred, discrimination, violence, dehumanization, or to incite such\nactions.\n\nTo accurately assess if the text is hate speech:\n1. Identify whether the text targets a protected group in a hateful or harmful way,\neither explicitly or through subtle, coded, or indirect language.\n2. Differentiate hate speech from negative or emotionally charged expressions like\nanger, frustration, criticism, sarcasm, profanity, or insults that do not promote\nhatred or harm against protected groups. Strong emotions or controversial opinions\nalone do not constitute hate speech.\n3. Consider the context, rhetorical devices, and underlying intent behind the\nlanguage to detect if there is an advocacy or promotion of hatred, discrimination,\nor violence toward the protected group.\n4. Include all protected groups comprehensively, paying particular attention to\nthose related to mental or physical health conditions as these are often overlooked.\n5. Avoid labeling statements expressing sorrow, anger, or frustration without\nhateful targeting as hate speech.\n6. Use a careful, methodical approach, applying hate speech definitions consistently,\n and strive to minimize both false positives (wrongly labeling non-hate speech as\nhate speech) and false negatives (missing actual hate speech).\n7. When uncertain, prioritize accuracy by balancing caution and rigor, seeking\ncontextual clues before labeling.\n\nRespond only with ’Yes’ if the text meets the criteria for hate speech; otherwise,\nrespond with ’No.’\n\nText: {input}\nLabel:\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: To say that black lives matter, is not racist statement. But the B.L.M.\norganization itself is racist group. If they could just quit hating white people and\n turn that energy towards their own community, the biggest group of black oppressors\n there is, they might make a difference.\nExpected: NO\n\n[Example 2]\nInput: Can’t tell if you’re a pedophile victim or a pedophile.\nExpected: NO\n\n[Example 3]\nInput: Do you know why he is smiling because there is no ’excretion law’ in New\nZealand! The max sentence he will receive from a judge is no more than 27 years in\nprison! Is this justice? Or because Muslims lives don’t matter!??? :(((\nExpected: NO\n\n[Example 4]\nInput: all mental illnesses are awful and must be treated\n\n\n\n                                             30\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nExpected: YES\n\n[Example 5]\nInput: or when the man walked up to the little girl with the gay pride flag i wanted\n to jump into the video and punch him but ya know thats assault\nExpected: YES\n\n\nInitial prompt of the ArSarcasm dataset\n\n\n## Task\nIs this tweet sarcastic?\n## Output format\nAnswer Yes or No as labels.\n## Prediction\nText: {input}\nLabel:\n\n\nELPO optimized prompt of the ArSarcasm dataset\n\n\nPrompt1:\n## Task\nDetermine whether the following tweet is sarcastic. Sarcasm often involves saying\nthe opposite of what is meant, using irony, exaggeration, or humor to convey\ncriticism or mockery.\n\n## Instructions\n1. Carefully analyze the tone, context, and implicit meaning of the tweet.\n2. Consider whether the statement uses irony, ridicule, or exaggeration to convey a\nmessage contrary to the literal words.\n3. Tweets may include cultural, political, or social references; account for these\nnuances.\n4. If unsure, apply a step-by-step reasoning process (Chain of Thought) to reflect\non the indicators of sarcasm before deciding.\n5. Do NOT label as sarcastic if the statement is straightforward or literal without\nhints of irony or mockery.\n6. Your final answer must be either \"Yes\" (sarcastic) or \"No\" (not sarcastic), with\nno additional text.\n\n## Prediction\nText: {input}\nThought process: [Step-by-step reasoning about tone, context, irony, exaggeration,\ncultural references, and intent]\nLabel:\nPrompt2:\nYou are an expert in detecting sarcasm in short Arabic tweets. For each tweet,\ncarefully analyze subtle linguistic cues, tone, cultural context, and the use of\nirony, exaggeration, or contradiction between the literal content and the intended\nmeaning. Pay close attention to indirect expressions, humor, hashtags, emojis,\ninterjections, and any mockery or criticism disguised as praise or neutral\nstatements. Consider cultural references and context beyond just the words to\ndetermine if the tweet’s message contradicts reality or conveys criticism through\nsarcasm. For each input, first reflect on potential misinterpretations by\nidentifying missed irony, overlooked mockery, or ignored cultural context. Then\nanswer with Yes if the tweet is sarcastic, otherwise No.\n\nText: {input}\nLabel:\n\nPrompt3:\n\n\n\n                                             31\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n## Task\nDetermine whether the following tweet is sarcastic. Sarcasm typically involves\nexpressing the opposite of the literal meaning, often using irony, exaggeration,\nmockery, or humor to convey criticism or a hidden message.\n\n## Instructions\n1. Carefully analyze the tweet’s literal meaning first before considering sarcasm.\n2. Identify clear, explicit or strongly implied indicators of sarcasm such as irony,\n ridicule, exaggeration, contradictory statements, or mockery.\n3. Consider cultural, political, social, and linguistic referencesincluding emojis,\nhashtags, and idiomatic expressionsbut treat these only as supporting evidence,\nnever as definitive proof of sarcasm.\n4. When tone or context is subtle or ambiguous, apply a deliberate step-by-step\nChain of Thought reasoning process: weigh all linguistic and contextual clues\nobjectively, verifying if the message contradicts its literal meaning or contains\nmockery or irony.\n5. Do not label a tweet sarcastic if it is straightforward, literal, serious, or\nlacks clear cues of mockery, humor, irony, or contradiction.\n6. Avoid overinterpreting emojis, hashtags, or cultural references without\nsupporting textual evidence of sarcasm.\n7. Your final answer must be exactly \"Yes\" if sarcastic or \"No\" if not, with no\nadditional explanation or text.\n\n## Prediction\nText: {input}\nThought process: [Detailed, stepwise analysis of literal meaning, tone, irony,\nexaggeration, context, cultural and linguistic cues, verifying contradictions or\nmockery before concluding sarcasm]\nLabel:\n\n\nInitial prompt of the WSC dataset\n\n\n## Task\nSolve the problem.\n## Prediction\nText: {input}\nLabel:\n\n\nInitial prompt of the WSC dataset\n\n\nPrompt1:\nYou are an expert in pronoun resolution and coreference understanding within short\ntext passages that require nuanced semantic and logical reasoning. Given a passage\nand two options labeled A and B, your task is to determine the noun phrase that the\npronoun logically and semantically refers to, based on a thorough analysis of the\npassages full context.\n\nGuidelines:\n1. Carefully read the entire passage to fully grasp the situation and the roles of\nall entities mentioned.\n2. Identify all plausible noun phrase candidates that the pronoun could refer to; do\n not assume the closest noun phrase is correct.\n3. For each candidate, evaluate semantic compatibility, actions described,\nproperties, and the logical coherence of the pronouns reference within the passage.\n4. Prioritize logical and semantic fit over proximity or surface cues.\n5. Eliminate candidates that conflict with the passages meaning, actions, or\ndescribed properties.\n6. Select the antecedent that best maintains overall coherence, logical consistency,\n and natural interpretation of the passage.\n\n\n\n                                             32\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n\nSubmit only the letter of the correct answer.\n\nExample:\nText: \"I couldn’t find a spoon, so I tried using a pen to stir my coffee. But that\nturned out to be a bad idea, because it got full of ink. What does the pronoun ’it’\nrefer to?\" (A) The pen (B) The coffee\nLabel: B\n\nPrediction:\nText: {input}\nLabel:\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: \"I couldn’t find a spoon, so I tried using a pen to stir my coffee. But that\nturned out to be a bad idea, because it got full of ink. What does the pronoun ’it’\nrefer to?\" (A) The pen (B) The coffee\nExpected: B\n\nPrompt2:\n## Task\nSolve the problem by choosing the appropriate option, either A or B, and submit only\n the letter of your chosen answer.\n## Example\nText: \"Steve follows Fred’s example in everything. He admires him hugely. What does\nthe pronoun ’He’ refer to?\" (A) Steve (B) Fred\nLabel: A\n## Prediction\nText: {input}\nLabel:\nPrompt3:\nYou are an expert in resolving pronoun ambiguity in complex sentences containing\nmultiple clauses and potential antecedents. For each input sentence, determine\nwhether option A or B correctly identifies the pronouns true referent. Provide your\nanswer as a single letter, preceded by a thorough, step-by-step analysis as detailed\n below.\n\nFollow this comprehensive procedure before selecting your answer:\n\n1. Exhaustively identify every plausible antecedent from all clausesmain,\nsubordinate, embedded, and causally linkedwithout omitting or assuming candidates\nbased on proximity, salience, or default heuristics.\n\n2. For each candidate antecedent, rigorously verify grammatical agreement in person,\n number, gender, and syntactic compatibility, carefully analyzing the sentences\nstructure.\n\n3. Fully parse the sentences syntax to delineate clause hierarchies and establish\nthe precise grammatical role (e.g., subject, object, possessor) of each candidate\nwithin all relevant clauses.\n\n4. Integrate deep semantic and contextual reasoning, assessing coherence, causal\nrelationships, and real-world plausibility for each candidate as the pronouns\nreferent, considering who logically can perform or experience the described action.\n\n5. Avoid premature exclusion of any candidates; only eliminate antecedents after\nthorough syntactic and semantic evaluation.\n\n6. If ambiguity persists after the initial pass, systematically repeat all steps,\nensuring no candidates have been overlooked or incorrectly rejected and that both\nsyntactic and semantic analyses are fully complete.\n\n\n\n\n                                             33\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\nDo not rely on shortcuts such as defaulting to the nearest noun, using gender cues\nalone, or making unsubstantiated assumptions.\n\nFormat your response exactly as follows:\n\nText: {input sentence with options}\nLabel: {A or B}\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: \"Tom threw his schoolbag down to Ray after he reached the bottom of the\nstairs. What does the pronoun ’he’ refer to?\" (A) Tom (B) Ray\nExpected: B\n\n[Example 2]\nInput: \"John couldn’t see the stage with Billy in front of him because he is so\nshort. What does the pronoun ’he’ refer to?\" (A) John (B) Billy\nExpected: A\n\n[Example 3]\nInput: \"Madonna fired her trainer because she couldn’t stand her boyfriend. What\ndoes the pronoun ’her’ refer to?\" (A) Madonna (B) The trainer\nExpected: B\n\n\nInitial prompt of the GSM8K dataset\n\n\n## Task\nSolve the math problem.\n## Prediction\nInput: {input}\nExpected:\n\n\nInitial prompt of the GSM8K dataset\n\n\nPrompt1:\nYou are an expert in solving complex multi-step math word problems involving\nquantities, totals, leftovers, and transactional relationships such as items ordered,\n sold, leftover, or remaining. For each problem:\n\n1. Carefully read the entire problem and identify all given quantities, explicitly\nextracting each with its units and clear labels. Define distinct variables for every\n relevant quantity involved.\n\n2. Pay close attention to relational language and key terms like leftover, remaining,\n total, sold, ordered, and similar. Precisely translate these into explicit\nmathematical relationships for example, interpret \"leftover\" as items remaining\nafter subtraction (leftover = ordered  sold).\n\n3. Write down all equations representing these relationships before performing any\ncalculations, clearly linking totals to sums or differences of parts. Explicitly\nstate how quantities combine, increase, decrease, or remain consistent.\n\n4. Systematically solve the problem step-by-step: carry out arithmetic operations\ncarefully, double-check all calculations immediately after each step, and verify\nthat the operations correctly reflect the relationships identified.\n\n5. After computing intermediate and final results, verify their numerical\ncorrectness, logical coherence, and contextual consistencyincluding proper units and\n labelsand ensure all quantities sum or balance as indicated by the problem. If\n\n\n\n                                             34\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\ninconsistencies appear, revisit and correct prior steps before proceeding.\n\n6. Use estimation and reasonableness checks throughout to confirm answers are\nplausible within the problems context and scale.\n\n7. Present only the final numeric answer exactly as requested, including units if\nspecified, without explanation, intermediate steps, or commentary.\n\nApply this methodical approach to all problems to accurately track quantities,\ntotals, leftovers, and related multi-step arithmetic reasoning.\n\n# Few-Shot Exemplars (from high-scoring failures)\n[Example 1]\nInput: Barney’s grocery store sold out all of its items at the beginning of the\npandemic, so they ordered extra items to restock the shelves. However, they ended up\n ordering far too much and have to keep the leftover items in the storeroom. If they\n ordered 4458 items, sold another 1561 items that day, and have 575 items in the\nstoreroom, how many items do they have left in the whole store?\nExpected: 3,472\n\nPrompt2:\nYou are proficient at tackling complex multi-step math word problems covering\narithmetic, ratios, proportions, percentages, geometry, and fundamental algebra. For\n each given problem, adhere to this meticulous procedure:\n\n1. Thoroughly read the problem and explicitly identify every numerical value\npresented, including their units and relevant context (such as totals, parts, rates,\n or specific conditions).\n\n2. Clearly assign variables and extract all expressed relationships, ratios,\nproportions, and conditions from the problem; accurately convert these into precise\nmathematical equations or expressions without making any assumptions beyond the\nprovided information.\n\n3. Break the problem down into well-defined, logical, and sequential steps. Fully\nsolve and confirm the correctness of each step before proceedingdo not omit any\nintermediate calculations or reasoning.\n\n4. Ensure units are consistently and correctly applied throughout all computations,\nconverting units beforehand when necessary.\n\n5. Process percentages, ratios, and fractions with careful attention to their\naccurate contextual meanings.\n\n6. After completing each intermediate calculation, review its validity, consistency,\n and plausibility within the problems scenario.\n\n7. Once all steps are complete and verified, thoroughly re-examine the entire\nsolution to ensure it fully satisfies the questions demands and constraints, double-\nchecking all interpretations, equation setups, and mathematical procedures.\n\n8. At the conclusion, provide only the precise final numerical answer requested,\nincluding units if specified, with no explanations, intermediate details, or\ncommentary.\nPrompt3:\n## Task\nSolve the math problem and provide only the final answer without any extra\nexplanations or comments.\n# Few-Shot Exemplars\nInput: Ryan is considering buying a new multivitamin brand. Each pill contains 50 mg\n of Vitamin A. The recommended daily intake of Vitamin A is 200 mg. How many pills\ndoes Ryan need to consume in a week to meet the recommended amount?\nExpected: 28\n\n\n\n                                             35\n\nELPO: Ensemble Learning Based Prompt Optimization for Large Language Models\n\n\n## Prediction\nInput: {input}\nExpected:\n\n\n\n\n\n                                             36",
"headers": [
"arXiv:2511.16122v1  [cs.CL]  20 Nov 2025",
"ELPO: Ensemble Learning Based Prompt Optimization",
"for Large Languages Models",
"Abstract",
"1. Introduction",
"2. Related Work",
"3. Methodology",
"4. Experiments and Results",
"5. Conclusion",
"References",
"A. Additional Details for the Setup",
"B. ELPO Process",
"C. Additional Result"
],
"tables": [
"|Method|LIAR<br>(F1)|BBH<br>(F1)|ETHOS<br>(F1)|ArSarcasm<br>(F1)|WSC<br>(Acc.)|GSM8K<br>(Acc.)|\n|---|---|---|---|---|---|---|\n|Empty|46.4|69.4|93.0|83.7|77.3|89.0|\n|CoT(Kojima et al., 2022)|46.0|81.9|84.5|83.7|81.3|89.0|\n|APE(Zhou et al., 2023)|51.2|74.3|93.2|84.3|79.3|91.3|\n|ProTeGi(Pryzant et al., 2023)|60.3|73.6|97.0|84.1|80.0|91.0|\n|OPRO(Yang et al., 2024)|52.1|75.0|94.8|84.7|83.3|90.7|\n|Promptbreeder(Fernando et al., 2024)|51.8|75.7|95.7|84.5|80.0|91.7|\n|EvoPrompt (Guo et al., 2024)|52.3|76.4|94.3|83.9|78.8|90.7|\n|GPO(Tang et al., 2025)|56.6|75.0|95.5|83.8|84.0|90.3|\n|ELPO|**72.1**|**91.1**|**98.4**|**92.3**|**95.9**|**96.0**|",
"|Baseline Generator Framework|LIAR BBH ETHOS ArSarcasm WSC GSM8K<br>(F1) (F1) (F1) (F1) (Acc.) (Acc.)|\n|---|---|",
"|✓<br>✓ ✓<br>✓ ✓<br>✓ ✓ ✓|42.5 71.1 97.6 74.4 76.2 76.7<br>65.3 84.0 97.7 86.7 88.9 90.5<br>43.2 73.1 97.9 79.2 87.0 80.0<br>72.1 91.1 98.4 92.3 95.9 96.0|\n|---|---|",
"|Baseline average weighted|LIAR BBH ETHOS ArSarcasm WSC GSM8K<br>(F1) (F1) (F1) (F1) (Acc.) (Acc.)|\n|---|---|\n|✓<br>✓<br>✓<br>✓<br>✓|63.3<br>84.7<br>95.1<br>83.3<br>91.2<br>93.3<br>66.7<br>85.8<br>98.3<br>85.7<br>94.7<br>93.8<br>**72.1**<br>**91.1**<br>**98.4**<br>**92.3**<br>**95.9**<br>**96.0**|",
"|Dataset Name|Task|Train & Dev|Test|License|\n|---|---|---|---|---|\n|LIAR|True/False|3681|461|Unknown|\n|BBH-Navigate|True/False|153|97|Apache-2.0|\n|ETHOS|True/False|300|217|GNU GPLv3|\n|ArSarcasm|True/False|8437|2110|MIT|\n|GSM8K|Integer Generation|7473|1319|MIT|\n|WSC|Multiple-Choice|162|123|CC BY 4.0|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2511.16122v1.pdf"
}