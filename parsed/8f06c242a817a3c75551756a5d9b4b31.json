{
"text": "GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via\n                              Reinforcement Learning\n\n                  Yuze Liu1, Tingjie Liu2, Tiehua Zhang3∗, Youhua Xia2∗, Jinze Wang4,\n                            Zhishu Shen5, Jiong Jin4 and Fei Richard Yu6\n                                          1Ant Group, Shanghai, China\n                       2Guangdong Laboratory of Artificial Intelligence (SZ), Shenzhen, China\n                     3School of Computer Science and Technology, Tongji University, Shanghai, China\n           4Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, Australia\n           5School of Computer Science and Artificial Intelligence, Wuhan University of Technology, Wuhan, China\n               6College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China\n                          Abstract                      2021). In the standard paradigm of ICL, a small\n                                                                   collection of examples, each of which comprises\n                Large language models (LLMs) have demon-\n                                                               small piece of contextual texts in natural language,\n                    strated impressive success in a wide range of\n                                                                                  is generally prepared to construct the prompt. This                    natural language processing (NLP) tasks due to\n                                                       prompt effectively empowers LLMs to excel in                     their extensive general knowledge of the world.2024                 Recent works discovered that the performance        tasks such as natural language understanding (Ye\n                  of LLMs is heavily dependent on the input        et al., 2023) and the logic reasoning (Lu et al.,\n                 prompt. However, prompt engineering is usu-       2023). In contrast to many previous studies thatNov              ally done manually in a trial-and-error fashion,                                                                 concentrate on fine-tuning a large number of train-\n               which can be labor-intensive and challenging\n19             in order to find the optimal prompts. To ad-       able parameters in the model for various down-\n                                                              stream tasks (Wei et al., 2022a), in-context learning\n                  dress these problems and unleash the utmost\n                                                            (ICL) enables LLMs to achieve competitive perfor-                    potential of LLMs, we propose a novel LLMs-\n                  agnostic framework for prompt optimization,      mance without relying on gradient-based training,\n                namely GRL-Prompt, which aims to automati-       thus avoiding over-reliance on computational re-\n                    cally construct optimal prompts via reinforce-       sources such as GPUs. An imperative issue in[cs.CL]          ment learning (RL) in an end-to-end manner.       in-context learning is how to select the most suit-\n               To provide structured action/state representa-                                                                  able examples to construct prompts that effectively\n                   tion for optimizing prompts, we construct a\n                                                        improve the LLM’s performance across various\n                knowledge graph (KG) that better encodes the\n                                                                    tasks (Xu et al., 2024).                   correlation between the user query and can-\n                                                               Previous research has addressed this challenge                   didate in-context examples.  Furthermore, a\n                  policy network is formulated to generate the       primarily through  heuristic  handcrafting,  ran-\n                 optimal action by selecting a set of in-context     dom selection, and retrieval-based selection meth-\n                examples in a rewardable order to construct       ods.  The heuristic handcrafting approach in-\n                   the prompt. Additionally, the embedding-based       volves manually creating examples for a partic-\n                 reward shaping is utilized to stabilize the RL                                                                    ular task, which can be both labor-intensive and\n                    training process. The experimental results show\n                                                          time-consuming (Wei et al., 2022b). Some studies\n                    that GRL-Prompt outperforms recent state-of-\n                                                            have randomly select in-context examples from the                     the-art methods, achieving an average increase\n                  of 0.10 in ROUGE-1, 0.07 in ROUGE-2, 0.07        training dataset to offer additional contextual infor-arXiv:2411.14479v1                    in ROUGE-L, and 0.05 in BLEU.                 mation, with the goal of directing LLMs toward\n                                                                  the desired responses (Brown et al., 2020). Unfor-\n          1  Introduction                                    tunately, the randomness of the selection strategy\n                                                                introduces uncertainty and noise into the prompt,\n           With ongoing advancements in large language mod-                                                                leading to a degradation in the performance of the\n               els (LLMs), such as GPT-3 (Brown et al., 2020)                                                  LLMs. Retrieval-based selection attempts to tackle\n            and LLaMA (Hugo et al., 2023), optimizing query-                                                                          this issue by obtaining semantically similar exam-\n            guided prompts, especially in the realm of in-                                                                   ples using static methods such as K-nearest neigh-\n             context learning (ICL), has been crucial in unlock-                                                               bors (Nie et al., 2022). However, this method fails\n             ing the full potential of LLMs, leading to signifi-                                                                      to thoroughly explore the prompt space, leading to\n             cant improvements across various downstream nat-                                                                   limited effectiveness in complex tasks. It has been\n              ural language processing (NLP) tasks. (Liu et al.,                                                              pointed out in recent studies that retrieval-based\n                  ∗Corresponding authors.                             selection methods face the challenge of overlook-\n\n\n                                                    1\n\ning the permutations of in-context examples, which   2  Related Work\ncan lead to a degradation in the performance of    In recent years, LLMs have achieved remarkable\nLLMs (Lu et al., 2022; Xu et al., 2024).             progress in the field of NLP (Lewis et al., 2019;\n  To alleviate this challenge, we propose a novel    Raffel et al., 2020). Notably, GPT-3 (Brown et al.,\nLLMs-agnostic framework for prompt optimiza-   2020) has demonstrated exceptional capabilities in\ntion, namely GRL-Prompt, which aims to automat-   few-shot ICL (Dong et al., 2022), where it learns\nically construct optimal prompts via reinforcement    tasks with only a few examples as demonstrations,\nlearning (RL) in an end-to-end manner. Our GRL-   without requiring any gradient updates (Liu et al.,\nPrompt framework comprises two key components:   2021; Min et al., 2022).\n1) a knowledge graph built from the user instruc-     To further unleash the potential of LLMs with-\ntion and candidate in-context examples, utilizing a    out updating the large amount of parameters in the\nheterogeneous graph neural network to encode the   model, a series of studies have focused on develop-\nstructured embedding representations of the graph;   ing prompt optimization techniques to guide mod-\nand 2) a policy network that includes a pairwise    els toward generating more accurate and relevant\nedge classifier and an in-context matching network,   outputs with high-quality input prompts. Wei et al.\ngenerating an optimal sequence of in-context exam-   (2022b) investigated how LLMs handle few-shot\nples to create the most effective prompts based on   prompting for reasoning tasks using prompts con-\nthe constructed knowledge graph. The knowledge    sisting of triples—input, chain of thought, and out-\ngraph and the policy network are updated collabo-   put—and examined chain-of-thought prompting as\nratively in an end-to-end manner. Our contributions    a straightforward and widely applicable technique\ncan be summarized as follows:                       for improving reasoning in these models. Follow-\n                                                  ing that, Wang et al. (2023) proposed a chain-of-\n    • We propose a novel LLMs-agnostic frame-                                            knowledge prompting approach that decomposes\n    work GRL-Prompt for prompt optimization.                                                 reasoning chains generated by LLMs into several\n    The knowledge graph is initially constructed                                                evidence triples and explanatory hints, thereby\n     to effectively capture the correlation between                                              enhancing the reasoning capabilities.  Li et al.\n     the user instruction and candidate in-context                                               (2024b) developed a tabular prompting method\n    examples through heterogeneous graph learn-                                                       called TableIE, which reframes the relational triple\n     ing. The designated policy network allows                                                     extraction task as a table generation problem and\n     the GRL-Prompt to function as an agent that                                                has demonstrated promising results in ICL. These\n      interacts iteratively with the LLMs.                                             methods have demonstrated how prompt optimiza-\n                                                       tion enhances the logical reasoning and content    • We formulate a policy network that incorpo-\n                                                   generation capabilities of LLMs.     rates a pairwise edge classifier (PEC) and an\n                                                   Despite these impressive successes, the afore-     in-context matching network (ICMN) to dy-\n                                             mentioned works still face challenges when tack-     namically learn the policy for generating an\n                                                       ling tasks that require deep and complex rea-     order-sensitive sequence of in-context sam-\n                                                soning (Xu et  al., 2024).  Knowledge Graphs      ples. PEC is designed to classify the order of\n                                          (KGs) (Chen et al., 2020) serve as an ideal foun-     examples, while ICMN aims to select the rele-\n                                                    dation for prompt optimization by providing struc-     vant examples to construct the optimal prompt\n                                                    tured knowledge representation. KGs encapsulate    based on the structured representation of a\n                                          domain knowledge through entities and relation-     constructed knowledge graph. Moreover, we\n                                                        ships, providing LLMs with rich contextual infor-     design the embedding-based reward shaping\n                                               mation and reasoning pathways. However, relying     to stabilize the RL training process.\n                                            on knowledge graphs alone for prompt construction\n    • We conduct extensive experiments on two     still has its limitations, as knowledge graph-based\n      distinct datasets, demonstrating that GRL-   prompts often overlook the context-dependence\n    Prompt outperforms state-of-the-art baselines    of natural language, resulting in degraded perfor-\n     in in-context learning. Furthermore, we have   mance on complex tasks. Therefore, it is essen-\n    made our source code publicly available to     tial to optimize knowledge graph-based prompts\n     contribute further to advancements in this field    through interactive feedback from LLMs (Li et al.,\n     (find the source code as supplementary mate-   2024c).\n      rials).                                   To address these challenges, several studies\n\n\n                                         2\n\nhave started to explore reinforcement learning    as a Markov Decision Process (MDP), which is a\nfor prompt optimization. Deng et al. (2023) pro-    sequential decision-making mathematical model in\nposes RLPROMPT, a discrete prompt optimiza-   which actions affect the current short-term rewards,\ntion approach with RL. This method formulates a   subsequent states, and future rewards (Sutton and\nparameter-efficient policy network that generates    Barto, 2018). The MDP is defined as the tuple\nthe optimized discrete prompt by training with the   {S, A, T , R, γ}, where s ∈S and a ∈A denote\ndesignated reward function. Qi et al. (2023) in-    state and action, respectively. T  : S×A →p(S) is\ntroduces PILLOW, a prompt-matching framework    identified by the states transition. p(·) denotes the\nenhanced by RL. This framework utilizes a match-    distribution of the states. R is the reward function,\ning network to select prompts from a user-defined   with γ ∈[0, 1] as the discount factor.\npool, concatenates them with user instructions as   In-context Learning  ICL is a paradigm that fa-\ninput, and performs inference with LLMs to im-    cilitates language models to learn tasks using only\nprove the efficiency of instruction fine-tuning. Si-   a few in-context examples presented in the form of\nmiliarly, PROMPTPG employs a RL-based policy    natural language (Brown et al., 2020). Formally,\ngradient to learn how to select in-context examples    the sequence of in-context examples is denoted as\nfrom a limited amount of training data and then dy-    ⃗Pic = [p1ic, ..., pKic ], where K is the number of in-\nnamically constructs the corresponding prompt (Lu    context examples provided to construct the prompt.\net al., 2023).                              Prompt Optimization Problem  Given a user\nRelation to Existing ICL methods  Compared to   query text q ∈Q and a set of candidate ex-\nthe random selection in the ICL paradigm (Brown   amples Pcand, we aim to learn a prompt opti-\net al., 2020; Wei et al., 2022b), GRL-Prompt up-   mization strategy fopt(q, Pcand) →Pic.  Each\ndates the policy network iteratively to generate    candidate example  pcand  ∈  Pc  is a  triple\nan optimal sequence of in-context examples for    of (query, context, response), where query de-\nprompt optimization. ICL-kNN (Nie et al., 2022),   notes the user query, context represents the extra\non the other hand, uses a static clustering method    information provided by the users (optional), and\n(k-nearest neighbor) to select in-context examples,   response is the expected answer. The generated se-\nwhich also lacks the dynamic learning on the pol-   quence of in-context examples provided for LLMs\nicy network. This method thus fails to thoroughly   can be used for a variety of downstream NLP tasks.\nexplore the prompt space, leading to limited effec-\ntiveness in complex tasks. Another issue is that   4  Methodology\nthe order of the in-context samples introduces vari-\n                                                   In this section, we introduce GRL-Prompt, anation in the performance of LLMs. GRL-Prompt\n                                             LLMs-agnostic framework for prompt optimiza-can dynamically learn the policy for generating an\n                                                         tion via RL. As illustrated in Figure 1, a knowledgeorder-sensitive sequence of in-context samples be-\n                                                graph is constructed using candidate examples andcause its policy network has two key components:\n                                                a user query, from which a heterogeneous grapha PEC and an ICMN. In contrast, PromptPG (Lu\n                                                   neural network is employed to encode their cor-et al., 2023) and Pillow (Qi et al., 2023) only select\n                                                       relations into high-dimensional embedding repre-a fixed-length set of in-context samples. Moreover,\n                                                     sentations for the downstream component. Thethe constructed knowledge graph, which encodes\n                                                   policy network in the agent learns to find the op-the correlation between the user query and the can-\n                                                    timal sequence of in-context examples from thedidate examples, provides a structured representa-\n                                                   constructed knowledge graph, aiming to maximiz-tion for GRL-Prompt to optimize prompts.\n                                                   ing the prediction rewards for the user query when\n3  Preliminary and Problem Definition                                                      interacting with LLM environment. Additionally,\n                                             an embedding-based reward shaping is utilized toIn this section, we first introduce the relevant defi-\nnitions and then formulate the key problems asso-    stabilize the RL training process.\nciated with prompt optimization.\n                                                    4.1  Knowledge Graph for PromptKnowledge Graph A knowledge graph is de-\n                                                   Construction\nfined as G = (V, E, R), with nodes vi ∈V and\ndirected edges (vi, rij, vj) ∈E, where rij ∈R is   The correlations between in-context examples and\nthe relation type of the edge between vi and vj.       the user query influence the performance of LLMs\nReinforcement Learning  The RL is formulated    in the ICL paradigm (Nie et al., 2022). GRL-\n\n\n                                         3\n\nFigure 1: An overview of GRL-Prompt. The circle and triangle represent the candidate nodes and the query\nnode, respectively. The green arrow lines denote candidate-to-candidate edges, the orange arrow lines indicate\nquery-to-candidate edges, and the blue arrow lines represent candidate-to-query edges in the knowledge graph.\n\nPrompt explores the application of KG to better    other candidate nodes vjc(j  ̸=  i) with relation\nencode the correlations between candidate exam-    rcc. The directed candidate-to-candidate edge is\nples and the user query, facilitating prompt con-   denoted as (vic, rcc, vjc), represented by the green\nstruction. The user query and candidate examples   arrow lines in Figure 1. Our formulation results in\nare provided to the KG construction and learning    bidirectional edges between each pair of candidate\nmodule, as indicated by a  in Figure 1.             nodes, i.e. both (vic, rcc, vjc) and (vjc, rcc, vic) ∈E.\n                                         The query-to-candidate edge is constituted by the\n4.1.1  Knowledge Graph Construction\n                                               query node vq and the candidate node vc, which\nSince there is no concept of nodes in the set of can-                                                                is denoted as (vq, rqc, vc). Meanwhile, each can-\ndidate examples Pc and the user query q, we treat                                                    didate node vic ∈V is bidirectionally connected\neach candidate example pic ∈Pcand, i ∈[1, ..., N]    to the query node vq, leading to the candidate-to-\nas a candidate node vic and the user query q as query   query edges (vic, rcq, vq) and query-to-candidate\nnode vq. The candidate node vic and the query node   edges (vq, rqc, vic), respectively. These two types\nvq are represented as a circle and a triangle in Fig-                                                   of edges are represented by the blue arrow lines\nure 1, respectively. The number of the candidate                                             and the orange arrow lines, respectively.\nexamples is N. The node set V = {vic}Ni=1 ∪vq\nin the knowledge graph G(q) consists of two types    4.1.2  Knowledge Graph Learning\nof nodes: candidate node vc and query node vq.   To capture the complex interactions between the\nWe utilize a pre-trained language model, such as                                                  candidate examples and the user query in the con-\nBERT (Lu et al., 2023) and RoBERTa (Ghosal et al.,                                                      structed knowledge graph, we utilize a two-layer\n2021), to generate the initial node embeddings in                                              Heterogeneous Graph Transformer (HGT) (Hu\nthe knowledge graph, which is defined as:                                                            et al., 2020) to perform knowledge graph learning.\n                                          The details of HGT can be found Appendix A.1.             X0 = PreLM(V)             (1)\n\n                                                    4.2  RL-based Prompt OptimizationHere, we use the values from the final layer of\nthe pre-trained language model as the initial node   Recent works have unveiled the high sensitivity\nembeddings X0 ∈R(N+1)×d.                       of LLMs to prompts (Li et al., 2024a; Bach et al.,\n  We construct three types of edges: candidate-   2022). Recent research has shown that the perfor-\nto-candidate edges, query-to-candidate edges and   mance of LLMs with in-context learning can be\ncandidate-to-query edges, which are based on the    highly unstable across different selections of in-\nconstituent nodes to encode different relations.   context examples and permutations of those exam-\nEach candidate node vic ∈V is connected to all    ples (Liu et al., 2021; Lu et al., 2022). Apart from\n\n\n                                         4\n\nthat, finding the optimal prompt for various LLMs    the query node vq. The calculation process of the\nin different tasks is usually done manually in a trial-    selecting probability is defined as follows:\nand-error fashion, which can be labor-intensive and\nchallenging. To alleviate this issue, we design a pol-     ficmn(vq,vic)=sigmoid(X(vq)·Wm·XT(vi√       c))  (4)\nicy network that utilizes the structural correlations                               d\nin the constructed knowledge graph, as indicated\n                            Wm ∈Rd×d denotes the learnable matrix, and\nby b  in Figure 1, to optimize prompts within the\n                                           sigmoid function maps the similarity between the\nRL paradigm, avoiding brute-force searching or\n                                                  candidate example and the user query into proba-                                √manually designed heuristics.\n                                                              bility space.  d acts as a scaling factor.\n4.2.1  Policy Network                         The policy network obtains the probability dis-\n                                                        tribution for the generation of the sequence of in-We design a policy network πθ (ˆo, Pic|G(q)) that\n                                                    context examples from the candidate examples setincorporates a PEC to predict the relative order of\nany two candidate examples, utilizing the structural   Pcand via the component of ICMN and PEC, which\n                                                                 is denoted as:representation of the candidate nodes within the\nknowledge graph. Furthermore, an ICMN in policy\n                                         p(π)=( Y fpec(vic, vjc))×( Y ficmn(vq,vic)) (5)network assesses the probability of each candidate\n                                                                                i∈[1,N]              i∈[1,N]\nexample being selected as an in-context example                 i<j\nwithin the constructed knowledge graph.           where N is the number of the set of candidate ex-\n  PEC uses the node embedding in the constructed    amples. Finally, given a user query q, the sequence\nknowledge graph to classify the order between two    of in-context examples is generated from the set\ncorresponding candidate examples. For instance,   of candidate examples according to the policy net-\ntaking pic and pjc as two candidate examples, where   work\ni ̸= j, and their corresponding nodes in the con-\n                                                                        ⃗Pic ∼TS(πθ (ˆo, Pic|G(q))),⃗Pic ∈F(Pcand) (6)structed knowledge graph G are vic and vjc, respec-\ntively. In this formulation, PEC considers the bidi-                                           where F(·) is the operator that constructs a set of\nrectional edges between vic and vjc in E of G(q) —                                                                  all possible order-sensitive subsets from a given set.\n(vic, rij, vjc) and (vjc, rji, vic). The classification ob-                                           The total number of possible order-sensitive subsets\njective is then to compare the pair scores of two                                            from a set with n elements is Pni=1 (n−i)!n!   (Velle-\nedges that are reversed in direction, which is de-\n                                     man and Call, 1995). We use F(Pcand) to denote\nnoted as:\n                                                      the state space in the RL in our formulation. TS(·)\n  fpec(vic, vjc) = max( ps(vic, vjc), ps(vjc, vic) ) (2)    represents the topological sorting method (Bom-\n                                              masani and Cardie, 2020), which is used to obtain\n   If ps(vic, vjc) > ps(vjc, vic), then we predict candi-   the final ordered sequence from the all the pair-\ndate example pic appears earlier than pjc (pic →pjc),   wise edges orders ˆo. If the PEC predicts that the\nor vice versa (pjc →pic). Naturally, the pair score    order between the candidate example pic and pjc is\nfunction ps(·, ·) must be sensitive to the order of    (pic →pjc), TS(·) ensures pic comes before pjc in\nits inputs, and the output represents the probability    the final ordering. We sample the set of in-context\nof the order between two candidate examples. We   examples Pic and pairwise orders ˆo associated with\ndefine the ps function as follows:                    the in-context examples from the policy network,\n                                              and the sequence of in-context examples ⃗Pic is gen-\n                           esin(X(vic)−X(vjc))·w\nps(vic, vjc)=                                         erated by TS(·).\n               esin(X(vic)−X(vjc))·w +esin(X(vjc)−X(vic))·w\n                                                 (3)    4.2.2  Reward Design\nwhere w ∈Rd is the learnable parameter of the   Since we test GRL-Prompt on general text-to-text\nfunction.                                          generation tasks, the reward is designed based on\n  ICMN calculates probability of selecting each    the evaluation of the responses from the LLMs us-\ncandidate examples pic ∈Pcand for the set of in-   ing the in-context examples that we predict. The\ncontext samples Pic based on the node embedding    variation in the responses from black-box LLMs\nof the knowledge graph. For each candidate exam-   poses challenges to the training efficiency and con-\nples pic, the corresponding node in the knowledge   vergence rate of the RL training process. To ad-\ngraph is vic, and the user query q corresponds to    dress these issues, we develop embedding-based\n\n\n                                         5\n\nAlpaca                                                Dolly\n  Model    Method\n                  ROUGE-1   ROUGE-2   ROUGE-L    BLEU    ROUGE-1   ROUGE-2   ROUGE-L    BLEU\n          Random-1      0.36          0.17          0.29          0.11          0.31          0.13          0.22          0.05\n          Random-2      0.35          0.17          0.27          0.10          0.35          0.15          0.25          0.07\n          PromptPG      0.38          0.18          0.30          0.11          0.42          0.21          0.32          0.11\n  GPT-4\n         CoT            0.18          0.07          0.13          0.03          0.18          0.06          0.12          0.02\n            Pillow          0.40          0.19          0.32          0.13          0.41          0.20          0.31          0.11\n       UDR           0.42          0.20          0.33          0.13          0.41          0.21          0.31          0.11\n          Ours       0.44 (+0.02)   0.23 (+0.03)   0.34 (+0.01)   0.15 (+0.02)   0.43 (+0.01)   0.21 (+0.00)   0.32 (+0.00)   0.11 (+0.00)\n          Random-1      0.41          0.20          0.32          0.13          0.38          0.18          0.30          0.10\n          Random-2      0.44          0.22          0.35          0.15          0.38          0.17          0.29          0.10\n          PromptPG      0.46          0.23          0.37          0.16          0.45          0.24          0.35          0.14\n  GPT-3\n         CoT            0.35          0.16          0.27          0.09          0.33          0.14          0.24          0.07\n            Pillow          0.47          0.23          0.38          0.16          0.43          0.23          0.35          0.13\n       UDR           0.46          0.23          0.37          0.16          0.43          0.24          0.35          0.13\n          Ours       0.50 (+0.03)   0.27 (+0.04)   0.39 (+0.01)   0.19 (+0.03)   0.47 (+0.02)   0.26 (+0.02)   0.37 (+0.02)   0.15 (+0.01)\n          Random-1      0.32          0.14          0.25          0.09          0.21          0.08          0.16          0.04\n          Random-2      0.30          0.14          0.23          0.08          0.26          0.09          0.18          0.04\n          PromptPG      0.31          0.14          0.23          0.08          0.38          0.19          0.30          0.10\n LLaMA\n         CoT            0.16          0.07          0.12          0.03          0.16          0.06          0.12          0.02\n            Pillow          0.33          0.16          0.27          0.11          0.36          0.19          0.28          0.10\n       UDR           0.41          0.20          0.33          0.13          0.39          0.19          0.30          0.11\n          Ours       0.45 (+0.04)   0.23 (+0.03)   0.35 (+0.02)   0.15 (+0.02)   0.41 (+0.02)   0.21 (+0.02)   0.31 (+0.01)   0.12 (+0.01)\nTable 1: Results on GRL-Prompt on Alpaca and Dolly. The score differences that indicate better performance than\nthe best baselines are marked with red color. Underlining indicates the value that achieves the best performance\namong all baselines\n                                            ∇E⃗P                                                                                                iic∼πθ(ˆoi,Piic|G(qi))[R(ai, ˆai)]\n                                           =E⃗P                                                                                                 iic∼πθ(ˆoi,Piic|G(qi))∇θlog(πθ(ˆoi,Piic|G(qi)))R(ai,ˆai)\n                                   m\n                             ≈1 X ∇θlog(πθ(ˆoi,Piic|G(qi)))R(ai,ˆai),⃗P iic ∼TS(πθ)                          m\n                                                        i=1                                         (8)\n\n     (a) Training loss on Alpaca.         (b) Training loss on Dolly.  Here, m denotes the size of each batch from our\nFigure 2: Results of training loss on different datasets.                                                      training set of user query Qtr. The learnable pa-\n                                                 rameters in the knowledge graph and the policy\nreward shaping to smooth the reward function and   network are updated iteratively as GRL-Prompt\nstabilize the RL training, which can be expressed    acts as an agent interacting with the LLMs, which\nas:                                                            is demonstrated in Figure 1. We uses the gener-\n                                                    ated sequence of in-context examples and the user\n  R(a, ˆa) = λRm(a, ˆa) + (1 −λ)Re(a, ˆa)   (7)\n                                               query to construct the final prompt when interact-\n                                                  ing with LLMs. The demonstration of the promptRm represents fuzzy textual similarity, and Re de-\n                                                format used in the prompt creator is shown in Ap-notes the cosine embedding similarity based on\n                                               pendix A.2, while the exemplary demonstrationsentence representations. a represent the expected\n                                                   of user query and the in-context example can beoutput. The generated response of LLMs denotes ˆa,\n                                             found in Figure 1. Intuitively, if the response ofwhich uses the predicted sequence of in-context ex-\n                                  LLMs is correct, we update the policy network toamples ⃗Pic and the given user query q to construct\n                                                     increase the probability of generating the same se-the prompt. λ is the hyperparameter for adjust-\n                                             quence of the in-context samples. Conversely, weing the weight of two components in the reward\n                                                  update the policy network to reduce the probabilityfunction.\n                                                    of generating that sequence.\n4.2.3  Training Process\nGiven the set of training user query Qtr = {qi}Mi=1   5  Experiment\nand their corresponding the expected responses\n                                                    5.1  Experimental Setup\n{a1, ..., aM}, our goal is to maximize the expected\nreward of the generated response using the policy    5.1.1  Dataset\n                                We evaluate our approach on two public datasets.network E⃗P                    iic∼πθ(ˆoi,Piic|G(qi))[R(ai, ˆai)]. We opti-\nmize the reward with respect to the parameters of   The following datasets are chosen because they\nthe policy network using the policy gradient algo-   encompass a variety of text-to-text generation tasks\nrithm (Williams, 1992), which is defined as follows:   and contain repetitive QA patterns.\n\n\n                                         6\n\nAlpaca (Taori et al., 2023) dataset consists of     Method  ROUGE-1  ROUGE-2  ROUGE-L  BLEU\n                                                         Ours      0.44         0.23         0.34         0.15\n52,000 instruction-response pairs generated by      -w/o KG   0.38(↓0.06)   0.18(↓0.05)   0.30(↓0.04)   0.12(↓0.03)\nOpenAI’s text-davinci-003 model. These instruc-      -w/o RF   0.37(↓0.07)   0.16(↓0.07)   0.28(↓0.06)   0.10(↓0.05)\ntions are expanded from existing open-source in-   Table 2: Performances of GRL-Prompt in ablation study\n                                               on Alpaca dataset using GPT-4struction datasets, covering a wide range of tasks\nfrom simple question-answering to complex rea-\n                                                             ularities. BLEU, on the other hand, measures the\nsoning and programming issues, thus providing\n                                                accuracy of the generated text by comparing the\nstrong cross-scenario adaptability.\n                                                       similarity between the generated text and the ref-\n  Dolly (Conover et al., 2023) contains 15,000                                                 erence text (Shang et al., 2021). It places greater\nhuman-annotated  instruction-following records                                              emphasis on precision matching than ROUGE.\nacross tasks  like brainstorming,  classification,\nclosed question-answering, text generation, and\n                                                    5.2  Performance Analysis\nsummarization.  Annotators were instructed to\navoid using online data, except Wikipedia for cer-  As shown in Table 1, GRL-Prompt can enhance\ntain categories, and to refrain from using generative    the in-context learning performance on both pub-\nAI in crafting instructions or responses, ensuring     lic datasets regarding Rouge and BLEU met-\nthe dataset’s quality and authenticity.                  rics compared with the above baselines.  Our\n                                          method outperforms all baseline methods across\n5.1.2  Baselines and Configurations                                           two datasets, showing an average improvement of\nWe compare GRL-Prompt with three baselines: (a)   0.10 on ROUGE-1, 0.07 on ROUGE-2, 0.07 on\nRandom selection: We randomly selected one ex-  ROUGE-L, and 0.05 on BLEU. It indicates that\nample and two additional examples from the train-   GRL-Prompt learns the order-sensitive policy from\ning set as the baselines in our experiments.  (b)    the constructed knowledge graph to generate the op-\nPromptPG (Lu et al., 2023), which utilizes policy    timal sequence of in-context examples for prompt\ngradient to learn to select in-context examples from    optimization, effectively reducing the uncertainty\na small amount of training data and then constructs   and noise introduced by the random selection and\nthe corresponding prompt for the test example. (c)   permutation of the in-context examples. It is worth\nCoT prompting (Wei et al., 2022b), which is a    noting that CoT prompting performs worse than\nwidely validated approach of in-context learning.   random selection because the examples of CoT are\n(d) Pillow (Qi et al., 2023), which designs a match-   manually designed for the specific task.\ning network to select in-context examples and fol-    As shown in Figures 2(a) and 2(b), we com-\nlows a reinforcement learning process to update    pare the trend of training loss with UDR, the base-\nits parameters. (e) UDR (Li et al., 2023), which    line that performed the best on nearly all met-\nproposes a bi-encoder architecture to represent can-    rics. Models trained with the GRL-Prompt method\ndidate in-context examples and designs a ranking     (i.e., Ours-GPT-4, Ours-GPT-3, and Ours-LLaMA)\ntraining algorithm to retrieve these examples.        demonstrate a significant reduction in loss, while\n  We randomly selected 1,800 data items: 200 for   converging faster in later epochs, reflecting strong\nRL training, 800 for validation, and 800 for testing.   learning performance and stability.  In contrast,\nExperiments for both the baselines and our GRL-  UDR method (i.e., UDR-GPT-4, UDR-GPT-3, and\nPrompt were repeated three times, and the average   UDR-LLaMA) start with lower loss values, and\nmetric is reported in Table 1. We implemented the    the rate of decrease diminishes as training pro-\nexperiments on a single NVIDIA A100 GPU for    gresses, with fluctuations and consistently higher\nmodel training and evaluation.                          final loss values.  Moreover, we conduct exten-\n                                                      sive experiments to compare the trend of training\n5.1.3  Evaluation Metric                                                       loss with all baselines, which can be found in Ap-\nROUGE is widely adopted to evaluate the infor-   pendix A.3. The fast convergence rate and lower\nmational completeness and coverage of generated    loss observed with the GRL-Prompt method are\ntexts by calculating the recall of generated text to    primarily attributed to the designated embedding-\nreference text. We use ROUGE-1, ROUGE-2 (Lin,   based reward, which effectively incorporates feed-\n2004), and ROUGE-L (Lin and Och, 2004) as the   back from LLMs during the prompt optimization\nevaluation metrics in this experiment to measure    phase. Additionally, the detailed case study can be\nthe quality of the generated text at different gran-   found in Appendix A.4.\n\n\n                                         7\n\n(a) ROUGE-1.                   (b) ROUGE-2.                    (c) ROUGE-L.                     (d) BLEU.\n                                Figure 3: Sensitivity analysis on HGT layer.\n\n\n\n\n\n                   (a) ROUGE-1.                   (b) ROUGE-2.                    (c) ROUGE-L.                     (d) BLEU.\n                                    Figure 4: Sensitivity analysis on λ.\n\n5.3  Ablation Study                                 formance, while start dropping after a certain\n                                                            point. It is observed that our model achievesTo validate the effectiveness of different compo-\n                                                         the best performance when the layer is setnents in GRL-Prompt, we design two variants\n                                                            to be 2.  Intuitively, continuously increas-of GRL-Prompt and compare the performance of\n                                                       ing the number of layers introduces the over-these variants with GRL-Prompt in this section.\n                                                  smoothing problem in graph learning, result-The results of comparison are shown in Table 2.\n                                                       ing in an unexpected decline in performance.GRL-Prompt w/o knowledge graph (KG) excludes\n                                                      2) Impact of λ in the reward: As observed in Fig-the processes of KG construction and learning, and\n                                                       ure 4, the performance of the model achievesdirectly represents the user query and the candi-\n                                                         the best when λ = 0.4, showing an optimaldate examples using the embedding from a pre-\n                                                              trade-off point between the fuzzy textual simi-trained language model. The results show that\n                                                                   larity and the cosine embedding similarity inGRL-Prompt outperforms over GRL-Prompt w/o\n                                                  most cases. It also indicates that over-relianceKG by margins of 0.03-0.06 on four different evalu-\n                                                on fuzzy textual similarity Rm aggravates theation metrics, indicating the correlation of the user\n                                                    performance of GRL-Prompt. However, usingquery and the candidate examples encoded by KG\n                                                           the embedding-based reward that incorporatesenhances the performance of GRL-Prompt. GRL-\n                                                two components of rewards can help improvePrompt w/o reward feedback (RF) replaces the RL-\n                                                          the performance of GRL-Prompt.based prompt optimization with k-NN method for\nselecting the in-context examples, setting the num-   6  Conclusion\nber of examples to form the prompt at 2 (Nie et al.,   In this paper, we propose a LLMs-agnostic prompt\n2022). The variant results in drops of 0.05 to 0.07    optimization framework called GRL-Prompt. This\nacross four evaluation metrics, inferring the neces-   framework first constructs a knowledge graph from\nsity of reward feedback for prompt optimization.   the user instruction and the candidate examples,\nAdditionally, the overall experimental results on    learning with HGT to better encode their correla-\ntwo datasets using three LLMs can be found in    tion. Afterwards, the policy network is designed to\nAppendix A.5.                                        find the optimal sequence of in-context examples\n                                          by incorporating the PEC and ICMN to classify5.4  Sensitivity Analysis\n                                                     the order of examples and select the relevant ones.\nWe study the impact of key parameters in GRL-                                We also conduct comprehensive experiments on\nPrompt, including the HGT layer and the λ in the                                           two widely-used public datasets and compare the\nreward on Dolly dataset, the results are illustrated                                                proposed GRL-Prompt with four baseline methods.\nin Figure 3 and 4, respectively.                                         The results and analysis reveal that the proposed\n  1) Impact of the total number of layer in the HGT:   framework achieves state-of-the-art performance\n    As shown in Figure 3, increasing the number    across different LLMs in the in-context learning\n     of layers generally enhances our model’s per-   paradigm.\n\n\n                                         8\n\nReferences                                  Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\n                                                          Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Al-                                                    Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\n   bert Webson, Colin Raffel, Nihal V. Nayak, Ab-                                                            noising sequence-to-sequence pre-training for natural\n   heesht Sharma, Taewoon Kim, M  Saiful  Bari,                                                        language generation, translation, and comprehension.\n  Thibault Fevry, Zaid Alyafeai, Manan Dey, An-                                                        arXiv preprint arXiv:1910.13461.\n  drea Santilli, Zhiqing Sun, Srulik Ben-David, Can-\n  wen Xu, Gunjan Chhablani, Han Wang, Jason Alan   Chengzhengxu Li, Xiaoming Liu, Yichen Wang, Duyi\n   Fries, Maged S. Al-shaibani, Shanya Sharma, Ur-       Li, Yu Lan, and Chao Shen. 2024a. Dialogue for\n  mish Thakker, Khalid Almubarak, Xiangru Tang,      prompting: a policy-gradient-based discrete prompt\n  Dragomir Radev, Mike Tian-Jian Jiang, and Alexan-      generation for few-shot learning.  In Proc. of the\n   der M. Rush. 2022. Promptsource: An integrated     AAAI Conference on Artificial Intelligence, pages\n  development environment and repository for natural      18481–18489.\n  language prompts. Preprint, arXiv:2202.01279.\n                                              Guozheng Li, Wenjun Ke, Peng Wang, Zijie Xu, Ke Ji,\nRishi Bommasani and Claire Cardie. 2020.  Intrinsic       Jiajun Liu, Ziyu Shang, and Qiqing Luo. 2024b. Un-\n   evaluation of summarization datasets. In Proc. of the      locking instructive in-context learning with tabular\n   International Conference on Empirical Methods in      prompting for relational triple extraction. In Proc. of\n  Natural Language Processing, pages 8075–8096.          International Conference on Computational Linguis-\n                                                                           tics, pages 17131–17143.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\n   Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind    Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu,\n   Neelakantan, Pranav Shyam, Girish Sastry, Amanda     Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng\n   Askell, et al. 2020. Language models are few-shot      Qiu. 2023. Unified demonstration retriever for in-\n   learners. In Proc. of the International Conference       context learning. In Proc. of the Annual Meeting of\n  on Neural Information Processing Systems, pages       the Association for Computational Linguistics, pages\n  1877–1901.                                        4644–4668.\n\nXiaojun Chen et al. 2020. A review: Knowledge rea-   Yihao Li, Ru Zhang, Jianyi Liu, and Gongshen Liu.\n  soning over knowledge graph. Expert Systems with      2024c. An enhanced prompt-based llm reasoning\n   Applications, 141:112948.                          scheme via knowledge graph-integrated collabora-\n                                                                    tion. arXiv preprint arXiv:2402.04978.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\n  Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,   Chin-Yew Lin. 2004. Rouge: A package for automatic\n  Matei Zaharia, and Reynold Xin. 2023. Free dolly:      evaluation of summaries.  In Proc. of the Interna-\n   Introducing the world’s first truly open instruction-       tional Conference on Association for Computational\n  tuned llm. Company Blog of Databricks.                   Linguistics, pages 74–81.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan   Chin-Yew Lin and Franz Josef Och. 2004.  Auto-\n  Wang, Han Guo, Tianmin Shu, Meng Song, Eric P      matic evaluation of machine translation quality using\n  Xing, and Zhiting Hu. 2023. Rlprompt: Optimizing       longest common subsequence and skip-bigram statis-\n   discrete text prompts with reinforcement learning. In        tics. In Proc. of the Annual Meeting of the Associa-\n  Proc. of the Conference on Empirical Methods in       tion for Computational Linguistics, pages 605–612.\n  Natural Language Processing, pages 3369–3391.\n                                                      Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-     Lawrence Carin, and Weizhu Chen. 2021. What\n  ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and     makes good in-context examples for gpt-3? arXiv\n  Zhifang Sui. 2022. A survey on in-context learning.      preprint arXiv:2101.06804.\n  arXiv preprint arXiv:2301.00234.\n                                               Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\nDeepanway Ghosal, Navonil Majumder, Rada Mihalcea,     Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\n  and Soujanya Poria. 2021. Stack: Sentence ordering      and Ashwin Kalyan. 2023. Dynamic prompt learning\n  with temporal commonsense knowledge. In Proc. of       via policy gradient for semi-structured mathematical\n   the International Conference on Empirical Methods       reasoning. pages 1–26.\n   in Natural Language Processing, pages 8676–8686.\n                                              Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou      and Pontus Stenetorp. 2022. Fantastically ordered\n  Sun. 2020. Heterogeneous graph transformer.  In      prompts and where to find them: Overcoming few-\n  Proc. of the Web Conference, pages 2704–2710.           shot prompt order sensitivity. In Proc. of the Annual\n                                                    Meeting of the Association for Computational Lin-\nTouvron Hugo, Lavril Thibaut, Izacard Gautier, Mar-                                                                   guistics, pages 8086–8098.\n   tinet Xavier, Lachaux Marie-Anne, Lacroix Tim-\n   othée, Rozière Baptiste, Goyal Naman, Hambro   Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\n   Eric, Azhar Faisal, Rodriguez Aurelien, Joulin Ar-     Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\n  mand, Grave Edouard, and Lample Guillaume. 2023.      moyer. 2022.  Rethinking the role of demonstra-\n  Llama: Open and efficient foundation language mod-       tions: What makes in-context learning work? arXiv\n   els. arXiv preprint arXiv:2302.13971.                     preprint arXiv:2202.12837.\n\n\n                                         9\n\nFeng Nie, Meixi Chen, Zhirui Zhang, and Xu Cheng.\n  2022. Improving few-shot performance of language\n  models via nearest neighbor calibration.   arXiv\n   preprint arXiv:2212.02216.\n\nZhenting Qi, Xiaoyu Tan, Shaojie Shi, Chao Qu,\n  Yinghui Xu, and Yuan Qi. 2023. Pillow: Enhancing\n   efficient instruction fine-tuning via prompt matching.\n   In Proc. of the Conference on Empirical Methods in\n  Natural Language Processing, pages 471–482.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\n  Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n  Wei Li, and Peter J Liu. 2020. Exploring the lim-\n   its of transfer learning with a unified text-to-text\n   transformer. Journal of machine learning research,\n  21(140):1–67.\n\nXindi Shang, Zehuan Yuan, Anran Wang, and Changhu\n  Wang. 2021. Multimodal video summarization via\n  time-aware transformers.  In Proc. of the Interna-\n   tional Conference on Multimedia, pages 1756–1765.\n\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\n  ment learning: An introduction. MIT press.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n  Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\n  and Tatsunori B Hashimoto. 2023. Stanford alpaca:\n  An instruction-following llama model.\n\nDaniel J Velleman and Gregory S Call. 1995. Permuta-\n   tions and combination locks. Mathematics Magazine,\n  68(4):243–253.\n\nJianing Wang, Qiushi Sun, Xiang Li, and Ming Gao.\n  2023.  Boosting language models reasoning with\n  chain-of-knowledge prompting.   arXiv preprint\n  arXiv:2306.06427.\n\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\n  Guu, Adams Wei Yu, Brian Lester, Nan Du, An-\n  drew M Dai, and Quoc V Le. 2022a.  Finetuned\n  language models are zero-shot learners. In Proc. of\n   the International Conference on Learning Represen-\n   tations, pages 1–46.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n  Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\n   et al. 2022b. Chain-of-thought prompting elicits rea-\n  soning in large language models.  In Proc. of the\n   International Conference on Neural Information Pro-\n   cessing Systems, pages 24824–24837.\n\nRonald J Williams. 1992. Simple statistical gradient-\n   following algorithms for connectionist reinforcement\n   learning. Machine learning, 8:229–256.\n\nXin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi,\n   et al. 2024. In-context learning with retrieved demon-\n   strations for language models: A survey.  arXiv\n   preprint arXiv:2401.11624.\n\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and\n  Lingpeng Kong. 2023. Compositional exemplars for\n   in-context learning. In Proc. of the International Con-\n   ference on Machine Learning, pages 39818–39833.\n\n\n                                         10\n\nA  Appendix                               A.2  Prompt Template\n\n A.1  The Details of Heterogeneous Graph       The prompt template is designed to interact with\n      Transformer (HGT)                           different LLMs using the proposed GRL-Prompt.\n                                                   Figure 1 demonstrates the designated prompt tem-  The HGT can model heterogeneous relations in\n                                                        plate format.  Notably, the sequence of the in- our knowledge graph using a multi-head attention\n                                                     context examples ⃗Pic = [p1ic, ..., pKic ] in the prompt mechanism. It calculates the multi-head attention\n                                                    template is generated by GRL-Prompt, and the user between a given node and its neighbor nodes with\n                                                query q is derived from the test data. In Figure 1, different relations, and then uses the normalized\n                                                     the placeholders (marked as red) denote the in- attention as the weight for the neighbor nodes to\n                                                     context examples and the user query. update the embedding of the given node. Taking\n the node vi as an example, the embedding learning\n process can be formulated as:\n\nQh (vj) = Xl−1 (vj) · Q-Linhϕ(vj)\nKh (vi) = Xl−1 (vi) · K-Linhϕ(vi)\n                                                   µ⟨ϕ(vi),rij,ϕ(vj)⟩\natth(vi,rij,vj)=(Qh (vj)·Θattrij ·Kh(vi)T)·   √\n                                    d\n                                                 (1)\n\n We use l ∈[1, ..., L] to denote the l-th HGT layer\n and L is the total number of layers. Given a target     Figure 1: The demonstration of the prompt template\n node vj, and all its neighbors vi ∈N(vj), we first\n project the target node into the h-th query vector\n Qh (vj) with a type-specific linear transformation\n                              d                       A.3  Supplementary Comparison of The\n matrix Q-Linh ∈Rd× H , where H is the num-                                                     Training Loss Trend with Baselines.\n ber of attention heads. ϕ(·) is the type mapping\n function. We also project the neighbor node vi   We conduct extensive experiments to compare\n into the h-th key vector Kh (vi) on the same di-   the trend of training loss with all baselines, as il-\n mension. Next, we apply a relation-specific weight    lustrated in the Figure 2. The fast convergence\n\n             H                     d × Hd to obtain the h-th attention    rate and lower loss observed with the GRL-Prompt matrix Θattrij ∈R              √                                           method are primarily attributed to the designated\n head atth (vi, rij, vj) and  d acts a scaling fac-\n                                               embedding-based reward, which effectively incor-\n  tor. Moreover, since not all the relations contribute\n                                                    porates feedback from LLMs during the prompt\n equally to the target nodes, a learnable vector µ as-\n                                                    optimization phase.\n sociated with the relation triplet ⟨ϕ(vi), rij, ϕ(vj)⟩\n acts as a scaling factor for this triplet relationship.\n                                             A.4  Case Study\n Finally, the weight of the neighbor node wh (vi)\n  is calculated by softmax normalization. The atten-    Figures 3 to 8 show the comparative case study\n tive aggregation of different heads across neighbor    of GRL-Prompt and other baseline methods, i.e.,\n nodes with various relations for updating node em-   Random-1, Random-2, CoT, and PromptPG, Pil-\n bedding of vj is defined as:                         low, UDR respectively. It is evident that prompts\n                                                 optimized with GRL-Prompt result in more rea-\n           1\n Xl(vj)=   X MLP(   wh(vi)·Kh (vi))  sonable responses from LLMs compared to other\n         |N(vj)|                  vi∈N(vj)     h∈[1,H]                baseline methods. For example, in Figure 3, the\n                                                  generated prompt from GRL-Prompt (marked in                                                  (2)\n                                                       blue) provides more instructive guidance for LLMs.\n where   represents concatenation. We first aggre-   Random-1, on the other hand, clearly gives an over-\n gate all embedding of neighbor nodes, and then    simplified prompt, leading to an undesired response\n concatenate all H heads. After that, the node em-   from the LLMs. Similar results can also be found\n bedding of vj is updated by a shallow multi-layer    in Random-2, CoT, and PromptPG, Pillow, UDR\n perceptron (MLP).                               (from Figures 4 to 8).\n\n\n                                          11\n\nAlpaca                                                Dolly\n  Model   Method\n                      Rouge-1     Rouge-2     Rouge-L     BLEU      Rouge-1     Rouge-2     Rouge-L     BLEU\n          Ours         0.44          0.23          0.34          0.15          0.43          0.21          0.32          0.11\n  GPT-4    -w/o KG  0.38(↓0.06)  0.18(↓0.05)  0.30(↓0.04)  0.12(↓0.03)   0.41(↓0.02)  0.20(↓0.01)  0.31(↓0.01)  0.08(↓0.03)\n           -w/o RF   0.37(↓0.07)  0.16(↓0.07)  0.28(↓0.06)  0.10(↓0.05)   0.37(↓0.06)  0.17(↓0.04)  0.27(↓0.05)  0.08(↓0.03)\n          Ours         0.50          0.27          0.39          0.19          0.47          0.26          0.37          0.15\n  GPT-3    -w/o KG  0.46(↓0.04)  0.23(↓0.04)  0.37(↓0.02)  0.16(↓0.03)   0.44(↓0.03)  0.24(↓0.02)  0.35(↓0.02)  0.14(↓0.01)\n           -w/o RF   0.41(↓0.09)  0.19(↓0.08)  0.32(↓0.07)  0.12(↓0.07)   0.40(↓0.07)  0.20(↓0.06)  0.32(↓0.05)  0.11(↓0.04)\n          Ours         0.45          0.23          0.35          0.15          0.41          0.21          0.31          0.12\n LLaMA  -w/o KG  0.35(↓0.10)  0.14(↓0.09)  0.26(↓0.09)  0.08(↓0.07)   0.39(↓0.02)  0.18(↓0.03)  0.30(↓0.01)  0.11(↓0.01)\n           -w/o RF   0.32(↓0.13)  0.12(↓0.11)  0.24(↓0.11)  0.07(↓0.08)   0.36(↓0.05)  0.15(↓0.06)  0.28(↓0.05)  0.09(↓0.02)\n\n                   Table 1: Performances of GRL-Prompt in supplementary ablation study.\n\n\n\n\n\n           (a) GPT-4 on Alpaca                    (b) GPT-3 on Alpaca                    (c) LLaMA on Alpaca\n\n\n\n\n\n           (d) GPT-4 on Dolly                       (e) GPT-3 on Dolly                         (f) LLaMA on Dolly\n\n                  Figure 2: The trend of training loss across different LLMs on two datasets.\n\n\nA.5  Supplementary Ablation Study            A.6  Limitations\n\n                                          The reward design in GRL-Prompt is based on au-\n To further validate the effectiveness of different    tomatic evaluation scores, which may not be suffi-\ncomponents in GRL-Prompt, we compare the two    cient for providing human feedback to the RL agent.\nvariants of GRL-Prompt with GRL-Prompt across   Such scores focus on the contextual discrepancy be-\ndifferent LLMs on two datasets. The overall re-   tween the generated and expected responses, which\nsults of the comparison are shown in Table 1. The   may not align with the results of human evaluations.\nresults show that GRL-Prompt outperforms both    In the future, we plan to integrate human feedback\nGRL-Prompt w/o KG and GRL-Prompt w/o RF    into the GRL-Prompt framework and investigate\nacross all different LLMs on two datasets, demon-   whether incorporating context-aware results and\nstrating an average improvement of 0.04 and 0.07,   feedback can help enhance LLMs performance on\nrespectively.  It highlights the significant contri-    different tasks.\nbutions of two components in GRL-Prompt that\nenhance the performance of LLMs.\n\n\n                                         12\n\nFigure 3: Comparative case study of GRL-Prompt and Random-1. The text highlighted in green indicates the\nkeywords from the test example. In contrast, the text marked in blue represents the keywords in the in-context\nexamples selected by GRL-Prompt, while the text highlighted in yellow corresponds to the keywords from the\nin-context examples chosen by Random-1.\n\n\n\n\n\n                                         13\n\nFigure 4: Comparative case study of GRL-Prompt and Random-2. The text highlighted in green indicates the\nkeywords from the test example. In contrast, the text marked in blue represents the keywords in the in-context\nexamples selected by GRL-Prompt, while the text highlighted in yellow corresponds to the keywords from the\nin-context examples chosen by Random-2.\n\n\n\n\n\n                                         14\n\nFigure 5: Comparative case study of GRL-Prompt and CoT. The text highlighted in green indicates the keywords\nfrom the test example. In contrast, the text marked in blue represents the keywords in the in-context examples\nselected by GRL-Prompt, while the text highlighted in yellow corresponds to the keywords from the in-context\nexamples chosen by CoT.\n\n\n\n\n\n                                         15\n\nFigure 6: Comparative case study of GRL-Prompt and PromptPG. The text highlighted in green indicates the\nkeywords from the test example. In contrast, the text marked in blue represents the keywords in the in-context\nexamples selected by GRL-Prompt, while the text highlighted in yellow corresponds to the keywords from the\nin-context examples chosen by PromptPG.\n\n\n\n\n\n                                         16\n\nFigure 7: Comparative case study of GRL-Prompt and Pillow. The text highlighted in green indicates the keywords\nfrom the test example. In contrast, the text marked in blue represents the keywords in the in-context examples\nselected by GRL-Prompt, while the text highlighted in yellow corresponds to the keywords from the in-context\nexamples chosen by Pillow.\n\n\n\n\n\n                                         17\n\nFigure 8: Comparative case study of GRL-Prompt and UDR. The text highlighted in green indicates the keywords\nfrom the test example. In contrast, the text marked in blue represents the keywords in the in-context examples\nselected by GRL-Prompt, while the text highlighted in yellow corresponds to the keywords from the in-context\nexamples chosen by UDR.\n\n\n\n\n\n                                         18",
"headers": [
"arXiv:2411.14479v1  [cs.CL]  19 Nov 2024",
"Reinforcement Learning",
"GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via"
],
"tables": [
"|Alpaca<br>Model Method<br>ROUGE-1 ROUGE-2 ROUGE-L BLEU|Dolly|\n|---|---|\n|Alpaca<br>Model<br>Method<br>ROUGE-1<br>ROUGE-2<br>ROUGE-L<br>BLEU|ROUGE-1<br>ROUGE-2<br>ROUGE-L<br>BLEU|\n|Random-1<br>0.36<br>0.17<br>0.29<br>0.11<br>Random-2<br>0.35<br>0.17<br>0.27<br>0.10<br>PromptPG<br>0.38<br>0.18<br>0.30<br>0.11<br>CoT<br>0.18<br>0.07<br>0.13<br>0.03<br>Pillow<br>0.40<br>0.19<br>0.32<br>0.13<br>UDR<br>0.42<br>0.20<br>0.33<br>0.13<br>GPT-4<br>**Ours**<br>0.44 (+0.02)<br>0.23 (+0.03)<br>0.34 (+0.01)<br>0.15 (+0.02)|0.31<br>0.13<br>0.22<br>0.05<br>0.35<br>0.15<br>0.25<br>0.07<br>0.42<br>0.21<br>0.32<br>0.11<br>0.18<br>0.06<br>0.12<br>0.02<br>0.41<br>0.20<br>0.31<br>0.11<br>0.41<br>0.21<br>0.31<br>0.11|\n|Random-1<br>0.36<br>0.17<br>0.29<br>0.11<br>Random-2<br>0.35<br>0.17<br>0.27<br>0.10<br>PromptPG<br>0.38<br>0.18<br>0.30<br>0.11<br>CoT<br>0.18<br>0.07<br>0.13<br>0.03<br>Pillow<br>0.40<br>0.19<br>0.32<br>0.13<br>UDR<br>0.42<br>0.20<br>0.33<br>0.13<br>GPT-4<br>**Ours**<br>0.44 (+0.02)<br>0.23 (+0.03)<br>0.34 (+0.01)<br>0.15 (+0.02)|0.43 (+0.01)<br>0.21 (+0.00)<br>0.32 (+0.00)<br>0.11 (+0.00)|\n|Random-1<br>0.41<br>0.20<br>0.32<br>0.13<br>Random-2<br>0.44<br>0.22<br>0.35<br>0.15<br>PromptPG<br>0.46<br>0.23<br>0.37<br>0.16<br>CoT<br>0.35<br>0.16<br>0.27<br>0.09<br>Pillow<br>0.47<br>0.23<br>0.38<br>0.16<br>UDR<br>0.46<br>0.23<br>0.37<br>0.16<br>GPT-3<br>**Ours**<br>0.50 (+0.03)<br>0.27 (+0.04)<br>0.39 (+0.01)<br>0.19 (+0.03)|0.38<br>0.18<br>0.30<br>0.10<br>0.38<br>0.17<br>0.29<br>0.10<br>0.45<br>0.24<br>0.35<br>0.14<br>0.33<br>0.14<br>0.24<br>0.07<br>0.43<br>0.23<br>0.35<br>0.13<br>0.43<br>0.24<br>0.35<br>0.13|\n|Random-1<br>0.41<br>0.20<br>0.32<br>0.13<br>Random-2<br>0.44<br>0.22<br>0.35<br>0.15<br>PromptPG<br>0.46<br>0.23<br>0.37<br>0.16<br>CoT<br>0.35<br>0.16<br>0.27<br>0.09<br>Pillow<br>0.47<br>0.23<br>0.38<br>0.16<br>UDR<br>0.46<br>0.23<br>0.37<br>0.16<br>GPT-3<br>**Ours**<br>0.50 (+0.03)<br>0.27 (+0.04)<br>0.39 (+0.01)<br>0.19 (+0.03)|0.47 (+0.02)<br>0.26 (+0.02)<br>0.37 (+0.02)<br>0.15 (+0.01)|\n|Random-1<br>0.32<br>0.14<br>0.25<br>0.09<br>Random-2<br>0.30<br>0.14<br>0.23<br>0.08<br>PromptPG<br>0.31<br>0.14<br>0.23<br>0.08<br>CoT<br>0.16<br>0.07<br>0.12<br>0.03<br>Pillow<br>0.33<br>0.16<br>0.27<br>0.11<br>UDR<br>0.41<br>0.20<br>0.33<br>0.13<br>LLaMA<br>**Ours**<br>0.45 (+0.04)<br>0.23 (+0.03)<br>0.35 (+0.02)<br>0.15 (+0.02)|0.21<br>0.08<br>0.16<br>0.04<br>0.26<br>0.09<br>0.18<br>0.04<br>0.38<br>0.19<br>0.30<br>0.10<br>0.16<br>0.06<br>0.12<br>0.02<br>0.36<br>0.19<br>0.28<br>0.10<br>0.39<br>0.19<br>0.30<br>0.11|\n|Random-1<br>0.32<br>0.14<br>0.25<br>0.09<br>Random-2<br>0.30<br>0.14<br>0.23<br>0.08<br>PromptPG<br>0.31<br>0.14<br>0.23<br>0.08<br>CoT<br>0.16<br>0.07<br>0.12<br>0.03<br>Pillow<br>0.33<br>0.16<br>0.27<br>0.11<br>UDR<br>0.41<br>0.20<br>0.33<br>0.13<br>LLaMA<br>**Ours**<br>0.45 (+0.04)<br>0.23 (+0.03)<br>0.35 (+0.02)<br>0.15 (+0.02)|0.41 (+0.02)<br>0.21 (+0.02)<br>0.31 (+0.01)<br>0.12 (+0.01)|",
"|Method|ROUGE-1 ROUGE-2 ROUGE-L BLEU|\n|---|---|\n|**Ours**|0.44<br>0.23<br>0.34<br>0.15|\n|**-w/o KG**|0.38(_↓_0.06)<br>0.18(_↓_0.05)<br>0.30(_↓_0.04)<br>0.12(_↓_0.03)|\n|**-w/o RF**|0.37(_↓_0.07)<br>0.16(_↓_0.07)<br>0.28(_↓_0.06)<br>0.10(_↓_0.05)|",
"|Alpaca<br>Model Method<br>Rouge-1 Rouge-2 Rouge-L BLEU|Dolly|\n|---|---|\n|Model<br>Method<br>Alpaca<br>Rouge-1<br>Rouge-2<br>Rouge-L<br>BLEU|Rouge-1<br>Rouge-2<br>Rouge-L<br>BLEU|\n|GPT-4<br>**Ours**<br>0.44<br>0.23<br>0.34<br>0.15<br>-w/o KG<br>0.38(_↓_0_._06)<br>0.18(_↓_0_._05)<br>0.30(_↓_0_._04)<br>0.12(_↓_0_._03)<br>-w/o RF<br>0.37(_↓_0_._07)<br>0.16(_↓_0_._07)<br>0.28(_↓_0_._06)<br>0.10(_↓_0_._05)|0.43<br>0.21<br>0.32<br>0.11|\n|GPT-4<br>**Ours**<br>0.44<br>0.23<br>0.34<br>0.15<br>-w/o KG<br>0.38(_↓_0_._06)<br>0.18(_↓_0_._05)<br>0.30(_↓_0_._04)<br>0.12(_↓_0_._03)<br>-w/o RF<br>0.37(_↓_0_._07)<br>0.16(_↓_0_._07)<br>0.28(_↓_0_._06)<br>0.10(_↓_0_._05)|0.41(_↓_0_._02)<br>0.20(_↓_0_._01)<br>0.31(_↓_0_._01)<br>0.08(_↓_0_._03)<br>0.37(_↓_0_._06)<br>0.17(_↓_0_._04)<br>0.27(_↓_0_._05)<br>0.08(_↓_0_._03)|\n|GPT-3<br>**Ours**<br>0.50<br>0.27<br>0.39<br>0.19<br>-w/o KG<br>0.46(_↓_0_._04)<br>0.23(_↓_0_._04)<br>0.37(_↓_0_._02)<br>0.16(_↓_0_._03)<br>-w/o RF<br>0.41(_↓_0_._09)<br>0.19(_↓_0_._08)<br>0.32(_↓_0_._07)<br>0.12(_↓_0_._07)|0.47<br>0.26<br>0.37<br>0.15|\n|GPT-3<br>**Ours**<br>0.50<br>0.27<br>0.39<br>0.19<br>-w/o KG<br>0.46(_↓_0_._04)<br>0.23(_↓_0_._04)<br>0.37(_↓_0_._02)<br>0.16(_↓_0_._03)<br>-w/o RF<br>0.41(_↓_0_._09)<br>0.19(_↓_0_._08)<br>0.32(_↓_0_._07)<br>0.12(_↓_0_._07)|0.44(_↓_0_._03)<br>0.24(_↓_0_._02)<br>0.35(_↓_0_._02)<br>0.14(_↓_0_._01)<br>0.40(_↓_0_._07)<br>0.20(_↓_0_._06)<br>0.32(_↓_0_._05)<br>0.11(_↓_0_._04)|\n|LLaMA<br>**Ours**<br>0.45<br>0.23<br>0.35<br>0.15<br>-w/o KG<br>0.35(_↓_0_._10)<br>0.14(_↓_0_._09)<br>0.26(_↓_0_._09)<br>0.08(_↓_0_._07)<br>-w/o RF<br>0.32(_↓_0_._13)<br>0.12(_↓_0_._11)<br>0.24(_↓_0_._11)<br>0.07(_↓_0_._08)|0.41<br>0.21<br>0.31<br>0.12|\n|LLaMA<br>**Ours**<br>0.45<br>0.23<br>0.35<br>0.15<br>-w/o KG<br>0.35(_↓_0_._10)<br>0.14(_↓_0_._09)<br>0.26(_↓_0_._09)<br>0.08(_↓_0_._07)<br>-w/o RF<br>0.32(_↓_0_._13)<br>0.12(_↓_0_._11)<br>0.24(_↓_0_._11)<br>0.07(_↓_0_._08)|0.39(_↓_0_._02)<br>0.18(_↓_0_._03)<br>0.30(_↓_0_._01)<br>0.11(_↓_0_._01)<br>0.36(_↓_0_._05)<br>0.15(_↓_0_._06)<br>0.28(_↓_0_._05)<br>0.09(_↓_0_._02)|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2411.14479v1.pdf"
}