{
"text": "GAAPO: Genetic Algorithmic Applied to Prompt Optimization\n\n                Xavier Sécheresse1, Jacques-Yves Guilbert–Ly1, and Antoine Villedieu de Torcy1\n\n                                                    1Biolevate\n\n\n                                                 April 17, 2025\n\n\n\n           Keywords: Artificial Intelligence, Prompt engineering, Genetic algorithmic, LLM, Prompt optimization.\n2025\n      Abstract\n\n         Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, with their perfor-Apr\n        mance heavily dependent on the quality of input prompts [1] [2]. While prompt engineering has proven effective, it\n          typically relies on manual adjustments, making it time-consuming and potentially suboptimal. This paper introduces16\n      GAAPO (Genetic Algorithm Applied to Prompt Optimization), a novel hybrid optimization framework that lever-\n         ages genetic algorithm [3] principles to evolve prompts through successive generations. Unlike traditional genetic\n         approaches that rely solely on mutation and crossover operations, GAAPO integrates multiple specialized prompt\n         generation strategies within its evolutionary framework. Through extensive experimentation on diverse datasets\n          including ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important point for the future development\n           of automatic prompt optimization methods: importance of the tradeoff between the population size and the number[cs.NE]\n           of generations, effect of selection methods on stability results, capacity of different LLMs and especially reasoning\n         models to be able to automatically generate prompts from similar queries... Furthermore, we provide insights into\n         the relative effectiveness of different prompt generation strategies and their evolution across optimization phases.\n        These findings contribute to both the theoretical understanding of prompt optimization and practical applications\n          in improving LLM performance.\n\n     1  Introduction\n\n         Large Language Models (LLMs) have gained significant attention following the public release of generative AI as-\n           sistants such as ChatGPT (2022) and Claude (2023). A critical factor in maximizing these models’ effectiveness\n            lies in the quality of input prompts - the instructions that guide LLMs toward generating relevant outputs. While\n         the impact of prompting on LLM performance has been well-documented through various benchmarks [1], the pro-\n          cess typically relies on manual adjustments, making it both time-consuming and susceptible to human error. This\n          highlights the necessity for developing automated methods to fully harness the capabilities of modern LLMs.arXiv:2504.07157v3\n             In response to this need, several machine learning approaches have been developed to automate prompt opti-\n          mization. Reinforcement learning has been employed to optimize evaluation costs and computational efficiency [4]\n             [5], while in-context learning focuses on improving prompt performance through example-based learning [6]. Re-\n          gression techniques have been explored to establish direct relationships between prompt characteristics and model\n         performance [7]. These diverse approaches aim to streamline the prompting process, reducing the reliance on manual\n          intervention while addressing different aspects of prompt optimization.\n            Recent research has shown that smaller language models can achieve performance comparable to larger LLMs\n         through various optimization techniques such as distillation [8] and prompt engineering [1]. While traditional ap-\n         proaches like distillation modify model weights, prompt optimization offers a more flexible alternative: it enhances\n        model performance without altering the underlying architecture. This approach is particularly valuable as it can be\n          applied to any LLM regardless of size or architecture, providing a generalizable framework for task-specific optimiza-\n          tion while maintaining cost-effectiveness.\n             In this work, we introduce GAAPO (Genetic Algorithmic Applied to Prompt Optimization), an algorithm that\n          integrates different prompt generation strategies into a hybrid prompt optimizer. This innovative approach capitalizes\n        on the strengths of diverse techniques, ensuring optimal performance. Crucially, it maintains a detailed record of the\n\n\n\n                                                           1\n\nevolution of prompting strategies, which is essential for tracking progress and making informed adjustments. The\ndesign of this optimizer prioritizes adaptability, ensuring it can seamlessly incorporate future advancements in the\nfield, thereby remaining relevant and effective as new techniques and models emerge.\n\n2  Related works\n\n2.1  Prompt Engineering\n\nPrompt engineering is a critical aspect of working with large language models (LLMs), as it involves crafting inputs\nthat guide the model to produce desired outputs.  It has been demonstrated that this step is critical to enhance\nLLM capabilities [1]. However, this process requires a deep understanding of both the model’s capabilities and the\nspecific task at hand. Traditionally, prompt engineering has been a manual process [9], relying on human intuition\nand expertise to iteratively refine prompts for optimal performance.\n\n2.2  Automatic Prompt Engineering\n\nThe limitations of manual prompt engineering have led to the development of automated approaches. These methods\nutilize machine learning algorithms to automatically generate and optimize prompts, reducing the need for manual\nintervention and democratizing access to advanced language processing capabilities. To standardize these develop-\nments, frameworks like DSPy [10] have emerged, providing a systematic approach to develop and evaluate automatic\nprompt optimization methods. Various approaches have been explored in this field, from \"gradient-oriented\" prompt\nevolution [11] to more sophisticated optimization techniques. Notable advances include APO [12], which introduced\ngradient-based prompt optimization, while OPRO [4] demonstrated the effectiveness of using LLMs themselves as\noptimizers. These automated methods can efficiently explore vast prompt spaces, identifying optimal prompts that\nmaximize model performance on specific tasks. This systematic approach has become increasingly important as LLMs\nare deployed in diverse applications, where task-specific prompt optimization can significantly impact performance.\n\n\n\n\n\n                    Figure 1: Schema of the general automatic prompt optimization process\n\n   Most prompt optimization techniques follow the same architecture, described in the figure 1.\n\n2.3  Genetic algorithm\n\nGenetic algorithms (GAs) are a class of optimization techniques inspired by the principles of natural selection and\ngenetics [3]. By mimicking the evolutionary process, GAs have been successfully applied to various machine learning\nand artificial intelligence tasks [13]. They are particularly effective in solving complex optimization problems where\ntraditional methods struggle, thanks to their ability to explore large and poorly understood search spaces. The\nGA process begins with a randomly initialized population of candidate solutions, each evaluated based on a fitness\n\n\n\n                                                  2\n\nfunction that measures its effectiveness in solving the problem. The best-performing individuals are selected for\nreproduction using evolutionary operators such as crossover, which recombines elements from two solutions, and\nmutation, which introduces random modifications to enhance diversity.\n   Over the years, GAs have been widely adopted in nearly every field of machine learning, including feature selection\n[14], neural network optimization [15], hyperparameter tuning [16], clustering [17], and reinforcement learning [18].\nFor example, GAs have been used to optimize neural network architectures by evolving network topologies and\nweight configurations, improving model performance and efficiency [15]. In reinforcement learning, they have been\nleveraged to evolve policies and reward functions, enabling agents to learn complex behaviors [18].  Additionally,\nhybrid approaches combining GAs with local search techniques have been developed to improve convergence speed\nand accuracy [19]. Parallel implementations of GAs further enhance their scalability, allowing them to tackle large-\nscale optimization problems efficiently [20]. The adaptability and robustness of genetic algorithms make them a\npowerful tool for advancing machine learning methodologies and solving a diverse range of computational challenges.\n\n2.4  Application to prompt optimization\n\nGenetic algorithms have been previously explored in prompt optimization, though their implementations often focus\non specific aspects of the prompt space. EvoPrompt [21] introduces a basic evolutionary approach where new prompts\nare primarily generated through crossover operations, combining successful segments from parent prompts followed\nby linguistic refinement. This method, while effective, primarily explores structural variations within a limited scope\nof the prompt space. A more sophisticated approach is demonstrated by PhaseEvo [22], which implements a two-\nphase evolutionary strategy. The first phase employs global mutations to identify promising regions in the prompt\nspace, effectively searching for potential global optima. The second phase then applies more focused optimizations\nthrough semantic mutations and gradient-based refinements.\n   However, despite their innovative contributions, these approaches operate within relatively narrow paradigms\nof prompt generation and modification. While they effectively handle structural and semantic modifications, they\ndon’t fully explore the broader spectrum of prompt transformation strategies. Moreover, they lack a comprehensive\nframework that could integrate existing prompt optimization techniques or adapt to emerging methodologies in the\nfield. This limitation in extensibility and modularity restricts their ability to evolve alongside new developments in\nprompt engineering.\n\n3  Data\n\nETHOS dataset\n   The ETHOS (Ethics in Text - Hate and Offensive Speech) multilabel dataset [23] is a specialized benchmark de-\nsigned to evaluate hate speech recognition capabilities in language models. It consists of 443 carefully annotated text\nsamples categorized across eight distinct dimensions of hate speech and offensive content, including race, gender, and\nviolence. Each sample in the dataset is labeled to indicate the presence or absence of specific types of harmful con-\ntent, enabling fine-grained evaluation of model performance in detecting various forms of hate speech. The dataset’s\nmulti-label structure allows for comprehensive assessment of language models’ ability to identify intersecting forms\nof discriminatory or offensive content, making it particularly valuable for evaluating ethical content moderation ca-\npabilities. The balanced distribution across different categories of hate speech ensures robust evaluation across the\nspectrum of harmful content typically encountered in real-world applications.\n\nComplementary datasets\n   To assess performances of our approach on a wide range of tasks, we evaluated our model on 3 other datasets,\nalongside with ETHOS-multilabel:\n\n   • The MMLU-Pro (Massive Multitask Language Understanding Professional)[24] dataset extends the famous\n   MMLU [25] dataset by complexifying it to a professional level, with our focus on two key subcategories. The\n     Engineering subcategory evaluates technical understanding across various engineering disciplines, testing knowl-\n     edge of fundamental principles, technical specifications, and complex problem-solving approaches encountered\n      in professional practice. The Business subcategory assesses comprehension of management principles, cor-\n     porate strategy, financial decision-making, and organizational behavior through practical business scenarios.\n     Repartition of MMLU-Pro dataset in subcategories is detailed in annex 6.\n\n   • GPQA (General Physics Question Answering)[26] presents a specialized evaluation framework for physics un-\n     derstanding through multiple-choice questions. The dataset covers a broad spectrum of physics topics, from\n\n\n\n                                                  3\n\nmechanics to quantum physics, requiring both theoretical knowledge and practical problem-solving abilities.\n     Questions are designed to test not only recall of physical principles but also their application in solving concrete\n     problems, making it an effective benchmark for assessing scientific reasoning capabilities in LLMs.\n\n4  Methods\n\n4.1  Models\n\n4.1.1 GAAPO: Genetic Algorithmic Applied to Prompt Optimization\n\nGAAPO (Genetic Algorithm Applied to Prompt Optimization) follows the principles of genetic algorithms to evolve\nand optimize prompts through successive generations. The algorithm combines multiple prompt optimization strate-\ngies to explore a broader prompt space than previous methods, leveraging the strengths of each approach while\nmaintaining the evolutionary nature of genetic algorithms. The optimization pipeline, inspired by existing works\n[12][27] and described in the figure 2, operates in three distinct phases during each generation:\n   • Generation phase: New prompt candidates are created using multiple strategies, with each strategy operating\n     on a subset of high-performing prompts from the previous generations.\n   • Evaluation phase: The newly generated population is evaluated on the validation set using either exhaustive\n      evaluation or a bandit-based approach to optimize computational resources.\n   • Selection phase: Top-performing prompts are selected based on their evaluation scores to serve as parents for\n     the next generation, ensuring best performers are used as parents at all time for the future generations.\n\n\n\n\n\n                          Figure 2: Description of the GAAPO optimization process.\n\n   This iterative process combines the exploration capabilities of genetic algorithms with specialized prompt opti-\nmization techniques, enabling efficient navigation of the prompt space while maintaining diversity in the population.\n\nPrompt generation\n   The genetic algorithm framework incorporates multiple prompt generation methods, each implementing distinct\noptimization strategies as detailed in sections 4.1.2 and 4.1.3. These methods, summarized in Table 1, represent\ndiverse approaches to prompt optimization, each with its own strengths and limitations. The hybrid nature of\nGAAPO leverages this diversity by combining these complementary strategies within a single optimization framework.\nThis integration enables the algorithm to capitalize on the advantages of each method while mitigating their individual\nlimitations through iterative application of varied optimization approaches. The synergistic combination of these\nmethods allows for more comprehensive exploration of the prompt space than would be possible with any single\nstrategy. Examples of generated prompts on a real-world task are presented in annex ??.\n   To streamline the optimization process, we unified the selection and evaluation phases across all different op-\ntimization methods into a single coherent framework.  This architectural decision maintains only the generative\n\n\n                                                  4\n\n(expansion) phases of these algorithms, integrating them as candidate generation strategies within GAAPO’s evolu-\ntionary cycle. This simplification allows for consistent evaluation metrics and selection criteria across all generated\ncandidates while preserving the unique prompt generation characteristics of each method.\n   Compared to already existing GA-related prompt optimization methods, this framework allows a wider explo-\nration of the prompt space, leveraging advantages of all implemented methods and not focusing on single-algorithm\nlocal improvements.\n\nEvaluation\n   To meaningfully compare new prompts, we evaluate them on a subset of the task we have at hand and compare\ntheir accuracy (in the current setting).\n   Several strategies has been implemented for the evaluation process to rank the individuals in each generation:\n   • Complete evaluation: Run a standard evaluation of each prompt on the evaluation set and rank new prompts\n     according to their accuracy.\n   • Successive halving (SH) process [28]: prompt accuracies are compared on a subset of the dataset, the top-\n     performing half of the models is retained, and the survivors are evaluated on a new subset. This process is\n     repeated iteratively until only a few models remain. This approach allows to drastically reduce the number\n      of API calls but increases the risk to remove interesting prompts from the evaluation very early due to the\n      disparity of evaluations results on subsets.\n   • Bandit selection algorithm[29]: run a multi-arm selection bandit algorithm. Evaluate subsets of the prompt\n     population on batches of data, and apply the UCB-E reward model [30] to identify the best arms. Note that\n      this method was also used in the original paper of APO [12].\n\nSelection\n   The selection step used to generate the new parents at each generation is quite simple. We simply chose, among\nall the prompts which have been evaluated, the best according to their evaluation score.\n\n\n4.1.2  Generation methods: \"forced\" evolutions\n\nThe first categories of generators were directly inspired from standard prompt optimization models, described below.\nThis methods directly use previous prompts to generate new ones, by using the errors made (APO) or trying to\nexpand a prompt trajectory (OPRO), hence the \"forced\" evolution.\n\nOPRO: Optimization by PROmpting\n\n  OPRO (Optimization by PROmpting) [4] is an iterative prompt optimization algorithm that leverages large\nlanguage models to generate and refine prompts through a trajectory-based optimization approach. The algorithm\nmaintains a trajectory of the top-performing prompts, ranked by their performance scores, and uses this historical\ninformation to guide the generation of new candidates. During training, OPRO employs a stochastic dropout mech-\nanism on the trajectory of best-performing prompts to maintain diversity and prevent convergence to local optima.\nThe filtered trajectory then serves as input for the generation of new candidate prompts, which are subsequently\nevaluated on the current set. This evaluation process updates the trajectory, maintaining a dynamic optimization\npath.\n\nProTeGi: Prompt Optimization with Textual Gradients\n\n   The Automatic Prompt Optimizer (APO/ProTeGi) [12] is an iterative algorithm designed to automatically opti-\nmize prompts for Large Language Models through a three-phase process described in figure 3. The expansion phase\nbegins by evaluating existing prompts to identify errors, which are then grouped for focused analysis. The algorithm\ngenerates improvement \"gradients\" from these errors and creates new candidate prompts. In the selection phase,\nAPO employs multi-armed bandit strategies (such as epsilon-greedy [31] or Bayesian UCB [30]) to efficiently identify\npromising candidates. This approach balances the exploration of new prompt variations with the exploitation of\nproven patterns, evaluating candidates on small batches for computational efficiency. The validation phase assesses\nselected candidates on a separate validation set ensuring the robustness of the optimized prompts. Key features\ninclude parallel processing, adaptive error analysis, and gradient-guided refinement.\n\n\n4.1.3  Generation methods: random evolutions\n\nTo complement the \"forced\" evolution optimization strategies, we developed three additional prompt generation\nmethods that were incorporated into GAAPO’s framework.  These supplementary approaches expand the algo-\n\n\n                                                  5\n\nFigure 3: Description of the APO optimisation process, which served as a basis for GAAPO.\n\n\nrithm’s capacity to explore diverse regions of the prompt space. They all use already existing prompts to generate\nnew ones by randomly modifying them.\n\nRandom Mutator: Prompt random mutation\n\n   The Random Mutator serves as a mutation operator within a genetic algorithm framework, designed to explore\nthe vast prompt space through controlled random modifications. This approach draws inspiration from biological\nmutations in genetic evolution, where random changes can lead to beneficial adaptations. The mutation process\noperates by randomly selecting from eight distinct mutation strategies, each targeting different aspects of prompt\nengineering:\n   • instruction expansion: adds detailed guidelines,\n   • expert persona injection: introduces specialized viewpoints,\n   • structural variation: modifies the prompt’s architecture,\n   • constraint addition: introduces new boundaries,\n   • creative backstory: weaves narrative elements,\n   • task decomposition: breaks down complex instructions,\n   • concise optimization: streamlines the content,\n   • role assignment: establishes specific model behaviors.\n   Each mutation creates a new variant of the original prompt (examples are presented in annex 6), potentially\ndiscovering more effective formulations. Like genetic mutations in nature, these modifications can range from subtle\nadjustments to significant transformations, allowing for both local and global exploration of the prompt space. This\nrandom but structured approach enables the discovery of novel prompt variations that might not be obvious through\ndeterministic methods.\n\nCrossover: random prompt merging\n\n   Crossover operations in prompt engineering also draw inspiration from genetic algorithms’ recombination mech-\nanisms, but require careful adaptation for text-based prompts. While traditional genetic algorithms can perform\nstraightforward splitting and merging of genetic sequences, prompt crossover needs to maintain semantic coherence\nand structural integrity. In our implementation, we developed a simple yet effective crossover mechanism: given two\nparent prompts that have demonstrated good performance, the operation splits each prompt approximately at its\nmidpoint and combines the first half of one prompt with the second half of the other. This approach, while basic\n(and which could be optimized), provides several advantages:\n   • It preserves coherent instruction blocks from each parent\n   • It enables the combination of different strategic elements (e.g., merging a prompt with strong reasoning guide-\n      lines with another that has effective constraint definitions)\n   • It maintains a balance between exploration and preservation of successful prompt components\n   However, this straightforward approach could be enhanced in future work through more sophisticated crossover\nmechanisms, such as semantic block identification and recombination, or intelligent selection of crossover points based\non prompt structure analysis.\n\n\n\n                                                  6\n\nNote: Already existing GA-related prompt optimization methods such as EvoPrompt [21] are a combination of\nthese first two categories of methods.\n\nFewshot: In-context learning for prompt optimization\n\n   In-context learning is a fundamental capability of large language models (LLMs)[6] that allows them to adapt\ntheir behavior based on examples provided within the prompt, without requiring model parameter updates. This\nability enables LLMs to understand and emulate patterns from demonstrated examples in real-time. The few-shot\nalgorithm for prompt optimization leverages this capability by augmenting existing prompts with selected examples\nwhile maintaining the original prompt’s structure and purpose. The process begins by randomly selecting 1 to 3\nlabeled examples from the training dataset for each parent prompt.  These examples are then appended to the\noriginal prompt in a structured format, with clear input-output pairs. The algorithm is computationally efficient as\nit doesn’t require complex prompt modifications or extensive evaluations. Instead, it relies on the natural ability of\nLLMs to learn from examples, making it a practical approach for prompt enhancement while maintaining the original\nprompt’s core functionality.\n\n      Method     Advantages                     Drawbacks\n        Mutations\n                         • Simple and efficient implementa-     • Can produce invalid prompts\n                                tion                               • Changes might be too random\n                         • Multiple   mutation   strategies     • Limited by predefined mutation\n                                available                                   strategies\n                         • Maintains prompt diversity\n                         • Low computational cost\n\n      APO\n                         • Error-driven optimization            • Computationally expensive\n                         • Targeted improvements based on     • Requires error examples\n                                  failure analysis                      • May overfit to specific error pat-\n                         • Systematic approach to prompt         terns\n                              refinement\n\n      OPRO\n                         • Learns from successful prompts       • Dependent on quality of previous\n                         • Efficient use of historical informa-        generations\n                                tion                               • Can converge to local optima\n                                                              • Higher LLM usage per generation\n\n         Crossover\n                         • Combines successful prompt fea-     • Simple   splitting  might  break\n                               tures                              prompt coherence\n                         • Preserves effective components        • Requires multiple good parents\n                         • Low computational cost              • Can produce semantically invalid\n                                                                  combinations\n\n        FewShot\n                         • Improves prompt with concrete     • Can make prompts too lengthy\n                           examples                           • Risk of overfitting to examples\n                         • Helps  model  understand  edge     • Limited by example quality and\n                               cases                                         availability\n                         • Direct performance feedback\n\n\n               Table 1: Comparison of Prompt Generation Methods in Hybrid Genetic Optimizer\n\n\n\n4.1.4  Examples\n\nTable 2 presents prompts obtained by several methodologies on the ETHOS multilabel dataset after 10 generations.\nThe original prompt from where all optimization trajetories started is:\n\n\n\n                                                  7\n\nA message from a user, your goal is to determine if this message is a hate speech or not: user_message.\nPossible class for the hate speech are: violence, directed_vs_generalized, gender, race, national_origin,\ndisability, religion, sexual_orientation.\n\n\n Method  Obtained Prompt\n APO\n\n            Analyze the given message to determine if it contains hate speech: user_message.\n            Definition: Hate speech is defined as language that attacks or diminishes a person or group\n            based on attributes such as race, religion, sexual orientation, gender, disability, national\n            origin, or other protected characteristics. Your task is to:\n            - Provide a detailed reasoning justifying your classification.\n            - Categorize the message into the most relevant categories from the following list. Each\n            category should be clearly justified in your reasoning:\n            + sexual_orientation: Attacks or offensive language targeting individuals based on their sexual\n            orientation or gender identity.\n            + religion: Language that attacks or demeans individuals based on their religious beliefs or\n            affiliation.\n            + [... definitions of other categories...]\n\n OPRO\n\n            Analyze the message to determine if it is hate speech: user_message.\n            Consider the following factors in your analysis:\n            - Presence of derogatory language or slurs targeting race, gender, religion, or other protected\n            characteristics.\n            - Any explicit or implicit threats, calls to violence, or incitement of harm.\n            - Discriminatory statements that demean or degrade individuals or groups.\n            - The overall tone, intent, and context of the message.\n            Ensure your reasoning is thorough and your solution is actionable and clear. Possible\n            categories for hate speeches are: violence, directed_vs_generalized, gender, race,\n            national_origin, disability, religion, sexual_orientation.\n\n Mutator\n\n            Analyze the message to determine if it is hate speech, using the following structured approach:\n            - Identify offensive language targeting protected characteristics.\n            - Assess intent and context.\n            - Evaluate potential harm.\n            - Ensure bias-free analysis adhering to hate speech policies.\n            Provide evidence-based reasoning and actionable solutions. Examples: Racial slurs dehumanize\n            based on race; messages promoting violence incite harm; derogatory gender terms cause\n            marginalization. Analyze: user_message.\n\n Ours\n\n            You are a team of experts tasked with determining if a given message contains hate speech. Your\n            team consists of three roles:\n            **The Critic**: Your first responsibility is to carefully analyze the message for any\n            indicators of hate speech, such as offensive language, derogatory terms, or discriminatory\n            content. Consider the context and tone of the message.\n            **The Problem Solver**: After the critic has provided their analysis, your role is to determine\n            whether the message qualifies as hate speech based on the indicators identified. Consider the\n            broader implications of labeling the message as hate speech.\n            **The Expert Reviewer**: Your role is to review the analysis and solution provided by the\n            previous roles.\n            Ensure that the reasoning is thorough, the solution is accurate, and the output is consistent\n            with the examples provided. Here is the message to analyze: user_message.\n\n\n        Table 2: Prompts obtained via different optimization methods on the ETHOS multilabel dataset\n\n\n\n\n\n                                                  8\n\n4.2  Optimization framework\n\nHOPR (Hint Optimization and Prompt Refinement) is a Python framework designed for systematic prompt opti-\nmization and evaluation. Like DSPy [10], it provides a structured approach to prompt engineering, but with a distinct\nfocus on evolutionary optimization techniques. While DSPy emphasizes the composition and chaining of language\nmodel operations through programmatic interfaces, HOPR specializes in automated prompt optimization through a\nvariety of implemented strategies extracted from the state of the art methods for automatic prompt engineering.\n   The framework is built around modular components: optimizers that implement different prompt generation\nstrategies, metrics for evaluation, and a core system for managing prompt evolution. HOPR’s architecture allows\nresearchers to easily implement and compare different prompt optimization techniques, track the evolution of prompts\nto study the best optimization methods, and maintain a \"hall of fame\" of top-performing candidates.\n   Unlike DSPy’s focus on prompt composition and application, HOPR emphasizes the development of automatic\nprompting methods by facilitating the implementation of concurrent strategies on the same problem. While being\neasily adaptible to new models, this allow a sain and reproducible comparative analysis of different prompt engineering\napproaches.\n  A key differentiator is HOPR’s hybrid approach, which allows multiple optimization strategies to work in parallel,\npotentially discovering more effective prompts than single-strategy approaches. This makes it especially valuable for\nresearchers studying prompt optimization methods and practitioners seeking to automatically optimize prompts for\nspecific tasks.\n\n4.3  Training pipeline\n\n4.3.1  Dataset Organization\n\nThe optimization process requires careful data partitioning to ensure robust evaluation and prevent overfitting. We\ndivide each dataset into three distinct subsets:\n   • Training set: Used during prompt generation for strategy-specific optimization. APO leverages this set for\n      error analysis and improvement, while the few-shot strategy uses it to select examples for in-context learning.\n   • Validation set: Employed during the optimization process to evaluate and compare generated prompts, enabling\n     the selection of promising candidates for subsequent generations.\n   • Test set: Reserved exclusively for final evaluation, measuring generalization capability and tracking performance\n      evolution across optimization steps.\n\n\n4.3.2  Population Management\n\nEach strategy is assigned a weight determining its contribution to the next generation’s population. The number of\ncandidates per strategy is calculated by multiplying these weights by the total population size. To maintain the exact\ndesired population size, any remaining slots are allocated to the strategy with the highest weight. This weighted\napproach ensures:\n   • Balanced exploration across different optimization techniques\n   • Customizable strategy emphasis based on task requirements\n   • Consistent population size maintenance throughout generations\n\n\n4.3.3  Evaluation Process\n\nThe evaluation of generated prompts follows a systematic approach:\n   • Initial evaluation on validation set to establish baseline performance.\n   • Generational evaluation to select promising candidates. Evaluations concerning advocated results in the paper\n     where conducted twice, due to disparities in LLM response and their probabilistic answers construction.\n   • Final testing on the held-out test set to measure true generalization.\nThis structured pipeline ensures robust optimization while maintaining the flexibility to adapt to different tasks and\nrequirements through adjustable strategy weights and evaluation parameters.\n\n\n4.3.4  Metrics\n\nETHOS multilabel dataset\n   For the multi-label classification task of the ETHOS dataset, we employ strict accuracy as our evaluation metric.\nA prediction is considered correct if and only if the set of predicted labels exactly matches the set of true labels,\n\n\n\n\n                                                  9\n\nregardless of their order. Formally, for a sample with true labels Y and predicted labels ˆY , the binary accuracy is\ndefined as:\n\n                                (                                                     1   if Y = ˆY\n                                       accuracy(Y, ˆY ) =\n                                                     0  otherwise\n\n   where Y and ˆY are treated as sets, meaning {a, b} = {b, a}. The final accuracy score is then computed as the\naverage of these binary evaluations across all samples in the dataset.\n\nMMLU & GPQA\n   For MMLU-Pro and GPQA datasets, we employ standard accuracy as our evaluation metric, where a prediction\nis considered correct if and only if it matches the correct answer. Formally, for a sample with true answer y and\npredicted answer ˆy, the binary accuracy is defined as:\n\n                                (                                                     1   if y ≡ˆy\n                                        accuracy(y, ˆy) =\n                                                     0  otherwise\n\n   where ≡denotes semantic equivalence rather than strict string matching. This equivalence consideration was\nnecessary as these datasets provide multiple-choice answers in a standardized format (typically including punctuation\nmarks like commas), but the LLM sometimes generated correct answers with slight formatting variations. To address\nthis, we implemented an LLM-based evaluation system that validates semantic correctness, ensuring that superficial\ndifferences in formatting do not impact the accuracy assessment. The reliability of this approach was verified through\nmanual inspection of a representative sample of model outputs.\n\n4.4  Experiments\n\nDatasets\n   300 samples were extracted to the orignal ETHOS dataset and separated in 3 subsets: 50 samples for the training\nset (used in the APO and the fewshot algorithms), 50 samples for the validation set (used for the selection of prompts\nat each generation) and 200 were used as test set to allow a meaningful comparison of different prompts while limiting\nthe risks of overfitting on other subsets during the optimzation process. Those numbers were chosen as a tradeoff\nbetween the budget allowed to the optimization process and the typical size of the datasets which can be obtained\nin real life optimization tasks. Most results displayed in this paper use the ETHOS dataset.\n\nModels\n  We computed prompt optimization for several methods, which we reimplemented, respecting the original descrip-\ntion made in their respective papers. In detail, APO [12], OPRO[4] were used as baselines, along with a random\nmutator described above.\n   In the GAAPO implementation, prompt generation was distributed across strategies with the following propor-\ntions: random mutations accounted for 40% of new prompts, while APO and OPRO each contributed 20%. The\nremaining 20% was equally divided between few-shot learning and crossover operations, each generating 10% of new\nprompts.\n   These numbers were chosen as a trade-off between random prompt modifications (mutations and crossover), local\nprompt optimization (APO and OPRO) and in-context learning (fewshot). We deliberately choose to limit the\nimportance of in-context learning as it has already been demonstrated that prompt efficiency scales with the number\nof given examples. Our goal here is to increase prompt efficiency for very small datasets to have a prompting method\nwhich can be used on real life prompts (comparison results are presented in section 5.1).\n   For each experiment, the number of generations and number of prompt generated at each generation was exper-\nimentally determined and will be justified in the Results section 5.3.\n\nLLMs\n   The experimental setup employs two distinct Large Language Models (LLMs) for different aspects of the opti-\nmization process.\n   For prompt generation, we utilize in most experiments DeepSeek-R1-distill-LLaMA-70B-versatile, a state-of-the-\nart open-source LLM based on the LLaMA architecture. This model, accessed through Groq’s inference platform,\noffers a balance between performance (with state-of-the-art performances on LLM tasks [32]) and computational\nefficiency (with inference times sensibly lower using Groq platform [33]). We compared the performance optimization\nobtained by this model to others in section 5.5.\n\n\n                                                  10\n\nFor the target model to be optimized through our prompting process, we employ GPT-4o-mini or llama3-8B-\ninstant [34]. We decided to use 2 models to assess the difference in evolution performance (which can be seen in\nsection 5.2) across different experiment settings, arguing that a prompt optimization could be model dependent.\n   This configuration allows us to assess the generalizability of our prompt optimization approach while maintaining\na clear separation between the prompt generation and evaluation phases of our methodology.\n\nGeneralization\n  We evaluated our prompt optimization approach across several widely used datasets, with results presented in\nsection 5.6. For each dataset, we maintained a consistent splitting strategy: 50 samples for training, 50 for validation,\nand up to 200 samples for testing (or the maximum available if fewer than 200 samples remained). This standardized\napproach, first validated on ETHOS, ensures fair comparison across different datasets while maintaining sufficient\nsamples for reliable evaluation. Complementary datasets used for this study are presented in section 3. Note that\nfor GPQA, only 98 samples were used in the testing set.\n\nOptimization of the selection process\n   To optimize the computational budget while maintaining effective prompt selection, we implemented and com-\npared three different selection strategies (see section 5.7 for results): complete evaluation, successive halving, and\nbandit selection.  These methods present different trade-offs between evaluation accuracy and computational ef-\nficiency.  For a representative scenario with a test dataset of 50 samples and a population of 50 prompts, the\ncomputational requirements vary significantly across methods.\n   • Complete evaluation, which tests every prompt against every sample, requires 2,500 LLM calls (50 prompts ×\n     50 samples), providing exhaustive but computationally intensive evaluation.\n   • Successive halving [28] offers a more efficient approach by progressively eliminating underperforming prompts.\n     In our implementation, we evaluate prompts on 20% of the dataset at each iteration and eliminate 40% of\n     the lowest-performing prompts. This process continues until reaching a predetermined number of prompts.\n     This strategy reduces the number of LLM calls to approximately 1,200, representing a 55% reduction in\n     computational cost compared to complete evaluation while maintaining robust selection pressure.\n   • The bandit selection method [29] provides the most efficient tradeoff [12], evaluating only 20 prompts on 15\n     samples over 5 iterations. This approach requires approximately 1,500 LLM calls (20 prompts × 15 samples ×\n     5 iterations), achieving a 40% reduction in computational cost compared to complete evaluation. While this\n     method samples less extensively, it leverages statistical efficiency to identify high-performing prompts.\n   These selection strategies offer different balances between evaluation thoroughness and computational efficiency,\nallowing practitioners to choose based on their specific constraints and requirements. Our empirical results suggest\nthat both successive halving and bandit selection maintain effective prompt identification while significantly reducing\ncomputational overhead.\n\n5  Results & Discussions\n\n5.1  Comparison with baselines\n\nThe experimental results demonstrate the effectiveness of our proposed GAAPO (Genetic Algorithm Assisted Prompt\nOptimization) approach on the ETHOS multilabel hate speech classification task. Figure 4 illustrates the validation\nperformance across different prompt optimization strategies over multiple iterations, while Table 3 presents the final\ntest and validation scores. Additionnally, obtained prompts are presented in table 2.\n\n      Model       Validation score                   Test score\n       Ours         0.60                                0.46\n      APO           0.52                                     0.38\n      OPRO         0.26                                     0.24\n        Mutator       0.52                                     0.34\n\nTable 3: Test and validations scores for the ETHOS dataset. Comparison of performance of prompt optimization\nusing llama3-8B-instant [34]\n\n  GAAPO demonstrates strong performance on the validation set, achieving a score of 0.46, which significantly\nsurpasses baseline methods including OPRO (0.24), Mutator (0.34), and APO (0.38). The evolution curve in Figure\n3 shows GAAPO’s ability to maintain consistent improvement throughout the optimization process.\n\n\n                                                  11\n\nFigure 4: Results obtained by using several prompt generation strategies. LLM-optimizer used: llama-3.1-8B\n\n\n  A critical analysis of test and validation scores reveals an important phenomenon common to genetic algorithms:\nselection bias. This is particularly evident in APO’s performance, where results at some iterations highlight a very\nhigh difference score between test and validation sets (culminating at 0.3 for the 2nd generation). This extreme\ndisparity illustrates how genetic algorithms can inadvertently optimize for specific test set characteristics rather\nthan general problem-solving capabilities. GAAPO mitigates this selection bias through its diverse strategy port-\nfolio, resulting in more balanced performance between test (0.60) and validation (0.46) scores, suggesting better\ngeneralization.\n   The lower performance of OPRO (test: 0.26, validation: 0.24) indicates that reinforcement learning-based ap-\nproaches struggle with exploring vast prompt spaces effectively. The Mutator approach achieves intermediate results\n(test: 0.52, validation: 0.34), but still shows signs of selection bias with its significant test-validation gap. These ob-\nservations highlight how selection bias can affect different optimization strategies to varying degrees, with GAAPO’s\nhybrid approach providing the most robust defense against this common genetic algorithm limitation.\n   Moreover, we can see a difference in the original score of the models (at iteration 0, all scores should be identicals).\nHowever, due to the disparity in LLM performance and their probabilistic caracters, results are not always exactly\nconsistent across time.\n\n5.2  Model evaluation comparison\n\nComparing the optimization trajectories between GPT-4o-mini and LLaMA3-8B (displayed in figure 5) reveals few\ndifferences in how these models respond to prompt optimization. Both models show significant improvement from\ntheir initial performance, but their learning patterns and final achievements slightly differ.\n   Both GPT-4o-mini and LLaMA3-8B demonstrate stable optimization trajectories, with consistent learning pat-\nterns and similar generalization characteristics across generations. However, GPT-4o-mini achieves notably superior\nperformance, reaching validation scores of up to 0.70 compared to LLaMA3-8B’s 0.60. Both models maintain steady\noptimization paths with comparable stability in their generalization gaps.\n   Examining test scores reveals GPT-4o-mini’s consistent edge in performance, maintaining a 0-0.05 point advantage\nover LLaMA3-8B throughout the optimization process. However, this superior performance must be interpreted with\ncaution, as both models show signs of potential overfitting in later generations. The increasing gap between validation\n\n\n                                                  12\n\nFigure 5: Comparison of optimization trajectories between GPT-4o-mini and LLaMA3-8B models on the ETHOS\ndataset for GAAPO. The plot shows the evolution of validation scores (solid lines) and test scores (dashed lines)\nacross generations for both models.\n\n\nand test scores after generation 8 suggests that while GPT-4o-mini achieves better absolute performance, careful\nmonitoring of generalization remains crucial for both models.\n   Given the higher performance metrics of GPT-4o-mini and comparable computational costs between the two mod-\nels in our experimental setup, we selected GPT-4o-mini as our primary LLM-optimizer for subsequent experiments.\nThis choice was driven by the quantitative advantages in optimization outcomes, while both models demonstrate\nequally reliable optimization stability.\n\n5.3  Influence of population size\n\n\n     Population  Number of gen-  Test score         Validation score  Number of LLM\n      size          erations                                                           calls\n      20            25                    0.50                  0.42                25000\n      30            17                    0.56                  0.50                25500\n      40            13                    0.62                  0.46                24500\n      50            10                    0.68                  0.46                25000\n\nTable 4: Test and validations scores for the ETHOS dataset. Comparison of different population size and number of\ngenerations for GPT-4o-mini. [34]\n\n  We conducted experiments with varying population sizes while maintaining a comparable total number of LLM\ncalls across configurations, as shown in Table 4. The results demonstrate a clear trade-off between population size\nand the number of generations required. Larger populations (50 prompts) with fewer generations (10) achieve higher\ntest scores (0.68) compared to smaller populations running for more generations (20 prompts, 25 generations, 0.50\ntest score).\n   While the configuration with 30 prompts shows the best validation score (0.50) and a smaller generalization\ngap, we opted for the 50-10 configuration for several practical advantages.  First, larger populations enable better\n\n\n                                                  13\n\nparallelization of prompt evaluation, significantly reducing wall-clock time. Second, this configuration aligns well\nwith optimized selection strategy, which benefits from a larger pool of candidates to select from in each generation.\n   However, the increased generalization gap in the 50-10 configuration (0.22 points between test and validation\nscores, compared to 0.08 points for 20-25) suggests a higher risk of overfitting.  This observation indicates that\nwhile larger populations can explore the prompt space more effectively within fewer generations, they may require\nmore robust validation strategies to ensure generalization. Despite this limitation, the practical benefits of faster\nconvergence and improved parallelization potential make the 50-10 configuration our recommended choice for prompt\noptimization tasks.\n\n5.4  Prompt generators comparison\n\nWe can now study in detail the prediction made by each prompt generator in GAAPO.\n  We conducted a detailed analysis of each prompt generator’s performance in GAAPO through two complementary\nperspectives. Figure 6 presents the overall distribution of validation scores for each strategy through boxplots, while\nFigure 7 tracks the improvement potential of each strategy across generations, showing both mean and maximum\nimprovements in score relative to parent prompts.\n   To obtain these visualizations, we first aggregated all prompts generated by each strategy and analyzed their\nvalidation scores (Figure 6). Additionally, we computed the improvement in validation score between each generated\nprompt and its parent prompt across generations (Figure 7), allowing us to understand not just absolute performance\nbut also each strategy’s ability to improve upon existing prompts.\n\n\n\n\n\nFigure 6: Performance distribution of individual prompt generation strategies in GAAPO on the validation set.\nModel used: GPT-4o-mini.\n\n   The analysis reveals several key insights about strategy effectiveness and the importance of maintaining diversity\nin optimization approaches:\n\n   • Strategy Effectiveness and Stability: Few-shot learning demonstrates superior performance (median ∼0.57)\n     with consistent results, as shown by its compact boxplot and positive improvement scores in early generations.\n     This aligns with existing literature [6], highlighting the value of example-based learning. OPRO maintains\n     strong and stable performance (median ∼0.55), though its evolution plot shows diminishing improvements over\n      generations. role_assignment and concise_optimization show reliable performance with tight distributions,\n     but their improvement potential decreases in later generations.\n\n\n\n\n                                                  14\n\nFigure 7: Evolution of improvement scores for each prompt generation strategy across generations. For each strat-\negy, we track both mean improvement (blue) and maximum improvement (red) relative to parent prompts. Mean\nimprovement represents the average score difference between generated prompts and their parents, while maximum\nimprovement shows the best improvement achieved in each generation.  Negative values indicate that generated\nprompts performed worse than their parents. Model used: GPT-4o-mini.\n\n\n\n\n\n                                                  15\n\n• Evolution patterns: Most strategies show declining improvement potential over generations, with negative\n    mean improvements in later stages, suggesting they work best in early exploration. APO’s boxplot shows\n     high variability (0.10-0.35), but its evolution plot reveals strong initial improvements followed by declining\n      effectiveness, supporting its potential role as an early-stage optimizer. Few-shot learning uniquely maintains\n      positive maximum improvements even in later generations, indicating sustained ability to generate beneficial\n      variations.\n\n   • Underperforming Strategies: Several mutation strategies, particularly structural_variation and task_decomposition,\n      consistently show negative improvement scores across generations, suggesting limited effectiveness for the cur-\n      rent task. However, completely removing these strategies could be counterproductive for two reasons:\n\n      – Task Dependency:  Different tasks may benefit from different prompt modification approaches. What\n          appears ineffective for one task might be crucial for another as every optimization task is learned in a\n            different optimization space.\n\n      – Exploration Value: Even seemingly underperforming strategies contribute to maintaining genetic diversity,\n           potentially enabling the discovery of novel promising prompt variations through combination with other\n          approaches.\n\n   • Strategic Implications: The analysis suggests implementing a dynamic, task-adaptive strategy:\n\n      – Early generations: Leverage APO and mutation strategies for broad exploration\n\n      – Mid-generations: Emphasize few-shot learning and OPRO for stable improvements\n\n      – Later generations: Focus on strategies showing consistent positive improvements (few-shot, role_assignment)\n            for refinement\n\n      – Maintain a minimum weight for all strategies to preserve optimization flexibility across different tasks\n\n   This comprehensive analysis reinforces the value of GAAPO’s adaptable framework, which can accommodate\nvarying strategy effectiveness across different tasks while maintaining the potential benefits of diverse optimization\napproaches. The framework’s ability to dynamically adjust strategy weights while preserving all methods makes it\nparticularly robust for general-purpose prompt optimization across diverse applications.\n    It should be notice that optimization methods tend to have descending curves which is logical: as we compare new\nprompts with their parent prompts, the task is more and more difficult (given that the reference prompt improves with\nthe generations). Moreover, studies on other datasets tend to highlight the fact that different prompt optimization\nmethods can perform very differently between tasks, highlightning the importance to keep methods in a general\nframework and the risk to select optimizers based on their results on a unique dataset.\n\n5.5  Model generators comparison\n\nThe comparison of different language models as prompt optimizers reveals striking patterns (which can be seen in\nFigure 8 in both performance and generalization capabilities. Most notably, reasoning-specialized models (QwQ32B\nand deepseek-R1) and O1 demonstrate superior performance compared to general-purpose models like GPT-4o-mini.\n  QwQ32B emerges as the top performer, showing consistent improvement in validation scores from an initial 0.28\nto a remarkable 0.70 by generation 10. Its learning trajectory is particularly stable, with steady increases and minimal\nfluctuations. However, its test scores (dashed line) plateau around 0.55, indicating a significant generalization gap\nof approximately 0.15 points.\n  A particularly interesting comparison emerges between DeepSeek-R1 and O1 models. While both achieve strong\nfinal validation scores (0.68 and 0.65 respectively), O1 demonstrates notably better generalization characteristics. By\ngeneration 10, O1 maintains test scores around 0.55, nearly matching its validation performance, while DeepSeek-R1\nshows a larger disparity with test scores around 0.45. This suggests that O1’s optimization process, while slightly\nlower in absolute validation performance, produces more robust and generalizable prompts.\n   In contrast, GPT-4o-mini shows notably inferior performance. While it achieves quick initial improvement, its\nvalidation scores stagnate around 0.45-0.50 after generation 2, with minimal subsequent improvement. However, like\nO1, it maintains a smaller generalization gap between validation and test scores, suggesting more robust, if modest,\noptimization capabilities.\n   The evolution of scores across generations reveals an interesting pattern: while reasoning models continue to\nimprove validation performance until the final generations, o1 maintains a more balanced improvement in both\nvalidation and test scores. This suggests that o1 might be particularly valuable for applications where generalization\nreliability is crucial, even if peak performance is slightly lower than specialized reasoning models.\n\n\n                                                  16\n\nFigure 8: Comparison of different LLMs as prompt optimizers in GAAPO. The plot shows validation (solid lines) and\ntest (dashed lines) scores across generations for four models: QwQ32B, DeepSeek-R1, O1, and GPT-4o-mini. While\nreasoning-specialized models achieve higher absolute scores, O1 demonstrates better generalization with smaller gaps\nbetween validation and test performance.\n\n\n   These findings indicate that while reasoning-specialized models achieve higher absolute performance, o1 offers\nan attractive compromise between performance and generalization stability, potentially making it more suitable for\npractical applications where robust generalization is essential.\n\n5.6  Applications on other datasets\n\nThe experimental results across multiple datasets demonstrate both the effectiveness of our approach and the varying\npotential for prompt optimization across different tasks. Table 5 presents validation scores for four distinct datasets,\nrevealing several important patterns.\n\n          Dataset    ETHOS  mul-  MMLU-Pro   MMLU-Pro   GPQA\n                            tilabel          engineering     Business\n              Initialization   0.28               0.39               0.72               0.38\n        APO           0.44               0.45               0.73               0.42\n        OPRO         0.38               0.44             0.76              0.43\n           Mutator       0.40               0.43              0.735              0.43\n       OURS       0.46            0.48              0.74               0.43\n\nTable 5: Validations scores for different datasets. Models used: Deepseek-R1 as Prompt Generator and GPT-4o-mini\nas Optimizer.\n\n  We can see on Table 5 that our method achieves superior performance on datasets where prompt engineering\nshows significant potential for improvement. For the ETHOS multilabel classification task, we observe a substantial\nimprovement from the initial score of 0.28 to 0.46, outperforming all baseline methods including APO (0.44), OPRO\n(0.38), and Mutator (0.40). Similarly, on the MMLU-Pro engineering dataset, our approach reaches 0.48, showing\nmeaningful improvement over the initialization score of 0.39 and competing methods.\n\n\n                                                  17\n\nHowever, the results also reveal that not all tasks benefit equally from prompt optimization. The MMLU-Pro\nBusiness dataset, with its high initialization score of 0.72, shows minimal room for improvement, with our method\nand the Mutator achieving only marginal gains (0.73 and 0.735 respectively). This suggests that some tasks may\nalready be well-aligned with LLMs’ base capabilities, limiting the potential impact of prompt optimization. The\nGPQA dataset presents another interesting case where all optimization methods, including ours, achieve similar\nmodest improvements (from 0.38 to 0.43), indicating that some tasks may have inherent complexity barriers that\nprompt optimization alone cannot overcome.\n   The varying effectiveness of prompt optimization across tasks can be attributed to multiple underlying factors.\nFirst, the overlap between an LLM’s training data and the target dataset can create a ceiling effect -  if similar\nexamples were present in the training corpus, the model may already demonstrate near-optimal performance with\nsimple prompts. Second, task-specific characteristics such as domain specificity and reasoning complexity influence\nthe optimization potential; technical domains often benefit more from structured prompting than general knowledge\ntasks. Third, the nature of the required output (e.g., multiple-choice vs. multi-label classification) affects the scope for\nimprovement through prompt engineering. Finally, the fundamental alignment between the task’s requirements and\nthe model’s learned representations determines whether performance limitations can be addressed through prompt\noptimization alone or require more substantial interventions such as fine-tuning.\n\n5.7  Selection method comparison\n\nWe conducted a comparative analysis of the three selection methods on the ETHOS dataset, evaluating their efficiency\nand performance trade-offs. The computational requirements varied significantly across methods: for a test set of 50\nsamples, the complete evaluation (\"all\") requires 2,500 LLM calls per generation, the bandit approach approximately\n1,500 calls, while successive halving (SH) uses only 1,500 calls per generation.\n   To ensure fair comparison, we also plotted results where the number of calls are equivalent between all methods.\nWe adjusted the test size to 110 samples to obtain the right number of calls for both bandit and SH selection methods.\n   Figure 9 presents the evaluation for both validation and test scores for the 5 mentioned processes: \"all\", \"bandit\"\nwith 50 samples, \"bandit\" with 110 samples, \"SH\" with 50 samples and \"SH\" with 110 samples.\n\n\n\n\n\nFigure 9: Comparison of different prompt selection strategies during GAAPO optimization. The plot shows the\nevolution of validation (solid lines) and test (dashed lines) scores across generations for different selection methods.\nModel used: GPT-4o-mini.\n\n\n                                                  18\n\nThe comparison of different selection strategies reveals compelling insights about the trade-offs between sample\nsize, computational efficiency, and performance stability. The complete evaluation method (\"all\"), using 50 samples,\nachieves the highest validation scores (peaking at 0.68) but requires significantly more computational resources.\nHowever, our analysis demonstrates that increasing the sample size from 50 to 110 samples for alternative strategies\ndoes not necessarily lead to better performance, suggesting that efficient sampling is more crucial than sample size.\n   The bandit method emerges as particularly noteworthy, showing remarkable stability in both its 50 and 110\nsample configurations. Despite using 40% fewer LLM calls, it maintains consistent performance around 0.45-0.50\nvalidation score with minimal fluctuations between generations. More importantly, the bandit approach exhibits a\nsmaller generalization gap between test and validation scores, indicating better resistance to overfitting. However,\nwe can observe a certain drop of performance between this selection method and \"all\".\n   In contrast, successive halving (SH) displays considerable volatility, especially evident in its performance spikes\nand drops across generations. While SH occasionally matches or exceeds the bandit’s performance (reaching peaks\naround 0.60-0.70), its inconsistency makes it less reliable for practical applications.  Interestingly, increasing the\nsample size for SH from 50 to 110 samples does not significantly mitigate this volatility, nor does it critically improve\nperformances.\n   These findings suggest that while complete evaluation with 50 samples provides the highest absolute performance,\nthe bandit approach with its reduced computational footprint and stable optimization trajectory offers an interesting\nalternative. The stability and efficiency of the bandit method, combined with its robust generalization characteristics,\nmake it a choice for resource-conscious prompt optimization scenarios.\n\n6  Conclusion\n\nGAAPO (Genetic Algorithm Applied to Prompt Optimization) represents a significant advancement in prompt op-\ntimization, combining evolutionary strategies with established optimization methods. Our comprehensive evaluation\ndemonstrates its effectiveness across multiple dimensions: superior validation performance with better generalization\nthan baseline methods, efficient resource utilization through several prompt selection methods, and robust perfor-\nmance across different language models (GPT-4o-mini and LLaMA3-8B). The framework’s modular architecture,\nincorporating multiple prompt generation strategies and selection methods, enables flexible adaptation to various\ntasks while maintaining optimization effectiveness.\n   However, several limitations warrant attention in future work. The framework shows increased generalization gaps\nwith larger population sizes, suggesting the need for more sophisticated validation strategies. The computational\noverhead, while reduced through bandit selection, remains significant for resource-constrained applications. Future\nimprovements could focus on developing more efficient prompt evaluation methods, incorporating active learning\nto reduce the number of required examples, and implementing adaptive population sizing strategies. Additionally,\ninvestigating the framework’s effectiveness across a broader range of tasks and language models would enhance its\ngeneralizability and practical applicability.\n\n\n\n\n\n                                                  19\n\nAcknowledgments\n\nWe extend our sincere appreciation to Joël Belafa, Florence Armstrong, Lina Faik, Maxime Gobin and Charles-Albert\nLehalle whose insightful comments and suggestions substantially enhanced this work. Their expertise and careful\nattention to detail helped us improve various aspects of the paper, from theoretical foundations to experimental\nvalidation.  Their feedback not only improved the current work but also provided valuable directions for future\nresearch in this area.\n\nReferences\n\n [1] Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng\n     Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff, Pranav Sandeep Dulepet, Saurav Vidyadhara, Dayeon Ki,\n    Sweta Agrawal, Chau Pham, Gerson Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava, Hevander Da Costa, Saloni\n    Gupta, Megan L. Rogers, Inna Goncearenco, Giuseppe Sarli, Igor Galynker, Denis Peskoff, Marine Carpuat,\n     Jules White, Shyamal Anadkat, Alexander Hoyle, and Philip Resnik. The prompt report: A systematic survey\n     of prompt engineering techniques, 2025.\n\n [2] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. A\n    systematic survey of prompt engineering in large language models: Techniques and applications, 2025.\n\n [3] De Jong K. Learning with genetic algorithms: An overview. Mach Learn 3, 1988.\n\n [4] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large\n    language models as optimizers, 2024.\n\n [5] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke\n    Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models,\n     2024.\n\n [6] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu,\n    Tianyu Liu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. A survey on in-context learning, 2024.\n\n [7] Michael Feffer, Ronald Xu, Yuekai Sun, and Mikhail Yurochkin. Prompt exploration with prompt regression,\n     2024.\n\n [8] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and\n    Tianyi Zhou. A survey on knowledge distillation of large language models, 2024.\n\n [9] Sondos Mahmoud Bsharat, Aidar Myrzakhan, and Zhiqiang Shen. Principled instructions are all you need for\n     questioning llama-1/2, gpt-3.5/4, 2024.\n\n[10] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Sai-\n     ful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher\n     Potts. Dspy: Compiling declarative language model calls into self-improving pipelines. 2024.\n\n[11] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting\n    knowledge from language models with automatically generated prompts, 2020.\n\n[12] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt opti-\n    mization with \"gradient descent\" and beam search, 2023.\n\n[13] Annu Lambora, Kunal Gupta, and Kriti Chopra. Genetic algorithm- a literature review. In 2019 International\n    Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon), pages 380–384, 2019.\n\n[14] J. Yang and V. Honavar. Feature subset selection using a genetic algorithm. IEEE Intelligent Systems, 1998.\n\n[15] K. O. Stanley and R. Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary\n    Computation, 2002.\n\n[16] T. Beielstein, C. Schumacher, and S. Markon.  Parallel genetic algorithm tuning for optimization problems.\n   IEEE Transactions on Evolutionary Computation, 2003.\n\n\n\n                                                  20\n\n[17] U. Maulik and S. Bandyopadhyay. Genetic algorithm-based clustering technique. Pattern Recognition, 2000.\n\n[18] S. Whiteson and P. Stone. Evolutionary function approximation for reinforcement learning. Journal of Machine\n    Learning Research, 2006.\n\n[19] P. Merz and B. Freisleben.  Fitness landscape analysis and memetic algorithms for the quadratic assignment\n    problem. IEEE Transactions on Evolutionary Computation, 2000.\n\n[20] E. Cantú-Paz. Efficient and accurate parallel genetic algorithms. Kluwer Academic Publishers, 2001.\n\n[21] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu\n    Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers, 2024.\n\n[22] Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley Malin, and Sricharan\n    Kumar. Phaseevo: Towards unified in-context prompt optimization for large language models, 2024.\n\n[23] Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, and Grigorios Tsoumakas. Ethos: a multi-label hate speech\n     detection dataset. Complex &amp; Intelligent Systems, 8(6):4663–4678, january 2022.\n\n[24] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran\n     Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language under-\n    standing benchmark. arXiv preprint arXiv:2406.01574, 2024.\n\n[25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\n    Measuring massive multitask language understanding, 2021.\n\n[26] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian\n    Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark, 2023.\n\n[27] Anthony Cui, Pranav Nandyalam, and Kevin Zhu.  Introducing mapo: Momentum-aided gradient descent\n    prompt optimization, 2025.\n\n[28] Robin Schmucker, Michele Donini, Muhammad Bilal Zafar, David Salinas, and Cédric Archambeau.  Multi-\n     objective asynchronous successive halving, 2021.\n\n[29] Aleksandrs Slivkins. Introduction to multi-armed bandits, 2024.\n\n[30] Qiyang Han, Koulik Khamaru, and Cun-Hui Zhang. Ucb algorithms for multi-armed bandits: Precise regret\n    and adaptive inference, 2024.\n\n[31] Volodymyr Kuleshov and Doina Precup. Algorithms for multi-armed bandit problems, 2014.\n\n[32] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\n    Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao,\n    Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang\n    Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin,\n    Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng\n    Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang,\n    Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong,\n    Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang,\n    Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang,\n    Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\n    Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou,\n    Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou,\n    Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen\n     Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan\n    Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li,\n    Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang\n    Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang,\n    Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong,\n    Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan\n    Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou,\n\n\n                                                  21\n\nY. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun\n    Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen\n    Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song,\n    Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning\n     capability in llms via reinforcement learning, 2025.\n\n[33] Dennis Abts, Garrin Kimmell, Andrew Ling, John Kim, Matt Boyd, Andrew Bitar, Sahil Parmar, Ibrahim\n    Ahmed, Roberto DiCecco, David Han, John Thompson, Michael Bye, Jennifer Hwang, Jeremy Fowers, Peter\n     Lillian, Ashwin Murthy, Elyas Mehtabuddin, Chetan Tekur, Thomas Sohmers, Kris Kang, Stephen Maresh,\n    and Jonathan Ross. A software-defined tensor streaming multiprocessor for large-scale machine learning. In\n     Proceedings of the 49th Annual International Symposium on Computer Architecture, ISCA ’22, page 567–580,\n   New York, NY, USA, 2022. Association for Computing Machinery.\n\n[34] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\n    Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\n    Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, As-\n    ton Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang,\n    Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller,\n    Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,\n    Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan,\n    Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily\n    Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee,\n    Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hai-\n     ley Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann,\n    Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay\n    Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng\n    Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca,\n    Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate\n    Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu,\n    Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan,\n     Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira,\n    Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli,\n   Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh,\n   Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning\n    Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter\n    Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao\n    Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta\n     Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sum-\n     baly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh,\n    Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen,\n    Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla,\n    Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas\n    Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami,\n    Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero,\n    Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang,\n    Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag,\n    Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre\n    Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam\n     Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex\n    Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei\n    Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ram-\n    chandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\n    Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie\n    Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram\n    Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina\n     Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris\n     Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David\n    Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang\n\n\n                                                  22\n\nWang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily\nHahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,\nFelix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide,\nGabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory\nSizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang,\nHanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan\nZhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weiss-\nman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang,\nJennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe\nCummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai\nWu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraragha-\nvan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen,\nLakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich,\nLuca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew\nLennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L.\nSeltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike\nMacey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini\nSanthanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier,\nNikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar\nSalpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager,\nPierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao,\nRachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy,\nRaymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin\nSiby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan,\nSaurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,\nSheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang\nZhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve\nKehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk,\nSuraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler,\nThomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta,\nVictoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poe-\nnaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will\nConstable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun\nChen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao,\nYuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu\nYang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024.\n\n\n\n\n\n                                             23\n\nAnnex\n\nMMLU-Pro dataset\n\n\n             MMLU-Pro: repartition of questions in the different categories\n\n             Category                    Number of questions\n\n             Math                                1351\n\n                Physics                              1299\n\n               Chemistry                            1131\n\n             Law                                 1101\n\n                Engineering                          969\n\n               Economics                           844\n\n                Health                               818\n\n                Psychology                           798\n\n                Business                             789\n\n                Biology                              717\n\n                Philosophy                           499\n\n              Computer science                     410\n\n                History                              381\n\n               Others                               924\n\n                Total                                12032\n\n                                 Table 6: MMLUPro question repartitions.\n\n\n\n\n\n                                                  24\n\nPrompt generation\n\n\n     Mutation Type     Obtained Prompt\n      Original prompt       Answer this math problem: problem\n     Expand Instructions   Answer this math problem: problem\n                        To solve this problem, follow these detailed step-by-step guidelines:\n                              • Understand the Problem: Identify what is being asked and determine\n                                   the type of math problem (e.g., algebra, geometry, calculus).\n                              • Identify Known and Unknown Variables: List all known values and\n                                   the variables to solve.\n                              • Develop a Plan: Choose the correct mathematical approach.\n                              • Execute the Plan: Carry out calculations accurately, step by step.\n                              • Verify Your Solution: Substitute the answer back into the problem to\n                                  check correctness.\n                              • Reflect on the Solution: Ensure clarity, correctness, and consider al-\n                                     ternative solutions.\n                          Quality Criteria: Clarity, correct notation, logical flow. Evaluation Met-\n                                 rics: Accuracy, completeness, coherence. Common Mistakes: Avoid calcu-\n                                 lation errors, logical jumps, and assumptions not supported by the problem.\n     Add Expert Persona   You are a Fields Medalist specializing in complex mathematical problem-\n                               solving and theoretical mathematics. With a Ph.D. in Mathematics and ex-\n                                pertise in algebraic topology, differential geometry, and number theory, you\n                           approach problems with analytical rigor and creative insight.\n                         Answer this math problem: problem\n      Task Decomposition   To solve the given math problem, follow these steps:\n                              • Understand the Problem: Carefully read the statement and identify\n                                     relevant details.\n                              • Break Down the Problem: Divide it into smaller, manageable parts.\n                              • Plan a Solution Strategy:  Determine which formulas or theorems\n                                     apply.\n                              • Execute the Plan: Perform calculations in a logical sequence.\n                              • Verify Intermediate Results: Check calculations for accuracy at each\n                                       step.\n                              • Combine Results: Integrate partial solutions to derive the final answer.\n                              • Review and Validate: Double-check correctness.\n                         Answer the math problem: problem\n      Concise Optimization   Solve: problem\n\n                         Table 7: Mutation Types and Their Corresponding Prompts\n\n\n\n\n\n                                                  25\n\nMutation Type    Obtained Prompt\nStructural Variation  Task Overview: You are tasked with solving a mathematical problem using\n                    a structured approach.\n                Problem Statement:\n                       • Math Problem: problem\n                   Solution Strategy:\n                       • Understand the problem statement.\n                       • Identify known variables and constraints.\n                       • Determine required formulas or theorems.\n                       • Solve systematically step-by-step.\n                       • Verify your solution for accuracy.\n             Common Mistakes to Avoid:\n                       • Misinterpreting the problem.\n                       • Skipping verification steps.\n                       • Incorrect formula application.\n                     Verification Steps:\n                       • Recheck each step for consistency.\n                       • Ensure logical correctness.\n                Output Format (JSON):\n\n                           {\n                               \"solution\": \"your detailed solution steps\",\n                               \"final_answer\": \"your final answer\"\n                           }\n\n\n\nCreative Backstory    In the year 2147, aboard the interstellar vessel Math Explorer, you are the\n                         chief mathematician responsible for ensuring safe passage. The ship’s naviga-\n                       tion system has encountered a critical error.\n                   Only by solving the following math problem can you recalibrate the system\n                   and prevent a catastrophic collision with a rogue asteroid. The fate of the crew\n                   and the success of the mission depend on your expertise.\n                   Answer this math problem: problem\nConstraint Addition  Answer this math problem:problem\n                       • Do not use any numbers greater than 10.\n                       • Explain your solution as if teaching a 10-year-old using biological analo-\n                                 gies.\n                       • Solve within 5 minutes.\n                       • Do not include algebraic expressions or terminology.\n                       • Present the final answer in haiku format.\n\nRole Assignment     You are a team of mathematicians working collaboratively to solve the problem.\n                       • Critic:  Analyze the problem’s complexity and identify possible chal-\n                              lenges: problem.\n                       • Problem Solver: Devise a step-by-step strategy.\n                       • Teacher: Explain the solution clearly for a novice audience.\n                   Your response should integrate insights from each role, providing a thorough\n                       yet accessible solution.\n\n                Table 8: Mutation Types and Their Corresponding Prompts (end)\n\n\n\n\n\n                                           26",
"headers": [
"arXiv:2504.07157v3  [cs.NE]  16 Apr 2025",
"GAAPO: Genetic Algorithmic Applied to Prompt Optimization",
"Abstract",
"1",
"Introduction",
"2",
"Related works",
"3",
"Data",
"4",
"Methods",
"5",
"Results & Discussions",
"6",
"Conclusion",
"Acknowledgments",
"References",
"Annex",
"Xavier Sécheresse",
", Jacques-Yves Guilbert–Ly",
", and Antoine Villedieu de Torcy",
"Biolevate",
"April 17, 2025",
"2.1",
"Prompt Engineering",
"2.2",
"Automatic Prompt Engineering",
"2.3",
"Genetic algorithm",
"2.4",
"Application to prompt optimization",
"4.1",
"Models",
"4.2",
"Optimization framework",
"4.3",
"Training pipeline",
"4.4",
"Experiments",
"5.1",
"Comparison with baselines",
"5.2",
"Model evaluation comparison",
"5.3",
"Influence of population size",
"5.4",
"Prompt generators comparison",
"5.5",
"Model generators comparison",
"5.6",
"Applications on other datasets",
"5.7",
"Selection method comparison",
"MMLU-Pro dataset",
"Prompt generation"
],
"tables": [
"|Method|Advantages|Drawbacks|\n|---|---|---|\n|Mutations|• Simple and efcient implementa-<br>tion<br>• Multiple<br>mutation<br>strategies<br>available<br>• Maintains prompt diversity<br>• Low computational cost|• Can produce invalid prompts<br>• Changes might be too random<br>• Limited by predefned mutation<br>strategies|\n|APO|• Error-driven optimization<br>• Targeted improvements based on<br>failure analysis<br>• Systematic approach to prompt<br>refnement|• Computationally expensive<br>• Requires error examples<br>• May overft to specifc error pat-<br>terns|\n|OPRO|• Learns from successful prompts<br>• Efcient use of historical informa-<br>tion|• Dependent on quality of previous<br>generations<br>• Can converge to local optima<br>• Higher LLM usage per generation|\n|Crossover|• Combines successful prompt fea-<br>tures<br>• Preserves efective components<br>• Low computational cost|• Simple<br>splitting<br>might<br>break<br>prompt coherence<br>• Requires multiple good parents<br>• Can produce semantically invalid<br>combinations|\n|FewShot|• Improves prompt with concrete<br>examples<br>• Helps<br>model<br>understand<br>edge<br>cases<br>• Direct performance feedback|• Can make prompts too lengthy<br>• Risk of overftting to examples<br>• Limited by example quality and<br>availability|",
"|Method|Obtained Prompt|\n|---|---|\n|APO|`Analyze the given message to determine if it contains hate speech: user_message.`<br>`Definition: Hate speech is defined as language that attacks or diminishes a person or group`<br>`based on attributes such as race, religion, sexual orientation, gender, disability, national`<br>`origin, or other protected characteristics. Your task is to:`<br>`- Provide a detailed reasoning justifying your classification.`<br>`- Categorize the message into the most relevant categories from the following list. Each`<br>`category should be clearly justified in your reasoning:`<br>`+ sexual_orientation: Attacks or offensive language targeting individuals based on their sexual`<br>`orientation or gender identity.`<br>`+ religion: Language that attacks or demeans individuals based on their religious beliefs or`<br>`affiliation.`<br>`+ [... definitions of other categories...]`|\n|OPRO|`Analyze the message to determine if it is hate speech: user_message.`<br>`Consider the following factors in your analysis:`<br>`- Presence of derogatory language or slurs targeting race, gender, religion, or other protected`<br>`characteristics.`<br>`- Any explicit or implicit threats, calls to violence, or incitement of harm.`<br>`- Discriminatory statements that demean or degrade individuals or groups.`<br>`- The overall tone, intent, and context of the message.`<br>`Ensure your reasoning is thorough and your solution is actionable and clear. Possible`<br>`categories for hate speeches are: violence, directed_vs_generalized, gender, race,`<br>`national_origin, disability, religion, sexual_orientation.`|\n|Mutator|`Analyze the message to determine if it is hate speech, using the following structured approach:`<br>`- Identify offensive language targeting protected characteristics.`<br>`- Assess intent and context.`<br>`- Evaluate potential harm.`<br>`- Ensure bias-free analysis adhering to hate speech policies.`<br>`Provide evidence-based reasoning and actionable solutions. Examples: Racial slurs dehumanize`<br>`based on race; messages promoting violence incite harm; derogatory gender terms cause`<br>`marginalization. Analyze: user_message.`|\n|**Ours**|`You are a team of experts tasked with determining if a given message contains hate speech. Your`<br>`team consists of three roles:`<br>`**The Critic**: Your first responsibility is to carefully analyze the message for any`<br>`indicators of hate speech, such as offensive language, derogatory terms, or discriminatory`<br>`content. Consider the context and tone of the message.`<br>`**The Problem Solver**: After the critic has provided their analysis, your role is to determine`<br>`whether the message qualifies as hate speech based on the indicators identified. Consider the`<br>`broader implications of labeling the message as hate speech.`<br>`**The Expert Reviewer**: Your role is to review the analysis and solution provided by the`<br>`previous roles.`<br>`Ensure that the reasoning is thorough, the solution is accurate, and the output is consistent`<br>`with the examples provided. Here is the message to analyze: user_message.`|",
"|Model|Validation score|Test score|\n|---|---|---|\n|**Ours**|**0.60**|**0.46**|\n|APO|0.52|0.38|\n|OPRO|0.26|0.24|\n|Mutator|0.52|0.34|",
"|Population<br>size|Number of gen-<br>erations|Test score|Validation score|Number of LLM<br>calls|\n|---|---|---|---|---|\n|20|25|0.50|0.42|25000|\n|30|17|0.56|0.50|25500|\n|40|13|0.62|0.46|24500|\n|50|10|0.68|0.46|25000|",
"|Dataset|ETHOS mul-<br>tilabel|MMLU-Pro<br>engineering|MMLU-Pro<br>Business|GPQA|\n|---|---|---|---|---|\n|Initialization|0.28|0.39|0.72|0.38|\n|APO|0.44|0.45|0.73|0.42|\n|OPRO|0.38|0.44|**0.76**|0.43|\n|Mutator|0.40|0.43|0.735|0.43|\n|**OURS**|**0.46**|**0.48**|0.74|0.43|",
"|MMLU-Pro: repartition of questions in the different categories|Col2|\n|---|---|\n|**Category**|**Number of questions**|\n|Math|1351|\n|Physics|1299|\n|Chemistry|1131|\n|Law|1101|\n|Engineering|969|\n|Economics|844|\n|Health|818|\n|Psychology|798|\n|Business|789|\n|Biology|717|\n|Philosophy|499|\n|Computer science|410|\n|History|381|\n|Others|924|\n|Total|12032|",
"|Mutation Type|Obtained Prompt|\n|---|---|\n|Originalprompt|Answer this mathproblem:problem|\n|Expand Instructions|Answer this math problem: problem<br>To solve this problem, follow these detailed step-by-step guidelines:<br>•** Understand the Problem**: Identify what is being asked and determine<br>the type of math problem (e.g., algebra, geometry, calculus).<br>•** Identify Known and Unknown Variables**: List all known values and<br>the variables to solve.<br>•** Develop a Plan**: Choose the correct mathematical approach.<br>•** Execute the Plan**: Carry out calculations accurately, step by step.<br>•** Verify Your Solution**: Substitute the answer back into the problem to<br>check correctness.<br>•** Reflect on the Solution**: Ensure clarity, correctness, and consider al-<br>ternative solutions.<br>**Quality Criteria**: Clarity, correct notation, logical fow.** Evaluation Met-**<br>**rics**: Accuracy, completeness, coherence.** Common Mistakes**: Avoid calcu-<br>lation errors, logicaljumps, and assumptions not supported by theproblem.|\n|Add Expert Persona|You are a Fields Medalist specializing in complex mathematical problem-<br>solving and theoretical mathematics. With a Ph.D. in Mathematics and ex-<br>pertise in algebraic topology, diferential geometry, and number theory, you<br>approach problems with analytical rigor and creative insight.<br>Answer this mathproblem:problem|\n|Task Decomposition|To solve the given math problem, follow these steps:<br>•** Understand the Problem**: Carefully read the statement and identify<br>relevant details.<br>•** Break Down the Problem**: Divide it into smaller, manageable parts.<br>•** Plan a Solution Strategy**: Determine which formulas or theorems<br>apply.<br>•** Execute the Plan**: Perform calculations in a logical sequence.<br>•** Verify Intermediate Results**: Check calculations for accuracy at each<br>step.<br>•** Combine Results**: Integrate partial solutions to derive the fnal answer.<br>•** Review and Validate**: Double-check correctness.<br>Answer the mathproblem:problem|\n|Concise Optimization|Solve:problem|",
"|Mutation Type<br>Structural Variation|Obtained Prompt<br>Task Overview: You are tasked with solving a mathematical problem using<br>a structured approach.<br>Problem Statement:<br>• Math Problem: problem<br>Solution Strategy:<br>• Understand the problem statement.<br>• Identify known variables and constraints.<br>• Determine required formulas or theorems.<br>• Solve systematically step-by-step.<br>• Verify your solution for accuracy.<br>Common Mistakes to Avoid:<br>• Misinterpreting the problem.<br>• Skipping verification steps.<br>• Incorrect formula application.<br>Verification Steps:<br>• Recheck each step for consistency.<br>• Ensure logical correctness.<br>Output Format (JSON):<br>{<br>\"solution\": \"your detailed solution steps\",<br>\"final answer\": \"your final answer\"<br>_<br>}|\n|---|---|\n|Creative Backstory|In the year 2147, aboard the interstellar vessel** Math Explorer**, you are the<br>chief mathematician responsible for ensuring safe passage. The ship’s naviga-<br>tion system has encountered a critical error.<br>Only by solving the following math problem can you recalibrate the system<br>and prevent a catastrophic collision with a rogue asteroid. The fate of the crew<br>and the success of the mission depend on your expertise.<br>Answer this mathproblem:problem|\n|Constraint Addition|Answer this math problem:problem<br>• Do not use any numbers greater than 10.<br>• Explain your solution as if teaching a 10-year-old using biological analo-<br>gies.<br>• Solve within 5 minutes.<br>• Do not include algebraic expressions or terminology.<br>• Present the fnal answer in haiku format.|\n|Role Assignment|You are a team of mathematicians working collaboratively to solve the problem.<br>•** Critic**: Analyze the problem’s complexity and identify possible chal-<br>lenges: problem.<br>•** Problem Solver**: Devise a step-by-step strategy.<br>•** Teacher**: Explain the solution clearly for a novice audience.<br>Your response should integrate insights from each role, providing a thorough<br>yet accessible solution.|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2504.07157v3.pdf"
}