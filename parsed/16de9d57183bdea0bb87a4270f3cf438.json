{
"text": "Joint Prompt Optimization of Stacked LLMs\n                         using Variational Inference\n\n\n\n\n                          Alessandro Sordoniab‚àóXingdi Yuana Marc-Alexandre C√¥t√©a Matheus Pereiraa\n\n                Adam Trischlera Ziang Xiaoa Arian Hosseinib Friederike Niedtnera Nicolas Le Rouxab\n2023                                          Microsoft Research Montr√©ala    MILAb\n                                                 Abstract\nDec                       Large language models (LLMs) can be seen as atomic units of computation map-\n                               ping sequences to a distribution over sequences. Thus, they can be seen as stochastic4\n                             language layers in a language network, where the learnable parameters are the\n                                  natural language prompts at each layer. By stacking two such layers and feeding the\n                               output of one layer to the next, we obtain a Deep Language Network (DLN). We\n                                           first show how to effectively perform prompt optimization for a 1-Layer language\n                            network (DLN-1). Then, we present an extension that applies to 2-layer DLNs\n                           (DLN-2), where two prompts must be learned. The key idea is to consider the[cs.CL]                         output of the first layer as a latent variable, which requires inference, and prompts\n                                  to be learned as the parameters of the generative distribution. We first test the\n                                  effectiveness of DLN-1 in multiple reasoning and natural language understanding\n                                    tasks. Then, we show that DLN-2 can reach higher performance than a single layer,\n                           showing promise that we might reach comparable performance to GPT-4, even\n                        when each LLM in the network is smaller and less powerful. The DLN code is\n                           open source.1\n\n\n                1  Introduction\n\n                  The size of large language models (LLMs) has grown significantly over the last few years, mainly\n                     because of emerging capabilities [5, 31], but at considerable technical and societal costs [49, 2, 4].\n                     Recent efforts have focused either on learning smaller models matching the abilities of larger ones on\n                   some tasks using distillation [43, 36, 29, 13], or offloading part of the computation to other dedicated\n                    components [28, 22, 25, 18]. In the latter case, this is done through carefully crafted instructions toarXiv:2306.12509v2\n                          retrieve the necessary information from these additional modules [48, 41, 6, 54, 24].\n\n                        Instruction-tuned LLMs map an input sequence to a distribution over output sequences conditioned\n                    on an instruction, or prompt. In this paper, we view such LLMs as stochastic language layers, whose\n                       learnable parameters are the prompts. Multiple layers can be stacked to form a Deep Language\n                   Network (DLN) whose learnable parameters are the prompts associated to each layer. Specifically,\n                     each layer uses a template to organize both its prompt and the inputs coming from the layer below\n                         into a single sequence before producing the output (see Figure 1). This layering induces a learnable\n                     decomposition of the task into a series of smaller sub-tasks, each of which might be more easily\n                       solvable by an LLM. This view shares similarities to recent works that chain LLM calls [41, 6, 54].\n                        In this work, we move towards integrating learnable components in the pipeline: each prompt can be\n                       learned to maximize the final objective.\n\n                          ‚àóCorresponding author: alsordon@microsoft.com\n                          1Code: https://github.com/microsoft/deep-language-networks.\n\n\n\n                        37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n\n(A)\n\n   üî•trainable\n                                                       LM\n        frozen\n\n                                                                                                                {{ prompt }}                class.\n                                                                                                                                            template\n                                  negative                      prompt                     {{ input }}\n                                                                                                                Your thoughts were:\n                                      ‚Ä¶ Additionally, consider              {{ hidden }}\n                                                                                 the fact that in some\n                                                                                contexts the day may                 Answer:\n                    LM                       come before the month\n                                                                                                   (e.g. in the UK). For\n                                                                           example, if the given                hidden h\n                                                                                context is ‚ÄúIt is 4/19/1969     The current date is May 30, 2021. One year\n                                      {{ prompt }}                             today. What is the date a      ago from today is May 30, 2020‚Ä¶\n      prompt                                          classiÔ¨Åcation         month ago in\n                                      {{ input }}         template          MM/DD/YYYY?‚Äù, you can ‚Ä¶ For example, if the\n                                                                                                  infer that‚Ä¶\n   sentence is ‚Äúsupported‚Äù,             Answer:              üî•        LM\n   choose ‚Äúpositive‚Äù, and if\n   the sentence is ‚Äúderail‚Äù,\n                                                                                                                  {{                                                                                                                     input                                                                                                                           }}   choose            ‚Äúnegative‚Äù.\n                                                        prompt    Similarly,                      if              the                sentence                                   is                                                                                                                                     hidden\n                                                                                                                  {{                                                                                                                     prompt                                                                                                                            }}   \"peace         and                  stability                   and\n                                                               Decompose                                                                                             the                                                                                      problem                                                                                                                  Let‚Äôs                                                                                                                        think                                                                                                                              step     template\n   prosperity\",             choose\n                                                                      by                                                                                  breaking                                                                                                                                                                  it                                                                        down                                                                                                                  into                                                                                                                  by                                                                                                                     step.                                 input x   \"positive\".            Note                    that\n                                                                                    steps.                                                                                                               First, Ô¨Ånd                                                                                                 the\n   words            like                \"artiÔ¨Åcial\" tend\n                                                                                    date                                                                                      from                                                                                                  the                                 a                                 decade of                                                dramatic                        current   to     have         a negative\n                                                                                given                                                                                           context.                                                                                         Then,                                  economic                                                   decline                                                                                                 input x   sentiment.\n                                                                                      calculate                                                                                          the                                                                             number                                                                                                              of       üî•\n                                                                       days or months to add or      The deadline is Jun 1, 2021, which is 2\n                                                                                   subtract from the current      days away from now. What is the date\n                                                                             date to get the desired        one year ago from today in\n                                                                                                  result. Additionally,‚Ä¶ üî•  MM/DD/YYYY?05/30/2020 \\n (B)\\n06/02/2020...Options\\n(A)\n\n\nFigure 1: Left: An illustration of a DLN-1 performing a sentiment analysis task: input and the\ntrainable prompt are merged using a template and fed to the LM for answer generation. Right: a\nDLN-2 with a residual connection, performing the date understanding task: two prompts need to be\nlearned. In this example, the hidden template extends Chain-Of-Thought [48] with a learnable prefix;\nwe consider the output of the first layer, hidden, as a latent variable h. We use variational inference\nto learn œÄ0, œÄ1. Templates can be considered as an hyperparameter of the network.\n\n\nWe first show how to perform prompt optimization in a shallow 1-layer language network (DLN-1)\nwhich parametrizes a distribution pLM(y|x, œÄ), where x and y are string input and output respectively,\nand œÄ is the learnable prompt (Figure 1, left). Our prompt optimization techniques extend the\nAutomatic Prompt Engineer (APE) procedure from Zhou et al. [57]. We show how our prompts\ncan include a verbalization of difficult examples from the task: the final prompts are a combination\nof instruction directives, akin to zero-shot learning [17], and task examples, akin to in-context\nlearning [20]. This significantly improves downstream performance, surpassing APE on several tasks.\n\nThen, we show how to train a 2-layer DLN (DLN-2), which parametrizes a probability distribution:\n\n                         pDLN-2(y|x) = X pLM(y|h, x, œÄ1) pLM(h|x, œÄ0) ,\n                                    h\n\nwhere h is the string output of the first LLM layer (Figure 1, right). We consider h as a latent variable:\nto maximize the marginal log-likelihood, we formulate a variational inference algorithm that uses an\napproximate posterior over h. Note that this formalism easily encompasses more than two layers.\n\nConsidering outputs of the hidden language layers as latent variables allows us to encompass various\nestablished prompting methods, such as Chain-Of-Thought (CoT) [48] and self-consistency (SC-\nCoT) [46]. Particularly, CoT can be seen as a particular DLN-2 with the first layer prompt set to\n‚ÄúLet‚Äôs think step by step‚Äù and the second layer prompt set to ‚ÄúThe answer is‚Äù; we can\neither learn such prompts or learn a supplement to those as in Figure 1. SC-CoT can be seen as\nmarginalizing over CoT strings sampled from a task-agnostic prior: when using the template in\nFigure 1 (right), our method generalizes this perspective by learning a task-specific prior distribution\nover successful CoTs.\n\nThe rest of the paper is organized as follows. First, we provide an interpretation of LLMs as shallow\nnetworks, drawing a number of analogies with standard parametric and non-parametric models and\nexplaining how best to train them. After exploring their limitations, we propose to stack two such\n\n\n                                        2\n\nLLMs to form a DLN-2. We show how they can be trained using a form of variational inference, then\ndemonstrate their performance on a series of reasoning and language understanding tasks.\n\n2  One-Layer Language Networks\n\nA pre-trained LLM with frozen weights might be thought of as a complete function class indexed by\nprompts. The output y of an LLM for an input x can be modulated through a prompt œÄ by feeding a\ncombination of œÄ and x to the LLM. Hence, from a low-level perspective, the function class of an\nLLM is defined by its architecture, i.e., its depth, number of heads, context size, etc., and training\nhappens at the parameter level. It is data and compute intensive and should be done rarely. From\na high-level perspective, the function class of an LLM is defined by the pre-trained model chosen\n(LLAMA [44], text-davinci-003, GPT-4, etc.), and training happens by fine-tuning the model or by\nchoosing the prompt.\n\nThere are two ways of optimizing an LLM at the prompt level. The first one is prompt engineering,\na parametric optimization, where the optimization space is independent of the size of the dataset.\nBecause this optimization usually happens in discrete space, gradient-based techniques do not apply\nand most efforts rely on a combination of random or local search and human heuristics [21, 55].\nThe second one is in-context learning (ICL), a non-parametric optimization technique where the\nsolution is a direct function of a subset of examples [5, 20]. This approach works well for few-shot\nlearning but scaling it to larger datasets has both performance and computational issues. We shall\nnow generalize previous work in discrete prompt optimization [57, 21] with the ultimate goal of\nlearning a set of prompts in a language network.\n\n\n2.1  Language Layers\n\nWe use language layer to refer to a (stochastic) computation that takes as input a string x and\noutputs a string y. This computation is modulated by another string, œÄ, generally called a prompt\nor instruction [57]. The string transduction is performed by an operator LM, by feeding x and œÄ as\ncontext and generating a continuation y. Templates describe the way x and œÄ are combined prior to\nbeing fed to the LM operator. These are functions that accept strings as variables and output a string.\nWe will denote such templates with this font T. A simple forward template F is the concatenation,\ni.e. F(x, œÄ) = ‚Äú{œÄ}{x}‚Äù. We also explore more complex ones, examples of which can be seen in\nFigure 1.\n\nGiven an input x, a prompt œÄ, and a template F, a language layer defines a probability distribution\npLM(y|F(x, œÄ)) over output strings y as computed by the LM. In the next section, we describe a generic\nframework for optimizing the weights œÄ for a language layer.\n\n\n2.2  Prompt Optimization: Improved APE\n\nBecause the search for the best prompt happens over a discrete space, we will rely on a local search\nmethod, using an LLM to implement a distance measure between prompts. The procedure can be\nseen as an extension of Automatic Prompt Engineer (APE), recently proposed by Zhou et al. [57], and\nwill serve as a stepping stone towards introducing our algorithm for training deep language networks.\nOur prompt optimization algorithm can be structured as follows:\n\n      1. Given the current prompt œÄ and a current batch of examples {x, y}, generate N ‚Äúlocal‚Äù\n         candidates œÄ1, . . . , œÄN using a prompt proposal distribution;\n\n      2. Score each candidate using a (potentially stochastic) scoring function s, then choose œÄ =\n        arg maxœÄn s(œÄn).\n\nPrompt Proposal Local search algorithms assume a distance measure between inputs to crawl the\nsearch space. In this setting, we rely on LLMs to generate local modifications to the prompts. Our\nprompt proposal distribution takes as conditioning information i) the batch given as input to the\nlayer, ii) its corresponding output {x, y, ÀÜy}, and iii) the current prompt œÄ. The proposal distribution\npLM(œÄn|BœÄ({x, y, ÀÜy}, œÄ)) wraps this information using a particular ‚Äúbackward‚Äù template BœÄ, which\ncan be found in Appendix D. This approach is similar to the instruction template used by Zhang et al.\n[55], with the exception that we also integrate information about the model‚Äôs own predictions, which\n\n\n                                       3\n\nAlgorithm 1 One-Layer Language Network (DLN-1) Training Algorithm\nRequire:  ÀÜy ‚àºptLM(y|c)                  ‚ñ∑generates a completion of prefix c with temperature t\nRequire: log pLM(h|c)                                      ‚ñ∑return log-prob of h following c\nRequire: N: prompt samples, I: iterations, D: dataset\nRequire: F: template for the inference/forward pass\nRequire: BœÄ: template for prompt proposal/backward pass.\n  1: Initialize œÄ with a task description or empty\n  2: for i in [1, I] do\n  3:     x, y ‚àºD                                              ‚ñ∑Sample minibatch\n  4:       ÀÜy ‚Üêp0LM(y|F(x, œÄ))                                ‚ñ∑Do inference pass\n  5:     œÄ1, . . . , œÄN ‚àºp0.7LM (œÄ|BœÄ({x, y, ÀÜy}, œÄ))                ‚ñ∑Sample N candidate prompts\n  6:     s1, . . . , sN ‚Üêlog pLM(y|F(x, œÄn))                              ‚ñ∑Score all prompts\n  7:    œÄ ‚Üêarg maxœÄn{s1, . . . , sN}                          ‚ñ∑Select prompt with best score\n  8: end for\n\n\n\nwe found to empirically help performance given that the model tends to propose prompts that correct\nits own errors. We sample from the prompt proposal distribution to generate a set of N prompts. A\nparticularly important aspect is ensuring the diversity of the candidate pool œÄ1, . . . , œÄN. We devise\nseveral strategies to improve the diversity and the usefulness of the candidate samples in Section 4.\n\nPrompt Selection Once a set of N prompts has been generated, we use a scoring function to select\nthe updated prompt. We assume access to the log-likelihoods of the LM operator and we rank the\ncandidate prompts to maximize data log-likelihood œÄ = arg maxœÄn log pLM(y|F(x; œÄn)). In practice,\nwe normalize this log-probability by the length of the output string. While we focus on that metric in\nthis work, there is no restriction on the scoring function that can be used. We use backtracking to\nincrease the robustness of our selection mechanism, as well as a memory of well-performing prompts\nfor efficiency. We present both strategies in Section 4. The sketch of a 1-layer prompt optimization\nalgorithm is described in Algorithm 1, ignoring backtracking and memory for simplicity.\n\nThe results of our prompt optimization may be found in Table 1 and will be discussed in detail in\nSection 5.2. We now turn to extending prompt optimization to architectures with two layers.\n\n\n3  Two-Layer Deep Language Networks (DLN-2)\n\n\nThe natural extension of DLN-1 is DLN-2, in which language layers are stacked, i.e. the output of\nthe first language layer is the input to the second one. A 2-layer network induces a distribution over\noutputs of the form:\n\n                      pDLN-2(y|x) = X pLM(y|Fr(h, x, œÄ1))pLM(h|F(x, œÄ0))                     (1)\n                                 h\n\nwhere h is a latent variable that potentially makes it easier to explain the target y. The output layer\nis also conditioned on x through Fr, forming a residual connection (Figure 1). This formulation is\nreminiscent of past work using latent language representations to guide document summarization [26].\nIn our case, however, the encoding/decoding distributions are parameterized by natural language\nprompts Œ† = {œÄ0, œÄ1}, and we do not assume access to the LLM parameters.\n\nWhile this architecture has more expressive power than a shallow language network, the prompt\noptimization problem becomes harder now that we have to jointly search over both œÄ0 and œÄ1. Doing\nrandom search in this space is impractical [8] and manual tuning of the weights is also exponentially\nharder than with a single prompt. We turn to variational inference to address this issue.\n\n\n3.1  Variational Inference Objective\n\nThe layerwise decomposition of our system allows us to leverage tools from approximate inference\nin probabilistic models to learn Œ†. In particular, we propose to use variational inference to learn\nŒ† [3, 16]. We posit an approximate posterior q(h) over the latent variable h, and bound the marginal\n\n\n                                       4\n\nlog-likelihood of y given x by computing the ELBO:\n\n         log pDLN-2(y|x) ‚â• X q(h) [log pLM(y|Fr(h, x, œÄ1))pLM(h|F(x, œÄ0))] + H [q(h)] ,       (2)\n                         h\n\nwhich allows us to decompose the optimization over Œ† in two independent optimization problems,\nover both œÄ0 and œÄ1:\n\n   œÄ‚àó0 = arg max X wh log p(h|F(x, œÄ0)), œÄ‚àó1 = arg max X wh log p(y|F(h, x, œÄ1)) .    (3)\n               œÄ0                                        œÄ1\n                      x, h                                               (x,y), h\n\nThe search over œÄ1 is identical to the prompt optimization described in Section 2, with the difference\nthat the inputs now depend on the approximate posterior samples h in addition to the inputs x. The\nsearch over œÄ0 uses a similar strategy but uses the posterior samples h as targets, instead of y.\n\nAlthough this bound allows us to decompose the optimization w.r.t. Œ†, it is only useful if it is close\nto the true value. Since its looseness is the KL divergence between q(h) and the true posterior,\nKL(q(h)||p(h|y, x)): we need to find an approximate posterior q closely matching the true posterior.\nIn what follows, we specify how we parametrize the approximate posterior and how we tighten the\napproximation via posterior sharpening.\n\nHidden Proposal We will also be using an LLM to sample candidate hidden states from q(h).\nUnless specified, for simplicity, we use the same LM operator used in our language layers. The\napproximate posterior can condition on arbitrary amount of information but especially useful might\nbe to condition on the true target y. If it conditions on the hidden state coming from the ‚Äúforward\npass‚Äù, ÀÜh ‚àºpLM(h|F(x, œÄ0)), then qedit(h) = pLM(h|Bh(ÀÜh, y, œÄ1)). Bh is a specifically tailored hidden\nproposal template (Appendix D). qedit performs a sort of edit operation, where the LM is tasked to\nrewrite the hidden variable ÀÜh given some extra knowledge of the ground-truth label y and of œÄ1.\nAlternatively, we can set the posterior to be equal to the prior, i.e. qpri(h) = pLM(h|F(x, œÄ0)), or the\nprior with additional information about the label y, qpri+(h) = pLM(h|By(x, œÄ0, y)) (Appendix D).\nThis amounts to re-computing the hidden state knowing privileged information about the label. We\nfound most effective to sample hidden states from a mixture of qpri and qpri+.\n\nPosterior Sharpening Given the absence of learnable parameters in q(h), the induced approximate\nposterior might still be far from the true posterior. To bridge this gap, we reweigh each sample\nhi based on its probability under the true posterior distribution. More precisely, we compute\nÀúwi = log pLM(y|Fr(hi, x, œÄ1)) + log pLM(hi|F(x, œÄ0)), then assign to each hi the probability wi =\nexp(Œ± Àúwi)/ Pj exp(Œ± Àúwj), where Œ± is a tunable temperature parameter that controls the entropy of\nthe posterior weights. The full algorithm for training a DLN-2 is presented in Algorithm 2.\n\n4  Practical Instantiation\n\nAlthough our method aim to learning prompts in stacked LLM architectures, we do rely on a good\namount of prompt engineering for our templates.  Hereafter, we detail some choices that were\nfundamental to make our approach work in practice.\n\nProposal Diversity To ensure a diversity of the samples for both the prompt proposal distribution, we\nfound helpful to use two strategies. The first is to modify the backward templates BœÄ before drawing a\nsample from the proposal distribution pLM(œÄ). To achieve so, we parametrize the basic templates with\na ‚Äú{message}‚Äù variable that we instantiate from a pool of hand-written instructions, that describe\ndifferent behaviors the model should follow to propose new œÄ, e.g. ‚Äúshorten the previous\ninstruction‚Äù, ‚Äúgive useful examples‚Äù, etc. These can be interpreted as meta-instructions, i.e.\nhigh-level directives that inform the model on how to create a better instruction for the task, and\nextend instruction-induction templates used in [12, 55]. These can be found in Appendix D. In the\nfuture, we could envision to extend learning to these instructions. In the case of BœÄ, they could\nfunction as parameters for a prior over the weights of the DLN. The second strategy to ensure more\ndiversity is that we instantiate BœÄ with a different random subset of examples in the current batch,\nbefore drawing each sample œÄn. This effectively modifies the generation context for each sample œÄn.\n\nLearning In-Context Learning One strategy we found particularly effective is to integrate in the\npool of meta-instructions an additional instruction that asks the LM to give useful examples to improve\n\n\n                                       5\n\nAlgorithm 2 Two-Layer Deep Language Network (DLN-2) Training Algorithm\n  1:  ÀÜy ‚àºptLM(c)                          ‚ñ∑generates a completion of prefix c with temperature t\n  2: log pLM(h|c)                                            ‚ñ∑return log-prob of h following c\n  3: N: prompt samples, K: posterior samples, I: iterations, D: dataset\n  4: F, Fr: templates for the inference/forward pass without and with residual connection\n  5: BœÄ, Bh: templates for prompt and hidden proposal/backward pass.\n  6: Initialize œÄ0, œÄ1 with a generic sentence or task description\n  7: for i in [1, I] do\n  8:     x, y ‚àºD                                              ‚ñ∑Sample minibatch\n  9:       ÀÜh ‚Üêp0LM(F(x, œÄ0))                          ‚ñ∑Do inference pass, first layer\n10:       ÀÜy ‚Üêp0LM(Fr(ÀÜh, x, œÄ1))                      ‚ñ∑Do inference pass, second layer\n11:     h1, . . . , hK ‚àºp0.7LM (Bh(ÀÜh, x, y, œÄ1, œÄ0))           ‚ñ∑Sample K posterior proposals for h\n12:    Œ±1, . . . , Œ±K ‚Üêlog pLM(hk|F(x, œÄ0))       ‚ñ∑Compute prior log-probs for all hk samples\n13:    Œ≤1, . . . , Œ≤K ‚Üêlog pLM(y|Fr(hk, x, œÄ1))     ‚ñ∑Compute log-likelihoods for all hk samples\n14:    qk ‚Üêexp(Œ±k + Œ≤k)/(Pk exp(Œ±k + Œ≤k))      ‚ñ∑Compute normalized posterior weights\n15:   h‚àó‚Üêarg maxh{q1, . . . , qK}                            ‚ñ∑Select best posterior sample\n16:    œÄ10, . . . , œÄN0 ‚àºp0.7LM (BœÄ({x, ÀÜh, h‚àó}, œÄ0))           ‚ñ∑Sample N candidate prompts for œÄ0\n17:    œÄ11, . . . , œÄN1 ‚àºp0.7LM (BœÄ({ÀÜh, ÀÜy, y}, œÄ1))            ‚ñ∑Sample N candidate prompts for œÄ1\n18:     s10, . . . , sN0 ‚ÜêPk qk log pLM(hk|F(x, œÄn0 ))        ‚ñ∑Compute ELBO for all prompts œÄn0\n19:     s11, . . . , sN1 ‚ÜêPk qk log pLM(y|Fr(hk, x, œÄn1 ))     ‚ñ∑Compute ELBO for all prompts œÄn1\n20:    œÄ0 ‚Üêarg maxœÄn0 {s10, . . . , sN0 }                       ‚ñ∑Select prompt œÄ0 with best score\n21:    œÄ1 ‚Üêarg maxœÄn1 {s11, . . . , sN1 }                       ‚ñ∑Select prompt œÄ1 with best score\n22: end for\n\n\n\n\n\nits current prompt œÄ. Empirically, we observed that this allows the model to sample candidate\nprompts œÄn that contain synthetic examples for the task, embedded in natural language. Examples\nof this interesting behavior can be found in Appendix F. We found that this behavior is particularly\ninteresting as the resulting prompts often perform better than standard ICL. We hypothesize this is\ndue to both i) the ‚Äúverbalization‚Äù of the example in the prompt, which modifies the dataset syntax\ninto a more suitable one, and ii) the fact that the model can dynamically select which examples are\nmost important to integrate in the prompts, given the errors made during training. Therefore, we\nsuspect that DLN achieves a similar effect to recent techniques that select important examples for\nICL [20, 35, 40, 18, 47] with the improvement of naturally conditioning the selection on the end task\nperformance via end-to-end training.\n\nBacktracking and Memory Optimization of both DLN-1 and DLN-2 is challenging due to the fact\nthat we do not have gradient information and we sample a restricted set of candidates œÄn at each\noptimization step due to computational reasons. We deploy multiple strategies to allow the network to\nbe robust to sampling/selection errors. First, we include the current prompt œÄ into the set of candidate\nprompts to be scored at the current iteration œÄn. This allows the model to not take the step if the\nprevious prompt performed better. Second, we keep a memory of M = 5 best prompts found by\ntracking validation set performance.\n\nExploration Reward When training a DLN-2, we empirically observed that the first layer prompt\nœÄ0 was updating very slowly. Due to the fact that the approximate posterior shares templates with\nthe prior used in the forward pass, the posterior samples hi are close to ÀÜh and the maximizer of\nEquation (3) remains œÄ0. To address this issue, we add to the scores of each candidate prompt\nan exploration reward that is proportional to the negative log-probability of those ÀÜh that led to an\nincorrect prediction: r = ‚àíŒª log pLM(ÀÜh|F(x, œÄn)), if ÀÜy Ã∏= y. This encourages the model to both find\nprompts that maximize the log-probability of high-probability posterior samples and at the same\ntime minimize the log-probability of prior samples that led to incorrect predictions. We anneal Œª\nto 0 during training with a constant schedule and we select the initial Œª by monitoring validation\nperformance for each task.\n\n\n                                       6\n\n5  Experiments and Results\n\nWe design and conduct a set of experiments to help answer two main research questions:\n\n ‚Ä¢ Q1: Can we outperform APE and In-Context Learning (ICL) with a DLN-1?\n ‚Ä¢ Q2: Does network depth provide further improvement upon DLN-1?\n\n5.1  Experimental Setup\n\nDatasets and Tasks We adopt a set of nine NLP and reasoning tasks commonly used in prior\nwork studying zero- or few-shot learning capabilities of LLMs [23, 10, 39, 42, 1]. We focus on\nclassification tasks. For tasks adopted from BigBench-Hard (BBH) [42] (Hyper., Nav., Date. and\nLogic.72), we use the 250 data points provided by BBH as test set. We take the remaining data points\nfrom BigBench [39] that were not included in BBH, and randomly split them (evenly) into training\nand validation sets. For tasks adopted from [23] (Mpqa, Trec, and Subj), we randomly sample 400\nand 250 data points from their training and test sets, respectively. We use the original validation sets.\nFor tasks adopted from Leopard [1] (Disaster and Airline), we randomly sample 400, 250, and 250\ndata points as training, valid, and test. We list all tasks and their statistics in Table 3 in the Appendix.\n\nWe use accuracy as the evaluation metric. Specifically, given an input, we compare a system‚Äôs output\nstring against the ground-truth output string provided by the dataset. We score 1 if the two strings\nare identical and 0 otherwise. Before the comparison, we process the strings from both the model\noutput and the ground-truth to deal with issues like tokenization and capitalization. In all our DLN\nexperiments, we perform a hyperparameter search and run the same hyperparameter setting with\nthree random seeds. We report the test accuracy averaged over three seeds corresponding to the\nhyperparameter setting that achieves the highest average validation accuracy. We report details of the\nhyperparameter search in the Appendix I.\n\nThroughout this paper, we use OpenAI‚Äôs models, specifically GPT-3 (text-davinci-003) and GPT-4, as\nthe backbone to our proposed systems unless otherwise specified. For DLNs, we use a batch size of\n20 and train for 20 iterations by early-stopping on validation performance evaluated every 2 iterations.\nWe then report test scores. We sample N = 20 prompt proposals and K = 5 hidden samples.\n\nBaselines We compare the DLN against two classes of baseline systems. First, we test a set of\nsystems equipped with the same backbone (i.e., GPT-3):\n\n ‚Ä¢ 0-shot: Given an input, the LLM is required to generate the answer in a zero-shot manner.\n ‚Ä¢ 5-shot (ICL): Given an input as well as five data points as in-context examples, the LLM is queried\n   to generate an answer. The five examples are randomly sampled from the training set.\n ‚Ä¢ KATE [20]: Given an input, we retrieve the five most similar data points from the training set using\n  an off-the-shelf sentence encoder, and use them as in-context examples.\n ‚Ä¢ APE [57]: The LLM is queried to generate a pool of candidate prompts for the task given few\n  input-output pair examples. The candidate prompts are evaluated on a validation set to find the\n   best performing instruction prompt. The best instruction is then used for 0-shot evaluation. We\n  optimize the prompt over both 15 and 400 examples (APE-15 and APE-400 respectively).\n ‚Ä¢ CoT [48]: Given an input, the LLM is first queried to generate a reasoning path with the prompt\n  ‚ÄúLet‚Äôs think step by step‚Äù. Then, conditioned on the input and its first output, the LLM\n   is queried to generate an answer. This is the zero-shot version of CoT and is a natural baseline\n   for DLN-2: it performs two LLM calls and can be seen as DLN-2 without optimization. We will\n   report performance of this baseline when comparing to DLN-2.\n\nAdditionally, we compare against one of the most advanced LLMs to date, GPT-4. We test 0-shot and\nICL settings with GPT-4.\n\n5.2  DLN-1\n\nOur first set of experiments evaluates the 1-layer language network (DLN-1) described in Section 2.\nTable 1 presents results on the full suite of test tasks. We see that it matches the performance of\n\n   2We only use the variant with seven objects.\n\n\n                                       7\n\nTable 1: Test accuracy averaged over three random seeds of a shallow, 1-layer language network\n(DLN-1) compared to baselines both on GPT-3 and GPT-4. For trainable systems (i.e., APE and\nDLN-1) or systems relying on GPT-4, we report the 95% confidence interval.\n\n\n                           BigBench Hard                     NLU                     Leopard\n\n  Method     Hyper.     Nav.         Date.        Logic.7    Mpqa      Trec       Subj        Disaster     Airline\n\n  GPT-3\n   0-shot       60.8        64.1         56.4         45.9         88.0        61.9        61.7        81.6         75.6\n   5-shot       55.6        56.5         62.1         36.7         87.2        80.0        76.4        81.2         82.7\n  KATE       71.1        56.9         61.1         44.4         88.4        77.6        71.1        76.0         81.6\n  APE-15     68.5¬±5.5    67.3¬±7.7     32.1¬±28.6    45.5¬±4.7     85.5¬±4.6    71.3¬±5.5    61.3¬±7.2    54.8¬±14.6    83.5¬±3.5\n  APE-400    65.5¬±4.7    56.9¬±32.9    23.5¬±14.1    45.6¬±12.4    84.9¬±9.7    72.0¬±1.7    63.7¬±9.2    60.3¬±37.4    82.3¬±10.0\n\n  DLN-1      91.9¬±3.0    68.5¬±4.7     55.7¬±4.5     47.5¬±2.1     88.5¬±2.5    89.7¬±3.2    83.2¬±6.5    81.7¬±6.5     83.2¬±5.5\n\n  GPT-4\n   0-shot       64.0¬±1.0    74.0¬±1.0     79.2¬±2.6     68.5¬±3.5     86.3¬±0.6    64.8¬±1.7    72.5¬±1.5    47.7¬±0.6     84.5¬±0.6\n   5-shot       88.4¬±2.6    75.7¬±1.5     79.3¬±1.1     62.8¬±1.7     88.0¬±3.0    82.5¬±3.8    94.7¬±3.5    63.6¬±8.5     88.0¬±1.0\n   16-shot     93.3¬±2.3    75.5¬±5.1     80.9¬±5.0     66.4¬±3.6     91.3¬±1.5    83.7¬±0.6    96.5¬±2.5    67.1¬±4.0     88.3¬±2.1\n\n  DLN-1      95.2¬±5.0    77.1¬±4.7     76.7¬±3.0     69.1¬±2.5     91.1¬±3.2    89.5¬±2.1    93.1¬±5.0    82.1¬±3.8     85.9¬±1.5\n\n\n\n\n DLN-1 prompt on Hyperbaton (GPT-3)\n\n When constructing a sentence with multiple adjectives, the order should be opinion, size, age, shape, color, origin, material,\n  and purpose. Adjectives of the same type should be listed in descending order from largest to smallest. When adjectives of\n  different types are used, the order should be opinion, size, age, shape, color, origin, material, and purpose. For example, in\n  the phrase ‚Äúmassive ancient chair‚Äù size (massive) should come before age (ancient). Examples: little old-fashioned Russian\n  silver rectangular ship; silly large old leather hiking chair; brand-new spherical Mexican sweater; enormous old spherical green\n  Nigerian exercise car; medium-size triangular wool eating ship; good square brown Egyptian ship; lovely massive drinking\n  monkey; archaic circular white plastic shoe. In each of the following examples, the adjective order is wrong. Identify the correct\n  adjective order:\n\n\n DLN-1 prompt on Hyperbaton (GPT-4)\n\n  To determine the correct adjective order, follow this sequence: opinion, size, shape, age, color, origin, material, and purpose.\n  For example, choose \"large red plastic ball\" over \"red large plastic ball\" since it follows the order: size (large), color (red),\n  and material (plastic). Not all adjectives may be present, but the order should still be maintained. If the options are \"ancient\n  prismlike white leather whittling match\" and \"leather white ancient prismlike whittling match\", choose the first option, as it\n  follows the order: age (ancient), shape (prismlike), color (white), material (leather), and purpose (whittling). Remember that\n  opinion always comes before age, so \"obnoxious old-fashioned typing shoe\" is correct over \"old-fashioned obnoxious typing\n  shoe.\" Ensure opinion adjectives come before other adjectives in the sequence. When comparing options, follow the order of\n  adjectives for each category: size before color, color before origin, and so on. In cases where purpose and material adjectives\n  are switched, like \"paper walking monkey\" vs \"walking paper monkey\", choose the option where material comes before the\n  purpose. Additionally, always prioritize the given sequence over the position of adjectives in the sentences. For example,\n  choose \"midsize brand-new gray Chinese wood sweater\" over \"Chinese brand-new gray midsize wood sweater\" as it follows the\n  order: size (midsize), age (brand-new), color (gray), origin (Chinese), and material (wood).\n\n\nFigure 2: The final prompt of the DLN-1 on Hyperbaton includes not only instructions but also\nexamples from the training set. These samples were automatically chosen by the prompt optimization.\nIn a way, this approach combines in-context learning and prompt optimization.\n\n\n\n\nthe best GPT-3-based method on Disaster, Mpqa and Airline and narrowly beats the best GPT-3\nbaseline on Logic.7 and Nav.. On Hyper., Trec, and Subj, DLN-1 significantly outperforms the\nbest GPT-3 baseline (by about 20, 10, and 7 percentage points, respectively). On Hyper., Trec, and\nDisaster, it even surpasses GPT-4 baselines, unsurprisingly underperforming GPT-4 on all other\ntasks. DLN-1‚Äôs excellent performance on Hyper., a BBH task about ordering adjectives according\nto linguistic convention, is a surprise. To better understand this result, we show the final prompt\nin Figure 2. We see that the prompt contains both instructions and a list of examples from the\ntraining set. These examples were automatically chosen by the optimizer based on their impact on the\nperformance. This can be seen as a combination of KATE, which selects training examples to put in\ncontext based on their similarity with the test example, and APE, which selects the prompt based on\nits performance. On Date., DLN-1 tends to systematically under-perform the 0-shot baseline both for\nGPT-3 and GPT-4. We observed that DLN-1 overfits due to paucity of examples in the validation set.\n\n\n                                       8\n\nTable 2: DLN-2 test accuracy using GPT-3 as LLM.\n\n           Method    Nav.       Date.     Logic.7   Disaster  Subj\n\n                0-shot      64.1       56.4      45.9      81.6       61.7\n           CoT       69.3       72.4      41.1      54.4       59.3\n          APE        67.3¬±7.7     32.1¬±28.5   45.5¬±4.7    54.8¬±14.6   61.3¬±7.2\n            APE-400   56.9¬±32.9   23.5¬±14.1   45.6¬±12.4   60.3¬±37.4   63.7¬±9.2\n\n           DLN-1     68.5¬±4.7     55.7¬±4.5    47.5¬±2.1    81.7¬±6.5    83.2¬±5.5\n           DLN-2      83.1¬±24.7    75.2¬±14.8   45.7¬±3.5    82.8¬±2.5    85.9¬±8.7\n\n\n\n5.3  DLN-2\n\nWe investigate the effectiveness of depth through experiments with 2-layer language networks (DLN-\n2) on tasks where we expect depth to be most useful, and on which DLN-1 significantly underperforms\nthe GPT-4 0-shot baseline, i.e., Nav., Date., and Logic.7 [42]. Since the Nav., Date. and Logic.7 tasks\nfrom BBH require more complex spatial and temporal reasoning, they are the ones where we most\nexpect a decomposition into subtasks to be helpful. We also include Subj and Disaster as an example\nwhere DLN-1 performs well (even outperforming the GPT-4 0-shot baseline), since we are interested\nto see to what extent DLN-2 can further push performance.\n\nResults for DLN-2 can be found in Table 2. Compared to DLN-1, DLN-2 provides an average boost\nof 7.2% absolute score. On Nav. and Date., DLN-2 largely improves the performance of DLN-1,\noutperforming all single layer networks. On Logic.7, all methods appear to perform similarly. This\ncould point to the fact that the task might be too hard for the base LLM and thus highlights the\nlimits of prompt optimization of a weak base model. On Subj and Disaster, DLN-2 achieves further\nimprovement over DLN-1. Compared to 0-shot GPT-4 results in Table 1, on Subj and Disaster,\nDLN-2 on average provides more than 20% in absolute improvement. We encourage readers to find\nadditional experimental results in Appendix C.\n\n\n6  Related Work\n\nPrompt-Based Machine Learning GPT-3 [5] launched a new paradigm in NLP called in-context\nlearning (ICL), now applied beyond traditional NLP tasks [21]. The discovery of chain-of-thought\nprompts (CoT) marked a major advance in prompting: LLM performance improves markedly when\nthe prompt includes examples of intermediate reasoning steps [48] (few-shot CoT), or simply instructs\nthe model to ‚Äúthink step by step‚Äù [17] (zero-shot CoT). Like CoT, DLNs break a problem down into\nintermediate steps but they operationalize these steps as separate LLM calls, each defined by its\nown learned prompt. Since the introduction of CoT, prompting techniques have evolved to be more\ndynamic and iterative. Recent methods often operate recursively. Examples include RECITE [41],\nSelf-ask [33], and related methods for question-answering Creswell et al. [6], Zhou et al. [56].\nA similar class of methods relies on ‚Äúintrospection‚Äù [14], where an LLM is prompted to ingest,\nevaluate then possibly act on its own previous output. Self-critique [46], ReAct [54], Reflexion [38],\nSelf-refine [24] fit this mould along with Hao et al. [11], Du et al. [9], Yao et al. [53].\n\nPrompt Optimization Techniques based on notions of self-talk and self-evaluation align naturally\nwith automatic prompt optimization‚Äîa core function in DLNs. Early work in this category includes\nAutoprompt [37] and GRIPS [32]. Deng et al. [7] argue that ‚Äòenumeration-then-selection‚Äô heuristics\nfor optimizing discrete prompts do not explore the prompt space systematically. They take an RL\napproach to overcome this problem, training a policy network, via soft Q-learning with a suitably\ndesigned and stabilized reward function, to generate effective prompts. Through Gibbs sampling,\nReprompting [52] iteratively searches CoT recipes to improve prompt performance automatically.\nMost relevant to DLNs, Zhou et al. [57] present Automatic prompt engineer (APE). APE optimizes\nan initial prompt by searching over a pool of candidates to maximize a score function. We use an\nAPE-inspired approach in DLNs and we cast the proposal/scoring functions as elements of variational\ninference. In a concurrent work, Pryzant et al. [34] proposed using textual gradients in automatic\nprompt optimization. This algorithm uses LLM‚Äôs nonparametric feedback to guide prompt generation\nand selection.\n\n\n                                       9\n\nMulti-Layer LLM systems Several recent works compose LLMs as nodes in a computational\ngraph, which is the core idea of DLNs. Some work cited above can be seen as instances of this\nidea. Similarly, Khot et al. [15] induce an LLM to generate a basic ‚Äúcontrol flow‚Äù that calls distinct\nLLM modules. Wu et al. [50] propose AI chains, an interactive system of chained LLMs based on\na set of ‚ÄúLLM primitive‚Äù operations. They conduct a 20-person user study in which participants\nmodify chains, and find this process to improve task performance, transparency, and controllability.\nDohan et al. [8] unify LLMs and graphical models as ‚Äúlanguage model cascades‚Äù. Specifically, they\ncast LLM compositions as graphical models with string-valued random variables.3 They show how\nscratchpad [30], chain-of-thought [48], tool use [25], and several other prompting strategies fit their\nformalism. DLNs can likewise be considered an instance of language model cascade, because of that\nframework‚Äôs generality. However, going beyond the conceptual work of Dohan et al. [8], we present\nan effective technique for doing inference in an LLM-based graphical model and we apply learned\nnetworks of LLMs to several downstream tasks.\n\n\n7  Conclusion and Future Work\n\n\nIn this paper we introduced an algorithm for joint prompt optimization in deep networks where\neach layer is an LLM. To do so, we consider outputs of each hidden LLM layer as a latent variable\nwe need to do inference over. From a conceptual perspective, we demonstrated how CoT can be\nseen as a DLN-2 with a residual connection.  Similarly, Generated Knowledge Prompting [19]\ncould be considered as a fixed forward-only DLN-2 where, in the first layer, an LLM generates\nrelated knowledge, and in the second layer, another LLM takes the generated knowledge as input\nand generates the final answer. Other prompting techniques like ReAct [54], Reflexicon [38], and\nSelf-Consistency [46] could all be ensembles of DLN-1s with different prompt initializations.\n\nAlthough we only tested 1-layer and 2-layer LNs so far, we already show that the performance of\nsmaller LLMs can be boosted when stacked and prompted properly. We believe the modularity of\nthese architectures will make them more adaptable and reusable to new use cases. While accuracy on\ndownstream tasks is an appealing metric, we argue that other considerations are just as important,\nfor example the ease of adapting a model to one‚Äôs own use case, or the ability to leverage multiple\nexisting models.\n\nWe noticed that GPT-3 has a tendency to always produce an answer given an example: this could be\ndue to the particular 0-shot fine-tuning procedure, which biases the model towards generating useful\nresponses. This raises the question of whether we can fine-tune ‚Äústackable‚Äù LLMs and whether DLNs\ncan be used as a framework to generate training data for that purpose. Second, we engineered our\nbackward and forward templates; in the future, we wish to expand our work to learn parts of such\ntemplates: we expect this to make the variational bound tighter and thus easing DLN‚Äôs optimization.\nAdditionally, while we only proposed 2-layer DLNs, the framework accommodates arbitrary directed\nacyclic graphs.\n\n\nImpact statement  While we are fully aware of the limitations of addressing societal issues through\ntechnical work, we hope that modular approaches like ours will alleviate some of the issues associated\nwith LLMs, like the concentration of power associated with the difficulty to train them. We also\nhope that, by facilitating the reusability and adaptivity of such models, we shall make them more\namenable to a wider variety of use cases. However, while we discuss the performance of these\nmodels on artificial benchmarks, we do not address the question of when and how such models\nshould be deployed, nor do we offer additional guarantees against their misuse. We also emphasize\nthat performance on artificial tasks, even if realistic, is neither representative of performance in\nuncontrolled environments, nor enough to justify the deployment of these models in high stakes\nsituations.\n\n\nAcknowledgements  We would like to acknowledge Silviu Pitis for the useful feedback on the draft,\nNikolay Malkin and Tong Wang for their advice during the first steps of this project.\n\n\n    3Earlier work by Miao and Blunsom [27] also treated strings as random variables.\n\n\n                                       10\n\nReferences\n\n[1] Bansal, T., Jha, R., and McCallum, A. (2020).  Learning to few-shot learn across diverse\n   natural language classification tasks. In Proceedings of the 28th International Conference on\n   Computational Linguistics, pages 5108‚Äì5123, Barcelona, Spain (Online). International Committee\n  on Computational Linguistics.\n\n[2] Bender, E. M., Gebru, T., McMillan-Major, A., and Mitchell, M. (2021). On the dangers of\n   stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference\n  on fairness, accountability, and transparency, pages 610‚Äì623.\n\n[3] Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. (2017). Variational inference: A review for\n   statisticians. Journal of the American statistical Association, 112(518):859‚Äì877.\n\n[4] Blodgett, S. L., Liao, Q. V., Olteanu, A., Mihalcea, R., Muller, M., Scheuerman, M. K., Tan, C.,\n  and Yang, Q. (2022). Responsible language technologies: Foreseeing and mitigating harms. In\n  Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, CHI\n  EA ‚Äô22, New York, NY, USA. Association for Computing Machinery.\n\n[5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,\n  Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances\n   in neural information processing systems, 33:1877‚Äì1901.\n\n[6] Creswell, A., Shanahan, M., and Higgins, I. (2022).  Selection-inference: Exploiting large\n   language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712.\n\n[7] Deng, M., Wang, J., Hsieh, C., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E. P., and Hu, Z.\n   (2022). Rlprompt: Optimizing discrete text prompts with reinforcement learning. In Goldberg, Y.,\n   Kozareva, Z., and Zhang, Y., editors, Proceedings of the 2022 Conference on Empirical Methods\n   in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December\n   7-11, 2022, pages 3369‚Äì3391. Association for Computational Linguistics.\n\n[8] Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski,\n   H., Saurous, R. A., Sohl-Dickstein, J., et al. (2022). Language model cascades. arXiv preprint\n   arXiv:2207.10342.\n\n[9] Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., and Mordatch, I. (2023). Improving factuality and\n   reasoning in language models through multiagent debate.\n\n[10] Gao, T., Fisch, A., and Chen, D. (2021). Making pre-trained language models better few-\n   shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational\n   Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n   1: Long Papers), pages 3816‚Äì3830, Online. Association for Computational Linguistics.\n\n[11] Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. (2023). Reasoning with\n   language model is planning with world model. arXiv preprint arXiv:2305.14992.\n\n[12] Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. (2022). Instruction induction: From\n  few examples to natural language task descriptions.\n\n[13] Hosseini, A., Reddy, S., Bahdanau, D., Hjelm, R. D., Sordoni, A., and Courville, A. C. (2021).\n   Understanding by understanding not: Modeling negation in language models.  In Toutanova,\n   K., Rumshisky, A., Zettlemoyer, L., Hakkani-T√ºr, D., Beltagy, I., Bethard, S., Cotterell, R.,\n   Chakraborty, T., and Zhou, Y., editors, Proceedings of the 2021 Conference of the North Ameri-\n  can Chapter of the Association for Computational Linguistics: Human Language Technologies,\n  NAACL-HLT 2021, Online, June 6-11, 2021, pages 1301‚Äì1312. Association for Computational\n   Linguistics.\n\n[14] Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch,\n    I., Chebotar, Y., et al. (2022). Inner monologue: Embodied reasoning through planning with\n   language models. arXiv preprint arXiv:2207.05608.\n\n\n                                       11\n\n[15] Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. (2023).\n  Decomposed prompting: A modular approach for solving complex tasks. International Conference\n  on Learning Representations.\n\n[16] Kingma, D. P. and Welling, M. (2022). Auto-encoding variational bayes.\n\n[17] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large language models\n   are zero-shot reasoners. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh,\n   A., editors, Advances in Neural Information Processing Systems, volume 35, pages 22199‚Äì22213.\n   Curran Associates, Inc.\n\n[18] Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. (2022). Internet-augmented\n   language models through few-shot prompting for open-domain question answering. arXiv preprint\n   arXiv:2203.05115.\n\n[19] Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Le Bras, R., Choi, Y., and Hajishirzi, H. (2022a).\n   Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual\n  Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3154‚Äì\n   3169.\n\n[20] Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. (2021). What makes good\n   in-context examples for gpt-3?\n\n[21] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023). Pre-train, prompt,\n  and predict: A systematic survey of prompting methods in natural language processing. ACM\n  Computing Surveys, 55(9):1‚Äì35.\n\n[22] Liu, R., Wei, J., Gu, S. S., Wu, T.-Y., Vosoughi, S., Cui, C., Zhou, D., and Dai, A. M.\n   (2022b). Mind‚Äôs eye: Grounded language model reasoning through simulation. arXiv preprint\n   arXiv:2210.05359.\n\n[23] Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. (2022). Fantastically ordered\n  prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings\n   of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\n   Papers), pages 8086‚Äì8098, Dublin, Ireland. Association for Computational Linguistics.\n\n[24] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N.,\n  Prabhumoye, S., Yang, Y., Welleck, S., Majumder, B. P., Gupta, S., Yazdanbakhsh, A., and Clark,\n   P. (2023). Self-refine: Iterative refinement with self-feedback.\n\n[25] Mialon, G., Dess√¨, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozi√®re, B.,\n   Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., Grave, E., LeCun, Y., and Scialom, T. (2023).\n  Augmented language models: a survey.\n\n[26] Miao, Y. and Blunsom, P. (2016a). Language as a latent variable: Discrete generative models for\n   sentence compression. In Proceedings of the 2016 Conference on Empirical Methods in Natural\n  Language Processing, pages 319‚Äì328, Austin, Texas. Association for Computational Linguistics.\n\n[27] Miao, Y. and Blunsom, P. (2016b). Language as a latent variable: Discrete generative models for\n   sentence compression. In Proceedings of the 2016 Conference on Empirical Methods in Natural\n  Language Processing, pages 319‚Äì328.\n\n[28] Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer,\n   L. (2022). Rethinking the role of demonstrations: What makes in-context learning work?  In\n   Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages\n  11048‚Äì11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n[29] Mukherjee, S. and Awadallah, A. H. (2020). Xtremedistil: Multi-stage distillation for massive\n   multilingual models. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. R., editors, Proceedings\n   of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,\n   July 5-10, 2020, pages 2221‚Äì2234. Association for Computational Linguistics.\n\n\n                                       12\n\n[30] Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan,\n   D., Lewkowycz, A., Bosma, M., Luan, D., et al. (2021). Show your work: Scratchpads for\n   intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n\n[31] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal,\n   S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human\n   feedback. Advances in Neural Information Processing Systems, 35:27730‚Äì27744.\n\n[32] Prasad, A., Hase, P., Zhou, X., and Bansal, M. (2022).  Grips: Gradient-free, edit-based\n   instruction search for prompting large language models. arXiv preprint arXiv:2203.07281.\n\n[33] Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. (2022). Measuring and\n   narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.\n\n[34] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and Zeng, M. (2023). Automatic prompt\n   optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495.\n\n[35] Rubin, O., Herzig, J., and Berant, J. (2022). Learning to retrieve prompts for in-context learning.\n   In Proceedings of the 2022 Conference of the North American Chapter of the Association for\n  Computational Linguistics: Human Language Technologies, pages 2655‚Äì2671.\n\n[36] Sanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert, a distilled version of BERT:\n   smaller, faster, cheaper and lighter. CoRR, abs/1910.01108.\n\n[37] Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. (2020). Autoprompt: Eliciting\n  knowledge from language models with automatically generated prompts.  In Proceedings of\n   the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n  4222‚Äì4235.\n\n[38] Shinn, N., Labash, B., and Gopinath, A. (2023). Reflexion: an autonomous agent with dynamic\n  memory and self-reflection. arXiv preprint arXiv:2303.11366.\n\n[39] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R.,\n   Santoro, A., Gupta, A., Garriga-Alonso, A., et al. (2022). Beyond the imitation game: Quantifying\n  and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.\n\n[40] Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, J., Zhang, R., Ostendorf, M., Zettlemoyer,\n   L., Smith, N. A., et al. (2023).  Selective annotation makes language models better few-shot\n   learners. International Conference on Learning Representations.\n\n[41] Sun, Z., Wang, X., Tay, Y., Yang, Y., and Zhou, D. (2022). Recitation-augmented language\n   models. arXiv preprint arXiv:2210.01296.\n\n[42] Suzgun, M., Scales, N., Sch√§rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A.,\n   Le, Q. V., Chi, E. H., Zhou, D., , and Wei, J. (2022). Challenging big-bench tasks and whether\n   chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.\n\n[43] Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and Lin, J. (2019). Distilling task-specific\n  knowledge from BERT into simple neural networks. CoRR, abs/1903.12136.\n\n[44] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi√®re, B.,\n   Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. (2023a).\n  Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n\n[45] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra,\n   S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu,\n   D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A.,\n   Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A.,\n   Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X.,\n   Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\n   K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R.,\n   Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S.,\n   Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. (2023b). Llama 2: Open foundation and\n   fine-tuned chat models.\n\n\n                                       13\n\n[46] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. (2023a). Self-consistency\n   improves chain of thought reasoning in language models. International Conference on Learning\n   Representations.\n\n[47] Wang, X., Zhu, W., Saxon, M., Steyvers, M., and Wang, W. Y. (2023b). Large language models\n   are implicitly topic models: Explaining and finding good demonstrations for in-context learning.\n\n[48] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and\n  Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. In\n  NeurIPS.\n\n[49] Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.-S., Mellor, J., Glaese, A., Cheng, M.,\n   Balle, B., Kasirzadeh, A., et al. (2022). Taxonomy of risks posed by language models. In 2022\n  ACM Conference on Fairness, Accountability, and Transparency, pages 214‚Äì229.\n\n[50] Wu, T., Terry, M., and Cai, C. J. (2022). Ai chains: Transparent and controllable human-ai\n   interaction by chaining large language model prompts. In Proceedings of the 2022 CHI Conference\n  on Human Factors in Computing Systems, pages 1‚Äì22.\n\n[51] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. (2023a).\n  Wizardlm: Empowering large language models to follow complex instructions.\n\n[52] Xu, W., Banburski-Fahey, A., and Jojic, N. (2023b). Reprompting: Automated chain-of-thought\n  prompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993.\n\n[53] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. (2023a). Tree\n   of thoughts: Deliberate problem solving with large language models.\n\n[54] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023b). React:\n   Synergizing reasoning and acting in language models. International Conference on Learning\n   Representations.\n\n[55] Zhang, Z., Zhang, A., Li, M., and Smola, A. (2023). Automatic chain of thought prompting in\n   large language models. International Conference on Learning Representations.\n\n[56] Zhou, D., Sch√§rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le,\n   Q., and Chi, E. (2023a). Least-to-most prompting enables complex reasoning in large language\n   models. International Conference on Learning Representations.\n\n[57] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2023b). Large\n   language models are human-level prompt engineers.  International Conference on Learning\n   Representations.\n\n\n\n\n\n                                       14\n\nContents in Appendices:\n\n        ‚Ä¢ In Appendix A, we list the contribution of each author to this work.\n        ‚Ä¢ In Appendix B, we provide additional experimental details including task statistics and the\n       prompt strings we used to initialize DLN.\n        ‚Ä¢ In Appendix C, we provide additional experiments and baselines we compare to.\n        ‚Ä¢ In Appendix D, we provide forward and backward templates being used in DLN.\n        ‚Ä¢ In Appendix E, we provide an algorithm that generalizes DLN training in a multiple layer\n          setting.\n        ‚Ä¢ In Appendix F, we show examples of learned weights that exhibit behavior similar to\n         in-context learning.\n        ‚Ä¢ In Appendix G, we show examples of learned weights by 2-Layer DLNs.\n        ‚Ä¢ In Appendix H, we show an example of the hidden states produced by a 2-Layer DLN.\n        ‚Ä¢ In Appendix I, we provide implementation details, including hyperparameter information.\n        ‚Ä¢ In Appendix J, we discuss resource used in DLN development and their pricing.\n\n\n\n\n\n                                       15\n\nA  Contributions\n\nAlessandro Sordoni proposed the general idea of DLN, where multiple prompts are learnt at each\nlayer through backward natural language operations; they proposed to generate synthetic in-context\nexamples and the exploration reward for DLN-2; they wrote the code and ran the experiments; they\nfocused on Sections 2, 3, 4 and contribute writing the rest of the sections.\n\nXingdi Yuan co-developed the basic idea of DLN and wrote part of the code, they also co-designed\nand helped conducting experiments. They contributed to the writing of the paper, mainly Sections 5\nand 6.\n\nMarc-Alexandre C√¥t√© helped with the experiments and the infrastructure to make calls to OpenAI\nmodels. They also built a demo to visualize the evolution of DLN‚Äôs prompts during training and\ncontributed to the writing of the paper, mainly focusing on the algorithms and the appendix.\n\nMatheus Pereira co-coded an earlier, non-variational backwards operator with AT, helped with\nthe APE and DLN-2 layers experiments, implemented the method for estimating the total cost\nof experiments, build a demo to visualize the evolution of DLN‚Äôs prompts during training, and\ncontributed to the release of the DLN code.\n\nAdam Trischler helped with template conception and iteration, co-coded an earlier, non-variational\nbackwards operator with MP, and contributed to paper writing, mainly the literature review.\n\nZiang Xiao helped with the model evaluation and experiment setup and contributed to the paper\nwriting, mainly the literature review and discussion.\n\nArian Hosseini participated in the development discussions throughout the project and contributed to\nwriting the literature review of the paper.\n\nFriederike Niedtner organized and managed the project, helping the team focus on the right priorities.\n\nNicolas Le Roux proposed the variational inference formulation and the posterior sharpening. They\noffered guidance and mentorship for the project. They also contributed to the writing of the paper,\nmainly sections 1, 2, and 3.\n\nB  Additional Experimental Details\n\nB.1  Additional Task Information\n\nIn Table 3, we provide short descriptions for all tasks we use and their statistics.\n\n                              Table 3: Tasks used in this work.\n\n  Task        |train|    |valid|    |test|    |class|   Description\n\n  Mpqa      400     256    250     2     Sentiment analysis.\n  Trec       400     256    250     6     Question type classification.\n  Subj       400     256    250     2     Determine whether a sentence is subjective or objective.\n   Disaster    400     250    250     2     Determine whether a sentence is relevant to a disaster.\n   Airline     400     250    250     3      Airline tweet sentiment analysis.\n  Hyper.     400    1000    250     2     Order adjectives correctly in English sentences.\n  Nav.       375     375    250     2      Spatial reasoning given navigation instructions.\n   Date.      59      60     250     6      Infer a date from context.\n  Logic.7    225     225    250     7     Deduce the order of seven objects given instruction.\n\n\nB.2  Prompt Initialization\n\nWe initialize the ‚Äúclassification‚Äù layer of the DLNs, i.e. the first layer of the 1-Layer LN and the\nsecond layer of the 2-layer DLN, with a question or a task description as reported in Table 4. We\nuse these same initializations to compute the 0-shot performance. Therefore, at initialization, a\n1-layer LN is equivalent to the 0-shot baseline. For the hidden layer of the 2-layer DLN, we initialize\nthe prompt to ‚ÄúDecompose the problem to make it simpler:‚Äù for Nav. and Subj, and ‚Äú‚Äù\n(empty string) for Date. and Logic.7. We didn‚Äôt try other initializations for this hidden layer, we leave\nthis for future explorations.\n\n\n                                       16\n\nTable 4: Prompt initializations.\n\n    Task        Initialization\n\n   Mpqa    Read the following review, then choose whether it is negative or positive.\n    Trec      Read the following question, then choose whether it is about a description, entity, expres-\n                 sion, human, location or number.\n    Subj     Read the following sentence, then choose whether it is subjective or objective\n     Disaster   Read the following sentence, then choose whether it is relevant to a disaster.\n     Airline   Read the following sentence, then choose whether it is positive, negative, or neutral.\n     Hyper.    Which sentence has the correct adjective order.\n    Nav.     Read the following sentence, then determine whether you return to the starting point.\n     Date.      Infer the date from context.\n     Logic.7   The following paragraphs each describe a set of seven objects arranged in a fixed order.\n             The statements are logically consistent within each paragraph.\n\n\n\nC  Additional Experiments\n\n\nIn addition to the prompt engineering and few-shot baselines from the main paper, we include here\ncomparisons to more of those on a subset of the datasets. Particularly we add results for:\n\n        ‚Ä¢ Table 5 shows DLN-1 and DLN-2 outperforming CoT+APE (implemented as described in\n        Section 4.3 of Zhou et al. [57]);\n\n        ‚Ä¢ Table 6 shows that even increasing the number of examples for ICL and KATE cannot match\n      DLN-1 and DLN-2 performance;\n\n        ‚Ä¢ Table 7 and Table 8 show results for DLN-1 using open-source language models such as\n       WizardLM-13b-v1.2 [51] and LLaMA2-70B-Chat [45] respectively.\n\n\n\nTable 5: Test accuracy averaged over three random seeds with 95% confidence interval. All models\nuse GPT-3. DLN-1 and DLN-2 outperform CoT+APE.\n\n                  Method     Hyper.    Nav.       Date.      Logic.7\n\n                   APE-15     68.5¬± 5.5   67.3¬± 7.7   32.1¬± 28.6   45.5¬± 4.7\n                 CoT+APE   50.9¬± 0.8   61.5¬± 1.5   58.6¬± 2.2   38.9¬± 1.6\n\n                 DLN-1      91.9¬± 3.0   68.5¬± 4.7   55.7¬± 4.5   47.5¬± 2.1\n                 DLN-2        -         83.1¬± 24.7   75.2¬± 14.8   45.7¬± 3.5\n\n\n\nTable 6: Test accuracy averaged over three random seeds with 95% confidence interval (where\napplicable). All models use GPT-3. Increasing the number of ICL examples helps performance, but\ncannot match DLN-1 and DLN-2 in general. Context length limit is an issue for ICL and KATE\n32-shot.\n\n                Method         Nav.       Date.      Logic.7   Subj\n\n                 ICL - 5-shot      56.5        62.1        36.7       76.4\n                 ICL - 10-shot     61.3        62.9        38.9       72.0\n                 ICL - 32-shot     66.0        63.5          -          83.2\n              KATE - 5-shot    56.9        61.1        44.4       71.1\n              KATE - 10-shot   59.5        62.0        41.6       73.9\n              KATE - 32-shot   67.5        62.8          -          80.4\n\n                DLN-1          68.5¬± 4.7   55.7¬± 4.5   47.5¬± 2.1   83.2¬±5.5\n                DLN-2          83.1¬± 24.7   75.2¬± 14.8   45.7¬± 3.5   85.9¬±8.7\n\n\n\n\n                                       17\n\nTable 7: Test accuracy using WizardLM-v1.2 13B as LLM. This open source model seems signifi-\ncantly less able to capture few-shot examples from the context. DLN-1 outperforms ICL on all tasks\nhere.\n\n                           Method   Nav.   Logic.7   Subj\n\n                                    0-shot     58.0    0.0       65.8\n                                    5-shot     56.0   28.0      50.8\n\n                          DLN-1    61.1   31.0      79.8\n\nTable 8: Test accuracy averaged over three random seeds with 95% confidence interval. All methods\nuse LLaMA2-70B-Chat as LLM, with DLN + GPT3 employing text-davinci-003 as the backward\nLLM for prompt and hidden proposals.\n\n                Method          Nav.       Date.      Logic.7    Subj\n\n                     0-shot              42.0¬±0.0    25.2¬±0.0    14.4¬±0.0    62.4¬±0.0\n                     5-shot              43.2¬±12.5   21.1¬±9.7    16.4¬±2.6    67.7¬±15.5\n\n                DLN-1 + GPT3   43.6¬±4.0    21.9¬±5.7    33.1¬±10.9   80.9¬±11.5\n                DLN-1            44.9¬±6.4    31.6¬±10.5   38.4¬±3.6    76.1¬±4.5\n\n                DLN-2 + GPT3   43.7¬±3.0    51.1¬±4.0    21.9¬±4.9    59.1¬±16.7\n                DLN-2            68.9¬±14.4   61.7¬±17.6   20.0¬±13.7   63.1¬±25.4\n\n\n\nC.1  Layerwise training of DLN-2\n\nWe also explored a different learning strategy for DLN-2: layer-wise pre-training. We start the\nlearning of a single layer DLN using the technique from Section 2. We call the prompt obtained at\nthe end of this optimization œÄ‚àó. Then, we train a DLN-2 by initializing œÄ1 = œÄ‚àó, and then learn œÄ0,\nthe parameters of the bottom layer, using variational inference. We explore two variants: one keeps\nthe last layer fixed and one it fine-tunes the last layer. We report their results in Table 9.\n\nTable 9: Test accuracy averaged over three random seeds. We compare the layerwise and the end-to-\nend trainings for DLN-2.\n\n                                             Nav.   Date.   Logic.7   Subj\n\n                    DLN-2 (fix 2nd)          73.1    61.6     43.3     80.2\n                    DLN-2 (fine-tune 2nd)    76.4    62.8     40.7     84.5\n\n                    DLN-2 (end-to-end)      83.1    75.2     45.7     85.9\n\n\n\n\n\n                                       18\n\nD  Templates\n\nD.1  Forward Templates\n\n‚ÄúClassification‚Äù Template F for 1-Layer LN  In the 1-layer LN, we use the following template to\nelicit the output y given the input x. prompt is substituted with the value of the current prompt.\n\n Classification Template F\n\n  template:\n      {{ prompt }}\n\n      {{ input }}\n\n    Answer:\n\n\nResidual Classification Template Fr for 2-Layer DLN  In the 2-layer LN, the last layer just\nconcatenates the input to the output of the first layer, ÀÜh, before eliciting an answer.\n\n Residual classification template Fr\n\n  template:\n      {{ prompt }}\n\n      {{ input }}\n    Your thoughts were:\n      {{ h }}\n\n    Answer:\n\n\nHidden Layer F  The variable prompt is substituted with the value of the current prompt œÄ0. This\nhas the effect of providing additional information about how ‚ÄúLet‚Äôs think step by step‚Äù should behave.\n\n Hidden Layer F\n\n  template:\n      {{ input }}\n\n      {{ prompt }} Let's think step by step.\n\nFor 2-Layer DLN on Subj, we use the following hidden template. We couldn‚Äôt run with the previous\ntemplate due to lack of time, as we observed that the step by step trigger tended to generate lengthy\nhidden states.\n\n Hidden Layer F\n\n  template:\n      {{ prompt }}\n\n      {{ input }}\n\n     Brief Analysis:\n\n\nD.2  Backward Templates (Prompt and Hidden Proposals)\n\nPrompt Proposal Template BœÄ  BœÄ is used to propose new candidate prompts. The template takes\nas input the current prompt, prompt, and a mini-batch of examples stored in backward_infos.\nbackward_info.input stores the input to the layer (x if we are proposing prompts for the first\nlayer or h if it is the second layer); backward_info.target stores the target to the layer (h‚àóif it is\nthe first layer or y otherwise); backward_info.output stores the predictions of the model during\nthe forward pass (ÀÜh if it is the first layer, and ÀÜy if it is the second layer). message is substituted with\none of the message_alternatives, sampled at random during the DLN training. This induces\ndiversity in the generated prompts and allows emergence of learning to ICL behaviors, where the\nprompts contain synthetic examples that can help solve the task.\n\n\n                                       19\n\nPrompt Proposal Template BœÄ\n\n\n  template:\n   A student is completing a task that requires producing a text output from a text input. The student receives an instruction\n     that describes how to produce the output given each input.\n    The student has made some errors. Your task is to improve the instruction such that the student can fix the errors.\n\n     This was the instruction.\n    ## Instruction\n    > {{ prompt }}\n    [END]\n\n    # Student successes\n   {% for backward_info in backward_infos %} {% if backward_info.loss == 0.0 %}\n    ## Input:\n    > {{ backward_info.input }}\n    ## Correct Output:\n    > {{ backward_info.target }}\n   {% endif %} {% endfor %}\n\n    # Student errors\n   {% for backward_info in backward_infos %} {% if backward_info.loss > 0.0 %}\n    ## Input:\n    > {{ backward_info.input }}\n    ## Student Output:\n    > {{ backward_info.output }}\n    ## Correct Output:\n    > {{ backward_info.target }}\n   {% endif %} {% endfor %}\n\n    Improve the instruction to fix the student errors. {{ message }}\n    ## Instruction\n    >\n\n  message_alternatives:\n     ‚àíClarify the instruction by adding few words or a short sentence. Be concise.\n    ‚àíImprove the instruction by providing examples on how to solve the task. Be concise.\n    ‚àíShorten the instruction by removing superflous words or sentences.\n    ‚àíRewrite the instruction by providing detailed information to avoid ambiguity. Be concise.\n\n\n\n\n\nBackward Hidden Templates Bh  We experiment with multiple backward templates to sample hid-\nden states from the approximate posterior distribution q(h). Each vary in the amount of conditioning\ninformation. The simpler way of sampling hidden states is to use the same hidden template as in\nthe forward pass. This corresponds to using a posterior distribution q(h) which is equivalent to the\nprior distribution p(h). The other alternative is to condition the posterior template on the answer y,\nas illustrated below:\n\n\n\n\n\n Backward Hidden Template By (y conditioning)\n\n  template:\n      {{ input }}\n\n    Given that the answer is:\n      {{ y }}\n\n      {{ prompt }} Let's think step by step.\n\n\n\n\n\nThe next alternative we experiment with is a more verbose template that takes as input the prompt for\nthe final layer œÄ1 in next_prompt, the input x in input, the hidden state from the forward pass ÀÜh in\nh and the ground-truth output y. We use a similar strategy of sampling different message alternatives\nto substitute with message to increase diversity of the hidden samples:\n\n\n                                       20\n\nBackward Hidden Template Bh\n\n  template:\n     This is the context needed to solve the problem:\n      {{ next_prompt }}\n\n     This is the problem:\n      {{ input }}\n\n    These were your thoughts:\n      {{ h }}\n\n    Given that this is the answer:\n      {{ y }}\n\n      {{ message }}\n    Thoughts:\n\n  message_alternatives:\n    ‚àíReflect and refine your thoughts for this problem by adding detailed explanations.\n    ‚àíFix the errors in your reasoning. Add examples to illustrate your thoughts. Be concise.\n\n\nIn practice, we found that sampling from hidden states from a mixture of forward template F, i.e.\npLM(h|x, œÄ0), and backward template with y conditioning, i.e. q(h|x, y, œÄ0) works well. We suspect\nthat this has the effect of capping the KL divergence term between posterior and prior distribution, i.e.\nthe KL(q(h)||pLM(h|x, œÄ0)) appearing in the ELBO. In the future, this could be addressed in a more\nprincipled way by learning a prompt for the posterior proposal.\n\n\nE  Generalized VI to Multiple Layers\n\n\nWe report the generalized training algorithm for multiple layers in Algorithm 3.\n\n\nAlgorithm 3 Deep Language Network Training Algorithm\n  1:  ÀÜh ‚àºptLM(x): generates a completion of prefix x with temperature t.\n  2: log pLM(h|x): return log-prob of h following x.\n  3: N: # prompt samples, K: # posterior samples, I: # iterations, L: # layers, D: dataset\n  4: F: template for the inference (forward pass).\n  5: BœÄ, Bh: templates for prompt and hidden proposal (backward pass).\n  6: Initialize all layers œÄl with a generic sentence or task description.\n  7: for i in [1, I] do\n  8:     x, y ‚àºD                                              ‚ñ∑Sample minibatch\n  9:      ÀÜh0 ‚Üêx\n10:       ÀÜhl, . . . , ÀÜhL ‚Üêp0LM(F(ÀÜhl‚àí1, œÄl‚àí1))               ‚ñ∑Inference pass for all layers 0 < l ‚â§L\n11:    h‚àóL ‚Üêy\n12:     for l in [L ‚àí1, 1] do\n13:       h1l , . . . , hKl  ‚àºp0.7LM (Bh(ÀÜhl, h‚àól+1, œÄl))         ‚ñ∑Sample K posterior proposals for hl\n14:       Œ±1l , . . . , Œ±Kl ‚Üêlog pLM(hkl |F(ÀÜhl‚àí1, œÄl‚àí1)) ‚ñ∑Compute prior log-probs for all hkl samples\n15:       Œ≤1l , . . . , Œ≤Kl ‚Üêlog pLM(h‚àól+1|F(hkl , œÄl))  ‚ñ∑Compute log-likelihoods for all hkl samples\n16:        qkl ‚Üêexp(Œ±kl + Œ≤kl )/(Pk exp(Œ±kl + Œ≤kl ))   ‚ñ∑Compute normalized posterior weights\n17:       h‚àól ‚Üêarg maxhl{q1l , . . . , qKl }               ‚ñ∑Select best posterior sample for layer l\n18:    end for\n19:     for l in [L ‚àí1, 0] do\n20:       œÄ1l , . . . , œÄNl  ‚àºp0.7LM (BœÄ({ÀÜhl, ÀÜhl+1, h‚àól+1}, œÄl))    ‚ñ∑Sample N candidate prompts for œÄl\n21:        s1l , . . . , sNl ‚ÜêPk Pk‚Ä≤ qkl qk‚Ä≤l+1 log pLM(hk‚Ä≤l+1|F(hkl , œÄnl ))     ‚ñ∑Compute ELBO for all\n    prompts œÄnl\n22:         œÄl ‚Üêarg maxœÄnl {s1l , . . . , sNl }                    ‚ñ∑Select prompt œÄl with best score\n23:    end for\n24: end for\n\n\n\n\n                                       21\n\nF  Learning to In-Context Learn: Additional Examples\n\n\nIn Figure 2, we report examples of prompts found by the 1-layer DLN on the Hyperbaton task, which\nexhibit either integration or verbalization of task examples. Here, we provide additional examples of\nprompts found by DLN-1 on the MPQA task.\n\n\n DLN-1 prompt on MPQA (GPT-3)\n\n  Read each sentence, then decide if the sentence is expressing a positive or negative sentiment. For example, if the sentence\n  is \"supported\", choose \"positive\", and if the sentence is \"derail\", choose \"negative\". Similarly, if the sentence is \"victorious\",\n  choose \"positive\", and if the sentence is \"would not be a bad idea\", choose \"positive\". Additionally, if the sentence contains\n  multiple words, consider the overall sentiment of the sentence and choose the appropriate option. For example, if the sentence\n  is \"counting on\", choose \"negative\", and if the sentence is \"peace and stability and prosperity\", choose \"positive\". Note that\n  words like \"artificial\" tend to have a negative sentiment.\n\n\n\n DLN-1 prompt on MPQA (GPT-4)\n\n  Determine whether the given input has a positive or negative connotation by analyzing the meaning of the words and phrases\n  in context.  If the input expresses a favorable, desirable, or pleasant meaning, choose \"positive.\" If the input expresses an\n  unfavorable, undesirable, or unpleasant meaning, choose \"negative.\" Consider the overall sentiment expressed by the input\n  rather than focusing on individual words or phrases. For example, \"calling for\" generally has a positive connotation as it implies\n  advocating or supporting something, while \"a true Muslim fighter\" can be seen as positive, since it refers to someone dedicated\n  to their beliefs. Keep in mind that some phrases may have a positive connotation when they imply improvement or resolution,\n  like \"put an end to.\" Additionally, phrases like \"to the contrary\" can have a positive connotation when they suggest a differing,\n  yet valid perspective or opinion. When analyzing the input, consider the context in which it is used, as the connotation of a word\n  or phrase can change depending on the situation. For example, \"extra vigil\" can have a positive connotation when it implies\n  increased awareness and preparedness.\n\n\n\nG  Examples of 2-Layer Best Weights (GPT-3)\n\n\nG.1  Navigate (81.6% Dev Acc)\n\n DLN-2 Prompt: œÄ0\n\n  Decompose the problem by breaking it down into steps and describing the new position after each step. Note that the direction\n  you are facing stays the same unless specified. Start by specifying the direction you are facing and the coordinates of the\n  starting point. For each step, describe the number of steps taken and the direction of movement (e.g. North, South, East, or\n  West). Specify the new coordinates and direction after each step.\n\n\n DLN-2 Prompt: œÄ1\n\n  Start facing north. Take the specified number of steps in the indicated direction and turn when specified. Make sure to keep\n  track of your direction and the number of steps taken to ensure you return to the starting point.\n\n\n\nG.2  Subj (89.9% Dev Acc)\n\n DLN-2 Prompt: œÄ0\n\n  Reflect on the input to produce an output that is either objective (factual) or subjective (involving opinion or value judgment).\n  Objective statements describe events or facts that can be verified, while subjective statements express opinion or personal\n  feelings about a fact, event, or situation that cannot be confirmed as an accurate statement of fact. For example, if the input\n  is \"the film was a success at the box office,\" the output would be \"This statement is an objective fact, as it describes events\n  that can be verified.\" If the input is \"the film was an amazing experience,\" the output would be \"This statement is subjective,\n  expressing a personal opinion about the film in question that reflects approval and judgement, and cannot be confirmed as an\n  accurate statement.\" If the input is \"neither Juan Antonio nor Se√±or Maximiliano know what they are in for when the tables are\n  turned by the sly Carmen,\" the output would be \"This statement is an objective fact, as it describes events that can be verified\n  regarding the actions of Juan Antonio, Se√±or, and Maximiliano, and the change of circumstances caused by Carmen.\" If the\n  input is \"humorless, self-conscious art drivel, made without a glimmer of intelligence or invention,\" the output would be \"This\n  statement is subjective, expressing a negative opinion about the film in question that reflects disapproval and judgement, and\n  cannot be confirmed as an accurate statement.\" If the input is a hypothetical situation, such as \"how would you feel if when you\n  woke, the nightmare had just begun?\", the output would be \"This statement is subjective, expressing a personal opinion about\n  a hypothetical situation that cannot be verified.\"\n\n\n\n                                       22\n\nDLN-2 Prompt: œÄ1\n\n  Read the sentence. Determine if it is expressing a fact or opinion. A fact is an accurate statement that can be confirmed,\n  while an opinion is a personal viewpoint that reflects someone‚Äôs beliefs. Facts are typically statements about something that\n  happened, such as events, actions, or conditions, or statements that describe a state of being, such as someone‚Äôs personality\n  or a physical object. Opinions are typically statements that express judgement, approval, or disapproval. Examples of facts\n  include statements about events that occurred, such as \"the film was released in 2012,\" or statements about conditions, such\n  as \"the weather is sunny,\" or statements that describe a state of being, such as \"the detective is strong and independent.\"\n  Examples of opinions include statements about how good or bad something is, such as \"the film was terrible\" or \"the weather is\n  beautiful.\" Examples of questions that are not facts or opinions include hypothetical questions, such as \"how would you feel if\n  when you woke, the nightmare had just begun?\". Select \"objective\" for a fact and \"subjective\" for an opinion.\n\n\nH  Examples of Hidden States\n\nWe report the forward pass produced by a 2-Layer DLN in Section G.1 on Navigate below.\n\n Input x\n\n   If you follow these instructions, do you return to the starting point? Take 3 steps. Take 10 steps. Take 4 steps. Take 1 step.\n  Options:\n  - Yes\n  - No\n\n Hidden ÀÜh\n\n  1. Take 3 steps: (3, 0) facing east\n  2. Take 10 steps: (13, 0) facing east\n  3. Take 4 steps: (17, 0) facing east\n  4. Take 1 step: (18, 0) facing east\n\n  Answer: No, you do not return to the starting point.\n\n Output ÀÜy\n\n  No\n\n\nI  Implementation Details\n\nWe report hyperparameter search space in Table 10. A brief description of the hyperparameters is as\nfollows:\n\n        ‚Ä¢ bh_tpl is the type of backward prompt template we use BœÄ. v3.5 is equal to the BœÄ we report\n          in Section D. In v3.0, we remove ‚ÄúBe concise.‚Äù at the end of each message_alternatives.\n     We noticed that in general v3.5 works better as it implements a sort of regularization on\n         the length of the found prompts. Future work could address length regularization in a more\n         principled manner.\n\n        ‚Ä¢ logp_penalty is the coefficient for the exploration reward we mentioned in the paper.\n\n        ‚Ä¢ num_h_samples is the number of h samples to generate from the approximate posterior\n          distribution.\n\n        ‚Ä¢ use_memory is whether or not we use the backtracking mechanism. Usually 2 works well\n         across tasks.\n\n        ‚Ä¢ held_out_prompt_ranking describes whether we use only half of the mini-batch exam-\n         ples for each prompt proposal, as described in the main paper.\n\n        ‚Ä¢ tolerance describes after how many iterations we reload the best weights found during\n         the last validation if the current validation score is lower than the best score obtained so far.\n\nFor the 2-Layer experiments, we have to restrict this search space due to computational costs. We\nuse bh_tpl = \"v3.5\", tolerance = 2, use_memory = 2, held_out_prompt_ranking = True,\nlogp_penalty = 0.5.\n\n\n                                       23\n\nTable 10: Hyperparameter search space.\n\n            hyperparam                  search space\n\n             1-Layer LN\n\n            bh_tpl                        q_action_prompt:v3.0, q_action_prompt:v3.5\n\n            tolerance                        -1, 0, 2\n\n            use_memory                      0, 2\n\n            held_out_prompt_ranking   True, False\n\n             2-Layer DLN PT + fix 2nd layer\n\n            bh_tpl                        q_action_prompt:v3.0, q_action_prompt:v3.5\n\n            logp_penalty                     0., 0.5, 2.\n\n             2-Layer DLN PT + fine-tune 2nd layer\n\n            bh_tpl                        q_action_prompt:v3.0, q_action_prompt:v3.5\n\n            logp_penalty                     0., 0.5, 2.\n\n             2-Layer DLN end-to-end\n\n            num_h_samples                  5, 10\n\nTable 11: Test accuracy along with inference cost expressed in tokens (in gray) averaged over three\nrandom seeds of a shallow, 1-layer language network (DLN-1) compared to baselines on GPT-4. We\nalso report the 95% confidence interval on the test accuracy. We emphasize the cost at the testing\ntime because it is more relevant in real-world deployment and the training cost is one-off.\n\n\n                           BigBench Hard                     NLU                    Leopard\n\n    Method     Hyper.       Nav.       Date.     Logic.7    Mpqa       Trec       Subj    Disaster      Airline\n\n    GPT-4\n       0-shot     64.0¬±1.0    74.0¬±1.0    79.2¬±2.6    68.5¬±3.5    86.3¬±0.6    64.8¬±1.7    72.5¬±1.5    47.7¬±0.6    84.5¬±0.6\n                          (7.6k)        (12.9k)        (23.6k)        (46.2k)          (3.7k)          (7.6k)        (10.2k)        (10.1k)          (9.9k)\n       5-shot     88.4¬±2.6    75.7¬±1.5    79.3¬±1.1    62.8¬±1.7    88.0¬±3.0    82.5¬±3.8    94.7¬±3.5    63.6¬±8.5    88.0¬±1.0\n                        (48.0k)        (79.2k)       (143.3k)       (287.5k)        (24.5k)        (52.5k)        (62.6k)        (63.5k)        (61.7k)\n      16-shot    93.3¬±2.3    75.5¬±5.1    80.9¬±5.0    66.4¬±3.6    91.3¬±1.5    83.7¬±0.6    96.5¬±2.5    67.1¬±4.0    88.3¬±2.1\n                       (136.8k)       (229.9k)       (405.1k)       (817.6k)        (70.3k)       (149.0k)       (177.9k)       (179.4k)       (175.2k)\n\n     DLN-1    95.2¬±5.0    77.1¬±4.7    74.3¬±1.5    69.1¬±2.5    91.1¬±3.2    89.5¬±2.1    93.1¬±5.0    82.1¬±3.8    85.9¬±1.5\n                        (77.2k)        (29.9k)        (52.3k)        (68.5k)        (65.4k)       (120.7k)        (46.5k)        (47.1k)        (38.2k)\n\n\nJ  Pricing\n\nWe keep track the number of tokens we interact with GPT-3 via its online API. According to\nOpenAI‚Äôs pricing policy, user pays for both the input tokens (prompts) and the output tokens. Using\nthe Hyperbaton task as an example, while training a 1-layer LN, the total number of tokens we use is\n2,941,360. For a 2-layer DLN, the total number of tokens we use is 13,654,962. According to the\ncurrent price for GPT-3 ($0.02/1k tokens), a single run of a 1-layer and 2-layer DLN cost roughly 59\nUSD and 273 USD, respectively.\n\nIn Table 11, we report the cost (lower is better) in terms of total number of tokens for the test set\n(prompts included). We emphasize the cost at the testing time because it is more relevant in real-world\ndeployment and the training cost is one-off. We can see DLN-1 improves over ICL on 5 out of 9\ntasks on GPT-4 at a comparable token cost. Some tasks do not benefit from ICL (i.e. reasoning tasks)\nwhile other tasks like Subj, Trec, and Hyper. benefit significantly.\n\n\n\n\n\n                                       24",
"headers": [
"arXiv:2306.12509v2  [cs.CL]  4 Dec 2023",
"Joint Prompt Optimization of Stacked LLMs",
"using Variational Inference",
"Abstract",
"1",
"Introduction",
"2",
"One-Layer Language Networks",
"3",
"Two-Layer Deep Language Networks (DLN-2)",
"4",
"Practical Instantiation",
"5",
"Experiments and Results",
"6",
"Related Work",
"7",
"Conclusion and Future Work",
"References",
"Contents in Appendices:",
"A",
"Contributions",
"B",
"Additional Experimental Details",
"C",
"Additional Experiments",
"D",
"Templates",
"E",
"Generalized VI to Multiple Layers",
"F",
"Learning to In-Context Learn: Additional Examples",
"G",
"Examples of 2-Layer Best Weights (GPT-3)",
"œÄ",
"H",
"Examples of Hidden States",
"x",
"ÀÜ",
"h",
"y",
"I",
"Implementation Details",
"J",
"Pricing"
],
"tables": [
"|üî•|t|rainable|Col4|\n|---|---|---|---|\n||_frozen_|_frozen_|_frozen_|",
"|prompt|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|‚Ä¶ Additionally, conside|‚Ä¶ Additionally, conside|‚Ä¶ Additionally, conside|‚Ä¶ Additionally, conside|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>the fact that in some|<br>the fact that in some|<br>the fact that in some|<br>the fact that in some|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|contexts the day may|contexts the day may|contexts the day may|contexts the day may|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>come before the month|<br>come before the month|<br>come before the month|<br>come before the month|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>(e.g. in the UK). For<br>example, if the given|<br>(e.g. in the UK). For<br>example, if the given|<br>(e.g. in the UK). For<br>example, if the given|<br>(e.g. in the UK). For<br>example, if the given|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>context is ‚ÄúIt is 4/19/19|<br>context is ‚ÄúIt is 4/19/19|<br>context is ‚ÄúIt is 4/19/19|<br>context is ‚ÄúIt is 4/19/19|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>today. What is the date|<br>today. What is the date|<br>today. What is the date|<br>today. What is the date|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>month ago in|<br>month ago in|<br>month ago in|<br>month ago in|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>MM/DD/YYYY?‚Äù, you c<br>|<br>MM/DD/YYYY?‚Äù, you c<br>|<br>MM/DD/YYYY?‚Äù, you c<br>|<br>MM/DD/YYYY?‚Äù, you c<br>|\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>MM/DD/YYYY?‚Äù, you c<br>|<br>MM/DD/YYYY?‚Äù, you c<br>|||\n|‚Ä¶ Additionally, consider<br>the fact that in some<br>contexts the day may<br>come before the month<br>(e.g. in the UK). For<br>example, if the given<br>context is ‚ÄúIt is 4/19/1969<br>today. What is the date a<br>month ago in<br>MM/DD/YYYY?‚Äù, you can<br>infer that‚Ä¶|<br>MM/DD/YYYY?‚Äù, you c<br>|<br>MM/DD/YYYY?‚Äù, you c<br>|||",
"|prompt|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>|\n|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>|‚Ä¶ For example, if the<br>sentence is ‚Äúsupported‚Äù,<br>choose ‚Äúpositive‚Äù, and if<br>the sentence is ‚Äúderail‚Äù,<br>choose ‚Äúnegative‚Äù.<br>Similarly, if the sentence is<br>\"peace and stability and<br>prosperity\", choose<br>\"positive\". Note that<br>words like \"artiÔ¨Åcial\" tend<br>to have a negative<br>sentiment.<br>||||",
"|prompt|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|\n|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|**input****_x_**|\n|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|The deadline is Jun 1, 2021, which is 2<br>days away from now. What is the date<br>one year ago from today in<br>MM/DD/YYYY? \\n Options\\n(A)<br>05/30/2020 \\n (B) 06/02/2020...<br>|\n|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶||||\n|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶|Decompose the problem<br>by breaking it down into<br>steps. First, Ô¨Ånd the<br>current date from the<br>given context. Then,<br>calculate the number of<br>days or months to add or<br>subtract from the current<br>date to get the desired<br>result. Additionally,‚Ä¶||||",
"|hidden<br>template|Col2|\n|---|---|",
"|inp|ut x|\n|---|---|\n|a decade of dramatic<br>economic decline<br>|a decade of dramatic<br>economic decline<br>|",
"|Col1|BigBench Hard|NLU|Leopard|\n|---|---|---|---|",
"|Method|Hyper. Nav. Date. Logic.7|Mpqa Trec Subj|Disaster Airline|\n|---|---|---|---|",
"|GPT-3<br>0-shot<br>5-shot<br>KATE<br>APE-15<br>APE-400|60.8 64.1 56.4 45.9 88.0 61.9 61.7 81.6 75.6<br>55.6 56.5 62.1 36.7 87.2 80.0 76.4 81.2 82.7<br>71.1 56.9 61.1 44.4 88.4 77.6 71.1 76.0 81.6<br>68.5¬±5.5 67.3¬±7.7 32.1¬±28.6 45.5¬±4.7 85.5¬±4.6 71.3¬±5.5 61.3¬±7.2 54.8¬±14.6 83.5¬±3.5<br>65.5¬±4.7 56.9¬±32.9 23.5¬±14.1 45.6¬±12.4 84.9¬±9.7 72.0¬±1.7 63.7¬±9.2 60.3¬±37.4 82.3¬±10.0|\n|---|---|",
"|GPT-4<br>0-shot<br>5-shot<br>16-shot|64.0¬±1.0 74.0¬±1.0 79.2¬±2.6 68.5¬±3.5 86.3¬±0.6 64.8¬±1.7 72.5¬±1.5 47.7¬±0.6 84.5¬±0.6<br>88.4¬±2.6 75.7¬±1.5 79.3¬±1.1 62.8¬±1.7 88.0¬±3.0 82.5¬±3.8 94.7¬±3.5 63.6¬±8.5 88.0¬±1.0<br>93.3¬±2.3 75.5¬±5.1 80.9¬±5.0 66.4¬±3.6 91.3¬±1.5 83.7¬±0.6 96.5¬±2.5 67.1¬±4.0 88.3¬±2.1|\n|---|---|",
"|DLN-1|95.2¬±5.0 77.1¬±4.7 76.7¬±3.0 69.1¬±2.5|91.1¬±3.2 89.5¬±2.1 93.1¬±5.0|82.1¬±3.8 85.9¬±1.5|\n|---|---|---|---|",
"|Method|Nav. Date. Logic.7 Disaster Subj|\n|---|---|",
"|0-shot<br>CoT<br>APE<br>APE-400|64.1 56.4 45.9 81.6 61.7<br>69.3 72.4 41.1 54.4 59.3<br>67.3¬±7.7 32.1¬±28.5 45.5¬±4.7 54.8¬±14.6 61.3¬±7.2<br>56.9¬±32.9 23.5¬±14.1 45.6¬±12.4 60.3¬±37.4 63.7¬±9.2|\n|---|---|",
"|DLN-1<br>DLN-2|68.5¬±4.7 55.7¬±4.5 47.5¬±2.1 81.7¬±6.5 83.2¬±5.5<br>83.1¬±24.7 75.2¬±14.8 45.7¬±3.5 82.8¬±2.5 85.9¬±8.7|\n|---|---|",
"|Task||train| |valid| |test| |class| Description|\n|---|---|",
"|Mpqa<br>Trec<br>Subj<br>Disaster<br>Airline<br>Hyper.<br>Nav.<br>Date.<br>Logic.7|400 256 250 2 Sentiment analysis.<br>400 256 250 6 Question type classification.<br>400 256 250 2 Determine whether a sentence is subjective or objective.<br>400 250 250 2 Determine whether a sentence is relevant to a disaster.<br>400 250 250 3 Airline tweet sentiment analysis.<br>400 1000 250 2 Order adjectives correctly in English sentences.<br>375 375 250 2 Spatial reasoning given navigation instructions.<br>59 60 250 6 Infer a date from context.<br>225 225 250 7 Deduce the order of seven objects given instruction.|\n|---|---|",
"|Method|Hyper. Nav. Date. Logic.7|\n|---|---|",
"|APE-15<br>CoT+APE|68.5¬± 5.5 67.3¬± 7.7 32.1¬± 28.6 45.5¬± 4.7<br>50.9¬± 0.8 61.5¬± 1.5 58.6¬± 2.2 38.9¬± 1.6|\n|---|---|",
"|DLN-1<br>DLN-2|91.9¬± 3.0 68.5¬± 4.7 55.7¬± 4.5 47.5¬± 2.1<br>- 83.1¬± 24.7 75.2¬± 14.8 45.7¬± 3.5|\n|---|---|",
"|Method|Nav. Date. Logic.7 Subj|\n|---|---|",
"|ICL - 5-shot<br>ICL - 10-shot<br>ICL - 32-shot<br>KATE - 5-shot<br>KATE - 10-shot<br>KATE - 32-shot|56.5 62.1 36.7 76.4<br>61.3 62.9 38.9 72.0<br>66.0 63.5 - 83.2<br>56.9 61.1 44.4 71.1<br>59.5 62.0 41.6 73.9<br>67.5 62.8 - 80.4|\n|---|---|",
"|DLN-1<br>DLN-2|68.5¬± 4.7 55.7¬± 4.5 47.5¬± 2.1 83.2¬±5.5<br>83.1¬± 24.7 75.2¬± 14.8 45.7¬± 3.5 85.9¬±8.7|\n|---|---|",
"|Method|Nav. Logic.7 Subj|\n|---|---|",
"|0-shot<br>5-shot|58.0 0.0 65.8<br>56.0 28.0 50.8|\n|---|---|",
"|DLN-1|61.1 31.0 79.8|\n|---|---|",
"|Method|Nav. Date. Logic.7 Subj|\n|---|---|",
"|0-shot<br>5-shot|42.0¬±0.0 25.2¬±0.0 14.4¬±0.0 62.4¬±0.0<br>43.2¬±12.5 21.1¬±9.7 16.4¬±2.6 67.7¬±15.5|\n|---|---|",
"|DLN-1 + GPT3<br>DLN-1|43.6¬±4.0 21.9¬±5.7 33.1¬±10.9 80.9¬±11.5<br>44.9¬±6.4 31.6¬±10.5 38.4¬±3.6 76.1¬±4.5|\n|---|---|",
"|DLN-2 + GPT3<br>DLN-2|43.7¬±3.0 51.1¬±4.0 21.9¬±4.9 59.1¬±16.7<br>68.9¬±14.4 61.7¬±17.6 20.0¬±13.7 63.1¬±25.4|\n|---|---|",
"|Col1|Nav. Date. Logic.7 Subj|\n|---|---|",
"|DLN-2 (fix 2nd)<br>DLN-2 (fine-tune 2nd)|73.1 61.6 43.3 80.2<br>76.4 62.8 40.7 84.5|\n|---|---|",
"|DLN-2 (end-to-end)|83.1 75.2 45.7 85.9|\n|---|---|",
"|hyperparam|search space|\n|---|---|",
"|bh tpl<br>_|q action prompt:v3.0, q action prompt:v3.5<br>_ _ _ _|\n|---|---|",
"|tolerance|-1, 0, 2|\n|---|---|",
"|use memory<br>_|0, 2|\n|---|---|",
"|held out prompt ranking<br>_ _ _|True, False|\n|---|---|",
"|bh tpl<br>_|q action prompt:v3.0, q action prompt:v3.5<br>_ _ _ _|\n|---|---|",
"|logp penalty<br>_|0., 0.5, 2.|\n|---|---|",
"|bh tpl<br>_|q action prompt:v3.0, q action prompt:v3.5<br>_ _ _ _|\n|---|---|",
"|logp penalty<br>_|0., 0.5, 2.|\n|---|---|",
"|num h samples<br>_ _|5, 10|\n|---|---|",
"|Col1|BigBench Hard|NLU|Leopard|\n|---|---|---|---|",
"|Method|Hyper. Nav. Date. Logic.7|Mpqa Trec Subj|Disaster Airline|\n|---|---|---|---|",
"|GPT-4<br>0-shot<br>5-shot<br>16-shot|64.0¬±1.0 74.0¬±1.0 79.2¬±2.6 68.5¬±3.5 86.3¬±0.6 64.8¬±1.7 72.5¬±1.5 47.7¬±0.6 84.5¬±0.6<br>(7.6k) (12.9k) (23.6k) (46.2k) (3.7k) (7.6k) (10.2k) (10.1k) (9.9k)<br>88.4¬±2.6 75.7¬±1.5 79.3¬±1.1 62.8¬±1.7 88.0¬±3.0 82.5¬±3.8 94.7¬±3.5 63.6¬±8.5 88.0¬±1.0<br>(48.0k) (79.2k) (143.3k) (287.5k) (24.5k) (52.5k) (62.6k) (63.5k) (61.7k)<br>93.3¬±2.3 75.5¬±5.1 80.9¬±5.0 66.4¬±3.6 91.3¬±1.5 83.7¬±0.6 96.5¬±2.5 67.1¬±4.0 88.3¬±2.1<br>(136.8k) (229.9k) (405.1k) (817.6k) (70.3k) (149.0k) (177.9k) (179.4k) (175.2k)|\n|---|---|",
"|DLN-1|95.2¬±5.0 77.1¬±4.7 74.3¬±1.5 69.1¬±2.5<br>(77.2k) (29.9k) (52.3k) (68.5k)|91.1¬±3.2 89.5¬±2.1 93.1¬±5.0<br>(65.4k) (120.7k) (46.5k)|82.1¬±3.8 85.9¬±1.5<br>(47.1k) (38.2k)|\n|---|---|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2306.12509v2.pdf"
}