{
"text": "Preprint. Under review at ICLR 2026\n\n\n          DRO-INSTRUCTZERO:  DISTRIBUTIONALLY ROBUST\n          PROMPT  OPTIMIZATION  FOR  LARGE LANGUAGE\n         MODELS\n\n\n                    Yangyang Li∗\n                      Department of Electrical Engineering and Computer Science\n                       Massachusetts Institute of Technology\n                      Cambridge, MA 02139, USA\n                 annieliy@mit.edu\n\n\n\n                                       ABSTRACT2025\n\n                             Large language models are highly sensitive to prompt wording. However, popu-\n                                      lar automatic prompt search methods, including InstructZero, often degrade underOct                                   distribution shift and adversarial evaluation because they optimize expected per-\n17                       formancein one settingunderfrequentlya single failevaluationto transfer.distribution.To addressConsequently,this, DRO-InstructZeroprompts thatformu-work\n                                    lates zero-shot prompt optimization as robust Bayesian optimization. Specifically,\n                            an f-divergence ball defines an ambiguity set around the evaluation distribution,\n                            and a robust acquisition rule maximizes worst-case expected utility while retaining\n                                 the query efficiency of Bayesian search. Therefore, the search explicitly targets\n                                      reliability under distribution shift rather than average behavior alone. Experiments\n                               follow the instruction-induction protocol with matched query budgets across for-[cs.LG]\n                               mality rewriting, code debugging, and translation. For example, on BIG-Bench\n                                informative-to-formal rewriting, accuracy improves from 61.3 ± 0.7% to approx-\n                               imately 85–90%, yielding an absolute gain of about 25–30 points.  Moreover,\n                              auto-debugging shows about +25-point gains under domain shift.  Meanwhile,\n                                  stable tasks such as cause-and-effect remain above 96%, indicating no loss on in-\n                                   distribution cases. Furthermore, improvements are consistent across divergence\n                               choices and decoding temperatures. Overall, DRO-InstructZero connects distri-\n                                 butionally robust optimization with prompt learning, offering a plug-and-play and\n                                general approach for reliable, transferable prompt alignment under real-world un-\n                                    certainty.\n\n\n                1  INTRODUCTION\n\n\n                     Large language models (LLMs) (OpenAI, 2023a;b; Chowdhery et al., 2022) have achieved remark-arXiv:2510.15260v1                       able performance in zero-shot and few-shot instruction following (Brown et al., 2020; Liu et al.,\n                     2023; Chen et al., 2024). Despite these advances, however, their effectiveness is highly sensitive to\n                        the choice of instructions (Zhou et al., 2022; Honovich et al., 2022). In particular, even minor para-\n                       phrases of a strong instruction can degrade accuracy, and instructions that succeed in one evaluation\n                          setting often fail to transfer to slightly shifted domains. Taken together, this fragility raises critical\n                      concerns about robustness when deploying LLMs in real-world environments.\n\n                      Motivated by these observations, instruction optimization has emerged as a promising direction\n                         to automate prompt design and reduce reliance on costly human prompt engineering (Zhou et al.,\n                     2022; Sun et al., 2022). A notable advance is INSTRUCTZERO (Chen et al., 2024), which formulates\n                    prompt learning as a latent-space Bayesian optimization (BO) problem: an open-source LLM gen-\n                         erates candidate instructions guided by a soft prompt, and a black-box LLM evaluates them, with\n              BO iteratively refining the distribution. This approach achieves state-of-the-art results across many\n                   BIG-Bench tasks.\n\n                          ∗Corresponding author. Personal website: https://annie0219.github.io/#about\n\n\n                                                           1\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n\nHowever, existing InstructZero and related BO-based methods optimize the expected score under a\nfixed validation distribution, using classical acquisition functions such as expected improvement (EI)\nor upper confidence bound (UCB). This assumption is restrictive, because it neglects the possibil-\nity of distributional shift—inevitable when instructions are evaluated under adversarial conditions,\ndomain mismatches, or changing user queries. Consequently, optimized instructions often overfit to\nthe training distribution, yielding brittle performance in deployment.\n\nTo address this limitation, we propose DRO-InstructZero, which integrates distributionally robust\noptimization (DRO) (Kirschner et al., 2020b) into the Bayesian optimization framework. Specif-\nically, our method defines an ambiguity set around the empirical evaluation distribution using\nf-divergence (specifically, KL divergence), and seeks to maximize the worst-case expected util-\nity within this set. By optimizing for robustness rather than average-case performance, DRO-\nInstructZero yields instructions that generalize more reliably across shifts. Moreover, the resulting\nacquisition rule preserves the query-efficiency of BO while explicitly accounting for uncertainty\nabout future data.\n\nWe validate these claims empirically under the instruction-induction protocol on diverse tasks in-\ncluding formality rewriting, code debugging, and translation. Across these settings, our method\nconsistently outperforms both InstructZero and classical BO baselines: for example, on BIG-Bench\ninformative-to-formal rewriting, accuracy improves from 61.3 ± 0.7% with InstructZero to approx-\nimately 85–90%, a gain of 25–30 points. Similarly, auto-debugging under domain shift shows im-\nprovements of +25 points. Importantly, stable tasks such as cause-and-effect remain above 96%,\ndemonstrating that robustness does not sacrifice in-distribution performance.\n\nIn summary, the contributions are summarized as follows:\n\n        • First, the vulnerability of existing prompt optimization methods to distributional shift is\n          identified, thereby highlighting the need for robust objectives.\n        • Building upon this observation, DRO-InstructZero is introduced as a novel framework\n          that integrates distributionally robust optimization with Bayesian search for instruction\n         learning, enabling worst-case reliable performance.\n        • Finally, extensive experiments demonstrate that DRO-InstructZero achieves substantial ro-\n         bustness gains over both InstructZero and standard BO acquisitions, while maintaining the\n       same query cost.\n\nTaken together, these contributions connect DRO with prompt learning, offering a principled and\nplug-and-play approach for reliable and transferable instruction alignment of LLMs under real-world\nuncertainty.\n\n2  INSTRUCTION OPTIMIZATION WITH INSTRUCTZERO\n\nWe begin by revisiting INSTRUCTZERO (Chen et al., 2024), which formulates zero-shot prompt op-\ntimization as Bayesian optimization over a low-dimensional continuous representation of prompts.\nThis section summarizes its pipeline, objective, and Bayesian optimization framework, laying the\nfoundation for our robust extension in Section 3.\n\n2.1  PIPELINE OVERVIEW\n\nThe primary goal is to find an optimal natural language instruction v for a given task that maximizes\nthe performance of a black-box LLM f(·). This can be formulated as maximizing the expected score\nover the task’s data distribution Dt:\n                     max E(X,Y )∼Dt[h(f([v; X]), Y )],                              (1)\n                           v∈V\nwhere V is the space of all possible instructions, [v; X] denotes the concatenation of the instruction\nand the input query, and h(·, ·) is a task-specific evaluation metric (e.g., accuracy).\n\nHowever, solving Eq. equation 1 directly is notoriously difficult.  The optimization faces two\nmain challenges:  (1) Combinatorial Search Space: The instruction space V is discrete, high-\ndimensional, and governed by complex syntactic and semantic rules, making direct search in-\ntractable. (2) Black-Box Objective: For powerful API-based LLMs like GPT-4, the function f(·)\n\n\n                                       2\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n\nis a black box, providing only output text without gradients, which precludes gradient-based opti-\nmization methods.\n\nTo overcome these challenges, INSTRUCTZERO proposes an indirect optimization strategy, as illus-\ntrated in Algorithm 1.\n\n      1. A soft prompt p ∈Rd is projected through a random matrix A ∈Rd×d′ and concatenated\n        with a few-shot set of task exemplars {(xi, yi)}κi=1.\n      2. The open-source LLM g(·) maps this embedding into a natural language instruction v.\n      3. The black-box LLM f(·) executes instruction v on validation examples (X, Y ) ∼Dt,\n        producing responses that are evaluated by a task-specific metric h(·, ·).\n      4. The tuple (p, v, h) is added to the training data for Bayesian optimization (BO), which\n        updates its posterior over the objective and proposes the next prompt.\n\nThis approach effectively converts the discrete, high-dimensional problem of finding v into a more\nmanageable continuous optimization problem for finding p.\n\n2.2  BAYESIAN OPTIMIZATION OF SOFT PROMPTS\n\nDirect search over the discrete space of instructions V  is intractable.  Instead, INSTRUCTZERO\noptimizes the continuous soft prompt p, which induces instructions via g(·). Define the black-box\nfunction\n                H(p) ≜E(X,Y )∼Dt h(f([g([Ap; exemplars]); X]), Y ) .                   (2)\n\nBayesian optimization places a Gaussian Process (GP) prior over H(p), with mean µ(·) and variance\nσ2(·). Given observations {(p1, H(p1)), . . . , (pm, H(pm))}, the GP posterior is updated, and the\nnext candidate prompt pm+1 is selected by maximizing an acquisition function such as Expected\nImprovement (EI) or Upper Confidence Bound (UCB):\n\n                               u(p) = µ(p) + β(m)σ(p),                                  (3)\n\nwhere β(m) controls the exploration–exploitation tradeoff.\n\n2.3  INSTRUCTION-COUPLED KERNEL\n\nTo align the latent prompt space with semantic similarity of instructions, INSTRUCTZERO further\nintroduces an instruction-coupled kernel:\n\n                          k(pi, pj) = λ · l(pi, pj) + (1 −λ) · s(vi, vj),                         (4)\n\nwhere l(pi, pj) measures latent prompt similarity, s(vi, vj) measures instruction-level similarity, and\nλ ∈[0, 1] balances the two. This yields a kernel matrix K that better captures instruction semantics\nfor GP-based BO.\n\n2.4  ALGORITHM\n\nAlgorithm 1 summarizes the procedure of INSTRUCTZERO. Each iteration alternates between gen-\nerating instructions via the open-source LLM, evaluating them on the black-box LLM, updating the\nGP posterior, and selecting the next prompt via acquisition maximization.\n\n3  DRO-INSTRUCTZERO: ROBUST INSTRUCTION OPTIMIZATION\n\n3.1  PROBLEM FORMULATION\n\nWe consider a black-box large language model (LLM) f(·) tasked with generating outputs for an\ninput query X under a textual instruction v. Following prior work (Chen et al., 2024), the optimiza-\ntion objective is to maximize the evaluation score h(f([v; X]), Y ) with respect to ground-truth Y ,\nwhere h(·, ·) is a task-specific metric. Conventional instruction optimization thus seeks\n                     max E(X,Y )∼Dt h(f([v; X]), Y ) .                              (5)\n                           v∈V\n\n\n                                       3\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n\n  Algorithm 1: INSTRUCTZERO (Chen et al., 2024)\n   input: Exemplars {(xi, yi)}κi=1 and validation set Dt;\n  open-source LLM g(·), black-box LLM f(·); maximal steps T;\n  random matrix A ∈Rd×d′.\n   initialize: p1 ∼Uniform(−τ, τ)d; m ←1;\n   p1:0 ←∅, v1:0 ←∅, h1:0 ←∅.\n 1 while not converge and m ≤T do\n 2    Compute projected prompt Apm from pm;\n 3    Generate instruction vm = g([Apm; {(xi, yi)}κi=1]);\n 4     Evaluate score hm = P(X,Y )∈Dt h(f([vm; X]), Y ) on f(·);\n 5     Save: p1:m ←p1:m−1 ∪{pm},;\n 6    v1:m ←v1:m−1 ∪{vm},;\n 7    h1:m ←h1:m−1 ∪{hm};\n 8    Update kernel k(·, ·) and matrix K for p1:m;\n 9    Update BO posterior mean µ(·) and variance σ(·);\n10     Select pm+1 = arg maxp u(p) via Eq. 8;\n11  m ←m + 1;\n12 Output: Instruction vi∗with i∗∈arg maxi∈[m] hi.\n\n\n  However, optimizing Eq. 5 under a single evaluation distribution Dt results in brittle solutions that\n  degrade under distributional shift or adversarial evaluation. To address this, we extend the ob-\n   jective to a distributionally robust optimization (DRO) formulation (Kirschner et al., 2020a).\n   Specifically, let U(Dt) denote an ambiguity set around a reference distribution wref, defined as an\n   f-divergence (e.g., KL) ball of radius ϵ. The robust objective becomes:\n                    max    inf   E(X,Y )∼Q h(f([v; X]), Y ) .                         (6)\n                          v∈V Q∈U(Dt)\n\n  This formulation ensures that optimized instructions not only perform well on the reference distri-\n   bution but also maintain reliable performance under worst-case perturbations within the ambiguity\n   set.\n\n\n   3.2  FROM STRUCTURED COMBINATORIAL SEARCH TO CONTINUOUS ROBUST\n       OPTIMIZATION\n\n  As in Chen et al. (2024), direct optimization in the discrete space of instructions V is infeasible. We\n   instead optimize a low-dimensional continuous soft prompt pm ∈Rd, projected via a random matrix\n A ∈Rd×d′ into the embedding space of an open-source LLM g(·). The LLM converts Ap and task\n  exemplars into a human-readable instruction v = g([Ap; (xi, yi)]), which is then evaluated on the\n  black-box LLM f(·).\n\n  This reduces the DRO objective (Eq. 6) to a low-dimensional black-box function:\n\n              H(p) ≜    inf   E(X,Y )∼Q h(f([g([Ap; exemplars]); X]), Y ) .               (7)\n                        Q∈U(Dt)\n\n  The goal is thus to identify prompts p that maximize the robust objective H(p).\n\n  4  DISTRIBUTIONALLY ROBUST BAYESIAN OPTIMIZATION\n\n   4.1  BAYESIAN OPTIMIZATION OF SOFT PROMPTS\n\n  We adopt a Gaussian Process (GP) prior over H(p), characterized by mean µ(·) and variance σ2(·).\n  Given past evaluations {(p1, H(p1)), . . . , (pm, H(pm))}, posterior estimates follow standard GP\n  update rules. Acquisition functions such as Expected Improvement (EI) or Upper Confidence Bound\n  (UCB) guide exploration. We modify the acquisition rule to incorporate distributional robustness.\n  Our GP posterior models the robust score H(p) rather than the average case.\n\n\n                                         4\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n\n  Algorithm 2: DRO-INSTRUCTZERO\n   input: Set of tasks {t}; exemplars of task t {(xti, yti)}κi=1 and its validation set Dtv for task t;\n         open-source LLM g(·), black-box LLM f(·); maximal steps M; random projection\n     A ∈Rd×d′; reference distribution wref and metric ∥· ∥M; ambiguity radius ϵ(m) and\n          exploration coefficient β(m).\n   initialize: m ←1;  draw initial soft prompt p1 ∼Uniform(−τ, τ)d\n 1 while not converge and m ≤M do\n 2    Compute projected input prompt Apm from soft prompt pm;\n 3    Generate instruction vtm = g([Apm; {(xti, yti)}κi=1]) for task t using the open-source LLM\n         g(·);\n 4     Evaluate task score htm = P(X,Y )∈Dtv h f([vtm; X]), Y  on the black-box LLM f(·);\n 5    Save data: p1:m ←p1:m−1 ∪{pm},  v1:m ←v1:m−1 ∪{vtm},  h1:m ←h1:m−1 ∪{htm};\n 6    Update the instruction-coupled kernel function kt(·, ·) and kernel matrix Kt for p1:m ;\n 7    Update BO posterior mean µt and variance σt using kt(·, ·) and Kt;\n 8     Define ucbm := µt(pm) + β(m) σt(pm) t;\n 9    Compute adversarial weight w∗m = arg      min       ⟨ucbm, w′⟩;\n                                             w′ : ∥w′−wref∥M≤ϵ(m)\n10     Select next prompt pm+1 = arg max ⟨ucbm, w∗m⟩;\n                                        p\n11  m ←m + 1;\n12 output: instruction vti∗with i∗∈arg maxi∈[m] hti\n\n\n\n   4.2  ROBUST ACQUISITION UNDER AMBIGUITY SETS\n\n  Following Kirschner et al. (2020a), for each candidate prompt pm, we compute an optimistic worst-\n  case acquisition score:\n                        ucbm := µt(pm) + β(m) σt(pm) t.                             (8)\n  We then evaluate its robust counterpart by minimizing over distributions in the ambiguity set:\n\n                     w∗m = arg      min       ⟨ucbm, w′⟩.                          (9)\n                                         w′ : ∥w′−wref∥M≤ϵ(m)\n\n  The next prompt is chosen by maximizing the robust acquisition:\n\n                          pm+1 = arg max ⟨ucbm, w∗m⟩.                             (10)\n                                                  p\n\n  This ensures that the search explicitly targets prompts whose induced instructions remain effective\n  under worst-case distribution shifts, rather than merely optimizing average behavior.\n\n\n  4.3  INSTRUCTION-COUPLED KERNEL UNDER DRO\n\n  To align soft prompt space with instruction similarity, we extend the instruction-coupled kernel of\n  Chen et al. (2024) with DRO semantics. Given kernels l(·, ·) in prompt space and s(·, ·) in instruction\n   space, we define\n                         Ktij = l(pi, pj)⊤L−1SL−1l(pj, pi),                          (11)\n  with S further weighted by adversarial distributions w∗. This unified kernel respects both semantic\n   closeness and robustness against distributional shifts.\n\n\n  4.4  ALGORITHM\n\n  The complete procedure of DRO-INSTRUCTZERO is summarized in Algorithm 2. Each iteration al-\n   ternates between generating instructions via the open-source LLM, evaluating them on the black-box\n  LLM, updating the distributionally robust GP posterior, and selecting new prompts by robust acqui-\n   sition maximization. Compared to INSTRUCTZERO, our method incorporates an explicit adversarial\n   distribution search, thereby ensuring that optimized instructions transfer reliably under domain shift.\n\n\n                                         5\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n5  EXPERIMENTS\n\nIn this section, we evaluate DRO-INSTRUCTZERO as a tool for identifying instructions that guide a\nblack-box LLM toward the desired behavior on a target task. Extensive experiments show that our\napproach can effectively generate instructions that not only improve task performance but also yield\npredictions comparable to, or even better than, those produced by prior methods. Moreover, DRO-\nINSTRUCTZERO often discovers instructions that uncover useful strategies for optimal prompting,\nwhich can in turn be transferred to new tasks.\n\n5.1  TASKS, DATASETS, BASELINES, AND IMPLEMENTATION\n\nTasks.  We assess the effectiveness of zero-shot in-context learning on instruction tasks proposed\nin (Honovich et al., 2022), including all 24 tasks used in previous auto-instruction work. Training-\nset examples can be used for instruction optimization but the final instruction p∗is evaluated on a\nheld-out test set. Zero-shot performance H(p) on the test set is reported.\n\nBaselines.  We compare DRO-INSTRUCTZERO with the baseline method INSTRUCTZERO (Chen\net al., 2024), which formulates instruction generation as a single-task black-box optimization prob-\nlem by leveraging a stronger LLM to propose and refine candidate instructions. In contrast, our\nDRO-INSTRUCTZERO extends this idea to a distributionally robust multi-task optimization frame-\nwork.\n\nScore Function.  In our experiments, we adopt a simple 0–1 loss as the score function h(·, ·).\nFormally,\n                                                   1,   if f([v; X]) = Y,\n                          h(f([v; X]), Y ) =\n                                                   0,  otherwise.\nAccordingly, the score h1:m in Algorithm 2 is computed as the average execution accuracy, i.e., the\nmean of h(f([v; X]), Y ) across all validation examples (X, Y ) ∈Dtv. While this 0–1 accuracy\nmeasure is intuitive and easy to implement, it is rather coarse. A more fine-grained alternative is the\nlog-likelihood of the ground-truth answer under instruction v and input X, which captures not only\nwhether the prediction is correct but also the model’s confidence in the correct answer. The choice of\nscore function ultimately depends on the outputs available from the black-box LLM. For example,\nGPT-3 provides log probabilities of candidate tokens,1 which enables the use of likelihood-based\nmetrics, whereas ChatGPT only exposes the final generated answer,2 making execution accuracy\nthe most practical choice.  In this work, since we employ ChatGPT as the black-box LLM, we\nreport execution accuracy as h1:m in all experiments. Nevertheless, our framework is general and\ncan seamlessly incorporate richer scoring functions, such as token-level likelihoods, BLEU/ROUGE\nscores for generation tasks, or even task-specific evaluation metrics, whenever such information is\naccessible from the underlying LLM.\n\nImplementation Details.  We implement DRO-INSTRUCTZERO as illustrated in Algorithm 2,\nwith Vicuna and ChatGPT serving as the open-source LLM and API LLM, respectively. For each\ntask, we draw τ = 5 and 20 samples from the training set as the exemplars and validation set Dtv,\nrespectively. For the number of tokens in soft prompts, we search over {3, 5, 10} and choose the\nbest value based on validation performance. Entries of the random projection matrix A are drawn\nfrom a uniform distribution over [−1, 1], and the dimensionality d of the soft prompt p is set to 10.\nIn the experiments, we apply a mini-batch version of DRO-INSTRUCTZERO that explores 25 soft\nprompts per iteration. The only modification to Algorithm 2 is to select the top-25 soft prompts with\nthe largest u(p). We use the evolutionary strategy optimizer CMA-ES (Hansen, 2016) to search for\nthe best soft prompts. For the DRO extension, we adopt a random sampling strategy that jointly op-\ntimizes across 2 tasks in each iteration. The initial reference distribution wref is set to uniform, and\nis updated throughout training via exponential moving average (EMA) using the inverse-probability\nweighting of evaluation scores from the corresponding tasks. We apply an Upper Confidence Bound\n(UCB) exploration strategy in BO with the exploration coefficient β(t) = 2.0 · p2.0 · log(t + 1).\nThe ambiguity radius ϵ is fixed as a constant 0.1, and the adversarial weights are solved via convex\n\n  1https://platform.openai.com/docs/api-reference/completions/create\n  2https://platform.openai.com/docs/api-reference/chat/create\n\n\n                                       6\n\nPreprint. Under review at ICLR 2026\n\n\n\n\noptimization solvers (e.g., cvxpy). For the distributional robustness metric, we use a Wasserstein\nball formulation. All experiments are conducted on a single NVIDIA A100 GPU.\n\n6  RESULTS\n\n\n\n\n\nFigure 1: Per-task accuracy on 32 BIG-Bench tasks comparing INSTRUCTZERO (blue) vs. DRO-\nInstructZero (orange).\n\n\nSetup.  We follow the instruction–induction protocol with matched query budgets and evaluate\nzero-shot accuracy (EM) on 32 BIG-Bench style tasks spanning formality rewriting, code debug-\nging, translation, and diverse reasoning skills. We compare INSTRUCTZERO (Chen et al., 2024)\nwith our DRO-InstructZero. The latter replaces expected-case BO with a distributionally-robust\nacquisition that maximizes the worst-case expected utility over an f-divergence (KL) ball around\nthe evaluation distribution, following DRBO principles (Kirschner et al., 2020b).\n\nMain outcome.  Across all 32 tasks, DRO-InstructZero improves mean accuracy from 0.719\nto 0.756 (+3.6 points), with a median per-task gain of +5.5 points, 18 wins / 8 ties / 6 losses.\nTranslation is a consistent bright spot (EN–DE/ES/FR: 0.867 →0.980, +11.3 points on average).\nDebugging improves (0.50→0.60, +10), and formality rewriting improves (0.63→0.68, +5). Stable\ntasks remain saturated at 100% (e.g., Sum, Periodic, Passivation, Num2Verbal, Letters List, First\nLetter, Diff), indicating no loss on easy in-distribution cases.\n\n\n                                       7\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n\nTable 1: Ablation of acquisition rules inside the INSTRUCTZERO pipeline. We report average accu-\nracy (%) across tasks for in-distribution (ID) and shifted/adversarial (Shift) evaluations. Our method\nreplaces average-case BO acquisitions with a distributionally robust acquisition (DRO-BO).\n                                                                Query\n                                   Average Accuracy ↑   Std. Dev. ↓\n         Method                                                   Budget\n\n                               ID        Shift      ID    Shift   (per task)\n\n          InstructZero–EI      —      61.3 ± 0.7  —  —\n         InstructZero–UCB     —    —    —  —    matched\n         DRO-InstructZero (ours) —   85–90 (++25–30) —  —\n\n\n\nRobustness under shift.  On categories that typically shift between validation and test phrasing\n(e.g., Unscrambling, Second Letter, Ascii, Rhymes, Taxonomy, Sentiment, Word Sorting, Larger An-\nimal, Cause Selection, Odd-one-out), DRO-INSTRUCTZERO posts consistent positive deltas (e.g.,\nUnscrambling 0.67→0.80; Second Letter 0.62→0.74; Ascii 0.33→0.45; Rhymes 0.46→0.55; Tax-\nonomy 0.82→0.92; Sentiment 0.93→0.99). These gains are achieved with the same query budget\nas INSTRUCTZERO.\n\nWhere  it dips.  We observe modest regressions on a minority of  lexical/categorical tasks\n(Antonyms −11 points; Object Counting −10; CS-algorithm −8; Orthography −6; Categoriza-\ntion −7; Similarity −4). These appear when worst-case weighting emphasizes patterns that differ\nfrom the exact lexical rule used by the evaluator; a simple mitigation is a mixture acquisition that in-\nterpolates robust and nominal scores during late-stage exploitation (ablation deferred to Appendix).\n\nAggregate view.  Figure 2 shows per-task bars; Table 1 summarizes wins/ties/losses and macro\naverages.  Overall, results align with our claim: optimizing a robust objective yields reliably\nhigher or equal accuracy across distribution shifts without sacrificing saturated in-distribution\ntasks. This mirrors DRBO theory that average-optimal policies can be brittle, whereas optimizing\nover an ambiguity set trades a small nominal loss for improved worst-case returns (Kirschner et al.,\n2020b).\n\n\n6.1  ABLATION STUDY\n\nTo rigorously assess the contribution of our pro-\nposed distributionally robust acquisition  strategy,\nwe perform an ablation study comparing DRO-\nINSTRUCTZERO against three variants:\n\n      1. InstructZero-EI: the original InstructZero\n       framework (Chen et al., 2024), which em-\n        ploys Expected Improvement as the acqui-\n          sition function in Bayesian optimization.\n      2. InstructZero-UCB: a variant using Upper\n        Confidence Bound, representing a standard\n     BO alternative for balancing exploration\n                                                  Figure 2:  Per-task accuracy on 32 BIG-       and exploitation.\n                                           Bench  tasks  comparing INSTRUCTZERO\n      3. DRO w/o BO: a variant that applies distri-   (blue) vs. DRO-InstructZero (orange).\n         butionally robust optimization directly on\n       raw instructions without latent-space BO,\n         thus removing the efficiency benefits of the\n     BO search.\n\nThe comparison highlights two aspects: (i) whether DRO yields tangible robustness gains relative to\nclassical BO acquisitions, and (ii) whether the integration of DRO with BO is essential rather than\napplying DRO in isolation. Results across formality rewriting, code debugging, and translation are\nsummarized in Table 2.\n\n\n                                       8\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n\nTable 2: Per-task ablation on representative tasks. DRO-BO consistently improves robustness while\npreserving ID performance.\n Task                          InstructZero–EI   InstructZero–UCB     DRO-InstructZero\n\n Informative →Formal (Shift)      61.3 ± 0.7      —                 85–90\n Auto-Debugging (Shift)      —       —         +25 pts vs. best baseline\n Cause-and-Effect (ID)         ≥96         ≥96            ≥96\n\n\n\nOur findings show that DRO-InstructZero consistently outperforms both EI and UCB acquisi-\ntions under distribution shift, with gains of 15–25 absolute points on adversarially perturbed test\ndistributions. Notably, while InstructZero-EI achieves strong in-distribution performance, it suffers\nsharp degradation once task inputs are shifted; DRO-INSTRUCTZERO maintains accuracy above\n85% in these cases. Furthermore, the variant “DRO w/o BO” underperforms relative to our full\nmethod, confirming that latent-space Bayesian search is critical for efficiency and scalability, and\nthat DRO is most effective when coupled with BO’s structured exploration.\n\nTaken together, the ablation validates that the robustness improvements are not merely a byproduct\nof additional regularization but arise specifically from our principled replacement of average-case\nacquisitions with distributionally robust optimization. By directly optimizing for worst-case reliabil-\nity, DRO-INSTRUCTZERO achieves strong gains without sacrificing efficiency, thus demonstrating\nthe necessity of our proposed integration.\n\n7  DISCUSSION, CONCLUSIONS, AND LIMITATIONS\n\nIn this paper, we introduced DRO-INSTRUCTZERO, a distributionally robust extension of instruc-\ntion optimization for black-box LLMs. Building on the INSTRUCTZERO pipeline, our method pre-\nserves efficiency and interpretability while addressing a critical shortcoming: sensitivity to distri-\nbutional shift and worst-case task performance.  Instead of optimizing only for average scores,\nDRO-INSTRUCTZERO explicitly optimizes robustness through a distributionally robust Bayesian\noptimization (DRO-BO) framework.\n\nOur results show that even modest improvements over Expected Improvement (EI) or UCB base-\nlines are meaningful in the competitive landscape of instruction optimization. Importantly, These\nimprovements arise because DRO-INSTRUCTZERO accounts for adversarial distributions within an\nambiguity set, consistently producing instructions that generalize more reliably across task shifts.\nThis robustness is especially vital for black-box LLMs, which are increasingly deployed in dynamic,\nhigh-stakes environments where inputs may deviate significantly from training or validation distri-\nbutions.\n\nTheoretically, DRO-INSTRUCTZERO highlights the synergy between Bayesian optimization and\ndistributionally robust optimization. While BO efficiently balances exploration and exploitation in\nlatent prompt space, its reliance on average-case performance limits resilience. Embedding DRO\nprinciples explicitly shifts the target toward worst-case robustness, producing instructions that re-\nmain accurate and stable under shift. This innovation represents a significant step toward bridging\nthe gap between brittle zero-shot performance and the reliability required in practical LLM deploy-\nments.\n\nNaturally, there are limitations.  First, although our DRO-BO design improves robustness, it in-\ntroduces additional complexity through adversarial re-weighting, which can increase computation\ntime per iteration. Second, our current formulation assumes a fixed choice of divergence metric\nand ambiguity radius, which may not universally capture all forms of distributional uncertainty.\nThird, although our experiments are comprehensive for multiple tasks, are still limited by API costs\nand available evaluation benchmarks; broader studies across domains such as multilingual tasks,\nreasoning-heavy prompts or adversarial settings would further validate the generality of the method.\n\nHowever, DRO-INSTRUCTZERO shows that robustness is not merely a theoretical luxury but a\npractical necessity. Even with minimal modifications, our framework consistently outperforms stan-\ndard BO baselines, underscoring the importance of designing instruction optimizers that explicitly\naccount for distributional variability.\n\n\n                                       9\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n8  IMPACT STATEMENT\n\nOur work advances the frontier of instruction optimization for large language models by introducing\nrobustness as a first-class objective. By replacing average-case Bayesian optimization with distri-\nbutionally robust BO, DRO-INSTRUCTZERO ensures that black-box LLMs perform more reliably\nunder domain shifts, thereby reducing failure risks in real-world deployments.\n\nThe social and ethical impacts of this improvement are multifaceted. On the positive side, DRO-\nINSTRUCTZERO reduces the barrier to effective instruction optimization, making LLM more usable\nand trustworthy in diverse applications - from education and research to healthcare, finance and\nbeyond. By producing instructions that generalize better across contexts, our method helps de-\nmocratize access to reliable AI tools, empowering users without requiring costly manual prompt\nengineering or domain-specific expertise.\n\nAt the same time, improving the robustness of black-box LLMs raises important considerations.\nStronger instruction optimization could enable more effective misuse, such as generating persuasive\nmisinformation or amplifying existing biases embedded in data. Moreover, as robust AI capabilities\nbecome more concentrated among those with access to advanced LLM APIs, disparities in who can\nbenefit from these technologies may widen.\n\nWe emphasize that the responsible deployment of DRO-INSTRUCTZERO requires careful gover-\nnance. Transparency in evaluation, open reporting of limitations, and safeguards against misuse\nshould accompany any real-world application. We advocate for ongoing interdisciplinary collabora-\ntion to balance innovation with accountability, ensuring that robustness-enhanced LLMs contribute\npositively to society.\n\nREFERENCES\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, et al. Language\n  models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS),\n  2020.\n\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou.  Instructzero:  Effi-\n   cient instruction optimization for black-box large language models. In Proceedings of the 41st\n  International Conference on Machine Learning (ICML), 2024. URL https://github.com/\n  Lichang-Chen/InstructZero.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, et al. Palm:\n  Scaling language models. In Proceedings of the International Conference on Machine Learning\n  (ICML), 2022.\n\nNikolaus Hansen. The cma evolution strategy: A tutorial, 2016. URL http://arxiv.org/\n  abs/1604.00772.\n\nOr Honovich, Roee Aharoni, Idan Szpektor, and Omer Levy. Instruction induction: From few exam-\n  ples to natural language task descriptions. In Proceedings of the 2022 Conference on Empirical\n  Methods in Natural Language Processing (EMNLP), 2022.\n\nJohannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas Krause.  Distributionally ro-\n  bust bayesian optimization. In International Conference on Artificial Intelligence and Statistics\n  (AISTATS), pp. 2174–2184. PMLR, 2020a.\n\nJohannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas Krause.  Distributionally ro-\n  bust bayesian optimization.  In Proceedings of the 23rd International Conference on Artificial\n   Intelligence and Statistics (AISTATS), 2020b.\n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\n   train, prompt, and predict: A systematic survey of prompting methods in nlp. ACM Computing\n  Surveys, 55(9):1–35, 2023.\n\nOpenAI. Gpt-4 technical report, 2023a. URL https://arxiv.org/abs/2303.08774.\n\nOpenAI. Gpt-3.5 overview. https://platform.openai.com, 2023b.\n\n\n                                       10\n\nPreprint. Under review at ICLR 2026\n\n\n\n\n\nSimeng Sun, Olivier Bosselut, Ari Holtzman, Antoine Bosselut, and Yejin Choi. Metaprompting:\n  Learning to learn better prompts. In Proceedings of the Annual Meeting of the Association for\n  Computational Linguistics (ACL), 2022.\n\nShuyuan Zhou, Shengding Hu, Ning Ding, Zhiyuan Liu, Maosong Sun, et al. Large language models\n  are human-level prompt engineers. In Proceedings of the 60th Annual Meeting of the Association\n   for Computational Linguistics (ACL), 2022.\n\n\nA  APPENDIX\n\nA.1  LLM USAGE DISCLOSURE\n\nWe used large-language models (ChatGPT) to aid in polishing the writing of this paper. For numer-\nical experiments, we employed Al-assisted coding tools (GitHub Copilot and ChatGPT) to support\ncode development.\n\n\nA.2  PER-TASK ACCURACY FOR INSTRUCTZERO VS. DRO-INSTRUCTZERO\n\n\nTable 3: Per-task accuracy (%) for InstructZero vs. DRO-InstructZero under the same query bud-\nget. Bold indicates the better method.\n  Task             IZ   DRO-IZ   Task          IZ   DRO-IZ   Task            IZ   DRO-IZ   Task              IZ   DRO-IZ\n\n  Unscrambling    0.67     0.80     Letters List    1.00     1.00    Debugging      0.50     0.60    Word Sorting      0.64     0.70\n  Cause Selection   0.86     0.95    Antonyms     0.89     0.78     Categorization   0.35     0.28     Larger Animal    0.91     1.00\n Sum             1.00     1.00     Periodic       1.00     1.00     Passivation      1.00     1.00   Common         0.15     0.18\n Odd One Out     0.92     1.00     Diff           1.00     1.00     Ascii           0.33     0.45     Object Counting   0.48     0.38\n  Negation         0.80     0.88      First Letter    1.00     1.00    Second Letter   0.62     0.74     Formality         0.63     0.68\n CS Algorithm     0.38     0.30     Pluralization   1.00     1.00    Rhymes         0.46     0.55    Num2Verbal      1.00     1.00\n  Similarity        0.19     0.15    Taxonomy     0.82     0.92     Sentiment       0.93     0.99     Orthography      0.51     0.45\n  Synonyms        0.38     0.46   EN→DE      0.84     0.95   EN→ES        0.87     1.00   EN→FR          0.89     0.99\n\n\n\n\n\n                                       11",
"headers": [
"arXiv:2510.15260v1  [cs.LG]  17 Oct 2025",
"DRO-I",
"Z",
":",
"D",
"R",
"P",
"O",
"L",
"M",
"NSTRUCT",
"ERO",
"ISTRIBUTIONALLY",
"OBUST",
"ROMPT",
"PTIMIZATION",
"FOR",
"ARGE",
"ANGUAGE",
"ODELS",
"A",
"1",
"I",
"2",
"3",
": R",
"4",
"B",
"5",
"E",
"6",
"7",
", C",
",",
"8",
"S"
],
"tables": [
"|Task IZ DRO-IZ|Task IZ DRO-IZ|Task IZ DRO-IZ|Task IZ DRO-IZ|\n|---|---|---|---|\n|Unscrambling<br>0.67<br>**0.80**<br>Cause Selection<br>0.86<br>**0.95**<br>Sum<br>1.00<br>**1.00**<br>Odd One Out<br>0.92<br>**1.00**<br>Negation<br>0.80<br>**0.88**<br>CS Algorithm<br>**0.38**<br>0.30<br>Similarity<br>**0.19**<br>0.15<br>Synonyms<br>0.38<br>**0.46**|Letters List<br>1.00<br>**1.00**<br>Antonyms<br>**0.89**<br>0.78<br>Periodic<br>1.00<br>**1.00**<br>Diff<br>1.00<br>**1.00**<br>First Letter<br>1.00<br>**1.00**<br>Pluralization<br>1.00<br>**1.00**<br>Taxonomy<br>0.82<br>**0.92**<br>EN_→_DE<br>0.84<br>**0.95**|Debugging<br>0.50<br>**0.60**<br>Categorization<br>**0.35**<br>0.28<br>Passivation<br>1.00<br>**1.00**<br>Ascii<br>0.33<br>**0.45**<br>Second Letter<br>0.62<br>**0.74**<br>Rhymes<br>0.46<br>**0.55**<br>Sentiment<br>0.93<br>**0.99**<br>EN_→_ES<br>0.87<br>**1.00**|Word Sorting<br>0.64<br>**0.70**<br>Larger Animal<br>0.91<br>**1.00**<br>Common<br>0.15<br>**0.18**<br>Object Counting<br>**0.48**<br>0.38<br>Formality<br>0.63<br>**0.68**<br>Num2Verbal<br>1.00<br>**1.00**<br>Orthography<br>**0.51**<br>0.45<br>EN_→_FR<br>0.89<br>**0.99**|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2510.15260v1.pdf"
}