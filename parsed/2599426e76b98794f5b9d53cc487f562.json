{
"text": "Dual-Phase Accelerated Prompt Optimization\n\n                     Muchen Yang1, Moxin Li2*, Yongle Li3, Zijun Chen3,\n                   Chongming Gao1*, Junqi Zhang4, Yangyang Li5, Fuli Feng 1,3\n                 1University of Science and Technology of China, 2National University of Singapore\n                           3Institute of Dataspace, Hefei Comprehensive National Science Center\n                                4AtomEcho Inc., 5Academy of Cyber\n              muchen00@mail.ustc.edu.cn, limoxin@u.nus.edu,  liyongle999@gmail.com\n           zijunchen248@gmail.com, chongminggao@ustc.edu.cn,  zhangjunqi@atomecho.xyz\n                           liyangyang@live.com,  fulifeng93@gmail.com\n\n                          Abstract                            SST-2      22.7%      Logical Five      6.7%\n                                                                   AG's News   10.0%        Hyperbaton     4.8%                  Gradient-free prompt optimization methods\n                 have made significant strides in enhancing the           TREC    12.0%    Disambiguation      6.4%\n                 performance of closed-source Large Language              Subj    14.4% Salient Translation      6.1%\n               Models (LLMs) across a wide range of tasks.\n                However, existing approaches make light of2024                                                             Figure 1:  Average accuracy improvement on eight\n                   the importance of high-quality prompt initial-\n                                                                           datasets with four optimization steps.\n                   ization and the identification of effective opti-\n                 mization directions, thus resulting in substan-Oct\n                        tial optimization steps to obtain satisfactory         Automatic prompt optimization can be broadly2\n                 performance. In this light, we aim to acceler-       categorized into gradient-based and gradient-free\n                   ate prompt optimization process to tackle the                                                          methods.  Gradient-based methods (Shin et al.,\n                   challenge of low convergence rate. We propose\n                                                            2020; Li and Liang, 2021; Liu et al., 2021b, 2022)\n                  a dual-phase approach which starts with gener-\n                                                                    are devised for open-source LLMs to enable the op-                   ating high-quality initial prompts by adopting\n                 a well-designed meta-instruction to delve into        timization of prompts through adjustments based[cs.CL]\n                    task-specific information, and iteratively opti-      on model gradient. Gradient-free methods have\n                mize the prompts at the sentence level, lever-      emerged as the predominant approach for closed-\n                 aging previous tuning experience to expand       source LLMs, which focuses on refining prompts\n                prompt candidates and accept effective ones.       without access to the model gradient (Prasad et al.,\n                  Extensive experiments on eight datasets demon-                                                          2022; Yang et al., 2023b; Guo et al., 2023). Start-\n                      strate the effectiveness of our proposed method,\n                                                              ing from initial prompts, these methods usually\n                  achieving a consistent accuracy gain over base-\n                                                       expand candidate prompts using searching meth-                    lines with less than five optimization steps.\n                                                         ods (Pryzant et al., 2023; Wang et al., 2023) and\n          1  Introduction                                then accepting the more prominent ones in an iter-\n                                                                      ative manner. This paper focuses on gradient-free\n         LLMs have demonstrated remarkable capabilities                                                         methods due to the distinguished abilities of closed-\n             across a wide range of natural language processing                                                               source LLMs and the challenge of optimizing their\n          (NLP) tasks, including machine translation (Qin                                                          prompts with limited model information.arXiv:2406.13443v2        et al., 2024), summarization (Goyal et al., 2022),\n                                           We argue that current gradient-free prompt opti-\n            and question answering (Zhang et al., 2023a). The\n                                                              mization methods have not adequately considered\n            dependency on prompt quality has led to the emer-\n                                                                 the rate of convergence. Typically, these methods\n            gence of prompt engineering (Diao et al., 2023b;\n                                                    demand an excessive number of optimization steps\n           White et al., 2023), aiming at crafting effective\n                                                                       to obtain satisfactory prompts due to the limited ac-\n            prompts to elicit the desired responses from LLMs.\n                                                                  cess to model details, the vast discrete search space,\n          As the need for efficient prompt design becomes\n                                                       and the uncertain optimization directions (Wang\n             increasingly evident (Liu et al., 2021b), automatic\n                                                                         et al., 2023; Pan et al., 2023; Yang et al., 2023b).\n            prompt optimization has been introduced to stream-\n                                                               Representative work such as OPRO (Yang et al.,\n               line the prompt design process, ensuring that LLMs\n                                                        2023b) even necessitates nearly 200 optimization\n              are utilized to their full potential (Gao et al., 2021;\n                                                                     steps for some NLP tasks. This requirement for ex-\n            Liu et al., 2021a; Reynolds and McDonell, 2021).\n                                                                   cessive optimization steps makes existing methods\n                        *  Corresponding Author                              impractical for real-world applications since users\n\n\n                                                    1\n\nare understandably reluctant to tolerate extensive      ing that the proposed method achieves satisfying\noptimization steps to achieve satisfactory perfor-     performance within few optimization steps.\nmance levels. Therefore, we aim to achieve accel-\nerated prompt optimization, obtaining satisfactory   2  Related Work\nperformance via few optimization steps (e.g., < 5).\n                                         The gradient-free prompt optimization for closed-\n  To achieve accelerated prompt optimization, two\n                                                 source LLMs typically contains two phases: ini-\ncrucial factors need to be considered: high-quality\n                                                          tialization and iterative optimization steps, where\ninitial prompts and effective optimization direc-\n                                                    the optimization step consists of expansion and\ntions. Firstly, the initialization of the prompt plays\n                                                       selection stages.\na crucial role in determining the efficiency of the\noptimization process (Ye et al., 2023), whereas ex-                                                       Initialization.  The prompt initialization for op-\nisting approaches pay insufficient attention to the                                                    timization can be  achieved manually  or  au-\nimpact of initialization on subsequent optimization.                                               tonomously. Manual initialization often entails\nTherefore, we aim to obtain initial prompts of high                                                     professional machine learning engineers formulat-\nquality, laying a solid foundation to accelerate opti-                                                   ing prompts, as delineated in (Pryzant et al., 2023).\nmization process. Secondly, the accelerated prompt                                                  Concurrently, works such as (Guo et al., 2023),\noptimization needs to identify the most effective op-                                               (Pan et al., 2023), and (Wang et al., 2023) utilize\ntimization directions in each step, streamlining effi-                                                       existing manual prompts as the foundational set to\ncient optimization from the initial prompts. Thus,                                                 harness human creativity. In contrast, automated\nwe aim to design a more refined expansion tuned                                                           initialization leverages the power of LLM genera-\nby experience and acceptance of candidate prompts                                                             tion, which is exemplified by (Zhang et al., 2023b),\nenhanced by examination of failure cases.                                                  generating prompts from few-shot exemplars and\n  To this end, we propose a dual-phase approach                                                 a rudimentary description, and (Zhou et al., 2022),\nto achieve the accelerated gradient-free prompt                                                      fabricating prompts based on meta-prompts and\noptimization.   Our approach consists of two                                                              illustrative input-output examples. Our method be-\nphases: high-quality initial prompt generation, and                                                   longs to the automated initialization, improving the\nexperience-tuned optimization. Firstly, we utilize a                                                                 initial prompt generation for acceleration.\nwell-designed meta-instruction to guide the LLM\nin generating high-quality and structured initial   Optimization.  The optimization step is achieved\nprompts that contain task-specific information, in-   by expanding prompt candidates by modifying\ncluding task type and description, output format   from the initial prompt and selecting the better can-\nand constraints, suggested reasoning process, and    didates for the next iteration. The expansion stage\nprofessional tips. After that, we devise a sentence-   can be executed through rephrasing, as in (Zhou\nlevel prompt optimization strategy for efficiently    et al., 2022), where high-scoring prompts undergo\noptimization on the long initial prompt, leverag-   evolution akin to a Monte Carlo search method-\ning previous direction tuning experience, together    ology, or through heuristic algorithms that auto-\nwith failure cases, to select sentences in the initial    matically revise prompts, as in (Guo et al., 2023)\nprompt to be expanded and accept effective prompt   and (Pan et al., 2023). More complex regenera-\ncandidates. Extensive experiments (cf. Figure 1)    tion strategies are employed by works like (Wang\non three LLMs across several datasets confirm the    et al., 2023), where the optimizer LLM progres-\neffectiveness and superiority of our method. Our    sively expands prompts based on task delineations\ncontributions are threefold:                      and historical iterations. The expansion can also\n                                             be implemented leveraging an open-source LLM\n‚Ä¢ We reveal the issue of low convergence rate in                                                   (Lin et al., 2023; Chen et al., 2023). Reinforcement\n  gradient-free prompt optimization, and highlight                                                    learning-based methods have also been adopted for\n  the problem of accelerated prompt optimization.                                            prompt modification (Diao et al., 2023a). More-\n                                                      over, the granularity of prompt modification ex-‚Ä¢ We propose a dual-phase approach, achieving\n                                                         hibits variation across studies.  Heuristic-based  accelerated prompt optimization through high-\n                                            methods and (Hsieh et al., 2023) work operate at  quality initial prompt generation and experience-\n                                                    the word/token granularity, while classical opti-  tuned optimization.\n                                                  mization algorithms like (Pryzant et al., 2023; Zhou\n‚Ä¢ We conduct extensive experiments, demonstrat-    et al., 2022) consider the entire prompt. The se-\n\n\n                                         2\n\nlection stage generally utilized the performance of    3.2  Accelerated Prompt Optimization\nthe prompt on a held-out validation set (Pryzant                                              Although current research on gradient-free prompt\net al., 2023; Zhou et al., 2022; Wang et al., 2023),                                                  optimization can achieve significant performance\nwhile recent work also explores human preference                                                   gains on multiple tasks, demands for a great num-\nfeedback (Lin et al., 2024) or score feedback from                                                    ber of optimization steps hinder their practicability\nother LLMs (Yang et al., 2024).                                                      in real-world scenarios. For instance, Yang et al.\n                                                 (2023b) does not converge even after over 150 steps\n3  Problem Formulation                                                         in some tasks; Wang et al. (2023) finds a good solu-\n                                                       tion in 50 to 75 steps. Therefore, we highlight the3.1  Gradient-Free Prompt Optimization\n                                            problem of accelerated prompt optimization, i.e.,\nFor a target NLP task T with input x, the closed-                                                   obtaining p‚àówith satisfactory performance in few\nsource LLM predicts the output ÀÜy given x concate-                                                   optimization steps, e.g., K < 5.\nnated with the prompt p, where x, ÀÜy and p are all\nword sequences. The aim for prompt optimization   4  Proposed Method\nis to find an optimal prompt p‚àóthat obtains the de-\n                                                    4.1  Motivationsired ÀÜy, which can be evaluated by metrics such as\naccuracy with reference to the ground truth y. The  We believe that two factors are crucial for achiev-\ngradient-free prompt optimization contains an ini-   ing accelerated prompt optimization, which current\ntialization phase followed by K iterative optimiza-   gradient-free prompt optimization methods fail to\ntion steps. The k-th optimization step starts from an    achieve. Firstly, the initial prompt p0 plays a cru-\ninitial prompt pk‚àí1, k ‚àà[1, K], and sequentially    cial role in accelerating the prompt optimization\nperforms two stages: expansion of prompt candi-   process (Ye et al., 2023), where p0 with better LLM\ndates, and acceptance of the prominent prompts as   performance makes the optimization towards bet-\nthe next initial prompts, as detailed below.             ter prompts easier, preventing LLMs from exces-\n                                                      sively exploring suboptimal prompt regions. This\nExpansion of Prompt Candidates.  At the k-th                                                                is generally overlooked by existing research that\noptimization step, The expansion stage search for                                                           utilizes uninformative initial prompts, e.g., (Yang\nnew prompt candidates with potential better perfor-                                                           et al., 2023b). Therefore, we propose to devise\nmance starting from pk‚àí1, with searching methods                                                    high-quality p0 by crafting a novel initial prompt\nsuch as edit-based (Prasad et al., 2022) and LLM                                              schema. Furthermore, a more precise expansion\nrewriting (Pryzant et al., 2023). Formally, the ex-                                              and acceptance of prompt candidates ensure highly\npansion function fE(¬∑) generates prompt candidate                                                             efficient optimization direction and fewer optimiza-\nset P kc = {pck1, ¬∑ ¬∑ ¬∑ , pckQ} with size Q.                tion steps. Current expansion and acceptance tech-\n                                                  niques optimize the prompt towards improving the\n          P kc = fE(pk‚àí1).              (1)    general task performance, where effective optimiza-\n                                                      tion direction in each step is hard to ensure. To\nAcceptance of Prominent Prompts.  The ac-   tackle this, we propose to utilize the past failure\nceptance stage evaluates the performance of each    cases from previous optimization steps to further\nprompt candidate in P kc to determine whether it    navigate the expansion and acceptance of prompt\nshould be continued for next optimization step.   candidates. We illustrate our dual-phase approach\nThis is usually achieved by evaluation on a held-out    as follows (cf. Figure 2).\nvalidation set V = {(xv, yv)}, and accepting the\ntop-performing prompt candidates. Formally, with    4.2  High-Quality Initial Prompt Generation\nthe evaluation function on LLM as fS(¬∑),        We think that a high-quality initial prompt that can\n                                                                elicit the desired response from LLMs should be\n   rki = fS(pcki, V ), i ‚àà[1, ¬∑ ¬∑ ¬∑ , Q],           (2)    able to provide clear task instruction and detailed\n   pk = pckj, where j = argmax({rk1, ..., rkQ}).       task-related information. Specifically, it should 1)\n                                                   give a clear definition of the task type and provide\nwhere argmax(¬∑) denotes the index of the maxi-   a detailed task description, 2) define the output for-\nmum value. At the final optimization step, the   mat and constraints, 3) provide insights on the rea-\ntop-performing prompt pK will be accepted as the    soning processes and professional tips. To achieve\noptimized prompt p‚àó.                            such initial prompts, we are inspired by the step-\n\n\n                                         3\n\nInitial Prompt Generation                                                      )                                                        Prompt ùëù),# = [ùë†#), ‚Ä¶ , ùë†$ ]\n                                Training Data: {(ùë•%, ùë¶%)}\n                 Meta-               Ôºå   Ôºå‚Ä¶Ôºå\n                instruction ùêº(                                               )         )                 )                             LLM                       ùë§#      ùë§*          ùë§$\n                                                                            Failure cases:       Sampling\n             Initial Prompt                                           ùêπ) = {(ùë•+), ùë¶+))}                      #  Ôºå   Ôºå‚Ä¶Ôºå       ùëù\" = [ùë†##, ‚Ä¶ , ùë†$ ]                                                                                                             ùë†-)\n                                                                 Meta-\n                                                                          instruction ùêº!                                                                  LLM\n         √óùêæ   Experience-tuned Optimization\n                                                        Prompt ùëù)                                           ùë†ÃÇ-)\n      Optimized              Prompt\n     ùëù‚àó= [ùë†#',          ‚Ä¶ , ùë†$']  Ôºå   Ôºå‚Ä¶Ôºå       Ôºå   Ôºå‚Ä¶Ôºå\n\n\n                               Figure 2: Illustration of the proposed method.\n\n\nback prompting (Zheng et al., 2023) which demon-   aging the past failure cases. We first split the ini-\nstrates LLM‚Äôs ability to derive high-level concepts     tial prompt p0 into M sentences, and initialize the\nand principles from examples. Thus, following   weight w1 for each sentence as 1.\n(Zhou et al., 2022), we design a meta-instruction\n                                                              p0 = [s11, s12, ..., s1M],            (4)Im (cf. Figure 3), leveraging LLM‚Äôs ability to gen-\nerate p0 by observing the input-output exemplars of             w1t = 1, t ‚àà[1, M].\nthe target task T and inferring the above required\n                                                   In the k-th optimization step, we compute the ac-information. Formally, defining input-output exem-\n                                                 ceptance probability Prk for each sentence:plars as D = {(xd, yd)},\n                                                             exp(wki )             p0 = LLM(Im, D).            (3)                Prki =                        .           (5)\n                                 PMj=1 exp(wkj )\n\n4.3  Experience-Tuned Optimization               After that, we sample a sentence for expan-\n                                                   sion based on the probability distribution Prk =In the optimization phase, it is necessary to tune the\n                                                       [Prk1, ¬∑ ¬∑ ¬∑ , PrkM], where the sampled sentence is de-expansion and acceptance of prompt candidates to\n                                                noted as sko, o ‚àà[1, M]. For expansion of sko, wequickly improve the task performance as evaluated\n                                                 design a meta-instruction Ie (cf. Figure 4) to in-on the validation set V and thus reduce optimiza-\n                                                          struct LLM to generate a revised sentence consid-tion steps. Inspired by previous research (Pryzant\n                                                     ering the past experience.et al., 2023), we intend to make the best of past fail-\nure cases to generate promising prompt candidates                   ÀÜsko = LLM(Ie, pk‚àí1, Fk, sko).        (6)\nand filter out unnecessary optimization attempts. In\neach optimization step, we maintain a failure case      Before passing ÀÜsko to the acceptance stage, we\nset Fk = {(xfk, yfk)} containing the examples from    design additional strategies to further guarantee the\nV where the initial prompt pk‚àí1 fails to predict the    effectiveness of the generated sentence leveraging\nground truth in the acceptance stage, i.e., ÀÜyfk Ã∏= yfk.   Fk. Firstly, to ensure ÀÜsko can actually improve over\n                                                  sko, we replace sko in pk‚àí1 with ÀÜsko, denoted as ÀÜpk,Expansion.  In the expansion stage, since the ini-\n                                            and evaluate whether ÀÜpk outperforms pk‚àí1 on Fk.tial prompts are long prompts with at least four sen-\n                                We accept ÀÜsko only when ÀÜpk has improved the per-tences, we aim to improve the expansion efficiency\n                                              formance over pk‚àí1 larger than a threshold HF .by segmenting them into individual sentences for\nsentence-level expansion following LongPO (Hsieh                                                                fS(ÀÜpk, Fk) ‚àífS(pk‚àí1, Fk) > HF .     (7)\net al., 2023). Moreover, since different sentences\nin the initial prompts contain different task-related    Besides, to avoid repeatedly generating the same\ninformation and may have different impacts on    ineffective ÀÜsko, we build a collection G of undesired\nthe task performance, we devise sentence weights    sentence revisions and check whether ÀÜsko has ap-\nwk to estimate the impact of each sentence on the   peared in G. If the above two criteria are not met,\nperformance improvement, which is updated lever-  we abandon ÀÜsko and regenerate starting from Eq. 6.\n\n\n                                         4\n\nmeta-instruction for optimization\n       meta-instruction for initialization\n                                                     I'm trying to write a zero-shot prompt which\n You gave me an instruction on a certain task\n                                                     consists of four parts.\n and some example inputs with chain-of-thought.\n                                                     My current prompt is:\n I read the instruction carefully and wrote an\n                                                     [{prompt_to_revise}]\n output with chain-of-thought for every input\n correctly. Here are some correct input-output\n                                                     But it gets the following outputs that fail to pairs which strictly meet all your\n                                                     match the expected outputs: requirements:\n                                                     {failed_cases}\n\n {example_pairs}\n                                                     The sentence I want to revise is:\n                                                     {sentences[chosen_sentence]}\n\n The instruction given contains the following\n                                                     Comparing the wrong outputs with their parts. Based on the input-output pairs\n                                                     corresponding expected answers under the same provided, give me the final complete\n instruction in English without any explanation:      input, optimize the above sentence to help AI\n                                                     understand the task more comprehensively and\n ###Task type###                                     accomplish this task better.\n Task type: This is a <...> task.                    Your response format is as follows.\n                                                     The given sentence\n ###Task detailed description###                     '{sentences[chosen_sentence]}' should be\n Task detailed description: <Task detailed           revised as:\n description>\n\n ###Your output must satisfy the following\n format and constraints###                           Figure 4: Meta-instruction used in the optimization\n Output format(type): <Output format or its          phase.\n type>\n Output constraints: <constraints on output>\n                                                gorithm to have converged.\n ###You must follow the reasoning process###\n <add several reasoning steps if it's               The weight formula is designed to adaptively up-\n necessary>                                          date the importance of each sentence in the prompt\n ###Tips###                                     based on its impact on overall performance im-\n <add several useful tips from a professional       provement. fR(ÀÜpk) modulates the magnitude of\n point of view to accomplish this task better>       the weight adjustment: a higher fR(ÀÜpk) leads to\n                                                       larger updates. Prki determines the weight‚Äôs con-\n                                                          tribution, while M is used for normalization toFigure 3: Meta-instruction used in our initialization\nphase to generate high-quality initial prompts.           ensure balanced weight updates. The learning rate\n                                           Œ∑ controls the extent of weight adjustments based\n                                           on the evaluation feedback. Inspired by the EXP3\nAcceptance.  In addition to evaluating ÀÜpk‚Äôs per-                                                 algorithm (Auer et al., 1995), these components\nformance on the entire failure case Fk, we also                                                               facilitate a dynamic and adaptive optimization pro-\nevaluate its performance on the validation set V .                                                       cess, tuned by empirical performance data. The\nWe accept ÀÜpk as the next initial prompt pk only                                      who process is summarized in Algorithm 1.\nwhen ÀÜpk has improved the performance over pk‚àí1\n                                       5  Experimentslarger than a threshold HV . Otherwise, we abandon\nÀÜpk and restart from sampling sko.                                                     In this section, we begin by detailing datasets, base-\n         fS(ÀÜpk, V ) ‚àífS(pk‚àí1, V ) > HV .      (8)    lines, and the implementation of the experiments.\n                                                 Following this, we conduct comprehensive and con-\nIf ÀÜpk is accepted, we update its sentence weights.                                                          trolled experiments on our method.\nWe calculate the mixed evaluation result fR(¬∑) and\nupdate the wk+1 as follows, where Œ± and the learn-   5.1  Experimental Settings\ning rate Œ∑ are adjusting hyperparameters.                                                 Datasets.  Our experiments are first conducted\n fR(ÀÜpk) = Œ±fS(ÀÜpk, V ) + (1 ‚àíŒ±)fS(ÀÜpk, Fk). (9)   on general natural language understanding tasks\n                                                     across four datasets to validate our method, specifi-\n  wk+1i  = wki exp(Œ∑fR(ÀÜpk) ).                       cally focusing on sentiment classification (SST-2\n                    Prki M                                                (Socher et al., 2013)), topic classification (AG‚Äôs\nWhen the number of times that Eq. 7 or Eq. 8 is   News (Zhang et al., 2015), TREC (Voorhees and\nnot satisfied accumulates to 5, we consider the al-   Tice, 2000)) and subjectivity classification (Subj\n\n\n                                         5\n\nAlgorithm 1                                               test split, we shuffle the data and allocate approxi-\nDual-Phase Accelerated Prompt Optimization       mately half for testing. The rest is used for training,\nRequire: Input-output exemplars D, validation   prompt generation, and optimization. For datasets\n     set V , meta-instruction Im and Ie.              with predefined test sets, we use those directly.\nEnsure: Optimized prompt p‚àó                      Unless otherwise stated, we evaluate perfor-\n  1: Initialize p0 (Eq. 3), derive failure case set F1   mance (i.e., accuracy) on GPT-3.5-Turbo using the\n  2: Split p0 into M sentences [s11, s12, . . . , s1M], ini-   OpenAI API1 (currently gpt-3.5-turbo-0125) in a\n     tialize sentence weights {w1i }Mi=1 ‚Üê1, k ‚Üê1    zero-shot prompt setting. The temperature is set to\n  3: while not converged do                     0 for prediction and 0.5 for prompt generation to en-\n  4:    ‚ñ∑Expansion                             hance diversity. To accelerate prompt optimization,\n  5:    Sample a sentence sko based on Prk (Eq. 5)   we limit the maximum optimization steps to four\n  6:    Generate revised sentence ÀÜsko (Eq. 6)         for all methods, while keeping other baseline pa-\n  7:    Replace sko in pk‚àí1 with ÀÜsko to get ÀÜpk         rameters and settings at default. At the beginning of\n  8:       if ÀÜsko ‚ààG or (Eq. 7) is not satisfied then     prompt initialization, eight exemplars are obtained\n  9:      Add ÀÜsko to G                        by concatenating unique input-output pairs from\n 10:        Regenerate ÀÜsko from line 6                the shuffled training data until the desired amount\n 11:    end if                                               is reached, ensuring no duplicate inputs. Due to\n 12:    ‚ñ∑Acceptance                                 limited computational resources, our approach gen-\n 13:       if (Eq. 8) is not satisfied then                 erates and optimizes only one initial prompt. By\n 14:         Restart from line 5                        default, we set HF = 0.3, HV = 0.1, Œ± = 0.4,\n 15:    end if                                and Œ∑ = 0.055 in Algorithm 1 to accelerate the\n 16:    pk ‚ÜêÀÜpk, update wk+1i     , k ‚Üêk + 1          optimization phase.\n 17:    Update Fk with new failure cases                                                    5.2  Main Results & Analysis\n 18: end while\n 19: return optimized prompt p‚àó= pk\n                                                                         Few-shot        Zero-shot\n\n                                                     Task            Manual APO APE  PA  Ours\n(Pang and Lee, 2004)). Then we perform our ap-    SST-2                            /      0.89  0.92  0.443 0.978\nproach to the challenging BBH tasks (Suzgun et al.,    AG‚Äôs News                     /      0.88  0.819 0.785 0.928\n                                       TREC                           /     0.795 0.513 0.687 0.785\n2022), which include manually provided few-shot                                                      Subj                              /      0.64  0.593 0.494  0.72\nChain-of-Thought (CoT) prompts containing task\n                                                           Logical Five         0.388   0.392 0.404 0.443  0.48\ndescriptions and demonstrations.                     Hyperbaton         0.744   0.808 0.865 0.823  0.88\n                                                       Disambiguation      0.580   0.688 0.645 0.696  0.74\nBaselines. We compare our method with three      Salient Translation   0.544   0.456 0.538 0.468 0.548\npopular prompt optimization methods for zero-shot     Avg.                 0.564   0.694 0.662 0.605 0.757\nblack-box prompting and the well-crafted prompts\nmanually provided in BBH tasks: APO (Pryzant    Table 1: Accuracy on eight tasks on GPT-3.5-Turbo.\net al., 2023): Generating natural language ‚Äúgradi-   PA indicates PromptAgent. Bold and underlined text\nents‚Äù to criticize and improve the current prompts.    indicate the best and second-best results, respectively.\nAPE (Zhou et al., 2022): Proposing both a naive\nand an iterative Monte Carlo search methods to ap-   Overall Results.  Table 1 demonstrates the effec-\nproximate the solution to the prompt optimization    tiveness of our accelerated dual-phase approach\nproblem. PromptAgent (Wang et al., 2023): Au-   across 8 NLP tasks compared to classic prompt\ntomating expert-level prompt generation by treat-   optimization methods. Our method significantly\ning it as a strategic planning problem using Monte   outperforms all baselines, achieving an average\nCarlo tree search and error feedback to refine   improvement of approximately 10.7% over APO,\nand optimize prompts. Manual Prompt (Suzgun   16.4% over APE, and 29.7% over PromptAgent\net al., 2022): The few-shot CoT version of human-   across the given tasks.\ndesigned prompts with teaching examples devel-     Our method also surpasses few-shot CoT human-\noped in BBH tasks.                                   crafted prompts with an approximately 17.6% aver-\n                                                age improvement on selected BBH tasks, indicatingImplementation Details.  In line with (Wang\net al., 2023), since BBH tasks lack an official train-      1https://chat.openai.com/\n\n\n                                         6\n\nOur method     APO     APE      PromptAgent\n              SST-2              AG's News            TREC                 Subj\n         1.0                                                          0.75\n                                 0.75                                                   0.6                                                          0.50\n                                 0.50         0.5             Accuracy                                                        Accuracy                                                           Accuracy 0.25                                                   Accuracy 0.4                                 0.25\n           0  1  2  3  4         0  1  2  3  4         0  1  2  3  4         0  1  2  3  4\n                  Step                     Step                     Step                     Step\n             Logical Five          Hyperbaton         Disambiguation       Salient Translation\n\n         0.4                                    0.8                                                   0.5                                                               0.6             Accuracy                                                           Accuracy 0.7                                                   Accuracy                                                           Accuracy         0.2                                                                              0.4                                                               0.4\n           0  1  2  3  4         0  1  2  3  4         0  1  2  3  4         0  1  2  3  4\n                  Step                     Step                     Step                     Step\n               Figure 5: Performance (accuracy) over 4 steps across 8 tasks on GPT-3.5-Turbo.\n\n\n                                                              Schema 4     Schema 3     Schema 2     Schema 1its ability to produce high-quality prompts that en-\nhance the black-box LLM‚Äôs capabilities in logical               SST-2             AG's News\ndeduction, grammar, language understanding, and                                                           0.95                      0.8\nmultilingual tasks without teaching examples.\n                                                                                        0.6                                                                                                                                                  Accuracy                                                         Accuracy                                                           0.90\nAnalysis.  To understand this result, we analyzed                                   0.4\nthe prompt expansion and acceptance processes:            0  1  2  3  4        0  1  2  3  4\n                                                                    Step                   StepIn prompt expansion, our method leverages past\nexperience, filters out unnecessary optimization at-   Figure 6: Results on GPT-3.5-Turbo with different ini-\ntempts, and collects undesired revisions. This con-     tial prompt schemas.\ntrasts with baseline methods that inefficiently ex-\n                                                                               0.01      0.03      0.055      0.07      0.09plore prompt space and underutilize past iterations.\n                                                        SST-2             AG's NewsAPE lacks reflection on past iterations, slowing its\n                                                                               0.925\nMonte Carlo-based search. APO uses error feed-       0.97\nback to guide beam search but is slowed by eval-       0.96                   0.900uating many paths. PromptAgent‚Äôs Monte Carlo           Accuracy 0.95                                           Accuracy                                                                               0.875\nSearch Tree explores prompt optimization through                                                       0  1  2  3  4         0  1  2  3  4\nsimulations, but limited steps lead to suboptimal                  Step                    Step\nresults.\n                                                       Figure 7: Results on GPT-3.5-Turbo with different opti-\n   In the acceptance process, inspired by the EXP3\n                                                      mization learning rates.\nalgorithm, our method uses weighted sentences and\nmodifications to enhance prompt quality, making\nit superior in identifying promising candidates and    reaching near-peak performance within the first\noptimizing directions.                         two steps. This rapid convergence highlights our\n                                                method‚Äôs efficiency in optimizing prompts quickly\nConvergence   Analysis.  To   evaluate   our                                            and effectively, making  it promising for tasks\nmethod‚Äôs convergence within four steps compared                                                     requiring prompt optimization within a few steps.\nto others, we examine how quickly each method\nachieves peak performance across datasets. Figure    5.3  Ablation Study\n5 shows the performance (accuracy) variation                                 We conduct several ablation experiments to assess\nof  four prompt  optimization methods  across                                                     the efficacy of our method.\neight datasets, with each subfigure representing\na  different  dataset.   While APO, APE, and    5.3.1  Different Initial Prompt Schemas\nPromptAgent experience fluctuations or plateau   Our method uses a meta-instruction to generate a\nat lower accuracy, our method demonstrates the   prompt with four components: a) task type and\nfastest convergence across most datasets, often    description, b) output format and constraints, c)\n\n\n                                         7\n\nOur method     APO\n            SST-2              AG's News            TREC                 Subj\n                                                         0.75\n     0.75                      0.85                                                   0.6\n                                                         0.50     Accuracy                                                                        Accuracy 0.80                                                Accuracy                                                                                                                                                                                                                  Accuracy 0.5     0.50\n         0  1  2  3  4          0  1  2  3  4          0  1  2  3  4          0  1  2  3  4\n                Step                     Step                     Step                     Step\n           Logical Five          Hyperbaton         Disambiguation       Salient Translation\n     0.45                    0.825                                                             0.7                      0.50                             0.800\n                                                                                  0.45     Accuracy 0.40                                             Accuracy 0.775                                                   Accuracy 0.6                                                Accuracy\n         0  1  2  3  4          0  1  2  3  4          0  1  2  3  4          0  1  2  3  4\n                Step                     Step                     Step                     Step\n\n                     Figure 8: Accuracy over 4 steps across 8 tasks on Baichuan2-Turbo.\n\n                                         Our method     APO\n            SST-2              AG's News            TREC                 Subj\n                               0.92     0.98                                                      0.875                        0.7\n                               0.90                                                      0.850     0.96     Accuracy                                                                        Accuracy                                                        Accuracy                                                                                                                                                                                                                  Accuracy 0.6                               0.88\n         0  1  2  3  4          0  1  2  3  4          0  1  2  3  4          0  1  2  3  4\n               Step                     Step                     Step                     Step\n           Logical Five          Hyperbaton         Disambiguation       Salient Translation\n     0.75                                  0.9                                                         0.75                      0.70\n     0.70     Accuracy                                                              Accuracy 0.8                                                Accuracy                                                           Accuracy 0.65                                                         0.70\n         0  1  2  3  4          0  1  2  3  4          0  1  2  3  4          0  1  2  3  4\n               Step                     Step                     Step                     Step\n\n                          Figure 9: Accuracy over 4 steps across 8 tasks on GPT-4.\n\n\nsuggested reasoning process, and d) professional    updates and responsiveness to recent performance\ntips. We define: Schema 4: All four components    changes, while a lower Œ∑ promotes stability with\nSchema 3: First three components Schema 2: First    gradual adjustments. This balance is crucial for\ntwo components Schema 1: Task type and descrip-   navigating the trade-off between exploration and\ntion only (common in current techniques). We vary    exploitation.\nthe meta-instructions for these schemas and con-    We conduct prompt optimization experiments\nduct four-step prompt optimization experiments on   on SST-2 and AG‚Äôs News within four steps, testing\nSST-2 and AG‚Äôs News to assess their impact on   Œ∑ values from 0.01 to 0.1. As shown in Figure 7,\noptimization.                                 Œ∑ = 0.055 and Œ∑ = 0.07 are the most and second\n  As shown in Figure 6,  initial prompts from   most effective in accelerating optimization.\nSchema 4 yield the highest evaluation results. In\n                                                      5.3.3  Performance on Different LLMscontrast, Schema 1 has the lowest metrics and often\nfalls into suboptimal local minima, a common issue   As Table 1 indicates, APO is the best baseline\nwith current methods. This comparison validates   method. Therefore, we compare our method with\nour meta-instruction design and underscores that  APO using Baichuan2 (Yang et al., 2023a) and\na high-quality initial prompt is crucial for quickly   GPT-4 accessed via the APIs. We conduct prompt\nidentifying the optimal prompt.                     optimization experiments on eight NLP datasets\n                                                    across four optimization steps.\n5.3.2  Sensitivity to Learning Rate                                                   Figure 8 and 9 illustrate the performance vari-\nDuring the optimization phase, the learning rate    ation of both methods across different datasets as\nŒ∑ controls the extent of sentence weight updates    optimization steps progress. APO fails to converge\nafter each round. A higher Œ∑ results in significant    within four steps and shows greater performance\n\n\n                                         8\n\nModel       APO  Ours                 dition, we mainly present the number of API calls,\n                                           which is an important metric for cost comparison            GPT-3.5-Turbo   0.36   0.392\n                                           on black-box LLMs.\n          GPT-4          0.448  0.488\n\n                                                             Task           APO   OursTable 2: Accuracy on Geometric Shapes task on GPT-\n3.5-Turbo and GPT-4.                                         SST-2               12,520   1,708\n                                                          AG‚Äôs News          12,733   2,089\n                      Our method     APO                    TREC               9,739   1,486\n          SST-2              AG's News                Subj                12,790   1,848\n    1.0                          0.925\n                                                                    Logical Five          9,631   1,512\n    0.8                    0.900                             Hyperbaton          9,934   1,626\n                                                                Disambiguation      9,471   1,187 Accuracy 0.6                                             Accuracy 0.875\n                                                                         Salient Translation   10,190   1,451\n       0  5  10 15 20         0  5  10 15 20              Geometric Shapes    9,648   1,496\n             Step                     Step\n                                                                  Avg.                10,739   1,600\nFigure 10: Accuracy over 20 steps on GPT-3.5-Turbo.\n                                                      Table 3: API calls consumed on nine tasks on GPT-4.\n\nvolatility compared to Baichuan2-Turbo and GPT-4.\n                                   We conduct our experiments with GPT-4 on nine\nIn contrast, our method demonstrates rapid conver-\n                                                         tasks. As shown in Table 3, our method requires\ngence and strong optimization acceleration. Except\n                                                 approximately 1/7 of the number of API calls com-\nfor the generalizability to other models, we also\n                                                 pared to the strongest baseline method, APO.\nfind that stronger LLM can achieve more effective\nprompt optimization with our method.                                       6  Conclusion\n\n5.3.4  Performance on Specialized                                                   In this paper, we addressed the issue of low con-\n       Domain-Specific Task                                               vergence rates in gradient-free prompt optimiza-\nTo evaluate our method performance on specialized    tion methods for LLMs. Our proposed dual-phase\ntasks that require domain knowledge, we conduct   approach effectively accelerates prompt optimiza-\nexperiments on the Geometric Shapes task (Suzgun    tion by generating high-quality initial prompts and\net al., 2022), which involves interpreting SVG paths    leveraging tuning experience to navigate the opti-\nto determine the geometric figures they represent,   mization process. Extensive experiments on sev-\na task that requires specific domain knowledge.       eral LLMs across diverse datasets demonstrated\n  As shown in Table 2, our approach demonstrates    the superiority of our method in achieving satis-\nconsistent performance improvement over the best    factory performance within few optimization steps.\nbaseline APO, revealing the effectiveness of our   Our approach not only enhances the efficiency of\nmethod in specialized task.                      prompt optimization but also improves the overall\n                                                performance of LLMs in various NLP tasks. Future\n5.3.5  Results without Step Constraint\n                                           work will focus on further refining the optimization\nWe report the results of prompt optimization with    strategies and exploring their applications in more\na maximum of 20 steps on two general NLU tasks.   diverse and complex scenarios.\nAs shown in Figure 10, the strongest baseline, APO,\nconverges on the SST-2 task with slightly lower ac-  Acknowledgements\ncuracy than our method. However, on the AG‚Äôs\n                                                  This work is supported by the National Natural Sci-\nNews task, APO‚Äôs performance fluctuates signif-\n                                              ence Foundation of China (62272437,62402470),\nicantly and lags behind our method.  Thus, our\n                                                      the University Synergy Innovation Program of An-\nmethod demonstrates superior performance and\n                                                  hui Province (GXXT-2023-071), and the Funda-\nfaster convergence compared to existing methods,\n                                                 mental Research Funds for the Central Universities\neven with fewer optimization steps.\n                                                   of China (WK2100000053, PA2024GDSK0107).\n5.3.6  Computational Complexity               This research is also supported by the advanced\n                                             computing resources provided by the Supercom-Since the running time is related to the number of\n                                                   puting Center of the USTC.API calls and may be affected by the network con-\n\n\n                                         9\n\nLimitations                                        Cho-Jui Hsieh, Si Si, Felix X. Yu, and Inderjit S. Dhillon.\n                                                      2023.   Automatic engineering of long prompts.\nWe acknowledge some limitations despite the      ArXiv, abs/2311.10117.\npromising results of our research that could pave\nthe way for future studies:                        Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\n                                                      Optimizing continuous prompts for generation. Pro-\n   1) Our experiments were limited to general NLP                                                        ceedings of the 59th Annual Meeting of the Asso-\ntasks and one domain-specific task, more perfor-      ciation for Computational Linguistics and the 11th\nmance assessment on specialized tasks remains to       International Joint Conference on Natural Language\nbe included. 2) Our method relies on labeled task      Processing (Volume 1: Long Papers), pages 4582‚Äì\n                                                       4597.\ndata for prompt generation and evaluation, raising\nconcerns about its robustness in personalized or                                                  Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-\nscenarios lacking labeled data. 3) Our experiments     Kiong Ng, Patrick Jaillet, and Kian Hsiang Low.\nwere confined to GPT-3.5-Turbo, Baichuan2-Turbo      2024. Prompt optimization with human feedback.\n                                                         ArXiv, abs/2405.17346.and GPT-4, leaving the effectiveness of our method\non other large language models to be validated in\n                                                  Xiaoqiang  Lin,  Zhaoxuan Wu,  Zhongxiang  Dai,\nfuture studies.                                   Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet,\n  Further study may be needed to address these      and Bryan Kian Hsiang Low. 2023. Use your instinct:\nlimitations so as to improve the generalizability and       Instruction optimization using neural bandits coupled\n                                                        with transformers. ArXiv, abs/2310.02905.robustness of our approach in broader and more\ncomplex real-world applications.                                                      Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\n                                                        Hiroaki Hayashi, and Graham Neubig. 2021a. Pre-\n                                                                      train, prompt, and predict: A systematic survey of\nReferences                                         prompting methods in natural language processing.\n                                    ACM Computing Surveys, 55:1 ‚Äì 35.\nPeter Auer, Nicol√≤ Cesa-Bianchi, Yoav Freund, and\n  Robert E. Schapire. 1995.  Gambling in a rigged\n                                                Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,\n   casino: The adversarial multi-arm bandit problem.\n                                                   Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-\n   In IEEE Annual Symposium on Foundations of Com-\n                                                            tuning: Prompt tuning can be comparable to fine-\n   puter Science.\n                                                           tuning across scales and tasks. In Annual Meeting of\n                                                             the Association for Computational Linguistics.Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng\n  Huang, and Tianyi Zhou. 2023.  Instructzero: Ef-\n   ficient instruction optimization for black-box large   Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\n   language models. ArXiv, abs/2306.03082.                Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt\n                                                           understands, too. ArXiv, abs/2103.10385.\nShizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun\n   Li, LIN Yong, Xiao Zhou, and Tong Zhang. 2023a.   Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun\n  Black-box prompt learning for pre-trained language     Shum,  Jipeng Zhang,  and Tong Zhang. 2023.\n   models.  Transactions on Machine Learning Re-     Plum: Prompt learning using metaheuristic. ArXiv,\n   search.                                               abs/2311.08364.\n\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong   Bo Pang and Lillian Lee. 2004. A sentimental education:\n  Zhang. 2023b.   Active prompting with chain-      Sentiment analysis using subjectivity summarization\n   of-thought  for large language models.   ArXiv,      based on minimum cuts. ArXiv, cs.CL/0409058.\n   abs/2302.12246.\n                                                     Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\n                                                         Bansal. 2022. Grips: Gradient-free, edit-based in-\n  Making pre-trained language models better few-shot\n                                                                 struction search for prompting large language models.\n   learners. In Annual Meeting of the Association for\n                                                         ArXiv, abs/2203.07281.\n  Computational Linguistics.\n\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.   Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\n  News summarization and evaluation in the era of      guang Zhu, and Michael Zeng. 2023.  Automatic\n   gpt-3. ArXiv, abs/2209.12356.                       prompt optimization with \"gradient descent\" and\n                                             beam search. In Conference on Empirical Methods\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao       in Natural Language Processing.\n   Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang,\n  Tsinghua University, and Microsoft Research. 2023.   Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu,\n   Connecting large language models with evolutionary     Yongheng Zhang, Yinghui Li, Min Li, Wanxiang\n   algorithms yields powerful prompt optimizers. ArXiv,      Che, and Philip S. Yu. 2024. Large language models\n   abs/2309.08532.                                   meet nlp: A survey. ArXiv, abs/2405.12819.\n\n\n                                         10\n\nLaria Reynolds and Kyle McDonell. 2021.  Prompt      2024.  Large language models as optimizers.  In\n  programming for large language models: Beyond the     The Twelfth International Conference on Learning\n  few-shot paradigm. Extended Abstracts of the 2021       Representations.\n  CHI Conference on Human Factors in Computing\n   Systems.                                      Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and\n                                                           Fereshte Khani. 2023. Prompt engineering a prompt\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric       engineer. ArXiv, abs/2311.05661.\n  Wallace, and Sameer Singh. 2020. Eliciting knowl-\n  edge from language models using automatically gen-   Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023a.\n   erated prompts. ArXiv, abs/2010.15980.                  Extractive summarization via chatgpt for faithful\n                                              summary generation. In Conference on Empirical\nRichard Socher, Alex Perelygin, Jean Wu, Jason      Methods in Natural Language Processing.\n  Chuang, Christopher D. Manning, A. Ng, and\n   Christopher Potts. 2013. Recursive deep models for    Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\n   semantic compositionality over a sentiment treebank.       Character-level convolutional networks for text classi-\n   In Conference on Empirical Methods in Natural Lan-       fication. In Neural Information Processing Systems.\n  guage Processing.\n                                                 Zhihan Zhang, Shuo Wang, W. Yu, Yichong Xu, Dan\nMirac Suzgun, Nathan Scales, Nathanael Scharli, Se-        Iter, Qingkai Zeng, Yang Liu, Chenguang Zhu, and\n   bastian Gehrmann, Yi Tay, Hyung Won Chung,     Meng Jiang. 2023b.  Auto-instruct: Automatic in-\n  Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin       struction generation and ranking for black-box lan-\n   Chi, Denny Zhou, and Jason Wei. 2022. Challenging      guage models. In Conference on Empirical Methods\n  big-bench tasks and whether chain-of-thought can       in Natural Language Processing.\n   solve them. In Annual Meeting of the Association for\n  Computational Linguistics.                          Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,\n                                                Heng-Tze Cheng, Ed Huai hsin Chi, Quoc V. Le,\nEllen M. Voorhees and Dawn M. Tice. 2000. Building      and Denny Zhou. 2023. Take a step back: Evoking\n  a question answering test collection. In Annual In-      reasoning via abstraction in large language models.\n   ternational ACM SIGIR Conference on Research and      ArXiv, abs/2310.06117.\n  Development in Information Retrieval.\n                                              Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Hao-      Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n   tian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing,      Ba. 2022. Large language models are human-level\n  and Zhiting Hu. 2023.   Promptagent:  Strategic      prompt engineers. ArXiv, abs/2211.01910.\n  planning with language models enables expert-level\n  prompt optimization. ArXiv, abs/2310.16427.\n\nJules White, Quchen Fu, Sam Hays, Michael Sandborn,\n  Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse\n  Spencer-Smith, and Douglas C. Schmidt. 2023. A\n  prompt pattern catalog to enhance prompt engineer-\n   ing with chatgpt. ArXiv, abs/2302.11382.\n\nAi Ming Yang, Bin Xiao, Bingning Wang, Borong\n  Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian\n  Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang,\n  Feng Liu, Guangwei Ai, Guosheng Dong, Hai Zhao,\n  Hang Xu, Hao-Lun Sun, Hongda Zhang, Hui Liu,\n  Jiaming Ji, Jian Xie, Juntao Dai, Kuncheng Fang,\n  Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao\n  Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan\n   Nie, Pei Guo, Ruiyang Sun, Zhang Tao, Tianpeng Li,\n  Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong\n  Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin\n  Yu, Xuehai Pan, Yan-Bin Shen, Yiding Wang, Yiyu\n   Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan\n  Zhou, and Zhiying Wu. 2023a. Baichuan 2: Open\n   large-scale language models. ArXiv, abs/2309.10305.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao\n   Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.\n  2023b. Large language models as optimizers. ArXiv,\n  abs/2309.03409.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao\n   Liu, Quoc V Le, Denny Zhou, and Xinyun Chen.\n\n\n                                         11",
"headers": [
"arXiv:2406.13443v2  [cs.CL]  2 Oct 2024",
"Dual-Phase Accelerated Prompt Optimization",
"Ôºå",
"‚Ä¶"
],
"tables": [
"|SST-2<br>AG's News<br>TREC<br>Subj|Col2|2.7% Logical Five<br>% Hyperbaton<br>% Disambiguation<br>% Salient Translation|6.<br>4.8<br>6.<br>6.1|7%<br>%<br>4%<br>%|\n|---|---|---|---|---|\n|Subj<br>TREC<br>AG's News<br>SS~~T-~~2|||||",
"|Col1|Col2|Ôºå|Col4|Ôºå‚Ä¶Ôºå|Col6|Col7|\n|---|---|---|---|---|---|---|",
"|Col1|meta-instruction for initialization|Col3|\n|---|---|---|\n||||",
"|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n|`I'm trying to write a zero-shot prompt which`<br>`consists of four parts.`<br>`My current prompt is:`<br>**`[{prompt_to_revise}]`**<br>**`meta-nstructon or optmzaton`**|`I'm trying to write a zero-shot prompt which`<br>`consists of four parts.`<br>`My current prompt is:`<br>**`[{prompt_to_revise}]`**<br>**`meta-nstructon or optmzaton`**|**`meta-nstructon or optmzaton`**|**`meta-nstructon or optmzaton`**|**`meta-nstructon or optmzaton`**|\n|`But it gets the following outputs that fail to`|`But it gets the following outputs that fail to`|`But it gets the following outputs that fail to`|`But it gets the following outputs that fail to`|`But it gets the following outputs that fail to`|\n|`match the expected outputs:`|`match the expected outputs:`|`match the expected outputs:`|`match the expected outputs:`|`match the expected outputs:`|\n|**`{`**|**`failed_cases`**|**`failed_cases`**|**`}`**|**`}`**|",
"|I'm trying to write a zero|Col2|Col3|-|shot prompt which|\n|---|---|---|---|---|\n|`consists of four parts.`|`consists of four parts.`|`consists of four parts.`|`consists of four parts.`|`consists of four parts.`|\n|`My current prompt is:`|`My current prompt is:`|`My current prompt is:`|`My current prompt is:`|`My current prompt is:`|\n|**`[{`**|**`prompt_to_revise`**|**`}]`**|**`}]`**|**`}]`**|",
"|and some example inputs with|Col2|Col3|Col4|Col5|chain|-|of|-|thought|Col11|.|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|`I read the instruction carefully and wrote an`|\n|`output with chain`|`  -`|`  of`|`  -`|`  thought for every input`|`  thought for every input`|`  thought for every input`|`  thought for every input`|`  thought for every input`|`  thought for every input`|`  thought for every input`|`  thought for every input`|\n|`correctly. Here are some correct input`|`correctly. Here are some correct input`|`correctly. Here are some correct input`|`correctly. Here are some correct input`|`correctly. Here are some correct input`|`correctly. Here are some correct input`|`correctly. Here are some correct input`|`correctly. Here are some correct input`|`correctly. Here are some correct input`|`     -`|`     output`|`     output`|\n|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|`pairs which strictly meet all your`|\n|`requirements:`|`requirements:`|`requirements:`|`requirements:`|`requirements:`|`requirements:`|`requirements:`|`requirements:`|`requirements:`|`requirements:`|`requirements:`|`requirements:`|",
"|{|example pairs<br>_|}|\n|---|---|---|",
"|The sentence I want to revise is:|Col2|Col3|\n|---|---|---|\n|**`{sentences[`**|**`chosen_sentence`**|**`]}`**|",
"|The instruction given contains the following|Col2|Col3|Col4|\n|---|---|---|---|\n|`parts. Based on the input`|`    -`|`    output pairs`|`    output pairs`|\n|`provided, give me the final complete`|`provided, give me the final complete`|`provided, give me the final complete`|`provided, give me the final complete`|\n|<br>`instruction in English without any explanation`|<br>`instruction in English without any explanation`|<br>`instruction in English without any explanation`|`     :`|\n|<br>`###Task type###`<br>`Task type: This is a <...> task.`<br>`###Task detailed description###`<br>`Task detailed description: <Task detailed`<br>`description>`<br>`###Your output must satisfy the following`<br>`format and constraints###`<br>`Output format(type): <Output format or its`<br>`type>`<br>`Output constraints: <constraints on output>`<br>`###You must follow the reasoning process###`<br>`<add several reasoning steps if it's`<br>`necessary>`<br>`###Tips###`<br>`<add several useful tips from a professional`<br>`point of view to accomplish this task better>`|<br>`###Task type###`<br>`Task type: This is a <...> task.`<br>`###Task detailed description###`<br>`Task detailed description: <Task detailed`<br>`description>`<br>`###Your output must satisfy the following`<br>`format and constraints###`<br>`Output format(type): <Output format or its`<br>`type>`<br>`Output constraints: <constraints on output>`<br>`###You must follow the reasoning process###`<br>`<add several reasoning steps if it's`<br>`necessary>`<br>`###Tips###`<br>`<add several useful tips from a professional`<br>`point of view to accomplish this task better>`|<br>`###Task type###`<br>`Task type: This is a <...> task.`<br>`###Task detailed description###`<br>`Task detailed description: <Task detailed`<br>`description>`<br>`###Your output must satisfy the following`<br>`format and constraints###`<br>`Output format(type): <Output format or its`<br>`type>`<br>`Output constraints: <constraints on output>`<br>`###You must follow the reasoning process###`<br>`<add several reasoning steps if it's`<br>`necessary>`<br>`###Tips###`<br>`<add several useful tips from a professional`<br>`point of view to accomplish this task better>`|<br>`###Task type###`<br>`Task type: This is a <...> task.`<br>`###Task detailed description###`<br>`Task detailed description: <Task detailed`<br>`description>`<br>`###Your output must satisfy the following`<br>`format and constraints###`<br>`Output format(type): <Output format or its`<br>`type>`<br>`Output constraints: <constraints on output>`<br>`###You must follow the reasoning process###`<br>`<add several reasoning steps if it's`<br>`necessary>`<br>`###Tips###`<br>`<add several useful tips from a professional`<br>`point of view to accomplish this task better>`|",
"|Comparing the wrong outputs with their|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|`corresponding expected answers under the same`|`corresponding expected answers under the same`|`corresponding expected answers under the same`|`corresponding expected answers under the same`|`corresponding expected answers under the same`|`corresponding expected answers under the same`|`corresponding expected answers under the same`|\n|`input, optimize the above sentence to help AI`|`input, optimize the above sentence to help AI`|`input, optimize the above sentence to help AI`|`input, optimize the above sentence to help AI`|`input, optimize the above sentence to help AI`|`input, optimize the above sentence to help AI`|`input, optimize the above sentence to help AI`|\n|`understand the task more comprehensively and`|`understand the task more comprehensively and`|`understand the task more comprehensively and`|`understand the task more comprehensively and`|`understand the task more comprehensively and`|`understand the task more comprehensively and`|`understand the task more comprehensively and`|\n|`accomplish this task better.`|`accomplish this task better.`|`accomplish this task better.`|`accomplish this task better.`|`accomplish this task better.`|`accomplish this task better.`|`accomplish this task better.`|\n|`Your response format is as follows.`|`Your response format is as follows.`|`Your response format is as follows.`|`Your response format is as follows.`|`Your response format is as follows.`|`Your response format is as follows.`|`Your response format is as follows.`|\n|`The given sentence`|`The given sentence`|`The given sentence`|`The given sentence`|`The given sentence`|`The given sentence`|`The given sentence`|\n|`'`|**`{sentences[`**|**`chosen_sentence`**|**`]}`**|`'`||` should be`|\n|`revised as:`|`revised as:`|`revised as:`|`revised as:`|`revised as:`|`revised as:`|`revised as:`|",
"|Col1|Few-shot|Zero-shot|\n|---|---|---|",
"|Task|Manual|APO APE PA Ours|\n|---|---|---|",
"|SST-2<br>AG‚Äôs News<br>TREC<br>Subj|/<br>/<br>/<br>/|0.89 0.92 0.443 0.978<br>0.88 0.819 0.785 0.928<br>0.795 0.513 0.687 0.785<br>0.64 0.593 0.494 0.72|\n|---|---|---|",
"|Logical Five<br>Hyperbaton<br>Disambiguation<br>Salient Translation|0.388<br>0.744<br>0.580<br>0.544|0.392 0.404 0.443 0.48<br>0.808 0.865 0.823 0.88<br>0.688 0.645 0.696 0.74<br>0.456 0.538 0.468 0.548|\n|---|---|---|",
"|Avg.|0.564|0.694 0.662 0.605 0.757|\n|---|---|---|",
"|el|APO Our|\n|---|---|",
"|-3.5-Turbo|0.36 0.39|\n|---|---|",
"|Task|APO|Ours|\n|---|---|---|",
"|Task APO Ours|Col2|Col3|\n|---|---|---|\n|**SST-2**<br>12,520<br>1,708<br>**AG‚Äôs News**<br>12,733<br>2,089<br>**TREC**<br>9,739<br>1,486<br>**Subj**<br>12,790<br>1,848|**SST-2**<br>12,520<br>1,708<br>**AG‚Äôs News**<br>12,733<br>2,089<br>**TREC**<br>9,739<br>1,486<br>**Subj**<br>12,790<br>1,848|**SST-2**<br>12,520<br>1,708<br>**AG‚Äôs News**<br>12,733<br>2,089<br>**TREC**<br>9,739<br>1,486<br>**Subj**<br>12,790<br>1,848|\n|**Logical Five**<br>9,631<br>1,512<br>**Hyperbaton**<br>9,934<br>1,626<br>**Disambiguation**<br>9,471<br>1,187<br>**Salient Translation**<br>10,190<br>1,451<br>**Geometric Shapes**<br>9,648<br>1,496|**Logical Five**<br>9,631<br>1,512<br>**Hyperbaton**<br>9,934<br>1,626<br>**Disambiguation**<br>9,471<br>1,187<br>**Salient Translation**<br>10,190<br>1,451<br>**Geometric Shapes**<br>9,648<br>1,496|**Logical Five**<br>9,631<br>1,512<br>**Hyperbaton**<br>9,934<br>1,626<br>**Disambiguation**<br>9,471<br>1,187<br>**Salient Translation**<br>10,190<br>1,451<br>**Geometric Shapes**<br>9,648<br>1,496|\n|**Avg.**|10,739|1,600|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2406.13443v2.pdf"
}