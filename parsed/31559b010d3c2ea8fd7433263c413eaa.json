{
"text": "Task Facet Learning: A Structured Approach To Prompt Optimization\n\n       Gurusha Juneja1,*, Gautam Jajoo2,*, Nagarajan Natarajan3, Hua Li4, Jian Jiao4, Amit Sharma3,†\n\n               1UC Santa Barbara, 2BITS Pilani, 3Microsoft Research, 4Microsoft Bing Ads\n\n                               *These authors contributed equally to this work.\n                                †Corresponding author: amshar@microsoft.com\n\n\n                          Abstract                      gineering (Liu et al., 2023). Typically, manually-\n                                                             developed prompts combine task description with a\n                Given a task in the form of a basic description      few in-context examples, along with modifiers like\n                and its training examples, prompt optimization                                                                 chain-of-thought (Kojima et al., 2024). For greater\n                       is the problem of synthesizing the given in-\n                                                                  accuracy, human prompt engineers spend consider-\n                 formation into a text prompt for a large lan-2025                                                          able time to identify errors with a current prompt,                guage model. Humans solve this problem by\n                   also considering the different facets that define        consider the different facets of a task (e.g., counter-\n                 a task (e.g., counter-examples, explanations,       examples, explanations, analogies) that may fixMay            analogies) and including them in the prompt.       those errors, and include them in the prompt. For\n                However, it is unclear whether existing algo-        instance, for a hate speech classification task, in ad-\n19            rithmic approaches, based on iteratively editing        dition to the definition, it may be helpful to specify\n                 a given prompt or automatically selecting a\n                                                                 the facets that lead to hate speech: the context of\n               few in-context examples, can cover the mul-\n                                                                    conversation, identifying intent, and differentiating                     tiple facets required to solve a complex task.\n                                                                hate speech from opinions or closely-related con-                  In this work, we view prompt optimization\n                  as that of learning multiple facets of a task        cepts such as vulgarity and profanity.[cs.AI]          from a set of training examples. We exploit      To avoid the above cumbersome manual process,\n                   structure in the prompt optimization problem        recent work aims to automate the process of gener-\n                and break down a prompt into loosely coupled        ating natural language prompts that are also inter-\n                 semantic sections. The proposed algorithm,                                                                     pretable. Since language tokens are discrete, this\n              UNIPROMPT, (1) clusters the input space and\n                                                                 leads to a challenging discrete optimization prob-\n                  uses clustered batches so that each batch likely\n                                                       lem with a combinatorial space of possible outputs.                  corresponds to a different facet of the task, and\n                   (2) utilizes a feedback mechanism to propose       Techniques for prompt optimization can be divided\n                  adding, editing or deleting a section, which in        in two categories: non-directional, e.g., random\n                   turn is aggregated over a batch to capture gen-       search (Zhou et al., 2022; Zhang et al., 2023) and\n                    eralizable facets. Empirical evaluation on mul-       genetic algorithms (Yang et al., 2023; Guo et al.,\n                      tiple datasets and a real-world task shows that                                                              2023), where the sampling of new input is “ran-\n                prompts generated using UNIPROMPT obtain\n                                                dom” and does not explicitly aim to reduce error\n                   higher accuracy than human-tuned prompts and\n                                                      on a train set; and directional, where the sampling                   those from state-of-the-art methods. In particu-arXiv:2406.10504v2                                                           of new input depends on some error measure on a                        lar, our algorithm can generate long, complex\n                prompts that existing methods are unable to        representative train sample. Recently, more com-\n                   generate. Code for UNIPROMPT is available at       plex methods have been proposed in the second cat-\n                   https://aka.ms/uniprompt.                        egory including RL (Zhang et al., 2022; Deng et al.,\n                                                               2022), updating prompts using feedback from aux-\n          1  Introduction                                        iliary LLMs (Hu et al., 2024; Pryzant et al., 2023),\n                                                       and optimizing the input to a small LM that gen-\n           Given a task, choosing an input prompt is a key\n                                                                    erates the prompt (Lin et al., 2024b; Chen et al.,\n              part of optimizing Large Language Model’s (LLM)\n                                                              2024). While these techniques focus on editing\n            performance (Kojima et al., 2024; Yang et al.,\n                                                                    parts of a given prompt, they are developed with\n             2023). Minor changes in prompt can lead to per-\n                                                                 the goal of obtaining a concise description of the\n            formance gains or losses, necessitating prompt en-\n                                                                       task. None of these focus on ensuring multiple\n                 5To appear in ACL Findings 2025.                       facets of a task are added to the prompt.\n\n\n                                                    1\n\nIn this paper, we propose UNIPROMPT, a prompt   2  Related Work\noptimization method to cover diverse, multiple\n                                                Here, we highlight relevant work that are not\nfacets of a task and improve overall accuracy. To\n                                                addressed in the manuscript otherwise.  Deng\nsimulate the manual prompt engineering process,\n                                                             et al. (2022) present a discrete prompt optimization\nwe propose that prompts be constructed from in-\n                                              method, RLPrompt, using reinforcement learning,\ndividual sections, where each section may corre-\n                                             where a policy network learns to generate effective\nspond to a different facet that humans may consider\n                                             prompts through reward-based training, with an\nfor the task. Prompt editing proceeds at a section-\n                                               emphasis on enhancing training efficiency through\nlevel: we can add, edit or delete a section from\n                                                        effective reward stabilization techniques. A draw-\nthe prompt. Similar to (Pryzant et al., 2023; Hu\n                                             back of such automatic prompt optimization ap-\net al., 2024), prompt edits are based on an auxil-\n                                                proaches (Pryzant et al., 2023; Zhou et al., 2022;\niary LLM’s feedback about example predictions\n                                       Deng et al., 2022; Yang et al., 2023) is that the\nwith the current prompt. We contribute two key\n                                              prompts generated tend to be short, often compris-\ninsights in this feedback-based optimization pro-\n                                                    ing only one or two sentences, which may not fully\ncess. First, we find that the feedback on a single\n                                                   encapsulate the complexity of the task at hand.\nexample or a randomly selected batch of examples\n                                               Another recent line of work leverages humandoes not yield generalizable facet descriptions. In-\n                                                 feedback. Automated Prompt Optimization with\nstead, we propose clustering the inputs and creating\n                                    Human Feedback  (Lin et al., 2024a) optimizesmini-batches such that each mini-batch is sourced\n                                              prompts for black-box LLMs using human prefer-from a single cluster. Second, even with clustered\n                                              ence feedback. Besides the obvious overhead, itbatches, the feedback tends to overfit to specific\n                                              might also introduce potential biases.\nexamples or their properties. To generate a prompt\n                                                         Prior research (Wei et al., 2023, 2024) has high-edit that conveys a generalizable concept relevant\n                                                      lighted the significance of specific sections withinto the task, we propose generating edits at a mini-\n                                                  prompts. However, existing methods do not specif-batch level and then aggregating them at the batch\n                                                           ically target the optimization of individual sectionslevel to yield the final edit (Figure 1). While the\n                                            and their respective contents within the prompts.two insights may appear simple, we show that they\n                                               Hsieh et al. (2023) investigate the use of greedy andsignificantly improve extracting diverse task facets.\n                                                   genetic algorithms to edit lengthy prompts. Their\n                                           method focuses on paraphrasing one line at a time  We evaluate UNIPROMPT on several bench-\n                                                         starting from an existing prompt, compared to ourmarks where it consistently achieves higher accu-\n                                                 goal of learning facets of a task from scratch. An-racy than existing prompt optimization methods.\n                                                     other orthogonal line of work explores algorithmicOn Ethos, a hate speech dataset, UNIPROMPT ob-\n                                                       selection of in-context examples (Min et al., 2022;tains 94% accuracy whereas the next best method\n                                             Gupta et al., 2023; Wu et al., 2023; Srivastava et al.,obtains 82%.  Even though UniPrompt focuses\n                                               2024; Sun et al., 2024).only on the instruction and does not include any\nin-context examples, we find that its instruction-                                       3  UniPrompt: Capturing Task Facets\nonly accuracy is often higher than methods such as\nDSPy (Khattab et al., 2024) that optimize both. In    State-of-the-art prompt optimization methods such\nthe few-shot setting, we also compare UNIPROMPT    as ProTeGi (Pryzant et al., 2023) and TextGrad\nto MedPrompt (Nori et al., 2023), a state-of-the-   (Hou et al., 2023) iteratively optimize the prompt\nart prompt composition method. We find that    for a given task. At a high-level, they proceed as\nUNIPROMPT, requiring only one LLM call at in-   follows: (1) start with an initial prompt and a train-\nference time, obtains the same accuracy as Med-   ing dataset of ⟨question, answer⟩pairs for the task,\nPrompt that requires five calls. If we allow multiple    (2) randomly sample from the questions wrongly\ncalls to UNIPROMPT, we obtain over 4% accuracy   answered by the current prompt to form a batch,\ngains. Finally, we also evaluate UNIPROMPT on a    (3) use an expert LLM to obtain feedback on the\nreal-world semantic matching task in a web search   random batch, (4) apply the feedback to the prompt.\nengine. Compared to the best manual prompt, the    This procedure is illustrated in Figure 1 [Left]. Our\nprompt generated from UNIPROMPT leads to over   work is motivated by three key observations.\n5% increase in accuracy on the rare class and nearly    1. Larger models are more amenable to prompt\n2% accuracy increase overall.                      optimization. We observe that the change in the\n\n\n                                         2\n\nTrain Examples                                                    Train Examples\n\n\n\n\n              Correct                                                        Tell if the following is hate speech(1)                                                  Correct\n             Predictions                                                           or not(0)                                                               Predictions\n\n                                     Solver                                              Solver\n                                                                               Initial Prompt (acc 0.4)\n                            LLM                                                                   LLM                        [\"By calling period, ...\", Hate],\n\n   [\"HE SHOT THE KID!?!?...\", Not-Hate],                                                                                                        [\"Easter is the most silly ... shame\", Hate],\n      [\"I propose ...\", Not-Hate],                                                                                                                          [\"Christians can get crazy ...\", Hate],\n   [\"Should'¬Ä¬ôve ... them in\", Not-Hate],                                                                                                 [\"The Problem with Islam ...\", Hate]\n    [\"Christians can get crazy ...\", Hate],                                                        Carefully analyze ... discriminatory,\n    [\"The Problem with Islam ...\", Hate],                                                      derogatory, or violent language ... a              [\"HE SHOT THE KID!?!? ...\", Not-Hate],\n        Wrong Examples                                                                  particular group, such as based on                        [\"I propose ...\", Not-Hate],\n                                                                                            religion or gender... potential impact                [\"Humiliating this ...\", Not-Hate]\n                                                                             ... the intent behind the words, and       on the targeted individual or group ...\n                                               the potential harm it may cause...                   justify your decision with a               Wrong Examples clustered into\n                                              Non-hate speech includes                  well-reasoned explanation.                          mini-batches\n                                             respectful and inclusive language\n                                                      that does                                                          not seek                                                                          to demean or             Final Prompt (acc 0.9)            Expert LLM                                              harm                                                                        others...\n                                          Final Prompt (acc 0.6)\n                                                                                                                Expert LLM\n    include a clear definition of hate\n    speech, …\n                                                                                                                                                                                                 ... impact of the statement on the targeted individual or group.\n    include the requirement to identify                                                                                                                                   ... emphasize the impact of language, and encourage critical\n                                                                           Expert\n    hate speech ...                                               LLM              analysis ... determine if they qualify as hate speech.\n\n                                                Expert                                                                               ... language that promotes hatred or discrimination towards a particular\n    include examples of hate speech,\n                                     LLM                                            gender.\n   guidance on identifying hate speech...                                                                                                                            ... potential harm or violence implied, as well as any discriminatory or\n          Proposed Edits                                                                                 derogatory language used... towards a particular religious group.\n                                                                                             Feedbacks over mini-batches grouped\n\n\nFigure 1: Existing prompt optimization methods (left) versus UNIPROMPT (right) on the Ethos dataset: [Left]\nState-of-the-art prompt optimization methods like ProTeGi (Pryzant et al., 2023) sample from the questions wrongly\nanswered by the current prompt, and use an expert LLM (e.g., GPT-4) to obtain feedback on the mistakes. This\napproach tends to give very general edits or overfits to specific examples. [Right] In contrast, UNIPROMPT identifies\nkey task facets by: (1) clustering examples with similar task facets, and (2) employing a two-tier feedback-based\nupdate strategy. The resulting prompt updates extract generalizable concepts from the specific examples.\n\n\nobjective function (i.e., loss on a validation set for    scription and a train set Dt of N ⟨question qi, an-\na given prompt) per change in input is relatively   swer ai⟩demonstrations. It extracts key concepts\nmore stable for larger models like GPT-4 (Figure 2)    or facets relevant to the task and updates prompt\nthan for GPT-3.5 (analysis in Appendix A.1).         sections using them, with the goal of increasing ac-\n2. Clustered-batching improves the quality of    curacy on the validation set Dv. We assume access\ntext gradients (i.e., feedback), as against the stan-    to an “expert LLM” such as GPT-4.\ndard random batching adopted in state-of-the-art\n                                                    3.1  Task facet learning using examplesprompt optimization methods (Section 5.1).\n3. Two-tier feedback helps learn generalizable    Extracting task-relevant concepts from a set of ex-\nfacets. Collecting feedback from an expert LLM   amples to refine a prompt is a complex problem\nover mini-batches, and then summarizing the indi-   comprising multiple steps. Given a set of incorrect\nvidual feedback texts via a second step (Section 5.1)    predictions, one needs to analyze what went wrong\nhelps learn generalizable task concepts in prompts.    in each prediction, form hypotheses, aggregate the\n  The proposed method UNIPROMPT, in Figure   hypotheses to identify specific concepts that are\n1 [Right], makes two contributions. First, we fol-   relevant for the task. Then, for each concept, one\nlow a two-tier setup of synthesizing feedback for   needs to attribute which facet/section of the current\na batch of training examples. We break up a batch   prompt needs to be edited to incorporate the con-\ninto mini-batches, collect feedback on each of the    cept. These operations are highly model-specific\nmini-batches and then use a separate prompt to   and are difficult to execute reliably. Therefore, we\naggregate the different feedback texts into a gener-   exclusively rely on an expert LLM.\nalizable concept. Second, to increase chances that    First, we prompt the expert LLM to diagnose mis-\na mini-batch corresponds to a coherent facet, we    takes (feedback) in each example given the answer\nperiodically (re)cluster the training data and ensure   and chain-of-thought reasoning produced by the\nthat each mini-batch consists of examples from the    solver LLM. Subsequently, we use this feedback\nsame cluster.                                         to generate precise edits for the prompt that may\nAlgorithm 1 receives as input a one-line task de-    fix the error. These individual edits are then aggre-\n\n\n                                         3\n\ngated over a mini-batch and fed back into the same    consisting of incorrectly-answered questions in m,\nLLM, which then identifies a few major edits to be    the chain-of-thought produced by the solver LLM,\napplied to the current prompt. To aid in identifying    their incorrect predictions and the ground-truth an-\nmajor edits that correspond to generalizable facets,   swers. We ask the expert to provide one feedback\nwe propose to cluster the examples as a prepro-    for the mini batch (prompt is provided in Appendix\ncessing step and create clustered batches, such that    A.12). The expert can suggest the following edits:\neach cluster shares some common facet of the task.   add a section or subsection, delete a section or sub-\n                                                        section, and edit a section or subsection.\n3.1.1  Clustering for identifying facets                                             Given the different edits for mini-batches within a\nWe explore two approaches for clustering: topic-   batch b, we invoke the expert LLM again to sum-\nbased clustering, and feedback-based clustering.     marize these edits into a single section update. This\nTopic-Based Clustering. Given a set of exam-   combination ensures some degree of smoothness\nples, we identify l topics spanning the entire train    at every update which helps stabilize training. To\nset. This type of clustering is motivated by the   make sure that the expert is able to generate gen-\nobservation that solver LLM may make similar    eralizable edits, we additionally provide a random\nmistakes on examples from the same topic. Hence,    set of incorrect examples that are not in the current\nfor such examples, a common edit to the prompt    batch and ask it to suggest an edit based on the\ncould improve predictions for all the examples. To    existing edits that can correct the errors. As before,\nobtain the clusters, the expert LLM is prompted    the class of edits allowed is the same.\n(for prompt see Appendix A.10) to provide a broad   History for effective exploration. To ensure com-\nsub-topic ti for each question. Then the resultant    prehensive, non-repetitive exploration of prompts,\nlist of sub-topics {t1, t2, . . . , tN} is again clustered   we also provide the batch-level history of ed-\ninto k topics {t′1, t′2, . . . , t′l} by prompting the ex-    its (Hu et al., 2024; Yang et al., 2023) in the mini-\npert LLM. Based on this clustering, each example    batch-level prompt. History H[b] is presented as\nqi, ai is assigned a cluster t′j corresponding to ti.     {ei, acci −acci−1} where ei is the edit proposed\nFeedback-Based Clustering. Examples that re-    at the ith update and acci is the accuracy of the\nceive similar feedback based on a prompt’s pre-    ith updated prompt (See Appendix A.12 for the\ndictions can help identify task facets. Consider a    prompt).\nphysics-based task where two examples from differ-\n                                                      3.1.3  Editing the promptent topics obtain the same feedback from the expert\nLLM to edit the “Rules” section of the prompt to in-   Once the final set of edits is received for a batch,\nclude the statement, “Draw all forces on each body   we use the expert LLM to apply edits to the current\nbefore writing the equations”. We argue that such   prompt (See Appendix A.13 for the prompt). An\nexamples can be clustered. This type of clustering    edit is accepted only if it increases the validation\nmakes the broad edit identification step easier. To   accuracy (Greedy). Alternatively, we maintain a\nobtain the clusters, we first evaluate all training   beam of 2 best performing prompts based on vali-\nexamples against the current best prompt and store    dation accuracy, apply the edit to the two prompts,\nthe feedback fi from the expert LLM, correspond-   and update the beam to retain the top 2 perform-\ning to each incorrectly answered example qi, ai (all    ing prompts (Beam). To avoid overfitting on the\nthe correctly answered questions form one cluster).    train examples (or adding unnecessary information\nWe then prompt the expert LLM to cluster these    to the prompt), we employ early stopping in the\nfeedbacks {f1, f2, . . . , fN} into l clusters (see Ap-   optimization process (more details in Section 4).\npendix A.11). For each cluster, we create a batch\n                                                    3.2  Prompt Initializationqi, ai corresponding to feedbacks in that cluster.\n                                 We use two types of initialization: (1) task descrip-\n3.1.2  Obtaining generalizable prompt edits                                                           tion, i.e., p0 has a single section titled Introduc-\nTwo-tier Feedback. To encourage generalizable    tion containing the input task description. (2) fine-\nfeedback from the expert LLM, we obtain feed-   tune Llama2-13B model to generate a prompt with\nback at two levels: mini-batch and batch. Given a    sections such as Introduction, Tricks, and Corner\nbatch (created using clustering discussed above),   Cases, similar to the initial prompt that a human\nwe break it up into mini-batches.                  prompt engineer may produce. To finetune, we use\nFor each mini-batch m, we construct a prompt   GPT-4 generated data consisting of (task descrip-\n\n\n                                         4\n\nAlgorithm 1: UNIPROMPT\n   Input: Train set Dt, validation set Dv, initial prompt for the task p0, one-line task description T\n  Output: Optimized prompt P ∗for the given task\n 1 C ←cluster(Dt, p0), initialize history H ←{}, and validation accuracies V ←[];\n 2 Initialize a beam of size 2 with the initial prompt: p1 ←p0 and p2 ←p0\n 3 for epoch e and each cluster c in C do\n 4     for each batch b ∈batches(C) do\n 5     F ←[]\n 6        for each mini-batch m ∈mini-batches(b) do\n 7            Evaluate the best prompt on mini-batch: am ←LLM(m, p1)\n 8          Get expert feedback: f ←Feedback(T, am, H[m])\n 9          F.insert(f)\n\n10       Combine feedbacks over a batch: Fb ←Combine(F)\n11       Apply feedback to get updated prompts: q1 ←apply(Fb, p1); q2 ←apply(Fb, p2)\n12       Update the beam: if not(p1 = p0) then p2 ←second-high-acc([p1, p2, q1, q2], b)\n13        p1 ←highest-acc([p1, q1, q2], b)\n14     Evaluate the best prompt on validation set: accv ←evaluate(p1, Dv)\n15   V ←V .append(accv)\n16       if early-stop-criteria (V ) then break\n17       if recluster(e) then C ←cluster(Dt, p1)\n18 return p1 as P ∗;\n\n\n\ntion, section title, section contents) triples. Details    as the test set, and 100 examples as the validation\nand examples are in Appendices A.4 and A.7.         set for all the compared methods. We use GPT-\n                                                3.5-Turbo as the solver model. For Feedback and\nComputational Complexity:  The complexity of\n                                           Combine in UNIPROMPT, we use GPT-4 as the ex-\nclustering and of getting mini-batch and batch-level\n                                                       pert (see ablation in Section 5.5). We maintain a\nfeedbacks per epoch is O(N) expert LLM queries,\n                                        beam size of 2. Mini-batch sizes (and batch sizes)\nwhere N is the number of training examples. De-\n                                                      are constrained by the context length of GPT-4. We\ntails are in Appendix A.3.\n                                                         find that mini-batch sizes 3 to 5 and batch sizes 5 to\n                                           7 work the best for our datasets. The temperature4  Experiments Setup\n                                                   of the LLMs for our method is set to 0 for repro-\nDatasets: We perform comprehensive evaluation    ducibility. We employ early stopping at batch-level\non five standard datasets: (1) Ethos (Mollas et al.,    in UNIPROMPT.\n2020), (2) ARC (Clark et al., 2018), (3) MedQA   Baselines: We compare UNIPROMPT with the\n(Jin et al., 2021), (4) GSM8K (Cobbe et al., 2021),   following techniques and baselines: (1) Task De-\nand (5) BBH (Suzgun et al., 2022). Ethos, ARC,   scription: prompt is the one line task description\nand MedQA contain multiple choice questions, and    that we use to initialize UNIPROMPT; (2) Chain-\nGSM8K contains questions with integer answers.   Of-Thought (or CoT) prompting (Kojima et al.,\nBBH is a subset of 10 tasks, spanning 4 main cat-   2024); (3) Expert Prompt: the prompt optimized\negories, from the challenging BIG-Bench bench-   by humans taken from prior works (Nori et al.,\nmark that requires multi-step reasoning. In addi-   2023); (4) OPRO (Yang et al., 2023), that uses\ntion, we also evaluate UNIPROMPT on the medical   LLMs for discrete optimization over text prompts;\nQnA datasets used in the MedPrompt (Nori et al.,    (5) ProTeGi (Pryzant et al., 2023) that proposes\n2023) work; as well as two popular code genera-    textual gradients and selects edits to prompts us-\ntion datasets, HumanEval (Chen et al., 2021) and    ing bandit techniques; (6) Evoke (Hu et al., 2024)\nMBPP (Austin et al., 2021).                            that uses two instances of LLM, one that scores the\nImplementation details: We set the initial prompt    current prompt, and the other that edits the prompt;\np0 for each task as the one-line task description. We    (7) EvoPrompt (Guo et al., 2023) that uses genetic\nuse 200 examples as the train set, 200 examples\n\n\n                                         5\n\nalgorithms to search through the space of prompts;       any discriminatory or derogatory lan-\n(8) TextGrad (Hou et al., 2023), state-of-the-art       guage used...towards a particular reli-\nframework for automatic differentiation of prompts        gious group.\nvia text; (9) DSPy (Khattab et al., 2024), a recent                                              The instruction should include ...think\nprogramming model for optimizing LLM prompts;                                                    about the impact of the statement on the\nand (10) MedPrompt (Nori et al., 2023), a state-                                                          targeted individual or group.\nof-the-art prompt composition method.\n                                              The instruction should...language that\n                                                  prompts hatred or discrimination to-5  Results and Analysis\n                                                 wards a particular gender.\nWe present detailed quantitative and qualitative re-\n                                          To contrast, we employ random batching as in the\nsults, along with key ablations.\n                                                  standard prompt optimization techniques, on the\n                                          same dataset. The feedback obtained, given below,5.1  Performance of UNIPROMPT\n                                                                  is relevant for the task, but fails to identify specific\nWe start with the zero-shot setting, where we do                                                    concepts.\nnot include labeled examples in the prompt for any\nof the compared methods. We report results for       The instruction should include a clear\ntwo versions of our method in Table 1, which differ         definition of hate speech...\nin the combining strategy (from Section 3.1.3)—       The instruction should include examples\nbeam search vs greedy.                                    of hate speech, guidance on identifying\nUNIPROMPT variants significantly outperform the        hate speech...\nbaselines including CoT and the state-of-the-art\nprompt optimization techniques like ProTeGi that     The former feedback (UNIPROMPT) captures\ncrucially leverage LLMs for performing iterative    the facet of measuring impact on the targeted entity\nprompt edits. UNIPROMPT is the best performing   whereas the latter only captures religious and harm-\nmethod on three out of four datasets. It achieves   based aspect of hate speech. The same expert\nmaximum gains on the Ethos dataset with a 18.2%  LLM is able to identify different facets due to\nincrease in accuracy over the expert prompt. Fur-   clustered batches.\nther, we see accuracy increases of 4.0% on MedQA,    2. Employing two-tier feedback: In Section 3, we\n3.5% on GSM8k, and 7.6% on ARC-Challenge    argued that employing two-tier feedback strategy to\ndatasets. We show UNIPROMPT training behavior    aggregate the feedback texts encourages the expert\nin Appendix A.19.                    LLM to propose edits that are generalizable. The\nWe  also  present comparisons  to  state-of-the-   following feedback is received on the Ethos dataset\nart DSPy method  in the few-shot  setting (8    after the aggregation:\nbootstrapped_demos) using two optimization set-                                              The    instruction    should...consider\ntings provided by their framework. The last two                                                    whether the statement contains discrimi-\nrows of Table 1 show that UNIPROMPT in the zero-                                                          natory, derogatory, or violent language\nshot setting convincingly outperforms DSPy in the                                                            that promotes hatred or harm towards\nfew-shot setting, on three out of four datasets.                                              a particular group, such as based on\nQualitative Analysis: ProTeGi and TextGrad also                                                            religion and gender.\nadopt batching by randomly sampling from train-\ning examples where the solver LLM made mistakes.  We see that two-tier feedback helps in distilling\nIn the early iterations of optimization, there can be    important aspects of the task implicit in the exam-\nmany such examples. So, do our key observations    ples, rather than directly using or rephrasing the\nand hypotheses (beginning of Section 3) hold em-   (limited) examples.\npirically? We give some evidence below.            Results on BBH: From Table 2, it is evident that\n1. Employing clustering to create batches (Sec-  UNIPROMPT shows a significant improvement over\ntion 3.1): An example feedback obtained on the  OPRO (that also evaluates on these tasks in their\nEthos dataset using UNIPROMPT is shown below:    paper) for a majority of tasks.   It achieves sig-\n                                                        nificantly higher accuracy in Boolean Expression\n    The instruction should include..potential       (92.37% vs. 78.74%), Date Understanding (81.96%\n    harm or violence implied, as well as         vs. 52.59%), and Navigate (77.16% vs. 51.74%).\n\n\n                                         6\n\nTable 1: Test accuracies (%) of the compared methods with GPT-3.5-Turbo as the solver model in the zero-shot\nsetting (best in bold; second best underlined). The two UNIPROMPT rows are our proposed method. We compare\nwith few-shot methods in the last two rows (DSPy variants); *best in bold to distinguish the few-shot setting.\n\n\n       Method                                      Ethos  ARC  MedQA  GSM8K\n\n       Task Description                                 76.8   79.7      52.7      59.4\n        Expert Prompt                                   74.1   78.4      53.1      78.9\n       Llama Prompt (Section 3.2)                       74.0   89.7      52.6      79.5\n      CoT                                            72.0   79.4      50.3      76.3\n     OPRO                                          65.4   79.1      53.3      77.1\n       ProTeGi                                        76.0   78.8      52.9      77.3\n       Evoke                                          63.5   89.0      52.8      81.0\n       EvoPrompt                                      81.6   89.9      50.3      81.4\n      DSPy (MIPRO v2, zero-shot)                      79.7   82.8      61.9      77.3\n       TextGrad                                        79.5   76.5      50.6      81.6\n      UNIPROMPT (Init = Task Description) + Beam     92.3   86.0      57.1      82.4\n      UNIPROMPT (Init = Task Description) + Greedy    93.7   90.5      55.5      82.3\n      DSPy (BootstrapFewShotWithRandomSearch)    86.6   87.5     *68.5      74.3\n      DSPy (MIPRO v2, few-shot)                       84.0   86.0      62.9      79.7\n\n\nTable 2: Test accuracies (%) on BBH dataset with GPT-   self-consistency and ensembling with option shuf-\n3.5-Turbo as the solver model                                                            fling at inference time. They evaluate on 4 medical\n                                                      datasets (that none of the competing methods in\n Task                Init  OPRO  UNIPROMPT                                                Table 1 evaluate on) using GPT-4 as the solver\n Algo & Multi-Step Arithmetic Reasoning       model. So, we compare UNIPROMPT in the same\n                                                          setting in Table 8 (in Appendix A.8). UNIPROMPT\n Bool Exp.      83.64   78.74         92.37\n                                                                   (first row), which requires only one call at infer-\n Logical Ded.   29.53   38.97         39.62\n                                               ence time, performs almost as well as MedPrompt\n Navigate       60.95   51.74         77.16\n                                                             (last row), which requires five calls, on three out\n Natural Language Understanding               of four datasets. As we incrementally add kNN\n                                                    few-shot, CoT, and ensembling to our prompt, we Snarks         67.00   67.88         74.30\n                                                  see a significant increase in accuracy of 4.35% on Disamb. QA   53.30   57.43         67.05\n                                                 average across all datasets. Fallacies       57.60   53.14         57.90\n\n Use of World Knowledge                         5.3  Performance on generation tasks\n\n Causal Judg.   54.29   57.24         59.37    Our evaluations so far have been on multiple-\n Movie Rec.    58.04   77.81         71.80     choice QnA, math, and classification datasets. We\n Dates          74.21   52.59         81.96    now evaluate UNIPROMPT on generating code\n                                                 given a natural language specification. We use\n Multilingual Knowledge & Reasoning\n                                          HumanEval (Chen et al., 2021) and MBPP (Austin\n Salient Trans.  42.59   50.61         50.77      et al., 2021) datasets consisting of Python coding\n                                                  problems. We initialize with a simple prompt, “You\n                                                 are a software engineer. You are given a function\n5.2  Comparison with MedPrompt               signature and a description of the function. You\n                                             have to complete the function.” We use GPT-4-\nMedPrompt (Nori et al., 2023) is a recent, com-   Turbo as both the solver and the expert LLM.\npetitive prompting technique without any training   HumanEval does not have train or validation sets.\ncomponent. It employs three key ingredients: (1)   So, we take random 100 examples from MBPP as\nfew-shot prompting, where five relevant examples    train. Similarly, for MBPP, we take random 50 ex-\nare selected using k-nearest neighbors (kNN); (2)   amples from HumanEval as train. We evaluate the\nCoT reasoning on the selected examples; and (3)    final prompts on HumanEval and MBPP test sets.\n\n\n                                         7\n\nThe results are given in Table 4 (in Appendix A.8).   Table 3: Impact of UNIPROMPT’s key hyperparameters\nThe metric is % of solved coding problems (evalu-\nated using the provided test cases) in the datasets.      Hyperparameter    Value  Accuracy\nThe prompts produced by UNIPROMPT outperform                                                                    2    84.90%\nstandard prompting of LLMs.                                                    Mini-batch Size         5    90.36%\n                                                                    8    91.15%\n5.4  Results on a Real-world Task\n                                                                    2    85.81%The task of inferring if two search queries share\n                                           Number of Clusters      5    90.36%identical intent or not arises in search and recom-\n                                                                   10    87.82%mendation pipelines. It is challenging because it\nrequires domain knowledge (e.g., brands and prod-\nuct categories), and depends on cultural and geo-\ngraphical biases (e.g., “cricket” in UK vs. “cricket   (“Fb Clustering”) is a better strategy than cluster-\ngame” in the US). So, examples are crucial for en-   ing based on topics, except for the Ethos dataset.\ngineering a prompt that generalizes well.          Impact of Mini-batch size and Number of\nWe sample 200 train, 50 validation, and a separate    Clusters:     First, we vary mini-batch size in\n2527 test user queries from a real proprietary appli-   UNIPROMPT. The results for the ARC dataset\ncation. More details on the dataset and the one-line    are shown in Table 3 (in Appendix A.8). With\ninitial prompt are provided in Appendix A.5.        an increase in mini-batch size, we observe an in-\nThe prompt obtained using UNIPROMPT improves    crease in accuracy. That said, it is a hyperparame-\nover the best manual prompt by 5.77% on the nega-    ter, hence there will be an optimal number for each\ntive (rare) class, by 0.23% on the positive class, and    dataset. The mini-batch size affects the feedback\nby 1.86% overall on the test set. The learnt prompt   based on the wrong examples that are obtained in\ncaptures the following task facets: (1) recogniz-   each round. Next, we vary the number of clusters\ning variations in names and abbreviations, and how    in UNIPROMPT. We find that the parameter has a\nthey do not change the context; (2) recognizing    clear impact on performance. We use the default\nbrand specificity, and how even minor variations do    choice of 5 clusters in all our experiments, which\nchange the context; and (3) recognizing the speci-   provides concise and generalizable feedback.\nficity of terms in queries, and how lack of specific   Impact of initial prompt and Expert model\nterms can indicate departure of intent.               capacity:  In Table 7 (Appendix A.8), we find\n                                                         that one-line task description initialization for\n5.5  Ablations                                    UNIPROMPT achieves the best accuracy on three\nImpact of Clustering, Inclusion of History, and    out of four datasets. On ARC, initializing with the\nGreedy Update: The results are shown in Table 6   prompt generated by the Llama2-13B model gives\n(in Appendix A.8). We see that clustering as well    significant improvement over other initializations.\nas edit history components (Section 3.1) are critical    In Table 5 (Appendix A.8), we show UNIPROMPT\nfor performance of UNIPROMPT in all the datasets.   improves prompts for more capable solver LLMs\nWe see a major drop of 14.8% in accuracy in the   while using less capable expert LLMs.\nEthos dataset when clustering is removed, and a\n4.3% drop when history component is removed. In\n                                       6  Conclusionsall the datasets except GSM8K, we find clustering\nis more important than history. This can attributed\nto limited variability of question types (all grade-8  We presented a method inspired by the human\narithmetic) in GSM8K than in others.             prompt engineering process to generate complex\nWe also find that the greedy update rule (Section   prompts from scratch that include different facets\n3.1.3) proves to be superior or competitive com-   of a task. Our algorithm provides significant im-\npared to beam search in relatively easier datasets —   provements over baseline prompt generation meth-\nwhere even less exploration produces good results,   ods on multiple standard datasets.  Just like in-\ngreedy proves to be a more effective update rule.   context learning (Ji et al., 2024), task facet learning\nOn the other hand, in more complex datasets like   could also benefit from connections to submodu-\nMedQA, greedy appears to be a bad strategy. We    lar optimization (Krause and Golovin, 2014). We\nalso see that clustering examples based on feedback    leave this as future work.\n\n\n                                         8\n\n7  Limitations                                Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yi-\n                                                    han Wang, Han Guo, Tianmin Shu, Meng Song, Eric\nWe provide an analysis of the impact of model      Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing\nsize on the amenability to prompt optimization in       discrete text prompts with reinforcement learning.\n                                                          In Proceedings of the 2022 Conference on Empiri-\nAppendix A.1. However, in our evaluation, we                                                             cal Methods in Natural Language Processing, pages\nonly use GPT-3.5 (and in some cases GPT-4) as      3369–3391, Abu Dhabi, United Arab Emirates. As-\nthe solver LLM. We want to leave extensive evalu-       sociation for Computational Linguistics.\nations of using open-source LLMs as solver LLMs,\n                                                Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\nand perhaps even as expert LLMs, to future work.                                                     Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\nFurther, we also want to evaluate on other gener-       jiu Yang. 2023. Connecting large language models\native tasks, besides the code generation task we      with evolutionary algorithms yields powerful prompt\n                                                             optimizers. arXiv preprint arXiv:2309.08532.study in the paper.\n\n                                                   Shivanshu Gupta, Matt Gardner, and Sameer Singh.\n                                                      2023.  Coverage-based example selection for in-\nReferences                                             context learning.  In Findings of the Association\n                                                                for Computational Linguistics: EMNLP 2023, pages\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten      13924–13950, Singapore. Association for Computa-\n  Bosma, Henryk Michalewski, David Dohan, Ellen       tional Linguistics.\n   Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\n  Program synthesis with large language models. arXiv    Bairu Hou, Jinghan Jia, Yihua Zhang, Guanhua Zhang,\n   preprint arXiv:2108.07732.                        Yang Zhang, Sijia Liu, and Shiyu Chang. 2023.\n                                                         Textgrad: Advancing robustness evaluation in nlp\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng      by gradient-driven optimization. In The Eleventh In-\n  Huang, and Tianyi Zhou. 2024.  Instructzero: Ef-       ternational Conference on Learning Representations.\n   ficient instruction optimization for black-box large\n   language models. In Forty-first International Confer-   Cho-Jui Hsieh, Si Si, Felix X. Yu, and Inderjit S. Dhillon.\n   ence on Machine Learning.                            2023.   Automatic engineering of long prompts.\n                                                         ArXiv, abs/2311.10117.\nMark Chen,  Jerry Tworek, Heewoo Jun, Qiming\n                                               Xinyu Hu, Pengfei Tang, Simiao Zuo, Zihan Wang,  Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\n                                                  Qiang Lou, Jian Jiao, and Denis Charles. 2024.   plan, Harri Edwards, Yuri Burda, Nicholas Joseph,\n                                                    Evoke: Evoking critical thinking abilities in llms  Greg Brockman, Alex Ray, Raul Puri, Gretchen\n                                                             via reviewer-author prompt editing. In ICLR 2024.   Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\n   try, Pamela Mishkin, Brooke Chan, Scott Gray,\n                                                    Baijun Ji, Xiangyu Duan, Zhenyu Qiu, Tong Zhang,  Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\n                                                       Junhui  Li, Hao Yang,  and Min Zhang. 2024.   Kaiser, Mohammad Bavarian, Clemens Winter,\n                                                    Submodular-based in-context example selection for   Philippe Tillet, Felipe Petroski Such, Dave Cum-\n                                                        llms-based machine translation. In Proceedings of   mings, Matthias Plappert, Fotios Chantzis, Eliza-\n                                                            the 2024 Joint International Conference on Compu-\n   beth Barnes, Ariel Herbert-Voss, William Hebgen\n                                                                tational Linguistics, Language Resources and Evalu-\n  Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\n                                                            ation (LREC-COLING 2024), pages 15398–15409.   Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\n  William Saunders, Christopher Hesse, Andrew N.                                                Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\n   Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan                                                    Hanyi Fang, and Peter Szolovits. 2021. What disease\n  Morikawa, Alec Radford, Matthew Knight, Miles                                                     does this patient have? a large-scale open domain\n   Brundage, Mira Murati, Katie Mayer, Peter Welinder,                                                           question answering dataset from medical exams. Ap-\n  Bob McGrew, Dario Amodei, Sam McCandlish, Ilya                                                              plied Sciences, 11(14):6421.\n   Sutskever, and Wojciech Zaremba. 2021. Evaluating\n   large language models trained on code.                                         Omar Khattab, Arnav Singhvi, Paridhi Maheshwari,\n                                                   Zhiyuan Zhang, Keshav Santhanam, Saiful Haq,\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,      Ashutosh Sharma, Thomas T Joshi, Hanna Moazam,\n  Ashish Sabharwal, Carissa Schoenick, and Oyvind      Heather Miller,  et  al. 2024.  Dspy:  Compiling\n   Tafjord. 2018. Think you have solved question an-       declarative language model calls into state-of-the-art\n   swering? try arc, the ai2 reasoning challenge. ArXiv,       pipelines. In The Twelfth International Conference\n   abs/1803.05457.                                  on Learning Representations.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,    Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\n  Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias       taka Matsuo, and Yusuke Iwasawa. 2024.  Large\n   Plappert, Jerry Tworek, Jacob Hilton, Reiichiro      language models are zero-shot reasoners.  In Pro-\n  Nakano, Christopher Hesse, and John Schulman.      ceedings of the 36th International Conference on\n  2021. Training verifiers to solve math word prob-      Neural Information Processing Systems, NIPS ’22,\n   lems. arXiv preprint arXiv:2110.14168.              Red Hook, NY, USA. Curran Associates Inc.\n\n\n                                         9\n\nAndreas Krause and Daniel Golovin. 2014. Submodular     Ruby Chen, Jason Chen, Mark Chen, Ben Chess,\n   function maximization. Tractability, 3(71-104):3.         Chester Cho, Casey Chu, Hyung Won Chung, Dave\n                                               Cummings, Jeremiah Currier, Yunxing Dai, Cory\nXiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-      Decareaux, Thomas Degry, Noah Deutsch, Damien\n  Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang       Deville, Arka Dhar, David Dohan, Steve Dowl-\n  Low. 2024a. Prompt optimization with human feed-       ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\n   back. In ICML 2024 Workshop on Models of Human     Tyna Eloundou, David Farhi, Liam Fedus, Niko\n  Feedback for AI Alignment.                                Felix, Simón Posada Fishman, Juston Forte,  Is-\n                                                             abella Fulford, Leo Gao, Elie Georges, Christian\nXiaoqiang  Lin,  Zhaoxuan Wu,  Zhongxiang  Dai,                                                      Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,\n  Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jail-                                               Rapha Gontijo-Lopes, Jonathan Gordon, Morgan\n   let, and Bryan Kian Hsiang Low. 2024b. Use your                                                             Grafstein, Scott Gray, Ryan Greene, Joshua Gross,\n   instinct: Instruction optimization for llms using neu-                                                        Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse\n   ral bandits coupled with transformers. In Forty-first                                                   Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-\n   International Conference on Machine Learning.                                                    hannes Heidecke, Chris Hesse, Alan Hickey, Wade\n                                                         Hickey, Peter Hoeschele, Brandon Houghton, KennyPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\n                                                     Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu  Hiroaki Hayashi, and Graham Neubig. 2023. Pre-\n                                                                  Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger   train, prompt, and predict: A systematic survey of\n                                                               Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie  prompting methods in natural language processing.\n                                                         Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, ACM Comput. Surv., 55(9).\n                                                         Ali Kamali, Ingmar Kanitscheider, Nitish Shirish\nCédric Malherbe and Nicolas Vayatis. 2017. Global       Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\n   optimization of lipschitz functions. In International     Kim, Christina Kim, Yongjik Kim, Hendrik Kirch-\n  Conference on Machine Learning, pages 2314–2323.       ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\n  PMLR.                                          Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-\n                                                                   stantinidis, Kyle Kosic, Gretchen Krueger, Vishal\nRavi Mangal, Kartik Sarangmath, Aditya V Nori, and      Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\n  Alessandro Orso. 2020. Probabilistic lipschitz analy-      Leike, Jade Leung, Daniel Levy, Chak Ming Li,\n   sis of neural networks. In International Static Analy-      Rachel Lim, Molly Lin, Stephanie Lin, Mateusz\n   sis Symposium, pages 274–309. Springer.                 Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\n                                              Anna Makanju, Kim Malfacini, Sam Manning, Todor\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,                                                   Markov, Yaniv Markovski, Bianca Martin, Katie\n  Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-                                                    Mayer, Andrew Mayne, Bob McGrew, Scott Mayer\n  moyer. 2022. Rethinking the role of demonstrations:                                                 McKinney, Christine McLeavey, Paul McMillan,\n  What makes in-context learning work? In Proceed-                                                       Jake McNeil, David Medina, Aalok Mehta, Jacob\n   ings of the 2022 Conference on Empirical Methods in                                                   Menick, Luke Metz, Andrey Mishchenko, Pamela\n  Natural Language Processing, pages 11048–11064,                                                      Mishkin, Vinnie Monaco, Evan Morikawa, Daniel\n  Abu Dhabi, United Arab Emirates. Association for                                                      Mossing, Tong Mu, Mira Murati, Oleg Murk, David\n  Computational Linguistics.                                                       Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\n                                                    Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\n                                                Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\n  and Grigorios Tsoumakas. 2020. Ethos: a multi-label\n                                                          Paino, Joe Palermo, Ashley Pantuliano, Giambat-\n   hate speech detection dataset. Complex & Intelligent\n                                                                          tista Parascandolo, Joel Parish, Emy Parparita, Alex\n   Systems, pages 1–16.\n                                                           Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-\nHarsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carig-     man, Filipe de Avila Belbute Peres, Michael Petrov,\n   nan, Richard Edgar, Nicolo Fusi, Nicholas King,      Henrique Ponde de Oliveira Pinto, Michael, Poko-\n  Jonathan Larson, Yuanzhi Li, Weishung Liu, et al.       rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-\n  2023. Can generalist foundation models outcom-        ell, Alethea Power, Boris Power, Elizabeth Proehl,\n   pete special-purpose tuning? case study in medicine.      Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\n  Medicine, 84(88.3):77–3.                          Cameron Raymond, Francis Real, Kendra Rimbach,\n                                                        Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\nOpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-       der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\n   wal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-      Girish Sastry, Heather Schmidt, David Schnurr, John\n  man, Diogo Almeida, Janko Altenschmidt, Sam Alt-     Schulman, Daniel Selsam, Kyla Sheppard, Toki\n  man, Shyamal Anadkat, Red Avila, Igor Babuschkin,      Sherbakov, Jessica Shieh, Sarah Shoker, Pranav\n   Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-     Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,\n   ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,      Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\n  Jake Berdine, Gabriel Bernadett-Shapiro, Christo-      Sokolowsky, Yang Song, Natalie Staudacher, Fe-\n  pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-       lipe Petroski Such, Natalie Summers, Ilya Sutskever,\n   laine Boyd, Anna-Luisa Brakman, Greg Brockman,       Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil\n  Tim Brooks, Miles Brundage, Kevin Button, Trevor        Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\n   Cai, Rosie Campbell, Andrew Cann, Brittany Carey,      ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\n  Chelsea Carlson, Rory Carmichael, Brooke Chan,       lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\n  Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,      Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\n\n\n                                         10\n\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,    Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale\n  CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-     Schuurmans, and Joseph E Gonzalez. 2022. Tem-\n   ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,       pera: Test-time prompting via reinforcement learning.\n  Clemens Winter, Samuel Wolrich, Hannah Wong,      arXiv preprint arXiv:2211.11890.\n  Lauren Workman, Sherwin Wu, Jeff Wu, Michael\n  Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-   Zhihan Zhang, Shuohang Wang, Wenhao Yu, Yichong\n   ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong      Xu, Dan Iter, Qingkai Zeng, Yang Liu, Chenguang\n  Zhang, Marvin Zhang, Shengjia Zhao, Tianhao      Zhu, and Meng Jiang. 2023.  Auto-instruct: Auto-\n  Zheng, Juntang Zhuang, William Zhuk, and Bar-      matic instruction generation and ranking for black-\n   ret Zoph. 2023. Gpt-4 technical report.  Preprint,     box language models. Preprint, arXiv:2310.13127.\n  arXiv:2303.08774.\n                                              Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-      Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  guang Zhu, and Michael Zeng. 2023.  Automatic      Ba. 2022. Large language models are human-level\n  prompt optimization with \"gradient descent\" and      prompt engineers.\n  beam search. In The 2023 Conference on Empiri-\n   cal Methods in Natural Language Processing.         Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh\n                                                                     Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan-\nDamien Sileo. 2023.  tasksource: Structured dataset      Fang Li. 2023.  On robustness of prompt-based\n   preprocessing annotations for frictionless extreme      semantic parsing with large pre-trained language\n   multi-task learning and evaluation. arXiv preprint      model: An empirical study on codex. arXiv preprint\n  arXiv:2301.05948.                                    arXiv:2301.12868.\n\nPragya  Srivastava,  Satvik Golechha, Amit Desh-                           A  Appendix\n  pande, and Amit Sharma. 2024.   Nice:  To op-\n   timize in-context examples or not?     Preprint,                                            A.1  When is directional text optimization\n  arXiv:2402.06733.\n                                                          feasible?\nZhongXiang Sun, Kepu Zhang, Haoyu Wang, Xiao                                               Consider the class of sequential algorithms like\n  Zhang, and Jun Xu. 2024. Effective in-context exam-\n                                             ProTeGi (Pryzant et al., 2023) and TextGrad (Hou   ple selection through data compression.\n                                                             et al., 2023) . The objective is to improve the accu-\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-   racy of a given (black-box) solver LLM f : X →R\n   bastian Gehrmann, Yi Tay, Hyung Won Chung,                                                        that takes as input a prompt x ∈X and outputs\n  Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi,\n                                                     the average accuracy on a validation set Dv. Since  Denny Zhou, and Jason Wei. 2022.  Challenging\n  big-bench tasks and whether chain-of-thought can    the set of prompts is combinatorially large, we as-\n   solve them. Preprint, arXiv:2210.09261.           sume that all prompts can be embedded in a vector\n                                                space such that distance between two prompts in\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n  Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and    the space correspond to their semantic similarity.\n  Denny Zhou. 2023. Chain-of-thought prompting elic-   The prompt optimization problem can be written\n   its reasoning in large language models.  Preprint,   as arg maxx∈X f(x; Dv).\n  arXiv:2201.11903.\n                                                   Previous work has shown  that LLMs can\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten   be brittle to their input:  changing the prompt\n  Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,    slightly can create a significant difference in perfor-\n  and Denny Zhou. 2024. Chain-of-thought prompt-   mance (Zhuo et al., 2023). We want to understand\n   ing elicits reasoning in large language models. In\n                                                                     if the optimization problem is well-conditioned.  Proceedings of the 36th International Conference on\n  Neural Information Processing Systems, NIPS ’22,    Typically, conditioning can be determined by the\n  Red Hook, NY, USA. Curran Associates Inc.          Hessian. However, since f is black-box, we approx-\n                                                imate it by measuring sensitivity, or more specif-\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-\n                                                             ically, Lipschitz continuity near the optimal solu-  peng Kong. 2023.  Self-adaptive in-context learn-\n   ing: An information compression perspective for in-    tion. Based on prior work on defining continuity\n   context example selection and ordering. In Proceed-   of neural networks (Mangal et al., 2020), we use a\n   ings of the 61st Annual Meeting of the Association for    probabilistic notion.\n  Computational Linguistics (Volume 1: Long Papers),\n  pages 1423–1436, Toronto, Canada. Association for    Definition 1  (Probabilistic Lipschitz Continu-\n  Computational Linguistics.                              ity (Mangal et al., 2020)). Given a probability dis-\n                                                          tribution over inputs X, r ≥0, and a distance mea-\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\n  Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023.   sure d such as ℓ1 or ℓ2 norm, a function f : X →R\n  Large language models as optimizers.                    is (L, ϵ)-probabilistically Lipschitz with constant\n\n\n                                         11\n\nFigure 2: Estimating (probabilistic) Lipschitz constant of models (Definition 1) on (left) Ethos (middle) GSM8K\nand (right) MedQA datasets for GPT-4 and GPT-3.5 models.\n\n\nL ≥0, if                                          then evaluated on the validation set Dv. For a mea-\n                                                    sure of distance between two prompts, we take the\n                                                  cosine similarity between the embeddings of two      x,x′∼X[Pr   d(f(x), f(x′)) ≤L · d(x, x′)\n                                                  prompts. We use text-ada-002 for generating the\n                    | d(x, x′) ≤r] ≥1 −ϵ.          (1)                                                          text embeddings for prompts.\n\n  Note the focus on small changes in input through   A.3  Computational Complexity\nthe parameter r. Intuitively, the Lipschitz property                                We now consider the compute complexity of\nbounds the maximum change in f given a small                                    UNIPROMPT in terms of the number of expert or\nchange in input prompt. Typically, the lower bound                                                     solver LLM calls per epoch, stage-wise.\nof error for any sequential optimization algorithm                                                 Clustering: First, we evaluate all the training ex-\nover f is directly proportional to the Lipschitz con-                                               amples using the current prompt. Second, for every\nstant L (Malherbe and Vayatis, 2017). Therefore,                                            wrongly predicted example, we obtain feedback\nfor faster convergence, it is desirable to have a low                                            from the expert LLM. Third, for the given set of\nL, especially near the optimal solution.                                                  feedbacks, we use a single call to cluster it into\n  Empirically, we estimate L by sampling task-     l clusters. Each of the above steps incurs O(N)\nrelevant prompts so that they are close to the opti-                                                      queries, so the total query complexity of the clus-\nmal solution. Then we make small changes to the                                                      tering stage is O(N). Finally, for each example,\nprompt such that the semantic meaning stays the                                                                        i.e., (question, answer) pair, we simply map it to\nsame and measure the change in f (See Appendix                                                     the l clusters (no LLM calls).\nA.2 for experimental details). We show the change                                            Mini-batch feedback and Batch-level aggrega-\nin f per change in input for GPT4 and GPT3.5                                                        tion: At a given epoch, we evaluate every question\nmodels in Figure 2 for the Ethos, GSM8K and                                                        in the mini-batch using the current prompt and the\nMedQA datasets. Assuming ϵ = 0.05, probabilis-                                                    solver LLM (N queries overall). Next, we obtain\ntic Lipschitz constant L for GPT4 is < 1, whereas                                            one feedback over all the wrong questions in the\nit is higher for GPT3.5. Thus, as the model sizes                                                  mini-batch m (N/|m| queries). We use one call to\nincreases, the probabilistic Lipschitz constant de-                                                 aggregate these feedbacks. For prompt selection,\ncreases. So, larger models are more amenable to                                      we evaluate 4 prompts on the batch b (2 per beam),\nprompt optimization.                                                so O(4|b|) queries per batch. Hence overall query\n                                                complexity is N + N/|m| + 4N + 1 or O(N).\nA.2  Details on estimation of Lipschitz\n                                            With LLM throughput of 0.5 qps, a training + val-\n      constant L\n                                                     idation set of 300 examples, 10 clusters, and 20\nTO calculate the Lipschitz constant for a given    epochs, it takes under 7 hours to train.\nLLM and task, we take a human written promp and\n                                            A.4 SLM Training Detailsgenerate it’s paraphrases using GPT-4. We prompt\nGPT-4 with the following text: “You are given a   To induce the ability of structured prompt gener-\nsentence, you have to generate 30 paraphrases of    ation in a smaller language model, we curate a\nthe sentence, make sure that the core content of    section-wise dataset of around 12,000 task-prompt\neach paraphrase is same, you can use add, sub-    pairs. The tasks for training dataset creation were\ntract or change words\". These paraphrases are    taken from tasksource library (Sileo, 2023) that\n\n\n                                         12\n\ncontains around five hundred classification tasks.       {data_point['section']}:\nWe extract the task description from tasksource-       {section_descriptions\\\ninstruct, which contains tasksource dataset recasted       [data_point['section']]}\nwith instructions. For instance, the task description\nfor BIG-bench Entailed Polarity task is, \"Given       ### Response:\na fact, answer the following question with a yes       {data_point['prompt']}\nor a no\". The dataset provides diverse tasks and\ntheir short description, but not the human-generated\n                                            A.7  Prompt Initializationprompts for each task. To approximate human-\ngenerated prompts, we use GPT-4 as a teacher   One line task descriptions:\nmodel.\n                                                            1. Ethos: In this task, you have to determine  By prompting GPT-4 with the task description\n                                                   whether a given text is hate speech or not.and section description, we ask it to generate the\ncontents of the section. To ensure that the generated                                                            2. ARC: You have to solve the following science\nsection-wise prompts are concise and relevant, we                                                          question.\nprompt GPT-4 to not generate more than five lines\nof content for each section. We use LLAMA2-13B      3. GSM8K: In this task, you are given a math\nmodel, which we finetune using LoRA adapters as         question. You have to solve the question.\nthe auxiliary LM that generates sections.\n                                                            4. MedQA: In this task, you are given a medical\nA.5  Data Set Creation of Real-World Task            question. You have to solve the question.\n\nWe sample real user queries from a proprietary ap-\nplication, rewrite them using ML models, and ask\nexpert judges to label the query-pairs as identical\nor otherwise based on prescribed guidelines. We\nuse a set of 200 examples as training data, and an\nadditional 50 examples as validation set, to learn a\nprompt using UNIPROMPT, starting from the one-\nline description: Tell if query A and query B have\nsame intent or not. The dataset is heavily biased to-\nwards positive samples, so the metric of success is\nimprovement in accuracy, over the best manually-\nengineered prompt, on the positive and negative\nclasses individually. For testing, we use a separate\nlabelled set of 2527 examples from two geogra-\nphies — one where the training data was sampled\nfrom, and the other unseen.\n\nA.6  Prompt to Llama2-13B for fine-tuning\n\n    ### Instruction:\n    You are a prompt engineer, you have\n    to write a structured prompt.\n    For the given task description,\n    examples and section description,\n    write the contents of the section\n    that align with\n    section description.\n\n    ### Task Description:\n    {data_point['task_description']}\n\n    ### Section Description:\n\n\n                                         13\n\nA.8  Additional Results\n\nTable 4: Performance (% solved problems) of UNIPROMPT (GPT-4-Turbo solver) on code generation datasets,\ncompared to GPT-4 (OpenAI et al., 2023) and newer models.\n\n\n                          Method      HumanEval  MBPP\n\n                         GPT-4                67.0     87.5\n                           GPT-4-Turbo         87.1     90.9\n                          GPT-4o              90.2     92.4\n                       UNIPROMPT          93.8     92.5\n\n\n\nTable 5: Ablation of LLM choices for UNIPROMPT on the Ethos dataset. ‘Init’ and ‘Final’ denote initial (i.e., task\ndescription) and final prompt accuracies.\n\n\n                            Expert LLM   Solver LLM    Init   Final\n\n                          GPT-3.5-T    GPT-3.5-T    76.8   82.4\n                       GPT-4       GPT-3.5-T    76.8   92.3\n                          GPT-3.5-T   GPT-4        89.8   91.4\n                       GPT-4       GPT-4        89.8   94.3\n\n\n\n         Table 6: Ablation of design choices in UNIPROMPT with GPT-3.5-Turbo as the solver model.\n\n                                                      Ethos  ARC  MedQA  GSM8K\n                   UNIPROMPT −History         88.0    84.6     55.3       80.8\n                   UNIPROMPT −Clustering       77.5    82.0     54.1       81.5\n                   UNIPROMPT                   92.3    86.0     57.1       82.4\n                   UNIPROMPT + Greedy         93.7    90.5     55.5       82.3\n                   UNIPROMPT + Fb Clustering    87.2    91.2     58.3       82.5\n\n\n  An example of sectioned initialization prompt generated using finetuned Llama Model\n\nIntroduction:\nAssume the role of a science expert and answer the given question by selecting one\nof the options A, B, C or D.\n     1. Understand and solve science questions by selecting the best answer\n     from a given list of options.\n     2. Identify the logic behind the choices provided and make an informed decision.\n     3. Use contextual clues to choose the most accurate answer.\n     4. Be aware of the differences between science and everyday language.\n\nTask Description:\n          Scientific inquiry: Science is the systematic study of\n          the structure and behavior of the physical and natural\n          world through observation and experiment. The\n          scientific method is a process for acquiring knowledge\n          that has been improved upon since its inception in the\n          17th century. It involves making observations,\n          formulating hypotheses as to their causes, and\n          experimenting with them to support or refute the\n          hypotheses.\n\nReal-life Application:\n\n\n                                         14\n\nTable 7: Ablation on the initial prompt for UNIPROMPT (best test accuracy in bold).\n\n\n                            Init Prompt       Ethos  ARC  MedQA  GSM8K\n\n                      Expert Prompt      84.0   86.0      52.3      82.4\n                   Llama Prompt      92.0   90.5      55.5      81.5\n                     Task Description    92.3   86.0      57.1      82.4\n\n\n\n       Table 8: Comparison of UNIPROMPT (“Ours”) with MedPrompt, with GPT-4 as the solver model.\n\n                                MedQA  PubMedQA  MedMCQA  MMLU MG\n               Ours                             80.9        70.3          79.2          78.0\n               Ours + kNN                      81.0        72.2          81.4          94.0\n               Ours + kNN + CoT                83.9        74.7          82.6          96.0\n               Ours + kNN+ CoT + Ensemble     87.0        75.6          84.5          99.0\n              MedPrompt                       80.6        71.2          79.1          98.0\n\n\n\n1. Assisting Students in Science Classes:\n    In the context of science education, the ability to solve\n    science questions can help students to better understand and\n    internalize the concepts. By familiarizing themselves with\n    the basic principles of science, students can develop a\n    stronger foundation of knowledge.\n2. Improving Scientific Literacy:\n    Scientific literacy is a critical skill in today's world,\n    where scientific knowledge is increasingly important. By\n    solving science questions, individuals can improve their\n    understanding of scientific concepts and be more informed\n    about scientific developments.\n3. Scientific Questions:\n    In daily life, there are many questions that require\n    scientific knowledge to answer. For example, understanding\n    the science behind certain phenomena, such as why a magnet\n    sticks to a refrigerator door, can help us in our day-to-day\n    life.\n4. Increased Awareness:\n    By answering scientific questions, we can develop a deeper\n    understanding of the world around us and increase our\n    awareness of scientific phenomena. This can help us in our\n    daily lives and make us more knowledgeable individuals.\n\n\n\nBackground Knowledge:\n    1. Understanding of the basic concepts of science and\n    physics, such as the difference between heat, temperature\n    and friction.\n    2. Basic knowledge of the different types of skin surfaces,\n    such as dry, wet, rough, smooth, etc.\n    3. Familiarity with the different types of magnets and their\n    properties.\n    4. Understanding of the different factors that affect the\n    adhesion of magnets to different surfaces.\n    5. Knowledge of the different types of sedimentary rocks and\n\n\n                                         15\n\ntheir properties.\n\nChallenges:\n1. Ambiguity in the question:\n    The question might be ambiguous in nature, and it can be\n    difficult to understand the exact meaning of the question.\n    In such cases, it is important to read the question\n    carefully and identify the key concepts or keywords. This\n    can help in arriving at the correct answer.\n2. Scientific terms or concepts:\n    The question might contain scientific terms or concepts that\n    are unfamiliar to the user. In such cases, it is important\n    to understand the meaning of these terms or concepts and\n    their relationship with the question.\n3. Difficulty in understanding the question:\n    Sometimes, the question might be complex or abstract, making\n    it difficult to understand or interpret.\n4. Misleading statements or information:\n    The question might contain misleading or false information,\n    making it difficult to determine the correct answer.\n5. Contradiction:\n    The answer can be in conflict with well-known scientific\n    facts or principles. In such cases, it is important to make\n    a careful analysis of the evidence and choose the answer\n    that is most consistent with the available\n\nSimplification:\n1. Identify the key elements in the question:\n    Ask yourself, \"What is the main question in the question?\"\n    Identify the key elements and focus on them to solve the\n    problem.\n2. Understand the context:\n    Understand the context of the question and the background\n    knowledge you need to answer it.\n3. Identify the answer choice:\n    Identify the answer choice that best fits the context and\n    background knowledge.\n4. Eliminate the distractors:\n    Eliminate the distractors that don't fit\n\nTricks:\n1. Read the question carefully: Understand the question and its\ncontext. This will help in understanding the information and\nconcepts needed to solve the question.\n2. Identify the key concepts: Identify the key concepts and\nkeywords in the question. This will help in understanding the\nmain idea and focus on the relevant information.\n3. Understand the question structure: Understand the structure\nof the question. This will help in identifying the appropriate\nanswer option and avoiding distractions.\n4. Look for clues: Look for clues in the question and the\nanswer options\n\n\n                                         16\n\nA.8.1  Examples of training dataset\n\nTask: super glue boolq\nTask Description: With no explanation, label the following with\neither False or True.\nSection: description\nPrompt:  As a prompt engineer, your task is to create a prompt\nthat accurately labels statements as either True or False\nwithout providing any explanations. The statements may involve\ngeneral knowledge, trivia, or facts about events, places,\npeople, or objects. Ensure you have a strong understanding of\nthe subject matter and write a concise and specific task\ndescription related to the given statements.\n\n===================================================\n\nTask: glue qnli\n\nTask Description: With no explanation, label A to B with either\nentailment or not entailment\n\nSection: background\n\nPrompt: 1. Entailment means that the information in statement B\ncan be inferred directly from statement A.\n\n2. Not entailment means that the information in statement B\ncannot be inferred directly from statement A or is unrelated.\n\n3. Understanding the context and relationship between the two\nstatements is crucial for accurate classification.\n\n==================================================\n\nTask: bigbench undo permutation\n\nTask Description: In the following sentences with shuffled\nwords, find the correct order of word swaps to unscramble the\nsentence.\n\nSection: tricks\n\nPrompt: 1. Identify the key words or phrases in the task to\nunderstand the context of the sentence. Look for nouns, verbs,\nand adjectives that seem related or could logically fit together.\n\n2. Start by solving the problem step by step and focus on one\nswap at a time. Breaking the problem into smaller sub-problems\nwill make it easier to manage.\n\n3. To make the task more manageable, first focus on swapping the\nwords that are clearly out of place, such as words that should\n\n\n                                         17\n\nbe at the beginning or end of the sentence.\n\n\n\nA.9  Prompt for identifying important facets\n\nyou are given a task, along with it's description, some examples\nof how to solve the task and section descriptions.\nWhat do you think would be the most important sections to\ninclude for the given task.\n## Task\n{task}\n## Task Descirption\n{tas_description}\n## Examples\n{Examples_string}\n## Section Descriptions\n{sections}\n\nA.10  Clustering Type 1\n\nYou are given a science question, you need to tell which broad\ntopic is this question from.\nQuestion: {train_questions_new[ij]}\nAnswer: {answer}\nGive your answer as a single word, between <Answer></Answer>\ntags like: <Answer>Thermodynmics</Answer> or\n<Answer>Botany</Answer>.\nSubtopic:\n\n\n\nA.11  Clustering Type 2\n\nYou are given a set of feedbacks, you need to cluster them into\nfive groups based on similarity, and then provide a summary of\neach group. You can use the following feedbacks to cluster: \\n\n{feedback}\n\nprovide each cluster explnation within the following tags:\n<Cluster></Cluster>\n\n\n\n\nYou are given a feedback and a set of clusters, you need to tell\nwhich cluster this feedback belongs to.\n\nThe clusters are: \\n {string_of_clusters}\n\nThe feedback is: {feedback}\n\ngive your final answer as the number of the correct cluster\nbetween <Answer></Answer> tags like: <Answer>1</Answer>.'''\n\nA.12  Feedback Prompts\n\nFeedback over mini-batch\n\n\n                                         18\n\nYou are a teacher and you have to give feedback to your\nstudents on their answers.\n\nYou are teaching how to solve math problems to your students.\nYou are given a question, it's true answer and answer given by\nstudent. You are also given the explanations written by your\nstudents while solving the questions.\n\nThe questions are answered wrong by the students.\nYou have to tell why is the solution wrong and what information\nis can be added to the in the Background Knowledge part that\nwould have helped the student to write better explanations.\n\n## IMPORTANT: You are also given a history of changes you made\nto the background knowledge part and the change in student's\naccuracy after making the change. You have to use this history\nto make your feedback.\n\nBe explicit and tell the exact information that can be added\nwithout further modification / addition.\n\n### IMPORTANT: Give feedback in form of instructions like  add a\nsection, add a subsection, set the content of a section, set the\ncontent of a subsection, delete a section or delete a subsection\nin the background knowledge part.\n\nGive very granular feedbacks, like if the student has made a\nmistake in the calculation, then tell what is the mistake in the\ncalculation and how to correct it, if the student has made a\nmistake in the concept, then tell what is the mistake in the\nconcept and how to correct it.\n\n## Background Knowledge\n     {current_prompt}\n\n## History\n    {history_string}\n\n\n\nNow, it is your turn to give feedbacks to the students.\nYou can only provide a one line feedback.\n\n========================================\nFeedback over batch\n\nYou are given a set of feedbacks for some problems. The set\nfeedbacks for each problem separated by =========== symbol.\nYou have to summarize the feedbacks into a final feedback.\nYou are also given a set of wrong questions. You need to tell\nwhich edit can be applied to aid the student in solving the\nwrong question.\n\nTo achieve your task, try to follow the following steps;\n\n\n                                         19\n\n1. Identify the general problem that is being solved by all the\nfeedbacks.\n2. Once you have identified the problem, try to make a new\nfeedback that covers most of the\nfeedbacks given.\nLet's say the problem in the first feedback is the absence of\nmethods to solve linear equation and in the second feedback it\nis the method to inverse a matrix.\nYou know that both of these problems can be caused by adding how\nto solve convert a matrix  into row rediced echolon form. So,\nadd that.\n3. Try and validate your feedback. Once, you have a feedback try\nto see if it covers every\nfeedback, if it does not cover any feedback, add that to your\nnew feedback.\n4. See the wrong questions and try to identify what is the\nproblem in the question.\nIf the problem is not covered by your feedback, add that to your\nfeedback.\n5. You can add specifics like examples, definitions etc make\nsure that the feedback is enough to be directly added without\nany modification.\n\nYou may use the following function templates-\n\nadd_section(sectioname)\nadd_subsection(section_name, subsection_name)\nset_section_content(section_name, new_content)\nset_subsection_content(section_name, subsection_name, new_content)\ndelete_section(section_name)\ndelete_subsection(section_name, subsection_name)\n\nYour summary cannot include more than four functions.\nMake sure that the content is useful,\nnot just a very general statement. Something specific.\n\nInstructions:\n{edits}\n\nWrong Questions:\n{wrong_examples_string}\n\nSummary:\n\nA.13  Editing Prompt\n\nYou are given an input prompt and a feedback, you have to\nincorporate the feedback into the input prompt and output the\nfinal prompt.\nAn example of the task is given below\n\n### Input Prompt\nIntroduction: In this task you have to answer the given question.\n\n\n                                         20\n\nTable 9: Analysis of the effect of length and contents on the performance of UNIPROMPT\n\n                                             Ethos  ARC  GSM8K\n                   UNIPROMPT          93.7   90.5     82.4\n                     ICL Prompt           63.0   86.7     76.3\n                    Wrong ICL           70.4   87.1     78.2\n                      Summarized Prompt   84.3   85.5     66.0\n\n\n\n\n### Feedback\nThe background knowledge is incomplete, it does not include what\nare the factors that affect the water usage and how many water\nsources are there.\n\\\\add_subsection(\"Background Knowledge\")\n\\\\add_subsection_content(water usage depends on the population,\nclimate, economic development, and availability of water\nsources. There are two sources of water, surface water and\ngroundwater.)\n\n### Final Prompt\nIntroduction: In this task you have to answer the given question.\nBackground Knowledge: water usage depends on the population,\nclimate, economic development, and availability of water\nsources. There are two sources of water, surface water and\ngroundwater.\n\nOnly output the final prompt nothing else.\n\n### INPUT PROMPT\n{current_prompt}\n\n### FEEDBACK\n{edits}\n\n\n\n### FINAL PROMPT\n\nA.14  Example of prompt evolution using our method\n\nSee example in Figure 3.\n\nA.15  Comparision of our method with existing methods\n\nSee Figure 4.\n\nA.16  Effect of length on performance of prompt\n\nHere we answer the question: How much does only length contribute to UNIPROMPT’s success?. To\nanswer this, we replace the prompt with in-context examples of the same context length and compare\nthe accuracies in Table 9. We also compare the case where we include only the examples that the solver\nLLM gives incorrect prediction on, denoted as “Wrong ICL” row in the table. We see that there is a slight\nincrease in accuracy when wrong examples are included in the prompt over randomly including examples.\nBut, overall, UNIPROMPT performs much better than including in-context examples. This shows that\nlength is not the only factor contributing to UNIPROMPT’s success.\n\n\n                                         21\n\nIntroduction:\n In this task, you have to determine whether a given text is hate speech or not. 0 means Non-Hate and 1 means Hate.\n\n\n\n\n Introduction:\n In this task, you have to determine whether a given text is hate speech or not. 0 means Non-Hate and 1 means Hate.\n\n Background Knowledge :\n Contextual Understanding:\n When determining if a text is hate speech, it is crucial to consider the context. Not all negative or critical statements are hate speech. Hate speech involves language that is used to insult, demean, or incite violence\n against a group based on attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender.\n\n Corner Cases:\n Differentiating Hate Speech from Vulgarity:\n Hate speech is distinct from vulgarity or rudeness. While hate speech involves promoting hatred against a protected group, vulgar language is often used to express strong emotions or as an insult but does not\n necessarily carry the same intent to demean a group based on protected characteristics.\n Differentiating Opinions from Hate Speech:\n When evaluating statements, consider the presence of explicit language aimed at a group with the intent to cause harm or incite discrimination. Opinions, even if controversial or unpopular, do not automatically\n qualify as hate speech unless they contain elements that specifically target a group with hateful intent\n\n\n\n\n  Introduction:\n  In this task, you have to determine whether a given text is hate speech or not. 0 means Non-Hate and 1 means Hate.\n\n  Description:\n  Hate speech involves language that is used to express hatred, discrimination, or prejudice against a group or individuals based on characteristics such as race, religion, ethnic origin, sexual orientation, disability,\n  or gender. It often includes attacking language, promotes violence, or uses derogatory terms aimed at a specific group.\n\n Background Knowledge:\n  Contextualizing Offensive Language in Various Scenarios: In different contexts, such as policy discussions or expressions of frustration, offensive language does not automatically qualify as hate speech. It is\n  important to distinguish between strong opinions or criticism and language that promotes hatred or discrimination against a group based on protected characteristics.\n Understanding Sarcasm and Critique in Statements: When evaluating statements that include sarcasm or critique the focus on a particular group, it is important to distinguish between language that is used to\n  express a point of view or to provoke thought, and language that promotes hatred or discrimination. Sarcasm, in particular, can be used to highlight perceived ironies or contradictions without any intent to harm or\n demean a group based on protected characteristics.\n\n Corner Cases:\n  Differentiating Between Offensive Language and Hate Speech:\n  Offensive language can be vulgar or distasteful but does not necessarily constitute hate speech. Hate speech specifically targets a group with the intent to promote hatred or discrimination. Assessing the intent\n  behind the language and whether it is directed at a group based on protected characteristics is essential.\n  Incoherent Text and Neutral Requests: Incoherent or fragmented text that does not form a complete thought or statement should not be classified as hate speech. It is essential to evaluate the presence of a\n  clear message or narrative that targets a group based on protected characteristics before making a classification. Requests for content that relate to personal or cultural experiences without expressing hatred or\n  discrimination should not be classified as hate speech. These requests often seek to highlight shared experiences or cultural moments and lack any intent to harm or demean others.\n\n  Tricks:\n  Identifying Implicit Discriminatory Narratives: Statements that imply a group is responsible for negative outcomes or that things were better without them, even if not overtly derogatory, can still constitute hate\n  speech. Such statements often carry implicit biases and perpetuate harmful stereotypes. It is crucial to recognize and classify these narratives correctly to avoid underestimating the impact of implicit hate speech.\n  Identifying Derogatory Terms and Their Impact: Derogatory terms that are used to demean or insult individuals based on their sexual orientation, gender identity, race, or other protected characteristics are a\n  clear indicator of hate speech. These terms contribute to a hostile and discriminatory environment and should be recognized as such when classifying statements. Examples of such terms include slurs or\n  pejorative language that is commonly understood to be offensive to a particular group.\n\n\nFigure 3: Evolution of prompts through iterations of UNIPROMPT on the Ethos dataset. Starting from a simple\none-line prompt having an accuracy of 82%, UNIPROMPT adds background knowledge, corner cases, and additional\nsub-sections yielding a prompt with accuracy 88%. After further iterations, our algorithm converges to a detailed,\nhuman-like longform prompt that achieves accuracy of 92%.\n\n\n\n   Human Prompt\n    Let’s differentiate using step by step reasoning like a medical expert.\n   Our Prompt\n    Introduction: In this task, you are given a medical question. You have to solve the question.\n    Description:  To solve medical questions effectively, it is important to understand various\n    medical conditions, their progression, and associated clinical features.\n    Background Knowledge: Differential Diagnosis of Subcutaneous Nodules:\n    When  evaluating  subcutaneous  nodules,  consider  mobility,  consistency,  and  skin  adherence.\n    Epidermoid cysts are firm,  non-tender,  and the skin cannot be pinched over them.  Lipomas\n    are soft, mobile, and have pinchable skin.\n    Corner Cases: Antiretroviral Therapy Complications:\n    Doctor should be aware of the common side effects of antiretroviral drugs,  with specific\n    attention to the association between didanosine and pancreatitis, and the recommended management\n    strategies, such as replacing didanosine with lamivudine.\n\n\n\n   Figure 4: Comparison of human-written Prompt and prompt produced by UNIPROMPT on MedQA dataset.\n\n\nA.17  Do diverse task facets organized as sections really help?\n\nWe want to empirically validate if all the diverse task facets that UNIPROMPT learns indeed contribute to\nthe performance gains that we observe in Table 1. We consider two ablations:\n  1) We successively remove each facet (i.e., sections) in the learnt prompt for the task and report the\nperformances of the prompts with fewer facets. In Figure 6, for the Ethos dataset, we see that almost every\nadditional facet contributes to non-trivial gains in accuracy.\n   2) Could we have captured the information differently and retained the performance? We do a simple\nexperiment – we summarize all the facets (i.e., learnt prompt) and evaluate the resulting prompt. In Figure\n\n\n                                         22\n\nOPRO optimized prompt\n    Start by dissecting the problem to highlight important numbers and their relations. Decide on\n    the necessary mathematical operations like addition, subtraction, multiplication, or division,\n    required for resolution. Implement these operations, keeping in mind any units or conditions.\n    Round off by ensuring your solution fits the context of the problem to ensure accuracy\n   Our Prompt\n    Introduction: In this task, you are given a math question. You have to solve the question.\n    Strategies for Word Problems:\n    1. Understanding Word Problems: When solving word problems, it is crucial to read each sentence\n    carefully and comprehend the time periods and quantities involved. Avoid incorrect multiplication\n    or addition by paying close attention to whether a quantity remains constant over a period or\n    changes. If a quantity is consistent, it does not need to be multiplied by the number of days\n    or weeks unless the problem specifies otherwise.\n    2.  Calculating Averages: To calculate the average of a set of numbers, add all the numbers\n    together and then divide by the number of items. In word problems, ensure you have the correct\n    total before dividing by the number of periods, such as weeks, to find the average for each\n    period.\n    3. Understanding Past and Future Events in Word Problems: Distinguish between past and future\n    events by identifying the starting and ending points. To calculate the time interval between two\n    events, determine the direction of time from past to future and compute the interval accordingly.\n    This understanding is essential when dealing with problems that ask for the time since a past\n    event or until a future event.\n\n\nFigure 5: Comparison of prompt produced by the state-of-the-art ORPO (Yang et al., 2023) and by UNIPROMPT on\nthe GSM8K dataset.\n\n\n6 (right) (green line), we see that the summarized prompt has a significant accuracy drop.\n\n              Table 10: Sensitivity of UNIPROMPT to expert LLM prompts, on the Ethos dataset.\n\n                      Expert LLM Prompt for UNIPROMPT    Test Accuracy\n                    Simple prompt for mini-batch feedback            83.5\n                      Simple prompt for batch feedback              91.0\n                        Detailed prompts (Appendix A.12)              93.7\n\n\nA.18  Sensitivity to prompts used for expert LLMs in UNIPROMPT\n\nThe prompts used for expert LLMs in our algorithm, i.e., for clustering, feedback over batches and\nmini-batches, and editing, do matter for obtaining good performance. However, note that the prompts\nare task-agnostic and can be used as-is for new tasks. Moreover, prompts for clustering and editing are\nvery simple and involved minimal human effort. Further, to study the reliance of UNIPROMPT on the\nquality of feedback prompts, we run an ablation study, where we replace the engineered prompts for\nfeedback at batch and mini-batch levels with simpler prompts. The results are given in Table 10 for the\nEthos dataset. We observe that the performance of UNIPROMPT depends heavily on the prompt used for\nobtaining feedback at mini-batch level; whereas simplifying prompt for feedback at the batch level has\nmuch less impact on the final accuracy.\n\nA.19  UNIPROMPT training behavior\n\nAn example of evolution of prompts using our algorithm is given in Appendix 3. It starts with a simple\ndescription of task and adds important facets like differentiating between hate speech and rudeness. In\ncontrast, ProTeGi (Pryzant et al., 2023) yields a rather terse prompt on the same dataset: “Does the\nfollowing text contain language that targets a group of people based on their religion, gender, or other\npersonal characteristics?”.\n  The training curves in Figure 6 show that our method initially performs edits on the prompt that\nsimultaneously increase the train as well as the validation accuracy. After about 10 or 15 iterations (each\nbatch update is an iteration), validation accuracy decreases while train accuracy continues increasing,\nindicating overfitting; which we overcome using early stopping.\n\n\n                                         23\n\nFigure 6: Training curves for MedQA (left) and ARC (middle) datasets when UNIPROMPT is initialized with\n(published) state-of-the-art prompts; (right) ablation of facets on Ethos.\n\n\n\n\n\n                                         24",
"headers": [
"arXiv:2406.10504v2  [cs.AI]  19 May 2025",
"Task Facet Learning: A Structured Approach To Prompt Optimization"
],
"tables": [
"|Carefully analyze ... discriminatory,|Col2|\n|---|---|\n|<br>derogatory, or violent language ... a|<br>derogatory, or violent language ... a|\n|particular group, such as based on|particular group, such as based on|\n|**eligion or gender**|... potential impac|\n|<br>n the targeted individual or group ..|<br>n the targeted individual or group ..|",
"|Col1|[\"Easter is the most silly ... shame\", Hate], [\"Christians can get crazy ...\", Hate], [\"The Problem with Islam ...\", Hate]|\n|---|---|\n||\"HE SHOT THE KID!?!? ...\", Not-Hate],<br>\"I propose ...\", Not-Hate],<br>\"Humiliating this ...\", Not-Hate]|\n|[<br>[<br>|[<br>[<br>|\n|**Wrong Examples clustered into**<br>**mini-batches**<br>|**Wrong Examples clustered into**<br>**mini-batches**<br>|",
"|... the|intent behind the words|Col3|Col4|, an|\n|---|---|---|---|---|\n|the|**potential harm** i|t may cause...|t may cause...|t may cause...|\n||Non-hate speech includes|Non-hate speech includes|Non-hate speech includes|Non-hate speech includes|\n|**respectful and inclusive**|**respectful and inclusive**|**respectful and inclusive**|language|language|",
"|Final Prompt|(acc 0.6)|\n|---|---|",
"|Method|Ethos|ARC|MedQA|GSM8K|\n|---|---|---|---|---|",
"|Method|Ethos|ARC|MedQA|GSM8K|\n|---|---|---|---|---|\n|Task Description<br>Expert Prompt<br>Llama Prompt (Section 3.2)|76.8<br>74.1<br>74.0|79.7<br>78.4<br>89.7|52.7<br>53.1<br>52.6|59.4<br>78.9<br>79.5|\n|CoT<br>OPRO<br>ProTeGi<br>Evoke<br>EvoPrompt<br>DSPy (MIPRO v2, zero-shot)<br>TextGrad|72.0<br>65.4<br>76.0<br>63.5<br>81.6<br>79.7<br>79.5|79.4<br>79.1<br>78.8<br>89.0<br>89.9<br>82.8<br>76.5|50.3<br>53.3<br>52.9<br>52.8<br>50.3<br>**61.9**<br>50.6|76.3<br>77.1<br>77.3<br>81.0<br>81.4<br>77.3<br>81.6|\n|UNIPROMPT (Init = Task Description) + Beam<br>UNIPROMPT (Init = Task Description) + Greedy|92.3<br>**93.7**|86.0<br>**90.5**|57.1<br>55.5|**82.4**<br>82.3|\n|DSPy (BootstrapFewShotWithRandomSearch)<br>DSPy (MIPRO v2, few-shot)|86.6<br>84.0|87.5<br>86.0|***68.5**<br>62.9|74.3<br>79.7|",
"|Task|Init|OPRO|UNIPROMPT|\n|---|---|---|---|",
"|Bool Exp.<br>Logical Ded.<br>Navigate|83.64<br>29.53<br>60.95|78.74<br>38.97<br>51.74|92.37<br>39.62<br>77.16|\n|---|---|---|---|",
"|Snarks<br>Disamb. QA<br>Fallacies|67.00<br>53.30<br>57.60|67.88<br>57.43<br>53.14|74.30<br>67.05<br>57.90|\n|---|---|---|---|",
"|Causal Judg.<br>Movie Rec.<br>Dates|54.29<br>58.04<br>74.21|57.24<br>77.81<br>52.59|59.37<br>71.80<br>81.96|\n|---|---|---|---|",
"|Salient Trans.|42.59|50.61|50.77|\n|---|---|---|---|",
"|Hyperparameter|Value|Accuracy|\n|---|---|---|",
"|Mini-batch Size|2<br>5<br>8|84.90%<br>90.36%<br>91.15%|\n|---|---|---|",
"|Number of Clusters|2<br>5<br>10|85.81%<br>90.36%<br>87.82%|\n|---|---|---|",
"|Method|HumanEval|MBPP|\n|---|---|---|",
"|GPT-4<br>GPT-4-Turbo<br>GPT-4o<br>UNIPROMPT|67.0<br>87.1<br>90.2<br>93.8|87.5<br>90.9<br>92.4<br>92.5|\n|---|---|---|",
"|Expert LLM|Solver LLM|Init|Final|\n|---|---|---|---|",
"|GPT-3.5-T<br>GPT-4<br>GPT-3.5-T<br>GPT-4|GPT-3.5-T<br>GPT-3.5-T<br>GPT-4<br>GPT-4|76.8<br>76.8<br>89.8<br>89.8|82.4<br>92.3<br>91.4<br>94.3|\n|---|---|---|---|",
"|Col1|Ethos|ARC|MedQA|GSM8K|\n|---|---|---|---|---|\n|UNIPROMPT_ −_History<br>UNIPROMPT_ −_Clustering<br>UNIPROMPT<br>UNIPROMPT + Greedy<br>UNIPROMPT + Fb Clustering|88.0<br>77.5<br>92.3<br>**93.7**<br>87.2|84.6<br>82.0<br>86.0<br>90.5<br>**91.2**|55.3<br>54.1<br>57.1<br>55.5<br>**58.3**|80.8<br>81.5<br>82.4<br>82.3<br>**82.5**|",
"|Init Prompt|Ethos|ARC|MedQA|GSM8K|\n|---|---|---|---|---|",
"|Expert Prompt<br>Llama Prompt<br>Task Description|84.0<br>92.0<br>92.3|86.0<br>90.5<br>86.0|52.3<br>55.5<br>57.1|82.4<br>81.5<br>82.4|\n|---|---|---|---|---|",
"|Col1|MedQA|PubMedQA|MedMCQA|MMLU MG|\n|---|---|---|---|---|\n|Ours<br>Ours + kNN<br>Ours + kNN + CoT<br>Ours + kNN+ CoT + Ensemble<br>MedPrompt|80.9<br>81.0<br>83.9<br>**87.0**<br>80.6|70.3<br>72.2<br>74.7<br>**75.6**<br>71.2|79.2<br>81.4<br>82.6<br>**84.5**<br>79.1|78.0<br>94.0<br>96.0<br>**99.0**<br>98.0|",
"|Col1|Ethos ARC GSM8K|\n|---|---|\n|UNIPROMPT<br>ICL Prompt<br>Wrong ICL<br>Summarized Prompt|93.7<br>90.5<br>82.4<br>63.0<br>86.7<br>76.3<br>70.4<br>87.1<br>78.2<br>84.3<br>85.5<br>66.0|",
"|Introduction:<br>In this task, you have to determine whether a given text is hate speech or not. 0 means Non-Hate and 1 means Hate.|Col2|\n|---|---|\n|||\n|**Introduction:**<br>In this task, you have to determine whether a given text is hate speech or not. 0 means Non-Hate and 1 means Hate.<br>**Background Knowledge :**<br>**Contextual Understanding:**<br>When determining if a text is hate speech, it is crucial to consider the context. Not all negative or critical statements are hate speech. Hate speech involves language that is used to insult, demean, or incite violence<br>against a group based on attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender.<br>**Corner Cases:**<br>**Differentiating Hate Speech from Vulgarity:**<br>Hate speech is distinct from vulgarity or rudeness. While hate speech involves promoting hatred against a protected group, vulgar language is often used to express strong emotions or as an insult but does not<br>necessarily carry the same intent to demean a group based on protected characteristics.<br>**Differentiating Opinions from Hate Speech:**<br>When evaluating statements, consider the presence of explicit language aimed at a group with the intent to cause harm or incite discrimination. Opinions, even if controversial or unpopular, do not automatically<br>qualify as hate speech unless they contain elements that specifically target a group with hateful intent|**Introduction:**<br>In this task, you have to determine whether a given text is hate speech or not. 0 means Non-Hate and 1 means Hate.<br>**Background Knowledge :**<br>**Contextual Understanding:**<br>When determining if a text is hate speech, it is crucial to consider the context. Not all negative or critical statements are hate speech. Hate speech involves language that is used to insult, demean, or incite violence<br>against a group based on attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender.<br>**Corner Cases:**<br>**Differentiating Hate Speech from Vulgarity:**<br>Hate speech is distinct from vulgarity or rudeness. While hate speech involves promoting hatred against a protected group, vulgar language is often used to express strong emotions or as an insult but does not<br>necessarily carry the same intent to demean a group based on protected characteristics.<br>**Differentiating Opinions from Hate Speech:**<br>When evaluating statements, consider the presence of explicit language aimed at a group with the intent to cause harm or incite discrimination. Opinions, even if controversial or unpopular, do not automatically<br>qualify as hate speech unless they contain elements that specifically target a group with hateful intent|\n|||\n|**Introduction:**<br>In this task, you have to determine whether a given text is hate speech or not. 0 means Non-Hate and 1 means Hate.<br>**Description:**<br>Hate speech involves language that is used to express hatred, discrimination, or prejudice against a group or individuals based on characteristics such as race, religion, ethnic origin, sexual orientation, disability,<br>or gender. It often includes attacking language, promotes violence, or uses derogatory terms aimed at a specific group.<br>**Background Knowledge:**<br>**Contextualizing Offensive Language in Various Scenarios:**In different contexts, such as policy discussions or expressions of frustration, offensive language does not automatically qualify as hate speech. It is<br>important to distinguish between strong opinions or criticism and language that promotes hatred or discrimination against a group based on protected characteristics.<br>**Understanding Sarcasm and Critique in Statements:**When evaluating statements that include sarcasm or critique the focus on a particular group, it is important to distinguish between language that is used to<br>express a point of view or to provoke thought, and language that promotes hatred or discrimination. Sarcasm, in particular, can be used to highlight perceived ironies or contradictions without any intent to harm or<br>demean a group based on protected characteristics.<br>**Corner Cases:**<br>**Differentiating Between Offensive Language and Hate Speech:**<br>Offensive language can be vulgar or distasteful but does not necessarily constitute hate speech. Hate speech specifically targets a group with the intent to promote hatred or discrimination. Assessing the intent<br>behind the language and whether it is directed at a group based on protected characteristics is essential.<br>**Incoherent Text and Neutral Requests:** Incoherent or fragmented text that does not form a complete thought or statement should not be classified as hate speech. It is essential to evaluate the presence of a<br>clear message or narrative that targets a group based on protected characteristics before making a classification. Requests for content that relate to personal or cultural experiences without expressing hatred or<br>discrimination should not be classified as hate speech. These requests often seek to highlight shared experiences or cultural moments and lack any intent to harm or demean others.<br>**Tricks:**<br>**Identifying Implicit Discriminatory Narratives:** Statements that imply a group is responsible for negative outcomes or that things were better without them, even if not overtly derogatory, can still constitute hate<br>speech. Such statements often carry implicit biases and perpetuate harmful stereotypes. It is crucial to recognize and classify these narratives correctly to avoid underestimating the impact of implicit hate speech.<br>**Identifying Derogatory Terms and Their Impact:**Derogatory terms that are used to demean or insult individuals based on their sexual orientation, gender identity, race, or other protected characteristics are a<br>clear indicator of hate speech. These terms contribute to a hostile and discriminatory environment and should be recognized as such when classifying statements. Examples of such terms include slurs or<br>pejorative language that is commonly understood to be offensive to a particular group.|**Introduction:**<br>In this task, you have to determine whether a given text is hate speech or not. 0 means Non-Hate and 1 means Hate.<br>**Description:**<br>Hate speech involves language that is used to express hatred, discrimination, or prejudice against a group or individuals based on characteristics such as race, religion, ethnic origin, sexual orientation, disability,<br>or gender. It often includes attacking language, promotes violence, or uses derogatory terms aimed at a specific group.<br>**Background Knowledge:**<br>**Contextualizing Offensive Language in Various Scenarios:**In different contexts, such as policy discussions or expressions of frustration, offensive language does not automatically qualify as hate speech. It is<br>important to distinguish between strong opinions or criticism and language that promotes hatred or discrimination against a group based on protected characteristics.<br>**Understanding Sarcasm and Critique in Statements:**When evaluating statements that include sarcasm or critique the focus on a particular group, it is important to distinguish between language that is used to<br>express a point of view or to provoke thought, and language that promotes hatred or discrimination. Sarcasm, in particular, can be used to highlight perceived ironies or contradictions without any intent to harm or<br>demean a group based on protected characteristics.<br>**Corner Cases:**<br>**Differentiating Between Offensive Language and Hate Speech:**<br>Offensive language can be vulgar or distasteful but does not necessarily constitute hate speech. Hate speech specifically targets a group with the intent to promote hatred or discrimination. Assessing the intent<br>behind the language and whether it is directed at a group based on protected characteristics is essential.<br>**Incoherent Text and Neutral Requests:** Incoherent or fragmented text that does not form a complete thought or statement should not be classified as hate speech. It is essential to evaluate the presence of a<br>clear message or narrative that targets a group based on protected characteristics before making a classification. Requests for content that relate to personal or cultural experiences without expressing hatred or<br>discrimination should not be classified as hate speech. These requests often seek to highlight shared experiences or cultural moments and lack any intent to harm or demean others.<br>**Tricks:**<br>**Identifying Implicit Discriminatory Narratives:** Statements that imply a group is responsible for negative outcomes or that things were better without them, even if not overtly derogatory, can still constitute hate<br>speech. Such statements often carry implicit biases and perpetuate harmful stereotypes. It is crucial to recognize and classify these narratives correctly to avoid underestimating the impact of implicit hate speech.<br>**Identifying Derogatory Terms and Their Impact:**Derogatory terms that are used to demean or insult individuals based on their sexual orientation, gender identity, race, or other protected characteristics are a<br>clear indicator of hate speech. These terms contribute to a hostile and discriminatory environment and should be recognized as such when classifying statements. Examples of such terms include slurs or<br>pejorative language that is commonly understood to be offensive to a particular group.|\n|||",
"|Expert LLM Prompt for UNIPROMPT|Test Accuracy|\n|---|---|\n|Simple prompt for mini-batch feedback<br>Simple prompt for batch feedback<br>Detailedprompts(Appendix A.12)|83.5<br>91.0<br>93.7|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2406.10504v2.pdf"
}