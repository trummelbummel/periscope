{
"text": "Preprint.\n\n\n          FE DPOB: SAMPLE-EFFICIENT FEDERATED PROMPT\n           OPTIMIZATION VIA BANDITS\n\n\n                     Pingchen Lu1,2∗, Zhi Hong1,2∗, Zhiwei Shang1, Zhiyong Wang3,\n                    Yikun Ban4, Yao Shu5, Min Zhang6, Shuang Qiu7, Zhongxiang Dai1†\n                     1The Chinese University of Hong Kong, Shenzhen, 2South China University of Technology,\n                         3University of Edinburgh, 4Beihang University,\n                     5The Hong Kong University of Science and Technology (Guangzhou),\n                        6East China Normal University, 7City University of Hong Kong\n\n\n                                       ABSTRACT2025\n                          The performance of large language models (LLMs) is highly sensitive to the input\n                              prompt, making prompt optimization a critical task. However, real-world appli-\n                                 cation is hindered by three major challenges: (1) the black-box nature of pow-Sep                                   erful proprietary LLMs, (2) the need for high sample efficiency due to query\n29                            costs,users. andTo address(3) the thesedesirechallengesfor privacy-preservingsimultaneously,collaborationwe introduceamonga novelmultipleframe-\n                          work for sample-efficient federated prompt optimization based on multi-armed\n                                bandits (MABs). The MAB framework is uniquely suited for this problem as it is\n                                  (1) inherently a black-box optimization method, (2) practically sample-efficient,\n                            and (3) enables collaborative learning with theoretically guaranteed benefit from\n                          more participating agents. We first propose the Federated Prompt Optimization\n                                 via Bandits (FedPOB) algorithm, a federated variant of the Linear UCB algo-[cs.LG]\n                                 rithm, where agents collaborate by sharing model parameters instead of raw data.\n                    We then extend our approach to the practical setting of comparative user feedback\n                           by introducing FedPOB with Preference Feedback (FedPOB-Pref), an efficient\n                               algorithm based on federated dueling bandits. Extensive experiments demonstrate\n                                    that both FedPOB and FedPOB-Pref significantly outperform existing base-\n                                   lines and that their performance consistently improves as more agents participate\n                                  in the collaboration, validating the effectiveness of our federated approach.\n\n\n                1  INTRODUCTION\n\n                     Large language models (LLMs) have achieved impressive performance in a variety of real-world\n                        applications (Guo et al., 2025). However, the performance of LLMs has been shown to be highly\n                         sensitive to the input prompt (Zhou et al., 2023; Lin et al., 2024b). Consequently, prompt optimiza-\n                           tion, in which we aim to find the best prompt for a task, has emerged as a critical research area.\n                      Despite its growing popularity, the widespread real-world adoption of prompt optimization is stillarXiv:2509.24701v1\n                      hindered by three important challenges.\n\n                   The first challenge is black-box access. Some of the most powerful LLMs, such as ChatGPT and\n                   Gemini (OpenAI, 2023b; Team et al., 2023), are proprietary, black-box models that are only acces-\n                          sible via API queries. This limited access creates an immense challenge to prompt optimization.\n                   The second challenge is sample efficiency. Since querying powerful LLMs is often costly in both\n                      time and financial resources, it is of paramount importance to develop methods that can identify\n                        the optimal prompt for a given task using a small number of interactions. The third challenge is\n                       enabling collaboration among multiple users. As LLMs become more widely adopted, a natural\n                    and important question arises: how can multiple users, each with their own prompt optimization\n                          tasks, collaborate to accelerate their progress? A key constraint in such a collaborative setting is\n                       user privacy, as participants are typically unwilling to share their proprietary data, such as the his-\n                        tory of tested prompts and their corresponding performance scores. This scenario naturally aligns\n\n                          ∗Equal contribution.\n                           †Corresponding author. Correspondence to daizhongxiang@cuhk.edu.cn.\n\n\n                                                           1\n\nPreprint.\n\n\n\n\n                                                FedPOB (Local)\n                                       Agent\n    Federated Aggregation                                   What is\n                                                                                  the moon?                 Response:\n                                                                                              The moon is a          Score: 0.8\n                                                                           Prompt:                           silver lamp\n                                                                                       Poetically     LLM                  Evaluator\n                       Agent 1\n                                           Score Feedback\n\n                                                     FedPOB-Pref (Local)\n  Server\n                                                                                     Explain\n        Global Model                                                                     gravity                 Response 1:\n                       Agent 2\n                   …                                                                         Gravity pulls\n\n                                                                                                                                                                                                                           I really like                                       Agent               “Scientific”ScientificallyPrompt 1:                masses together       response                                                                                                                           2\n\n                       Agent n\n         Local parameters                                                   Explain     LLM\n         Global parameters                                                     gravity                 Response 2:\n                                                                                                                Gravity makes\n                                                                     Prompt 2:                        things fall\n                                                                                           Vividly\n    Local Model   Prompt Space\n                                              Preference Feedback\n\nFigure 1: An overview of our proposed federated prompt optimization frameworks. FedPOB han-\ndles direct score feedback, while FedPOB-Pref is designed for pairwise preference feedback.\n\nwith the principles of federated learning (FL) (Kairouz et al., 2019; McMahan et al., 2017), where\ndistributed agents collaborate on their machine learning tasks without exposing their raw data.\n\nTo tackle the combined challenges of black-box access, sample efficiency and privacy-preserving\ncollaboration, we propose a new class of federated prompt optimization algorithms built upon the\nmulti-armed bandit (MAB) framework (Lattimore & Szepesv´ari, 2020). MABs are exceptionally\nwell-suited for this problem for three main reasons. First, MAB algorithms do not require gradient\ninformation and are inherently black-box optimization methods. Second, they are designed to\nefficiently balance the exploration-exploitation trade-off, enabling them to solve complex black-box\noptimization problems in a sample-efficient manner, a property that has been successfully leveraged\nin recent work on prompt optimization (Lin et al., 2024b; Wu et al., 2024). Thirdly, federated MAB\nalgorithms (Shi & Shen, 2021; Dubey & Pentland, 2020; Dai et al., 2023) provide strong theoretical\nguarantees, ensuring that performance improves as more agents participate in the collaboration\n(Wang et al., 2020).\n\nOur first contribution is the Federated Prompt Optimization via Bandits (FedPOB) algorithm. This\nmethod is based on a federated variant of the classic Linear Upper Confidence Bound (LinUCB)\nalgorithm (Abbasi-Yadkori et al., 2011; Wang et al., 2020). In our FedPOB algorithm, each agent\nutilizes a pre-trained embedding model to represent the prompts and a linear model to predict their\nperformance. Collaboration is achieved by having agents periodically exchange and aggregate their\nLinUCB parameters, thereby learning from the collective experience of all agents without requiring\nthem to share any sensitive raw data. Importantly, thanks to the solid theoretical guarantees of the\nfederated LinUCB algorithm (Wang et al., 2020), the performance of our FedPOB algorithm is\ntheoretically guaranteed to improve with a larger number of collaborating agents.\n\nIn addition, we consider the highly practical setting of prompt optimization with preference feed-\nback, where explicit performance scores are unavailable and we are only able to observe relative\npreference feedback (e.g., the user prefers the response from prompt A than that from prompt B).\nThis problem was recently introduced by Lin et al. (2024a) to address scenarios where user feedback\nis inherently comparative. To enable sample-efficient federated prompt optimization in this novel\nsetting, we introduce our second algorithm, FedPOB with Preference Feedback (FedPOB-Pref).\nThis algorithm is a practical adaptation and modification of the federated linear dueling bandit\nframework proposed by Huang et al. (2025). Specifically, our FedPOB-Pref algorithm signifi-\ncantly reduces the communication complexity of the methods from Huang et al. (2025) while main-\ntaining the strong empirical performance. An overview of both FedPOB and FedPOB-Pref is\nillustrated in Fig. 1.\n\nWe conduct extensive experiments to validate our proposed methods. The results demonstrate that\nboth FedPOB and FedPOB-Pref achieve considerably better performance than the previous base-\n\n\n                                       2\n\nPreprint.\n\n\n\n\n\nline methods in various tasks. Furthermore, we empirically verify that the performance of our al-\ngorithms consistently improves as the number of participating agents increases, highlighting the\nbenefits of our collaborative approach. In summary, our key contributions are as follows:\n\n• We propose FedPOB, a novel algorithm for sample-efficient federated prompt optimization that\n  enables multiple agents to collaborate on finding the best prompts without sharing their raw data.\n• We extend our algorithm to the practical setting of preference-based feedback by introducing the\n  FedPOB-Pref algorithm, which is based on federated linear dueling bandits.\n• We conduct extensive experiments to validate our approach, demonstrating that our algorithms\n   significantly outperform existing baselines and scale effectively with more agents.\n\n2  PROBLEM SETTING\n\nPrompt Optimization. We address the problem of black-box prompt optimization, where the ob-\njective is to find an optimal prompt p that maximizes the performance of a black-box LLM on a\ngiven task D = (X, Y). The task consists of a set of queries X = {xk} and their corresponding\nground-truth answers Y = {yk}. Since the internal parameters of the black-box LLMs (e.g., GPT-\n4o-mini) are inaccessible and only API queries are allowed, we model the performance of the LLM\nvia an external score function. Specifically, we define\n\n                                          h               i                          s(p | D) = E(x,y)∈D m(LLM(p, x), y)  ,                           (1)\n\nin which m is a metric function that compares the model response LLM(p, x) induced by the prompt\np with the ground-truth answer y and provides a score s(p  | D). The optimization target is then\nformulated as\n                      p∗= arg max s(p | D),                                    (2)\n                                          p∈P\nwhere P denotes the space of all possible prompts.\n\nFederated Prompt Optimization. We extend the black-box prompt optimization problem to the\nfederated setting, which involves multiple agents. We consider a scenario with a set of N > 1\nagents, denoted by A, who all aim to solve the same task D. To account for agent heterogeneity, we\nallow each agent a ∈A to have its own prompt space denoted as Pa. This increases the generality of\nour setting by allowing each user to define a prompt space uniquely suited to their own preferences.\nFurthermore, each agent can generate its local prompt space Pa using existing techniques (Zhou\net al., 2023). As a result, the federated prompt optimization problem can be expressed as follows:\n                   p∗a = arg max E(x,y)∈D h m(LLM(pa, x), y)i , ∀a ∈A                   (3)\n                             pa∈Pa\nHere, each agent a ∈A aims to find the optimal prompt p∗a from its own prompt space Pa that maxi-\nmizes its performance on the task D. To achieve greater sample efficiency, all agents in A collaborate\nwithout sharing their raw data (i.e., the history of tested prompts and their scores). This problem\nformulation naturally aligns with common paradigms in the federated bandit literature (Wang et al.,\n2020; Dai et al., 2023). Therefore, we adopt the federated bandit framework to tackle this problem.\n\nFeedback Model. To solve the federated black-box prompt optimization problem, we cast the opti-\nmization process into an iterative protocol, where we sequentially select candidate prompts for eval-\nuation. At each round t, each agent a selects one or two candidate prompts and receives feedback.\nThe selection of the prompts is guided by theoretically principled bandit policies, which leverage the\ncollective observation history from all agents to achieve sample-efficient optimization (more details\nin Sec. 3). Depending on the type of feedback available, we consider two settings:\n\n• Score feedback: In this setting, each agent selects a single prompt pt,a at each round t, and\n  receives a numeric score ˆst,a as feedback, which directly reflects the performance of the prompt\n   pt,a on task D. Specifically, given a validation set DV representing the task D, the score can be\n                                  h                 i  obtained as follows: ˆst,a = E(x,y)∈DV m(LLM(pt,a, x), y)  .\n• Preference feedback: In this setting, every agent a selects a pair of prompts (p1t,a, p2t,a) at round\n   t, and observes a binary signal indicating which of the two performs better, i.e., which prompt\n  yielded the better response. For example, such feedback may be directly provided by human\n  evaluators (Lin et al., 2024a). Following the common practice from dueling bandits (Bengs et al.,\n\n\n                                       3\n\nPreprint.\n\n\n\n\nAlgorithm 1 FedPOB (Agent a ∈A)\n 1: Initialize: Wsync = Wnew,a = 0d×d, Vt,a = λId×d, bsync = bnew,a = 0d, tlast = 0\n 2: for t = 1, 2, . . . , T do\n 3:   Compute Vt,a ←λI + Wsync + Wnew,a\n                             −1 4:   Update local model ˆθt,a ←V t,a (bsync + bnew,a)\n\n                                                             −1 5:    Select prompt pt,a ←arg maxp∈Pa⟨ˆθt,a, u(p)⟩+ ν||u(p)||Vt,a\n 6:   Query pt,a to observe score feedback ˆst,a\n 7:   Update Wnew,a ←Wnew,a + ut,au⊤t,a, bnew,a ←bnew,a + ut,aˆst,a\n 8:     if (t −tlast) · log(det Vt,a/det Vlast,a) > D then\n 9:     Send a communication request to the central server\n10:     if a communication round is started then\n11:     Upload {Wnew,a, bnew,a} to the central server. Reset Wnew,a = 0d×d, bnew,a = 0d\n12:      Receive {Wsync, bsync} from server\n\n\nAlgorithm 2 FedPOB (Central Server)\n 1:  if Central server receives a communication request from any agent then\n 2:    Initiate a communication round\n 3: receive { Wnew,a and bnew,a}a∈A from each agent\n 4: Update Wsync ←Wsync + Pa∈A Wnew,a ,  bsync ←bsync + Pa∈A bnew,a\n 5: Broadcast Wsync and bsync to all agents\n\n\n  2022), we assume that the preference feedback is generated by the Bradley–Terry–Luce (BTL)\n  model (Hunter, 2004).\n\n3  FEDERATED PROMPT OPTIMIZATION VIA BANDITS\n\nWe adopt linear models, rather than more complex ones such as neural networks, to learn the\nunknown reward function for federated prompt optimization.  Accordingly, our FedPOB and\nFedPOB-Pref algorithms (illustrated in Fig. 1) are based on linear bandits (Abbasi-Yadkori et al.,\n2011) and linear dueling bandits (Bengs et al., 2022), respectively. This choice is motivated by the\nbalance linear models offer between expressiveness, simplicity, and theoretical guarantees: (1) Mod-\nern text embedding techniques powered by transformers are sufficiently mature and effective (Shi\net al., 2024; Hu et al., 2024), enabling a simple linear function to model the relationship between\nprompts and scores. (2) Linear models enable lightweight algorithmic designs. (3) Unlike feder-\nated neural bandits using neural networks for reward estimation (Dai et al., 2023), federated linear\nbandit methods provide theoretical guarantees on collaboration which ensure that the performance\nimproves as more agents join the federation (Wang et al., 2020).\n\n3.1  THE FEDPOB ALGORITHM: SCORE FEEDBACK\n\nFollowing recent works on black-box prompt optimization (Shi et al., 2024; Hu et al., 2024), we first\nmap each discrete prompt p into a continuous embedding vector u(p) ∈U using a pre-trained model.\nThis allows us to leverage rich semantic representations and simplifies the optimization problem. We\nthen model the score of a prompt for each agent a using a linear model: sa = ⟨θa, u(pa)⟩, which is\nstandard in the multi-armed bandit literature (Abbasi-Yadkori et al., 2011).\n\nLocal Prompt Selection. At the beginning of each round t, in lines 3-4 of Algo. 1, each agent a\nfirst updates its information matrix Vt,a and estimated linear parameters ˆθt,a using (1) the aggregated\ninformation from all agents received from the central server (i.e., Wsync and bsync, more details below)\nand (2) its newly collected local information (i.e., Wnew,a and bnew,a). Next, using the parameters\nVt,a and ˆθt,a, agent a selects the next prompt to query following the Upper Confidence Bound (UCB)\nstrategy (line 5 of Algo. 1):\n\n                                                              −1                           (4)                               pt,a = arg maxp∈Pa⟨ˆθt, u(p)⟩+ ν||u(p)||Vt,a\nHere the parameter ν balances exploitation (choosing prompts with large predicted rewards) and\nexploration (choosing prompts with large uncertainty). Next, we test the selected prompt pt,a using\n\n\n                                       4\n\nPreprint.\n\n\n\n\nAlgorithm 3 FedPOB-Pref (Agent a ∈A)\n 1: Initialize: Wsync = Wnew,a = 0d×d, ˆθ0 ∼N(0, σ2Id) with small σ2,\n 2: for t = 1, 2, . . . , T do\n 3:    Select first prompt p1t,a ←arg maxp∈Pa⟨ˆθt−1, u(p)⟩\n\n                                                                                       −1 4:    Select second prompt p2t,a ←arg maxp∈Pa⟨ˆθt−1, u(p)−u(p1t,a)⟩+ βt||u(p)−u(p1t,a)||W sync\n 5:   Query p1t,a, p2t,a to observe preference feedback ˆωt,a = 1(p1t,a ≻p2t,a)\n 6:   Update local model ˆθt,a ←arg minp∈Pa Lt,a(θ) −⟨∇La(ˆθt−1,a), θ⟩+ λ2 ||θ −ˆθt−1||2\n 7:   Update ∇La(θt,a) ←∇La(θt−1,a) −λ(ˆθt,a −θt−1)\n 8:   Compute Wnew,a = [u(p1t,a) −u(p2t,a)][u(p1t,a) −u(p2t,a)]⊤\n 9:   Upload {ˆθt,a, ∇La(ˆθt,a), Wnew,a} to server\n\n\nAlgorithm 4 FedPOB-Pref (Central Server)\n 1: receive {ˆθt,a, ∇La(ˆθt,a), Wnew,a}a∈A from each agent\n 2: Update server model ˆθt ←1n Pa∈A ˆθt,a −1n Pa∈A λ∇La(ˆθt,a)1\n 3: Update Wsync ←Wsync + Pa∈A Wnew,a\n 4: Broadcast ˆθt and Wsync to all agents\n\n\nthe validation set DV, to obtain score feedback ˆst,a (line 6 of Algo. 1). Then, we update the newly\ncollected local information Wnew,a and bnew,a (line 7 of Algo. 1).\n\nAgent-Server Communication. To reduce the communication cost, we only start a communication\nround when the new information collected by any agent exceeds a threshold D, i.e., when the cri-\nterion in line 8 of Algo. 1 is satisfied. If a communication request is sent by any agent, the trusted\ncentral server initiates a communication round (line 1-2 of Algo. 2) and all agents upload their local\nparameters Wnew,a and bnew,a to the central server (lines 10-11). The central server then aggregates\nthese local parameters to produce synchronized parameters Wsync and bsync (line 3-4 of Algo. 2),\nwhich are then broadcast to all agents. After the agents receive the aggregated parameters Wsync and\nbsync, they can use them to select the prompt in the next iteration, and the algorithm repeats.\n\n3.2  THE FEDPOB-PREF ALGORITHM: PREFERENCE FEEDBACK\n\nIn many practical applications, obtaining explicit numerical scores is challenging, whereas collecting\npairwise preference feedback is often more natural and cost-effective. For instance, in human-in-\nthe-loop scenarios, users can more reliably state a preference between two generated outputs than\nassign them absolute scores (Yue et al., 2012; Lin et al., 2024a). This setting, however, introduces\na significant technical hurdle: the parameter estimation for linear dueling bandits does not have a\nclosed-form solution (Bengs et al., 2022). This limitation prevents the use of the simple parameter\naggregation strategy employed by our FedPOB algorithm.\n\nThe absence of a closed-form solution naturally leads to gradient-based optimization approaches.\nRecent work by Huang et al. (2025) introduced federated linear dueling bandit algorithms (FLDB-\nGD and FLDB-OGD) that achieve collaboration by aggregating local gradients. While theoreti-\ncally sound, these methods face a practical dilemma: FLDB-GD incurs high communication costs,\nwhereas the more communication-efficient FLDB-OGD suffers significant performance degrada-\ntion. We attribute this to the fact that preference feedback is inherently noisier and less informative\nthan numerical scores, making it particularly challenging to achieve both competitive performance\nand communication efficiency. To overcome this, we draw inspiration from classical federated\nlearning for solving supervised learning problems (McMahan et al., 2017). Specifically, instead of\naggregating gradients, we aggregate model parameters, which allows us to adopt a dynamic regu-\nlarization technique that has proven effective in federated learning (Acar et al., 2021) for further\nperformance improvement. This leads to our proposed FedPOB-Pref algorithm (Algos. 3 and 4).\n\nOur FedPOB-Pref algorithm offers several key advantages: (1) it is highly sample-efficient, ca-\npable of learning the underlying reward model from a small number of preference queries; (2) it is\nrobust to agent heterogeneity, and its performance scales effectively with the number of collabo-\n\n\n                                       5\n\nPreprint.\n\n\n\n\nrating agents; and (3) when compared to the baselines from Huang et al. (2025), FedPOB-Pref si-\nmultaneously reduces communication costs and improves performance (Sec. 4.2).\n\nThe overall workflow of FedPOB-Pref is outlined in Algorithms 3 and 4. At each round t, every\nagent a selects a pair of prompts based on the global model ˆθt−1. The first prompt, p1t,a, represents\npure exploitation (line 3), while the second, p2t,a, incorporates an exploration bonus to discover\nmore informative options (line 4).  This dueling selection strategy is grounded in the theory of\ndueling bandits (Bengs et al., 2022; Verma et al., 2024). We then obtain binary preference feedback\nωt,a = 1p1t,a≻p2t,a for this pair of selected prompts (line 5). The core of our method lies in the local\nmodel update (line 6), which optimizes an objective that combines the standard logistic loss with a\ndynamic regularizer (Acar et al., 2021). The first component is the pairwise logistic loss over the\nagent’s local history:\n\n\n            t−1\nLt,a(θ) = − X  ωτ,a log σ θ⊤ u(p1τ,a) −u(p2τ,a)  +(1−ωτ,a) log σ θ⊤ u(p2τ,a) −u(p1τ,a)      .\n           τ=1\n                                                                                                   (5)\nThis term is the negative log-likelihood of the observed preferences under the BTL model (Bengs\net al., 2022). The second component is a dynamic regularization term consisting of (i) a linear\npenalty, −⟨∇La(ˆθt−1,a), θ⟩, which corrects for local gradient drift, and (ii) a quadratic penalty,\nwhich prevents the local model from deviating excessively from the previous global model (Acar\net  al., 2021).  After this local update (lines 6-8), agents upload their new parameters to the\ncentral server for aggregation, which then broadcasts the aggregated global parameters for the\nnext round. Of note, we conduct theoretical analysis to motivate the local objective function of\nFedPOB-Pref (App. D), providing theoretical justification for its strong performance (Sec. 4.2).\n\n\n4  EXPERIMENTS\n\n\nWe adopt MPNet (Song et al., 2020) as the text embedding model, and use GPT-3.5-turbo (OpenAI,\n2023a) in the experiments unless specified otherwise. Of note, we also test two other models, GPT-\n4o-mini (OpenAI, 2023b) and Qwen3-235B-A22B-2507 (Bai et al., 2023), in Sec. 5. Evaluation is\nperformed on the Instruction Induction (Chen et al., 2023; Lin et al., 2024b) and BIG-Bench Hard\ndatasets (Suzgun et al., 2023), which collectively cover over 50 tasks that span diverse areas such\nas reasoning, language comprehension, and code generation. To account for agent heterogeneity,\nwe ensure that the prompt domains of all agents contain both shared prompts and unique prompts.\nFor fair comparisons, we ensure an equal validation query budget across all algorithms and ana-\nlyze the corresponding communication costs in the federated setting. We defer more details on the\nexperimental setting to App. B.\n\n\n4.1  SCORE FEEDBACK: FEDPOB\n\nIn the setting with score-based feedback, every tested prompt receives a numerical score indicating\nthe quality of its induced response. Here we assess performance of a prompt using a validation set\nand adopt the validation accuracy as the corresponding score. The objective is to identify the optimal\nprompt (i.e., the one that achieves the highest validation score). We compare our FedPOB with a\nrepresentative baseline method on federated prompt optimization: FedOne (Wang et al., 2025), as\nwell as two other baselines on standard prompt optimization: INSTINCT (Lin et al., 2024b) and\nPromptBreeder (Fernando et al., 2024).\n\nTable 1 and 2 report the final scores achieved by the best prompt discovered by each algorithm\nin various tasks. The results demonstrate the superior capability of our FedPOB, which achieves\nthe highest score on the majority of the tasks under the setting of ten agents.  Fig. 2 depicts the\nperformance of FedPOB across different iterations, where we observe a positive correlation be-\ntween the number of agents and the achieved prompt score, highlighting the benefits of multi-agent\ncollaboration. In addition, FedPOB achieves a near-optimal score with a small batch of samples,\ndemonstrating its sample efficiency.\n\n\n                                       6\n\nPreprint.\n\n\n\n\nTable 1: Average validation accuracy (with standard error) of the best prompt found by each al-\ngorithm in the Instruction Induction dataset, averaged over 5 independent trials with different\nrandom seeds. For clarity, only a representative subset of challenging tasks. The complete results\nfor all tasks are provided in Table 5 (App. C.3) and the results are consistent.\n\n\n   Dataset               INSTINCT     PromptBreeder    FedOne (10 agents)               FedPOB (ours)\n\n                                                                                   1 Agent        3 Agents        10 Agents\n\n   Active to Passive           0.940±0.053      1.000±0.000         1.000±0.000       0.804±0.160    0.960±0.014    0.972±0.023\n   Auto Categorization        0.313±0.012      0.220±0.020         0.264±0.004       0.272±0.030    0.308±0.018    0.288±0.023\n  Antonyms                0.767±0.023      0.840±0.020         0.870±0.005       0.792±0.046    0.812±0.027    0.828±0.023\n  Common Concept          0.217±0.040      0.118±0.010         0.136±0.003       0.188±0.015    0.210±0.007    0.208±0.018\n   Informal to Formal         0.570±0.020      0.521±0.067         0.605±0.005       0.528±0.028    0.528±0.039    0.570±0.030\n   Larger Animal            0.993±0.012      0.987±0.012         0.829±0.037       0.984±0.017    0.992±0.011    0.989±0.011\n   Negation                 0.860±0.020      0.927±0.012         0.897±0.010       0.856±0.061    0.940±0.014    0.920±0.032\n   Orthography Starts With    0.767±0.214      0.813±0.061         0.436±0.024       0.804±0.100    0.828±0.056    0.832±0.087\n  Rhymes                  0.493±0.142      0.393±0.031         0.916±0.027       0.664±0.120    0.776±0.187    0.844±0.106\n   Second Word Letter        0.847±0.110      0.947±0.042         0.625±0.034       0.792±0.199    0.880±0.157    0.972±0.023\n   Sentence Similarity        0.467±0.031      0.380±0.020         0.360±0.035       0.540±0.094    0.508±0.082    0.448±0.018\n   Sentiment                0.973±0.012      0.993±0.012         0.996±0.002       0.988±0.018    0.972±0.023    0.972±0.027\n  Synonyms                0.327±0.150      0.333±0.115         0.320±0.023       0.324±0.103    0.296±0.041    0.384±0.124\n  Taxonomy Animal         0.947±0.023      0.967±0.042         0.805±0.026       0.924±0.073    0.980±0.024    0.972±0.034\n   Translation En-De         0.820±0.020      0.820±0.060         0.927±0.004       0.820±0.047    0.840±0.032    0.868±0.036\n   Translation En-Es          0.747±0.042      0.746±0.023         0.950±0.012       0.756±0.026    0.740±0.072    0.728±0.030\n   Translation En-Fr          0.947±0.023      0.920±0.040         0.919±0.005       0.944±0.033    0.940±0.283    0.948±0.018\n  Word in Context           0.553±0.058      0.620±0.040         0.409±0.091       0.460±0.084    0.640±0.020    0.608±0.036\n   Object Counting           0.520±0.106      0.473±0.110         0.497±0.019       0.520±0.074    0.616±0.039    0.588±0.050\n  Odd One Out              0.867±0.058      0.833±0.116         0.859±0.024       0.800±0.122    0.900±0.000    0.900±0.000\n  Word Sorting              0.753±0.058      0.753±0.099         0.497±0.026       0.756±0.093    0.744±0.065    0.828±0.063\n  Word Unscrambling        0.687±0.012      0.687±0.023         0.728±0.005       0.724±0.046    0.716±0.026    0.720±0.028\n\n   Average (22 Tasks)             0.669             0.665                0.645               0.663            0.701            0.712\n\n\nTable 2: Performance on the Big-Bench Hard (BBH) dataset under the same experimental settings.\n\n\n  Dataset                        INSTINCT   PromptBreeder  FedOne (10 agents)            FedPOB (ours)\n\n                                                                                       1 Agent       3 Agents      10 Agents\n\n  Boolean Expressions                   0.793±0.046    0.853±0.012       0.883±0.003     0.800±0.025   0.836±0.021   0.844±0.026\n  Date Understanding                    0.587±0.012    0.593±0.030       0.633±0.007     0.580±0.028   0.576±0.033   0.572±0.030\n  Disambiguation QA                    0.713±0.031    0.753±0.023       0.858±0.011     0.816±0.026   0.844±0.017   0.840±0.032\n Dyck Languages                      0.713±0.031    0.693±0.012       0.722±0.005     0.672±0.018   0.668±0.023   0.680±0.032\n  Formal Fallacies                      0.687±0.031    0.967±0.058       0.991±0.002     0.700±0.121   0.872±0.175   0.812±0.172\n  Geometric Shapes                     0.453±0.058    0.360±0.060       0.272±0.007     0.436±0.022   0.412±0.039   0.448±0.036\n  Hyperbaton                          0.913±0.046    0.907±0.023       0.946±0.003     0.868±0.522   0.928±0.027   0.948±0.018\n  Logical Deduction Five Objects          0.473±0.046    0.460±0.053       0.466±0.009     0.464±0.041   0.452±0.030   0.476±0.017\n  Logical Deduction Seven Objects         0.513±0.046    0.473±0.031       0.485±0.002     0.476±0.043   0.492±0.046   0.488±0.415\n  Logical Deduction Three Objects         0.600±0.053    0.573±0.046       0.635±0.009     0.604±0.033   0.636±0.017   0.644±0.009\n  Movie Recommendation                0.820±0.069    0.767±0.023       0.688±0.004     0.720±0.037   0.720±0.032   0.732±0.027\n  Multistep Arithmetic Two               0.647±0.129    0.601±0.030       0.685±0.017     0.580±0.105   0.648±0.018   0.692±0.046\n  Navigate                             0.707±0.031    0.760±0.020       0.755±0.028     0.688±0.052   0.720±0.042   0.716±0.026\n  Penguins in a Table                    0.577±0.031    0.694±0.016       0.581±0.031     0.562±0.035   0.584±0.031   0.605±0.015\n  Reasoning about Colored Objects         0.547±0.023    0.593±0.023       0.440±0.008     0.548±0.036   0.528±0.034   0.568±0.027\n  Ruin Names                          0.707±0.023    0.767±0.042       0.625±0.003     0.688±0.039   0.660±0.042   0.724±0.067\n  Salient Translation Error Detection       0.573±0.012    0.633±0.070       0.500±0.055     0.584±0.033   0.588±0.018   0.600±0.028\n  Snarks                               0.778±0.022    0.770±0.051       0.675±0.003     0.779±0.022   0.791±0.012   0.782±0.019\n  Sports Understanding                  0.440±0.106    0.540±0.072       0.669±0.004     0.524±0.114   0.552±0.073   0.564±0.078\n  Temporal Sequences                   0.647±0.050    0.473±0.046       0.403±0.019     0.612±0.058   0.648±0.050   0.652±0.052\n  Tracking Shuffled Objects Five Objects    0.300±0.053    0.287±0.012       0.279±0.030     0.296±0.017   0.304±0.017   0.328±0.023\n  Tracking Shuffled Objects Seven Objects   0.280±0.020    0.253±0.042       0.281±0.006     0.268±0.023   0.268±0.023   0.256±0.029\n  Tracking Shuffled Objects Three Objects   0.473±0.046    0.440±0.020       0.413±0.018     0.432±0.039   0.420±0.049   0.400±0.014\n Web of Lies                          0.633±0.023    0.607±0.012       0.627±0.012     0.640±0.039   0.644±0.043   0.636±0.026\n\n  Average (24 Tasks)                          0.607           0.618              0.605             0.596          0.616          0.625\n\n\n\n4.2  PREFERENCE FEEDBACK: FEDPOB-PR E F\n\nTo simulate user preference feedback in our experiments, we adopt the protocol from Lin et al.\n(2024a). For any pair of prompts (pt,1, pt,2), we first compute their ground-truth scores, s(pt,1)\nand s(pt,2), on a validation set. The preference probability is then determined by the Bradley-\nTerry-Luce (BTL) model (Hunter, 2004): P(pt,1 ≻pt,2) = σ(s(pt,1) −s(pt,2)), where σ(·) is\nthe sigmoid function. A binary preference outcome yt = 1(pt,1 ≻pt,2) is then sampled from a\nBernoulli distribution with this probability. We compare FedPOB-Pref against federated baselines\nFLDB-GD and FLDB-OGD (Huang et al., 2025), as well as standard prompt optimization methods\nAPOHF (Lin et al., 2024a) and DoubleTS (Dwaracherla et al., 2024).\n\n\n                                       7\n\nPreprint.\n\n\n\n\n\n     0.82                                                                        0.64                                                                                         0.80                                                                          0.60\n     0.80                                                                                  0.62                                                                                                                                                                                0.78\n                                                                                                                                                                                                                                                             0.58     0.78\n                                                                                  0.60                                                                                                                                                          076     0.76\n                                                                                                                                                                                                                                                             0.56                                                                                  0.58     0.74                                                                                                                                                                                0.74   Reward                                                                                                                                                        Reward                                                                                                                                                                                                                                                                                                                                              Reward                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Reward     0.72                                                                                  0.56\n                                                                                                                                                                                0.72                                                                                                                                                                                                                                                             0.54\n     0.70                                                                                  0.54\n                                                                                                                                                                                0.70     0.68                                                                                                                                                                                                                                                             0.52    Average                                                                                                                                                                                 Average                                                                                  0.52                                                  Agent=1                                                                                                              Agent=1                                                                                                                                                                                                                                                                                                                                                                                                      Average                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Average     0.66                                                                                                                                                                                0.68                                                  Agent=3                                                                                                              Agent=3                                                                                                                                                                                                   Agent=1                                                                                                                                                                                                                                                                   Agent=1                                                                                  0.50     0.64                                                                                                                                                                                                                                                             0.50                                                  Agent=10                                                                                                              Agent=10                                                                                                                                                                                                   Agent=3                                                                                                                                                                                                                                                                   Agent=3                                                                                                                                                                                0.66\n                                                                                                                                                                                                   Agent=10                                                                                                                                                                                                                                                                   Agent=10     0.62 0          10          20                                           30          40                                                                 50                                                                                  0.48 0          10          20                                                                                                              30          40                                                                                                                                    50                                                                                                                                                                                0.64\n                                                                                                                                                             0          10          20          30          40                                                                                                                                                                                                                       50                                                                                                                                                                                                                                                             0.48 0          10          20          30          40                                Iterations                                                                                                 Iterations\n                                                                                                                                                                                   Iterations                                                         Iterations        (a) Instruction                                                                                                                                                                                                                                   50                                   (b) BBH                        (a) Instruction\n         Induction                                                                          (b) BBH                                                               Induction\nFigure 2: Performance of FedPOB with varying    Figure 3: Performance of FedPOB-Pref with\nnumbers of agents.                              varying numbers of agents.\n\n\n\nThe results, summarized in Table 3, demonstrate that FedPOB-Pref consistently achieves the\nbest performance across different numbers of agents. Our method establishes a superior trade-off\nbetween performance and communication cost. Specifically, FedPOB-Pref matches the commu-\nnication efficiency of FLDB-OGD while delivering substantially better results. Conversely, while\nFLDB-GD obtains the second-best performance, it does so at a considerably higher communication\ncost. Fig. 3 further highlights that the sample efficiency of FedPOB-Pref improves as more agents\ncollaborate. Additional results are available in Fig. 10 (App. C.2).\n\n\nTable 3: Score and number of communication rounds under\npreference feedback.                                                                                 0.62\n\n                                                                                                                             0.61\n                                     Instruction Induction      BBH\n    Method           Agent\n                                                                                                                                     0.6\n                                       Perf.    Comm.       Perf.    Comm.                                                                                                                                                                                                                                            Reward\n                                                                                                                             0.59\n   APOHF                  -      0.7681           -        0.5838         -\n    Double TS               -      0.7859           -        0.5983         -                   0.58                                                                                                                                                                                                                                                                                   Average\n                       1      0.7624     1500      0.5868    1500                0.57      FedBOP(Agent=3)\n   FLDB-GD          3      0.7959     1500      0.6204    1500                       FedBOP(Agent=10)\n                      10     0.8244     1500      0.6457    1500                0.56  0                 10                100      300      1000\n                       1      0.6872      50       0.5286     50                      Communication Threshold (log scale)\n   FLDB-OGD        3      0.7687      50       0.5880     50\n                      10     0.8123      50       0.6271     50       Figure 4:  Scores of FedPOB with\n                       1      0.8000      50       0.6213     50       varying communication thresholds D.\n   FedPOB-Pref     3      0.8145      50       0.6357     50\n                      10     0.8482      50       0.6583     50\n\n\n\n5  ABLATION STUDY\n\nPerformance vs. Communication in FedPOB.  In federated learning, communication is inher-\nently costly, making frequent interactions with the central server impractical. Thus, an effective\nalgorithm should maintain strong performance even with infrequent communications. Here we re-\nduce the interaction frequency by varying the communication threshold D in FedPOB in the range:\n{0, 10, 100, 300, 1000}. Note that a larger D results in less communication rounds. The results in\nFig. 4 reveal a clear trade-off between performance (the best score after 20 iterations) and com-\nmunication, i.e., fewer communication rounds (i.e., larger D) result in worse performance. More\nimportantly, our FedPOB still achieves strong performance even with infrequent communications,\ndemonstrating its robustness and practical effectiveness in realistic federated environments.\n\n\nGeneralization to Other LLMs.  While the response quality of an LLM depends not only on the\nprompt design but also on the inherent capability of the backbone model, we examine whether\nthe observed performance gains of our algorithms can generalize to other LLMs.  To this end,\nwe replace the GPT-3.5-Turbo model used in our main experiments by GPT-4o-mini and Qwen\n(OpenAI, 2023a;b; Bai et al., 2023), while keeping all other settings fixed. As shown in Fig. 5,\nour FedPOB consistently discovers high-score prompts and achieves better performance with a\nlarger number of agents, regardless of the underlying LLM. Additional results on the performance\nof FedPOB-Pref can be found in App. C.4, which lead to consistent observations.\n\n\n                                       8\n\nPreprint.\n\n\n\n\n\n   0.78                                                                        0.78                                                                        0.82                                                                        0.76\n                                                                                                                                                            0.80   0.76                                                                                0.76                                                                                                                                                                                                                                         0.74\n                                                                                                                                                            0.78\n   0.74                                                                                0.74\n                                                                                                                                                            0.76\n   0.72                                                                                0.72                                                                                                                                                            0.74                                                                                                                                                                                                                                         0.70Reward                                                                                                                                                                                                                                      0.72                                                                                                                                                            0.72                                                                                                                                      Reward   0.70                                                                                                                                      Reward                                                                                0.70                                                                                                                                      Reward\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Average 0.66Average 0.680.66                                                                        0.68                                                                        0.70                                                                        0.68                                                                                                                                                                             Average 0.66                                                 Agent=1                                                                                                             Agent=1                                                                                                                                                                         Agent=1                                                                                                                                                                                                                                     Agent=1                                                                                                                                                                                                                                                                                                                                                          Average 0.680.66\n                                                                                                                                                                         Agent=3                                                 Agent=3                                                                                                             Agent=3                                                                                                                                                                                                                                     Agent=3                                                                                                                                                                                                                                         0.64   0.64                                                                                0.64                                                                                                                                                            0.64                                                 Agent=10                                                                                                             Agent=10                                                                                                                                                                         Agent=10                                                                                                                                                                                                                                     Agent=10\n   0.62 0          10          20          30          40        50     0.62 0          10          20          30          40        50     0.62 0          10          20          30          40        50     0.62 0          10          20          30          40        50\n                              Iterations                                                        Iterations                                                        Iterations                                                        Iterations\n\n  (a) Instruction Induction            (b) BBH            (c) Instruction Induction            (d) BBH\n     (GPT-4o-mini)           (GPT-4o-mini)             (Qwen)                (Qwen)\n\n             Figure 5: The performance of FedPOB using GPT-4o-mini and Qwen.\n\n\nEffectiveness of Dynamic Regularization in FedPOB-Pref.  We further assess the neces-\nsity of the dynamic regularization term in FedPOB-Pref, which mitigates the dynamic drift\namong heterogeneous clients and accelerates collaboration.  We compare the performance of\nFedPOB-Pref with and without this term, the latter of which is equivalent to the classical Fe-\ndAvg algorithm (McMahan et al., 2017)). Fig. 6 shows that incorporating dynamic regularization\nstabilizes performance, speeds up convergence, and reduces fluctuations caused by inter-agent het-\nerogeneity. These results highlight its critical role in enabling efficient and robust federated prompt\noptimization in heterogeneous federated environments.\n\n\n\n              Dataset\n\n\n\n\n\n Instruction\n Induction\n\n\n\n\n\n  BBH\n\n\n\n\n\n                                                                                                                                          Agent Number\n\n                              Agent=1                                        Agent=3                                        Agent=10\n\nFigure 6: Impact of the dynamic regularization term in FedPOB-Pref. FedAvg corresponds to\nremoving this term.\n\n\n\n6  RELATED WORK\n\nFederated Prompt Optimization.  Federated Learning enables collaborative model training with-\nout sharing private data (Kairouz et al., 2019; McMahan et al., 2017). However, applying FL to\nLLMs faces a critical barrier: the prohibitive cost of communicating updates for models of such\nmassive scale. A natural workaround is to combine FL with parameter-efficient prompt tuning\n(Zhao et al., 2023; Che et al., 2023; Deng et al., 2024; Wei et al., 2023), where only lightweight soft\nprompts are trained and communicated. While resource-efficient, this paradigm operates in a white-\nbox setting and thus fails in API-based black-box scenarios. This limitation has motivated research\non black-box federated prompt optimization (Lin et al., 2023). Early efforts such as FedBPT (Zhang\net al., 2023) adopt soft prompts with gradient-free optimization, but remain incompatible with API-\nonly LLMs. More recent work addresses discrete prompt optimization, e.g., FedOne (Wang et al.,\n2025), which learns categorical distributions to sample prompts. Despite solving discreteness, these\nmethods suffer from inefficiency and poor semantic quality, leaving open the challenge of develop-\ning a query-efficient federated method that produces semantically meaningful discrete prompts for\nblack-box LLMs. We defer a detailed discussion of the related works on standard non-federated\nprompt optimization to App. A due to space constraint.\n\n\n                                       9\n\nPreprint.\n\n\n\n\n7  CONCLUSION\n\nIn this paper, we introduced FedPOB and FedPOB-Pref, novel algorithms for sample-efficient\nfederated prompt optimization. Built upon the theory of federated multi-armed bandits, our meth-\nods enable multiple agents to effectively collaborate to find optimal prompts for black-box LLMs\nwithout sharing raw data. Extensive experiments demonstrate that our algorithms significantly out-\nperform existing baselines under both score and preference feedback, with performance consistently\nimproving with an increasing number of participating agents. Notably, FedPOB-Pref establishes\na superior performance-to-communication trade-off in the practical preference-based setting.\n\nREFERENCES\n\nYasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic\n   bandits. In Proc. NIPS, 2011.\n\nDurmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Paul N. Whatmough Matthew Mattina,\n  and Venkatesh Saligrama. Federated learning based on dynamic regularization. In Proc. ICLR,\n  2021.\n\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\n  Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Dayiheng Liu, Gao Liu,\n  Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi\n  Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng\n  Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi\n  Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang\n  Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint\n  arXiv:2309.16609, 2023.\n\nViktor Bengs, Aadirupa Saha, and Eyke H¨ullermeier. Stochastic contextual dueling bandits under\n   linear stochastic transitivity models. In Proc. ICML, 2022.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\n  wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\n  Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n   Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\n  Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\n   Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proc. NeurIPS,\n  2020.\n\nTianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor Sheng, Huaiyu Dai, and Dejing\n  Dou. Federated learning of large language models with parameter-efficient prompt tuning and\n  adaptive optimization. In Proc. EMNLP, 2023.\n\nLichang Chen, Jiuhai Li, Tiejun Zhang, and Bo Zhou. InstructZero: A preference-based iterative\n  prompt optimization framework. In Proc. EMNLP, 2023.\n\nZhongxiang Dai, Arun Verma Yao Shu, Flint Xiaofeng Fan, and Bryan Kian Hsiang Low. Federated\n  neural bandits. In Proc. ICLR, 2023.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Zhang, Han Li, Yaliang Chen, Lidong Zhao, Jing Liu,\n  Yang Chen, and Xiang Liu. RLPrompt: Optimizing discrete text prompts with reinforcement\n   learning. In Proc. EMNLP Findings, 2022.\n\nWenlong Deng, Christos Thrampoulidis, and Xiaoxiao Li. Unlocking the potential of prompt-tuning\n   in bridging generalized and personalized federated learning. In Proc. CVPR, 2024.\n\nShizhe Diao, Zhichao Huang, Ruijie Xu, Xuechun Li, Lin Yong, Xiao Zhou, and Tong Zhang.\n  Black-box prompt learning for pre-trained language models. Transactions on Machine Learning\n  Research, 2023.\n\nAbhimanyu Dubey and Alex Pentland.  Differentially-private federated linear bandits.  In Proc.\n  NeurIPS, pp. 6003–6014, 2020.\n\n\n                                       10\n\nPreprint.\n\n\n\n\n\nVikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, and Benjamin Van Roy.  Efficient\n  exploration for LLMs. In Proc. ICML, 2024.\n\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt¨aschel.\n  Promptbreeder: Self-referential self-improvement via prompt evolution. In Proc. ICLR, 2024.\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\n  Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\n  via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n\nQingyan Guo, Rui Wang, Junzhe Guo, Boyu Li, Kai Song, Xu Tan, Guoqing Liu, Jiang Bian, and\n  Yanyang Yang. Connecting large language models with evolutionary algorithms yields powerful\n  prompt optimizers. In Proc. ICLR, 2024.\n\nWenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu, Xiangqiang Lin, Zhongxiang Dai, See-Kiong\n  Ng, and Bryan Kian Hsiang Low. Localized zeroth-order prompt optimization. In Proc. NeurIPS,\n  2024.\n\nXuhan Huang, Yan Hu, Zhiyan Li, Zhiyong Wang, Benyou Wang, and Zhongxiang Dai. Federated\n   linear dueling bandits. arXiv preprint arXiv:2502.01085, 2025.\n\nDavid R Hunter. Mm algorithms for generalized bradley-terry models. Annals of Statistics, 2004.\n\nGurusha Juneja, Gautam Jajoo, Nagarajan Natarajan, Hua Li, Jian Jiao, and Amit Sharma. Task\n   facet learning: A structured approach to prompt optimization. In Proc. ACL, 2025.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin\n  Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances\n  and open problems in federated learning. arXiv:1912.04977, 2019.\n\nWeize Kong,  Spurthi Hombaiah, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky.\n  PRewrite: Prompt rewriting with reinforcement learning. In Proc. ACL Short Papers, 2024.\n\nTor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n  tuning. In Proc. EMNLP, 2021.\n\nXiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing continuous prompts for generation. In\n  Proc. ACL, 2021.\n\nXiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang\n  Low. Prompt optimization with human feedback. arXiv preprint arXiv:2405.17346, 2024a.\n\nXiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick\n   Jaillet, and Bryan Kian Hsiang Low. Use your INSTINCT: Instruction optimization using neural\n  bandits coupled with transformers. In Proc. ICML, 2024b.\n\nZihao Lin, Yitao Zeng, Sicheng Yu, Lue Tao, Yuxin Chen, Wenhao Yu, and Lifu Huang.\n   Efficient federated prompt tuning for black-box large pre-trained models.   arXiv preprint\n  arXiv:2310.03123, 2023.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\n  Understands, Too. In Proc. ACL, 2021.\n\nYichong Luo, Huaxiu Yao, Feng-Shih Chang, Zhi-Kai Zhang, and Jian-Yun Nie.  Black-box\n  prompt optimization: Aligning large language models without model training.  arXiv preprint\n  arXiv:2311.02646, 2023.\n\nO. Ma˜nas, P. Astolfi, M. Hall, C. Ross, J. Urbanek, A. Williams, A. Agrawal, A. Romero-Soriano,\n  and M. Drozdzal. Improving text-to-image consistency via automatic prompt optimization. arXiv\n  preprint arXiv:2403.17804, 2024.\n\nH Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient\n  learning of deep networks from decentralized data. In Proc. AISTATS, 2017.\n\n\n                                       11\n\nPreprint.\n\n\n\n\nOpenAI. GPT-3.5: Openai language model. https://platform.openai.com/, 2023a. Ac-\n  cessed: 2025-09-24.\n\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023b.\n\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. GrIPS: Gradient-free, edit-based in-\n   struction search for prompting large language models. In Proc. ACL, 2023.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\n  optimization with ”gradient descent” and beam search. In Proc. EMNLP, 2023.\n\nL. Schneider, M. Wistuba, A. Klein, J. Golebiowski, G. Zappella, and F. A. Merra. Hyperband-based\n  bayesian optimization for black-box prompt selection. arXiv preprint arXiv:2412.07820, 2024.\n\nChengshuai Shi and Cong Shen. Federated multi-armed bandits. In Proc. AAAI, 2021.\n\nChengshuai Shi, Kun Yang, Jing Yang, and Cong Shen. Best arm identification for prompt learning\n  under a limited budget. arXiv preprint arXiv:2402.09723, 2024.\n\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh.  Auto-\n  Prompt: Eliciting knowledge from language models with automatically generated prompts.  In\n  Proc. EMNLP, 2020.\n\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-\n   training for language understanding. In Proc. NeurIPS, 2020.\n\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\n  Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, and Jason Wei. Challenging big-\n  bench tasks and whether chain-of-thought can solve them. In Proc. ACL Findings, 2023.\n\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\n  Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\n  capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n\nArun Verma, Zhongxiang Dai, Xiaoqiang Lin, Patrick  Jaillet, and Bryan Kian Hsiang Low.\n  Neural dueling bandits: Preference-based optimization with human feedback.  arXiv preprint\n  arXiv:2407.17112, 2024.\n\nGanyu Wang, Yuekang Li, Yi Zeng, Tianyu Wang, Kang Yang, and Kai Chen.   FedOne:\n  Query-efficient federated learning for black-box discrete prompt learning.   arXiv preprint\n  arXiv:2502.04943, 2025.\n\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P.\n  Xing, and Zhiting Hu. PromptAgent: Strategic planning with language models enables expert-\n   level prompt optimization. In Proc. ICLR, 2024.\n\nYuanhao Wang, Jiachen Hu, Xiaoyu Chen, and Liwei Wang.  Distributed bandit learning: Near-\n  optimal regret with efficient communication. In Proc. ICLR, 2020.\n\nGuoyizhe Wei, Feng Wang, Anshul Shah, and Rama Chellappa. Dual prompt tuning for domain-\n  aware federated learning. In Proc. ECCV Workshop, 2023.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V. Le, and\n  Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proc.\n  NeurIPS, 2022.\n\nZhaoxuan Wu, Xiaoqiang Lin, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick\n   Jaillet, and Bryan Kian Hsiang Low. Prompt optimization with EASE? efficient ordering-aware\n  automated selection of exemplars. In Proc. NeurIPS, 2024.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\n  Chen. Large language models as optimizers. In Proc. ICLR, 2024.\n\nChun-Pai Yang, Kan Zheng, and Shou-De Lin.  Plhf: Prompt optimization with few-shot human\n  feedback. arXiv preprint arXiv:2505.07886, 2025.\n\n\n                                       12\n\nPreprint.\n\n\n\n\n\nZiyu Ye, Hao-Yang Chen, Yong-Qiang Hu, Zhen-Yu Su, Qing-An Yao, Yu-Hong Liu, Xiao-Rong\n   Lai, and Yi-Feng Wu. Align-Pro: A principled approach to prompt optimization for llm align-\n  ment. arXiv preprint arXiv:2308.11585, 2023.\n\nYisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits\n  problem. Journal of Computer and System Sciences, 2012.\n\nRuichen Zhang, Zechu Li, Zhaoxuan Wu, Zhongxiang Dai, Yao Shu, and Bryan Kian Hsiang\n  Low. FedBPT: Efficient federated black-box prompt tuning for large language models. In Proc.\n  NeurIPS, 2023.\n\nHaodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. Fedprompt: Communication-\n   efficient and privacy-preserving prompt tuning in federated learning. In Proc. ICASSP, 2023.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\n  Jimmy Ba. Large language models are human-level prompt engineers. In Proc. ICLR, 2023.\n\n\nA  ADDITIONAL RELATED WORK\n\nThe performance of Large Language Models (LLMs) is highly sensitive to the quality of input\nprompts (Zhou et al., 2023; Lin et al., 2024b). While carefully handcrafted prompts (Brown et al.,\n2020; Wei et al., 2022) can substantially enhance model capabilities, the manual design process\nis time-consuming and heavily reliant on expert intuition. To address this challenge, early studies\nfocused on white-box prompt optimization, including AutoPrompt (Shin et al., 2020), Prefix-Tuning\n(Li & Liang, 2021), P-Tuning (Liu et al., 2021), and Prompt Tuning (Lester et al., 2021). More\nrecently, increasing attention has been devoted to black-box prompt optimization (Yang et al., 2024;\nMa˜nas et al., 2024; Juneja et al., 2025; Schneider et al., 2024), with representative methods such as\nGRIPS (Prasad et al., 2023), BDPL (Diao et al., 2023), PRewrite (Kong et al., 2024), PromptAgent\n(Wang et al., 2024), and APO (Pryzant et al., 2023). RLPrompt (Deng et al., 2022) addresses\nthe discrete black-box setting by optimizing a probability distribution over prompts, from which\ncandidates are sampled to identify the optimal one. Evolutionary approaches, such as EvoPrompt\n(Guo et al., 2024) and Promptbreeder (Fernando et al., 2024), employ mutation and crossover to\niteratively improve prompts. Zhou et al. (Zhou et al., 2023) introduced APE, which leverages an\nLLM to generate candidate instructions and refines those with high evaluation scores. However,\nthese approaches often require extensive sampling and validation, making them sample-inefficient.\nA key direction has been reframing black-box prompt optimization as a continuous problem, as in\nInstructZero (Chen et al., 2023) and ZOPO (Hu et al., 2024). Building on this idea, INSTINCT (Lin\net al., 2024b) employs neural bandits to sequentially select instructions to query, leveraging neural\nnetworks to better capture the relationship between prompts and their performance, thereby enabling\nmore efficient optimization.\n\nRecent work has investigated prompt optimization in scenarios where direct human feedback is dif-\nficult to obtain and only preference feedback is available. BPO (Luo et al., 2023) trains an indepen-\ndent optimizer that automatically rewrites initial prompts using paired preference data, encouraging\nblack-box LLMs to produce better responses. Align-Pro (Ye et al., 2023) develops a theoretical\nframework based on the Bradley–Terry model to analyze and guide optimization through pairwise\ncomparisons. APOHF (Lin et al., 2024a) formulates prompt optimization as a dueling bandits prob-\nlem, directly leveraging pairwise preferences (e.g., A is better than B) to efficiently identify the best\nprompt among candidates. Building on this idea, PLHF (Yang et al., 2025) extends preference-based\noptimization to a few-shot setting, demonstrating that high-quality prompts can be identified with\nonly a small number of comparisons, thereby greatly reducing annotation costs.\n\nB  MORE DETAILS ON THE EXPERIMENTAL SETTING\n\nB.1  DATASETS AND MODELS\n\nDatasets. We use 29 tasks from the Instruction-Induction dataset (Lin et al., 2024b), excluding the\nauto-debugging task which contains only 8 instances, and the Cause-and-Effect task. The Cause-\nand-Effect task is an open-ended reasoning problem where multiple answers may be reasonable, but\n\n\n                                       13\n\nPreprint.\n\n\n\n\n\nonly one ground-truth is provided. Existing metrics cannot accurately evaluate responses, and most\nautomatic scores are generally zero. For example, a few instances are:\n\n        • Cause: “The child hurt their knee.” Effect: “The child started crying.”\n\n        • Cause: “My car got dirty.” Effect: “I washed the car.”\n\n        • Cause: “Someone fainted.” Effect: “Someone called 911.”\n\nFor the BBH dataset (Suzgun et al., 2023), we adopt 24 tasks, excluding 3 tasks that overlap with\nInstruction-Induction to avoid double evaluation.\n\nModels.   Our  experiments  are  conducted on  three LLMs,  OpenAI/GPT-3.5-turbo-0613,\nOpenAI/GPT-4o-mini, and Qwen/Qwen3-235B-A22B-2507 via the OpenRouter API. We use MP-\nNet (Song et al., 2020) as the embedding model.\n\n\nB.2  PROMPT SPACE GENERATION\n\nTo simulate a realistic federated setting, we adopt the APE algorithm (Zhou et al., 2023) to construct\na prompt pool from a small initial task description (i.e., a set of input–output exemplars). From this\npool, each agent samples both shared and personalized prompts, thereby capturing the inherent data\nheterogeneity—where shared prompts model the common knowledge across agents, while personal-\nized prompts reflect the distinct distributions, preferences, and contextual variations specific to each\nclient.\n\nPrompt Template. We follow INSTINCT (Lin et al., 2024b) for prompt template to automatically\ngenerate prompt space. We use 5 exemplars in datasets to query LLM to induct prompt.\n\n\n   Prompt Generation Template\n\n    Input: [INPUT]\n    Output: [OUTPUT]\n   <More exemplars...>\n    Input: [INPUT]\n    Output: [OUTPUT]\n   The instruction was to\n\n\n               Figure 7: Prompt Generation template for prompt space generation.\n\n\n\n   Prompt Generation Example\n\n    Input: [Today is Christmas Eve of 1937. What is the date 10 days later?]\n    Output: [01/03/1938]\n   <More exemplars...>\n    Input: [Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later.\n   What is the date 24 hours later?]\n    Output: [03/13/2002]\n   The instruction was to\n\n\n             Figure 8: Illustrative example of prompt generation with the template.\n\n\n\nB.3  IMPROVED EVALUATION METHOD\n\nEvaluation Challenges. Due to the complex nature of the BBH tasks, we observed that large lan-\nguage models (LLMs) often generate detailed explanations along with their final answers, unlike the\nmore direct outputs seen in the Instruction-Induction tasks. This behavior was particularly prevalent\n\n\n                                       14\n\nPreprint.\n\n\n\n\n\nwhen using models such as GPT-4o-mini and Qwen3. A small number of tasks in the Instruction-\nInduction dataset also exhibited this tendency toward verbose responses. Standard evaluation metrics\nsuch as exact match, contain, or F1-score proved unreliable in this context. Since the ground-truth\nanswers are typically concise, the verbosity of model outputs frequently led to misclassification. In\nsome cases, a model’s response was fully correct from a human perspective, yet automated metrics\nincorrectly assigned a score of zero.\n\nMulti-choice Metric. To mitigate this issue, we designed a new evaluation metric, termed Multi-\nchoice, specifically tailored to handle the verbose outputs of LLMs on BBH tasks. Our approach\nnormalizes the model’s output and checks whether the ground-truth answer is present. In practice,\nwe extract the final sentence of the model’s prediction and verify if it contains the ground-truth\nanswer.\n\nMetrics. For BBH, we evaluate on 24 tasks using the Multi-choice metric. For Instruction-Induction\n(29 tasks), we follow Lin et al. (2024b) and adopt the same evaluation setup. Concretely, we use the\nF1 metric for “Common concept” and “Informal to formal”; exact set matching for “Orthography\nstarts with” and “Taxonomy animal”; and label containment for “Synonyms”. For the remaining\ntasks, we apply exact match. Additionally, for “Diff” and “Odd one out”, when evaluated with GPT-\n4o-mini or Qwen3 (where verbose explanations are frequent), we employ the Multi-choice metric\ninstead of exact match.\n\nCached Prompt Scoring. We leverage the alignment between prompts and their validation scores.\nSince our validation set is relatively large (50 samples), we observed that the scores obtained for a\ngiven prompt remain stable across repeated evaluations. Consequently, for all algorithms that require\noptimization over a prompt space (excluding FedOne and PromptBreeder, which do not depend on a\nprompt space), we evaluate each prompt once on the validation set and cache the resulting score for\nsubsequent use. This strategy substantially reduces computation time while maintaining evaluation\nreliability.\n\n\nB.4  HYPERPARAMETERS OF OUR ALGORITHMS\n\nIn FedPOB, we set λ = 1, ν = 0.3, D = 10.0, and d = 768, where d matches the output feature\ndimension of MPNet (Song et al., 2020). For FedPOB-Pref, we set λ = 1 and use a learning rate\nof 0.001 to update θt,a (line 7 of Algo. 3). Training is conducted for 30 iterations.\n\nThe parameter βt is time-dependent. Following (Huang et al., 2025), we set\n\n              r                         tκµ\n                            βt =   2 log(1/δ) + d log 1 + dλ   ,\n\nwhere κµ denotes the number of agents and d is the feature dimension (here d = 768 for compati-\nbility with MPNet).\n\n\nB.5  HYPERPARAMETERS OF BASELINE AND FAIR COMPARISONS\n\nTo ensure fairness, we set the total number of validation queries to be the same across all methods\nand report them consistently in our experimental results (see Tables 1, 2, and 3 in Sec. 4, as well as\nTable 5 in App. C).\n\nFor score feedback baselines, only INSTINCT and our method share the same evaluation protocol,\nwhere each iteration queries the validation set once. Therefore, we ensure fairness by comparing\nthe best reward obtained within the first 50 validation queries, rather than rewards at every single\niteration. For preference-feedback baselines, all methods query the validation set twice per iteration,\nas two prompts are sampled for pairwise comparison. Running 50 iterations thus corresponds to 100\nvalidation queries in total. For consistency, we report the score of the first (exploitation) prompt\nselected by each method. This is consistent with the work of Lin et al. (2024a). The reward curves\nare plotted across iterations, where the x-axis represents the number of iterations (equivalently,\npreference-feedback steps).\n\nScore Feedback. For FedPOB, we run 50 iterations, thus querying the validation set 50 times. We\nreport the best reward at the 50th iteration. For INSTINCT, we follow the default settings from\ntheir paper, which are consistent with our protocol (one query per iteration), and also report the\n\n\n                                       15\n\nPreprint.\n\n\n\n\n               Table 4: Query settings and reported metrics for different methods.\n\n                                   Score Feedback\n\n       Method          Queries/Iter   Total Queries      Reported Metric\n\n      FedPOB             1            50         Best reward at 50th iter.\n       INSTINCT           1            50         Best reward at 50th iter.\n        PromptBreeder        5            50        Best reward at 10th round.\n       FedOne              5            50         Best reward at 50th iter.\n\n                                 Preference Feedback\n\n      FedPOB-Pref       2           100         Best reward at 50th iter.\n      FLDB-OGD          2           100         Best reward at 50th iter.\n      FLDB-GD            2           100         Best reward at 50th iter.\n      APOHF              2           100         Best reward at 50th iter.\n        Double-TS           2           100         Best reward at 50th iter.\n\n\n\n\n\nbest reward at the 50th iteration. For PromptBreeder, which is an evolutionary algorithm, half of the\npopulation queries the validation set in each round. With a population size of 10 (2 mutation prompts\n× 5 thinking styles), this results in 5 queries per round and 50 queries in total over 10 rounds; we\nreport the best reward at the 10th round. For FedOne, we follow the original paper and construct its\nvocabulary using the PMI algorithm, sampling frequent and high-quality words or word pairs from\nthe large prompt domain generated by APE. The setup involves 10 agents, each sampling 5 prompts\nper round for 50 iterations. To ensure a fair comparison with 50 validation queries, we pair agents\nand take the maximum score among the prompts they generate as the final performance of FedOne.\n\nPreference Feedback.  For methods based on preference feedback, including FedPOB-Pref,\nFLDB-OGD, FLDB-GD, APOHF, and Double-TS, each iteration samples two prompts and queries\nthe validation set twice to obtain a pairwise preference. Running for 50 iterations therefore requires\n100 validation queries in total. We report the best reward at the 50th iteration (based on 100 queries\nin total). Other hyperparameters follow their original settings to ensure a fair comparison.\n\n\n\n\n\nC  MORE EXPERIMENTAL RESULTS\n\n\n\n\nC.1  ADDITIONAL EXPERIMENTS ON PROMPT DOMAIN GENERATION METHODS\n\n\n\nPerformance and Stability Across Different Prompt Domains.  In the experiment section, we\nuse GPT-3.5-Turbo to generate the prompt domain via APE. To further validate that our algorithm\nachieves superior performance across different prompt domains generated by different methods, we\nreplace GPT-3.5-Turbo with GPT-4o-mini while keeping all other settings fixed, such as running\nboth our algorithm and the baselines under the same LLM model, GPT-3.5-Turbo. As shown in\nFig. 9, Our method consistently achieves strong performance across different prompt domains, un-\nderscoring its robustness to domain variability. Beyond maintaining high accuracy, it is capable of\nidentifying near-optimal prompts in a sample-efficient manner, thereby reducing the overall cost of\nAPI queries to LLMs.\n\n\n                                       16\n\nPreprint.\n\n\n\n\n\n   0.60                                                                                       0.58\n\n   0.58                                                                                       0.56\nReward 0.560.54                                                                                                                                                                 Reward 0.540.52\n\n   0.52                                                                                       0.50\nAverage 0.50                                                                                                                                                                                            Average 0.48\n                                                   Agent=1                                                                    Agent=1\n   0.48                                             Agent=3                       0.46                                             Agent=3\n                                                   Agent=10                                                                   Agent=10\n   0.46                                                                                       0.44\n       0          10          20          30          40        50                     0          10          20          30          40        50\n                              Iterations                                                                    Iterations\n\n                    (a) FedPOB                                              (b) FedPOB-Pref\n\n                    Figure 9: Performance across different prompt domains\n\n\n\n\n\nC.2  COMPLETE RESULTS IN FEDPOB-PR E F\n\n\n\n\n\n              Dataset\n                                                           FedPOB-Pref (ours)                FLDB-GD             FLDB-OGD             APOHF                Double TS\n\n\n\n\n\n Instruction\n Induction\n\n\n\n\n\n  BBH\n\n\n\n\n\n                                                                                                                                            Agent Number\n\n                              Agent=1                                      Agent=3                                   Agent=10\n\n         Figure 10: More detailed comparison for FedPOB-Pref using GPT-3.5-Turbo.\n\n\n\n\n\nC.3  COMPLETE RESULTS FOR FE DPOB\n\n\n\n       Table 5: Performance comparison on the complete set of Instruction Induction tasks.\n\n\n                                       17\n\nPreprint.\n\n\n\n\n\n Dataset             INSTINCT   PromptBreeder  FedOne (10 agents)              FedPOB\n\n                                                                         1 Agent      3 Agents     10 Agents\n\n Active to Passive         0.940±0.053    1.000±0.000       1.000±0.000      0.804±0.160   0.960±0.014   0.972±0.023\n Auto Categorization      0.313±0.012    0.220±0.020       0.264±0.004      0.272±0.030   0.308±0.018   0.288±0.023\n Antonyms              0.767±0.023    0.840±0.020       0.870±0.005      0.792±0.046   0.812±0.027   0.828±0.023\n Common Concept        0.217±0.040    0.118±0.010       0.136±0.003      0.188±0.015   0.210±0.007   0.208±0.018\n Diff                    1.000±0.000    1.000±0.000       1.000±0.000      0.992±0.018   1.000±0.000   1.000±0.000\n  First Word Letter         1.000±0.000    1.000±1.000       0.713±0.089      1.000±1.000   1.000±1.000   1.000±1.000\n Informal to Formal       0.570±0.020    0.521±0.067       0.605±0.005      0.528±0.028   0.528±0.039   0.570±0.030\n Larger Animal           0.993±0.012    0.987±0.012       0.829±0.037      0.984±0.017   0.992±0.011   0.989±0.011\n  Letters List             1.000±0.000    1.000±0.000       0.831±0.095      0.952±0.107   1.000±0.000   1.000±0.000\n Negation               0.860±0.020    0.927±0.012       0.897±0.010      0.856±0.061   0.940±0.014   0.920±0.032\n Num to Verbal           1.000±0.000    1.000±0.000       1.000±0.000      1.000±0.000   1.000±0.000   1.000±0.000\n Orthography Starts With   0.767±0.214    0.813±0.061       0.436±0.024      0.804±0.100   0.828±0.056   0.832±0.087\n Rhymes                0.493±0.142    0.393±0.031       0.916±0.027      0.664±0.120   0.776±0.187   0.844±0.106\n Second Word Letter      0.847±0.110    0.947±0.042       0.625±0.034      0.792±0.199   0.880±0.157   0.972±0.023\n Sentence Similarity       0.467±0.031    0.380±0.020       0.360±0.035      0.540±0.094   0.508±0.082   0.448±0.018\n Sentiment               0.973±0.012    0.993±0.012       0.996±0.002      0.988±0.018   0.972±0.023   0.972±0.027\n Singular to Plural        0.993±0.012    1.000±0.000       1.000±0.000      1.000±0.000   0.996±0.009   1.000±0.000\n Sum                   1.000±0.000    1.000±0.000       1.000±0.000      0.984±0.036   1.000±0.000   1.000±0.000\n Synonyms              0.327±0.150    0.333±0.115       0.320±0.023      0.324±0.103   0.296±0.041   0.384±0.124\n Taxonomy Animal       0.947±0.023    0.967±0.042       0.805±0.026      0.924±0.073   0.980±0.024   0.972±0.034\n Translation En-De        0.820±0.020    0.820±0.060       0.927±0.004      0.820±0.047   0.840±0.032   0.868±0.036\n Translation En-Es        0.747±0.042    0.746±0.023       0.950±0.012      0.756±0.026   0.740±0.072   0.728±0.030\n Translation En-Fr        0.947±0.023    0.920±0.040       0.919±0.005      0.944±0.033   0.940±0.283   0.948±0.018\n Word in Context         0.553±0.058    0.620±0.040       0.409±0.091      0.460±0.084   0.640±0.020   0.608±0.036\n Object Counting         0.520±0.106    0.473±0.110       0.497±0.019      0.520±0.074   0.616±0.039   0.588±0.050\n Odd One Out            0.867±0.058    0.833±0.116       0.859±0.024      0.800±0.122   0.900±0.000   0.900±0.000\n Periodic Elements        1.000±0.000    1.000±0.000       0.946±0.017      0.976±0.054   1.000±0.000   1.000±0.000\n Word Sorting            0.753±0.058    0.753±0.099       0.497±0.026      0.756±0.093   0.744±0.065   0.828±0.063\n Word Unscrambling      0.687±0.012    0.687±0.023       0.728±0.005      0.724±0.046   0.716±0.026   0.720±0.028\n\n Average 29 Task            0.7715          0.7687             0.7356           0.7637        0.7977        0.8068\n\n\n\n\n\nC.4  FURTHER EVALUATION ACROSS LLM MODELS\n\n\n\n\n\n              Dataset                                FedPOB-Pref (ours)                FLDB-GD             FLDB-OGD             APOHF                Double TS\n\n\n\n\n\nInstruction\nInduction\n\n\n\n\n\n  BBH\n\n\n\n\n\n                                                                                                                                           Agent Number\n\n                             Agent=1                                     Agent=3                                      Agent=10\n\n          Figure 11: More detailed comparison for FedPOB-Pref using GPT-4o-mini.\n\n\n                                       18\n\nPreprint.\n\n\n\n\n\n              Dataset                                 FedPOB-Pref (ours)                FLDB-GD             FLDB-OGD             APOHF                Double TS\n\n\n\n\n\n Instruction\n Induction\n\n\n\n\n\n  BBH\n\n\n\n\n\n                                                                                                                                            Agent Number\n\n                              Agent=1                                       Agent=3                                    Agent=10\n\n    Figure 12: More detailed comparison for FedPOB-Pref using Qwen3-235B-A22B-2507.\n\n\n\n\n\n   0.78                                                                              0.78\n\n   0.76                                                                              0.76\n\n   0.74                                                                              0.74\n\n   0.72                                                                              0.72Reward                                                                                                                                                         Reward\n   0.70                                                                              0.70\n\n   0.68                                                                              0.68Average                                                                                                                                                                                  Average   0.66                                                                              0.66\n                                                     Agent=1                                                             Agent=1\n   0.64                                              Agent=3             0.64                                              Agent=3\n                                                     Agent=10                                                            Agent=10\n   0.62                                                                              0.62\n       0          10          20          30          40         50            0          10          20          30          40         50\n                               Iterations                                                            Iterations\n\n                (a) Instrcution Induction                                          (b) BBH\n\n     Figure 13: The performance of FedPOB-Pref across different iterations GPT-4o-mini.\n\n\n\n\n\n   0.80                                                                              0.74\n\n   0.78                                                                                     0.72\n   0.76\n                                                                                     0.70\n   0.74\nReward 0.72                                                                                                                                                Reward 0.68\n   0.70                                                                              0.66\n   0.68Average                                                                                                                                                                                  Average 0.64\n   0.66                                              Agent=1                                                             Agent=1\n   0.64                                              Agent=3             0.62                                              Agent=3\n                                                     Agent=10                                                            Agent=10\n   0.62                                                                                   0.6\n       0          10          20          30          40         50            0          10          20          30          40         50\n                               Iterations                                                            Iterations\n\n                (a) Instrcution Induction                                          (b) BBH\n\nFigure 14: The performance of FedPOB-Pref across different iterations Qwen3-235B-A22B-\n2507.\n\n\n                                       19\n\nPreprint.\n\n\n\n\nD  MATHEMATICAL PRINCIPLES OF THE LOCAL OBJECTIVE FUNCTION\n   ADOPTED BY FE DPOB-PREF\n\nThis section provides a rigorous mathematical analysis of the local objective function adopted by\nFedPOB-Pref for federated optimization. We derive the first-order optimality conditions and\ndemonstrate the necessity of the linear dual term for ensuring convergence to a globally opti-\nmal and consistent solution. The results here provide theoretical support for the design of our\nFedPOB-Pref algorithm.\n\n\nD.1  PROBLEM FORMULATION\n\nThe standard federated learning objective is to minimize a global function F(θ), defined as the\naverage of m local client objectives fi : Rd →R:\n\n                             m\n                                      1\n                              F(θ) = X fi(θ).\n                      m\n                                               i=1\n\nFor distributed optimization, this is equivalently formulated as a constrained problem with local\nvariables θi and a global consensus variable θ:\n\n                  m\n                       1\n               min  X fi(θi)   s.t.   θi −θ = 0,   ∀i ∈{1, . . . , m}.                (6)\n                   θ,{θi}mi=1 m                             i=1\n\nD.2  THE AUGMENTED LAGRANGIAN METHOD\n\nThe constrained problem in Eq. equation 6 can be solved using the Method of Multipliers. We\nintroduce a dual variable (Lagrange multiplier) ai ∈Rd for each consensus constraint and add a\nquadratic penalty term for the constraint violation. This forms the augmented Lagrangian function\nL:\n                     m      m           m\n                            1                         γ\n            L({θi}, θ, {ai}) = X fi(θi) + X⟨ai, θi −θ⟩+ X ∥θi −θ∥2,\n               m                          2\n                                  i=1         i=1                i=1\nwhere γ > 0 is a penalty parameter. An iterative algorithm then seeks a saddle point of this function.\n\n\nD.3  FIRST-ORDER STATIONARITY CONDITIONS\n\nA stationary point of the augmented Lagrangian must satisfy ∇θiL = 0 and ∇θL = 0. These\nfirst-order conditions are derived as follows.\n\nThe partial derivative with respect to a local variable θi is:\n\n                  ∂L    1\n                 = m∇fi(θi) + ai + γ(θi −θ) = 0.                           (7)                          ∂θi\n\nThe partial derivative with respect to the global variable θ is:\n\n             m     m               m       m\n        ∂L\n         = − X ai −γ X(θi −θ) = 0  =⇒ X ai = −γ X(θi −θ).          (8)\n          ∂θ\n                    i=1       i=1                       i=1         i=1\n\nTo see the implication of these conditions, we sum Eq. equation 7 over all clients i:\n\n                 m        m     m\n                      1\n        X ∇fi(θi) + X ai + γ X(θi −θ) = 0.\n            m\n                            i=1           i=1       i=1\n\nSubstituting the expression for Pi ai from Eq. equation 8 into the above yields:\n\n               m         m        m\n                   1\n       X ∇fi(θi) −γ X(θi −θ) + γ X(θi −θ) = 0,\n          m\n                        i=1             i=1             i=1\n\n\n                                       20\n\nPreprint.\n\n\n\n\n\nwhich simplifies to:\n                         m\n                                1\n            X ∇fi(θi) = 0.\n                  m\n                                        i=1\nThis proves that any stationary point of L satisfies that the average of the local gradients is zero. If\nthe solution is also primally feasible (i.e., θi = θ), this condition becomes precisely the first-order\noptimality condition for the original global problem:\n\n                  m\n                       1\n         X ∇fi(θ) = 0 ⇐⇒  ∇F(θ) = 0.\n             m\n                             i=1\n\nD.4  ANALYSIS OF THE FORMULATION\n\nD.4.1  PROOF OF NECESSITY FOR THE LINEAR DUAL TERM\n\nTo prove that the linear term ⟨ai, θi−θ⟩is necessary, we analyze the case where it is omitted, relying\nsolely on a quadratic penalty. The objective would be:\n\n                             1           γ\n                                 ˜L = X fi(θi) + X ∥θi −θ∥2.\n                m           2\n                                                         i                     i\n\nThe first-order condition with respect to θi for this objective is:\n\n                             1\n                           m∇fi(θi) + γ(θi −θ) = 0.\nAt a point of consensus where θi = θ for all i, the penalty term vanishes, and the condition strin-\ngently requires that:\n                      1\n                    m∇fi(θ) = 0  =⇒   ∇fi(θ) = 0,   ∀i.\nThis is a significantly stronger condition than global optimality, as it requires the solution θ to be a\nstationary point for every client’s objective function simultaneously. Such a point is generally non-\nexistent for heterogeneous data distributions where local minima differ. Therefore, the inclusion\nof the linear dual term is mathematically essential to relax this condition to the correct global one,\nPi ∇fi(θ) = 0.\n\nD.4.2  INTERPRETATION OF THE DUAL VARIABLES AT CONVERGENCE\n\nIn iterative methods that solve for a saddle point of L, the dual variables are typically updated via\ndual ascent:\n                              at+1i  = ati + γ(θt+1i   −θt+1).                                 (9)\nIf the algorithm converges to a primally feasible solution θ⋆, then limt→∞(θt+1i  −θt+1) = 0. At\nthis limit, the stationarity condition from Eq. equation 7 must hold. As θi →θ⋆and θ →θ⋆, the\nequation implies that the dual variables converge to a fixed point a⋆i :\n\n             1              + a⋆i + γ(θ⋆−θ⋆) = 0  =⇒   a⋆i = −1            m∇fi(θ⋆)                                  m∇fi(θ⋆).\nThis result provides a clear interpretation of the dual variable at the optimal solution: a⋆i is precisely\nthe negative of the i-th client’s scaled local gradient at the global optimum. The condition Pi a⋆i = 0\n(from Eq. equation 8 at convergence) then mathematically guarantees that Pi ∇fi(θ⋆) = 0. The\ndual variables are thus the mechanism that allows local gradients to be non-zero while ensuring their\nsum is zero.\n\nE  OPTIMIZED PROMPTS FROM FEDPOB AND FEDPOB-PREF\n\nIn this section, we present the optimized prompts together with their validation-set scores obtained\nby our FedPOB and FedPOB-Pref across all 53 tasks in both the Instruction Induction and\nBBH datasets after 50 optimization rounds. For each task in the tables, the upper row reports the\n\n\n                                       21\n\nPreprint.\n\n\n\n\nprompt and score optimized by FedPOB, while the lower row corresponds to those optimized by\nFedPOB-Pref.\n\n         Table 6: Optimized prompts and their scores for the Instruction Induction tasks\n\n\n  Task                    Prompt                                                                        Score\n\n                                Rewrite the sentence passively.                                                        0.972\n   active to Passive                           The sentence should be changed to passive voice: “The sentence is to be changed\n                                                                                                                   0.993\n                             from active to passive voice.”\n\n                              change the prefix of the word to make it have the opposite meaning.                       0.288\n  antonyms\n                                    find the opposite of each given word.                                                  0.293\n\n                                 provide an appropriate category for each group of items.                                 0.828\n   auto categorization\n                                     identify the category or group that each set of inputs belong to.                           0.840\n\n                                 provide a connection between two seemingly unrelated words or phrases.                  0.208\n  common concept\n                                 provide a connection between two seemingly unrelated items.                            0.250\n\n                              change the prefix of the word to make it have the opposite meaning.                       1.000\n   diff                               Find the disparity between the initial number and the subsequent number in every\n                                                                                                                   1.000\n                                     input.\n\n                               Return the initial letter of every word provided as input.                                  1.000\n    first word letter                                   State the initial letter of the specified word.                                             1.000\n\n                                  rephrase the given sentences, not just provide synonyms. Here are the revised sen-\n                                    tences: Input: Can you complete all of these tasks?  Output: Are you capable of\n                                completing all of these tasks?  Input:  It is not advisable to take any action at this\n                                   time. Output:  It is not recommended to do anything right now. Input:  I’ll see you        0.570\n   informal to formal                                       this evening. Output: I anticipate seeing you tonight. Input: Would you like me to\n                            accompany you? Output: Do you want me to go along with you? Input: The entire\n                                    narrative was fabricated. Output: The entire story was created.\n                                  rephrase the sentences using different words or phrases with the same meaning.             0.607\n\n                               choose the animal with the larger size or more strength.                                  0.989\n   larger animal\n                               choose the larger animal in each pair.                                                  1.000\n\n                         Add a space between each letter within a word.                                         1.000\n   letters list                         Show each individual letter of the given word with a space between each letter.             1.000\n\n                              change the sentences to negative form, indicating that the statements are false.              0.920\n   negation\n                              change the statements to the opposite meaning.                                         0.947\n\n                                 Create a program that translates a provided number into its equivalent word form.           1.000\n  num to verbal                               Write out the number in words from one to nine thousand, nine hundred and ninety-\n                                                                                                                   1.000\n                                    nine.\n\n                                count the total number of animals/items mentioned in the input sentence.                   0.588\n   object counting\n                                count the number of items listed in the input.                                           0.660\n\n                               Find the word that is not the same as the others in the group.                              0.900\n  odd one out                                   Select the word that is not related to the rest.                                            1.000\n\n                                     identify and output the word that starts with the specified letter.                           0.832\n   orthography start with\n                                     identify the word in the sentence that starts with the given letter.                          0.907\n\n                              Give the names of the elements that match the provided atomic numbers.                   1.000\n   periodic element\n                                    List the names of the elements corresponding to the provided atomic numbers.              1.000\n\n                                    find a word that rhymes with the given word, so in the case of ”buy”, the output would\n                                                                                                                   0.844\n  rhymes                     be ”buy” as it already rhymes with itself.\n                              change the first letter of the word to make a new word.                                   0.993\n\n                                  Retrieve the second letter from the given word.                                          0.972\n  second word letter                                    Print the second-to-last letter of the input word.                                         0.980\n\n                                determine the likelihood that the two sentences are talking about the same topic. The\n                                  outputs provided are the level of certainty in the similarity of the topics discussed in        0.448\n   sentence similarity\n                                   the sentences.\n                             compare the similarity between two sentences using a scale from 0 to 5, with 0 being\n                                     ”definitely not ” similar and 5 being ”perfectly ” similar. The output provided for\n                                                                                                                   0.613\n                               each pair of sentences indicates the level of similarity between them based on the\n                                comparison.\n\n                                     classify the input as either positive or negative based on the given statement.                0.972\n   sentiment                                 provide an output (positive or negative) based on the given input.                          1.000\n\n                                     pluralize the given input words.                                                       1.000\n   singular to plural\n                             add the letter ”s” to the end of the word.                                                1.000\n\n                                  Calculate the total by adding the two numbers given as input.                             1.000\n  sum\n                          sum the two inputted numbers.                                                        1.000\n\n                                 provide alternative words for the given inputs.                                          0.384  synonyms\n                                                                         Continued on next page\n\n\n                                       22\n\nPreprint.\n\n\n\n\n         Table 6: Optimized prompts and their scores for the Instruction Induction tasks\n\n\n  Task                    Prompt                                                                        Score\n\n                                 provide an antonym, synonym, or rhyme for the given word.                              0.500\n\n                                              list the animals from the input words.                                                  0.972\n  taxonomy animal\n                                    List the animals from the given words.                                                 1.000\n\n                                   Translate the specified words from English into German.                                 0.868\n   translation en-de                ¨Ubersetze die gegebenen englischen W¨orter ins Deutsche.                                0.887\n\n                                  traduce cada palabra al espa˜nol.                                                       0.728\n   translation en-es                               Convert the following words from English to Spanish: 1. wardrobe - armario 2. care\n                                                                                                                   0.807\n                                           - preocuparse 3. dissatisfaction - insatisfacci´on 4. pond - estanque 5. trial - prueba\n\n                                      translate the words provided from the English language to French.                         0.948\n   translation en-fr                                   turn the words into French.                                                           0.960\n\n                                determine if the word is used in the same context in both sentences. In this case, the\n                            word ”academy” is used in different contexts in the two sentences, so the output is        0.608\n  word in context                                 ”not the same.”\n                                determine if the two sentences provided have the same meaning based on the given\n                                                                                                                   0.700\n                               word.\n\n                                      sort the words in the provided list in alphabetical order. Each output should be a single\n                                                                                                                   0.828\n  word sorting                     line of the sorted words, separated by spaces.\n                                  rearrange the words in the list in alphabetical order.                                      0.867\n\n                               Solve the jumbled words provided.                                                    0.720\n  word unscrambling\n                              Arrange the scrambled words in the correct order.                                       0.793\n\n\n\n                Table 7: Optimized prompts and their scores for the BBH tasks\n\n\n  Task                    Prompt                                                                        Score\n\n                               Assess the provided logical expressions and produce the result.                           0.844\n  boolean expressions\n                               Assess the provided logical expressions and give the resulting output.                      0.860\n\n                                determine the date a specific number of days or years ago from a given date.                0.572\n   date understanding\n                                determine the date one week ago or one week from today based on the given informa-\n                                                                                                                   0.613\n                                       tion.\n\n                                     identify the antecedent of the pronoun in each sentence or state if it is ambiguous.\n                           The correct antecedent for each sentence is as follows: ’1. (C) Ambiguous 2. (B) The\n                                                                                                                   0.840\n   disambiguation qa               office was Sam’s office 3. (A) The technician completed the repair 4. (A) Alex could\n                                 not meet 5. (B) Asked the cleaner\n                                  explain the antecedent of the pronoun in the given sentences or state if it is ambiguous.\n                                                                                                                   0.793\n                           The correct antecedent for each sentence is provided in the output.\n\n                                  Finish the remaining part of the series and ensure that all parentheses are closed cor-\n                                                                                                                   0.680\n  dyck languages                    rectly.\n                               Continue the sequence, ensuring that all parentheses are closed correctly.                  0.740\n\n                                determine if the argument, given the explicitly stated premises, is deductively valid or\n                                                                                                                   0.812\n   formal fallacies                  invalid. The output for all the provided inputs is ”invalid.”\n                                determine whether the arguments, given the explicitly stated premises, are deductively\n                                                                                                                   1.000\n                                    valid or invalid.\n\n                                    Identify the geometric shape represented by the given SVG path element, with the\n                                                                                                                   0.448\n   geometric shapes              provided outputs indicating the corresponding shape based on the paths.\n                              Determine the shape illustrated by the given SVG path element.                           0.487\n\n                               choose the sentence with the correct adjective order, which is the order of opinion,\n                                                                                                                   0.948\n   hyperbaton                        size, age, shape, color, origin, material, and purpose.\n                               choose the sentence with the correct adjective order.                                     0.973\n\n                                determine which object is in a specific position in the given set of objects based on the\n   logical deduction five                                                                                             0.476\n                                 information provided in each paragraph.\n   objects\n                                determine which object finished first in each scenario. The correct outputs are: 1. (C)\n                          Ada finished first 2. (E) The falcon is the third from the left 3. (E) Amy finished first\n                                                                                                                   0.473\n                                      4. (D) The plums are the second-cheapest 5. (D) The orange book is the third from\n                                   the left.\n\n                                determine which object is in a specific position in the set of seven objects based on\n   logical deduction seven                                                                                           0.488\n                                   the given statements.\n   objects\n                                determine which object is in a specific position in the given arrangement of objects.         0.540\n\n                                determine which object is in a specific position based on the given information. In\n   logical deduction three         each case, the correct output is provided based on the logical consistency of the state-        0.644\n   objects                     ments within the paragraph.\n                                determine which object is in the leftmost position based on the given information.           0.653\n\n                                                                         Continued on next page\n\n\n                                       23\n\nPreprint.\n\n\n\n\n                Table 7: Optimized prompts and their scores for the BBH tasks\n\n\n  Task                    Prompt                                                                        Score\n\n                                    find a movie similar to the given list of movies. The correct options are selected based\n                                                                                                                   0.732\n  movie recommendation       on the similarity to the movies listed in the input.\n                                    find a movie similar to a given list of movies. The correct option for each set of movies\n                                           is as follows: 1. (C) The Usual Suspects 2. (D) Fargo 3. (A) Pulp Fiction 4. (B) The        0.780\n                               Matrix 5. (A) Schindler’s List\n\n                               Find the difference between the first set of parentheses and the second set, and then\n                                                                                                                   0.692\n   multistep arithmetic two        simplify the expression.\n                                determine the outcome of the provided mathematical equation.                            0.700\n\n                              ”Turn right. Take 10 steps. Turn around. Take 10 steps.”                                 0.716\n   navigate\n                                  take 9 steps left, then 10 steps forward, then 9 steps right, and finally 10 steps back-\n                                ward. By following these instructions, you would return to the starting point, so the        0.773\n                                 output is Yes.\n\n                                determine specific information based on the given table of penguins and provide the\n                                                                                                                   0.605\n  penguins in a table              correct answer from the options provided.\n                                determine specific information about the penguins based on the given data and answer\n                                                                                                                   0.586\n                                   the questions accordingly.\n\n   reasoning about colored        determine the color or quantity of items based on their arrangement in a row.               0.568\n   objects                       determine the color of the item directly to the right of a specified color in a given\n                                                                                                                   0.580\n                               arrangement of items.\n\n                                     identify the humorous edit of the artist or movie name, and the correct answer for each\n                                                                                                                   0.724\n   ruin names                     input is provided in the output.\n                                    find the humorous edit of the artist or movie name.                                      0.813\n\n   salient translation error        Find the mistake in the given translations.                                              0.600\n   detection                     Find the mistake in the German to English translations given.                             0.613\n\n                                     identify the sarcastic statement from the given options. The selected statement typ-\n                                      ically conveys an opposite meaning or is exaggerated in a way that highlights the        0.782\n   snarks                                  absurdity of the situation.\n                                     identify the sarcastic statement from the given options.  In each case, the sarcastic\n                                 statement is one that implies the opposite of what it literally says, often highlighting        0.793\n                                  absurdity or exaggeration.\n\n                                determine if the sentences were plausible based on common sports terminology.            0.564\n   sports understanding\n                                determine if the sentences provided are plausible in a sports context.                       0.580\n\n                                determine between what times the person could have gone to the specified location\n                               based on the given information about their activities throughout the day. The correct        0.652\n   temporal sequence\n                                time range is then provided as the output.\n                                determine between what times the person could have gone to a specific location based\n                            on the given information. The correct options for each scenario are as follows: 1.\n                                                                                                                   0.700\n                             David could have gone to the construction site between 8am to 12pm (Option A). 2.\n                                   Leslie could have gone to the market between 11am to 5pm (Option B).\n\n                                determine who Claire is dancing with at the end of the dance. In the given scenario,\n   tracking shuffled objects                                                                                           0.328\n                                       at the end of the dance, Claire is dancing with option (B) Sam.\n   five objects\n                                determine who ends up with a specific item or partner after a series of swaps or trades.        0.353\n\n                                determine the final position/book/ball of a specific person/player after a series of\n   tracking shuffled objects                                                                                           0.256\n                                swaps.\n   seven objects\n                                determine the final partner, gift, ball, or book that a specific person has at the end of\n                                                                                                                   0.293\n                                   the given scenario.\n\n   tracking shuffled objects       determine the final position or item that Bob ends up with after a series of swaps.           0.400\n   three objects                  determine who ends up with a specific item after a series of swaps in a white elephant\n                                                                                                                   0.433\n                                         gift exchange.\n\n                                determine if Inga tells the truth based on the statements given by the other individuals.\n                                  In this case, the answer is ”No” because Inga says Fidel tells the truth, but Fidel says\n                                                                                                                   0.636\n  web of lies                     Vernell lies. Since there is a contradiction in the statements, Inga does not tell the\n                                        truth.\n                                determine if Christie tells the truth based on the statements of the other individuals.\n                                    Christie says that Teressa tells the truth. Since Teressa says that Leda lies, and Leda\n                                                                                                                   0.667\n                                 says that Shaunda lies, and Shaunda says that Ryan tells the truth, we can conclude\n                                      that Christie is telling the truth.\n\n\n\n\n\n                                       24",
"headers": [
"arXiv:2509.24701v1  [cs.LG]  29 Sep 2025",
"F",
"POB",
": S",
"-E",
"P",
"O",
"B",
"E D",
"AMPLE",
"FFICIENT",
"EDERATED",
"ROMPT",
"PTIMIZATION VIA",
"ANDITS",
"A",
"1",
"I",
"2",
"S",
"3",
"4",
"E",
"5",
"6",
"R",
"W",
"7",
"C",
"M",
"D",
"L",
"POB-P",
"Preprint.",
"Pingchen Lu",
", Zhi Hong",
", Zhiwei Shang",
", Zhiyong Wang",
",",
"Yikun Ban",
", Yao Shu",
", Min Zhang",
", Shuang Qiu",
", Zhongxiang Dai",
"The Chinese University of Hong Kong, Shenzhen,",
"South China University of Technology,",
"University of Edinburgh,",
"Beihang University,",
"The Hong Kong University of Science and Technology (Guangzhou),",
"East China Normal University,",
"City University of Hong Kong",
"The performance of large language models (LLMs) is highly sensitive to the input",
"prompt, making prompt optimization a critical task. However, real-world appli-",
"cation is hindered by three major challenges: (1) the black-box nature of pow-",
"erful proprietary LLMs, (2) the need for high sample efficiency due to query",
"costs, and (3) the desire for privacy-preserving collaboration among multiple",
"users. To address these challenges simultaneously, we introduce a novel frame-",
"work for sample-efficient federated prompt optimization based on multi-armed",
"bandits (MABs). The MAB framework is uniquely suited for this problem as it is",
"(1) inherently a black-box optimization method, (2) practically sample-efficient,",
"and (3) enables collaborative learning with theoretically guaranteed benefit from",
"more participating agents. We first propose the",
"Federated Prompt Optimization",
"via Bandits",
"(",
"FedPOB",
") algorithm, a federated variant of the Linear UCB algo-",
"rithm, where agents collaborate by sharing model parameters instead of raw data.",
"We then extend our approach to the practical setting of comparative user feedback",
"by introducing",
"with Preference Feedback",
"FedPOB-Pref",
"), an efficient",
"algorithm based on federated dueling bandits. Extensive experiments demonstrate",
"that both",
"and",
"significantly outperform existing base-",
"lines and that their performance consistently improves as more agents participate",
"in the collaboration, validating the effectiveness of our federated approach.",
"Large language models (LLMs) have achieved impressive performance in a variety of real-world",
"applications (Guo et al., 2025). However, the performance of LLMs has been shown to be highly",
"sensitive to the input",
"prompt",
"(Zhou et al., 2023; Lin et al., 2024b). Consequently,",
"prompt optimiza-",
"tion",
", in which we aim to find the best prompt for a task, has emerged as a critical research area.",
"Despite its growing popularity, the widespread real-world adoption of prompt optimization is still",
"hindered by three important challenges.",
"The first challenge is",
"black-box access",
". Some of the most powerful LLMs, such as ChatGPT and",
"Gemini (OpenAI, 2023b; Team et al., 2023), are proprietary, black-box models that are only acces-",
"sible via API queries. This limited access creates an immense challenge to prompt optimization.",
"The second challenge is",
"sample efficiency",
". Since querying powerful LLMs is often costly in both",
"time and financial resources, it is of paramount importance to develop methods that can identify",
"the optimal prompt for a given task using a small number of interactions. The third challenge is",
"enabling",
"collaboration",
"among multiple users. As LLMs become more widely adopted, a natural",
"and important question arises: how can multiple users, each with their own prompt optimization",
"tasks, collaborate to accelerate their progress? A key constraint in such a collaborative setting is",
"user privacy, as participants are typically unwilling to share their proprietary data, such as the his-",
"tory of tested prompts and their corresponding performance scores. This scenario naturally aligns",
"Figure 1: An overview of our proposed federated prompt optimization frameworks.",
"han-",
"dles direct score feedback, while",
"is designed for pairwise preference feedback.",
"with the principles of",
"federated learning",
"(FL) (Kairouz et al., 2019; McMahan et al., 2017), where",
"distributed agents collaborate on their machine learning tasks without exposing their raw data.",
"To tackle the combined challenges of black-box access, sample efficiency and privacy-preserving",
"collaboration, we propose a new class of federated prompt optimization algorithms built upon the",
"multi-armed bandit",
"(MAB) framework (Lattimore & Szepesv´ari, 2020). MABs are exceptionally",
"well-suited for this problem for three main reasons. First, MAB algorithms do not require gradient",
"information and are inherently",
"black-box optimization methods",
". Second, they are designed to",
"efficiently balance the exploration-exploitation trade-off, enabling them to solve complex black-box",
"optimization problems in a",
"sample-efficient",
"manner, a property that has been successfully leveraged",
"in recent work on prompt optimization (Lin et al., 2024b; Wu et al., 2024). Thirdly, federated MAB",
"algorithms (Shi & Shen, 2021; Dubey & Pentland, 2020; Dai et al., 2023) provide strong theoretical",
"guarantees, ensuring that",
"performance improves as more agents participate in the collaboration",
"(Wang et al., 2020).",
"Our first contribution is the",
"Federated Prompt Optimization via Bandits",
") algorithm. This",
"method is based on a federated variant of the classic Linear Upper Confidence Bound (LinUCB)",
"algorithm (Abbasi-Yadkori et al., 2011; Wang et al., 2020). In our",
"algorithm, each agent",
"utilizes a pre-trained embedding model to represent the prompts and a linear model to predict their",
"performance. Collaboration is achieved by having agents periodically exchange and aggregate their",
"LinUCB parameters, thereby learning from the collective experience of all agents without requiring",
"them to share any sensitive raw data. Importantly, thanks to the solid theoretical guarantees of the",
"federated LinUCB algorithm (Wang et al., 2020), the performance of our",
"algorithm is",
"theoretically guaranteed to improve with a larger number of collaborating agents.",
"In addition, we consider the highly practical setting of prompt optimization with",
"preference feed-",
"back",
", where explicit performance scores are unavailable and we are only able to observe relative",
"preference feedback (e.g., the user prefers the response from prompt A than that from prompt B).",
"This problem was recently introduced by Lin et al. (2024a) to address scenarios where user feedback",
"is inherently comparative. To enable sample-efficient federated prompt optimization in this novel",
"setting, we introduce our second algorithm,",
").",
"This algorithm is a practical adaptation and modification of the federated linear dueling bandit",
"framework proposed by Huang et al. (2025). Specifically, our",
"algorithm signifi-",
"cantly reduces the communication complexity of the methods from Huang et al. (2025) while main-",
"taining the strong empirical performance. An overview of both",
"is",
"illustrated in Fig. 1.",
"We conduct extensive experiments to validate our proposed methods. The results demonstrate that",
"both",
"achieve considerably better performance than the previous base-",
"line methods in various tasks. Furthermore, we empirically verify that the performance of our al-",
"gorithms consistently improves as the number of participating agents increases, highlighting the",
"benefits of our collaborative approach. In summary, our key contributions are as follows:",
"•",
"We propose",
", a novel algorithm for sample-efficient federated prompt optimization that",
"enables multiple agents to collaborate on finding the best prompts without sharing their raw data.",
"We extend our algorithm to the practical setting of preference-based feedback by introducing the",
"algorithm, which is based on federated linear dueling bandits.",
"We conduct extensive experiments to validate our approach, demonstrating that our algorithms",
"significantly outperform existing baselines and scale effectively with more agents.",
"Prompt Optimization.",
"We address the problem of black-box prompt optimization, where the ob-",
"jective is to find an optimal prompt",
"p",
"that maximizes the performance of a black-box LLM on a",
"given task",
"= (",
"X",
"Y",
")",
". The task consists of a set of queries",
"=",
"{",
"x",
"}",
"and their corresponding",
"ground-truth answers",
"y",
". Since the internal parameters of the black-box LLMs (e.g., GPT-",
"4o-mini) are inaccessible and only API queries are allowed, we model the performance of the LLM",
"via an external score function. Specifically, we define",
"s",
"|",
") =",
"h",
"m",
"(LLM(",
"p, x",
", y",
"i",
"(1)",
"in which",
"is a metric function that compares the model response",
"LLM(",
"induced by the prompt",
"with the ground-truth answer",
"and provides a score",
". The optimization target is then",
"formulated as",
"= arg max",
"(2)",
"where",
"denotes the space of all possible prompts.",
"Federated Prompt Optimization.",
"We extend the black-box prompt optimization problem to the",
"federated setting, which involves multiple agents. We consider a scenario with a set of",
"N >",
"agents, denoted by",
", who all aim to solve the same task",
". To account for agent heterogeneity, we",
"allow each agent",
"a",
"∈",
"to have its own prompt space denoted as",
". This increases the generality of",
"our setting by allowing each user to define a prompt space uniquely suited to their own preferences.",
"Furthermore, each agent can generate its local prompt space",
"using existing techniques (Zhou",
"et al., 2023). As a result, the federated prompt optimization problem can be expressed as follows:",
", x",
"∀",
"(3)",
"Here, each agent",
"aims to find the optimal prompt",
"from its own prompt space",
"that maxi-",
"mizes its performance on the task",
". To achieve greater sample efficiency, all agents in",
"collaborate",
"without sharing their raw data (i.e., the history of tested prompts and their scores). This problem",
"formulation naturally aligns with common paradigms in the federated bandit literature (Wang et al.,",
"2020; Dai et al., 2023). Therefore, we adopt the federated bandit framework to tackle this problem.",
"Feedback Model.",
"To solve the federated black-box prompt optimization problem, we cast the opti-",
"mization process into an iterative protocol, where we sequentially select candidate prompts for eval-",
"uation. At each round",
"t",
", each agent",
"selects one or two candidate prompts and receives feedback.",
"The selection of the prompts is guided by theoretically principled bandit policies, which leverage the",
"collective observation history from all agents to achieve sample-efficient optimization (more details",
"in Sec. 3). Depending on the type of feedback available, we consider two settings:",
"Score feedback:",
"In this setting, each agent selects a single prompt",
"at each round",
", and",
"receives a numeric score",
"ˆ",
"as feedback, which directly reflects the performance of the prompt",
"on task",
". Specifically, given a validation set",
"representing the task",
", the score can be",
"obtained as follows:",
".",
"Preference feedback:",
"In this setting, every agent",
"selects a pair of prompts",
", p",
"at round",
", and observes a binary signal indicating which of the two performs better, i.e., which prompt",
"yielded the better response. For example, such feedback may be directly provided by human",
"evaluators (Lin et al., 2024a). Following the common practice from dueling bandits (Bengs et al.,",
"Algorithm 1",
"(Agent",
"Initialize:",
"0",
"V",
"λI",
"b",
"= 0",
"for",
"= 1",
", . . . , T",
"do",
"Compute",
"←",
"+",
"Update local model",
"θ",
"Select prompt",
"arg max",
"⟨",
", u",
"⟩",
"ν",
"||",
"u",
"Query",
"to observe score feedback",
"Update",
", b",
"if",
"−",
"·",
"log(det",
"/",
"det",
"> D",
"then",
"Send a communication request to the central server",
"a communication round is started",
"Upload",
"to the central server. Reset",
"Receive",
"from server",
"Algorithm 2",
"(Central Server)",
"Central server receives a communication request from",
"any agent",
"Initiate a communication round",
"receive",
"from each agent",
"Broadcast",
"to all agents",
"2022), we assume that the preference feedback is generated by the Bradley–Terry–Luce (BTL)",
"model (Hunter, 2004).",
"We adopt",
"linear models",
", rather than more complex ones such as neural networks, to learn the",
"unknown reward function for federated prompt optimization.",
"Accordingly, our",
"algorithms (illustrated in Fig. 1) are based on linear bandits (Abbasi-Yadkori et al.,",
"2011) and linear dueling bandits (Bengs et al., 2022), respectively. This choice is motivated by the",
"balance linear models offer between expressiveness, simplicity, and theoretical guarantees: (1) Mod-",
"ern text embedding techniques powered by transformers are sufficiently mature and effective (Shi",
"et al., 2024; Hu et al., 2024), enabling a simple linear function to model the relationship between",
"prompts and scores. (2) Linear models enable lightweight algorithmic designs. (3) Unlike feder-",
"ated neural bandits using neural networks for reward estimation (Dai et al., 2023), federated linear",
"bandit methods provide theoretical guarantees on collaboration which ensure that",
"the performance",
"improves as more agents join the federation",
"3.1",
"T",
"Following recent works on black-box prompt optimization (Shi et al., 2024; Hu et al., 2024), we first",
"map each discrete prompt",
"into a continuous embedding vector",
"U",
"using a pre-trained model.",
"This allows us to leverage rich semantic representations and simplifies the optimization problem. We",
"then model the score of a prompt for each agent",
"using a linear model:",
", which is",
"standard in the multi-armed bandit literature (Abbasi-Yadkori et al., 2011).",
"Local Prompt Selection.",
"At the beginning of each round",
", in lines 3-4 of Algo. 1, each agent",
"first updates its information matrix",
"and estimated linear parameters",
"using (1)",
"the aggregated",
"information from all agents",
"received from the central server (i.e.,",
", more details below)",
"and (2) its newly collected local information (i.e.,",
"). Next, using the parameters",
", agent",
"selects the next prompt to query following the Upper Confidence Bound (UCB)",
"strategy (line 5 of Algo. 1):",
"(4)",
"Here the parameter",
"balances",
"exploitation",
"(choosing prompts with large predicted rewards) and",
"exploration",
"(choosing prompts with large uncertainty). Next, we test the selected prompt",
"using",
"Algorithm 3",
"∼N",
", σ",
"with small",
"σ",
"Select first prompt",
"Select second prompt",
"β",
"to observe preference feedback",
"ω",
"≻",
"arg min",
"−⟨∇",
", θ",
"∇",
"←∇",
"λ",
"= [",
")][",
")]",
", W",
"to server",
"Algorithm 4",
"Update server model",
"(ˆ",
"the validation set",
", to obtain score feedback",
"(line 6 of Algo. 1). Then, we update the newly",
"collected local information",
"(line 7 of Algo. 1).",
"Agent-Server Communication.",
"To reduce the communication cost, we only start a communication",
"round when the new information collected by any agent exceeds a threshold",
", i.e., when the cri-",
"terion in line 8 of Algo. 1 is satisfied. If a communication request is sent by any agent, the trusted",
"central server initiates a communication round (line 1-2 of Algo. 2) and all agents upload their local",
"parameters",
"to the central server (lines 10-11). The central server then aggregates",
"these local parameters to produce synchronized parameters",
"(line 3-4 of Algo. 2),",
"which are then broadcast to all agents. After the agents receive the aggregated parameters",
", they can use them to select the prompt in the next iteration, and the algorithm repeats.",
"3.2",
": P",
"In many practical applications, obtaining explicit numerical scores is challenging, whereas collecting",
"pairwise preference feedback is often more natural and cost-effective. For instance, in human-in-",
"the-loop scenarios, users can more reliably state a preference between two generated outputs than",
"assign them absolute scores (Yue et al., 2012; Lin et al., 2024a). This setting, however, introduces",
"a significant technical hurdle:",
"the parameter estimation for linear dueling bandits does not have a",
"closed-form solution",
"(Bengs et al., 2022). This limitation prevents the use of the simple parameter",
"aggregation strategy employed by our",
"algorithm.",
"The absence of a closed-form solution naturally leads to gradient-based optimization approaches.",
"Recent work by Huang et al. (2025) introduced federated linear dueling bandit algorithms (FLDB-",
"GD and FLDB-OGD) that achieve collaboration by aggregating local gradients. While theoreti-",
"cally sound, these methods face a practical dilemma: FLDB-GD incurs high communication costs,",
"whereas the more communication-efficient FLDB-OGD suffers significant performance degrada-",
"tion. We attribute this to the fact that",
"preference feedback is inherently noisier and less informative",
"than numerical scores",
", making it particularly challenging to achieve both competitive performance",
"and communication efficiency. To overcome this, we draw inspiration from",
"classical federated",
"learning",
"for solving supervised learning problems (McMahan et al., 2017). Specifically, instead of",
"aggregating gradients, we aggregate model parameters, which allows us to adopt a dynamic regu-",
"larization technique that has proven effective in federated learning (Acar et al., 2021) for further",
"performance improvement. This leads to our proposed",
"algorithm (Algos. 3 and 4).",
"Our",
"algorithm offers several key advantages: (1) it is highly",
", ca-",
"pable of learning the underlying reward model from a small number of preference queries; (2) it is",
"robust to",
"agent heterogeneity",
", and its performance scales effectively with the number of collabo-",
"rating agents; and (3) when compared to the baselines from Huang et al. (2025),",
"si-",
"multaneously",
"reduces communication costs and improves performance",
"(Sec. 4.2).",
"The overall workflow of",
"is outlined in Algorithms 3 and 4. At each round",
", every",
"agent",
"selects a pair of prompts based on the global model",
". The first prompt,",
", represents",
"pure",
"(line 3), while the second,",
", incorporates an",
"bonus to discover",
"more informative options (line 4). This dueling selection strategy is grounded in the theory of",
"dueling bandits (Bengs et al., 2022; Verma et al., 2024). We then obtain binary preference feedback",
"for this pair of selected prompts (line 5). The core of our method lies in the local",
"model update (line 6), which optimizes an objective that combines the standard logistic loss with a",
"dynamic regularizer (Acar et al., 2021). The first component is the pairwise logistic loss over the",
"agent’s local history:",
"\u0010",
"log",
"\u0000",
"\u0002",
"\u0003\u0001",
"+(1",
") log",
"\u0011",
"(5)",
"This term is the negative log-likelihood of the observed preferences under the BTL model (Bengs",
"et al., 2022). The second component is a dynamic regularization term consisting of (i) a linear",
"penalty,",
", which corrects for local gradient drift, and (ii) a quadratic penalty,",
"which prevents the local model from deviating excessively from the previous global model (Acar",
"et al., 2021).",
"After this local update (lines 6-8), agents upload their new parameters to the",
"central server for aggregation, which then broadcasts the aggregated global parameters for the",
"next round. Of note, we conduct theoretical analysis to motivate the local objective function of",
"(App. D), providing theoretical justification for its strong performance (Sec. 4.2).",
"We adopt MPNet (Song et al., 2020) as the text embedding model, and use GPT-3.5-turbo (OpenAI,",
"2023a) in the experiments unless specified otherwise. Of note, we also test two other models, GPT-",
"4o-mini (OpenAI, 2023b) and Qwen3-235B-A22B-2507 (Bai et al., 2023), in Sec. 5. Evaluation is",
"performed on the Instruction Induction (Chen et al., 2023; Lin et al., 2024b) and BIG-Bench Hard",
"datasets (Suzgun et al., 2023), which collectively cover over 50 tasks that span diverse areas such",
"as reasoning, language comprehension, and code generation. To account for agent heterogeneity,",
"we ensure that the prompt domains of all agents contain both shared prompts and unique prompts.",
"For fair comparisons, we ensure an equal validation query budget across all algorithms and ana-",
"lyze the corresponding communication costs in the federated setting. We defer more details on the",
"experimental setting to App. B.",
"4.1",
":",
"In the setting with score-based feedback, every tested prompt receives a numerical score indicating",
"the quality of its induced response. Here we assess performance of a prompt using a validation set",
"and adopt the validation accuracy as the corresponding score. The objective is to identify the optimal",
"prompt (i.e., the one that achieves the highest validation score). We compare our",
"with a",
"representative baseline method on federated prompt optimization: FedOne (Wang et al., 2025), as",
"well as two other baselines on standard prompt optimization: INSTINCT (Lin et al., 2024b) and",
"PromptBreeder (Fernando et al., 2024).",
"Table 1 and 2 report the final scores achieved by the best prompt discovered by each algorithm",
"in various tasks. The results demonstrate the superior capability of our",
", which achieves",
"the highest score on the majority of the tasks under the setting of ten agents. Fig. 2 depicts the",
"performance of",
"across different iterations, where we observe a positive correlation be-",
"tween the number of agents and the achieved prompt score, highlighting the benefits of multi-agent",
"collaboration. In addition,",
"achieves a near-optimal score with a small batch of samples,",
"demonstrating its sample efficiency.",
"Table 1: Average validation accuracy (with standard error) of the best prompt found by each al-",
"gorithm in the",
"Instruction Induction dataset",
", averaged over 5 independent trials with different",
"random seeds. For clarity, only a representative subset of challenging tasks. The complete results",
"for all tasks are provided in Table 5 (App. C.3) and the results are consistent.",
"Table 2: Performance on the",
"Big-Bench Hard (BBH) dataset",
"under the same experimental settings.",
"4.2",
"To simulate user preference feedback in our experiments, we adopt the protocol from Lin et al.",
"(2024a). For any pair of prompts",
", we first compute their ground-truth scores,",
", on a validation set. The preference probability is then determined by the Bradley-",
"Terry-Luce (BTL) model (Hunter, 2004):",
"))",
", where",
"the sigmoid function. A binary preference outcome",
"is then sampled from a",
"Bernoulli distribution with this probability. We compare",
"against federated baselines",
"FLDB-GD and FLDB-OGD (Huang et al., 2025), as well as standard prompt optimization methods",
"APOHF (Lin et al., 2024a) and DoubleTS (Dwaracherla et al., 2024).",
"Figure 2: Performance of",
"with varying",
"numbers of agents.",
"Figure 3: Performance of",
"with",
"varying numbers of agents.",
"The results, summarized in Table 3, demonstrate that",
"consistently achieves the",
"best performance across different numbers of agents. Our method establishes a superior trade-off",
"between performance and communication cost. Specifically,",
"matches the commu-",
"nication efficiency of FLDB-OGD while delivering substantially better results. Conversely, while",
"FLDB-GD obtains the second-best performance, it does so at a considerably higher communication",
"cost. Fig. 3 further highlights that the sample efficiency of",
"improves as more agents",
"collaborate. Additional results are available in Fig. 10 (App. C.2).",
"Table 3: Score and number of communication rounds under",
"preference feedback",
"Figure 4:",
"Scores of",
"varying communication thresholds",
"Performance vs. Communication in",
"In federated learning, communication is inher-",
"ently costly, making frequent interactions with the central server impractical. Thus, an effective",
"algorithm should maintain strong performance even with infrequent communications. Here we re-",
"duce the interaction frequency by varying the communication threshold",
"in",
"in the range:",
"10",
"100",
"300",
"1000",
". Note that a larger",
"results in less communication rounds. The results in",
"Fig. 4 reveal a clear trade-off between performance (the best score after",
"20",
"iterations) and com-",
"munication, i.e., fewer communication rounds (i.e., larger",
") result in worse performance. More",
"importantly, our",
"still achieves strong performance even with infrequent communications,",
"demonstrating its robustness and practical effectiveness in realistic federated environments.",
"Generalization to Other LLMs.",
"While the response quality of an LLM depends not only on the",
"prompt design but also on the inherent capability of the backbone model, we examine whether",
"the observed performance gains of our algorithms can generalize to other LLMs. To this end,",
"we replace the GPT-3.5-Turbo model used in our main experiments by GPT-4o-mini and Qwen",
"(OpenAI, 2023a;b; Bai et al., 2023), while keeping all other settings fixed. As shown in Fig. 5,",
"our",
"consistently discovers high-score prompts and achieves better performance with a",
"larger number of agents, regardless of the underlying LLM. Additional results on the performance",
"of",
"can be found in App. C.4, which lead to consistent observations.",
"8",
"Figure 5: The performance of",
"using GPT-4o-mini and Qwen.",
"Effectiveness of Dynamic Regularization in",
"We further assess the neces-",
"sity of the dynamic regularization term in",
", which mitigates the dynamic drift",
"among heterogeneous clients and accelerates collaboration.",
"We compare the performance of",
"with and without this term, the latter of which is equivalent to the classical Fe-",
"dAvg algorithm (McMahan et al., 2017)). Fig. 6 shows that incorporating dynamic regularization",
"stabilizes performance, speeds up convergence, and reduces fluctuations caused by inter-agent het-",
"erogeneity. These results highlight its critical role in enabling efficient and robust federated prompt",
"optimization in heterogeneous federated environments.",
"Figure 6: Impact of the dynamic regularization term in",
". FedAvg corresponds to",
"removing this term.",
"Federated Learning enables collaborative model training with-",
"out sharing private data (Kairouz et al., 2019; McMahan et al., 2017). However, applying FL to",
"LLMs faces a critical barrier: the prohibitive cost of communicating updates for models of such",
"massive scale. A natural workaround is to combine FL with parameter-efficient prompt tuning",
"(Zhao et al., 2023; Che et al., 2023; Deng et al., 2024; Wei et al., 2023), where only lightweight soft",
"prompts are trained and communicated. While resource-efficient, this paradigm operates in a white-",
"box setting and thus fails in API-based black-box scenarios. This limitation has motivated research",
"on black-box federated prompt optimization (Lin et al., 2023). Early efforts such as FedBPT (Zhang",
"et al., 2023) adopt soft prompts with gradient-free optimization, but remain incompatible with API-",
"only LLMs. More recent work addresses discrete prompt optimization, e.g., FedOne (Wang et al.,",
"2025), which learns categorical distributions to sample prompts. Despite solving discreteness, these",
"methods suffer from inefficiency and poor semantic quality, leaving open the challenge of develop-",
"ing a query-efficient federated method that produces semantically meaningful discrete prompts for",
"black-box LLMs. We defer a detailed discussion of the related works on standard non-federated",
"prompt optimization to App. A due to space constraint.",
"9",
"In this paper, we introduced",
", novel algorithms for sample-efficient",
"federated prompt optimization. Built upon the theory of federated multi-armed bandits, our meth-",
"ods enable multiple agents to effectively collaborate to find optimal prompts for black-box LLMs",
"without sharing raw data. Extensive experiments demonstrate that our algorithms significantly out-",
"perform existing baselines under both score and preference feedback, with performance consistently",
"improving with an increasing number of participating agents. Notably,",
"establishes",
"a superior performance-to-communication trade-off in the practical preference-based setting.",
"Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic",
"bandits. In",
"Proc. NIPS",
", 2011.",
"Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Paul N. Whatmough Matthew Mattina,",
"and Venkatesh Saligrama. Federated learning based on dynamic regularization. In",
"Proc. ICLR",
"2021.",
"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin",
"Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Dayiheng Liu, Gao Liu,",
"Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi",
"Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng",
"Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi",
"Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang",
"Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report.",
"arXiv preprint",
"arXiv:2309.16609",
", 2023.",
"Viktor Bengs, Aadirupa Saha, and Eyke H¨ullermeier. Stochastic contextual dueling bandits under",
"linear stochastic transitivity models. In",
"Proc. ICML",
", 2022.",
"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-",
"wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,",
"Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.",
"Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,",
"Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,",
"Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In",
"Proc. NeurIPS",
"2020.",
"Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor Sheng, Huaiyu Dai, and Dejing",
"Dou. Federated learning of large language models with parameter-efficient prompt tuning and",
"adaptive optimization. In",
"Proc. EMNLP",
"Lichang Chen, Jiuhai Li, Tiejun Zhang, and Bo Zhou. InstructZero: A preference-based iterative",
"prompt optimization framework. In",
"Zhongxiang Dai, Arun Verma Yao Shu, Flint Xiaofeng Fan, and Bryan Kian Hsiang Low. Federated",
"neural bandits. In",
"Mingkai Deng, Jianyu Wang, Cheng-Ping Zhang, Han Li, Yaliang Chen, Lidong Zhao, Jing Liu,",
"Yang Chen, and Xiang Liu. RLPrompt: Optimizing discrete text prompts with reinforcement",
"learning. In",
"Proc. EMNLP Findings",
"Wenlong Deng, Christos Thrampoulidis, and Xiaoxiao Li. Unlocking the potential of prompt-tuning",
"in bridging generalized and personalized federated learning. In",
"Proc. CVPR",
", 2024.",
"Shizhe Diao, Zhichao Huang, Ruijie Xu, Xuechun Li, Lin Yong, Xiao Zhou, and Tong Zhang.",
"Black-box prompt learning for pre-trained language models.",
"Transactions on Machine Learning",
"Research",
"Abhimanyu Dubey and Alex Pentland. Differentially-private federated linear bandits. In",
"Proc.",
"NeurIPS",
", pp. 6003–6014, 2020.",
"Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, and Benjamin Van Roy. Efficient",
"exploration for LLMs. In",
"Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt¨aschel.",
"Promptbreeder: Self-referential self-improvement via prompt evolution. In",
"Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,",
"Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms",
"via reinforcement learning.",
"arXiv preprint arXiv:2501.12948",
", 2025.",
"Qingyan Guo, Rui Wang, Junzhe Guo, Boyu Li, Kai Song, Xu Tan, Guoqing Liu, Jiang Bian, and",
"Yanyang Yang. Connecting large language models with evolutionary algorithms yields powerful",
"prompt optimizers. In",
"Wenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu, Xiangqiang Lin, Zhongxiang Dai, See-Kiong",
"Ng, and Bryan Kian Hsiang Low. Localized zeroth-order prompt optimization. In",
"2024.",
"Xuhan Huang, Yan Hu, Zhiyan Li, Zhiyong Wang, Benyou Wang, and Zhongxiang Dai. Federated",
"linear dueling bandits.",
"arXiv preprint arXiv:2502.01085",
"David R Hunter. Mm algorithms for generalized bradley-terry models.",
"Annals of Statistics",
", 2004.",
"Gurusha Juneja, Gautam Jajoo, Nagarajan Natarajan, Hua Li, Jian Jiao, and Amit Sharma. Task",
"facet learning: A structured approach to prompt optimization. In",
"Proc. ACL",
"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin",
"Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances",
"and open problems in federated learning. arXiv:1912.04977, 2019.",
"Weize Kong, Spurthi Hombaiah, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky.",
"PRewrite: Prompt rewriting with reinforcement learning. In",
"Proc. ACL Short Papers",
"Tor Lattimore and Csaba Szepesv´ari.",
"Bandit algorithms",
". Cambridge University Press, 2020.",
"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt",
"tuning. In",
", 2021.",
"Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing continuous prompts for generation. In",
"Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang",
"Low. Prompt optimization with human feedback.",
"arXiv preprint arXiv:2405.17346",
", 2024a.",
"Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick",
"Jaillet, and Bryan Kian Hsiang Low. Use your INSTINCT: Instruction optimization using neural",
"bandits coupled with transformers. In",
", 2024b.",
"Zihao Lin, Yitao Zeng, Sicheng Yu, Lue Tao, Yuxin Chen, Wenhao Yu, and Lifu Huang.",
"Efficient federated prompt tuning for black-box large pre-trained models.",
"arXiv:2310.03123",
"Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT",
"Understands, Too. In",
"Yichong Luo, Huaxiu Yao, Feng-Shih Chang, Zhi-Kai Zhang, and Jian-Yun Nie.",
"Black-box",
"prompt optimization: Aligning large language models without model training.",
"arXiv:2311.02646",
"O. Ma˜nas, P. Astolfi, M. Hall, C. Ross, J. Urbanek, A. Williams, A. Agrawal, A. Romero-Soriano,",
"and M. Drozdzal. Improving text-to-image consistency via automatic prompt optimization.",
"arXiv",
"preprint arXiv:2403.17804",
"H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient",
"learning of deep networks from decentralized data. In",
"Proc. AISTATS",
", 2017.",
"11",
"OpenAI. GPT-3.5: Openai language model.",
"https://platform.openai.com/",
", 2023a. Ac-",
"cessed: 2025-09-24.",
"OpenAI. GPT-4 technical report.",
"arXiv preprint arXiv:2303.08774",
", 2023b.",
"Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. GrIPS: Gradient-free, edit-based in-",
"struction search for prompting large language models. In",
"Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt",
"optimization with ”gradient descent” and beam search. In",
"L. Schneider, M. Wistuba, A. Klein, J. Golebiowski, G. Zappella, and F. A. Merra. Hyperband-based",
"bayesian optimization for black-box prompt selection.",
"arXiv preprint arXiv:2412.07820",
"Chengshuai Shi and Cong Shen. Federated multi-armed bandits. In",
"Proc. AAAI",
"Chengshuai Shi, Kun Yang, Jing Yang, and Cong Shen. Best arm identification for prompt learning",
"under a limited budget.",
"arXiv preprint arXiv:2402.09723",
"Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh.",
"Auto-",
"Prompt: Eliciting knowledge from language models with automatically generated prompts. In",
", 2020.",
"Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-",
"training for language understanding. In",
"Mirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,",
"Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, and Jason Wei. Challenging big-",
"bench tasks and whether chain-of-thought can solve them. In",
"Proc. ACL Findings",
"Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,",
"Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly",
"capable multimodal models.",
"arXiv preprint arXiv:2312.11805",
"Arun Verma, Zhongxiang Dai, Xiaoqiang Lin, Patrick Jaillet, and Bryan Kian Hsiang Low.",
"Neural dueling bandits: Preference-based optimization with human feedback.",
"arXiv:2407.17112",
"Ganyu Wang, Yuekang Li, Yi Zeng, Tianyu Wang, Kang Yang, and Kai Chen.",
"FedOne:",
"Query-efficient federated learning for black-box discrete prompt learning.",
"arXiv:2502.04943",
"Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P.",
"Xing, and Zhiting Hu. PromptAgent: Strategic planning with language models enables expert-",
"level prompt optimization. In",
"Yuanhao Wang, Jiachen Hu, Xiaoyu Chen, and Liwei Wang. Distributed bandit learning: Near-",
"optimal regret with efficient communication. In",
"Guoyizhe Wei, Feng Wang, Anshul Shah, and Rama Chellappa. Dual prompt tuning for domain-",
"aware federated learning. In",
"Proc. ECCV Workshop",
"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V. Le, and",
"Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In",
"Zhaoxuan Wu, Xiaoqiang Lin, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick",
"Jaillet, and Bryan Kian Hsiang Low. Prompt optimization with EASE? efficient ordering-aware",
"automated selection of exemplars. In",
"Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun",
"Chen. Large language models as optimizers. In",
"Chun-Pai Yang, Kan Zheng, and Shou-De Lin. Plhf: Prompt optimization with few-shot human",
"feedback.",
"arXiv preprint arXiv:2505.07886",
"12",
"Ziyu Ye, Hao-Yang Chen, Yong-Qiang Hu, Zhen-Yu Su, Qing-An Yao, Yu-Hong Liu, Xiao-Rong",
"Lai, and Yi-Feng Wu. Align-Pro: A principled approach to prompt optimization for llm align-",
"ment.",
"arXiv preprint arXiv:2308.11585",
"Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits",
"problem.",
"Journal of Computer and System Sciences",
", 2012.",
"Ruichen Zhang, Zechu Li, Zhaoxuan Wu, Zhongxiang Dai, Yao Shu, and Bryan Kian Hsiang",
"Low. FedBPT: Efficient federated black-box prompt tuning for large language models. In",
"Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. Fedprompt: Communication-",
"efficient and privacy-preserving prompt tuning in federated learning. In",
"Proc. ICASSP",
"Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and",
"Jimmy Ba. Large language models are human-level prompt engineers. In",
"The performance of Large Language Models (LLMs) is highly sensitive to the quality of input",
"prompts (Zhou et al., 2023; Lin et al., 2024b). While carefully handcrafted prompts (Brown et al.,",
"2020; Wei et al., 2022) can substantially enhance model capabilities, the manual design process",
"is time-consuming and heavily reliant on expert intuition. To address this challenge, early studies",
"focused on white-box prompt optimization, including AutoPrompt (Shin et al., 2020), Prefix-Tuning",
"(Li & Liang, 2021), P-Tuning (Liu et al., 2021), and Prompt Tuning (Lester et al., 2021). More",
"recently, increasing attention has been devoted to black-box prompt optimization (Yang et al., 2024;",
"Ma˜nas et al., 2024; Juneja et al., 2025; Schneider et al., 2024), with representative methods such as",
"GRIPS (Prasad et al., 2023), BDPL (Diao et al., 2023), PRewrite (Kong et al., 2024), PromptAgent",
"(Wang et al., 2024), and APO (Pryzant et al., 2023). RLPrompt (Deng et al., 2022) addresses",
"the discrete black-box setting by optimizing a probability distribution over prompts, from which",
"candidates are sampled to identify the optimal one. Evolutionary approaches, such as EvoPrompt",
"(Guo et al., 2024) and Promptbreeder (Fernando et al., 2024), employ mutation and crossover to",
"iteratively improve prompts. Zhou et al. (Zhou et al., 2023) introduced APE, which leverages an",
"LLM to generate candidate instructions and refines those with high evaluation scores. However,",
"these approaches often require extensive sampling and validation, making them sample-inefficient.",
"A key direction has been reframing black-box prompt optimization as a continuous problem, as in",
"InstructZero (Chen et al., 2023) and ZOPO (Hu et al., 2024). Building on this idea, INSTINCT (Lin",
"et al., 2024b) employs neural bandits to sequentially select instructions to query, leveraging neural",
"networks to better capture the relationship between prompts and their performance, thereby enabling",
"more efficient optimization.",
"Recent work has investigated prompt optimization in scenarios where direct human feedback is dif-",
"ficult to obtain and only preference feedback is available. BPO (Luo et al., 2023) trains an indepen-",
"dent optimizer that automatically rewrites initial prompts using paired preference data, encouraging",
"black-box LLMs to produce better responses. Align-Pro (Ye et al., 2023) develops a theoretical",
"framework based on the Bradley–Terry model to analyze and guide optimization through pairwise",
"comparisons. APOHF (Lin et al., 2024a) formulates prompt optimization as a dueling bandits prob-",
"lem, directly leveraging pairwise preferences (e.g., A is better than B) to efficiently identify the best",
"prompt among candidates. Building on this idea, PLHF (Yang et al., 2025) extends preference-based",
"optimization to a few-shot setting, demonstrating that high-quality prompts can be identified with",
"only a small number of comparisons, thereby greatly reducing annotation costs.",
"B.1",
"Datasets.",
"We use 29 tasks from the Instruction-Induction dataset (Lin et al., 2024b), excluding the",
"auto-debugging task which contains only 8 instances, and the Cause-and-Effect task. The Cause-",
"and-Effect task is an open-ended reasoning problem where multiple answers may be reasonable, but",
"13",
"only one ground-truth is provided. Existing metrics cannot accurately evaluate responses, and most",
"automatic scores are generally zero. For example, a few instances are:",
"• Cause: “The child hurt their knee.” Effect: “The child started crying.”",
"• Cause: “My car got dirty.” Effect: “I washed the car.”",
"• Cause: “Someone fainted.” Effect: “Someone called 911.”",
"For the BBH dataset (Suzgun et al., 2023), we adopt 24 tasks, excluding 3 tasks that overlap with",
"Instruction-Induction to avoid double evaluation.",
"Models.",
"Our experiments are conducted on three LLMs,",
"OpenAI/GPT-3.5-turbo-0613",
"OpenAI/GPT-4o-mini",
"Qwen/Qwen3-235B-A22B-2507",
"via the OpenRouter API. We use MP-",
"Net (Song et al., 2020) as the embedding model.",
"B.2",
"G",
"To simulate a realistic federated setting, we adopt the APE algorithm (Zhou et al., 2023) to construct",
"a prompt pool from a small initial task description (i.e., a set of input–output exemplars). From this",
"pool, each agent samples both shared and personalized prompts, thereby capturing the inherent data",
"heterogeneity—where shared prompts model the common knowledge across agents, while personal-",
"ized prompts reflect the distinct distributions, preferences, and contextual variations specific to each",
"client.",
"Prompt Template.",
"We follow INSTINCT (Lin et al., 2024b) for prompt template to automatically",
"generate prompt space. We use 5 exemplars in datasets to query LLM to induct prompt.",
"Prompt Generation Template",
"Input: [INPUT]",
"Output: [OUTPUT]",
"<",
"More exemplars...",
">",
"The instruction was to",
"Figure 7: Prompt Generation template for prompt space generation.",
"Prompt Generation Example",
"Input: [Today is Christmas Eve of 1937. What is the date 10 days later?]",
"Output: [01/03/1938]",
"Input: [Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later.",
"What is the date 24 hours later?]",
"Output: [03/13/2002]",
"Figure 8: Illustrative example of prompt generation with the template.",
"B.3",
"Evaluation Challenges.",
"Due to the complex nature of the BBH tasks, we observed that large lan-",
"guage models (LLMs) often generate detailed explanations along with their final answers, unlike the",
"more direct outputs seen in the Instruction-Induction tasks. This behavior was particularly prevalent",
"14",
"when using models such as GPT-4o-mini and Qwen3. A small number of tasks in the Instruction-",
"Induction dataset also exhibited this tendency toward verbose responses. Standard evaluation metrics",
"such as exact match, contain, or F1-score proved unreliable in this context. Since the ground-truth",
"answers are typically concise, the verbosity of model outputs frequently led to misclassification. In",
"some cases, a model’s response was fully correct from a human perspective, yet automated metrics",
"incorrectly assigned a score of zero.",
"Multi-choice Metric.",
"To mitigate this issue, we designed a new evaluation metric, termed Multi-",
"choice, specifically tailored to handle the verbose outputs of LLMs on BBH tasks. Our approach",
"normalizes the model’s output and checks whether the ground-truth answer is present. In practice,",
"we extract the final sentence of the model’s prediction and verify if it contains the ground-truth",
"answer.",
"Metrics.",
"For BBH, we evaluate on 24 tasks using the Multi-choice metric. For Instruction-Induction",
"(29 tasks), we follow Lin et al. (2024b) and adopt the same evaluation setup. Concretely, we use the",
"F1 metric for “Common concept” and “Informal to formal”; exact set matching for “Orthography",
"starts with” and “Taxonomy animal”; and label containment for “Synonyms”. For the remaining",
"tasks, we apply exact match. Additionally, for “Diff” and “Odd one out”, when evaluated with GPT-",
"4o-mini or Qwen3 (where verbose explanations are frequent), we employ the Multi-choice metric",
"instead of exact match.",
"Cached Prompt Scoring.",
"We leverage the alignment between prompts and their validation scores.",
"Since our validation set is relatively large (50 samples), we observed that the scores obtained for a",
"given prompt remain stable across repeated evaluations. Consequently, for all algorithms that require",
"optimization over a prompt space (excluding FedOne and PromptBreeder, which do not depend on a",
"prompt space), we evaluate each prompt once on the validation set and cache the resulting score for",
"subsequent use. This strategy substantially reduces computation time while maintaining evaluation",
"reliability.",
"B.4",
"H",
"In",
", we set",
"= 10",
"d",
"= 768",
"matches the output feature",
"dimension of MPNet (Song et al., 2020). For",
"and use a learning rate",
"of 0.001 to update",
"(line 7 of Algo. 3). Training is conducted for 30 iterations.",
"The parameter",
"is time-dependent. Following (Huang et al., 2025), we set",
"r",
"2 log(1",
"/δ",
") +",
"1 +",
"κ",
"denotes the number of agents and",
"is the feature dimension (here",
"for compati-",
"bility with MPNet).",
"B.5",
"To ensure fairness, we set the total number of validation queries to be the same across all methods",
"and report them consistently in our experimental results (see Tables 1, 2, and 3 in Sec. 4, as well as",
"Table 5 in App. C).",
"For score feedback baselines, only INSTINCT and our method share the same evaluation protocol,",
"where each iteration queries the validation set once. Therefore, we ensure fairness by comparing",
"the best reward obtained within the first 50 validation queries, rather than rewards at every single",
"iteration. For preference-feedback baselines, all methods query the validation set twice per iteration,",
"as two prompts are sampled for pairwise comparison. Running 50 iterations thus corresponds to 100",
"validation queries in total. For consistency, we report the score of the",
"first",
"(exploitation) prompt",
"selected by each method. This is consistent with the work of Lin et al. (2024a). The reward curves",
"are plotted across iterations, where the",
"-axis represents the number of iterations (equivalently,",
"preference-feedback steps).",
"Score Feedback.",
"For",
", we run 50 iterations, thus querying the validation set 50 times. We",
"report the best reward at the 50th iteration. For INSTINCT, we follow the default settings from",
"their paper, which are consistent with our protocol (one query per iteration), and also report the",
"15",
"Table 4: Query settings and reported metrics for different methods.",
"Score Feedback",
"Method",
"Queries/Iter",
"Total Queries",
"Reported Metric",
"50",
"Best reward at 50th iter.",
"INSTINCT",
"PromptBreeder",
"Best reward at 10th round.",
"FedOne",
"Preference Feedback",
"FLDB-OGD",
"FLDB-GD",
"APOHF",
"Double-TS",
"best reward at the 50th iteration. For PromptBreeder, which is an evolutionary algorithm, half of the",
"population queries the validation set in each round. With a population size of 10 (2 mutation prompts",
"×",
"5 thinking styles), this results in 5 queries per round and 50 queries in total over 10 rounds; we",
"report the best reward at the 10th round. For FedOne, we follow the original paper and construct its",
"vocabulary using the PMI algorithm, sampling frequent and high-quality words or word pairs from",
"the large prompt domain generated by APE. The setup involves 10 agents, each sampling 5 prompts",
"per round for 50 iterations. To ensure a fair comparison with 50 validation queries, we pair agents",
"and take the maximum score among the prompts they generate as the final performance of FedOne.",
"Preference Feedback.",
"For methods based on preference feedback, including",
"FLDB-OGD, FLDB-GD, APOHF, and Double-TS, each iteration samples two prompts and queries",
"the validation set twice to obtain a pairwise preference. Running for 50 iterations therefore requires",
"100 validation queries in total. We report the best reward at the 50th iteration (based on 100 queries",
"in total). Other hyperparameters follow their original settings to ensure a fair comparison.",
"C.1",
"Performance and Stability Across Different Prompt Domains.",
"In the experiment section, we",
"use GPT-3.5-Turbo to generate the prompt domain via APE. To further validate that our algorithm",
"achieves superior performance across different prompt domains generated by different methods, we",
"replace GPT-3.5-Turbo with GPT-4o-mini while keeping all other settings fixed, such as running",
"both our algorithm and the baselines under the same LLM model, GPT-3.5-Turbo. As shown in",
"Fig. 9, Our method consistently achieves strong performance across different prompt domains, un-",
"derscoring its robustness to domain variability. Beyond maintaining high accuracy, it is capable of",
"identifying near-optimal prompts in a sample-efficient manner, thereby reducing the overall cost of",
"API queries to LLMs.",
"16",
"Figure 9: Performance across different prompt domains",
"C.2",
"Figure 10: More detailed comparison for",
"using GPT-3.5-Turbo.",
"C.3",
"Table 5: Performance comparison on the complete set of Instruction Induction tasks.",
"17",
"C.4",
"LLM M",
"Figure 11: More detailed comparison for",
"using GPT-4o-mini.",
"18",
"Figure 12: More detailed comparison for",
"using Qwen3-235B-A22B-2507.",
"Figure 13: The performance of",
"across different iterations GPT-4o-mini.",
"Figure 14: The performance of",
"across different iterations Qwen3-235B-A22B-",
"2507.",
"19",
"This section provides a rigorous mathematical analysis of the local objective function adopted by",
"for federated optimization. We derive the first-order optimality conditions and",
"demonstrate the necessity of the linear dual term for ensuring convergence to a globally opti-",
"mal and consistent solution. The results here provide theoretical support for the design of our",
"D.1",
"The standard federated learning objective is to minimize a global function",
", defined as the",
"average of",
"local client objectives",
"f",
"→",
"For distributed optimization, this is equivalently formulated as a constrained problem with local",
"variables",
"and a global consensus variable",
"min",
"s.t.",
"∈{",
", . . . , m",
"(6)",
"D.2",
"The constrained problem in Eq. equation 6 can be solved using the Method of Multipliers. We",
"introduce a dual variable (Lagrange multiplier)",
"for each consensus constraint and add a",
"quadratic penalty term for the constraint violation. This forms the augmented Lagrangian function",
", θ,",
"γ",
"∥",
"γ >",
"is a penalty parameter. An iterative algorithm then seeks a saddle point of this function.",
"D.3",
"-O",
"A stationary point of the augmented Lagrangian must satisfy",
". These",
"first-order conditions are derived as follows.",
"The partial derivative with respect to a local variable",
"is:",
") = 0",
"(7)",
"∂",
"∂θ",
"The partial derivative with respect to the global variable",
"⇒",
"(8)",
"To see the implication of these conditions, we sum Eq. equation 7 over all clients",
"from Eq. equation 8 into the above yields:",
"Substituting the expression for",
"which simplifies to:",
"This proves that any stationary point of",
"satisfies that the average of the local gradients is zero. If",
"the solution is also primally feasible (i.e.,",
"), this condition becomes precisely the first-order",
"optimality condition for the original global problem:",
"⇐⇒",
"D.4",
"D.4.1",
"N",
"To prove that the linear term",
"is necessary, we analyze the case where it is omitted, relying",
"solely on a quadratic penalty. The objective would be:",
"˜",
"The first-order condition with respect to",
"for this objective is:",
"At a point of consensus where",
"for all",
", the penalty term vanishes, and the condition strin-",
"gently requires that:",
"i.",
"This is a significantly stronger condition than global optimality, as it requires the solution",
"to be a",
"stationary point for every client’s objective function simultaneously. Such a point is generally non-",
"existent for heterogeneous data distributions where local minima differ. Therefore, the inclusion",
"of the linear dual term is mathematically essential to relax this condition to the correct global one,",
"D.4.2",
"In iterative methods that solve for a saddle point of",
", the dual variables are typically updated via",
"dual ascent:",
"(9)",
"If the algorithm converges to a primally feasible solution",
", then",
"lim",
". At",
"this limit, the stationarity condition from Eq. equation 7 must hold. As",
", the",
"equation implies that the dual variables converge to a fixed point",
"This result provides a clear interpretation of the dual variable at the optimal solution:",
"is precisely",
"the negative of the",
"-th client’s scaled local gradient at the global optimum. The condition",
"(from Eq. equation 8 at convergence) then mathematically guarantees that",
". The",
"dual variables are thus the mechanism that allows local gradients to be non-zero while ensuring their",
"sum is zero.",
"In this section, we present the optimized prompts together with their validation-set scores obtained",
"by our",
"across all 53 tasks in both the Instruction Induction and",
"BBH datasets after 50 optimization rounds. For each task in the tables, the",
"upper row",
"reports the",
"21",
"prompt and score optimized by",
", while the",
"lower row",
"corresponds to those optimized by",
"Table 6: Optimized prompts and their scores for the Instruction Induction tasks",
"22",
"Table 7: Optimized prompts and their scores for the BBH tasks",
"23",
"24",
"BSTRACT",
"NTRODUCTION",
"ROBLEM",
"ETTING",
"XPERIMENTS",
"BLATION",
"TUDY",
"ELATED",
"ORK",
"ONCLUSION",
"EFERENCES",
"DDITIONAL",
"ORE",
"ETAILS ON THE",
"XPERIMENTAL",
"ESULTS",
"ATHEMATICAL",
"RINCIPLES OF THE",
"OCAL",
"BJECTIVE",
"UNCTION",
"DOPTED BY",
"REF",
"PTIMIZED",
"ROMPTS",
"ROM",
"ED",
"AND",
"Equal contribution.",
"Corresponding author. Correspondence to daizhongxiang@cuhk.edu.cn.",
"1:",
"2:",
"3:",
"4:",
"5:",
"6:",
"7:",
"8:",
"9:",
"10:",
"11:",
"12:",
"(a) Instruction",
"Induction",
"(b) BBH",
"(a) Instruction Induction",
"(GPT-4o-mini)",
"(c) Instruction Induction",
"(Qwen)",
"(d) BBH",
"(a)",
"(b)",
"(a) Instrcution Induction",
"Continued on next page"
],
"tables": [
"|0.82<br>0.80<br>0.78 0.76 Reward<br>0.74<br>0.72 0.70 Average<br>000 ... 666 468 A Ag ge en nt t= =1<br>A g e n t = 3<br>1<br>0.620 10 20 30 40<br>Iterations|Col2|Col3|Col4|Col5|0.64<br>0.62<br>0.60 Reward<br>0.58<br>0.56 Average<br>0.54<br>0.52<br>0 0.50<br>50 0.480 10 20<br>Itera|\n|---|---|---|---|---|---|",
"|0.80<br>0.78<br>076 Reward<br>0.74<br>0.72 Average<br>0.70<br>0.68 Agent=1<br>0.66 A Ag ge en nt t= =3 10<br>0.640 10 20 30 40 5<br>Iterations|Col2|Col3|Col4|Col5|Col6|0.60<br>0.58<br>0.56 Reward<br>0.54 Average<br>0.52<br>0.50<br>0 0.480 10 20<br>Itera|\n|---|---|---|---|---|---|---|\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.64<br>0.66<br>0.68<br>0.70<br>0.72<br>0.74<br>076<br>0.78<br>0.80<br>Agent=1<br>~~Agent=3~~<br>Agent=10<br>|||||Agent=1<br>~~Agent=3~~<br>Agent=10|Agent=1<br>~~Agent=3~~<br>Agent=10|",
"|Col1|Col2|\n|---|---|\n|||\n|||\n|||\n|||\n|FedBOP(Agent=3<br>FedBOP(Agent=1|)<br>0)|",
"|0.78<br>0.76<br>0.74 Reward<br>0.72<br>0.70 Average<br>0.68<br>0.66 Agent=1<br>0.64 A Ag ge en nt t= =3 10<br>0.620 10 20 30 40<br>Iterations|Col2|Col3|Col4|Col5|Col6|Col7|0.78<br>0.76<br>0.74 Reward<br>0.72<br>0.70 Average<br>0.68<br>0.66<br>0.64<br>50 0.620 10 20 30<br>Iterations|Col9|0.82<br>0.80<br>0.78 0.76 Reward<br>0.74<br>0.72 Average<br>0.70<br>A Ag ge en nt t= =1 000 ... 666 468 A Ag ge en nt t= =1<br>A g e n t = 3 A g e n t = 3<br>10 10<br>40 50 0.620 10 20 30 40<br>Iterations|Col11|Col12|Col13|Col14|Col15|Col16|0.76<br>0.74<br>0.72 Reward<br>0.70<br>0.68 Average<br>0.66 Age<br>0.64 Age<br>Age<br>50 0.620 10 20 30 40<br>Iterations|Col18|Col19|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>**Average Reward**<br>0.62<br>0.64<br>0.66<br>0.68<br>0.70<br>0.72<br>0.74<br>0.76<br>0.78<br>~~Agent=1~~<br>Agent=3<br>Agent=10|||||||||||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>**Average Reward**<br>0.62<br>0.64<br>0.66<br>0.68<br>0.70<br>0.72<br>0.74<br>0.76<br>0.78<br>~~Agent=1~~<br>Agent=3<br>Agent=10|||||||||||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>**Average Reward**<br>0.62<br>0.64<br>0.66<br>0.68<br>0.70<br>0.72<br>0.74<br>0.76<br>0.78<br>~~Agent=1~~<br>Agent=3<br>Agent=10|||||||||||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>**Average Reward**<br>0.62<br>0.64<br>0.66<br>0.68<br>0.70<br>0.72<br>0.74<br>0.76<br>0.78<br>~~Agent=1~~<br>Agent=3<br>Agent=10||||~~Agent=1~~|~~Agent=1~~|~~Agent=1~~|~~Agent=1~~||||||Ag|Ag|ent=1|ent=1||Age|\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>**Average Reward**<br>0.62<br>0.64<br>0.66<br>0.68<br>0.70<br>0.72<br>0.74<br>0.76<br>0.78<br>~~Agent=1~~<br>Agent=3<br>Agent=10||||Agent=3<br>Agent=10|Agent=3<br>Agent=10|Agent=3<br>Agent=10|Agent=3<br>Agent=10||||||Ag<br>~~Ag~~|Ag<br>~~Ag~~|ent=3<br>~~ent=10~~|ent=3<br>~~ent=10~~||~~Age~~<br>Age|",
"|0.60<br>0.58<br>0.56 Reward<br>0.54<br>Average<br>0.52<br>0.50<br>Agent=1<br>0.48 Agent=3<br>Agent=10<br>0.46<br>0 10 20 30 40 5<br>Iterations|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>0.60<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>0.60<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>0.60<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>0.60<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>0.60<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||Agent=1<br>|Agent=1<br>|Agent=1<br>|Agent=1<br>|\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>0.60<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||~~Agent=3~~<br>Agent=10|~~Agent=3~~<br>Agent=10|~~Agent=3~~<br>Agent=10|~~Agent=3~~<br>Agent=10|",
"|0.58<br>0.56<br>0.54 Reward<br>0.52<br>Average<br>0.50<br>0.48<br>Agent=1<br>0.46 Agent=3<br>Agent=10<br>0.44<br>0 10 20 30 40 5<br>Iterations|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.44<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.44<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.44<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.44<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>Agent=1<br>~~Agent=3~~<br>Agent=10|||||||||||\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.44<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>Agent=1<br>~~Agent=3~~<br>Agent=10||||||||||Agent=1<br>|\n|**Iterations**<br>0<br>10<br>20<br>30<br>40<br>5<br>**Average Reward**<br>0.44<br>0.46<br>0.48<br>0.50<br>0.52<br>0.54<br>0.56<br>0.58<br>Agent=1<br>~~Agent=3~~<br>Agent=10||||||||||~~Agent=3~~<br>Agent=10|",
"|Col1|APOHF|\n|---|---|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n||||||||||||Agent=1<br>~~At=3~~|Agent=1<br>~~At=3~~|Agent=1<br>~~At=3~~|\n||||||||||||~~gen~~<br>Agent=10|~~gen~~<br>Agent=10|~~gen~~<br>Agent=10|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n||||||||||||||Agent=1<br>~~At=3~~|Agent=1<br>~~At=3~~|Agent=1<br>~~At=3~~|\n||||||||||||||~~gen~~<br>Agent=10|~~gen~~<br>Agent=10|~~gen~~<br>Agent=10|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n||||||||||||||~~Agent=1~~<br>~~At=3~~|~~Agent=1~~<br>~~At=3~~|~~Agent=1~~<br>~~At=3~~|\n||||||||||||||~~gen~~<br>Agent=10|~~gen~~<br>Agent=10|~~gen~~<br>Agent=10|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n|||||||||||Agent=1<br>~~At=3~~|\n|||||||||||~~gen~~<br>Agent=10|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2509.24701v1.pdf"
}