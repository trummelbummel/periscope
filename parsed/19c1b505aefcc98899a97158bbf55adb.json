{
"text": "Published as a conference paper at ICLR 2026\n\n\n\n           TIPO: TEXT TO IMAGE WITH TEXT PRESAMPLING FOR\n          PROMPT OPTIMIZATION\n\n                      Shih-Ying Yeh⋆‡ ♠♦   Yi Li⋆♣   Sang-Hyun Park♡   Giyeong Oh♡   Xuehai Wang⋆\n\n                 Min Song♡♢              Youngjae Yu†             Shang-Hong Lai♠\n\n                                National Tsing Hua University♠   Nanyang Technological University♣   Yonsei University♡\n                      Onoma AI♢   Anhui Medical University⋆   Seoul National University†  Comfy Org Research♦\n                                                   Corresponding Author‡   Equal Contribution⋆\n                          kohaku@kblueleaf.net  • Code: github.com/KohakuBlueleaf/KGen\n\n                                       ABSTRACT2026\n                        TIPO (Text-to-Image Prompt Optimization) introduces an efficient approach for\n                               automatic prompt refinement in text-to-image (T2I) generation. Starting from sim-Jan                          ple user prompts, TIPO leverages a lightweight pre-trained model to expand these\n29                       promptsfrom a targetedinto richer,sub-distributiondetailed versions.withinConceptually,the broader semanticTIPO samplesspace,refinedpreservingpromptsthe\n                                  original intent while significantly improving visual quality, coherence, and detail.\n                              Unlike resource-intensive methods based on large language models (LLMs) or re-\n                               inforcement learning (RL), TIPO provides computational efficiency and scalability,\n                             opening new possibilities for effective, automated prompt engineering in T2I tasks.\n                              Extensive experiments across multiple domains demonstrate that TIPO delivers\n                                stronger text alignment, reduced visual artifacts, and consistently higher human[cs.CV]                         preference rates, while maintaining competitive aesthetic quality. These results\n                                 highlight the effectiveness of distribution-aligned prompt engineering and point\n                             toward broader opportunities for scalable, automated refinement in text-to-image\n                                 generation.\n\n\n                1  INTRODUCTION\n\n                   The rapid proliferation of Text-to-Image (T2I) generative models has revolutionized artistic creation\n                     (Ossa et al., 2024; Betker et al., 2023; Esser et al., 2024a; Saharia et al., 2022; Ramesh et al., 2021;\n                     2022; Shi et al., 2020; Rombach et al., 2022a; Podell et al., 2024; Sauer et al., 2024; Chen et al.,\n                      2024b;a; Li et al., 2024b; Esser et al., 2024b; black-forest labs, 2024). These models offer direct\n                        control over generative visual content via text prompts. To achieve precise control, modern T2I\n                         architectures are often trained on lengthy, detailed text descriptions, which may consist of individual,\n                      formatted tags of objects, backgrounds, styles, or complex, integrated paragraphs outlining image\n                       content and layout. However, the increasing complexity of prompts often forces users to iterativelyarXiv:2411.08127v5                          refine them to convey intent. Moreover, most state-of-the-art T2I models are aesthetically fine-tuned\n                             (e.g., on LAION-aesthetics (Podell et al., 2024)) to favor nuanced artistic and stylistic cues, making\n                        high-quality T2I artwork mainly accessible to those with significant artistic expertise.\n\n                      Extensive efforts have been made to reduce the reliance on human expertise through prompt opti-\n                        mization, i.e., expanding and refining a user’s primitive input into a more detailed prompt to enhance\n                       generation quality. As shown in Figure 1(a), a straightforward approach is to leverage pre-trained\n                     Large Language Models (LLMs) to rewrite prompts in a zero-shot manner (Ma˜nas et al., 2024). Yet,\n               LLMs are primarily trained on general natural language, such as paragraphs and dialogues, which\n                           differ significantly from the structured prompts used for T2I models. This discrepancy often leads to\n                        additional effort in crafting LLM prompts and increased misalignment between generated images\n                    and intended prompts. Figure 1(b) shows a more effective approach: training LLMs directly on\n                    prompt data collected from model users (AUTOMATIC, 2022; daspartho, 2022). While promising,\n                          this method is inherently constrained by the varying levels of user expertise, often resulting in\n                        inconsistent or suboptimal outputs. Recent work (Hao et al., 2023) trains LLMs with reinforcement\n                        learning, where aesthetic scores of generated images serve as rewards, as depicted in Figure 1(c).\n\n\n                                                           1\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nHowever, reinforcement learning is performed on one specific T2I model with high computational\ncost, hindering its application to a broader variety of models.\n\n\n\n     Detailed Prompt                            Detailed Prompt                              Detailed Prompt                                 Detailed Prompt\n    Contextual Variety       User Input      Contextual Variety        User Input       Contextual Variety        User Input           Contextual Variety            User Input\n    Dataset-Aligned Prompt                     Dataset-Aligned Prompt                      Dataset-Aligned Prompt                         Dataset-Aligned Prompt\n\n                                                                                                                     Train                                        Prompt DB    Train\n         Instruction         LLM                                                        LLM                                                                                        LLM                  Text       OriginalCaption  Train    LLM\n                                             Midjourney\n                                           prompts                              RL                           Image        Degraded\n                         Optimized                                                                     Optimized \n                        Prompt            Dall-E 3                                                          Prompt                  Scorer         Optimized Prompt          Text-Image     Caption         Optimized Prompt\n                                         prompts                                                                                         Pair\n                                                                                                     Iterative\n                                 T2I                  Others            T2I            Update                 T2I                    Trained Dataset          T2I\n                          Model                              Model                               Model                                      Model\n\n          (a) Direct LLM            (b) Extend Prompt with Prompt DB    (c) Optimization with T2I Output      (d) Match Distribution of Dataset (Ours)\n\nFigure 1: Comparison of prompt optimization methods using LLM. (a) uses instructions for prompting\nbut its understanding is constrained by the LLM’s knowledge base, not the T2I model. (b) relies\non a curated prompt database, enhancing detail but limiting variety by not fully leveraging the T2I\nmodel’s learned distribution. (c) optimizes using the scorer with RL, requiring multi-turn inference\nwith additional cost. (d) aligns prompts with the T2I model’s training distribution, ensuring detailed\nand diverse prompt generation that fits the target T2I model.\n\nIn contrast to prior work, we argue that good prompts should align with the large-scale text distribu-\ntions of T2I models’ training, including those emphasized in aesthetic fine-tuning. Such alignment\nallows models to better interpret user intent and leverage their learned priors, resulting in stronger\ntext alignment and overall image quality. Based on this insight, we introduce TIPO (Text to Image\nwith text pre-sampling for Prompt Optimization), a framework that brings prompt optimization\ninto the domain of large-scale multi-task pre-training. TIPO is supported by a curated 30M-pair,\n40B-token caption corpus, which is filtered and balanced to maximize compatibility with leading T2I\nmodels and to preserve aesthetic quality. On top of this corpus, we design a suite of pretext tasks that\nreformulate raw user inputs, including both concise natural sentences and tag-based prompts, into\nenriched and distribution-consistent forms. Through this multi-task sampling pipeline, a lightweight\nlanguage model expands (rather than fully rewrites) user prompts, preserving original semantics\nwhile enriching details that are diverse, edit-friendly, and aligned with T2I training distributions.\nExtensive experiments on both in-domain and out-of-domain prompts show that TIPO consistently\noutperforms strong baselines, achieving a 62.8% win rate in human preference (validated by more\nthan 1,400 pairwise comparisons from 221 volunteers), and providing up to a 29.4% runtime effi-\nciency improvement. Figure 1 illustrates the conceptual differences between existing methods and\nTIPO. To summarize, our contributions are at least threefold:\n\n      1. We introduce TIPO, a prompt optimization framework that leverages the large-scale text\n         distributions used in text-to-image (T2I) training.\n\n      2. We train a lightweight multi-task language model that progressively refines both tag-based\n       and natural language inputs into unified prompts, enhancing compatibility across a broad\n        spectrum of T2I models.\n\n      3. Extensive experiments demonstrate that TIPO achieves superior image quality, stronger text\n        alignment, higher human preference, competitive aesthetic quality, and improved runtime\n         efficiency against strong baselines with SOTA T2I models, highlighting its practical value.\n\n2  RELATED WORK\n\nPrompt optimization for T2I models typically leverages language models, which can be broadly\nclassified into two categories: (1) Model-specific strategies that tailor prompts for a particular T2I\nmodel, and (2) Universal strategies that improve prompt quality across a variety of T2I models.\n\nModel-specific Strategies  T2I models generate images whose quality is often measured using\nmetrics such as fidelity, aesthetics, and user preference. These metrics facilitate reinforcement learning\napproaches that optimize prompts for a specific T2I model. For instance, Promptist (Hao et al., 2023)\nfine-tunes a pre-trained language model by using CLIP relevance scores as rewards.  Similarly,\nPAE (Mo et al., 2024) extends this approach by generating dense text embeddings rather than discrete\n\n\n                                       2\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\ntext tokens, with additional control vectors during online reinforcement learning. However, these\nmethods are computationally intensive, often struggling with a larger number of training prompts.\nMoreover, a model optimized for one specific T2I system may not generalize well to others. In\ncontrast, our method leverages over 30 million text descriptions to cover a wide range of high-quality\nprompts, ensuring compatibility with a broad spectrum of T2I models.\n\nUniversal Strategies  To reduce the dependency on specific T2I models, some researchers have\nfocused on refining prompts solely using language models. For example, CogView3 (Zheng et al.,\n2024) employ GLM-4 (GLM et al., 2024), and Lee et al. (2024) employ GPT-J and Text Style Transfer\n(TST) techniques, respectively, for prompt enhancement. However, both of them rely heavily on the\nLLM’s inherent understanding of visual content descriptions, which may result in a misalignment with\nthe diverse requirements of various T2I models. Alternatively, other approaches collect high-quality\nprompts from T2I model users to fine-tune or train LLMs (AUTOMATIC, 2022; succintly, 2022;\ndaspartho, 2022). Such methods, however, are limited by the inconsistent expertise of users. More\nrecently, He et al. (2025); Liu et al. (2024b) leverage vision language models to optimize prompts\nin an iterative loop: a user prompt generates images via a T2I model, the prompt–image pairs are\nevaluated by the VLM to suggest refinements, and the revised prompts are reapplied to T2I generation\nfor continual improvement. While such iterative refinement can improve quality, the distribution\nmismatch between VLMs and T2I training data persists. Conversely, our approach constructs both\ntag-based and natural language prompts using a large-scale dataset of image-text descriptions, thereby\naligning closely with the text distributions underlying T2I models.\n\n3  PRELIMINARIES\n\nWe present the formal definition of T2I models and the problem statement of this work.\n\nText-to-Image Model. A text-to-image (T2I) model defines a conditional distribution\n                               Ip = P(x | p),  x ∈X,\nwhich maps a prompt p to a distribution of images over the space of all possible images X.\n\nProblem statement. Let Iu denote the user’s intended distribution over X. The task of prompt\noptimization is to find an optimized prompt po from the space of all possible prompts P to minimize\na distance d between the T2I output distribution Ip and the intended distribution Iu:\n\n                                po = arg min d Ip, Iu  ,\n                                     p∈P\n\nwhere d(·, ·) is a distance between image distributions (e.g., Fr´echet Inception Distance).\n\n4  METHODOLOGY\n\nWe aim to optimize user prompts to enhance image generation quality.  Instead of end-to-end\noptimizations tailored to a single T2I model, our focus is on prompt rewriting that generalizes across\na broad spectrum of models. Our core intuition is that an ideal prompt should align with the texts\nused in T2I model training. However, rapid advances in image captioning have rendered these texts\nincreasingly diverse and complex. To address this, we propose to (1) design a clearly structured\nprompt schema compatible with most text descriptions, and (2) implement a pre-sampling algorithm\nthat progressively refines arbitrary, coarse user input into organized, fine-grained prompts.\n\n4.1  TEXT SET PREPARATION\n\nAlthough the text descriptions used for T2I model training are notably diverse, most are image\ncaptions that fall into two broad categories: tag-based and natural language (NL)-based captions.\nTag-based captions, such as those in the Danbooru2023 dataset (nyanko202, 2023; Yeh, 2024a), use\ncomma-separated, succinct terms to describe image content. In contrast, NL-based captions, typically\ngenerated by language models with visual capabilities (Liu et al., 2023; Agrawal et al., 2024; OpenAI,\n2024; Li et al., 2024a; Bai et al., 2023; Dai et al., 2024; Xiao et al., 2024; Deitke et al., 2024), may\ncomprise multiple sentences. We represent both types using a unified text set T = {t1, t2, t3, . . . , tn},\nwhere each element is an individual tag or sentence.\n\n\n                                       3\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n   (a) No Pre-Sampling         (b) Add Details        (c) Add Random Words    (d) TIPO Pre-Sampling          Distribution of All Pixels\n\n                              All                                   All                                   All                                   All\n                   Images            Inferred        Images                     Images                     Images          Distribution of All the Images\n        Inferred                        Images                            Inferred                        Inferred\n       Images                                                Images                     Images\n                                                                                                     Expected Output Distribution of\n                  Expected                                                                           Expected                                                                                                                    Expected                                               Expected\n                  Images                                                                        Images                                                                                                   Images          Input Prompt for Model                                             Images\n                                                                                                                     Inferred Ouput Distribution of\n                                                                                             Model for Given Input Prompt\n\n                                                                                                         Nonsensical or\n                                                                                                Not User Indent Image\n\n                                                                                       Prompt\n\n                                                                                    An astronaut rides horse on Mars + < >\n\n                                                                                        <  >: Tansformation function - LLM, Random, or TIPO\n\nFigure 2: Illustration of various pre-sampling method for generating the T2I prompt An astronaut\nrides horse on Mars + <T >. (a) yields a basic image. (b) enhances details of images but\nrequires manual refinement. (c) adding random words may introduce irrelevant content (red boxes),\nexceeding the user’s intent.  (d) TIPO pre-sampling (ours) aligns outputs with expected intent,\nmaintaining both detail and variety. <T > represents a transformation function for pre-sampling.\n\n\nWhile the original image captions are fine-grained and detailed, which can yield high-quality images\nwhen all elements are used, they often result in prompts that are excessively lengthy or overloaded\nwith information. Such prompts diverge from typical user input and pose alignment challenges. To\nmitigate this, we construct a simpler subset by removing some tags and sentences from the original\nprompt 1set, as detailed in Section 4.2.\n\n4.2  FORMATTED PROMPT CONSTRUCTION\n\nWe  aim  to  construct  prompts  in  a  unified  format  compatible  with  existing  image\ncaptions.     First, we  incorporate  the common  metadata  present  in  these  image  cap-\ntions,   typically  represented  as  <Category>: <Content>.     These  metadata  cat-\negories   primarily  include  style,  aspect ratio,  quality,   and  year   (e.g.,\nquality: masterpiece, style: Impressionist).    This  structured metadata  is\nintuitive for users to read and edit, while also providing strong guidance to downstream T2I models\non the generation scope.\n\nNext, we construct both tag-based and NL-based prompts using text sets T. Our design generates both\nsimple (incomplete) and complete prompts for each image, and we train an auto-regressive language\nmodel to extend the simple prompts into complete versions. For tag-based prompts, since the tags are\nlargely order-insensitive (i.e., the order has minimal impact on T2I outcomes), we propose a prefix-\nbased dropout strategy. We first randomly shuffle the complete set of tags T = {t1, t2, t3, . . . , tn}\nfrom a given image caption. Then, we construct a simpler tag set Ts = {t1, t2, . . . , tm} by randomly\nselecting m < n tags. The prompts are constructed as:\n                             ps = concat(Ts),   po = concat(T)\nHere, ps and po denote the simple and original prompts, respectively. By ensuring that ps is always a\nprefix of po, the language model can readily expand simple tag-based prompts into complete versions.\n\nFor NL-based prompts, however, this strategy cannot be applied directly because the first sentence\noften contains crucial information (Godbole et al., 2024) and the order of sentences significantly\ninfluences the caption’s semantics. Therefore, we preserve the first sentence and randomly drop some\nof the subsequent sentences without changing their order. Let:\n               S = [sentence1, sentence2, sentence3, . . . , sentencen]\nrepresent the ordered sequence of sentences in an image caption. We derive a simple subsequence Ss\nby randomly selecting m sentences from S while ensuring that the first sentence is always included\nand that the original order is maintained. In other words,\n                      Ss = [sentence1, sentencei2, . . . , sentenceim],\nwith 1 < i2 < . . . < im ≤n and m < n. The simple and complete NL-based prompts are then\nconstructed as:\n                           ps = concat(Ss),   po = concat(Ss, S)\n\n\n                                       4\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nThis ensures that ps remains a prefix of po. Although some sentences may be repeated in po, selecting\na smaller m effectively mitigates this, and it does not empirically affect the generation quality.\n\n\n4.3  TEXT PRE-SAMPLING\n\nWe aim to reformulate user input into forms that better align with the high-quality training text distri-\nbution po via pre-sampling, which stands for “text sampling before image sampling.” A na¨ıve strategy\nis plain text completion, where tokens are directly appended to the user input in an unstructured\nmanner. Such completion often mirrors the low-quality phrasing of the input, deviates from the\ndistribution of high-quality prompts, and produces inferior generations. On the other hand, a full\nrewrite risks deviating from the user’s original intention. To balance these issues, TIPO preserves the\noriginal input and appends a structured, distribution-consistent expansion. This appended segment\nis typically paragraph-like or tag-like, making it both informative and easy to edit or remove (see\nFigure 2 for an illustration and Appendix E for concrete examples).\n\nWe propose the core technique of TIPO, a flexible pre-sampling mechanism that decomposes prompt\noptimization into three subtasks: enriching tag sequences, extending natural language (NL) prompts,\nand refining NL prompts. For example, a short NL prompt can be expanded into a detailed tag\nsequence (short to tag), as illustrated in Figure 3. We further distinguish between basic tasks, which\nperform a single transformation, and composite tasks, which chain multiple transformations within\na single forward pass. The latter expose the model to more holistic training signals while reducing\ncomputational overhead. Table 1 summarizes all tasks and their input–output forms.\n\n\n    Task                Description\n\n     Basic tasks\n     tag to long           Generate a new NL prompt given tags.\n     long to tag          Extend a tag sequence given an NL prompt.\n     short to tag          Extend a tag sequence given a short/simple prompt ps.\n     short to long         Generate a refined, detailed NL prompt given a user-provided NL prompt.\n\n    Composite tasks\n     short to tag to long  From a short NL prompt or tags, produce a refined NL prompt.\n     short to long to tag  From a short or generated NL prompt, extend a tag sequence.\n     tag to short to long  From tags or NL prompts, generate a refined NL prompt.\n\nTable 1: Pre-sampling tasks in TIPO. Basic tasks focus on one-step transformations, while composite\ntasks combine two basic tasks within a single forward pass.\n\nWe randomly select from the aforementioned tasks during training to enhance model generalization.\nBy extensively training on these tasks, TIPO can seamlessly adapt to various input types, flexibly\nrefining user input whether it consists of tags, short sentences, or long sentences. Figure 3 (b)\nillustrates a scenario where both tag captions Ts and short NL captions Ss are available. In such\ncases, TIPO processes each input type separately to maintain clarity and coherence:\n\n               Ss = [A young girl with long hair...],  metadata = ∅\n                Ts = {outdoors, scenery, water, wind, landscape, . . . }\n\nThe generation proceeds sequentially as follows:\n\n      1. short to tag: TIPO uses Ts as the primary prompt to generate a detailed tag sequence Td.\n      2. tag to long: Td is incorporated into the metadata, and TIPO produces a refined short NL\n        prompt Ss based on Td.\n      3. short to tag to long: With both Td and Ss in the metadata, TIPO generates a comprehen-\n         sive long NL prompt Sd, ensuring a more detailed output.\n      4. TIPO aggregates Td, Sd, and any additional metadata to construct a context-rich prompt pd.\n\nThis progressive process enables TIPO to build prompts that are both detailed and contextually\naligned with the user’s input.\n\n\n                                       5\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                                                                                                                                  A                                                                                                                                                       young                                                                                                                                                                                                                                                            girl                                                                                                                                                                                    with                                                                                            scenery,                                                                                                               aerial\n                                                                                                                                                                                                     hair                                                                                                                                                                                                                                                                                                                                                               ...                                                                                                                                                                                         standing                                                                                                                                                  Prompt      long                                                           Prompt       fireworks,                                                                                         blue                                                                                                              sky, ...                                          outdoors,water,scenery,...                                              scenery                                                            A peaceful                                                                                              nighttime                                                                                                                                        on                                                                                                                                              a                                                                                                                                                                sandy                                                                                                                                                                 beach                                                                                                                                                                                                                                                                                                                                                                               ...                                                                   Optimization                                                                                                                                                                           Optimization\n                                                                              scene with a lake ...                                 A young girl with ...                    sunset over ...\n                                      User Input                           Final Output                                          User Input                           Final Output\n                                         User Input                                                                                     User Input\n                                                                                      scenery,                                                                                                                                                                                ..., cloudy                                                                                                                                                                     outdoors,                                                                                                                                                                                       scenery,                                                                                                                                    outdoors,                                                                                                                                                    scenery,                                                                                          sky,                                                                                        firework                                                                                                                                                , forest,                                       TIPO                                                                                                  TIPO                                                                                                                                                water,                                                                                                                                                                                                                                                                        ...                                                                                                                                                                               water,                                                                                                                                                                     beach,                                                                                                                                                                                                                                        1girl,                                              scenery                                                                         mountainous                                                                      short                                                                                                                                                                                        to tag                                                                                                                                                                long black                                                                                                                                                                                                 hair                                                                                                                                                                                                                                                                                                                                             ...                                                                                                                  young                                                                                                                                                                                           girl                                                                                                                                          with ...    short                                                                            to tag     horizon,                                                                                                    starry                                                                                                         sky, ...                                A\n\n                                  Long Tags                                                                       Long Tags\n   Tag Prompt                                                                      Tag Prompt\n                                               scenery,                                                                                                       ..., cloudy                                                          A beautiful                                                                                              night                                                                                                                                    outdoors,                                                                                                                                                    scenery,                                                                                                                                  A                                                                                                                                                     young                                                                                                                                                                                                                                                      girl                                                                                                                                                                                 with   scenery,         no            humans,                          aerial                                                                                                  outdoors,                                                                                                                 scenery,                                                                                                                          water,                                                 sky,                                                 firework                                                                                      , forest,                                                                          scene                                                                                  with a                                                                                              lake                                                                                                       in       1girl,                                       TIPO                                                                                                  TIPO                                                                                                                                           water,                                                                                                                                     beach,                                                                                                                                                                                            1girl,                                                                                                                                                                     long                                                                                                                                                                                                 hair                                                                                                                                                                                                                                                                                                                                                    ...                                                                                                                                                                                      standing                                         mountainous                                                                            the                                                                                                                                                                 ...                                                                            mountains                                                                                                       in    fireworks,             nature,                   blue                           sky, cat, dusk,                                                                                          long                                                                                                 black                                                                                                                               hair,                                                                                                       white                                                                                                                            dress,                                                                                                                   blue                                                                          to long                                                                                                                                                                                       to long                                                                                                                                long black                                                                                                                                                            hair                                                                                                                                                                                                                                                                               ...     tag                                                                                                                                       on                                                                                                                                             a                                                                                                                                                             sandy                                                                                                                                                               beach                                                                                                                                                                                                                                                                                                                                                                   ...                                                          starry                                                               sky, ...    tag                                                                            the                                                                                       background...\n    fireworks,              grass,                     lake,                       outdoors,                                                            ...         horizon,                                                                                                  eyes,                                                                                                    sundress,                                                                                               beach                                                                                                                                                                                                                ...\n   Natural Language Prompt          Long NL                                       Final Output     Natural Language Prompt          Long NL                                       Final Output\n                                                                                        scenery,                                                                      no                                                                                humans,                                                                                                                                  A                                                                                                                                                       young                                                                                                                                                                                                                                                            girl                                                                                                                                                                                    with                                A beautiful                                                         night  A     peaceful             nighttime                     scene                           with                                                                          A                                                                                 young                                                                                                                                       girl                                                                                                     with                                                                                                         long                                                                                                                            hair                                                                                                 and                                                                                                        a                                                                                                        A                                                                                                                       young                                                                                                                                                                                                      girl                                                                                                                                                with                                                            A                                                                                             beautiful                                                                                                night                                                                                           scene                                       TIPO                                                                                                  TIPO                                                                                                                                                                        long                                                                                                                                                                                                     hair                                                                                                                                                                                                                                                                                                                                                               ...                                                                                                                                                                                         standing                                         scene                                               with a                                                         lake                                                                in  two       cats resting                    in grass,                       framed                                                                                          red                                                                       bow                                                                                                                 in                                                                                                   her                                                                                                                                    hair,                                                                                                               standing                                                                                                 on                                                                                 with a                                                                                             lake                                                                                                      in the                                                                                                                                                                                                 ...                                                                                                         a      long                                                                                                                                                           hair                                                                                                                                                                                                                                                                                  ...                                                                                                                                                   standing                                                                              short                                                                                 to tag                                                                                                                                                                                                   short                                                                                                                                                                                                    to tag                                          the                                                                                           ...                                            mountains                                                                in                                                                                                                                        on                                                                                                                                             a                                                                                                                                                                sandy                                                                                                                                                                 beach                                                                                                                                                                                                                                                                                                                                                                               ...   by       trees,                    ...            mountains                          in                         the                                                                                    sandy                                                                                       beach                                                                                                                                                                                                 ...                                                                                                         sunset                                                                                                              over                                                                                                                   the                                                                                         fireworks                                                                                above                                                                                              the                                                                                                            on                                                                                                                 a                                                                                                                              sandy                                                                                                                                beach                                                                                                                                                                                                                                                                                                 ...                                                                              to long                                                                                                                                                                                                 to long                                          the                                                 background...                                                                                                                                                                              sunset                                                                                                                                                                                 over                                                                                                                                                                                                                                                                                                                                                                         ...   background.                                                                                   ocean                                                                                                                                                                                      ...                                                                                             she                                                                                                                                             is                                                                                                    wearing                                                                                                  a ...                                                                                             lake                                                                                                                                                                                      ...\n\n                (a) Scenery tag only input                              (b) Tag + short NL input\n\nFigure 3: Example prompt optimization paths in TIPO. (a) shows generation from a single tag input,\nwhile (b) uses both tag and natural language input. These illustrate representative pipelines, not the\nfull range of use cases. Blue shading indicates increasing prompt richness.\n\n\n\nImplementation Details  In implementation, TIPO adopt the LLaMA architecture (Touvron et al.,\n2023a;b; AI@Meta, 2024)1, with all experiments are conducted with a 200M-parameter model 2.\nOur training dataset is about 40 billion tokens curated from Danbooru2023 (nyanko202, 2023; Yeh,\n2024b), GBC10M (Hsieh et al., 2024), and CoyoHD11M (CaptionEmporium, 2024).\n\n\n5  EXPERIMENTS\n\n5.1  EXPERIMENTAL SETTINGS\n\nBaselines and T2I Models We compare against representative prompt-optimization baselines:\nGPT-4o-mini for zero-shot rewriting (OpenAI, 2024), MagicPrompt, which fine-tunes GPT-2 on\ncommunity-collected prompts (daspartho, 2022), Promptist, which applies reinforcement learning\nfor model-preferred prompt optimization (Hao et al., 2023), and Gemini-2.0-flash-image 3, which\nuniquely serves both as a prompt-refinement baseline and a T2I generator. For image generation\nbackbones, our main experiments adopt SDXL-base-1.0 (Podell et al., 2024), Illustrious v3.5 (v-\npred), Kohaku-XL-Zeta, and Stable Diffusion 3.5 Large(Esser et al., 2024a). To further assess\ngeneralization, we additionally evaluate on four diverse backbones with undisclosed training data:\nFLUX.1-dev (black-forest labs, 2024), Omnigen2 (Wu et al., 2025), Lumina-2 (Qin et al.,\n2025), and HiDream-I1 (Cai et al., 2025). The details of T2I models are provided in Appendix B.\n\n\nEvaluation Metrics  We employ four latest metrics FDD (Stein et al., 2023), Aesthetic Score (dis-\ncus0434, 2024), AI Corrupt Score (narugo1992, 2023), and Vendi Score (Friedman & Dieng, 2022)\nto measure the quality of generated images. Specifically, FDD (Fr´echet DINO Distance) quantifies\nfidelity by comparing the distribution of DINOv2 features (Oquab et al., 2023) between reference\nimages in the evaluation dataset and images generated from the corresponding captions, which better\naligns with human perception than traditional FID (Heusel et al., 2017). Aesthetic Score is computed\nvia Aesthetic Predictor V2.5 (discus0434, 2024), quantifying visual appeal, composition quality, and\nartistic merit. AI Corrupt Score detects technical flaws in generated images by identifying visual\nartifacts. Vendi Score quantifies image diversity by calculating the von Neumann entropy from a\nnormalized cosine similarity matrix using DinoV2 embeddings. Notably, Vendi is defined directly\non feature-space dispersion and is sensitive to non-semantic variations such as low-level noise or\nartifacts. As a result, it “should be used alongside a quality metric” (Friedman & Dieng, 2022) and a\nhigher value alone does not always correspond to more meaningful semantic diversity.\n\n   1The multi-task design of TIPO is compatible with many autoregressive language models (e.g., GPT, LLaMA,\nQwen, etc.). Intuitively, adopting more advanced backbones could further enhance efficiency and effectiveness,\nbut such exploration would require extra training cost, which we defer to future work.\n   2We also trained TIPO-100M and 500M variants to analyze the impact of model scales. See Appendix C.\n   3https://developers.googleblog.com/experiment-with-gemini-20-flash-native-image-generation/\n\n\n                                       6\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n   (a) Scenery Tag Only     (b) Scenery Tag + TIPO   (c) Truncated Long Prompt    (d) Truncated + TIPO\n\nFigure 4: Generated images from 4 types of prompts: (a) simple scenery tag, (b) scenery tag enhanced\nby TIPO, (c) truncated (< 40 words) long prompt, (d) TIPO-enhanced truncated prompt. TIPO adds\ndetail and maintains variety, yielding coherent images from simple prompts.\n\n\n\nEvaluation Protocols  Our proposed TIPO leverages large-scale image caption datasets for training,\nwhich overlap with the training text distributions of many T2I models. Following Promptist (Hao\net al., 2023), we divide our experiments into two settings: (1) In-domain, where the T2I model’s\ntraining texts overlap with those used by TIPO, and (2) Out-of-domain, where no overlap exists.\n\n\n5.2  EXPERIMENTAL RESULTS\n\nIn-domain Tag-based Prompt Optimization  To assess prompt optimization performance on\ntag-based prompts, we generate scenery images, as they contain abundant descriptive tags for objects\nand backgrounds. We randomly sample 32,768 tag-based prompts from Danbooru2023 (Yeh, 2024b;\nnyanko202, 2023), shuffle and concatenate the scenery tags into new prompts (thereby preventing\ndata leakage of the original captions), and generate one image per prompt using Kohaku-XL-Zeta.\nThis evaluation tests the in-domain capabilities, as Danbooru2023 is used during both TIPO and\nKohaku-XL-Zeta training. The results in Table 2 reveal two key insights. First, MagicPrompt and\nPromptist, which rely on user prompts or reinforcement learning, underperform in Aesthetic and AI\nCorrupted Scores due to the quality or quantity limitations of their collected samples (e.g., Promptist\nuses 90K samples, limited by the high reinforcement learning cost). In contrast, GPT and TIPO\nbenefit from large-scale training corpora (>30M samples), yielding higher-quality outputs. Second,\nTIPO achieves the best FDD by a substantial margin over GPT, which can be attributed to its superior\ndistribution alignment with T2I models.\n\n\nIn-domain NL-based Prompt Optimization  We evaluate the prompt optimization performance\non NL-based prompts by selecting 10,000 short prompts and 10,000 long prompts from CaptionEm-\nporium (CaptionEmporium, 2024) and GBC (Hsieh et al., 2024) as test prompts. In particular, since\nthe long prompts are much longer than typical user input, we truncate them to two sentences (<\n40 words) to simulate real-world applications. We use SDXL-1.0-base as the T2I model, whose\ntraining text data largely overlap with TIPO. Table 2 demonstrates that TIPO achieves either the best\nor second-best scores in Aesthetic and AI Corrupt Score by effectively enriching the original prompt\nwith appropriate textual elements while rarely introducing extraneous noise. While all methods com-\npromise fidelity and diversity, as reflected in the FDD and Vendi Score, TIPO remains competitive\nbecause it maintains small semantic deviation from the original sentences via progressive refinement.\n\n\nOut-of-domain Performance  Some recent T2I models are trained on proprietary images and\ncaptions. As a representative example, SD-3.5-Large is trained on private images captioned with\nCogView (Zheng et al., 2024), which differ markedly from the texts used to train TIPO. To evaluate\nmodel performance in this out-of-domain scenario, we generate 8,192 original tag- and NL-based\nprompts using the baseline GPT-4o-mini rather than relying on existing prompt datasets. We apply\nthe remaining methods to these prompts and assess their performance.\n\nAs shown in Table 2, SD-3.5-Large faithfully generates images that align well with GPT-produced\nprompts. Consequently, additional optimizations tend to reduce fidelity and introduce more artifacts.\nNevertheless, GPT-generated prompts are accurate and lack diversity. TIPO optimization enriches\n\n\n                                       7\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nTable 2: Comprehensive performance comparison of TIPO against baselines across different in-domain prompt\ntypes. Metrics are marked with ↑(higher is better) or ↓(lower is better). For OOD tests, the ’Original’ baseline\nand the FDD metric was not applicable. In the table, Aesthetic refers to Aesthetic Score, Corrupt to AI Corrupt\nScore, and Vendi to Vendi Score. TIPO demonstrates significant improvements, achieving the highest average\nrank among all baselines. Best results are in bold and second-best are underlined.\n\n\n    Prompt Type / Task  Metric          Original  GPT  MagicPrompt  Promptist  TIPO\n\n                 FDD ↓           0.3558   0.5414     0.3247       0.2350    0.2282\n     In-domain            Aesthetic ↑       5.0569   6.3676     6.1609       5.9468    6.2571\n     Tag-based Prompts   Corrupt ↓       0.5743   0.2510     0.4976       0.4331    0.0805\n                        Vendi ↑          16.814   8.663      11.901       14.327    13.307\n\n                 FDD ↓           0.0957   0.1668     0.0980       0.1783    0.1168\n     In-domain            Aesthetic ↑       5.8370   6.0589     5.8213       5.7963    5.8531\n     NL-based (Short)    Corrupt ↓       0.2887   0.3015     0.2936       0.3686    0.2870\n                        Vendi ↑          38.172   34.714     38.155       34.127    37.065\n\n                 FDD ↓           0.0955   0.1683     0.1247       0.2096    0.1210\n     In-domain\n                           Aesthetic ↑       5.7497   6.0168     5.8191       5.7759    5.8364\n     NL-based\n                       Corrupt ↓       0.3132   0.3288     0.3259       0.4075    0.2870\n     (Truncated Long)\n                        Vendi ↑          38.253   34.811     37.841       33.527    37.090\n\n                           Aesthetic ↑      N/A    6.7125     6.4507       6.3924    6.0536\n     Out-of-Domain\n                       Corrupt ↓       N/A    0.0518     0.1423       0.0947    0.0720\n    (OOD) Test\n                        Vendi ↑         N/A    8.9718     15.872       16.489    21.571\n\n      Overall             Average Rank ↓    2.58     3.00        3.00          3.87      2.07\n\n\n    Table 3: TIPO on T2I models with undisclosed training data. Significant improvements are in bold.\n\n\n      Model                  Variant      Aesthetic ↑  AI Corrupt ↓  FDD ↓   Vendi ↑\n\n                                  Original        5.2029        0.1202      0.1185   36.2597\n      FLUX.1-dev\n                          TIPO          5.2746        0.0938      0.1202   35.6489\n\n                                  Original        5.2629        0.1110      0.1373   33.7724\n      Omnigen2\n                          TIPO          5.0661        0.1187      0.1253   34.2681\n\n                                  Original        5.2588        0.0981      0.1318   34.7599\n      Lumina-2\n                          TIPO          5.4105        0.0916      0.1286   34.4655\n\n                                  Original        5.7768        0.1055      0.1444   34.6318\n      HiDream-I1\n                          TIPO          5.8143        0.1019      0.1379   34.3544\n\n                                     Self-refined     5.2584        0.0928      0.8084   32.7422\n       Gemini-2.0-Flash-Image\n                          TIPO          5.2649        0.0700      0.7964   32.1136\n\n\n\nthe prompts with additional details that harmonize with the original themes, significantly enhancing\nthe diversity of the generated images.\n\n\nCompatibility with other T2I Models  In addition to the above baselines with publicly known\ntraining data, we further evaluate TIPO on recent models with undisclosed training sources:\nFLUX.1-dev (black-forest labs, 2024), Omnigen2 (Wu et al., 2025), Lumina-2 (Qin et al.,\n2025), HiDream-I1 (Cai et al., 2025), and Gemini-2.0-flash-image (Google), using 1,000\nprompts sampled from the COYO/GBC datasets. Notably, Gemini-2.0-flash-image can perform its\nown prompt optimization, making it a strong closed-source baseline. As shown in Table 3, TIPO gen-\nerally improves image quality and alignment relative to the baselines. For example, Lumina-2 and\nHiDream-I1 achieve higher aesthetic scores and lower corruption rates after TIPO optimization.\nDespite Gemini’s integrated optimization, TIPO still yields measurable improvements, highlighting\nthe practical value of our lightweight, task-specific strategy. Overall, these results demonstrate that\nTIPO maintains robust compatibility with heterogeneous T2I models, even when their training data\nare unavailable and potentially mismatched with TIPO’s optimization corpus.\n\n\n                                       8\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nEfficiency A key concern is whether TIPO’s iterative pre-sampling strategy introduces noticeable\nlatency. Hence, we benchmark prompt-generation latency in Table 4, where TIPO reduces per-prompt\nlatency with an improvement up to 29.4%. Details of training and inference are in Appendix H.4.\n\n                      Table 4: Prompt generation latency and relative speedup\n\n\n                           TIPO   Promptist   PromptExtend   MagicPrompt\n\n             Avg. Time (s)            1.03      1.46          1.38            1.14\n           TIPO speedup vs. each  —    +29.4%      +25.6%        +9.6%\n\n\n\nHuman Preference Evaluation  Quantitative metrics may not fully align with human preference.\nTherefore, we conducted a user study based on pairwise image comparisons between the original\nprompt, MagicPrompt, Promptist, and TIPO on over 1,400 images, gathering preferences from\n221 volunteers. As illustrated in Figure 5a, TIPO achieved the highest overall win rate at 51.3%,\nsignificantly outperforming competitors. In out-of-domain scenarios, TIPO’s win rate increased to\n52.5%, demonstrating consistently strong user preference across different contexts. For further results\nand statistics, please refer to Appendix G.\n\n                  All Data              Out of Domain\n                                                                                   1080                                                                                 Initial ELO  100\n                                           Lose                                                                                                      Prompt Adherence\n                                                 Tie\n                                        Win       36.3%                                 1060                                                      Image Quality        32.1%\n                                                    42.5% 40.1% 46.8% 43.9%                                                                                Aesthetic   80        41.9% 45.5% 45.4%\n                                  38.0%\n                                                                                   1040(%)                                                                                                                                                                           Overall\n   60  16.6%                                11.2%        11.9%                     1020               21.2% 17.5% 18.6% 24.2%             17.6%        18.7% 23.3%         Rating\n   40                                                                      ELO 1000\n                                             52.5%Percentage     51.3%                                                          48.0%                      980\n   20        36.9% 37.0% 36.0% 37.7%             39.9%        34.5% 32.8%\n\n                                                                                    960\n    0\n     TIPOPromptistpromptextmagicpromptoriginal    TIPOPromptistpromptext agicpromptoriginal            TIPO        Promptist        promptext          agicprompt           original\n\n(a) Pairwise comparison results showing win-tie-lose  (b) ELO ratings comparing TIPO and baseline methods\npercentages for overall user preference. Evaluations  across four criteria: Prompt Adherence, Image Quality,\ncover ‘All Data’ and ‘Out-of-Domain’ sets for TIPO  Aesthetic, and Overall. The dashed line indicates the\nand baseline methods.                                         initial ELO rating.\n\nFigure 5: Human preference evaluation demonstrates that TIPO consistently achieves higher user\npreference compared to baseline prompt optimization methods (Promptist, prompttext, magicprompt,\nand original prompts). All evaluated images were generated using SD-3.5-Medium.\n\n\nPrompt Distribution Alignment  While TIPO consistently achieves the best or competitive results\nin image quality, it remains unclear whether such gains arise from a closer distributional alignment\nbetween its optimized prompts and the T2I models’ training text corpora. To verify this hypothesis,\nwe sample from two representative T2I training sets, obtaining 1,000 natural-language captions from\nCOYO and 1,000 tag-based captions from Danbooru2023. We then encode ground-truth captions\nand the outputs of all compared optimization methods using two widely adopted text encoders—T5-\nXXL (Raffel et al., 2020a) (used in SD3, Flux, and PixArt) and jina-embeddings-v3 (Sturua\net al., 2024) (a popular recent text-embedding model). Finally, we measure the embedding-space\nalignment between the ground-truth captions and optimized prompts with Fr´echet Distance (FD) and\nMaximum Mean Discrepancy (MMD with RBF kernel). As shown in Table 5, baseline methods show\nvarying extents of alignment across different prompt types, text encoders, and metrics, while TIPO\nmaintains consistently better alignment under all settings. This indicates that TIPO aligns well with\nthe T2I training-text distribution in a prompt-compatible and encoder-insensitive manner.\n\nPrompt–Image Alignment  To further assess the semantic consistency between optimized prompts\nand their generated images, we compute CLIPScore between each prompt and its corresponding\nSD1.5-generated image using openai/clip-vit-large-patch14-336, on the same prompt\nsets as in the Prompt Distribution Alignment experiment. As shown in Table 6, TIPO achieves\n\n\n                                       9\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nTable 5: Embedding-space distances (FD and MMD) between optimized prompts and T2I training\ncorpora. Lower is better. Best results are in bold and second-best are underlined.\n\n    Prompt    Encoder  Metric  TIPO   Promptist  MagicPrompt  GPT-4o-mini\n\n                   FD      0.0322    0.1003       0.0385        0.1064\n                   Jina\n               MMD   0.0320    0.1501       0.0624        0.1699\n     NL-short\n                   FD      0.0704    0.2072       0.1441        0.1252\n              T5\n               MMD   0.1438    0.2914       0.1972        0.2297\n\n                   FD      0.0309    0.1192       0.0493        0.0963\n                   Jina\n               MMD   0.0359    0.1700       0.0772        0.1642\n     NL-trunc\n                   FD      0.0674    0.2312       0.1884        0.1276\n              T5\n               MMD   0.1404    0.3147       0.2270        0.2323\n\n                   FD      0.1094    0.1891       0.1958        0.2479\n                   Jina\n               MMD   0.1539    0.2473       0.2415        0.3050\n     Tag-based\n                   FD      0.0524    0.2080       0.2578        0.0728\n              T5\n               MMD   0.1846    0.3573       0.3948        0.2194\n\n\nTable 6: CLIPScore between optimized prompts and T2I generated images. Higher is better. Best\nresults are in bold and second-best are underlined.\n\n          Prompt Type  TIPO   MagicPrompt  GPT-4o-mini   Promptist\n\n            Tag-based      0.2217      0.1782         0.1774       0.1642\n            NL-short       0.2413      0.2834         0.2378       0.2347\n            NL-trunc       0.2310      0.2517         0.2275       0.2063\n\n\n\nthe strongest alignment for tag-based prompts. While MagicPrompt shows clear advantages on\nNL-based prompts, this is likely due to its 1.8M+ SD1.5 community training corpus, where prompts\nare typically explicit and stylistically strong, effectively eliciting the CLIP encoder to produce high\nalignment scores. However, such stylistic bias is often less preferred by mainstream users. In contrast,\nTIPO attains competitive performance on NL-based prompts without such bias, as reflected by the\nwin rate in Table 11, where MagicPrompt vs. TIPO = 11:48.\n\n6  CONCLUSION\n\nWe introduced TIPO, a lightweight prompt pre-sampling framework designed for efficient real-world\nText-to-Image (T2I) applications. By aligning user prompts with the intrinsic distributions of T2I\ntraining datasets, TIPO enhances semantic coherence, image fidelity, and diversity with minimal\ninference overhead. Experimental results show that TIPO consistently outperforms existing prompt\noptimization methods across multiple evaluation metrics, while extensive user studies confirm its\nstrong alignment with human preferences. Despite these promising results, several aspects remain\nopen for future exploration, such as generalization to out-of-distribution user input, model-specific\nadaptation, personalization, image-feedback-aware refinement, and large-scale model scaling. We\ndiscuss these limitations and potential extensions in Appendix J. To encourage wider adoption and\nfacilitate reproducibility, we release our trained models and source code. We hope TIPO will inspire\nfurther advancements in efficient, scalable, and robust generative frameworks for creative systems.\n\n\n\n\n\n                                       10\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nETHICS STATEMENT\n\nOur work democratizes prompt optimization for text-to-image models, but automatic prompt enrich-\nment can inherit biases from training data or be misused to produce misleading or harmful content.\nWe therefore emphasize responsible use, including bias mitigation, transparency, and appropriate\nuser controls. No human subjects or sensitive personal data were involved, and we avoided including\nidentifiers or metadata that could raise copyright- or attribution-related concerns.\n\nREPRODUCIBILITY STATEMENT\n\nAn anonymized repository with code and configuration files is provided in the supplementary materi-\nals, enabling reproduction of all reported results. The main paper and appendix reference the exact\nexperimental settings, hyperparameters, random seeds, and evaluation scripts; data preprocessing\nsteps and any additional resources needed to re-create the experiments are also documented there.\nFor hardware, we report that all experiments were conducted on NVIDIA RTX 3090 and/or A6000\nGPUs, with full details described in the appendix and repository.\n\nREFERENCES\n\nPravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica\n  Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, Soham\n  Ghosh, Am´elie H´eliou, Paul Jacob, Albert Q. Jiang, Kartik Khandelwal, Timoth´ee Lacroix,\n  Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall,\n  Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat,\n   Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozi`ere, Alexandre Sablayrolles, Lucile Saulnier,\n  Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim\n   Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang, and Sophia Yang. Pixtral 12b, 2024.\n  URL https://arxiv.org/abs/2410.07073.\n\nAI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/\n  blob/main/MODEL_CARD.md.\n\nMichael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants.\n  arXiv preprint arXiv:2209.15571, 2022.\n\nAUTOMATIC.        promptgen-lexart.     https://huggingface.co/AUTOMATIC/\n  promptgen-lexart, 2022.\n\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\n  Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities.\n  arXiv preprint arXiv:2308.12966, 2023.\n\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\n  Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\n  Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\n\nblack-forest labs.  black-forest-labs/flux: Official inference repo for FLUX.1 models. https:\n  //github.com/black-forest-labs/flux, 2024.\n\nDavid M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine\n  Learning research, 3(Jan):993–1022, 2003.\n\nVincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding\n   of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008\n  (10):P10008, 2008.\n\nPhillip Bonacich. Factoring and weighting approaches to status scores and clique identification.\n  Journal of Mathematical Sociology, 2(1):113–120, 1972.\n\nQi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng\n  Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: A high-efficient image generative foundation\n  model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025.\n\n\n                                       11\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nCaptionEmporium. CaptionEmporium/coyo-hd-11m-llavanext. https://huggingface.co/\n  datasets/CaptionEmporium/coyo-hd-11m-llavanext, 2024.\n\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.  Conceptual 12M: Pushing\n  web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.\n\nJunsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping\n  Luo, Huchuan Lu, and Zhenguo Li. Pixart-Σ: Weak-to-strong training of diffusion transformer for\n  4k text-to-image generation, 2024a. URL https://arxiv.org/abs/2403.04692.\n\nJunsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok,\n  Ping Luo, Huchuan Lu, and Zhenguo Li.  Pixart-α: Fast training of diffusion transformer for\n   photorealistic text-to-image synthesis.  In The Twelfth International Conference on Learning\n  Representations, 2024b. URL https://openreview.net/forum?id=eAKmQPe3m1.\n\nWenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki,\n  Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms.\n  arXiv preprint arXiv:2409.11402, 2024.\n\ndaspartho.  prompt-extend. https://huggingface.co/daspartho/prompt-extend,\n  2022.\n\nMatt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Moham-\n  madreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open\n  weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146,\n  2024.\n\ndiscus0434. aesthetic-predictor-v2-5: SigLIP-based Aesthetic Score Predictor. https://github.\n  com/discus0434/aesthetic-predictor-v2-5, 2024.\n\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\n  Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\n  high-resolution image synthesis. In Forty-first International Conference on Machine Learning,\n  2024a.\n\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\n  Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English,\n  Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow trans-\n  formers for high-resolution image synthesis, 2024b. URL https://arxiv.org/abs/2403.\n  03206.\n\nDan Friedman and Adji Bousso Dieng. The vendi score: A diversity evaluation metric for machine\n   learning. arXiv preprint arXiv:2210.02410, 2022.\n\nGeorgi Gerganov. GitHub - ggerganov/llama.cpp: LLM inference in C/C++ — github.com. https:\n  //github.com/ggerganov/llama.cpp, 2023.\n\nTeam GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego\n  Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie\n  Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li, Lei Zhao, Lindong\n  Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi\n  Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao\n  Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song,\n  Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao\n  Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang.\n  Chatglm: A family of large language models from glm-130b to glm-4 all tools, 2024. URL\n  https://arxiv.org/abs/2406.12793.\n\nAditi Godbole, Jabin Geevarghese George, and Smita Shandilya. Leveraging long-context large\n  language models for multi-document understanding and summarization in enterprise applications,\n  2024. URL https://arxiv.org/abs/2409.18454.\n\nGoogle. gemini-2.0-flash-preview-image-generation. Accessed: 2025-09.\n\n\n                                       12\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation.\n  In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:\n  //openreview.net/forum?id=BsZNWXD3a1.\n\nJohn A Hartigan, Manchek A Wong, et al. A k-means clustering algorithm. Applied statistics, 28(1):\n  100–108, 1979.\n\nYutong He, Alexander Robey, Naoki Murata, Yiding Jiang, Joshua Nathaniel Williams, George J.\n  Pappas, Hamed Hassani, Yuki Mitsufuji, Ruslan Salakhutdinov, and J. Zico Kolter. Automated\n  black-box prompt engineering for personalized text-to-image generation. Trans. Mach. Learn.\n  Res., 2025, 2025. URL https://openreview.net/forum?id=IVYVDN6pJ6.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochre-\n   iter. Gans trained by a two time-scale update rule converge to a local nash equilibrium.  In\n   I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\n   nett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-\n   ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/\n  2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf.\n\nYu-Guan Hsieh, Cheng-Yu Hsieh, Shih-Ying Yeh, Louis B´ethune, Hadi Pour Ansari, Pavan Ku-\n  mar Anasosalu Vasu, Chun-Liang Li, Ranjay Krishna, Oncel Tuzel, and Marco Cuturi. Graph-\n  based captioning: Enhancing visual descriptions by interconnecting region captions, 2024. URL\n  https://arxiv.org/abs/2407.06723.\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n  Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022.\n\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\n  Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali\n  Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/\n  zenodo.5143773.\n\nHamed Jelodar, Yongli Wang, Chi Yuan, Xia Feng, Xiahui Jiang, Yanchao Li, and Liang Zhao.\n  Latent dirichlet allocation (lda) and topic modeling: models, applications, a survey, 2018. URL\n  https://arxiv.org/abs/1711.04305.\n\nSeunghun Lee, Jihoon Lee, Chan Ho Bae, Myung-Seok Choi, Ryong Lee, and Sangtae Ahn. Op-\n  timizing prompts using in-context few-shot learning for text-to-image generative models. IEEE\n  Access, 12:2660–2673, 2024. doi: 10.1109/ACCESS.2023.3348778.\n\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei\n   Li, Ziwei Liu, and Chunyuan Li.  Llava-onevision: Easy visual task transfer. arXiv preprint\n  arXiv:2408.03326, 2024a.\n\nZhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang,\n  Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen\n  Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan\n   Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue,\n  Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao\n  Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang,\n  and Qinglin Lu. Hunyuan-dit: A powerful multi-resolution diffusion transformer with fine-grained\n  chinese understanding, 2024b. URL https://arxiv.org/abs/2405.08748.\n\nZongyu Lin, Wei Liu, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai,\n  Liangchen Song, Bowen Zhang, et al. Stiv: Scalable text and image conditioned video generation.\n  arXiv preprint arXiv:2412.07730, 2024.\n\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching\n   for generative modeling. arXiv preprint arXiv:2210.02747, 2022.\n\nFeng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang,\n  Qixiang Ye, and Fang Wan. Timestep embedding tells: It’s time to cache for video diffusion model.\n  In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 7353–7363, 2025.\n\n\n                                       13\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.   Visual instruction tuning.\n  In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Ad-\n  vances in Neural Information Processing Systems, volume 36, pp. 34892–34916. Curran Asso-\n   ciates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/\n  2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.\n  Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https:\n  //llava-vl.github.io/blog/2024-01-30-llava-next/.\n\nShihong Liu, Samuel Yu, Zhiqiu Lin, Deepak Pathak, and Deva Ramanan. Language models as\n  black-box optimizers for vision-language models. In IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 12687–12697.\n  IEEE, 2024b. doi: 10.1109/CVPR52733.2024.01206. URL https://doi.org/10.1109/\n  CVPR52733.2024.01206.\n\nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and\n   transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.\n\nI Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts.  In\n  International Conference on Learning Representations, 2017. URL https://openreview.\n  net/forum?id=Skq89Scxx.\n\nOscar Ma˜nas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya\n  Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving text-to-image consistency\n  via automatic prompt optimization, 2024. URL https://arxiv.org/abs/2403.17804.\n\nRada Mihalcea and Paul Tarau. TextRank: Bringing order into text. In Dekang Lin and Dekai Wu\n   (eds.), Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,\n  pp. 404–411, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL\n  https://aclanthology.org/W04-3252.\n\nWenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, and Qing Yang. Dynamic prompt\n  optimizing for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition, pp. 26627–26636, 2024.\n\nnarugo1992. Ai-corrupt score for anime images. https://huggingface.co/deepghs/ai_\n  image_corrupted, 2023.\n\nnyanko202. Danbooru2023: A large-scale crowdsourced and tagged anime illustration dataset.\n  https://huggingface.co/datasets/nyanko7/danbooru2023, 2023.\n\nOpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.\n\nMaxime Oquab, Timoth´ee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov,\n   Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao\n  Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran,\n  Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut,\n  Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision,\n  2023.\n\nJuan Ossa, Eren Do˘gan, Alex Birch, and F Johnson. Improvements to sdxl in novelai diffusion v3.\n  arXiv preprint arXiv:2409.15997, 2024.\n\nSang Hyun Park, Jun Young Koh, Junha Lee, Joy Song, Dongha Kim, Hoyeon Moon, Hyunju\n  Lee, and Min Song.  Illustrious: an open advanced illustration model, 2024. URL https:\n  //arxiv.org/abs/2409.19946.\n\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe\n  Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image\n   synthesis. In The Twelfth International Conference on Learning Representations, 2024. URL\n  https://openreview.net/forum?id=di52zR8xgf.\n\n\n                                       14\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nQi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang\n  Liu, et al. Lumina-image 2.0: A unified and efficient image generative framework. arXiv preprint\n  arXiv:2503.21758, 2025.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n  Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n  models from natural language supervision. In International conference on machine learning, pp.\n  8748–8763. PMLR, 2021.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\n  Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\n  transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020a.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\n  Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\n  transformer. Journal of machine learning research, 21(140):1–67, 2020b.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\n  Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang\n   (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of\n  Proceedings of Machine Learning Research, pp. 8821–8831. PMLR, 18–24 Jul 2021.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n  conditional image generation with clip latents, 2022. URL https://arxiv.org/abs/2204.\n  06125.\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\n   resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\n  ence on Computer Vision and Pattern Recognition (CVPR), pp. 10684–10695, June 2022a.\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\n   resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\n  ence on computer vision and pattern recognition, pp. 10684–10695, 2022b.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n  Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J\n   Fleet, and Mohammad Norouzi.  Photorealistic text-to-image diffusion models with deep lan-\n  guage understanding. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh\n   (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 36479–36494. Curran\n  Associates, Inc., 2022.\n\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022.\n  URL https://arxiv.org/abs/2202.00512.\n\nAxel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach.\n  Fast high-resolution image synthesis with latent adversarial diffusion distillation, 2024. URL\n  https://arxiv.org/abs/2403.12015.\n\nZhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu. Improving image captioning with better use of\n   captions, 2020. URL https://arxiv.org/abs/2006.11807.\n\nGeorge Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Leigh Ross, Valentin Vil-\n   lecroze, Zhaoyan Liu, Anthony L. Caterini, Eric Taylor, and Gabriel Loaiza-Ganem. Expos-\n  ing flaws of generative model evaluation metrics and their unfair treatment of diffusion mod-\n   els.  In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\n  https://openreview.net/forum?id=08zf7kTOoh.\n\nKeith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. Exploring topic coherence\n  over many models and many topics.  In Jun’ichi Tsujii, James Henderson, and Marius Pas¸ca\n   (eds.), Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language\n  Processing and Computational Natural Language Learning, pp. 952–961, Jeju Island, Korea,\n  July 2012. Association for Computational Linguistics. URL https://aclanthology.org/\n  D12-1087.\n\n\n                                       15\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nSaba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael G¨unther, Bo Wang, Markus Krimmel,\n  Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, and Han Xiao. jina-embeddings-\n  v3: Multilingual embeddings with task lora. CoRR, abs/2409.10173, 2024.\n\nsuccintly.    text2image-prompt-generator.   https://huggingface.co/succinctly/\n  text2image-prompt-generator, 2022.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\n  Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n   Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n  models, 2023a. URL https://arxiv.org/abs/2302.13971.\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\n  Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\n   tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\n  Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\n  Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\n  Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\n  Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\n  Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\n  Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\n  Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\n  Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\n  Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n  2023b. URL https://arxiv.org/abs/2307.09288.\n\nChenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan\n  Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation.\n  arXiv preprint arXiv:2506.18871, 2025.\n\nBin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu,\n  and Lu Yuan. Florence-2: Advancing a unified representation for a variety of vision tasks. In\n  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n  4818–4829, 2024.\n\nEnze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li,\n  Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion\n  transformers. arXiv preprint arXiv:2410.10629, 2024.\n\nShih-Ying Yeh. HakuBooru: text-image dataset maker for anime-style images. https://github.\n  com/KohakuBlueleaf/HakuBooru, 2024a.\n\nShih-Ying Yeh.  danbooru2023-webp-4Mpixel. https://huggingface.co/datasets/\n  KBlueLeaf/danbooru2023-webp-4Mpixel, 2024b.\n\nMingyang Yi, Aoxue Li, Yi Xin, and Zhenguo Li. Towards understanding the working mechanism of\n  text-to-image diffusion model. Advances in Neural Information Processing Systems, 37:55342–\n  55369, 2024.\n\nYing Zhao and George Karypis. Evaluation of hierarchical clustering algorithms for document\n   datasets. In Proceedings of the Eleventh International Conference on Information and Knowledge\n  Management, CIKM ’02, pp. 515–524, New York, NY, USA, 2002. Association for Computing\n  Machinery. ISBN 1581134924. doi: 10.1145/584792.584877. URL https://doi.org/10.\n  1145/584792.584877.\n\nWendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong,\n  Ming Ding, and Jie Tang. Cogview3: Finer and faster text-to-image generation via relay diffusion,\n  2024. URL https://arxiv.org/abs/2403.05121.\n\n\n\n\n\n                                       16\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                        Appendix\n\n\n\nTABLE OF CONTENTS\n\n\nA  Dataset/Resource                                                            19\n\n   A.1  Danbooru2023  .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   19\n\n   A.2  GBC10M .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   19\n\n   A.3  Coyo HD 11M  .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   19\n\nB   Baselines/T2I models                                                         19\n\n   B.1  Prompt-Optimization Baselines    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   19\n\n   B.2  T2I Models   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   20\n\nC  TIPO Implementation Details                                                   21\n\n   C.1  TIPO Training Data Construction .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   21\n\n   C.2  TIPO Training Settings and Model Configurations   .    .    .    .    .    .    .    .    .    .    .   22\n\n   C.3  TIPO Inference Settings  .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   23\n\n   C.4  Impact of Model Size on Performance   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   24\n\n   C.5  Impact of Model Size on Inference Speed   .    .    .    .    .    .    .    .    .    .    .    .    .    .   24\n\nD  Evaluation Statistics                                                          25\n\n   D.1  In-domain test regarding scenery tag .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   25\n\n   D.2  In-domain prompt generation test .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   26\n\n   D.3  Out-of-domain evaluation    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   28\n\n   D.4  Ablation Test     .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   31\n\nE  TIPO example                                                               32\n\nF  Image Examples                                                             35\n\n    F.1   In-domain test regard to scenery tag  .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   35\n\n    F.2   In-domain prompt generation test .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   35\n\nG  Human Preference                                                           36\n\n   G.1  User Interface for Human Preference Evaluation  .    .    .    .    .    .    .    .    .    .    .    .   39\n\n   G.2  Extended Human Evaluation.   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   39\n\n   G.3  ELO Ratings.    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   39\n\n   G.4  Human Preference ELO Method   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   39\n\n\n\n                                       17\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n   G.5   Statistical Significance    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   40\n\n   G.6  Survey Response Examples .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   41\n\n   G.7  Conclusion   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   44\n\nH  Ablation Study on TIPO                                                       45\n\n   H.1  Experimental Setup    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   45\n\n   H.2  Evaluation Metrics.    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   45\n\n   H.3  Results & Discussion .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   46\n\n   H.4  Speed Test and Overhead Analysis    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   46\n\n   H.5  Conclusion of Ablation   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   47\n\nI   Topic Distribution Visualization                                                 47\n\nJ   Discussion and Future Work                                                    50\n\nK  Disclosure of LLM Usage                                                     50\n\n\n\n\n\n                                       18\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nA  DATASET/RESOURCE\n\nA.1  DANBOORU2023\n\nThe Danbooru2023 dataset (Yeh, 2024b;a; nyanko202, 2023) is an extensive collection of images\nand their corresponding tags, compiled from the Danbooru image board. This dataset includes\nimages annotated with particular and detailed tags, providing a rich resource for training both the\nText-to-Image (T2I) and Large Language Models (LLMs) involved in the TIPO framework. The\ndataset contains data up to image ID 7,349,999, encompassing various visual content with granular\nannotations. These annotations allow for creating nuanced and precise prompts, ensuring that longer,\nmore detailed prompts can indicate subsets of shorter prompts.\n\n\nKey Characteristics:\n\n        • Rich Annotations: Detailed tags differentiate subtle variations, crucial for specific image\n         generation.\n\n        • Large Volume: Extensive dataset size ensures diverse training examples.\n\n        • Tag-Based Prompting: Refined prompts from detailed tags enhance image generation\n         accuracy.\n\n\nA.2  GBC10M\n\nThe GBC10M dataset (Hsieh et al., 2024) is a large-scale collection of 10 million images sourced from\nCC12M (Changpinyo et al., 2021), annotated using the Graph-Based Captioning (GBC) approach.\nEach image is represented by a graph where nodes correspond to object regions, compositions, and\nrelations, and edges define their hierarchical relationships. Annotations are generated automatically\nthrough a pipeline leveraging pretrained multimodal large language models (MLLM) and object\ndetection tools. The GBC structure enhances traditional image captions by providing detailed\ndescriptions and structural information. Data is provided in JSON lines format, including image\nURLs, bounding boxes, and captions.\n\nIn TIPO, only the root node captions from GBC10M are utilized for concise yet descriptive prompts.\n\n\nA.3  COYO HD 11M\n\nThe Coyo HD 11M dataset (CaptionEmporium, 2024) consists of 11.4 million high-resolution,\nhigh-concept-density images paired with 22.8 million synthetic captions generated from the Coyo-\n700M dataset. Images maintain a minimum of 512 pixels on the shortest edge to ensure high visual\nquality. Captions, generated with the LLaVA-Next-8B model (Liu et al., 2024a) based on LLaMA\n3 (AI@Meta, 2024), undergo post-processing for conciseness and clarity.\n\nTIPO uses short and long captions, booru tags, and open image tags from this dataset.\n\n\nB  BASELINES/T2I MODELS\n\nB.1  PROMPT-OPTIMIZATION BASELINES\n\nGPT-4o-mini.  GPT-4o-mini(OpenAI, 2024) is a multimodal model introduced by OpenAI in July\n2024 as a cost-efficient variant of GPT-4o. It supports a 128k-token context window and up to 16k\noutput tokens, trained on text and vision data. In this paper, it serves as a zero-shot rewriting baseline\nfor prompt refinement.\n\n\nMagicPrompt.  MagicPrompt(daspartho,  2022)  fine-tunes GPT-2  models  on  large-scale,\ncommunity-collected prompts (e.g., from Lexica.art and the Stable Diffusion Prompts dataset).\nIt learns to generate extended prompts with richer descriptive content, originally designed to improve\nprompt quality for Stable Diffusion.\n\n\n                                       19\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nPromptist.  Promptist(Hao et al., 2023) is a reinforcement learning framework for prompt optimiza-\ntion. Starting from a seed prompt, it generates refined prompts using supervised pre-training and\nreinforcement learning, guided by aesthetic and semantic alignment rewards. It consistently improves\nmodel-preferred prompt quality when paired with diffusion backbones.\n\nGemini-2.0-Flash-Image.  Gemini-2.0-Flash-Image4 is a variant of Google’s Gemini 2.0 Flash\nfamily with native image generation support. It provides both prompt-refinement and text-to-image\ngeneration within a single model, supporting high-fidelity text rendering, compositional control, and\niterative editing. Unlike other baselines, it functions as both optimizer and generator.\n\n\nB.2  T2I MODELS\n\nSDXL  Stable Diffusion XL (SDXL) (Podell et al., 2024) improves upon earlier models (Rombach\net al., 2022b) with a more considerable UNet backbone and dual text encoders (CLIP ViT-L (Radford\net al., 2021) and OpenCLIP ViT-bigG (Ilharco et al., 2021)), enhancing text conditioning. Supporting\nresolutions up to 1024×1024, SDXL accepts natural language prompts and tags, suitable for diverse\nimage generation.\nIn this paper, three SDXL models are used without the refiner model:5 SDXL-base-1.06 , Illustrious\nv3.5 - vpred7, and Kohaku-Zeta8.\n\nIllustrious   Illustrious is a series of fine-tuned Stable Diffusion XL models primarily trained on\nthe Danbooru2023 dataset. In this study, we specifically employ the v3.5 version variant with v-\nparameterization (Salimans & Ho, 2022), which is notable for its extensive incorporation of natural\nlanguage prompts. The inclusion of both tag-based and natural language formats allows Illustrious to\nleverage a broad range of semantic knowledge for image generation.\n\nWithin TIPO, we perform an ablation study to analyze the effectiveness of different prompting\nstrategies—namely extended tags versus natural language prompts—to identify which approach\ncontributes most significantly to enhanced image generation performance.\n\nStable Diffusion 3.5  Stable Diffusion 3.5 (SD-3.5) incorporates the MMDiT architecture (Esser\net al., 2024a) and the Rectified Flow formulation (Liu et al., 2022; Albergo & Vanden-Eijnden,\n2022; Lipman et al., 2022) for improved text-to-image generation. Utilizing triple text encoders\n(CLIP/ViT-L, OpenCLIP/ViT-G, T5-XXL (Raffel et al., 2020b)), SD-3.5 supports resolutions up to\n1024×1024 and uses a 50/50 mix of original and CogVLM-generated captions. Figures confirm the\ncapability to process both natural language prompts and tags.\nThis study employs SD-3.5-Large9 (8B parameters) with FP8 inference on RTX 3090 or RTX 4090\nGPUs.\n\nFLUX.1-dev  FLUX.1-dev is an open-weights text-to-image model released by Black Forest Labs\nas a guidance-distilled variant of their FLUX.1 family, targeting research and non-commercial\nuse (black-forest labs, 2024). Architecturally, FLUX adopts a transformer-based rectified-flow formu-\nlation (Lipman et al., 2022) with a hybrid stack of multimodal/parallel diffusion transformer blocks\nand rotary position encodings, scaled to ∼12B parameters, emphasizing prompt adherence, typog-\nraphy, and aspect-ratio flexibility (black-forest labs, 2024). We use the FLUX.1-dev checkpoint\nfor T2I without additional refiners; it accepts both natural-language prompts and tag-like inputs and\nsupports resolutions in the 0.1–2.0 MP range.10\n\nOmniGen2  OmniGen2 is a unified multimodal generator that decouples autoregressive text mod-\neling from diffusion-based image generation via two distinct pathways with unshared parameters\n\n  4https://developers.googleblog.com/experiment-with-gemini-20-flash-native-image-generation/\n  5https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0\n  6https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n  7OnomaAIResearch/Illustrious-xl-early-release-v0\n  8https://huggingface.co/KBlueLeaf/Kohaku-XL-Zeta\n  9https://huggingface.co/stabilityai/stable-diffusion-3.5-large\n  10https://huggingface.co/black-forest-labs/FLUX.1-dev\n\n\n                                       20\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n(Wu et al., 2025). The diffusion side conditions on hidden states from the MLLM while exclusively\nfeeding VAE features into the diffusion decoder to preserve low-level fidelity; a 3D rotary scheme\n(Omni-RoPE) disentangles sequence ID and 2D spatial coordinates to stabilize editing and in-context\ngeneration (Wu et al., 2025). Beyond standard T2I, OmniGen2 natively supports image editing and\nsubject-driven in-context generation, and introduces a reflection mechanism/dataset to iteratively\nrefine outputs.\n\nLumina-Image 2.0  Lumina-Image 2.0 proposes a unified and efficient T2I framework built on\nUnified Next-DiT—a single-stream DiT that performs joint self-attention over text and image to-\nkens—paired with a task-tailored Unified Captioner (UniCap) that produces multi-granularity, multi-\nlingual captions for training (Qin et al., 2025). The model employs Multimodal RoPE, progressive\nresolution training (256→1024), and efficient inference (CFG-Renorm/Trunc (Lin et al., 2024; Yi\net al., 2024), Flow-DPM-Solver (Xie et al., 2024), TeaCache (Liu et al., 2025)) to improve prompt-\nfollowing and speed at only ∼2.6B parameters (Qin et al., 2025).\n\nHiDream-I1  HiDream-I1 is a 17B-parameter image foundation model based on a sparse Diffusion\nTransformer with dynamic Mixture-of-Experts (MoE) (Cai et al., 2025). It employs a dual-stream\n(text/image) sparse DiT for separate encoding followed by a single-stream sparse DiT to fuse\nmodalities efficiently; hybrid text encoders (e.g., CLIP-L/14, CLIP-G/14, T5-XXL) and an LLM\naggregator provide robust conditioning (Cai et al., 2025). The suite includes I1-Full (50+ steps),\nI1-Dev (guidance-distilled, 28 steps), and I1-Fast (14 steps), with a GAN-powered diffusion\ndistillation to retain sharpness at low step counts.\n\nC  TIPO IMPLEMENTATION DETAILS\n\nIn this appendix, we provide all the necessary details including our dataset construction process,\nmodel configurations, inference pipeline, and the model’s properties not mentioned in Section 4.2\nand 4.3.\n\nC.1  TIPO TRAINING DATA CONSTRUCTION\n\nThis section details our methodology for constructing and preprocessing training data to ensure robust\nmodel performance across various input scenarios.\n\nLength Control  To systematically control output prompt length, we implement a structured length\ncategorization system using unique length tags. These tags enforce specific constraints on tag counts\nand natural language sentence lengths. For instance, the <long> tag specifies that the corresponding\nprompt must contain between 36 and 52 tags (inclusive), accompanied by 4 to 8 sentences of natural\nlanguage description. We define four distinct length categories, each with strict bounds for tag count\nand sentence length.\n\n\n               Type          Very Short   Short  Long  Very Long\n\n                Tags (count)        18       36     48       72\n           NL (sentences)       2        4      8        18\n\nTable 7: Maximum length specifications for each category and caption type. For each category, the\nactual count/length must not exceed these values.\n\n\nRandom Augmentation  To enhance input diversity and better simulate real-world usage patterns,\nwe implement several data augmentation strategies:\n\n        • Metadata Tags: For tags representing image metadata (e.g., artist, character, aspect ratio),\n      we employ two randomization techniques:\n         – Random removal of metadata tags\n         – Random repositioning of metadata tags to the end of the prompt, after all content-related\n              descriptions\n\n\n                                       21\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                     TIPO-100M    TIPO-200M stage1  TIPO-200M stage2    TIPO-500M\n\n   Architecture                              LLaMA\n  Type                        Pretrain              Pretrain             Finetune             Pretrain\n  Vocab Size                                         32013\n  Hidden Dim               640              768                        -               1280\n   Attention Heads            10               12                         -                20\n  MLP Dim                2240             2304                       -               3840\n  Hidden Layers             10               20                         -                20\n  Model Parameters        100M          203M                       -            508M\n\n  Max Learning Rate          5e-4               2e-4                5e-5               2e-4\n   Optimizer                              AdamW\n  LR scheduler                                   Cosine Annealing LR\n   betas                                                           0.9, 0.98\n   weight decay                                             0.01\n\n   Dataset              Coyo, GBC, Dan     GBC, Dan       Coyo, GBC, Dan   Coyo, GBC, Dan\n  Epoch                    1                5                 3                5\n  max context length         512              512              1024             1024\n   global batch size           1024             2048              2048             3584\n  Token Seen              6.0240B          22.625B           18.339B          31.274B\n  Hardware             4 × RTX3090      4 × RTX3090       4 × RTX3090        8 × H100\n   Training Time (wall)      22.5 hour          150 hour           270 hour          100 hour\n\nTable 8: Training settings for TIPO models. The datasets include CoyoHD11M (Coyo), GBC10M\n(GBC), and Danbooru2023 (Dan). Stage 2 additionally incorporates Pixtral (Agrawal et al., 2024) to\ngenerate NL captions from Danbooru2023 dataset.\n\n\n        This approach encourages the model to handle varying metadata positions and availability,\n        while maintaining the ability to infer metadata relationships from content descriptions.\n        • Content Tags: For tags describing image content (e.g., objects, actions, attributes), we\n        implement:\n         – Random shuffling of tag order within the content section\n         – Length-based truncation to meet target length constraints while preserving key content\n             information\n        • Natural Language: For natural language descriptions exceeding length limitations, we\n       employ selective sentence removal, targeting middle sentences to preserve context-setting\n        opening sentences and concluding details. This maintains coherent narrative flow while\n        meeting target length requirements.\n\nThese augmentation strategies create a more diverse training dataset that better reflects real-world\nprompt variations, improving the model’s robustness and adaptability to different input styles and\nformats.\n\nC.2  TIPO TRAINING SETTINGS AND MODEL CONFIGURATIONS\n\nTokenizer and Task Tokens  TIPO employs a vocabulary derived from LLaMA2 (Touvron et al.,\n2023b) consisting of 32,000 tokens, with additional tokens (13 tokens) specifically designated for\ntask and length control or placeholders. This extended vocabulary includes task identifiers and length\nmodifiers to ensure flexibility across different prompt types:\n\n        • Placeholder Token (1 token):\n\n          <|empty|>\n\n        • Task Tokens (8 tokens):\n\n          <|gen_meta|>, <|tag_to_long|>, <|short_to_tag|>,\n          <|long_to_tag|>, <|short_to_long|>, <|short_to_tag_to_long|>,\n          <|short_to_long_to_tag|>, <|tag_to_short_to_long|>\n\n        • Length Tokens (4 tokens):\n\n\n                                       22\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nFigure 6: TIPO inference workflow, with solid arrows denoting the primary generation steps and\ndashed arrows indicating alternative generation paths within the same cycle. <TOKEN> represents\nspecial tokens, with all tokens detailed in Section C.2.\n\n\n          <|very_short|>, <|short|>, <|long|>, <|very_long|>\n\nOptimizer and Learning Schedule  Training is performed using the AdamW optimizer (Loshchilov,\n2017), with a cosine annealing learning rate scheduler (Loshchilov & Hutter, 2017). The optimizer\nparameters include β1 = 0.9, β2 = 0.98, and a weight decay of 0.01. Maximum learning rates are\nadjusted per model size, as outlined in Table 8.\n\nTraining Configurations  TIPO models are trained in multiple stages. Table 8 summarizes the\nconfigurations for pretraining and fine-tuning TIPO-100M, TIPO-200M, and TIPO-500M. Both\npretraining and fine-tuning was conducted on datasets like Danbooru2023 (nyanko202, 2023),\nGBC10M (Hsieh et al., 2024), and CoyoHD11M (CaptionEmporium, 2024).\n\nAugmented Task Representation  Each dataset entry undergoes random task assignment and\nsplitting to simulate a wide range of input-output mappings, effectively increasing the dataset size.\nFor example, a single entry may contribute to tasks like short to tag or tag to long, with\nlength modifiers dynamically controlling the output verbosity. This approach ensures the model can\nhandle diverse tasks while maintaining robust generalization.\n\nHardware and Time Requirements  Training was conducted on NVIDIA RTX3090 GPUs for\nsmaller models and H100 GPUs for TIPO-500M. Total wall-clock training times ranged from 22.5\nhours for TIPO-100M to 270 hours for fine-tuning TIPO-200M.\n\nToken Seen and Effective Training  Non-padding tokens are used to measure the effective token\ncount during training, ensuring efficiency given the short and variable data lengths. Table 8 details\nthe total tokens seen per model and training stage, illustrating the comprehensive exposure to diverse\ndata entries.\n\n\nC.3  TIPO INFERENCE SETTINGS\n\nSampling Strategy  We employ a hybrid stochastic decoding strategy combining nucleus sampling\n(top-p = 0.95) and top-k = 60 filtering. This follows standard practice in open-ended text generation,\nas adopted in the official Hugging Face generation examples 11. This hybrid approach maintains\ndiversity while preserving coherence, preventing both overly deterministic and excessively noisy\ngenerations.\n\nInference Pipeline  The TIPO inference pipeline is designed to handle various input types and\nscenarios, combining different tasks to refine or expand both tag-based and natural language prompts.\nFigure 6 illustrates this comprehensive workflow. Our framework processes tags and natural language\n\n  11Hugging Face. “Usage — transformers 2.11.0 documentation.” Example of text generation with XLNet uses\ntop-p = 0.95 and top-k = 60. https://huggingface.co/transformers/v2.11.0/usage.html\n\n\n                                       23\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\ninputs separately, allowing for specialized handling of each input type. This flexible pipeline allows\nTIPO to adapt to various input scenarios, whether the user provides tags, natural language descriptions,\nor both. By leveraging different task combinations, TIPO ensures that tag-based and natural language\nprompts are optimized, resulting in more detailed and effective input for text-to-image models.\n\n\nC.4  IMPACT OF MODEL SIZE ON PERFORMANCE\n\nTo analyze the impact of model scales on prompt-optimization performance, we compare TIPO-200M\nand TIPO-500M using a 1,000-image subsample from the COYO and GBC datasets. Results are\nshown in Table 9.\n\n  Table 9: Prompt optimization performance of TIPO-200M and TIPO-500M on a 1k subsample.\n\n\n                  Metric          Task     TIPO-200M     TIPO-500M\n\n             FDD (↓)        NL-short       0.1529            0.1356\n                                NL-long        0.1650            0.1398\n                    Aesthetic (↑)    NL-short   5.8531 ± 0.7501   5.8943 ± 0.7064\n                                NL-long   5.8364 ± 0.7501   5.9030 ± 0.7015\n                AI Corrupt (↓)   NL-short   0.2870 ± 0.4167   0.2891 ± 0.4189\n                                NL-long   0.2870 ± 0.4150   0.2862 ± 0.4151\n\n\nOverall, TIPO-500M shows consistent gains in FDD and Aesthetic scores, while performance on\nAI Corrupt remains comparable. However, the larger 500M variant entails substantially higher\ncomputational cost without delivering proportionally greater benefits, which limits its practicality for\ncommunity use; hence, all main experiments are conducted with TIPO-200M.\n\n\nC.5  IMPACT OF MODEL SIZE ON INFERENCE SPEED\n\nWe conducted comprehensive speed tests of our 100M, 200M and 500M parameter models using\nthe inference pipeline described in Section 5.2. Each prompt requires two sequential generation\nsteps. Our primary metric is average tokens generated per second, which reflects real-world task\nperformance rather than theoretical maximum throughput.\n\nThe evaluation was performed using llama.cpp (Gerganov, 2023), an efficient C++ implementation\nthat provides optimized support for various hardware accelerators, including CUDA, HIP, and Apple\nMetal.\n\n\n\n\n\n                                       24\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                               TIPO-100M      TIPO-200M      TIPO-500M\n\n           Hardware Platform       tok/sec  gen time   tok/sec  gen time   tok/sec  gen time\n\n        M1 Max (32 GPU cores)   339.4     0.66     190.0     1.23     119.4     2.02\n         RTX 3090                558.5     0.42     341.4     0.69     289.8     0.81\n         RTX 4090                742.9     0.29     454.5     0.51     359.7     0.63\n\nTable 10: Model performance comparison across different hardware platforms. Tokens per second\n(tok/sec) represents the average generation speed, while generation time (gen time) shows the average\ntime in seconds required for a complete two-step prompt optimization process.\n\n\nD  EVALUATION STATISTICS\n\nIn this appendix, we provide more statistics for the result obtained in Section 5.\n\n\nD.1  IN-DOMAIN TEST REGARDING SCENERY TAG\n\n\n\n\n\n                        (a) The box plot for the Aesthetic Score result of scenery tag test.\n\n\n\n\n\n                      (b) The box plot for the AI Corrupt Score result of scenery tag test.\n\n\n\n\n\n         (c) The KDE plot for the Aesthetic Score re-   (d) The KDE plot for the AI Corrupt Score\n         sult of scenery tag test.                            result of scenery tag test.\n\n         Figure 7: The distribution of aesthetic and AI corrupt score for scenery tag test.\n\n\n                                       25\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nThe box plot and Kernel Density Estimation (KDE) plot displayed in Figure 7 illustrate the aesthetic\nscores and AI corruption scores from the scenery tag test described in Section 5.2. The analysis\nshows that TIPO significantly outperforms all other methods, demonstrating a considerable margin of\nimprovement.\n\n\n\n\n\nD.2  IN-DOMAIN PROMPT GENERATION TEST\n\n\n\n\n\n                       (a) The box plot for the Aesthetic Score result of short prompt input.\n\n\n\n\n\n                     (b) The box plot for the AI Corrupt score result of short prompt input.\n\n\n\n\n\n   (c) The KDE plot for the Aesthetic Score result           (d) The KDE plot for the AI Corrupt Score result\n   of short prompt input.                                  of short prompt input.\n\n\n        Figure 8: The distribution of aesthetic and AI corrupt score for short prompt input.\n\n\n                                       26\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                  (a) The box plot for the Aesthetic Score result of truncated long prompt input.\n\n\n\n\n\n                (b) The box plot for the AI Corrupt score result of truncated long prompt input.\n\n\n\n\n\n   (c) The KDE plot for the Aesthetic Score result           (d) The KDE plot for the AI Corrupt Score result\n   of truncated long prompt input.                         of truncated long prompt input.\n\n   Figure 9: The distribution of aesthetic and AI corrupt score for truncated long prompt input.\n\n\n\n\n\nFigures 8 and 9 display the box plots and KDE plots of aesthetic scores and AI corruption scores\nobtained from the In-domain prompt generation test detailed in Section F.2. While the box plots\nreveal subtle differences in performance between various methods, the AI corruption scores provide\nvaluable insights. Specifically, these scores indicate that implementations supported by TIPO produce\nmore stable output images than other methods.\n\n\n                                       27\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nD.3  OUT-OF-DOMAIN EVALUATION\n\n\n\n\n\n                        (a) The box plot for the Aesthetic Score result of out-of-focus test.\n\n\n\n\n\n                      (b) The box plot for the AI Corrupt Score result of out-of-focus test.\n\n\n\n\n\n (c) The KDE plot for the Aesthetic Score result of       (d) The KDE plot for the AI Corrupt Score result of\n out-of-focus test.                                       out-of-focus test.\n\n        Figure 10: The distribution of aesthetic and AI corrupt score for out-of-focus test.\n\n\n\n\n\n                                       28\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                  (a) Original Caption                                   (b) Prompt by GPT4o-mini\n\n\n\n\n\n             (c) Prompt by MagicPrompt                                 (d) Prompt by Promtist\n\n\n\n\n\n                                                 (e) Prompt by TIPO\n\nFigure 11: The similarity matrix for the 100 best aesthetic results generated in the SD3.5-Large\nexperiments. Off-diagonal elements of the matrix indicate the similarity between different images. A\nlower value for an off-diagonal element indicates greater diversity among the generated images.\n\n\n\n\n\n                                       29\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                  (a) Original Caption                                   (b) Prompt by GPT4o-mini\n\n\n\n\n\n             (c) Prompt by MagicPrompt                                 (d) Prompt by Promtist\n\n\n\n\n\n                                                 (e) Prompt by TIPO\n\nFigure 12: The similarity matrix between 100 images of worst aesthetic generated results of SD3.5-\nLarge experiments.\n\n\n\n\n\nFigures 11 and 12 present similarity matrices for different prompt generation methods and their\ncorresponding aesthetic outputs on SD3.5-Large (Esser et al., 2024b). A matrix with predominantly\nlower similarity values (brighter appearance) indicates high diversity among generated images, while\nhigher values (darker appearance) suggest consistent but less diverse outputs. Please refer to Table 2\nin Section 5.\n\n\n                                       30\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nD.4  ABLATION TEST\n\n\n\n\n\n                        (a) The box plot for the Aesthetic Score result of NL ablation test.\n\n\n\n\n\n                      (b) The box plot for the AI Corrupt Score result of NL ablation test.\n\n\n\n\n\n (c) The KDE plot for the Aesthetic Score result of       (d) The KDE plot for the AI Corrupt Score result of\n NL ablation test.                        NL ablation test.\n\n        Figure 13: The distribution of aesthetic and AI corrupt score for NL ablation test.\n\n\n\n\n\n                                       31\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                        (a) The box plot for the Aesthetic Score result of tags ablation test.\n\n\n\n\n\n                      (b) The box plot for the AI Corrupt Score result of tags ablation test.\n\n\n\n\n\n    (c) The KDE plot for the Aesthetic Score result of      (d) The KDE plot for the AI Corrupt Score result\n   tags ablation test.                                    of tags ablation test.\n\n        Figure 14: The distribution of aesthetic and AI corrupt score for tags ablation test.\n\n\nFigures 14 present the tag ablation test in the TIPO effect on the aesthetic score and AI Corrupt Score\namong the original tag, tag-extend and the tags TIPO. The box plot reveals that the tag TIPO is better\nthan the original tag and the tag extend is the best. In detail, KDE plot reveals that the tag TIPO has\na similar performance compared with the tag extend. Both of them are better than the original tag,\nwhich indicates that the tag TIPO aspect helps control corruption and promotes the aesthetic score.\n\nE  TIPO EXAMPLE\n\nIn this section, we provide some text example of TIPO’s input and output.\n\n\n\n\n\n                                       32\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n   TIPO Format Template\n\n   User Input:\n    1girl, ciloranko, maccha (mochancc), ningen mame, ask (askzy), solo, masterpiece,\n        absurdres, newest, safe\n\n\n    A girl sits in a cozy cafe, cradling a cup of coffee in her hand\n\n\n\n   Formatted TIPO Input for Expand Tags:\n    meta: absurdres\n    rating: safe\n    style: anime-style illustration, digital art, character design, fantasy concept art\n    quality: masterpiece, newest\n    aspect_ratio: 1.0\n    target: <|short|> <|short_to_tag|>\n    short: A girl sits in a cozy cafe, cradling a cup of coffee in her hand\n    tag: 1girl, solo\n\n\n\n   Formatted TIPO Output after Expand Tags and Expand Natural Prompt:\n    meta: absurdres\n    rating: safe\n    style: anime-style illustration, digital art, character design, fantasy concept art\n    quality: masterpiece, newest\n    aspect_ratio: 1.0\n    target: <|short|> <|tag_to_long|>\n    tag: 1girl, solo, sitting, closed mouth, jewelry, long hair, looking at viewer, crossed\n         legs, plant, table, couch, bracelet, cup, smile, teacup, indoors, blue eyes,\n        blonde hair, holding\n    long: A girl sits in a cozy cafe, cradling a cup of coffee in her hand. The cafe has\n        large windows with green plants on the walls and a wooden table in front of her.\n        Behind her is a staircase leading to another room. The overall atmosphere of the\n        image is serene and inviting.\n\n\n\n   Formatted Output for Text-to-Image:\n    1girl, ciloranko, maccha (mochancc),\n        ningen mame, ask (askzy),\n\n\n    solo, sitting, closed mouth, jewelry,\n        long hair, looking at viewer, crossed\n         legs, plant, table, couch, bracelet,\n         cup, smile, teacup, indoors, blue\n        eyes, blonde hair, holding,\n\n\n    A girl sits in a cozy cafe, cradling a\n        cup of coffee in her hand. Behind her\n         is a staircase leading to another\n        room. The cafe has large windows with\n         green plants on the walls and a\n        wooden table in front of her. The            User Input          TIPO Output\n        overall atmosphere of the image is\n        serene and inviting.\n\n\n    masterpiece, newest, absurdres, safe\n\n\n\n       Figure 15: An example of formatted content used for training and inference in TIPO.\n\n\n\n\n\n                                       33\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n   TIPO Format template\n\n\n   User Input:\n\n    scenery  ,  no humans ,  masterpiece  ,  absurdres  ,  newest  ,  s a f e\n\n\n\n   Formatted TIPO Input For Expand Tags:\n\n    meta: absurdres\n    rating: safe\n    quality: masterpiece, newest\n    aspect_ratio: 1.0\n    target: <|long|>\n    tag: scenery, no humans\n\n\n\n   Formatted TIPO Output after Expand Tags and tag to long task:\n\n    meta: absurdres\n    rating: safe\n    quality: masterpiece, newest\n    aspect_ratio: 1.0\n    target: <|long|> <|tag_to_long|>\n    tag: scenery, no humans, storefront, motor vehicle, road sign, power lines, plant,\n        railing, flower pot, vanishing point, outdoors, sign, potted plant, sidewalk,\n        awning, tree, bicycle, window, railroad crossing, bush, building, utility pole,\n        lamppost, shop, truck, traffic light, fence, chinese text, stairs, door, bicycle\n        basket, town, day, streetcar (cafe), lamp, road\n    long: A small town with a variety of buildings and houses. the sky is blue and there\n        are trees in the background. on the left side of the image, there is an orange\n        building with a sign that reads \"chinese restaurant\". on the right side, there are\n         several other buildings with different types of shops and restaurants. in front\n        of the buildings, there appears to be a street with cars parked along the road.\n\n\n    in the center of the illustration, we can see a train crossing signal with two red\n        lights and a blue sky above it. there is also a yellow building with white walls\n        and a green roof. on top of the traffic light pole, there seems to be an air\n        conditioning unit. the street is lined with trees and bushes, and there is\n        graffiti on the ground.\n\n\n\n   Formatted Output For Text-to-Image\n\n    scenery, no humans, storefront, motor vehicle, road sign, power\n        lines, plant, railing, flower pot, vanishing point, outdoors\n        , sign, potted plant, sidewalk, awning, tree, bicycle,\n        window, railroad crossing, bush, building, utility pole,\n        lamppost, shop, truck, traffic light, fence, chinese text,\n        stairs, door, bicycle basket, town, day, streetcar \\(cafe\\),\n         lamp, road,\n\n    A small town with a variety of buildings and houses. the sky is   User Input\n        blue and there are trees in the background. on the left side\n         of the image, there is an orange building with a sign that\n        reads \"chinese restaurant\". on the right side, there are\n        several other buildings with different types of shops and\n        restaurants. in front of the buildings, there appears to be\n        a street with cars parked along the road. in the center of\n        the illustration, we can see a train crossing signal with\n        two red lights and a blue sky above it. there is also a\n        yellow building with white walls and a green roof.\n\n                                                           TIPO Output    masterpiece, newest, absurdres, safe\n\n\n\n      Figure 16: An example formatted content we used for training and inference in TIPO.\n\n\n                                       34\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nF  IMAGE EXAMPLES\n\nIn this section, we present sample images from the experiments described in Section 5 to visually\ndemonstrate the improvements achieved by TIPO.\n\n\nF.1  IN-DOMAIN TEST REGARD TO SCENERY TAG\n\n\n\n\n\nFigure 17: Comparison of generated images using simple input (left) vs. TIPO-enhanced input (right)\nfor the scenery tag\n\n\nFigure 17 demonstrates the difference in output diversity between simple input and TIPO-enhanced\ninput for the scenery tag. As observed, TIPO significantly expands the range of generated sceneries,\nbetter reflecting the variety present in the Danbooru2023 dataset (Yeh, 2024b). The left column\nshows results from simple input (scenery tag only), while the right column illustrates the enhanced\ndiversity achieved with TIPO-enhanced input.\n\n\nF.2  IN-DOMAIN PROMPT GENERATION TEST\n\n\n\n\n\n                         (a) Short Caption                         (b) TIPO-Generated Caption\n\nFigure 18: Comparison of generated images using original input (left) vs. TIPO-enhanced input\n(right)\n\n\nFigure 18 illustrates the differences between short captions, truncated long captions, TIPO-generated\ncaptions, and TIPO-extended captions. The “short prompt” and “truncated long prompt” used in this\nexperiment typically consist of 1-2 sentences, resulting in reasonably good quality outputs. However,\nthe use of TIPO to refine or extend these prompts still yields noticeable improvements in aesthetics\nand overall quality.\n\n\n\n\n                                       35\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nG  HUMAN PREFERENCE\n\n\n\n\n\n                      (a) ELO rating on Illustrious                     (b) Win rate matrix on Illustrious\n\n\n\n\n\n                   (c) ELO rating on SD3.5-medium                (d) Win rate matrix on SD3.5-medium\n\nFigure 19: ELO ratings and win rate matrices across different experimental settings comparing five\nprompting methods (TIPO, Promptist, Promptextend, MagicPrompt, and Original) on three evaluation\ndimensions\n\n\n Comparison               Win Ratio (A:B)   Proportion A    p-value    Significant?\n  Original vs. PromptExtend             45:66            0.405        0.0572     Marginally\n  Original vs. Tipo                     28:80            0.259    < 0.0001     Yes***\n  Original vs. Promptist                 35:64            0.354        0.0046      Yes**\n PromptExtend vs. Promptist           41:80            0.339        0.0005      Yes***\n Promptist vs. Tipo                   30:101           0.229    < 0.0001     Yes***\n PromptExtend vs. Tipo                30:90            0.250    < 0.0001     Yes***\n MagicPrompt vs. Promptist            14:34            0.292        0.0055      Yes**\n MagicPrompt vs. Tipo                11:48            0.186    < 0.0001     Yes***\n MagicPrompt vs. Original             15:28            0.349        0.0660      No\n MagicPrompt vs. PromptExtend        19:35            0.352        0.0402       Yes*\n\nTable 11: Pairwise win rates and statistical significance (Overall Dimension). Significance levels: *\np < 0.05, ** p < 0.01, *** p < 0.001\n\n\nWe conducted a series of A/B tests to compare five prompt transformations, (TIPO, Promptist,\nPromptext, MagicPrompt, and Original(unmodified)), for two models, Illustrious, SD3.5-medium,\nwhich is known for both core word/natural language understanding. In total, we collected responses\nfor ∼1,500 pairwise comparisons, from more than 20 anonymous evaluators. Each evaluation asked\n\n\n                                       36\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                                             (a) Full Win-Tie-Lose plot\n\n\n\n\n\n                                        (b) Illustrious Win-Tie-Lose plot\n\n\n\n\n\n                                          (c) SD3.5-medium Win-Tie-Lose\n\nFigure 20: Win-Tie-Lose comparison across different experimental settings showing the relative\nperformance of five prompting methods on prompt adherence, image quality, and aesthetic appeal\nmetrics\n\n\n\n\n\n                                       37\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                            (a) The UI of survey system before submitting the choices.\n\n\n\n\n\n                           (b) The UI of survey system after submitting the choices.\n\nFigure 21: Survey interface for human evaluation of image pairs, showing the evaluation process\nbefore submission (a) where users compare two images based on four metrics, and after submission\n(b) where the generated prompts for each image are revealed\n\n\n\n\n\n                                       38\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nparticipants to compare two generated images—labeled A and B—and select which they preferred (or\na tie) according to specific criteria (e.g., prompt adherence, image quality, or aesthetic appeal).\n\n\nG.1  USER INTERFACE FOR HUMAN PREFERENCE EVALUATION\n\nWe developed a specialized survey interface to facilitate efficient and unbiased human evaluation\nof generated images. As illustrated in Figure 21, the interface presents evaluators with an original\nprompt and two corresponding images (labeled A and B) generated using different prompting methods.\nBefore submission, users can see the original prompt in the center panel while the processed prompts\nused to generate each image remain hidden to prevent bias.\n\nThe evaluation framework requires participants to compare the image pairs across four distinct\nmetrics: prompt adherence (how well the image follows the original prompt), image quality (detail\nand correctness), aesthetic appeal (color, composition, and style), and overall personal preference.\nFor each metric, users can select one of three options: “A is better,” “A and B are equal,” or “B is\nbetter.”\n\nWhen evaluators encounter image pairs that appear to be from different prompts or settings, they\nare instructed to click “Refresh” to obtain a new comparison. After submitting their evaluations, the\ninterface reveals the transformed prompts used to generate each image, providing transparency about\nhow the original prompt was modified by each method.\n\n\nG.2  EXTENDED HUMAN EVALUATION.\n\nParticipants assessed each image’s performance on prompt adherence, image quality, and aesthetic\nappeal, with visually shown unmodified and image pairs. TIPO exhibited superior outcomes in all\ncomparison settings. Notably, it attained a 64.4% peak win rate (against MagicPrompt) under the\nFull scenario and 57.5% (also against MagicPrompt) under SD35-medium, emphasizing TIPO’s\nproficiency in generating images that closely follow prompt specifications while maintaining visual\ncoherence.\n\n\nG.3  ELO RATINGS.\n\nWe computed theoretical ELO ratings from the aggregated pairwise comparisons to quantify overall\nperformance differences among the five methods. The rating update rules were based on each pair’s\nbinary outcome (win or lose), ignoring tie cases. The result is depicted in Figure 19, TIPO has\nsecured the highest ELO rating over other models.\n\n\nG.4  HUMAN PREFERENCE ELO METHOD\n\nWe computed theoretical ELO ratings from human-judged pairwise preference data to quantitatively\nevaluate the relative performance of each prompting method. The ELO rating system, initially\ndesigned for ranking chess players, aggregates binary outcomes into numerical ratings representing\ncomparative performance.\n\nPairwise Outcomes.  Human evaluators assessed comparisons between methods, resulting in one\nof three outcomes:\n\n        • Method i wins: assigned a score of 1 for method i, and 0 for method j.\n        • Method j wins: assigned score 1 for method j, and 0 for method i.\n        • Tie: assigned score 0.5 to both methods.\n\nConversion to ELO Differences.  Win and tie rates were converted to ELO rating differences using:\n\n                                                          Tie Rate\n                         Adjusted Win Rate = Win Rate +\n                                                     2\n\n                                                  Adjusted Win Rate\n            ELO Difference = 400 × log10\n                                           1 −Adjusted Win Rate\n\n\n                                       39\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nTo ensure numerical stability, extreme adjusted win rates were constrained as follows:\n\n                                 −800,  Adjusted Win Rate ≤0.001\n             ELO Difference =\n                                 +800,  Adjusted Win Rate ≥0.999\n\nCalculating Method ELO Ratings.  Final ELO ratings were determined by averaging each\nmethod’s pairwise ELO differences and centering these averages around a baseline rating (e.g.,\n1000):\n\nELOmethodi = Base Rating+(Average ELO Difference for method i −Overall Mean ELO Difference)\n\nInterpretation of ELO Scores.  Methods with higher ELO scores consistently outperform lower-\nscored methods. A rating difference of 400 points corresponds to a 90% expected win probability for\nthe superior method.\n\n\nG.5  STATISTICAL SIGNIFICANCE\n\nAs summarized in Table 11, we conducted two-sided binomial and McNemar’s tests (p <0.05) to\nassess the statistical significance of observed differences. The result confirms that TIPO’s advantages\nare unlikely to be explained by random variation, which also supports a consistent performance\nhierarchy: TIPO ranks highest, followed by Promptist, PromptExtend, Original, and MagicPrompt.\nCollectively, these findings illustrate TIPO’s robust, model-agnostic effectiveness and underscore the\nmodel-sensitivity of alternative methods, particularly Promptext and MagicPrompt.\n\n\n\n\n\n                                       40\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nG.6  SURVEY RESPONSE EXAMPLES\n\nIn this section we provided some responses of our human preference survey as reference.\n\n\n\n\n\nFigure 22: Some survey responses on illustrious-3.5-vpred generated image with different prompt\noptimization method\n\n\n\n\n\n                                       41\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nFigure 23: Some survey responses on illustrious-3.5-vpred generated image with different prompt\noptimization method\n\n\n\n\n\n                                       42\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nFigure 24: Some survey responses on SD3.5-medium generated image with different prompt opti-\nmization method\n\n\n\n\n\n                                       43\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nFigure 25: Some survey responses on SD3.5-medium generated image with different prompt opti-\nmization method\n\n\nG.7  CONCLUSION\n\nThe extended evaluations presented here reinforce TIPO’s standing as a reliable and effective prompt-\noptimization strategy. Its consistent performance gains under diverse model conditions highlight its\npotential for broad applicability, with its strong alignment with user-specified prompts, high image\nquality, and favorable aesthetic outcomes.\n\n\n\n\n\n                                       44\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nH  ABLATION STUDY ON TIPO\n\nIn this section, we investigate the effect of incorporating TIPO (Tags + Inferred Prompt Objects)\nacross various generation settings. Our primary goal is to validate whether additional structured\ninformation (e.g., core tags and minimal spatial/contextual cues) can improve image quality, reduce\nartifacts.\n\nH.1  EXPERIMENTAL SETUP\n\nPrompt Variants  To systematically analyze TIPO’s contribution, we consider four types of input\nprompts improvement task:\n\n      1. Tag →More core words: Given an initial set of core words, generate more refined or\n        expanded core words.\n      2. NL →More NL: Given a short natural language (NL) description, elaborate into a richer NL\n        prompt.\n      3. Tag →(More core words + NL): Combine expanded tags with a corresponding NL descrip-\n         tion derived from them.\n      4. NL →(More NL + core words): Use the NL prompt to add relevant tags, forming a mixed\n        prompt of NL plus core words.\n\nIn each case, we compare the baseline prompts (without TIPO cues) against prompts incorporating\nTIPO’s structured, tag-based critical information and minimal spatial hints.\n\nData Preparation We start by randomly sampling core words from a word table to represent a\ndiverse range of topics (e.g., objects, environments, descriptors). Additionally, for each word set, we\ngenerate a corresponding short NL sentence using a compact language model (GPT4o-mini). Overall,\nthe six prompt variants are tested on 4,000 images, ensuring a balanced comparison.\n\nInference Procedure  Prompts are fed into our image-generation pipeline under identical model\nsettings (classifier free guidance, sampler, steps, etc.), using the v-parameterized variant of Illustrious\nv3.5(Park et al., 2024). We focus on how TIPO modifications alter the generation outcomes and\nwhether they introduce additional computational overhead.\n\nH.2  EVALUATION METRICS\n\n\n\n\n\n          (a) Corrupt Score Distributions                              (b) Aesthetic Score Distributions\n\nFigure 26: Side-by-side comparison of Corrupt (left) and Aesthetic (right) score distributions across\nprompt types.\n\n\nAesthetic Score  We employ an off-the-shelf aesthetic predictor to estimate image quality. In the\nfollowing paragraph, we discuss the model’s bias.\n\n\n                                       45\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nAI Corruption Score  Using an automated ’ AI corruption ’ detection model, we measure generation\nartifacts, such as distorted objects and unnatural shapes. Higher scores imply cleaner, more coherent\noutputs.\n\n\nH.3  RESULTS & DISCUSSION\n\nImpact on Aesthetics  Figure 26b shows that TIPO-enhanced prompts generally achieve higher\naesthetic scores than their non-TIPO counterparts, albeit with some variance. Notably, we observe a\ncorrelation between wider color ranges and higher aesthetic scores discussed in Figure 27, suggesting\na bias toward more colorful or varied compositions.\n\n\nImprovements in Corruption Score  As shown in Figure 26a, TIPO-based prompts yield signifi-\ncantly lower corruption scores, indicating fewer artifacts. We hypothesize that the additional spatial\nand contextual details encoded via TIPO help the model place objects more consistently.\n\n\n\n\n\n (a) Saturation vs. Aesthetic Score   (b) Binned Saturation vs. Aesthetic    (c) Saturation vs. Corrupt Score\n\nFigure 27: Scatter plots (left and right) and binned analysis (middle) showing the relationship between\nsaturation and image metrics. We find a moderate positive correlation between saturation and aesthetic\nscore (Pearson r = 0.2821), particularly at lower saturation ranges, based on 24k samples. However,\nsaturation shows no notable correlation with corrupt score (Pearson r = 0.0125).\n\n\n\nH.4  SPEED TEST AND OVERHEAD ANALYSIS\n\nInference Speed A key concern for production pipelines is whether TIPO generation imposes a\nsubstantial time overhead. We benchmarked prompt-generation inference on four smaller models,\nexcluding any large proprietary LLMs. As illustrated in Table 12, the additional TIPO-related\ncomputation remains well below the image-generation time. Hence, even in a synchronous pipeline,\nTIPO prompt expansion does not constitute a bottleneck.\n\n               Table 12: Speed Test Results for TIPO and Other Prompt Methods\n\n      Method            Model/Config  # Runs   Avg. Time (s)   Std. Dev. (s)\n\n                     LLaMA-500M     500         1.4207        1.0730\n      TIPO            LLaMA-200M     200         1.0306        0.8982\n                     LLaMA-100M     200         1.0078        0.9394\n\n      PROMPTIST        GPT2-125M    1000         1.4593        0.2857\n     PROMPTEXTEND    GPT2-125M    1000         1.3849        0.2151\n     MAGICPROMPT     GPT2-125M    1000         1.1398        0.4043\n\n\n\nMemory Footprint  We also confirm that TIPO’s overhead in terms of VRAM usage is minimal\n(e.g., < 0.5 GB for TIPO-200M and < 1.5 GB for TIPO-500M) with the quantization supported\nby llama.cpp. The practical adoption of TIPO in pipelines has shown no critical memory concerns,\nwhich aligns with our measurements.\n\n\n                                       46\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\nTraining Costs  To contextualize the training cost of TIPO, we compare its training time with\nreinforcement learning (RL)-based prompt optimization methods, based on the reported settings in\ntheir original papers. While it is technically difficult to reproduce RL-based methods on our hardware\n(4×RTX 3090), their reported GPU-hour budgets provide an approximate reference for scale and\nefficiency. The comparison is summarized in Table 13.\n\nAlthough TIPO requires more total GPU-hours than PAE,  it is trained on over 30 million\nprompts—two orders of magnitude larger than both Promptist and PAE. After normalization, TIPO\nachieves the lowest cost per 1k prompts, demonstrating strong scalability. It is also worth noting\nthat Promptist and PAE rely on reinforcement learning with external T2I rollouts. Even for SD1.5,\neach rollout takes roughly five seconds, and the cost increases dramatically for larger models such as\nSDXL, SD3, or Flux. By contrast, TIPO requires no rollouts, so its training cost scales linearly with\ncorpus size and remains independent of the target T2I model.\n\n\nTable 13: Training cost comparison between TIPO and RL-based prompt optimization methods.\nGPU-hours per 1k prompts are normalized for fairness.\n\n\n     Method   #Params #Prompts  #GPUs          GPU-h   GPU-h /1k\n\n     TIPO     200M    30,000k   4×RTX 3090        1,680     0.056\n     PAE      125M    450k     1×A800           90        0.20\n       Promptist  125M    90k      4×V100    (SFT),  63        0.70\n                               32×V100 (RL)\n\n\n\nH.5  CONCLUSION OF ABLATION\n\nOur experiments suggest that TIPO (1) consistently lowers AI corruption artifacts, (2) can boost\naesthetic scores, and (3) remains computationally inexpensive. The improvements in metrics support\nthe viability of TIPO prompts for real-world image-generation tasks. In short, a concise natural\nlanguage prompt with core tag-based critical information appears to be an effective, suggested form\nfor most use cases.\n\n\nI  TOPIC DISTRIBUTION VISUALIZATION\n\n\nLatent Dirichlet Allocation (LDA) (Blei et al., 2003) is a generative probabilistic model for topic\nmodeling (Jelodar et al., 2018), which assumes that each document is a mixture of topics, with\neach topic represented by a distribution over words. LDA uncovers hidden thematic structures by\nanalyzing word co-occurrence patterns, while methods like TF-IDF and TextRank (Mihalcea & Tarau,\n2004) enhance its ability to extract meaningful insights from large textual datasets. We implemented\na multi-stage topic modeling and clustering methodology using LDA to extract varying numbers\nof topics (20, 30, 50, and 100) from the corpus. This approach focuses on identifying significant\nrepresentative words while filtering out stop words and irrelevant terms to ensure meaningful topic\nclassification.\n\nWe empirically assessed whether the resulting topics were sufficiently large and diverse by employing\nmulti-level topic analysis. This iterative process mitigates potential challenges such as substantial\ntopic overlap, which can diminish distinctiveness when extracting a large number of topics (Stevens\net al., 2012).\n\nTo address the potential overlap and further assess the diversity and meaningfulness of the topics,\nwe performed a secondary clustering (Zhao & Karypis, 2002). We grouped the initially extracted\ntopics into five major clusters using k-means clustering. We evaluated the clustering performance by\ncalculating the inertia (Sum of Squared Distances) (Hartigan et al., 1979), shown in Table 14, 15, and\n16. Since the topics have already been filtered for meaningful content, a higher inertia value indicates\ngreater diversity among the clusters, reflecting a broader range of valid and meaningful topics across\nthe dataset. This two-tiered approach allows for a more nuanced analysis of topic diversity and\nensures the robustness of the topic modeling against meaningless word groupings.\n\n\n                                       47\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n     Size    MagicPrompt      GPT4o-mini        Promptist        TIPO\n         Run 1   Run 2   Run 1   Run 2   Run 1  Run 2   Run 1   Run 2\n    20     170.78    137.94   1078.38   204.71   125.90   144.59   1037.44   271.77   Z\n    30     461.79    417.74   1758.40   736.74   327.48   195.13   1323.88   512.07\n    50     829.68    730.73    861.15   1036.33   400.90   373.74   823.51    959.18\n    100   1656.74   1245.60   1987.63   1628.32   877.36   657.14   1622.79   1777.61\n\n                 Table 14: Inertia for COYO-Dataset inference, higher is better\n\n\n     Size    MagicPrompt      GPT4o-mini        Promptist        TIPO\n         Run 1   Run 2   Run 1   Run 2   Run 1  Run 2   Run 1   Run 2\n\n    20     184.53    352.16    244.79    246.15   125.47   198.94   278.56    211.98\n    30     452.01    566.74    505.56    441.34   204.28   328.70   372.07    471.28\n    50     571.77    895.47   1227.30   990.17   438.89   313.48   737.65    788.41\n    100   1291.60   1742.36   1675.41   1550.32   631.61   628.78   1573.47   1855.90\n\n                  Table 15: Inertia for GBC-Dataset inference, higher is better\n\n\nWe attach a simple visualization of topics in scenery prompt generation, with topic n=100, cluster\nk=5.\n\n\n\n\n\n                                       48\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\n\n                 Size  MagicPrompt  GPT4o-mini   Promptist   TIPO\n\n               20        60.82         734.52       210.60     139.29\n               30        275.76        1141.77       415.95     355.20\n               50        630.50         826.29       722.75    1002.36\n               100      2026.39        1879.08       802.93    1883.70\n\n                 Table 16: Inertia for Scenery extend inference, higher is better\n\n\n\n\n\n     (a) GPT-4o-mini         (b) MagicPrompt          (c) Promptist               (d) TIPO\n\nFigure 28: Topic visualization for scenery prompt generation. A wider spread indicates a greater\ndiversity of generated topics.\n\n\n\n\n\n                                                    6+girls        2boys\n\n\n                                                                                        2girls\n                                   5girls\n                                              book\n                                    3boys                                                                           3d\n                           bag                           architecture\n\n           4boys\n                                                                                       air_conditioner                           6+boys\n\n                               cloud     scenery     artist_name\n                           4girls\n                                                     no_humans          1other\n                                            day     blue_sky                         abandoned\n\n             ._.                                                                                      1girl           arch\n                  2others       animal                bird\n\n\n                                                               bridge           1boy\n\n                      6+others\n                                                                   3girls  buildings\n                                                 aircraft\n\n\n\n\nFigure 29: This visualization represents a filtered subset of posts from the Danbooru2023 dataset,\ncentered on the ’scenery’ tag. The network graph is an ego network (depth 1), which includes only\nnodes directly connected to the ’scenery’ tag. To refine the data and focus on meaningful associations,\nuncommon tags with fewer than 10 occurrences were excluded. The analysis, conducted using\nGephi, focuses on nodes with a degree greater than 600 to highlight critical components. Nodes are\ncolor-coded by modularity class by Fast Unfolding Algorithm (Blondel et al., 2008), revealing clusters\nof closely associated tags. Node size reflects Eigenvector Centrality(Bonacich, 1972), emphasizing\nhighly connected and influential tags within their network.\n\n\n\n\n                                       49\n\nPublished as a conference paper at ICLR 2026\n\n\n\n\nJ  DISCUSSION AND FUTURE WORK\n\nDespite the promising performance demonstrated by TIPO, several limitations and future directions\nremain open for further study.\n\nDistribution Dependence and OOD Generalization.  The optimization behavior of TIPO inher-\nently depends on the distributional bias of the open text–image corpus used for training. As a result,\nit may exhibit unstable behavior on extremely rare or stylistically unconventional prompts. Besides,\nwhen applied to out-of-distribution (OOD) T2I models whose training data or prompting conventions\ndeviate significantly from LAION-style distributions, aesthetic performance may degrade slightly. In\naddition, our current OOD evaluation uses GPT-4o-mini–generated prompts, which are high-quality\nbut do not fully represent real-world user queries, thus limiting external validity. Addressing the\ngeneralization to unseen models and long-tail prompts remains an important direction for future work,\nwhich can be pursued from two complementary perspectives: improving the model’s generalization\ncapacity via stronger backbones and domain-diverse fine-tuning, and enhancing evaluation through\nlarge-scale collection of authentic user prompts and cross-model benchmarking.\n\nStronger Backbone Initialization.  Our current implementation trains a mid-sized LLaMA variant\nfrom scratch. Future work could instead initialize from stronger open-source LLM backbones and\nfine-tune them on T2I corpora, potentially improving robustness on long-tail and domain-specific\ndistributions. This could also mitigate failures on small or highly biased datasets by leveraging more\ngeneral linguistic priors.\n\nModel-Specific Adaptation and Style Variance.  For models that require structured or JSON-\nformatted prompts, TIPO can be combined with a lightweight adapter or fine-tuned on a small set\nof model-specific data. Extending TIPO with such adaptation modules could better accommodate\nsystems whose prompt syntax diverges from mainstream diffusion models. In the longer term, TIPO\ncan also serve as a backbone integrated with RL-based refinement for model-specific alignment.\n\nPersonalization and Style Preservation.  The current TIPO is a general-purpose optimizer and\ndoes not incorporate user-level or stylistic conditioning. Building on prior personalization techniques\nsuch as LoRA (Hu et al., 2022), future work could explore lightweight adapters or online learning\nmechanisms that track user preferences and maintain project-level stylistic consistency, enabling\npersonalized and context-aware prompt optimization.\n\nImage-Aware and Feedback-Driven Refinement.  TIPO currently operates purely in the text\ndomain without utilizing generated images or user feedback. A promising extension is to incorporate\nvision–language models like Qwen3-VL (Bai et al., 2023) for image-aware refinement, allowing\niterative refinement with optimized prompts, visual outcomes, and user instructions. However, such\nintegration requires non-trivial data curation and training pipelines, which we leave for future work.\n\nScaling Behavior and Model Capacity.  From 200M to 500M parameters, TIPO continues to yield\nimprovements in FDD and Aesthetic scores. Due to limited compute resources, we could not explore\nlarger configurations to observe scaling saturation. A systematic study of TIPO’s scaling behavior and\narchitectural variants would be a valuable direction for future research. Such analysis could reveal\nscaling laws unique to prompt optimizers and guide practical model sizing for future deployments.\n\nK  DISCLOSURE OF LLM USAGE\n\nWe used GPT-5 only to polish writing by improving the readability and grammar correctness. No\nLLMs were used in the main contributions of this work, such as ideation, experiment design, or result\nanalysis.\n\n\n\n\n\n                                       50",
"headers": [
"arXiv:2411.08127v5  [cs.CV]  29 Jan 2026",
"P",
"O",
"TIPO: T",
"I",
"T",
"Appendix",
"ROMPT",
"PTIMIZATION",
"EXT TO",
"MAGE WITH",
"EXT",
"RESAMPLING FOR",
"A",
"1",
"2",
"R",
"W",
"3",
"4",
"M",
"5",
"E",
"6",
"C",
"S",
"D",
"/R",
"B",
"/T2I",
"TIPO I",
"TIPO",
"F",
"G",
"H",
"V",
"J",
"K",
"LLM U"
],
"tables": [
"|(a) No Pre-Sampling (b) Add Details (c) Add Random Words (d) TIPO Pre-Sampling Distr<br>All All All All<br>Inferred Images I In mfe ar gre ed s Images Inferred Images Inferred Images Distr<br>Images Images Images<br>Expe<br>E Ix mpe ac gt ee sd E Ix mpe ac gt ee sd E Ix mpe ac gt ee sd E Ix mpe ac gt ee sd Inpu<br>Infer<br>Mod<br>Prompt<br>An astronau<br>< >: Tansform|ibution of All Pixels<br>ibution of All the Images<br>cted Output Distribution of<br>t Prompt for Model<br>red Ouput Distribution of<br>el for Given Input Prompt<br>Nonsensical or<br>Not User Indent Image<br>t rides horse on Mars + < ><br>ation function - LLM, Random, or TIPO|\n|---|---|\n|||",
"|40<br>51.3<br>20<br>0<br>TIPO|21.2% 17.5% 18<br>%<br>36.9% 37.0% 36|11.2% 11.<br>.6% 24.2% 17.6%<br>52.5%<br>48.<br>.0% 37.7% 39.9%|9%<br>18.7% 23.3%<br>0%<br>34.5% 32.8%<br>xt agicprompt original|\n|---|---|---|---|\n|<br>TIPO<br><br>40<br>20<br>0<br><br><br>51.3|romptist<br>romptext<br>agicpro|original<br>mpt<br>TIPO<br>romptist<br>rompte|original<br>mpt<br>TIPO<br>romptist<br>rompte|\n|<br>TIPO<br><br>40<br>20<br>0<br><br><br>51.3|P<br>p<br>m|P<br>p|P<br>p|",
"|Col1|TIPO-100M TIPO-200M stage1 TIPO-200M stage2 TIPO-500M|\n|---|---|\n|Architecture<br>Type<br>Vocab Size<br>Hidden Dim<br>Attention Heads<br>MLP Dim<br>Hidden Layers<br>Model Parameters|LLaMA<br>Pretrain<br>Pretrain<br>Finetune<br>Pretrain<br>32013<br>640<br>768<br>-<br>1280<br>10<br>12<br>-<br>20<br>2240<br>2304<br>-<br>3840<br>10<br>20<br>-<br>20<br>100M<br>203M<br>-<br>508M|\n|Max Learning Rate<br>Optimizer<br>LR scheduler<br>betas<br>weight decay|5e-4<br>2e-4<br>5e-5<br>2e-4<br>AdamW<br>Cosine Annealing LR<br>0.9, 0.98<br>0.01|\n|Dataset<br>Epoch<br>max context length<br>global batch size<br>Token Seen<br>Hardware<br>Training Time (wall)|Coyo, GBC, Dan<br>GBC, Dan<br>Coyo, GBC, Dan<br>Coyo, GBC, Dan<br>1<br>5<br>3<br>5<br>512<br>512<br>1024<br>1024<br>1024<br>2048<br>2048<br>3584<br>6.0240B<br>22.625B<br>18.339B<br>31.274B<br>4 × RTX3090<br>4 × RTX3090<br>4 × RTX3090<br>8 × H100<br>22.5 hour<br>150 hour<br>270 hour<br>100 hour|",
"|TIPO-100M TIPO-200M TIPO-500M<br>Hardware Platform tok/sec gen time tok/sec gen time tok/sec gen time|Col2|Col3|Col4|\n|---|---|---|---|\n|M1 Max (32 GPU cores)<br>RTX 3090<br>RTX 4090|339.4<br>0.66<br>558.5<br>0.42<br>742.9<br>0.29|190.0<br>1.23<br>341.4<br>0.69<br>454.5<br>0.51|119.4<br>2.02<br>289.8<br>0.81<br>359.7<br>0.63|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2411.08127v5.pdf"
}