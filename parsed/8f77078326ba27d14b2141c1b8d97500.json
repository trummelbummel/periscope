{
"text": "In-context Demonstration Matters: On Prompt Optimization\n                          for Pseudo-Supervision Refinement\n\n       Zhen-Yu Zhang                                                     zhen-yu.zhang@riken.jp\n          Center for Advanced Intelligence Project, RIKEN\n        Jiandong Zhang                                          zhang.jiando@northeastern.edu\n           Northeastern University\n       Huaxiu Yao                                                            huaxiu@cs.unc.edu\n         UNC-Chapel Hill\n       Gang Niu                                                                gang.niu@riken.jp\n          Center for Advanced Intelligence Project, RIKEN2025        Masashi Sugiyama                                                     sugi@k.u-tokyo.ac.jp\n          Center for Advanced Intelligence Project, RIKEN\n          Graduate School of Frontier Sciences, The University of TokyoMay\n26                                      Abstract\n\n               Large language models (LLMs) have achieved great success across diverse tasks, and fine-tuning is some-\n               times needed to further enhance generation quality. Most existing methods rely on human supervision or\n              parameter retraining, both of which are costly in terms of data collection and computational resources.\n            To handle these challenges, a direct solution is to generate â€œhigh-confidenceâ€ data from unsupervised\n             downstream tasks and use them for in-context prompting or prompt optimization to refine the pseudo-[cs.CL]\n                supervision. However, relying solely on such data may lead to overfitting. In this paper, we leverage\n               the in-context learning (ICL) abilities of LLMs and propose a novel approach, pseudo-supervised demon-\n                 strations aligned prompt optimization (PAPO) algorithm, which jointly refines both the prompt and the\n                 overall pseudo-supervision. The proposed learning objective ensures that the optimized prompt guides\n               the LLM to generate consistent responses for a given input when pseudo-supervised data from the down-\n              stream task are used as demonstrations, enabling refinement over the entire pseudo-supervision. The\n             prompt is optimized by translating gradient signals into textual critiques, which serve as feedback to\n                 iteratively refine the prompt and model responses.  Theoretical analysis in a simplified classification\n                setting shows that the refined pseudo-supervision exhibits a geometric clustering structure, helping to\n               mitigate overfitting. Experiments on question answering, natural language inference benchmarks, and a\n                real-world molecule optimization task, show the effectiveness of the proposed algorithm.\n\n\n         1. IntroductionarXiv:2410.03124v2\n         Large language models have shown impressive performance on various real-world tasks (Brown et al.,\n          2020; Achiam et al., 2023; Yang et al., 2024). Since most LLMs are trained for general purpose use,\n          fine-tuning is often necessary to enhance their performance on specific downstream applications. For\n           instance, reinforcement learning from human feedback (RLHF) techniques align LLMs using human\n          preference data (Ouyang et al., 2022; Rafailov et al., 2023). Despite their effectiveness, these approaches\n           typically involve retraining, which can be time-consuming and limit the modelâ€™s responsiveness to rapidly\n         changing data distributions and task requirements. Meanwhile, these methods require supervised data\n          to update the model, while human feedback is hard to obtain in many real-world tasks.  Therefore,\n              it is important to design algorithms that improve the generation quality of LLMs at test time using\n          unsupervised data, without retraining the model parameters.\n             Existing approaches relevant to this learning problem include test-time alignment and self-refinement\n           strategies. However, current test-time alignment methods mostly consider the preference alignment task\n        and heavily rely on human supervision, whereas self-refinement approaches typically require retraining\n\n\n\n           Â©2024 Zhen-Yu Zhang, Jiandong Zhang, Huaxiu Yao, Gang Niu, and Masashi Sugiyama..\n\nZhang, Zhang, Yao, Niu, Sugiyama\n\n\n\n           RLHF                Test-time Alignment (TTA)              Performance Curve\n                   ğŸ”¥                                            Pseudo-supervised          ModelğŸ”¥                                                Pseudo-supervised\n                                                   generation\n                                                       generation    Data                                                                              Initial                                                                        Refined            â„                               Model            Reward modelâ„                                         Reward modelâ„\n\n         Self-Refine                   PAPO                   Comparison with Related Works\n                                                                                         w/o Human   w/o Fine-                                           Pseudo-supervised ğŸ”¥           ModelğŸ”¥                       Pseudo-supervised                    Feedback     Tuning                                                  generation\n                                                                                              Initial      generation          RLHF      âŒ     âŒ\n                                                                       Refined    Data         â„                              Model                              TTA       âŒ     âœ…\n            High-confidence                   High-confidence               Self-Refine    âœ…     âŒ\n                  data                              data                                                           PAPO      âœ…     âœ…\n\nFigure 1: Comparison between training-time optimization (e.g., RLHF and Self-Refine) and test-time\noptimization with or without human supervision (e.g., Test-time Alignment and PAPO). PAPO enables\ntest-time refinement without retraining model parameters or requiring human supervision.\n\n\nmodel parameters, which can be computationally expensive.  For instance, Best-of-N sampling is a\ntypical test-time alignment method (Lightman et al., 2024; Zhang et al., 2024) that generates multiple\ncandidate responses and selects the one with the highest score according to a reward model trained on\nhuman feedback. Although these methods are effective, their dependence on human supervision can\nlimit generalization. To handle this challenge, self-refinement techniques (Huang et al., 2023; Wang\net al., 2023; Sun et al., 2024) allow models to explore â€œhigh-confidenceâ€ self-generated responses and\nupdate themselves accordingly.  Nevertheless, these methods require retraining of model parameters,\nresulting in substantial computational overhead.\n   To support test-time refinement without retraining model parameters, a feasible method  is to\nfirst identify â€œhigh-confidenceâ€ pseudo-supervised data, which can be obtained either via the chain-\nof-thought (CoT) mechanism (Wei et al., 2022) or through scoring functions. Building on this, recent\nseminal works leverage these selected data as in-context demonstrations (Brown et al., 2020) to guide\nfinal predictions (Wan et al., 2023a,b; Guo et al., 2024; Li et al., 2024). While effective, these approaches\ndepend heavily on the selected â€œhigh-confidenceâ€ data. In practice, over-reliance on such self-selected\npseudo-supervised data may lead to overfitting (Bishop, 2006; Goodfellow et al., 2016). This may lead\nthe model to reinforce biases present in these data, ultimately resulting in degraded performance.\n   In this paper, we explore the in-context learning (Brown et al., 2020) ability of LLMs for test-time\nrefinement. ICL generates responses by conditioning on labeled demonstrations, and has been theo-\nretically shown to perform equivalently to gradient descent under certain conditions, without updating\nmodel parameters (Bai et al., 2023). We incorporate pseudo-supervised data in the downstream task as\ndemonstrations during prompt optimization to mitigate overfitting. This is achieved by regularizing the\nrefined pseudo-supervised data to exhibit internal consistency: when used as in-context demonstrations,\nthey should guide the model to produce aligned outputs, even when those demonstrations are not explic-\nitly provided. Ideally, in a simplified classification setting, the empirical risk minimizer over any subset\nis expected to yield consistent pseudo-labels on the rest of the data after refinement. In other words, we\npropose encouraging a cluster structure in the refined pseudo-supervised data to mitigate overfitting, a\nwell-established principle in semi-supervised learning and self-supervised learning (Chapelle et al., 2006;\nBelkin et al., 2006).\n   Building on this idea, we propose a novel test-time refinement algorithm, called pseudo-supervised\ndemonstrations aligned prompt optimization (PAPO), which enhances generation quality without re-\nlying on human supervision or retraining model parameters, as illustrated in Figure 1.  Specifically,\n\n\n                                                 2\n\nIn-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement\n\n\n\nwe integrate prompt optimization with the ICL capability of LLMs by iteratively identifying a set of\nâ€œhigh-confidenceâ€pseudo-labeled data, and then jointly refining the modelâ€™s generation and optimiz-\ning the prompt based on these examples.  The learning objective is to minimize the loss on these\nâ€œhigh-confidenceâ€ data, with the pseudo-supervised data serving as in-context demonstrations. We use\nTextGrad (Yuksekgonul et al., 2024) to optimize the prompt through gradient-based updates with tex-\ntual feedback, similar to gradient descent. Theoretical analysis shows that, in the simplified setting of\nclassification, the refined output exhibits a cluster structure that helps alleviate the overfitting issue.\nWe evaluate the quality of the refined generation by PAPO and other contenders on several benchmark\ndatasets and a real-world molecule optimization task. Experimental results demonstrate the effectiveness\nof PAPO in producing high-quality refined generation without human supervision.\n\n\n2. Related Work\n\nTest-Time Alignment.  Different from RLHF methods that directly update model parameters, re-\ncent studies have explored test-time alignment approaches that align LLMs with human preferences at\ninference time. Best-of-N (BoN) sampling (Lightman et al., 2024) selects the most preferred output from\nmultiple candidates generated by the LLM using a reward model. To improve its efficiency, Speculative\nBoN accelerates generation by discarding low-quality responses early in the decoding process (Zhang\net al., 2024). Building on BoN, TreeBoN further enhances inference-time alignment by a speculative\ntree-search framework (Qiu et al., 2024). TPO (Li et al., 2025) introduces an iterative refinement ap-\nproach in which the model receives and incorporates textual feedback at test time to align its generations\nwith implicit preferences.\n   Some prior works also explore prompt optimization to for test-time alignment. For example, BPO\nfirst collects human feedback data, and then trains a prompt optimization model to guide the LLM\ntoward generating more preferred responses (Cheng et al., 2024). However, this method still relies on\nhuman feedback for alignment. URIAL employs three fixed stylistic examples with a system prompt,\nachieving results comparable to RLHF (Lin et al., 2024). In contrast, our method jointly optimizes the\nprompt and downstream pseudo-supervision to achieve more tailored performance on specific tasks.\n\nSelf-Refinement.  Self-refinement algorithms allow an LLM to generate initial responses on a down-\nstream task, provide feedback on them, and iteratively refine its responses, leading to improved perfor-\nmance. For instance, LMSI employs CoT prompting (Wei et al., 2022) to generate high-quality labels\nfor unlabeled datasets, which were then used to optimize the model (Huang et al., 2023). LLMRefine\nemploys a fine-grained feedback model to identify defects in outputs and guide iterative refinements, op-\ntimizing performance during inference without additional training (Xu et al., 2024). Similarly, SALMON\nretrieves high-quality samples relevant to the downstream task and used them as ICL demonstrations\nto generate additional samples, which were then iteratively employed to fine-tune the LLM (Sun et al.,\n2024). ISARA is an improved self-refinement methods without human-crafted instructions and labeled\nrewards (Guo et al., 2024).\n   Several recent seminal works explored using ICL prompting for self-refinement without retraining\nmodel parameters (Wan et al., 2023a,b; Li et al., 2024). These methods first identify â€œhigh-confidenceâ€\npseudo-supervised data using carefully designed scoring functions, and then leverage the selected data as\nin-context demonstrations to guide final predictions. We further explore the pseudo-supervision across\nthe entire downstream task as an implicit form of regularization to mitigate overfitting, drawing on\nwell-established principles from self-supervised learning (Chapelle et al., 2006; Belkin et al., 2006).\n\nPrompt Optimization.  Prompt learning provides a lightweight alternative for enhancing the gener-\nation quality of LLMs on downstream tasks without requiring fine-tuning on model parameters. BBT\noptimizes the prompt for adaptation using derivative-free optimization techniques such as evolutionary\nalgorithms (Sun et al., 2022). BDPL employs policy gradient algorithms to optimize the prompt (Diao\net al., 2022). Typically, these methods still require labeled data to optimize the prompt.\n\n\n                                                 3\n\nZhang, Zhang, Yao, Niu, Sugiyama\n\n\n\n3. Our Approach\n\nIn this section, we begin by introducing the notations, then describe the PAPO algorithm in detail, and\nfinally provide a theoretical analysis of its properties in a simplified setting of classification.\n\n\n3.1 Notations\n\nIn this part, we introduce the notations. Let xl âˆˆX be the l-th query in the unsupervised dataset of size\nn, where X is the textual space. We denote by z âˆˆX the prompt and z0 be the system default prompt.\nWe define two functions associated with the LLM. First, let LLMopt(Â·)  : x 7â†’p denote the prompt\noptimization function, where p âˆˆX.  It takes a textual prompt (e.g., loss or gradient information)\nas input and outputs a response p. To model the ICL capability of LLMs, we define the generation\nfunction as LLMgen(Â·, Â·, Â·) : (x, z, D) 7â†’y, where x is the query, y âˆˆX is the response in textual space,\n                                 k=1 is a set of K pseudo-supervised demonstrations drawn from thez is the prompt, and D = {(xk, byk)}K\ndownstream task. D can be an empty set, e.g., LLMgen(Â·, z0, âˆ…), indicating that the LLM is used with\ndefault prompt for prediction without demonstrations.\n   Following the formulation in TextGrad (Yuksekgonul et al., 2024), we use a prompting function\nPloss(Â·  |  Â·, Â·, Â·)  : (z  | x, y, y) 7â†’p to represent the loss function (e.g., prediction consistency), where\np âˆˆX denotes the loss expressed in textual format. The LLM then generates critiques that evaluate\nhow well the pseudo-supervision by, produced using prompt z, addresses the query x with its underlying\nsupervision y. Formally, the loss L(z) associated with prompt z is defined as follows:\n\n                               L(z) := LLMopt(Ploss(z | x, by, y))                                 (1)\n   Next, we define the prompting function Pgrad(Â·), which incorporates the textual loss L(z) to elicit\nupdate instructions, resulting in a textual gradient as follows:\n\n                          âˆ‚L\n                                   := LLMopt(Pgrad(L(z)))                                    (2)\n                              âˆ‚z\n\n    Finally, we define the prompting function Pupdate(Â·), which applies the textual gradient to generate\na refined variable, analogous to a gradient descent update, as follows:\n\n                                   znew = LLMopt(Pupdate(âˆ‚L ))                                   (3)\n                                                 âˆ‚z\n\n   Since the downstream task is unsupervised, we propose to identify â€œhigh-confidenceâ€ pseudo-supervised\ndata to optimize the prompt. Following the approach used in previous self-refinement methods (Huang\net al., 2023), we adopt the self-consistency CoT (Wang et al., 2022) to identify â€œhigh-confidenceâ€ data\nand estimate the confidence of pseudo-supervised outputs with prompt z.  Specifically, we perform\nmultiple-path decoding with a sampling temperature T > 0, automatically generating m reasoning\npaths with z and corresponding answers {yl1, . . . , ylm} for each query xl. We then apply majority vot-\ning (self-consistency) to select the most consistent and highest-confidence answer as byl, and define the\nconfidence as follows (Huang et al., 2023):\n\n                                        1 m\n                                                     cl =                X 1(ylj = byl).                                       (4)                       m\n                                               j=1\n\n3.2 Pseudo-supervised Demonstrations Aligned Prompt Optimization\n\nIn this part, we present the proposed PAPO algorithm, which jointly optimizes the prompt and refines\nthe pseudo-supervision for the downstream task in an iterative manner.\n   To optimize the prompt for specific downstream tasks, a straightforward approach is to generate the\nresponses, identify â€œhigh-confidenceâ€ pseudo-supervised data, and then optimize the prompt based on\n\n\n                                                 4\n\nIn-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement\n\n\n  Step 1: Generate \"high-confidenceâ€         ğ±!\"     \"ğ‘¦\n       pseudo-supervised data\n                                     ğ±!\"     \"ğ‘¦                                LLM\n                                     ğ±!\"     \"ğ‘¦                                                                        demonstrations  prompt\n         LLM\n                                    Step 2: Construct prompt and in-context demonstrations\n                                   ğ±#$%&  pred                                        â€¦\n                                   ğ±#$%&  pred        LLM                               prompt\n                                   ğ±#$%&  pred                  ğ±!\"     \"ğ‘¦                                                          Step 3: Update the\n                                          in-context demonstrations with                            prompt via TextGrad                  ğ±!\"     \"ğ‘¦         tunable pseudo-supervision\n       unlabeled     ğ±!\"     \"ğ‘¦         data                                                 ğ±#$%&\n                                                                       in-context demonstrations   ğ±#$%&                                                                         selection algorithm ğ‘“\n                                               unlabeled dataset                      unlabeled demonstrations\n\n   We iteratively identify â€œhigh-confidenceâ€ pseudo-supervised data and construct demonstrations\n for each data by selecting a set of data from the downstream task using a specific sample selection\n algorithm. Each selected sample is assigned pseudo-supervision generated by the LLM, guided by\n the current prompt and its corresponding demonstrations. We jointly refine the pseudo-supervision\n and optiomize the prompt by learning with the â€œhigh-confidenceâ€ data, ensuring that the LLMâ€™s\n predictions, generated based on the prompt and corresponding demonstrations, align with the original\n pseudo-supervision of the â€œhigh-confidenceâ€ data.\n\n                        Figure 2: An illustration of the PAPO algorithm.\n\n\nthese data, following the principle idea from (Wan et al., 2023b; Guo et al., 2024; Li et al., 2024). For\nexample, we optimize the following:\n\n                    n\n            arg min X 1[cl â‰¥Î³] Â· LLMopt(Ploss(z | xl, LLMgen(xl, z, âˆ…), LLMgen(xl, z0, âˆ…))),          (5)\n             zâˆˆZ                      l=1\n\nwhere 1[Â·] is the indicator function and Î³ âˆˆ[0, 1] is a threshold for selecting â€œhigh-confidenceâ€ pseudo-\nsupervised data in the downstream task.\n   Although learning prompts with â€œhigh-confidenceâ€ pseudo-supervised data is feasible, relying solely\non such data may lead to overfitting, since the prompt training procedure in Eqn. (5) does not consider\nthe use of the entire downstream dataset. To handle this problem, we propose to refine the pseudo-\nsupervision for the entire unsupervised downstream task and optimize the prompt simultaneously. Since\nthe in-context learning capability allows LLMs to implicitly learn a classifier from pseudo-supervised\ndemonstrations and apply it to other data in downstream tasks, we define the objective function for\nprompt optimization with pseudo-supervised demonstrations as follows:\n\n                    n\n         Lm(z) = X 1[cl â‰¥Î³] Â· LLMopt(Ploss(z | xl, LLMgen(xl, z, Dl), LLMgen(xl, z0, âˆ…))),         (6)\n                      l=1\n\nwhere Dl is a set of in-context demonstrations for the query xl selected by algorithm f(xl; z), with the\npseudo-supervision of these demonstrations determined with the prompt z. Specifically, f(xl, z) outputs\na set of pseudo-supervised demonstrations selected from the downstream task:\n\n                         Dl = {(xk, LLMgen(xk, z, Dk))|xk âˆˆSl}Kk=1 ,\n\nwhere LLMgen(xk, z, Dk) is the pseudo-supervision of xk, guided by both z and Dk.\n\n\n                                                 5\n\nZhang, Zhang, Yao, Niu, Sugiyama\n\n\n\n   For demonstration selection, following the seminal works on in-context example selection (Liu et al.,\n2022; Min et al., 2022), we choose the K nearest samples for each input xl as its in-context demonstration\nset, denoted by Sl:\n                                     K\n                                    Sl =    arg min X d(xl, xkj),                                 (7)\n                                          {kj}Kj=1âŠ‚{1,...,n} j=1\n\nwhere d(Â·, Â·) is a distance measure between two queries. We follow the same procedure outlined in (Liu\net al., 2022), introducing a sentence encoder Î¸(Â·) and defining the distance as d(xl, xk) = âˆ¥Î¸(xl)âˆ’Î¸(xk)âˆ¥2.\nIn addition, to address majority label bias in the in-context demonstrations, we incorporate the plug-in\nde-biasing method (Zhao et al., 2021) into our algorithm in practice.\n  We illustrate the proposed PAPO algorithm in Figure 2 and summarize it with pseudo-code in\nAlgorithm 1. We explore the entire downstream dataset by iteratively identifying â€high-confidenceâ€\npseudo-supervised data, then simultaneously optimizing the prompt and refining the pseudo-supervision.\n\n\n3.3 Theoretical Analysis\n\nIn this part, we provide theoretical insights to help explain why PAPO is effective. We emphasize\nthat the presented theorem is standard and only serves to support our approach from a theoretical\nperspective; however, it is not intended as a theoretical contribution of this work.\n   The following theoretical analysis shows that PAPO refines generation in a way that encourages the\npseudo-supervision to exhibit a clustering structure in the output space. In the simplified case of multi-\nclass classification, PAPO encourages the refined labels to exhibit a multi-manifold structure, where each\nclass occupies a disjoint convex region. When queries convey similar meanings, the refinement process\nencourages their labels to belong to the same class.\n   Recent seminal works have shown that ICL can be interpreted as a form of implicit empirical risk\nminimization (Min et al., 2022; Xie et al., 2022; Bai et al., 2023). We first introduce the following lemma\nin (Bai et al., 2023).\n\nLemma 1 (Corollary G.1 in (Bai et al., 2023)). For any transformer with layer L â‰¥1, under the same\nsetting as Theorem G.1 in (Bai et al., 2023), the (2L)-layer transformer TFÎ¸ there approximates the\ntrue gradient descent trajectory {wâ„“GD}â„“â‰¥0: For the intermediate iterates {bwâ„“}â„“âˆˆ[L] considered therein,\nwe have\n                                                               f (1 + Î·Lf)â„“Îµ,                          âˆ¥bwâ„“âˆ’wâ„“GDâˆ¥2 â‰¤Lâˆ’1\nwhere Lf = supwâˆˆW âˆ¥âˆ‡2 bLN(w)âˆ¥op denotes the smoothness of bLN within W.\n\n  Lemma 1 shows that transformers implement gradient descent on two-layer neural networks in-\ncontext.  Similarly, the outputs of PAPO exhibit comparable behavior: for any pseudo-supervised ex-\nample, when other pseudo-supervised examples are provided as in-context demonstrations, the model\ngenerates the same pseudo-label as when using the prompt. We build on this observation and Lemma 1\nto make the following assumption about the refined outputs produced by PAPO.\n\nAssumption 1 (Linear Separability). Consider a multi-class classification task, such as multiple-choice\nquestion answering. Let {x1, . . . , xn} âŠ‚Rd be data points, and yi âˆˆ{1, 2, . . . , K} be multi-class la-\nbels refined by PAPO. Suppose there exists a linear multi-class classifier defined by K weight vectors\n{w1, . . . , wK} and bias terms {b1, . . . , bK} such that:\n\n                              A(x) = arg max  wâŠ¤k x + bk\n                                                           k=1,...,K\n\nand for every i, A(xi) = yi.\n\n  We now show that linearly separable labels reveal an underlying clustering structure in the data.\n\n\n                                                 6\n\nIn-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement\n\n\n\nAlgorithm 1 Pseudo-supervised-demonstrations Aligned Prompt Optimization (PAPO)\n  1: Set total number of iterations T, number of in-context demonstrations K, total number of sampling\n  m for confidence estimation, and confidence threshold Î³.\n  2: for t = 1 to T do\n  3:   Stochastic sampling: Sample a mini-batch of data from the downstream task\n  4:   Confidence estimation: Estimate the confidence by Eqn. (4) with z(t)\n  5:  Compute loss: Compute loss by Eqn. (6) and generate gradient by Eqn. (2)\n                                              âˆ‚L\n  6:  Update prompt: z(t+1) = LLMopt(Pupdate( âˆ‚z(t) ))\n  7:   Refine output: âˆ€l âˆˆ[n], byl = LLMgen(xl, z(t+1), Dl)\n  8: end for\n\n\nTheorem 1. Suppose the linear separability in Assumption 1 holds. For each class k, the class-specific\nsample set Sk := {xi | yi = k} is contained in a convex polyhedral region\n\n                        n                           o                    Rk :=  x âˆˆRd wâŠ¤k x + bk > wâŠ¤j x + bj, âˆ€j Ì¸= k   ,\n\nwith pairwise disjointness\n                              Rk âˆ©Rj = âˆ…,  âˆ€k Ì¸= j,\n\nand separation\n                                           dist(Rk, Rj) > 0.\n\n   Theorem 1 shows that the geometry of the pseudo-supervision refined by PAPO exhibits a low-\ndimensional, cluster-aligned structure that aligns with the clustering induced by graph Laplacian mini-\nmization. Detailed proofs are deferred to the Appendix B.\n\n\n4. Experiments\n\nIn this section, we evaluate the proposed PAPO algorithm alongside several contenders using a range\nof benchmarks. We then conduct ablation studies to assess the contribution of each component in our\napproach. Finally, we apply the proposed method to a real world molecular optimization task.\n\n\n4.1 Experimental Setup\n\nTasks and Datasets. We evaluate PAPO on three tasks: two benchmarks (question answering and\nnatural language inference) and one real-world application (molecule optimization).\n\n  Question Answering. We use google-proof question answering (GPQA) dataset (Rein et  al.,\n   2024), SimpleQA dataset (Wei et al., 2024), and the MMLU subsets (Hendrycks et al., 2021) astron-\n  omy (AST), high-school-cs (HSCS), high-school-mathematics (HSM), college-mathematics (Cmath),\n   college-cs (CCS), college-medicine (CMed), management (MAN), marketing (MAR), and all-random\n  (RND).\n\n  Natural Language Inference. We use the GLUE subsets (Wang et al., 2018), CoLA, SST-2,\n  QQP, MRPC, MNLI, WNLI, and RTE, which contain sentences labeled as entailment, neutrality, or\n   contradiction.\n\n  Molecule Optimization: We also evaluate PAPO on a real-world molecular optimization task using\n   the DOCKSTRING dataset (GarcÂ´Ä±a-OrtegÂ´on et al., 2022). Each molecule is represented as a SMILES\n   string (Yuksekgonul et al., 2024), and the learning problem is to generate an improved version that\n   surpasses the original in terms of important chemical properties, specifically the Vina score, which\n   reflects binding affinity, and the QED score, which measures drug-likeness (Trott and Olson, 2010).\n\n\n                                                 7\n\nZhang, Zhang, Yao, Niu, Sugiyama\n\n\n\n\nTable 1: Performance comparisons across Question Answering (QA), Natural Language Infer-\nence (NLI) tasks. We report the average accuracy (%) and standard deviation over 5 runs. The best\nresults are in bold and (â†‘Â·) indicates the improvement over Direct in terms of average accuracy.\n\n     Task Dataset     Direct     ICL    Auto-CoT   USP    SR (BDPL) SR (RLprompt)     PAPO\n        GPQA     37.9 Â± 1.3  37.3 Â± 0.9  38.4 Â± 0.5  38.6 Â± 0.7   37.9 Â± 1.0     37.5 Â± 0.9    39.9 Â± 0.6 (â†‘2.0)\n           SimpleQA  38.2 Â± 0.8  37.5 Â± 1.2  38.9 Â± 1.0  38.2 Â± 0.9   38.1 Â± 1.3     37.4 Â± 1.1    39.6 Â± 0.9 (â†‘1.4)\n        MAR      90.2 Â± 2.0  90.7 Â± 1.7  88.9 Â± 1.7 92.4 Â± 0.9  91.3 Â± 1.8     91.0 Â± 0.8     92.1 Â± 0.8 (â†‘1.9)\n       MAN      76.8 Â± 1.4  76.4 Â± 1.0  76.5 Â± 1.0  77.5 Â± 1.6   79.0 Â± 1.2     78.2 Â± 0.9    81.1 Â± 1.4 (â†‘4.3)\n        HSM       50.9 Â± 2.9  47.5 Â± 2.2  47.4 Â± 2.2  51.4 Â± 2.3   53.4 Â± 1.8     53.2 Â± 1.1    55.6 Â± 1.6 (â†‘4.7)\n    QA  HCS       90.8 Â± 2.7  91.0 Â± 2.1  89.1 Â± 2.1  89.9 Â± 2.3   92.5 Â± 2.1     91.3 Â± 2.2    93.1 Â± 1.4 (â†‘2.3)\n         CMed      61.9 Â± 1.8  58.4 Â± 3.4  58.4 Â± 3.4  61.8 Â± 2.1   61.4 Â± 1.7     59.5 Â± 3.0    63.8 Â± 2.3 (â†‘1.9)\n          CMath     40.7 Â± 4.2  40.8 Â± 2.5  40.2 Â± 2.5  41.1 Â± 2.8   44.3 Â± 2.7     43.3 Â± 1.3    46.1 Â± 1.6 (â†‘5.4)\n         CCS       68.4 Â± 2.4  71.5 Â± 1.3  69.6 Â± 1.3  69.8 Â± 2.3   71.8 Â± 1.8     71.0 Â± 1.6    73.2 Â± 1.0 (â†‘4.8)\n         AST       86.6 Â± 2.5  86.8 Â± 2.3  86.5 Â± 2.3  87.1 Â± 2.1   85.6 Â± 3.6    88.0 Â± 2.8    87.2 Â± 1.5 (â†‘0.6)\n        RND       68.7 Â± 1.1  68.9 Â± 1.2  68.3 Â± 1.2  70.4 Â± 1.7   70.6 Â± 1.7     70.5 Â± 1.3    72.8 Â± 2.0 (â†‘4.1)\n         MNLI      91.7 Â± 2.3  90.4 Â± 2.0  90.4 Â± 2.0  90.8 Â± 1.1  92.8 Â± 1.6     92.1 Â± 0.8     92.0 Â± 1.8 (â†‘0.3)\n        QQP       71.5 Â± 1.0  71.6 Â± 2.0  68.6 Â± 2.0  71.8 Â± 1.6   71.9 Â± 1.6     69.3 Â± 3.1    73.2 Â± 2.0 (â†‘1.7)\n           SST-2      89.6 Â± 1.5  88.4 Â± 0.7  88.4 Â± 0.7  90.0 Â± 1.9   90.3 Â± 2.1     89.6 Â± 2.0    92.7 Â± 1.1 (â†‘3.1)\n     NLI  MRPC     90.9 Â± 2.0  91.0 Â± 1.5  90.1 Â± 1.5  69.8 Â± 2.8   90.9 Â± 1.0     90.1 Â± 1.8    93.4 Â± 1.7 (â†‘2.5)\n         CoLA      69.7 Â± 1.7  69.7 Â± 2.3  65.8 Â± 2.3  68.9 Â± 2.3   67.4 Â± 2.9     69.9 Â± 1.3    71.2 Â± 1.1 (â†‘1.5)\n         WNLI      90.8 Â± 1.6  87.3 Â± 1.7  87.3 Â± 1.7  89.0 Â± 2.3   88.8 Â± 2.0     88.5 Â± 1.8    91.1 Â± 1.4 (â†‘1.1)\n         RTE       92.9 Â± 1.2  93.1 Â± 1.0  88.7 Â± 1.0  70.5 Â± 2.1   91.9 Â± 1.3     90.0 Â± 1.5    94.9 Â± 1.6 (â†‘2.0)\n\nContenders. We compare our proposed algorithm against one baseline and five strong contenders.\nThe baseline is Direct, where the LLM is prompted with the default prompt to generate predictions.\nWe include Auto-CoT (Zhang et al., 2022) as a contender, which automatically generates intermediate\nreasoning steps in inference. This encourages the LLM to follow a CoT process before producing a final\nanswer, improving generation quality on downstream tasks. Moreover, we include USP (Wan et al.,\n2023b), which uses carefully designed scoring functions to select â€œhigh-confidenceâ€ data and applies ICL\nfor prediction.\n   For the other three contenders, we identify the â€œhigh-confidenceâ€ pseudo-supervised examples using\nthe same mechanism as in the proposed PAPO algorithm. Based on these examples, we apply ICL (Liu\net al., 2022), using these examples as demonstrations to predict the remaining unlabeled instances. We\nfurther include two approaches that integrate prompt learning algorithms with the self-refinement strat-\negy proposed by Huang et al. (2023): SR (BDPL) (Diao et al., 2022) and SR (RLprompt) (Deng\net al., 2022).  Specifically, SR (BDPL) employs a policy gradient method to optimize prompts based\non the â€œhigh-confidenceâ€ pseudo-labeled data, while SR (RLprompt) uses a parameter-efficient policy\nnetwork that adaptively generates prompts conditioned on these â€œhigh-confidenceâ€ pseudo-labeled ex-\namples. For both the SR (BDPL) and SR (RLprompt) algorithms, we use the default parameter settings\nfrom their original papers. Moreover, we incorporated the plug-in de-bias method (Zhao et al., 2021) in\nall contenders.\n\nImplementation Details.  In all experiments, we used GPT-4o1 and GPT-4o-mini2, provided by\nOpenAI, where GPT-4o-mini is much cheaper than GPT-4o. In all experiments, except for the ablation\nstudy on the choice of LLM, we use GPT-4o. Due to page limits, more implementation details on\nhyperparameters setting and prompts design are provided in Appendix C.\n   In some real-world tasks, users may prefer a customized model instead of relying on refined generation\nfor downstream applications. To support this, we use the refined pseudo-supervision and apply OpenAIâ€™s\ncommercial fine-tuning service to obtain a customized model. Fine-tuning is performed using the official\nOpenAI API3.\n\n 1. https://platform.openai.com/docs/models/gpt-4o\n 2. https://platform.openai.com/docs/models/gpt-4o-mini\n 3. https://platform.openai.com/docs/guides/fine-tuning\n\n\n                                                 8\n\nIn-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement\n\n\n\n\n\n                            Confidence Threshold vs Accuracy                                                                                                                                                             Accuracy Comparison of LLMs\n      90                                   Method      SST-2  GPQA  SimpleQA                                                                    GPT-4o-miniGPT-4o-mini-OursDirect\n      80                                                                                                                                                                                                                80                                                           GPT-4-DirectGPT-4-Ours\n                                               Direct       89.6/5.5  37.9/5.9   38.2/5.8\n      70\n  (%)                                                                                         SST2                                USP         90.0/9.6  38.6/10.1  38.2/9.8        (%) 60\n                                                                                          RTE      60\n                                                                                           MNLI                                                                            MMLU       ICL         88.4/10.4 37.3/10.7  37.5/10.5                  Accuracy 40      Accuracy 50\n                                 SR (BDPL) 90.3/11.3 37.9/10.8  38.1/11.6\n      40                            PAPO      92.7/13.5 39.9/13.9  39.9/14.1         20\n      30\n                0.1           0.2           0.3           0.4           0.5           0.6           0.7           0.8           0.9                                                                                                                                0             GLUE(SST2)                      MMLU(RND)\n                                       Confidence Threshold (  )                                                                                                                                                                                                                    Dataset\n                                               (b) Acc. (%) and Runtime (s).\n        (a) Acc. on different Î³.                                                             (c) Acc. on different LLMs.\n\n                        Figure 3: Ablation studies of the PAPO algorithm.\n\n4.2 Performance Comparison on Benchmarks\n\nIn this section, we compare the proposed PAPO algorithm with other contenders on benchmark datasets.\nWe set all termination T = 10. For both ICL and PAPO, the number of demonstrations is set to 5. The\nconfidence threshold is fixed at Î³ = 0.65 for PAPO and all competing methods.\n  We first report the average accuracy and standard deviation of the refined generations by the pro-\nposed PAPO algorithm and other contenders on question answering and natural language inference\ntasks, as shown in Table 1. The proposed PAPO algorithm consistently outperforms nearly all other\nmethods across the evaluated datasets. Compared to the Direct algorithm and the Auto-CoT method,\nour approach achieves superior performance, demonstrating the effectiveness of leveraging downstream\nunsupervised data and prompt optimization to refine model generation.  Furthermore, the proposed\nPAPO algorithm outperforms USP, SR (BDPL), and SR (RLPrompt), highlighting the importance of\nrefining in-context pseudo-supervised demonstrations during the learning process, rather than solely\nrelying on â€œhigh-confidenceâ€ data to predict the remaining examples.\n   In certain cases, users may prefer a customized model over refined generation for downstream tasks.\nTo evaluate the performance of the fine-tuned model for both the proposed method and the baselines,\nwe first learn the prompt and pseudo-supervision using 20% of the original dataset. The model is then\nfine-tuned on this refined dataset and evaluated on the remaining 80% of the data. Our proposed method\nconsistently outperforms other contenders, indicating higher quality in the refined generation compared\nto existing approaches. Due to space constraints, additional comparison results for customized models\nfine-tuned with the refined outputs from all contenders and PAPO are provided in Appendix A, example\noutputs on the benchmark datasets are provided in Appendix D.\n\n\n4.3 Ablation Studies\n\nIn this part, we conduct ablation studies on the proposed PAPO algorithm, analyzing the impact of\ngeneration of â€œhigh-confidenceâ€ pseudo-supervised data, the selection of in-context demonstrations, the\ncomputational overhead of PAPO, and the choice of LLM used in the pipeline.\n\nGeneration of â€œhigh-confidenceâ€ pseudo-supervised data. We first investigate the confidence\nthreshold Î³ for generating â€œhigh-confidenceâ€ pseudo-labeled data. Experiments are conducted across\nboth the question answering and natural language inference tasks, using average accuracy as the evalu-\nation metric. The results are presented in Figure 3(a). We observe that setting the confidence threshold\nbetween 0.6 and 0.7 yields stable and satisfactory performance across all experiments. A lower threshold\nmay introduce incorrect pseudo-labels, negatively affecting performance, while a higher threshold can\nlimit the amount of selected pseudo-supervised data, also leading to performance degradation. Based\non these findings, we recommend setting the confidence threshold in the range of 0.6 to 0.7 for practical\napplications.\n\nComputational overhead of PAPO.  Next, we analyze the computational overhead of PAPO. Our\nmethod incurs additional overhead from computing the distance matrix for the unlabeled data and per-\nforming pseudo-supervision refinement during each round of prompt updating. We empirically compare\n\n\n                                                 9\n\nZhang, Zhang, Yao, Niu, Sugiyama\n\n\n\n\nTable 2: Performance comparisons with varying number of in-context demonstrations on benchmark\ndatasets. We report the average accuracy (%) and standard deviation over 5 runs. The best results are\nin bold.\n\n    Method       MNLI    QQP      SST-2    MRPC     CoLA    WNLI     RTE     RND\n     Direct          91.7 Â± 2.3   71.4 Â± 1.0   89.6 Â± 1.5   90.9 Â± 2.0   69.7 Â± 1.7   90.8 Â± 1.6   92.9 Â± 1.2   68.7 Â± 1.1\n    ICL (k = 3)     89.3 Â± 1.9   68.5 Â± 2.1   88.9 Â± 2.4   88.3 Â± 1.7   66.4 Â± 2.3   87.5 Â± 1.7   88.3 Â± 1.2   67.5 Â± 1.5\n    ICL (k = 5)     90.4 Â± 2.0   71.6 Â± 2.0   88.4 Â± 0.7   91.0 Â± 1.5   69.7 Â± 2.3   87.3 Â± 1.7   93.1 Â± 1.0   68.9 Â± 1.2\n   PAPO (k = 3)  91.5 Â± 2.1   72.5 Â± 2.1   91.3 Â± 1.7   92.3 Â± 1.8  71.8 Â± 1.5  91.0 Â± 1.7   93.1 Â± 2.0   71.5 Â± 2.6\n   PAPO (k = 5)  92.0 Â± 1.8 73.2 Â± 2.0 92.7 Â± 1.1 93.4 Â± 1.7  71.2 Â± 1.1  91.1 Â± 1.4 94.9 Â± 1.6 72.8 Â± 2.0\n\nthe average accuracy and average runtime (10 rounds) of our method against other methods, as re-\nported in Figure 3(b). The results show PAPO achieves better performance with an acceptable increase\nin computational and time resources.\n\nChoice of LLM used in the pipeline. We then evaluate the performance of the PAPO algorithm\nwith different LLMs.  Experiments are conducted on both question answering and natural language\ninference tasks, with the number of demonstrations fixed at 5 for fair comparison. Figure 3(c) presents\nthe average accuracy on unlabeled data using GPT-4o and GPT-4o-mini.  The proposed algorithm\nachieves higher accuracy with GPT-4o than with GPT-4o-mini, which aligns with the relative capabilities\nof the two models. These results suggest that PAPO benefits from stronger LLMs, leading to improved\nperformance.\n\nSelection of in-context demonstrations.  Finally, we investigate the impact of the number of in-\ncontext demonstrations by selecting different numbers of K-nearest samples for each query, following\nthe distance metric used in (Liu et al., 2022). The comparison results are reported in Table 2. It can\nbe observed that the PAPO algorithm outperforms both the Direct and ICL methods on nearly all\ndatasets across different values of K. This highlights the benefit of leveraging pseudo-supervised data\nas in-context demonstrations during the prompt optimization phase. Based on our empirical results,\nsetting K = 5 is recommended to achieve satisfactory performance.\n\n\n4.4 Molecule Optimization\n\n                                                                                                                                         6.0\nIn this part, we apply the proposed PAPO algorithm to a                      PAPO\n                                                                                                                  Auto CoT\nreal world drug molecular optimization task. The super-\n                                                                                                                                         6.5\nvision for each molecule is defined by the optimal coun-\nterparts, evaluated based on the Vina score and QED                           score)\nscore. We begin with five clinically approved drugs from                       (Vina  7.0                                                      Ciprofibrate\nthe dataset as the initial set of â€œhigh-confidenceâ€ pseudo-                                    affinity\n                                                                                                                                         7.5\nsupervised data. GPT-4o is used as the LLM, with the                                binding\nprompt text adopted from TextGrad (Yuksekgonul et al.,                                                                                                           Fenofibric acid\n                                                                                                                                         8.0\n2024).                                                                                                                                                                                                                                              Fenofibrate\n   In Figure 4, we present the drug molecules refined by\n                                                                                                                                         8.5\nthe proposed PAPO in the final three iterations, along-                      0.4       0.5       druglikeness0.6       0.7(QED)  0.8       0.9\nside the molecule refined by Auto-CoT and three clinically\napproved drugs Ciprofibrate, Fenofibrate, and Fenofibric  Figure 4:  Vina score and QED score of\nacid. We observe that the molecule refined by PAPO  the molecules refined by PAPO and Auto-\nis structurally close to clinically approved drugs, while CoT compared to clinically approved com-\nachieving better QED and Vina scores and outperforming  pounds.  The molecule refined by PAPO\nthe Auto-CoT method.                                      exhibits greater structural similarity to its\n   Based on this empirical result, PAPO explores the en-  closest approved counterpart while achiev-\ntire unsupervised dataset to generate more refined outputs,  ing better QED and Vina scores.\n\n\n                                                10\n\nIn-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement\n\n\n\nwhile leveraging the TextGrad framework to produce explainable decisions, which allow researchers to\nclearly understand how and why a moleculeâ€™s structure is generated.  These results underscore the\npromising potential of the proposed PAPO algorithm in scientific discovery tasks.\n\n\n5. Conclusion\n\nIn this paper, we investigate test-time pseudo-supervision refinement without retraining model param-\neters or relying on human supervision. A direct solution is to use â€œhigh-confidenceâ€ pseudo-supervised\ndata for in-context prompting or prompt tuning, but relying solely on such data can lead to the over-\nfitting issue. We propose PAPO, a novel algorithm that iteratively identifies â€œhigh-confidenceâ€ pseudo-\nsupervised data and jointly optimizes the prompt and refines the pseudo-supervision. We regularize\nthe refined pseudo-supervised data to exhibit internal consistency: when used as in-context demonstra-\ntions, they guide the LLM to generate consistent outputs on the â€œhigh-confidenceâ€ pseudo-supervised\ndata. Theoretical analysis shows that, in a simplified multi-class classification setting, PAPO encour-\nages pseudo-supervision to form a low-dimensional structure aligned with graph Laplacian clustering,\nhelping to mitigate overfitting and improve generalization. Experiments on question answering and nat-\nural language inference benchmarks, and a real-world molecule optimization task, show the effectiveness\nof PAPO. The refined pseudo-supervised data can further be used to obtain a customized model with\ncommercial fine-tuning service, and the experimental results also show the superiority of the proposed\nPAPO algorithm.\n\n\n\n\n\n                                                11\n\nZhang, Zhang, Yao, Niu, Sugiyama\n\n\n\nReferences\n\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,\n  S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\nY. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: Provable in-context\n  learning with in-context algorithm selection.  Advances in Neural Information Processing Systems\n  (NeurIPS), 36:57125â€“57211, 2023.\n\nM. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning\n  from labeled and unlabeled examples. Journal of Machine Learning Research, 7(11), 2006.\n\nC. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\n\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n  G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in Neural Information\n  Processing Systems (NeurIPS), 33:1877â€“1901, 2020.\n\nO. Chapelle, B. SchÂ¨olkopf, and A. Zien. Semi-Supervised Learning. MIT Press, 2006.\n\nJ. Cheng, X. Liu, K. Zheng, P. Ke, H. Wang, Y. Dong, J. Tang, and M. Huang. Black-box prompt\n  optimization: Aligning large language models without model training.  In Proceedings of the 62nd\n  Annual Meeting of the Association for Computational Linguistics (ACL), pages 3201â€“3219, 2024.\n\nM. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E. Xing, and Z. Hu. Rlprompt:\n  Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference\n  on Empirical Methods in Natural Language Processing (EMNLP), pages 3369â€“3391, 2022.\n\nS. Diao, Z. Huang, R. Xu, X. Li, L. Yong, X. Zhou, and T. Zhang.  Black-box prompt learning for\n  pre-trained language models. Transactions on Machine Learning Research, 2022.\n\nM. GarcÂ´Ä±a-OrtegÂ´on, G. N. Simm, A. J. Tripp, J. M. HernÂ´andez-Lobato, A. Bender, and S. Bacallado.\n  Dockstring: easy molecular docking yields better benchmarks for ligand design. Journal of Chemical\n  Information and Modeling, 62(15):3486â€“3502, 2022.\n\nI. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.\n\nH. Guo, Y. Yao, W. Shen, J. Wei, X. Zhang, Z. Wang, and Y. Liu. Human-instruction-free llm self-\n  alignment with limited samples. arXiv preprint arXiv:2401.06785, 2024.\n\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\n  multitask language understanding. In The 10th International Conference on Learning Representations\n  (ICLR), 2021.\n\nJ. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can self-\n  improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), pages 1051â€“1068, 2023.\n\nR. Li, G. Wang, and J. Li. Are human-generated demonstrations necessary for in-context learning? In\n  The 12th International Conference on Learning Representations, 2024.\n\nY. Li, X. Hu, X. Qu, L. Li, and Y. Cheng. Test-time preference optimization: On-the-fly alignment via\n  iterative textual feedback. arXiv preprint arXiv:2501.12895, 2025.\n\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever,\n  and K. Cobbe. Letâ€™s verify step by step. In The 12th International Conference on Learning Repre-\n  sentations, 2024.\n\n\n                                                12\n\nIn-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement\n\n\n\nB. Y. Lin, A. Ravichander, X. Lu, N. Dziri, M. Sclar, K. Chandu, C. Bhagavatula, and Y. Choi. The\n  unlocking spell on base llms: Rethinking alignment via in-context learning. In The 12th International\n  Conference on Learning Representations, 2024.\n\nJ. Liu, D. Shen, Y. Zhang, W. B. Dolan, L. Carin, and W. Chen. What makes good in-context exam-\n  ples for gpt-3?  In The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning\n  Architectures, pages 100â€“114, 2022.\n\nS. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the\n  role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference\n  on Empirical Methods in Natural Language Processing (EMNLP), pages 11048â€“11064, 2022.\n\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\n  A. Ray, et al. Training language models to follow instructions with human feedback. Advances in\n  Neural Information Processing Systems (NeurIPS), 35:27730â€“27744, 2022.\n\nJ. Qiu, Y. Lu, Y. Zeng, J. Guo, J. Geng, H. Wang, K. Huang, Y. Wu, and M. Wang. Treebon: En-\n  hancing inference-time alignment with speculative tree-search and best-of-n sampling. arXiv preprint\n  arXiv:2410.16033, 2024.\n\nR. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimiza-\n  tion: Your language model is secretly a reward model. Advances in Neural Information Processing\n  Systems (NeurIPS), 36:53728â€“53741, 2023.\n\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.\n  Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling,\n  2024.\n\nT. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu. Black-box tuning for language-model-as-a-service. In\n  Proceedings of the 39th International Conference on Machine Learning (ICML), pages 20841â€“20855,\n  2022.\n\nZ. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan.  Principle-driven self-\n  alignment of language models from scratch with minimal human supervision. Advances in Neural\n  Information Processing Systems (NeurIPS), 36:2511â€“2565, 2024.\n\nO. Trott and A. J. Olson. Autodock vina: improving the speed and accuracy of docking with a new\n  scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31\n  (2):455â€“461, 2010.\n\nX. Wan, R. Sun, H. Dai, S. Arik, and T. Pfister. Better zero-shot reasoning with self-adaptive prompting.\n  In Findings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages\n  3493â€“3514, 2023a.\n\nX. Wan, R. Sun, H. Nakhost, H. Dai, J. M. Eisenschlos, S. O. Arik, and T. Pfister.  Universal self-\n  adaptive prompting. In The 2023 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), pages 7437â€“7462, 2023b.\n\nA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Glue: A multi-task benchmark and\n  analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop\n  BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353â€“355, 2018.\n\nX. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou.\n  Self-consistency improves chain of thought reasoning in language models. In The 11th International\n  Conference on Learning Representations (ICLR), 2022.\n\n\n                                                13\n\nZhang, Zhang, Yao, Niu, Sugiyama\n\n\n\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi.  Self-instruct:\n  Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting\n  of the Association for Computational Linguistics (ACL), pages 13484â€“13508, 2023.\n\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought\n  prompting elicits reasoning in large language models.  Advances in Neural Information Processing\n  Systems (NeurIPS), 35:24824â€“24837, 2022.\n\nJ. Wei, N. Karina, H. W. Chung, Y. J. Jiao, S. Papay, A. Glaese, J. Schulman, and W. Fedus. Measuring\n  short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024.\n\nS. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit\n  bayesian inference. In The 11th International Conference on Learning Representations, 2022.\n\nW. Xu, D. Deutsch, M. Finkelstein, J. Juraska, B. Zhang, Z. Liu, W. Y. Wang, L. Li, and M. Freitag.\n  Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback. In\n  Findings of the Association for Computational Linguistics (NAACL), pages 1429â€“1445, 2024.\n\nA. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2.\n  5 technical report. arXiv preprint arXiv:2412.15115, 2024.\n\nM. Yuksekgonul, F. Bianchi, J. Boen, S. Liu, Z. Huang, C. Guestrin, and J. Zou. Textgrad: Automaticâ€\n  differentiationâ€ via text. arXiv preprint arXiv:2406.07496, 2024.\n\nR. Zhang, M. Haider, M. Yin, J. Qiu, M. Wang, P. Bartlett, and A. Zanette. Accelerating best-of-n via\n  speculative rejection. In ICML 2024 Workshop on Structured Probabilistic Inference and Generative\n  Modeling, 2024.\n\nZ. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language\n  models. In The 11th International Conference on Learning Representations (ICLR), 2022.\n\nZ. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot perfor-\n  mance of language models. In Proceedings of the 38th International Conference on Machine Learning\n  (ICML), pages 12697â€“12706, 2021.\n\n\n\n\n\n                                                14\n\nIn-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement\n\n\n\nAppendix\n\nAppendix A. Additional Experimental Results\n\nIn this section, we report the performance of fine-tuned models on benchmark datasets. As shown in\nTable 3, we compare models fine-tuned on pseudo-supervised datasets generated by PAPO and other\nmethods.  Results show that the model trained with PAPO achieves better performance.  Notably,\nconsistent improvements in pseudo-supervised data quality directly translate to better fine-tuning results,\nhighlighting the superiority of the proposed PAPO algorithm.\n\n\nTable 3: Performance comparisons across Question Answering (QA), Natural Language Infer-\nence (NLI) tasks. We report the average accuracy (%) and standard deviation over 5 runs. The best\nresults are in bold.\n\n      Task Dataset     Direct     ICL    Auto-CoT   USP   SR (BDPL) SR (RLprompt)     PAPO\n        GPQA     38.5 Â± 1.7  38.7 Â± 1.0  39.5 Â± 0.7  39.5 Â± 0.2  38.2 Â± 1.5     37.8 Â± 1.2    40.1 Â± 0.3 (â†‘1.6)\n           SimpleQA  38.6 Â± 0.4  37.9 Â± 1.0  39.4 Â± 0.9  39.0 Â± 0.9  38.8 Â± 1.6     37.9 Â± 0.8    40.6 Â± 0.9 (â†‘2.0)\n        MAR      91.1 Â± 2.3  88.9 Â± 1.5  89.9 Â± 1.3  92.7 Â± 1.1  91.5 Â± 1.7     92.4 Â± 0.4    93.6 Â± 0.8 (â†‘2.5)\n        MAN      76.9 Â± 1.1  77.8 Â± 1.5  77.0 Â± 1.3  78.5 Â± 2.0  79.5 Â± 1.6     79.0 Â± 1.0    82.0 Â± 1.8 (â†‘5.1)\n         HSM       51.1 Â± 2.8  47.9 Â± 2.0  47.6 Â± 2.2  52.0 Â± 1.9  54.0 Â± 2.1     53.7 Â± 0.7    56.9 Â± 2.1 (â†‘5.8)\n    QA  HCS       91.6 Â± 2.5  89.6 Â± 2.4  89.7 Â± 1.8  91.6 Â± 1.8  93.7 Â± 2.0     92.2 Â± 1.7    94.1 Â± 1.7 (â†‘2.5)\n          CMed      62.9 Â± 1.5  59.9 Â± 2.9  59.4 Â± 3.0  62.6 Â± 2.5  62.7 Â± 1.9     61.2 Â± 2.6    64.1 Â± 2.4 (â†‘1.2)\n          CMath     41.6 Â± 4.0  41.2 Â± 2.3  41.3 Â± 2.0  42.4 Â± 2.9  45.1 Â± 2.6     44.3 Â± 1.6    47.2 Â± 1.8 (â†‘5.6)\n         CCS       69.7 Â± 2.0  70.8 Â± 1.4  70.6 Â± 1.1  70.6 Â± 2.5  72.9 Â± 1.6     72.4 Â± 1.8    74.7 Â± 1.3 (â†‘5.0)\n         AST       87.4 Â± 2.6  87.7 Â± 2.4  87.3 Â± 2.1  88.5 Â± 2.1  86.7 Â± 3.1    89.1 Â± 2.4    88.7 Â± 1.9 (â†‘1.3)\n        RND       69.4 Â± 1.3  69.3 Â± 1.4  69.5 Â± 1.1  71.3 Â± 1.4  71.9 Â± 1.4     71.6 Â± 1.0    73.8 Â± 1.8 (â†‘4.4)\n          MNLI      92.2 Â± 2.1  91.2 Â± 1.6  91.4 Â± 1.8  91.6 Â± 1.0  93.8 Â± 1.6     92.7 Â± 1.0     93.5 Â± 1.4 (â†‘1.3)\n        QQP       72.2 Â± 0.9  69.6 Â± 1.7  69.7 Â± 1.5  73.1 Â± 1.5  73.0 Â± 1.8     70.7 Â± 2.8    74.5 Â± 2.1 (â†‘2.3)\n            SST-2      90.8 Â± 1.6  89.8 Â± 0.8  89.7 Â± 0.9  91.2 Â± 1.8  91.4 Â± 2.3     90.7 Â± 2.1    93.8 Â± 1.3 (â†‘3.0)\n      NLI  MRPC     91.6 Â± 2.1  90.8 Â± 1.4  91.1 Â± 1.7  70.9 Â± 2.9  92.1 Â± 1.2     91.2 Â± 1.7    94.5 Â± 1.6 (â†‘2.9)\n          CoLA      70.7 Â± 1.6  66.9 Â± 2.1  66.9 Â± 2.4  70.1 Â± 2.5  68.7 Â± 2.6     71.1 Â± 1.1    72.2 Â± 1.3 (â†‘1.5)\n         WNLI      91.9 Â± 1.4  88.7 Â± 1.4  88.9 Â± 1.8  90.0 Â± 2.2  89.9 Â± 2.0     89.6 Â± 1.5    92.8 Â± 1.7 (â†‘0.9)\n         RTE       94.1 Â± 1.3  89.5 Â± 1.2  89.6 Â± 1.1  72.0 Â± 1.8  93.1 Â± 1.1     91.4 Â± 1.2    96.2 Â± 1.7 (â†‘2.1)\n\n\nAppendix B. Proof of Theorem 1\n\nBy the assumption that A(xi) = yi, we have for each i with yi = k:\n                       wâŠ¤k xi + bk > wâŠ¤j xi + bj,  âˆ€j Ì¸= k.\n\nTherefore, xi lies in the region\n\n                         n                           o                     Rk :=  x âˆˆRd wâŠ¤k x + bk > wâŠ¤j x + bj âˆ€j Ì¸= k   ,\n\nwhich is the intersection of K âˆ’1 open half-spaces and hence is a convex open polyhedron.\n   Because the regions are defined by strict inequalities, any two distinct regions Rk and Rj are disjoint:\n\n                              Rk âˆ©Rj = âˆ…,  âˆ€k Ì¸= j.\n\nFurthermore, since each xi lies in one of finitely many disjoint convex regions, and the dataset is finite,\nthere exists a minimum separation margin:\n\n                                     Î´ :=   min   âˆ¥x âˆ’xâ€²âˆ¥> 0.\n                                              xâˆˆSk,xâ€²âˆˆSj\n                                                           kÌ¸=j\n\n   Assume now that the data points {xi} are sampled from a smooth probability distribution P sup-\nported on a compact subset of Rd. Then, for each class k, the conditional distribution P(x | y = k) is\nsupported within Rk.\n\n\n                                                15\n\nZhang, Zhang, Yao, Niu, Sugiyama\n\n\n   Since Rk is convex and bounded (from finite data), and P is smooth, the support of P(x | y = k) is\na compact, connected set with locally regular density. This satisfies the regularity conditions for being\nlocally approximated by a smooth low-dimensional manifold Mk âŠ‚Rk.\n   Therefore, the dataset exhibits a multi-manifold structure, with each class associated to a well-\nseparated, compact, structured region in Rd.\n\n\nAppendix C. Implementation Details\n\nIn this section, we present the prompts (manual templates) used by TextGrad for each dataset.\n\n\nC.1 Prompt Design in TextGrad\n\nFor every task we compose a system prompt that fixes the global behaviour of GPT-4o and a task\nprompt that encodes the input variables.\n   The forward model receives the concatenation: <task-prompt> + <in-context demos> + <query>.\n\nConfidence filter. A sample is kept in the loss only if\n\n                            max pÎ¸(y = c | x) â‰¥0.80\n                                                      c\n\n   This threshold was tuned once on GLUE and reused everywhere else.\n\nHyper-parameters.\n\n  â€¢ Optimiser: TGD (step size 1.0, temperature 0.7);\n\n  â€¢ Prompt length cap: 256 GPT-4o tokens;\n\n  â€¢ Demonstrations per query: K = 4;\n\n  â€¢ PAPO iterations T: 10 (classification) / 5 (reasoning datasets).\n\n\nC.2 Prompt Design for Each Task\n\nAppendix D. Illustrative Example\n\nIn this section, we present the optimized prompts for the SimpleQA (Wei et al., 2024) dataset as an\nillustration.\n\n   Example of the SimpleQA dataset\n\n  Prompt at initialization:\n   You will answer a general-knowledge question on $topic topic. Always conclude the last line of your\n   response should be of the following format: â€™Answer: $VALUEâ€™ where VALUE is a $answer type\n   value.â€\n\n  Prompt refined by PAPO:\n   You will answer a general-knowledge question. Restate the question in your own words to ensure\n   understanding. Compare it with the examples provided above, note any shared entities and rela-\n    tions. Reason through the composition using evidence from both the question and demonstrations.\n   Cross Check your conclusion, ensure it does not contradict any high confidence example. Always\n   conclude the last line of your response should be of the following format: â€™Answer: $VALUEâ€™\n   where VALUE is a $answer type value.â€\n\n\n\n\n\n                                                16\n\nIn-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement\n\n\n\n\n\n          Dataset                                  Initial prompt z0\n\n         SST-2           Review: {sentence}, Options: {options}. Answer:\n       CoLA           Sentence: {sentence} Options: {options}. Answer:\n       MNLI          Premise: {premise}\\nHypothesis: {hypothesis}\\nOptions: {\n                           options}. Answer:\n      QQP            Question 1: {question1}\\nQuestion 2: {question2}\\nOptions:\n                           {options}. Answer:\n      MRPC         Sentence 1: {sentence1}\\nSentence 2: {sentence2}\\nOptions:\n                           {options}. Answer:\n       RTE            Premise: {sentence1}\\nHypothesis: {sentence2}\\nOptions: {\n                           options}. Answer:\n       WNLI          Sentence 1: {sentence1}\\nSentence 2: {sentence2}\\nOptions:\n                           {options}. Answer:\n       CAIS/MMLU  Question: {question}, Options: {options}. Answer:\n        SimpleQA      You will answer a general-knowledge question on $topic topic. Always\n                             conclude the last line of your response should be of the following\n                              format: â€™Answer: $VALUEâ€™ where VALUE is a $answer type value.â€\n      GPQA         You will answer a professional knowledge question. Think step-by-\n                                 step. Always finish with Answer: $OPTION where OPTION is the\n                                   letter of the correct choice.\n\n\n              Table 4: Initial prompt templates for all datasets evaluated in the paper.\n\n\n\n\n\n                                                17",
"headers": [
"arXiv:2410.03124v2  [cs.CL]  26 May 2025",
"ğŸ”¥",
"â„",
"In-context Demonstration Matters: On Prompt Optimization",
"for Pseudo-Supervision Refinement"
],
"tables": [
"|Col1|w/o Human<br>Feedback|w/o Fine-<br>Tuning|\n|---|---|---|\n|RLHF|âŒ|âŒ|\n|TTA|âŒ|âœ…|\n|Self-Refine|âœ…|âŒ|\n|PAPO|âœ…|âœ…|",
"|ğ±<br>!\"|ğ‘¦\"|\n|---|---|\n|ğ±!\"|\"ğ‘¦|\n|ğ±!\"|\"ğ‘¦|",
"|ğ±<br>#$%&|pred|\n|---|---|\n|ğ±#$%&|pred|\n|ğ±#$%&|pred|",
"|ğ±<br>!\"|ğ‘¦\"|\n|---|---|\n|ğ±!\"|\"ğ‘¦|\n|ğ±!\"|\"ğ‘¦|",
"|earning|wi|th|\n|---|---|---|\n|he prom<br>|pt<br>|an<br>   â€|\n|confden|ce|d|",
"|lts are in bold|and (â†‘Â·) indicates the improvement over Direct in terms of average accuracy.|\n|---|---|\n|**Task Dataset**|Direct<br>ICL<br>Auto-CoT<br>USP<br>SR(BDPL) SR(RLprompt)<br>PAPO|\n|QA<br>GPQA<br>SimpleQA<br>MAR<br>MAN<br>HSM<br>HCS<br>CMed<br>CMath<br>CCS<br>AST<br>RND|37.9_ Â±_ 1.3 37.3_ Â±_ 0.9 38.4_ Â±_ 0.5<br>38.6_ Â±_ 0.7<br>37.9_ Â±_ 1.0<br>37.5_ Â±_ 0.9<br>**39.9**_ Â±_** 0.6 (**_â†‘_**2.0)**<br> 38.2_ Â±_ 0.8 37.5_ Â±_ 1.2 38.9_ Â±_ 1.0<br>38.2_ Â±_ 0.9<br>38.1_ Â±_ 1.3<br>37.4_ Â±_ 1.1<br>**39.6**_ Â±_** 0.9 (**_â†‘_**1.4)**<br>90.2_ Â±_ 2.0 90.7_ Â±_ 1.7 88.9_ Â±_ 1.7** 92.4**_ Â±_** 0.9**<br>91.3_ Â±_ 1.8<br>91.0_ Â±_ 0.8<br>92.1_ Â±_ 0.8** (**_â†‘_**1.9)**<br>76.8_ Â±_ 1.4 76.4_ Â±_ 1.0 76.5_ Â±_ 1.0<br>77.5_ Â±_ 1.6<br>79.0_ Â±_ 1.2<br>78.2_ Â±_ 0.9<br>**81.1**_ Â±_** 1.4 (**_â†‘_**4.3)**<br>50.9_ Â±_ 2.9 47.5_ Â±_ 2.2 47.4_ Â±_ 2.2<br>51.4_ Â±_ 2.3<br>53.4_ Â±_ 1.8<br>53.2_ Â±_ 1.1<br>**55.6**_ Â±_** 1.6 (**_â†‘_**4.7)**<br>90.8_ Â±_ 2.7 91.0_ Â±_ 2.1 89.1_ Â±_ 2.1<br>89.9_ Â±_ 2.3<br>92.5_ Â±_ 2.1<br>91.3_ Â±_ 2.2<br>**93.1**_ Â±_** 1.4 (**_â†‘_**2.3)**<br>61.9_ Â±_ 1.8 58.4_ Â±_ 3.4 58.4_ Â±_ 3.4<br>61.8_ Â±_ 2.1<br>61.4_ Â±_ 1.7<br>59.5_ Â±_ 3.0<br>**63.8**_ Â±_** 2.3 (**_â†‘_**1.9)**<br>40.7_ Â±_ 4.2 40.8_ Â±_ 2.5 40.2_ Â±_ 2.5<br>41.1_ Â±_ 2.8<br>44.3_ Â±_ 2.7<br>43.3_ Â±_ 1.3<br>**46.1**_ Â±_** 1.6 (**_â†‘_**5.4)**<br>68.4_ Â±_ 2.4 71.5_ Â±_ 1.3 69.6_ Â±_ 1.3<br>69.8_ Â±_ 2.3<br>71.8_ Â±_ 1.8<br>71.0_ Â±_ 1.6<br>**73.2**_ Â±_** 1.0 (**_â†‘_**4.8)**<br>86.6_ Â±_ 2.5 86.8_ Â±_ 2.3 86.5_ Â±_ 2.3<br>87.1_ Â±_ 2.1<br>85.6_ Â±_ 3.6<br>**88.0**_ Â±_** 2.8**<br>87.2_ Â±_ 1.5** (**_â†‘_**0.6)**<br>68.7_ Â±_ 1.1 68.9_ Â±_ 1.2 68.3_ Â±_ 1.2<br>70.4_ Â±_ 1.7<br>70.6_ Â±_ 1.7<br>70.5_ Â±_ 1.3<br>**72.8**_ Â±_** 2.0 (**_â†‘_**4.1)**|\n|NLI<br>MNLI<br>QQP<br>SST-2<br>MRPC<br>CoLA<br>WNLI<br>RTE|91.7_ Â±_ 2.3 90.4_ Â±_ 2.0 90.4_ Â±_ 2.0<br>90.8_ Â±_ 1.1<br>**92.8**_ Â±_** 1.6**<br>92.1_ Â±_ 0.8<br>92.0_ Â±_ 1.8** (**_â†‘_**0.3)**<br>71.5_ Â±_ 1.0 71.6_ Â±_ 2.0 68.6_ Â±_ 2.0<br>71.8_ Â±_ 1.6<br>71.9_ Â±_ 1.6<br>69.3_ Â±_ 3.1<br>**73.2**_ Â±_** 2.0 (**_â†‘_**1.7)**<br>89.6_ Â±_ 1.5 88.4_ Â±_ 0.7 88.4_ Â±_ 0.7<br>90.0_ Â±_ 1.9<br>90.3_ Â±_ 2.1<br>89.6_ Â±_ 2.0<br>**92.7**_ Â±_** 1.1 (**_â†‘_**3.1)**<br>90.9_ Â±_ 2.0 91.0_ Â±_ 1.5 90.1_ Â±_ 1.5<br>69.8_ Â±_ 2.8<br>90.9_ Â±_ 1.0<br>90.1_ Â±_ 1.8<br>**93.4**_ Â±_** 1.7 (**_â†‘_**2.5)**<br>69.7_ Â±_ 1.7 69.7_ Â±_ 2.3 65.8_ Â±_ 2.3<br>68.9_ Â±_ 2.3<br>67.4_ Â±_ 2.9<br>69.9_ Â±_ 1.3<br>**71.2**_ Â±_** 1.1 (**_â†‘_**1.5)**<br>90.8_ Â±_ 1.6 87.3_ Â±_ 1.7 87.3_ Â±_ 1.7<br>89.0_ Â±_ 2.3<br>88.8_ Â±_ 2.0<br>88.5_ Â±_ 1.8<br>**91.1**_ Â±_** 1.4 (**_â†‘_**1.1)**<br>92.9_ Â±_ 1.2 93.1_ Â±_ 1.0 88.7_ Â±_ 1.0<br>70.5_ Â±_ 2.1<br>91.9_ Â±_ 1.3<br>90.0_ Â±_ 1.5<br>**94.9**_ Â±_** 1.6 (**_â†‘_**2.0)**|",
"|90|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>0.<br>Confidence Threshold ( )<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>Accuracy (%)<br>SS<br>RT<br>MN<br>MM|||||||SS<br>RT<br>MN|\n|0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>0.<br>Confidence Threshold ( )<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>Accuracy (%)<br>SS<br>RT<br>MN<br>MM|||||||MM|",
"|GPT-4o-mini Direct<br>GPT-4o-mini-Ours<br>80 G GP PT T- -4 4- -D Oi ure rsct<br>60 (%)<br>Accuracy<br>40<br>20<br>0 GLUE(SST2) MMLU(RND)<br>Dataset|Col2|Col3|Col4|GP<br>GP<br>GP<br>GP|T-4o-mini Direct<br>T-4o-mini-Ours<br>T-4-Direct<br>T-4-Ours|\n|---|---|---|---|---|---|",
"|bold.|Col2|\n|---|---|\n|Method|MNLI<br>QQP<br>SST-2<br>MRPC<br>CoLA<br>WNLI<br>RTE<br>RND|\n|Direct|91.7_ Â±_ 2.3<br>71.4_ Â±_ 1.0<br>89.6_ Â±_ 1.5<br>90.9_ Â±_ 2.0<br>69.7_ Â±_ 1.7<br>90.8_ Â±_ 1.6<br>92.9_ Â±_ 1.2<br>68.7_ Â±_ 1.1|\n|ICL (_k_ = 3)<br>ICL(_k_ = 5)|89.3_ Â±_ 1.9<br>68.5_ Â±_ 2.1<br>88.9_ Â±_ 2.4<br>88.3_ Â±_ 1.7<br>66.4_ Â±_ 2.3<br>87.5_ Â±_ 1.7<br>88.3_ Â±_ 1.2<br>67.5_ Â±_ 1.5<br>90.4_ Â±_ 2.0<br>71.6_ Â±_ 2.0<br>88.4_ Â±_ 0.7<br>91.0_ Â±_ 1.5<br>69.7_ Â±_ 2.3<br>87.3_ Â±_ 1.7<br>93.1_ Â±_ 1.0<br>68.9_ Â±_ 1.2|\n|PAPO (_k_ = 3)<br>PAPO(_k_ = 5)|91.5_ Â±_ 2.1<br>72.5_ Â±_ 2.1<br>91.3_ Â±_ 1.7<br>92.3_ Â±_ 1.8<br>**71.8**_ Â±_** 1.5**<br>91.0_ Â±_ 1.7<br>93.1_ Â±_ 2.0<br>71.5_ Â±_ 2.6<br>** 92.0**_ Â±_** 1.8 73.2**_ Â±_** 2.0 92.7**_ Â±_** 1.1 93.4**_ Â±_** 1.7**<br>71.2_ Â±_ 1.1<br>**91.1**_ Â±_** 1.4 94.9**_ Â±_** 1.6 72.8**_ Â±_** 2.0**|",
"|ts are in bold.|Col2|\n|---|---|\n|**Task Dataset**|Direct<br>ICL<br>Auto-CoT<br>USP<br>SR(BDPL) SR(RLprompt)<br>PAPO|\n|QA<br>GPQA<br>SimpleQA<br>MAR<br>MAN<br>HSM<br>HCS<br>CMed<br>CMath<br>CCS<br>AST<br>RND|38.5_ Â±_ 1.7 38.7_ Â±_ 1.0 39.5_ Â±_ 0.7 39.5_ Â±_ 0.2<br>38.2_ Â±_ 1.5<br>37.8_ Â±_ 1.2<br>**40.1**_ Â±_** 0.3 (**_â†‘_**1.6)**<br> 38.6_ Â±_ 0.4 37.9_ Â±_ 1.0 39.4_ Â±_ 0.9 39.0_ Â±_ 0.9<br>38.8_ Â±_ 1.6<br>37.9_ Â±_ 0.8<br>**40.6**_ Â±_** 0.9 (**_â†‘_**2.0)**<br>91.1_ Â±_ 2.3 88.9_ Â±_ 1.5 89.9_ Â±_ 1.3 92.7_ Â±_ 1.1<br>91.5_ Â±_ 1.7<br>92.4_ Â±_ 0.4<br>**93.6**_ Â±_** 0.8 (**_â†‘_**2.5)**<br>76.9_ Â±_ 1.1 77.8_ Â±_ 1.5 77.0_ Â±_ 1.3 78.5_ Â±_ 2.0<br>79.5_ Â±_ 1.6<br>79.0_ Â±_ 1.0<br>**82.0**_ Â±_** 1.8 (**_â†‘_**5.1)**<br>51.1_ Â±_ 2.8 47.9_ Â±_ 2.0 47.6_ Â±_ 2.2 52.0_ Â±_ 1.9<br>54.0_ Â±_ 2.1<br>53.7_ Â±_ 0.7<br>**56.9**_ Â±_** 2.1 (**_â†‘_**5.8)**<br>91.6_ Â±_ 2.5 89.6_ Â±_ 2.4 89.7_ Â±_ 1.8 91.6_ Â±_ 1.8<br>93.7_ Â±_ 2.0<br>92.2_ Â±_ 1.7<br>**94.1**_ Â±_** 1.7 (**_â†‘_**2.5)**<br>62.9_ Â±_ 1.5 59.9_ Â±_ 2.9 59.4_ Â±_ 3.0 62.6_ Â±_ 2.5<br>62.7_ Â±_ 1.9<br>61.2_ Â±_ 2.6<br>**64.1**_ Â±_** 2.4 (**_â†‘_**1.2)**<br>41.6_ Â±_ 4.0 41.2_ Â±_ 2.3 41.3_ Â±_ 2.0 42.4_ Â±_ 2.9<br>45.1_ Â±_ 2.6<br>44.3_ Â±_ 1.6<br>**47.2**_ Â±_** 1.8 (**_â†‘_**5.6)**<br>69.7_ Â±_ 2.0 70.8_ Â±_ 1.4 70.6_ Â±_ 1.1 70.6_ Â±_ 2.5<br>72.9_ Â±_ 1.6<br>72.4_ Â±_ 1.8<br>**74.7**_ Â±_** 1.3 (**_â†‘_**5.0)**<br>87.4_ Â±_ 2.6 87.7_ Â±_ 2.4 87.3_ Â±_ 2.1 88.5_ Â±_ 2.1<br>86.7_ Â±_ 3.1<br>**89.1**_ Â±_** 2.4**<br>88.7_ Â±_ 1.9** (**_â†‘_**1.3)**<br>69.4_ Â±_ 1.3 69.3_ Â±_ 1.4 69.5_ Â±_ 1.1 71.3_ Â±_ 1.4<br>71.9_ Â±_ 1.4<br>71.6_ Â±_ 1.0<br>**73.8**_ Â±_** 1.8 (**_â†‘_**4.4)**|\n|NLI<br>MNLI<br>QQP<br>SST-2<br>MRPC<br>CoLA<br>WNLI<br>RTE|92.2_ Â±_ 2.1 91.2_ Â±_ 1.6 91.4_ Â±_ 1.8 91.6_ Â±_ 1.0** 93.8**_ Â±_** 1.6**<br>92.7_ Â±_ 1.0<br>93.5_ Â±_ 1.4** (**_â†‘_**1.3)**<br>72.2_ Â±_ 0.9 69.6_ Â±_ 1.7 69.7_ Â±_ 1.5 73.1_ Â±_ 1.5<br>73.0_ Â±_ 1.8<br>70.7_ Â±_ 2.8<br>**74.5**_ Â±_** 2.1 (**_â†‘_**2.3)**<br>90.8_ Â±_ 1.6 89.8_ Â±_ 0.8 89.7_ Â±_ 0.9 91.2_ Â±_ 1.8<br>91.4_ Â±_ 2.3<br>90.7_ Â±_ 2.1<br>**93.8**_ Â±_** 1.3 (**_â†‘_**3.0)**<br>91.6_ Â±_ 2.1 90.8_ Â±_ 1.4 91.1_ Â±_ 1.7 70.9_ Â±_ 2.9<br>92.1_ Â±_ 1.2<br>91.2_ Â±_ 1.7<br>**94.5**_ Â±_** 1.6 (**_â†‘_**2.9)**<br>70.7_ Â±_ 1.6 66.9_ Â±_ 2.1 66.9_ Â±_ 2.4 70.1_ Â±_ 2.5<br>68.7_ Â±_ 2.6<br>71.1_ Â±_ 1.1<br>**72.2**_ Â±_** 1.3 (**_â†‘_**1.5)**<br>91.9_ Â±_ 1.4 88.7_ Â±_ 1.4 88.9_ Â±_ 1.8 90.0_ Â±_ 2.2<br>89.9_ Â±_ 2.0<br>89.6_ Â±_ 1.5<br>**92.8**_ Â±_** 1.7 (**_â†‘_**0.9)**<br>94.1_ Â±_ 1.3 89.5_ Â±_ 1.2 89.6_ Â±_ 1.1 72.0_ Â±_ 1.8<br>93.1_ Â±_ 1.1<br>91.4_ Â±_ 1.2<br>**96.2**_ Â±_** 1.7 (**_â†‘_**2.1)**|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2410.03124v2.pdf"
}