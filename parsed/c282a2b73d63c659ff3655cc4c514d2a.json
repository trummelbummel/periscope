{
"text": "J6: Jacobian-Driven Role Attribution for\n               Multi-Objective Prompt Optimization in LLMs\n\n\n\n\n                                                   Yao Wu\n                                                      School of Engineering\n                                                        Westlake University\n                                                      Hangzhou, China\n                                              wuyao@westlake.edu.cn\n2025\nAug                         In large language model (LLM) adaptation,Abstractbalancing multiple optimization ob-\n                             jectives—such as improving factuality (heat) and increasing confidence (via low\n16                      entropy)—poses a fundamental challenge, especially when prompt parameters\n                                       (e.g., hidden-layer insertions h and embedding modifications w) interact in non-\n                                       trivial ways. Existing multi-objective optimization strategies often rely on scalar\n                                gradient aggregation, ignoring the deeper geometric structure between objec-\n                                   tives and parameters. We propose J6, a structured Jacobian-based method that\n                           decomposes the gradient interaction matrix into six interpretable components:\n                               [∥J11∥2, ⟨J11, J22⟩, ∥J12∥2, ∥J21∥2, ∥J22∥2, ⟨J21, J12⟩]. This decomposition[cs.CL]                               enables both hard decision-making (e.g., choosing the dominant update direction\n                                via arg max) and soft strategies (e.g., attention-style weighting via softmax over\n                                     J6), forming a dynamic update framework that adapts to local conflict and synergy.\n                             Moreover, the interpretable structure of J6 provides insight into parameter attri-\n                                 bution, task interference, and geometry-aligned adaptation. Our work introduces\n                             a principled and extensible mechanism for conflict-aware prompt optimization,\n                           and opens a new avenue for incorporating structured Jacobian reasoning into\n                                 multi-objective neural tuning.\n\n\n                1  Introduction\n\n                     Large Language Models (LLMs) are increasingly deployed in real-world scenarios that demand\n                       not only high accuracy (e.g., factual correctness), but also strong reliability (e.g., output certainty).\n                      This dual requirement gives rise to a growing need for multi-objective prompt optimization, wherearXiv:2508.12086v1                  practitioners aim to improve both fidelity and certainty simultaneously—two goals that are often at\n                     odds during model inference Nema et al. [2025], Wang et al. [2024].\n\n                       Existing prompt tuning approaches typically optimize a single scalar loss or apply fixed-weight\n                        multi-task objectives. However, these approaches treat all prompt parameters homogeneously and\n                             fail to model the underlying structure between different parameter types and their contributions to\n                          different objectives Dun et al. [2023]. Specifically, they overlook the fact that different perturbation\n                   types—such as hidden-layer perturbations (h) and embedding perturbations (w)—may affect each\n                        objective differently, and often nonlinearly, due to the multiplicative nature of the logit computation:\n                (H + h)(W + w)T .\n\n             A natural yet flawed solution is to statically assign h to optimize fidelity (i.e., cross-entropy\n                       minimization) and w to optimize certainty (i.e., entropy minimization). However, such a rigid role\n                      assignment is overly simplistic and brittle in practice. Both h and w may influence both objectives in\n                       entangled and dynamic ways Wang et al. [2024]. A hidden-layer perturbation that improves answer\n\n\n                        38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n\naccuracy may simultaneously disrupt confidence, and vice versa. In such settings, pre-assigning roles\nleads to suboptimal updates and gradient conflicts.\n\nWe argue for a fundamentally different perspective: instead of statically binding h and w to fixed\ntasks, we enable dynamic and competitive role attribution, where each parameter group autonomously\ndecides—at each optimization step—whether to respond to one objective, both, or neither. This\nallows parameters to behave as adaptive agents that sniff task relevance based on local gradient\nsignals Quinton and Rey [2025].\n\nTo implement this, we introduce J6, a Jacobian-based attribution framework that reveals the fine-\ngrained interaction structure between objectives and parameter groups. At each step, we compute\na local 2 × 2 Jacobian matrix J over two objectives (heat and confidence) and two parameter\ngroups (h, w), and decompose J into six interpretable components:\n\n                                                   \n           J6 = ∥J11∥2 ,   ⟨J11, J22⟩,  ∥J12∥2 ,  ∥J21∥2 ,  ∥J22∥2 ,   ⟨J21, J12⟩\n                 |h→Heat{z }   | Alignment{z  }   |w→Heat{z }   |h→Conf{z }   |w→Conf{z }   | Alignment{z  }\n\nEach J6 entry quantifies either a role-specific contribution (e.g., h →heat) or a structural interaction\n(e.g., cross-objective alignment) within the gradient space.\n\nBuilding on this decomposition, we propose two update schemes:\n\n        • Hard Strategy: Select the maximal entry in the J6 vector to determine which parameter\n         to update and which objective to prioritize.  This produces interpretable, discrete role\n        assignments at each step.\n        • Soft Strategy: Normalize J6 scores via softmax and use them as continuous weights to\n        guide simultaneous updates across h and w, allowing gradient blending without dimensional\n         expansion.\n\nBeyond J6, we further extend this idea to a richer gradient attribution space J+, comprising 15 high-\norder alignment terms that account for cross-role synergy, intra-role consistency, and asymmetric\ndominance. This extended view is not used directly in training, but provides the theoretical foundation\nfor our Soft Strategy. Notably, we show that a properly weighted J6 vector is sufficient to emulate the\noptimization behavior of J+—offering high expressivity without incurring extra computation.\n\nEmpirically, we evaluate J6 on three benchmarks—MathQA, GSM8K, and TruthfulQA—under settings\nwith severe objective conflicts. Our method consistently outperforms strong baselines including Slot,\nPCGrad, and Pareto-based methods Nema et al. [2025], while offering insightful visualizations of\nhow h and w adapt roles over time. These findings demonstrate that Jacobian-driven role attribution\nis not only effective, but also interpretable and extensible.\n\nContributions. This paper makes the following key contributions:\n\n      1. We introduce the problem of role attribution in multi-objective prompt tuning, and formalize\n             it through Jacobian decomposition Quinton and Rey [2025].\n      2. We propose J6, a simple yet expressive mechanism that supports both hard (argmax) and\n          soft (weighted) update strategies.\n      3. We extend the attribution space to a 15-term vector J+, and show that soft-weighted J6\n          suffices to emulate it.\n      4. We demonstrate strong empirical results on three tasks, along with interpretable analysis of\n         parameter-objective dynamics.\n\n2  Related Work\n\n2.1  Multi-Objective Optimization in Deep Learning\n\nBalancing multiple, often conflicting, objectives has been a long-standing challenge in multi-task and\nmulti-objective learning. A classic solution is PCGrad Yu et al. [2020], which reduces destructive\ninterference by projecting gradients onto the normal plane of each other. ParetoMTL Lin et al. [2019]\n\n\n                                       2\n\nexplores Pareto frontiers to generate non-dominated trade-offs across objectives. Geometry-aware\napproaches Sener and Koltun [2018] further introduce vector projections and alignment constraints to\ndecouple task interactions.\n\nWhile effective at the global task level, these methods typically operate on coarse-grained gradient\nrepresentations and overlook the heterogeneous roles that different parameter groups may play. In\ncontrast, our approach performs local, parameter-level decomposition via a Jacobian-based scoring\nmechanism, enabling fine-grained control over optimization responsibilities within the model.\n\n\n2.2  Prompt Tuning and Delta-Based Adaptation\n\nPrompt tuning has emerged as a lightweight alternative to full fine-tuning, allowing efficient adaptation\nof LLMs with minimal additional parameters Lester et al. [2021]. Methods such as Prefix Tuning Li\nand Liang [2021] and LoRA Hu et al. [2021] introduce trainable vectors or low-rank matrices into\nintermediate activations or weight spaces. Delta Tuning Ding et al. [2022] generalizes this idea by\nenabling parameter insertions at arbitrary locations, making adaptation more modular and extensible.\n\n\nSLOT Method\n\nSLOT Hu et al. [2025] introduces instance-level adaptation at test time by injecting a perturbation\ninto the hidden state of the LLM. Given the final hidden state H and output projection matrix W,\nSLOT modifies the logits as:\n                                            logits′ = (H + δ)W ⊤\n\nwhere δ is a test-time learned perturbation, optimized via cross-entropy over the same prompt:\n\n                         δ = arg min CrossEntropy((H + δ)W ⊤, y)\n                                          δ\n\nThis hidden-layer intervention tunes the model’s behavior without retraining, improving sample-\nspecific alignment.\n\nThis optimization encourages the model to better align its internal representations with prompt intent,\nserving as an effective one-shot alignment strategy under limited supervision. Despite their flexibility,\nthese methods typically collapse multi-objective behaviors (e.g., accuracy and confidence) into a\nsingle scalar loss, making them vulnerable to hidden conflicts among competing goals. For example, a\nperturbation that enhances token prediction (fidelity) might inadvertently inflate entropy (uncertainty).\n\nTo overcome this limitation, we propose J6, a Jacobian-guided attribution mechanism that explicitly\nseparates gradient responsibilities across multiple objectives. Unlike SLOT’s single-loss formulation,\nJ6 empowers parameters such as h and w to dynamically specialize in response to distinct objectives,\nHeat (accuracy) and Confidence (certainty), based on fine-grained Jacobian signals. This allows\nfor more interpretable, role-specific optimization and mitigates gradient interference during multi-\nobjective prompt adaptation.\n\n\nEM-INF\n\nEM-INF Agarwal et al. [2025] directly optimizes entropy at the logits layer:\n\n                                  Lentropy = −Entropy(softmax(logits))\n\nBy targeting certainty as an independent goal, EM-INF enhances confidence in complex reasoning\ntasks. Yet, this can degrade factual accuracy if confidence diverges from correctness.\n\nOur method draws from EM-INF’s focus on entropy but embeds it into a two-objective system. We\nuse the Jacobian structure to allocate optimization pressure across parameters, allowing fidelity and\ncertainty to co-adapt rather than compete blindly.\n\n\n2.3  Jacobian-Based Modeling and Optimization\n\nJacobian matrices have long been used to analyze neural network sensitivity Sundararajan et al.\n[2017], generalization via neural tangent kernels Jacot et al. [2018], and regularization Drucker\nand Le Cun [1992]. More recently, per-layer Jacobians have been applied to investigate learning\n\n\n                                       3\n\ndynamics Fort et al. [2021] and to improve representation alignment in multi-view learning Navon\net al. [2023].\n\nHowever, existing works primarily use Jacobians as static diagnostics or post-hoc analytical tools. In\ncontrast, our work treats the Jacobian as a first-class optimization signal, dynamically decomposing\nits local structure during training to guide parameter-objective assignment. This shift transforms the\nJacobian from a passive monitor into an active control mechanism.\n\n2.4  Factuality vs. Confidence in LLMs\n\nMaintaining both factual accuracy and confident output generation is a central concern for large\nlanguage models Kadavath et al. [2022]. Prior efforts such as TruthfulQA Lin et al. [2022] and\nSelfCheckGPT Manakul and Gales [2023] expose the trade-off between informativeness and entropy.\nEntropy filtering Lee et al. [2022] and output calibration have been proposed as inference-time\nremedies, but they do not intervene during model optimization.\n\nOur method addresses this gap by directly incorporating the dual objectives of fidelity (via cross-\nentropy) and certainty (via entropy minimization) into the training process. Using Jacobian-based\nattribution, we enable the model to resolve their competition adaptively during learning, rather than\nrelying on fixed heuristics or post-hoc adjustments.\n\n3  Method\n\nWe propose the J6 Strategy, a Jacobian-guided optimization framework for multi-objective prompt\ntuning. Our method aims to jointly optimize two often competing goals in large language model\n(LLM) inference: fidelity (i.e., task accuracy, denoted as Heat) and certainty (i.e., output confidence,\ndenoted as Confidence). Unlike traditional approaches that bind each objective to specific parame-\nters or scalarize multiple losses into a single function, J6 introduces a structured and interpretable\nmechanism for dynamically allocating optimization responsibility across parameter groups.\n\nWe begin by formalizing the problem setup and highlighting the limitations of static role assignments\nin multi-objective tuning. We then introduce the Jacobian Interaction Matrix—a local 2 × 2 matrix\ncapturing how each objective gradient aligns with each parameter direction—and decompose it into\nthe J6 score vector. This vector encodes role-specific effectiveness and cross-objective alignment.\n\nFinally, we describe two optimization modes built a top J6: a hard routing strategy that selects\ndominant roles via argmax, and a soft weighting strategy that adaptively blends gradients based on\nnormalized Jacobian scores. Together, these components enable flexible, conflict-aware updates\nwithout increasing model dimensionality.\n\n3.1  Problem Formulation\n\nWe consider a prompt-tuning setting where the goal is to adapt a frozen Large Language Model\n(LLM) to specific tasks while simultaneously balancing two optimization objectives. To enable\nlightweight yet expressive adaptation, we introduce two additive parameter groups:\n\n        • h ∈Rdh: perturbations applied to the hidden layers,\n        • w ∈Rdw: perturbations applied to the vocabulary embedding layer.\n\nThese parameters modulate the model’s output logits, which we approximate as:\n                                      logits = (H + h)(W + w)T ,\n\nwhere H and W represent the frozen base representations (i.e., the hidden states and vocabulary\nembeddings of the pretrained LLM), and (h, w) are the tunable low-rank adaptations. Specifically,\nW corresponds to the pre-trained vocabulary embeddings, and the perturbation w is added to each\nembedding vector, adjusting how each token interacts with the hidden state.\n\nWe define the objectives as:\n\n         ob1(h, w) = CrossEntropy(logits, y)  (Heat) Hu et al. [2025]                       (1)\n         ob2(h, w) = −Entropy(softmax(logits))  (Confidence) Agarwal et al. [2025]       (2)\n\n\n                                       4\n\nThe Heat loss is derived from the cross-entropy between the model’s logits and the true target,\nensuring task alignment. The Confidence loss minimizes the entropy of the softmax probabilities,\nencouraging confident predictions. In the SLOT framework, this loss can be interpreted as the model’s\nself-prediction of the next token, guiding parameter optimization for task-specific tuning.\n\nHowever, these objectives often conflict. In tasks requiring reasoning or domain adaptation, improving\naccuracy may reduce confidence, and vice versa. Our goal is to design a mechanism that dynamically\nbalances these objectives by routing gradient influence through the appropriate parameter group,\nleveraging geometric attribution.\n\n3.2  Motivation: Dynamic Role Attribution\n\nTraditional prompt tuning methods statically assign specific roles to hidden perturbations (h) and\nembedding perturbations (w)—where h is responsible for optimizing Heat (cross-entropy) and w\noptimizes Confidence (entropy minimization). However, this rigid assignment often fails to capture\nthe dynamic interactions between these two tasks in practice.\n\nGiven the multiplicative structure of the logits in large language models (LLMs):\n\n                                      logits = (H + h)(W + w)T ,\n\nupdates to h and w are inherently entangled—each parameter can influence both objectives in\ncomplex, unpredictable ways. For example, adjusting h for improved accuracy can inadvertently\naffect the confidence, and modifying w to enhance certainty can potentially undermine the accuracy.\n\nTo address these limitations, we propose a dynamic role attribution framework that eliminates the\nneed for fixed roles. Instead of predefining tasks for each parameter group, we allow h and w to\ndynamically choose their optimization strategy at each step. They can choose to support Heat,\nConfidence, both, or remain inactive, depending on which task they can most effectively contribute\nto at that moment.\n\nThis dynamic decision-making process is guided by the local gradient landscape. Specifically, we\nconstruct a Jacobian Interaction Matrix that quantifies how each parameter influences each objective.\nFrom this matrix, we derive the J6 score, a fine-grained signal that enables either competitive or\ncooperative updates, achieved through discrete strategy routing or soft weighting.\n\nIn this way, J6 overcomes the limitations of brittle, pre-defined heuristics by providing interpretable,\ngeometry-aware role decisions, ultimately enabling conflict-aware prompt optimization under multi-\nobjective supervision Yu et al. [2020], Liu et al. [2021].\n\n3.3  Jacobian Construction and J6 Score Vector\n\nTo capture how each group of prompt parameters affects each optimization objective, we construct\nthe local Jacobian matrix J ∈R2×2 at each step of training. It summarizes the partial derivatives of\ntwo objectives with respect to two parameter groups:\n\n                     J11  J12\n             J =                   ,  where   Jij = ∇θjobi,   θ1 = h, θ2 = w.                     J21  J22\n\nWe instantiate this Jacobian for our specific objective setting: ob1 = Heat (Cross-Entropy), and\nob2 = Confidence (Negative Entropy). Thus, J becomes:\n\n\n                            ∂ob1                                   ∂ob1\n                       ∂h                          ∂w     ∇hHeat  ∇wHeat     J11  J12              J =  ∂ob2   ∂ob2 =            =                               ∇hConf  ∇wConf     J21  J22                       ∂h   ∂w\n\nWe now define the J6 Score Vector to decompose the Jacobian into six interpretable components:\n\n\n                                                       \n    J6 = ∥J11∥2 ,   ∥J12∥2 ,     ∥J21∥2    ,     ∥J22∥2    ,    ⟨J11, J22⟩ ,    ⟨J21, J12⟩ \n          h| →Heat{z }   w| →Heat{z }   h →Confidence| {z }    w →Confidence| {z }        Cross|   Alignment{z  }      Cross|   Alignment{z  }\n\n\n                                       5\n\nEach component reflects a specific geometric or optimization property of the parameter-objective\nlandscape, as summarized in the table below:\n\nTable 1: Interpretation of each J6 score component. ∥Jij∥2 quantifies parameter-objective influence;\ninner products reflect alignment.\n\n J6 Component   Gradient Role                Optimization Interpretation\n     ∥J11∥2       h’s contribution to Heat       h →Heat\n     ∥J22∥2      w’s contribution to Confidence  w →Confidence\n     ∥J12∥2      w’s influence on Heat       w →Heat\n     ∥J21∥2       h’s influence on Confidence    h →Confidence\n     ⟨J11, J22⟩     Cross-path interaction        h →Heat and w →Confidence  coupling\n    ⟨J21, J12⟩     Cross-path interaction        h →Confidence and w →Heat  coupling\n\nEach component quantifies a distinct aspect of the local interaction geometry, including how each\nparameter group contributes to its primary objective, how it interferes with the other, and whether\nthe two groups align or conflict in their gradient directions. These interpretations are summarized in\nTable 1.\n\nThis decomposition enables interpretable and structured decision-making in prompt optimization.\nRather than applying static or uniform updates to h and w, J6 provides a geometry-aware signal that\nsupports two optimization strategies:\n\n        • Hard Strategy: discretely selects one of six roles per parameter group at each step (e.g.,\n        \"optimize for Heat\", \"remain frozen\").\n        • Soft Strategy: continuously reweights objective gradients using J6-aligned scores, enabling\n       smooth interpolation and partial cooperation across objectives.\n\nWe formalize this in the following optimization algorithm, which can be invoked at each training or\nadaptation step for a given prompt instance.\n\nAlgorithm 1 J6 Slot Optimization and Inference for a Given Prompt\nRequire: Base representations H, W; tunable parameters h, w; input prompt x and target y.\n  1: Compute logits: logits = (H + h)(W + w)T\n  2: Compute losses: ob1 (Heat), ob2 (Confidence)\n  3: Compute gradients: ∇hob1, ∇hob2, ∇wob1, ∇wob2\n  4: Construct Jacobian blocks: J11, J12, J21, J22\n  5: Compute J6 score vector:\n                 J6 =  ∥J11∥2, ⟨J11, J22⟩, ∥J12∥2, ∥J21∥2, ∥J22∥2, ⟨J21, J12⟩\n\n\n  6:  if Hard Routing then\n  7:   Assign update role to h and w based on max-scoring J6 slot\n  8:   Apply gradient update only along selected objective direction(s)\n  9: else if Soft Weighting then\n10:   Normalize J6 components to obtain gradient weights α1, α2, . . .\n11:   Combine gradients: ∇h = α(1)h ∇hob1 + α(2)h ∇hob2\n12:    Similarly compute weighted ∇w\n13: end if\n14: Update h, w via optimizer step\n\n\n3.4  J+: Fine-Grained Attribution via Expanded Jacobian\n\nThe J6 vector provides a compact representation of the key roles of h and w through six gradient\nmetrics. While useful, it offers only a coarse partition of the parameter-objective interactions and\nmisses out on capturing more complex behaviors, such as when h supports both Heat and Confidence\nsimultaneously, or when w aids one objective while conflicting with the other.\n\n\n                                       6\n\nFrom J6 to J+.  Although J6 captures the key gradient interactions between perturbation parameters\nh and w, it serves as a low-dimensional approximation of the full gradient interaction space encoded\nby J+. While J6 retains essential components like primary gradient magnitudes, interference terms,\nand alignment scores, it cannot fully represent the complexity of h and w’s interactions, particularly\nwhen they simultaneously affect multiple objectives in non-linear ways.\nTo address these limitations, we propose J+, an extended Jacobian-based scoring system that decom-\nposes the full 2 × 2 gradient Jacobian matrix J into 15 interpretable components. These components\nenumerate all possible gradient magnitudes, alignments, and cross-objective contributions, thus\nproviding a more granular and expressive optimization framework.\nThe J+ space consists of 4 × 4 −1 = 15 interpretable terms, where the \"4\" represents the four\ndirectional roles for each parameter group: h1, h2, h1 + h2, and None for h; and similarly for w. The\nsubtraction of 1 removes the degenerate case (None–None), which is uninformative in optimization.\n\nThese 15 interactions represent meaningful directional relationships between h and w, enabling both\ndiscrete and soft-weighted optimization strategies based on the gradient geometry.\n\nTable 2: J+ Gradient Interaction Matrix: Each cell represents a geometric interaction between h and\nw directions toward specific objectives.\n\n\n         w\n                  w1 (Heat)       w2 (Conf)          w1 + w2           None\n h\n    h1 (Heat)          ⟨J11, J12⟩         ⟨J11, J22⟩         ⟨J11, J12 + J22⟩        ∥J11∥2\n    h2 (Conf)          ⟨J21, J12⟩         ⟨J21, J22⟩         ⟨J21, J12 + J22⟩        ∥J21∥2\n     h1 + h2       ⟨J11 + J21, J12⟩   ⟨J11 + J21, J22⟩   ⟨J11 + J21, J12 + J22⟩   ∥J11 + J21∥2\n      None            ∥J12∥2           ∥J22∥2           ∥J12 + J22∥2        x (invalid)\n\nTherefore, we can define the J+ vector as:\n\n     ∥J11∥2,           ∥J12∥2,           ∥J21∥2,           ∥J22∥2,                  ⟨J11, J22⟩,     \nJ+ =   ⟨J21, J12⟩,         ⟨J11, J21⟩,         ⟨J12, J22⟩,         ⟨J11 + J21, J12 + J22⟩,  ⟨J11, J12 + J22⟩,                                                                     \n         ⟨J21, J12 + J22⟩,  ⟨J11 + J21, J12⟩,  ⟨J11 + J21, J22⟩,  ∥J11 + J21∥2,           ∥J12 + J22∥2\n\nCompared to J6, the J+ vector introduces additional gradient interactions and aggregated behaviors:\n\n        • ⟨J11, J21⟩— Captures h’s internal consistency across Heat and Confidence.\n        • ⟨J12, J22⟩— Captures w’s internal consistency across Heat and Confidence.\n        • ⟨J11 +J21, J12 +J22⟩— Measures total coordination between h and w over both objectives.\n        • ⟨J11, J12 + J22⟩— Measures how h (on Heat) aligns with overall w.\n        • ⟨J11 + J21, J22⟩— Measures how overall h aligns with w (on Confidence).\n        • ∥J11 + J21∥2, ∥J12 + J22∥2 — Aggregate total influence strengths of h and w, respectively.\n\nThis decomposition provides finer-grained attribution signals, allowing for nuanced role assignments\nand strategic adaptation in multi-objective tuning. J+ serves as a superset of J6, forming the\nfoundation for our soft optimization controller.\n\nThat we can conclude:\n     ∥J11∥2 ,              ∥J12∥2 ,           ∥J21∥2 ,           ∥J22∥2 ,                        ⟨J11, J22⟩        , \n        |h→Heat{z }               |w→Heat{z }            |h→Conf{z }            |w→Conf{z }                         Cross Align|   (h-Heat,{z  }w-Conf)\n              ⟨J21, J12⟩        ,  ⟨J11, J21⟩,         ⟨J12, J22⟩,         ⟨J11 + J21, J12 + J22⟩,  ⟨J11, J12 + J22⟩,\nJ+ =\n           Cross Align|   (h-Conf,{z  }w-Heat)   | h →Both{z  }         | w →Both{z  }         |      Joint Align{z (h + w)    }   |  h dominates{z  Heat }\n         ⟨J21, J12 + J22⟩,      ⟨J11 + J21, J12⟩,  ⟨J11 + J21, J22⟩,  ∥J11 + J21∥2 ,           ∥J12 + J22∥2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n        | h dominates{z  Conf }      | w dominates{z  Heat }   | w dominates{z  Conf }   | h Total{zStrength }            |w Total{zStrength}\n\n\n                                       7\n\nGiven the full Jacobian alignment matrix J+ ∈R15, we define a decision mechanism that selects the\ndominant optimization pathway by identifying the most aligned gradient channel. Specifically, we\ncompute:\n                            j_index = arg max J+j\n                                                                   j\nBased on this matrix, we compile a detailed attribution and decision table to guide both hard and soft\nstrategies under different optimization intents:\n\n     Table 3: J+ Attribution and Action Table: Strategy Selection for Hard and Soft Updates\n\n J Index   Optimization Action         J+ Component                Optimization Suggestion\n    1    h →Heat                     ∥J11∥2                        Optimize h1 only\n    2    w →Heat                    ∥J12∥2                        Optimize w1 only\n    3    h →Conf                    ∥J21∥2                        Optimize h2 only\n    4    w →Conf                    ∥J22∥2                        Optimize w2 only\n    5      Cross Align (h–Heat, w–Conf)   ⟨J11, J22⟩                          Jointly optimize h1, w2\n    6      Cross Align (h–Conf, w–Heat)   ⟨J21, J12⟩                          Jointly optimize h2, w1\n    7    h →Both                       ⟨J11, J21⟩                      Optimize both h1 + h2\n    8    w →Both                       ⟨J12, J22⟩                      Optimize both w1 + w2\n    9       Joint Align (h + w)             ⟨J11 + J21, J12 + J22⟩             Jointly optimize h1 + h2, w1 + w2\n    10    h dominates Heat                ⟨J11, J12 + J22⟩                    Prioritize h1, auxiliary w\n    11    h dominates Conf                ⟨J21, J12 + J22⟩                    Prioritize h2, auxiliary w\n    12   w dominates Heat               ⟨J11 + J21, J12⟩                    Prioritize w1, auxiliary h\n    13   w dominates Conf               ⟨J11 + J21, J22⟩                    Prioritize w2, auxiliary h\n    14    h Total Strength               ∥J11 + J21∥2                      Joint optimization over h1 + h2\n    15   w Total Strength               ∥J12 + J22∥2                      Joint optimization over w1 + w2\n\n\nThe J+ vector expands upon J6 by offering a finer decomposition of parameter-objective interactions,\nwith 15 terms that cover all gradient alignments and cross-objective contributions. J6, as an orthogonal\nprojection basis of J+, selectively retains the most interpretable components—direct magnitudes,\ninterference terms, and cross-objective alignments—forming a compact yet expressive representation\nof the local gradient-field dynamics.\n\nThis allows J6 to provide a concise control signal for optimization. By applying a soft-weighted\ncombination of J6 components, we can emulate J+’s directional guidance without the full dimensional\ncost, enabling both discrete and differentiable strategies for conflict-aware parameter updates.\n\n3.5  Soft (Weighted) Strategy\n\nTo enhance optimization flexibility and interpretability, we introduce a soft (weighted) update\nstrategy based on the geometry of the Jacobian matrix. This strategy is grounded in the relationship\nbetween the J6 vector and the full J+ interaction matrix (Table 2).\n\nFrom J+ to J6.  The J+ matrix represents all meaningful pairwise interactions between perturbation\ndirections h, w and objectives (Heat, Confidence). It spans a 4×4 interaction space including align-\nment, interference, and composite behaviors. However, many of these components are interdependent\nor redundant in optimization.\nIn contrast, the J6 vector is constructed as a low-dimensional projection of J+—specifically chosen\nfor its semantic interpretability and optimization utility. It extracts six components: primary gradients\n(∥J11∥2, ∥J22∥2), cross-objective interferences (∥J12∥2, ∥J21∥2), and synergy terms (⟨J11, J22⟩,\n⟨J21, J12⟩).\n\nThis geometric foundation enables us to construct a differentiable soft routing mechanism over the\nJ6 vector, allowing parameter updates to adaptively emphasize useful directions while remaining\n\n\n                                       8\n\nfully trainable. Our soft strategy thus acts as a low-rank controller that approximates the full behavior\nspace of J+ through interpretable, learnable signal composition.\n\nStep 1: Temperature-scaled weighting.  We begin by applying a softmax with temperature τ to\nthe raw J6 scores:\n                                           exp(J6[i]/τ)\n                                                 ˜αi =                 Pj exp(J6[j]/τ),\nwhere τ controls the sharpness of selection. A small τ leads to near-argmax behavior (hard attention),\nwhile a large τ yields more uniform weighting.\n\nStep 2: Competitive contrast enhancement.  To further amplify high-scoring components while\npreserving smoothness, we apply a competitive reward operator, squaring each weight and re-\nnormalizing:\n                                                ˜αγi\n                              αi =            ,  where γ > 1.\n               Pj ˜αγj\nThis enhances contrast (akin to energy-based methods) without excluding weaker signals. For\nexample, (0.5, 0.3, 0.1, 0.1) →(0.25, 0.09, 0.01, 0.01) after squaring.\n\nStep 3: Weighted updates.   Finally, the parameters are updated via weighted gradients over multiple\nobjectives:\n\n     ∆h = −ηh (α0∇hob1 + α3∇hob2) ,   ∆w = −ηw (α2∇wob1 + α4∇wob2) ,\n\nwhere ob1, ob2 represent Heat and Confidence objectives respectively. Additional components α1\nand α5 can scale auxiliary updates, such as alignment penalties or entropy regularization.\n\nThis soft strategy offers a continuous and interpretable optimization mechanism.  It balances ob-\njectives dynamically using geometric signals, and encourages diverse directional exploration while\nmaintaining differentiability.\n\nAlgorithm 2 J6 Weighted Update: Soft Strategy\nRequire: J6 vector J6 ∈R6, temperature τ, exponent γ\n  1:  ˜αi ←exp(J6[i]/τ) for i = 0, . . . , 5\n  2: Normalize: ˜αi ←˜αi/ Pj ˜αj\n  3: Apply reward operator: αi ←˜αγi\n  4: Renormalize: αi ←αi/ Pj αj\n  5: Compute update for h: ∆h ←−ηh(α0∇hob1 + α3∇hob2)\n  6: Compute update for w: ∆w ←−ηw(α2∇wob1 + α4∇wob2)\n  7: return ∆h, ∆w\n\n\n4  Conclusion\n\nIn this paper, we introduced J6, a structured Jacobian-based approach for multi-objective prompt\noptimization in large language models (LLMs). We identified the fundamental challenge of balancing\nmultiple, often conflicting, objectives—specifically fidelity (accuracy) and certainty (confidence)—in\nthe context of prompt tuning. Traditional methods fall short in modeling the complex interactions\nbetween parameters and objectives, typically relying on scalar aggregation techniques that ignore the\nunderlying gradient geometry.\n\nOur proposed solution, J6, decomposes the gradient interaction matrix into six interpretable compo-\nnents, providing a clear framework for dynamic role attribution. By using hard and soft strategies,\nJ6 offers both discrete role assignments and continuous gradient blending, allowing the model to\nadaptively respond to local conflicts and synergies. This enables a more flexible, interpretable, and\neffective optimization process compared to existing methods.\n\nWe extended this approach to J+, a richer Jacobian space that incorporates 15 alignment terms,\noffering even finer granularity.\n\n\n                                       9\n\nOur work presents a novel direction for integrating Jacobian-based reasoning into prompt optimization,\noffering both theoretical insight and practical performance gains. The J6 framework not only provides\na more effective means of balancing accuracy and confidence but also opens the door for further\nexploration of structured gradient reasoning in neural network optimization.\n\nFuture work can explore extending J6 to even larger models, incorporating more complex multi-\nobjective settings, and further refining the soft weighting strategies for finer control. Additionally,\nexploring the application of J6 in other areas such as reinforcement learning or multi-task learning\ncould provide valuable insights into its broader applicability.\n\nReferences\n\nShivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effec-\n   tiveness of entropy minimization in llm reasoning, 2025. URL https://arxiv.org/abs/2505.\n  15134.\n\nNing Ding, Zhen Hu, Yulin Wang, et al. Delta tuning: A comprehensive study of parameter efficient\n  methods for pre-trained language models. In ICLR, 2022.\n\nHarris Drucker and Yann Le Cun. Improving generalization performance using double backpropaga-\n   tion. IEEE Transactions on Neural Networks, 1992.\n\nChen Dun, Mirian Hipolito Garcia, Guoqing Zheng, Ahmed Hassan Awadallah, Anastasios Kyrillidis,\n  and Robert Sim. Sweeping heterogeneity with smart mops: Mixture of prompts for llm task\n  adaptation. arXiv preprint arXiv:2310.02842, 2023. Available at https://arxiv.org/abs/\n  2310.02842.\n\nStanislav Fort, Jie Ren, et al. Drawing conclusions from gradient-based explanations without gradient\n  explosion. arXiv preprint arXiv:2101.11607, 2021.\n\nEdward J Hu, Yelong Shen, Philip Wallis, et al. Lora: Low-rank adaptation of large language models.\n  arXiv preprint arXiv:2106.09685, 2021.\n\nYang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, and Guojun Qi.\n   Slot: Sample-specific language model optimization at test-time, 2025. URL https://arxiv.\n  org/abs/2505.12392.\n\nArthur Jacot, Franck Gabriel, and Clément Hongler.  Neural tangent kernel: Convergence and\n  generalization in neural networks. NeurIPS, 2018.\n\nSaurav Kadavath  et  al.   Language models  (still) can’t solve arithmetic.   arXiv preprint\n  arXiv:2201.11903, 2022.\n\nJinhyuk Lee et al.  Factuality enhanced language models for open-domain qa.  arXiv preprint\n  arXiv:2205.12011, 2022.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n  tuning. In EMNLP, 2021.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\n  ACL, 2021.\n\nStephanie Lin, Jacob Hilton, and Amanda Askell. Truthfulqa: Measuring how models mimic human\n  falsehoods. arXiv preprint arXiv:2109.07958, 2022.\n\nZiwei Lin, Fengxiang Yu, Cheng Liu, Yang Song, Baoxiang Li, Yu Qiao, and Wei Wang. Pareto\n  multi-task learning. In NeurIPS, 2019.\n\nBo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for\n  multi-task learning. In NeurIPS, 2021.\n\nPhongtharin Manakul and Mark Gales. Selfcheckgpt: Zero-resource black-box hallucination detection\n   for generative large language models. arXiv preprint arXiv:2303.08896, 2023.\n\n\n                                       10\n\nYair Navon, Shai Shalev-Schwartz, and Gal Chechik. Multi-task learning as a sparse factorization\n  problem. arXiv preprint arXiv:2309.02734, 2023.\n\nAashutosh Nema, Samaksh Gulati, Evangelos Giakoumakis, and Bipana Thapaliya. Modp: Multi\n  objective directional prompting. arXiv preprint arXiv:2504.18722, 2025. Available at https:\n  //arxiv.org/abs/2504.18722.\n\nPierre Quinton and Valérian Rey. Jacobian descent for multi-objective optimization. arXiv preprint\n  arXiv:2406.16232, 2025. Available at https://arxiv.org/abs/2406.16232.\n\nOzan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In Advances in\n  Neural Information Processing Systems, pages 527–538, 2018.\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML,\n  2017.\n\nRui Wang, Fei Mi, Yi Chen, Boyang Xue, Hongru Wang, Qi Zhu, Kam-Fai Wong, and Ruifeng Xu.\n  Role prompting guided domain adaptation with general capability preserve for large language\n  models. arXiv preprint arXiv:2403.02756, 2024. Available at https://arxiv.org/abs/2403.\n  02756.\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\n  Gradient surgery for multi-task learning. In Advances in Neural Information Processing Systems,\n  volume 33, pages 5824–5836, 2020.\n\n\n\n\n\n                                       11",
"headers": [
"arXiv:2508.12086v1  [cs.CL]  16 Aug 2025",
"J6: Jacobian-Driven Role Attribution for",
"Multi-Objective Prompt Optimization in LLMs",
"Abstract",
"1",
"Introduction",
"2",
"Related Work",
"3",
"Method",
"4",
"Conclusion",
"References"
],
"tables": [
"|J6 Component|Gradient Role|Optimization Interpretation|\n|---|---|---|\n|_∥J_11_∥_~~2~~<br>_∥J_22_∥_2<br>_∥J_12_∥_2<br>_∥J_21_∥_2<br>_⟨J_11_, J_22_⟩_<br>_⟨J_21_, J_12_⟩_|_h_’s contribution to Heat<br>_w_’s contribution to Confidence<br>_w_’s influence on Heat<br>_h_’s influence on Confidence<br>Cross-path interaction<br>Cross-path interaction|_h →_Heat<br>_w →_Confidence<br>_w →_Heat<br>_h →_Confidence<br>_h →_Heat and_ w →_Confidence coupling<br>_h →_Confidence and_ w →_Heat coupling|",
"|w<br>h|w (Heat)<br>1|w (Conf)<br>2|w + w<br>1 2|None|\n|---|---|---|---|---|\n|_h_1 (Heat)|_⟨J_11_, J_12_⟩_|_⟨J_11_, J_22_⟩_|_⟨J_11_, J_12 +_ J_22_⟩_|_∥J_11_∥_2|\n|_h_2 (Conf)|_⟨J_21_, J_12_⟩_|_⟨J_21_, J_22_⟩_|_⟨J_21_, J_12 +_ J_22_⟩_|_∥J_21_∥_2|\n|_h_1 +_ h_2|_⟨J_11 +_ J_21_, J_12_⟩_|_⟨J_11 +_ J_21_, J_22_⟩_|_⟨J_11 +_ J_21_, J_12 +_ J_22_⟩_|_∥J_11 +_ J_21_∥_2|\n|None|_∥J_12_∥_2|_∥J_22_∥_2|_∥J_12 +_ J_22_∥_2|x (invalid)|",
"|J Index|Optimization Action|J+ Component|Optimization Suggestion|\n|---|---|---|---|\n|1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15|_h →_Heat<br>_w →_Heat<br>_h →_Conf<br>_w →_Conf<br>Cross Align (_h_–Heat,_ w_–Conf)<br>Cross Align (_h_–Conf,_ w_–Heat)<br>_h →_Both<br>_w →_Both<br>Joint Align (_h_ +_ w_)<br>_h_ dominates Heat<br>_h_ dominates Conf<br>_w_ dominates Heat<br>_w_ dominates Conf<br>_h_ Total Strength<br>_w_ Total Strength|_∥J_11_∥_2<br>_∥J_12_∥_2<br>_∥J_21_∥_2<br>_∥J_22_∥_2<br>_⟨J_11_, J_22_⟩_<br>_⟨J_21_, J_12_⟩_<br>_⟨J_11_, J_21_⟩_<br>_⟨J_12_, J_22_⟩_<br>_⟨J_11 +_ J_21_, J_12 +_ J_22_⟩_<br>_⟨J_11_, J_12 +_ J_22_⟩_<br>_⟨J_21_, J_12 +_ J_22_⟩_<br>_⟨J_11 +_ J_21_, J_12_⟩_<br>_⟨J_11 +_ J_21_, J_22_⟩_<br>_∥J_11 +_ J_21_∥_2<br>_∥J_12 +_ J_22_∥_2|Optimize_ h_1 only<br>Optimize_ w_1 only<br>Optimize_ h_2 only<br>Optimize_ w_2 only<br>Jointly optimize_ h_1,_ w_2<br>Jointly optimize_ h_2,_ w_1<br>Optimize both_ h_1 +_ h_2<br>Optimize both_ w_1 +_ w_2<br>Jointly optimize_ h_1 +_ h_2,_ w_1 +_ w_2<br>Prioritize_ h_1, auxiliary_ w_<br>Prioritize_ h_2, auxiliary_ w_<br>Prioritize_ w_1, auxiliary_ h_<br>Prioritize_ w_2, auxiliary_ h_<br>Joint optimization over_ h_1 +_ h_2<br>Joint optimization over_ w_1 +_ w_2|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2508.12086v1.pdf"
}