{
"text": "Momentum-Aided Natural Language Gradient Descent for Prompt\n                                    Optimization\n\n\n                Anthony Cui*   Pranav Nandyalam  Andrew Rufail  Ethan Cheung\n                              Aiden Lei  Kevin Zhu  Sean O’Brien\n                                           Algoverse AI Research\n                       anthonycui@u.northwestern.edu, kevin@algoverse.us\n\n\n\n                          Abstract\n\n               Prompt optimization is crucial for improving\n                   the output quality of Large Language Models\n               (LLMs), but many existing methods are inef-2025              ficient, requiring extensive computation and\n                manual tuning. We propose Momentum-Aided\n               Prompt Optimization (MAPO), which buildsJun          on ProTeGi (Pryzant et al., 2023) by incorpo-\n26             ratingand a momentum-basedpositive natural languagememory mechanism\"gradients\"\n                    to refine prompts while avoiding local minima\n                and oscillations. It also employs beam search\n                and an Upper Confidence Bound (UCB)\n                  algorithm for balanced candidate expansion\n                and selection. MAPO achieves faster conver-[cs.CL]           gence time with fewer API calls and higher\n                performance than ProTeGi,  demonstrating\n                        its  effectiveness as a robust and scalable\n                   solution for automated prompt optimization\n                   in LLMs.  Our code is available online at\n                 https://github.com/AnthonyCui7/momentum-\n                  aided-prompt-optimization.\n\n          1  Introduction\n\n            Large Language Models (LLMs) have gained sig-\n              nificant attention since the release of ChatGPT\n            (OpenAI, 2022), leading to the development of new\n                                                                          Figure 1: High-Level Overview of MAPO\n            prompting techniques that have greatly improved\n        LLM performance (Schulhoff et al., 2024). WhilearXiv:2410.19499v3\n                   it has been shown that the prompts given to the     Recent work has explored implementing tradi-\n        LLM greatly affect performance (Pawlik, 2025),    tional machine learning algorithms in a natural\n            prompts can still be unclear, biased, or incomplete,   language format, with one of the first being Pro-\n              limiting LLM capabilities (Sahoo et al., 2024). For   TeGi’s “Automatic Prompt Optimization with ‘Gra-\n             these reasons, prompt engineering has become a    dient Descent’ and Beam Search” (Pryzant et al.,\n                critical aspect of leveraging an LLM’s capabilities.   2023). While ProTeGi introduced an innovative\n             Often, current prompt engineering methods require   framework, it has limitations, including high com-\n           manual adjustments by the user, making them time-   putational costs and resource consumption due to\n            consuming, error-prone, and constrained by human    excessive API calls making large-scale prompt op-\n              limitations (Lin et al., 2024). This highlights an in-   timization impractical. Furthermore, ProTeGi does\n             creasing need for an automated system to improve    not track previous refinements, leading to oscilla-\n              the quality of prompts without the need for human    tory behavior and slow convergence. Ultimately,\n              intervention.                                         the strengths of prompts are underutilized.\n                 *Lead Author                         We introduce Momentum-Aided Prompt Opti-\n\nmization (MAPO), a method that extends ProTeGi   3  Methods\nby using positive natural language “gradients” with\n                                                    3.1  Momentum-Aided Prompt Optimization\nmomentum to automate prompt refinement. Gradi-\nents are suggested feedback for the current prompt     First, the current prompt p is evaluated based on a\ngenerated using correct examples from a mini-   minibatch of training data to obtain randomly sam-\nbatch of training data, guiding the LLM to refine    pled strings s representing correct LLM predictions\nprompts in a consistent semantic direction. Beam    aligned with ground-truth labels. We then provide\nsearch expands the generated candidate pool of    the LLM with a static prompt τ (see Appendix A)\nimproved child prompts, and the best-arm identi-    to generate numerous positive “gradients” ∇p in\nfication algorithm using UCB bandits, selects the    natural language, appraising the current prompt p\ntop prompts for further evaluation. MAPO sig-   using the sampled strings s. Our “gradients” ∇p\nnificantly improves upon ProTeGi by enhancing    are the natural language outputs of the LLM’s con-\nefficiency and effectiveness through momentum-    tinuation of static prompt τ. In traditional machine\nbased adjustments. Unlike ProTeGi, which treats    learning, gradient descent uses numerical gradients,\neach optimization step as an independent improve-   representing a vector in parameter space that points\nment, MAPO tracks the history of gradient updates,    in the direction of steepest ascent of the loss func-\npreventing oscillations and reducing the risk of    tion, which the algorithm counters by moving in\nsuboptimal prompts (local minima in the semantic    the opposite direction to minimize the loss func-\nspace). In our experimental results in Section 4,    tion and improve model performance; in contrast,\nMAPO achieves a 77.9% reduction in convergence    our natural language gradients ∇p represent direc-\ntime compared to ProTeGi, 88.0% reduction in con-    tions in semantic space (Pryzant et al., 2023). We\nvergence API calls, as well as a 5.28% increase in    use another static prompt α (Appendix A) to apply\npeak performance, providing a scalable solution    these gradients to the initial prompt p, allowing us\nfor automated prompt engineering in LLMs.          to move along the same semantic direction as the\n                                                      positive natural language gradients, refining and\n                                               improving the initial prompt.\n\n                                                    3.2  Expansion and Selection\n2  Related Works\n\n                                            Algorithm 1 MAPO Beam Search\nPrompt Optimization. In this work, we draw upon   Require: p0:  initial prompt, b: beam width, r:\nexisting prompt engineering techniques and focus        search depth, m: metric function, G: gradient\non incorporating optimization algorithms into our        history\nframework to enhance the effectiveness of prompt      1: B0 ←{p0}\noptimization. There is an increasingly diverse set      2: G0 ←∅\nof general frameworks that previous works have fo-     3:  for i ←0 to r −1 do\n                                                                     4:   C ←∅cused on: LLM optimization (Pryzant et al., 2023;\nZelikman et al., 2023; Fernando et al., 2023; Zhou      5:     for all p ∈Bi do\net al., 2022; Yang et al., 2023), reinforcement learn-     6:     C ←C ∪Expand(p, Gi) ▷Section 3.2\ning (Ma et al., 2023; Zhang et al., 2022; Deng      7:    end for\net al., 2022), and in-context learning (Shum et al.,      8:    Bi+1 ←Selectb(C, m)  ▷UCB Bandits\n2023). However, these approaches are often in-     9:    Gi+1 ←SampleGradient(Bi+1)\nfeasible when there is no architectural informa-    10: end for\ntion introduced and only an API is provided to the    11:  ˆp ←argmaxp∈Br m(p)\nLLM. More specifically, we base most of our work    12:  return ˆp\non improving automatic prompt engineering tech-\nniques with LLM gradient-based methods (Shin     As outlined in Algorithm 1, our method employs\net al., 2020; Pryzant et al., 2023). Though, many   beam search to explore the space of prompt vari-\nof the current methods involving some form of it-   ations generated during optimization. In each op-\nerative refinement technique, such as APE (Zhou    timization round, the LLM utilizes static prompt\net al., 2022) and ProTeGi all face similar struggles  α containing sampled strings s and gradients ∇p\nwith cumulative costs of running their programs.     to edit the top k best-performing prompts from\n\nthe previous round, producing c new candidate   a much faster rate of convergence during prompt\nprompts, denoted as pc. After each round, less    optimization.\npromising candidates are pruned, and only the top\n                                       4  Experimentsk prompts are retained for further gradient-based\nimprovements, evaluated by a scoring function that                                                    4.1  Setup\nassesses how well they meet our predefined objec-\n                                          Our experimental setup closely follows that of Pro-\ntives, such as the F1 score (Manning et al., 2008).\n                                               TeGi, allowing for a direct comparison between\n   In our implementation, we utilize an Upper Con-\n                                                our extension method and their baseline. We use\nfidence Bound (UCB) Bandit Selection algorithm,\n                                          200 randomly sampled data points as the test set,\nwhich is applied during each beam search expan-\n                                                      retaining most hyperparameters from ProTeGi’s\nsion to evaluate candidate prompts. The UCB Ban-\n                                                       configuration, including a temperature of 0, a mini-\ndit Selection algorithm we employ is outlined in\n                                                   batch size of 64, and 6 rounds of beam search with\nAlgorithm 2 (Pryzant et al., 2023).\n                                                a beam width of 4. Each parent prompt expands\n                                                       into 8 new candidate prompts, with 2 positive gra-Algorithm 2 UCB Bandits\n                                                       dients generated from 3 randomly sampled correct\nRequire: n prompts p1, . . . , pn, dataset Dtr, T                                              examples from the minibatch, which creates fewer\n    time steps, metric function m, exploration cv                                                    gradients than ProTeGi’s method of using 4 neg-\n 1:  Initialize N(pi) ←0,  Q(pi) ←0  ∀i ∈                                                        ative gradients. This trade-off improves runtime\n     [1..n]\n                                                       efficiency while handling the added complexity of\n 2:  for t = 1, . . . , T do\n                                                    gradient history. The primary evaluation metric is\n 3:    Sample Dsample ⊂Dtr uniformly                                                    the F1 score, and results reflect the highest score\n              n     q log t o\n 4:      pi ←arg maxp  Qt(p) + cv   Nt(p)       among the top k beam search candidates, averaged\n 5:     Observe reward ri,t = m(pi, Dsample)        over three trial runs. All experiments use the Octo-\n 6:     Nt(pi) ←Nt(pi) + |Dsample|                ber 2024 release of GPT-3.5-turbo unless otherwise\n                                             ri,t                       stated. 7:     Qt(pi) ←Qt(pi) +                               Nt(pi)\n 8: end for\n                                                      4.1.1  Baseline\n 9:  return SELECTTOPb(QT )\n                                            ProTeGi.  Developed by (Pryzant et al., 2023),\n                                            ProTeGi employs natural language gradient de-\n  We also record the gradients from static prompt                                                    scent with negative gradients from incorrect ex-\nα used to generate the top k candidates in each                                           ample sampling to refine prompts.  It iteratively\nround to incorporate our novel momentum exten-                                                    applies these gradients to address prompt weak-\nsion into natural language gradient descent. Draw-                                                      nesses, expanding the candidate pool using Monte\ning on the physics intuition of momentum, tradi-                                                Carlo sampling to generate paraphrased versions\ntional gradient descent uses this extension to im-                                                with synonyms or semantically similar variations.\nprove stability and convergence, helping the model                                                 This ensures candidate diversity while guiding the\navoid oscillations and escape local minima, thereby                                                   optimization process.\nreaching global minima more efficiently (Malin-\ngan, 2023). Analogously, our method maintains    4.1.2  Benchmarks\na history of past gradients, guiding the movement   ProTeGi has been evaluated on benchmark datasets,\nof the initial prompt p in each beam search round   such as the Liar Dataset (Wang, 2017), which fo-\nthrough semantic space, helping it converge on    cuses on detecting fake news by classifying claims\nthe optimal prompt rather than just an incremental    as true or false, and the Ethos Dataset (Mollas\nimprovement. A single positive gradient is ran-    et al., 2022), which is designed for identifying hate\ndomly sampled from a pool of all the gradients   speech in online conversations. Our method will\nused to generate the top k prompts in each beam   be evaluated on these same benchmarks for com-\nsearch round, representing the positive gradient his-    parison. Additionally, we evaluate on the Word-in-\ntory. This gradient history is then incorporated into   Context dataset (Pilehvar and Camacho-Collados,\nour static prompts τ and α as textual momentum,   2018), which tests models on their ability to under-\nguiding the LLM to generate new gradients ∇p    stand and represent the meanings of certain words\nand new prompt candidates pc, allowing the initial   based on the context in which they appear, to fur-\nprompt p to “roll down the hill” faster, i.e., achieve    ther expand the range of NLP tasks tested. Despite\n\nFigure 2: Test performance (F1 Score) versus Time for Liar, Ethos, and Word-in-Context dataset.\n\n\n\n\n\nFigure 3: Test performance (F1 Score) versus Cumulative API Calls for Liar, Ethos, and Word-in-Context dataset.\n\n\nutilizing the publicly available code from ProTeGi    We attribute MAPO’s efficiency primarily to\nto replicate the baseline, we were unable to re-    its use of momentum, which allows it to incor-\nproduce the level of metric performance for their    porate feedback from previous optimization steps\nmethod on the Liar Dataset as reported in the origi-   when refining prompts. Instead of treating each up-\nnal paper.                                           date independently, MAPO builds on prior success-\n                                                          ful changes, resulting in more consistent progress,\n4.2  Analysis                                                 fewer wasted iterations, and significantly faster con-\nEfficiency. Figures 2, 3, and 4 show the signifi-   vergence across multiple metrics. The use of pos-\ncant efficiency gains of MAPO over ProTeGi in    itive gradients further constrains the search space\nterms of convergence time for MAPO to reach the    to prompt edits supported by observed success,\nsame peak F1 score for ProTeGi. Overall, there is    rather than encouraging arbitrary variation.  To-\nan average 77.9% reduction in convergence time.    gether, these design choices enable a more focused\nThis is notable since ProTeGi’s runtimes can ex-   and informed optimization process, reducing the\ntend into hours (Pryzant et al., 2023), highlighting   number of steps and API calls required to reach\nthe resource-intensive nature of automatic prompt    strong performance.\noptimization.                                    While MAPO does incur longer processing times\n  Figure 3 illustrates that ProTeGi uses about 417   per iteration due to more complex prompt struc-\nAPI calls to reach its final F1 score, whereas MAPO    tures and the inclusion of positive gradient history,\nrequires an average of 56 to reach the same per-   the overall reduction in steps and runtime demon-\nformance, resulting in an average 88.0% reduc-    strates that these trade-offs do not detract from its\ntion. MAPO also completes all six rounds of beam    overall efficiency.\nsearch with 105 fewer API calls on average, ad-      Efficacy. Figure 4 shows that MAPO consis-\ndressing critical computational constraints in large-    tently outperforms ProTeGi at every beam search\nscale optimization.                             round for both Liar and Ethos. MAPO’s F1 score\n  Figure 4 reinforces these efficiency gains by    steadily increases while ProTeGi quickly converges\nshowing that MAPO surpasses ProTeGi’s perfor-   and then plateaus or slightly declines. This results\nmance after just 2 or 3 optimization steps, while    in a notable 5.28% average increase in peak perfor-\nProTeGi requires 6 steps for lower performance.   mance for MAPO.\nThis reduction in steps not only saves time but also    MAPO’s consistent improvement highlights its\nsuggests a more robust optimization mechanism in    effectiveness in leveraging positive gradients and\nMAPO.                                             the momentum-based extension to thoroughly ex-\n\nFigure 4: Test performance (F1 Score) versus Beam Search Round for Liar, Ethos, and Word-in-Context datasets.\n\n\n              Dataset   Method/Metric        GPT-3.5-Turbo             GPT-4o Mini\n\n                               Initial F1                      0.9                          0.975\n              Ethos   MAPO           0.98 / 871 / 364 / 146 / 56 / 2    0.985 / 1325 / 364 / 96 / 19 / 1\n                       ProTeGi            0.965 / 687 / 472 / - / - / -       0.985 / 1328 / 423 / - / - / -\n\n                               Initial F1                   0.475                          0.52\n               Liar    MAPO           0.64 / 904 / 364 / 260 / 98 / 3     0.625 / 1414 / 364 / - / - / -\n                       ProTeGi            0.58 / 737 / 472 / - / - / -        0.64 / 1362 / 399 / - / - / -\n\n                               Initial F1                     0.52                         0.665\n          WiC    MAPO            0.66 / 937 / 364 / 73 / 16 / 1    0.73 / 1252 / 364 / 533 / 157 / 3\n                       ProTeGi            0.635 / 755 / 472 / - / - / -        0.73 / 1119 / 406 / - / - / -\n\nTable 1: Comparison of MAPO (ProTeGi) and baselines across tasks and models. Each cell shows: Final F1 Score /\nTotal Time (s) / Total API Calls / Convergence Time (s) / Convergence API Calls / Convergence Optimization Steps.\nThe row Initial F1 reports performance of the initial prompt before optimization.\n\n\nplore and refine the prompt search space. By incor-     Gradient Ablation.  As shown in Table 2,\nporating momentum, MAPO helps it avoid local    the method using only positive gradients achieves\nminima and erratic updates that hinder progress.   the highest F1 score with the shortest total time,\nThis leads to a more robust optimization mecha-   demonstrating its efficiency and effectiveness. The\nnism compared to ProTeGi. In contrast, ProTeGi’s   method combining both positive and negative gra-\nperformance dips suggest it struggles with opti-   dients has the fastest convergence time, but its F1\nmization challenges like local minima and oscilla-   score is slightly lower, and it requires the longest\ntions due to the lack of a stabilizing mechanism.    total time. This suggests that while the combined\nMAPO’s use of momentum and positive gradient   method offers faster convergence, it sacrifices some\nhistory allows it to maintain steady progress, effec-   performance. On the other hand, the method using\ntively \"remembering\" beneficial adjustments and   only negative gradients performs the worst across\nreducing the likelihood of stagnation. This results     all metrics. These results highlight the importance\nin more reliable convergence toward higher perfor-   of positive gradients for optimizing both accuracy\nmance levels, demonstrating the superior efficacy   and efficiency, while the combined method excels\nof MAPO’s optimization strategy.                    in rapid convergence scenarios. Although it takes\n  Evaluation Across Models. Focusing on the    longer to converge, the positive-only method signif-\nperformance of GPT-4o mini in Table 1, the model    icantly reduces overall runtime, as well as a slightly\nconsistently demonstrates a strong initial F1 score,   higher peak F1 score. Overall, we conclude that\nlikely attributable to  its advanced architecture.   the benefits of a significantly shorter total time and\nWhile ProTeGi yields competitive F1 scores with    a higher F1 score outweigh the slightly higher con-\nGPT-4o mini, MAPO achieves notably faster con-   vergence time.\nvergence in terms of optimization steps (1 or 3    Momentum Ablation. In our momentum ab-\nsteps) and convergence API calls (e.g., Ethos: 19    lation study, we compared the convergence time\nAPI calls for MAPO to reach ProTeGi performance    required to reach peak ProTeGi performance with\nwith 423 API calls). This consistency of improved   and without the use of momentum, as well as the\nefficiency reinforces MAPO’s utility as a scalable   peak MAPO F1 score performance. Table 3 shows\nand adaptable prompt optimization framework be-    that the peak F1 score of MAPO remains essentially\nyond the original GPT-3.5-Turbo setting.              identical regardless of the inclusion of momentum,\n\nLiar Dataset\n\n                                  F1 Score  Conv. Time (s)   Total Time (s)\n\n                Positive Only (MAPO)       0.66         285           903\n               Negative Only               0.60         303          1228\n             Both Positive & Negative     0.65         275          1467\n\n\nTable 2: Comparison of Past Gradient Methods on the Liar Dataset. Convergence time represents time to reach peak\nProTeGi performance for Liar\n\n\n                Liar Dataset                   motes stability during training. Collectively, these\n                                                       findings validate the efficiency of incorporating mo-\n                   Time (s)  F1 Score\n                                       mentum in prompt optimization, proving that our\n   No Momentum      390       0.64          approach accelerates convergence while maintain-\n    With Momentum    285       0.64           ing, or even slightly improving, the model’s peak\n                                                 performance.              Ethos Dataset\n                                             Mathematical Reasoning. Since MAPO is a\n                   Time (s)  F1 Score                                                  general prompt optimization algorithm, it can be\n   No Momentum      407       0.98           easily generalized to many other tasks, not just bi-\n    With Momentum     77       0.985          nary classification. We test MAPO on the GSM8K\n                                                      dataset (Cobbe et al., 2021) which contains grade-\nTable 3: Comparison of Momentum Ablation on Liar    level math word problems that some LLMs struggle\nand Ethos Datasets: Peak F1 Score and Convergence    with. We also test on the AGIEval-Math dataset\nTime to Reach Peak ProTeGi Performance            (Zhong et al., 2023) which contains much harder\n                                                     questions. The results in Table 4 are based on ex-\n         Math Dataset Accuracy               periments using GPT-4o-mini and report accuracy\n                                                    as the evaluation metric, rather than F1 score.\n            GSM8K  AGIEval-Math\n                                           The results presented in Table 4 indicate that\n MAPO             95.5         75.3         MAPO’s optimized prompt yields modest accu-\n Original prompt    94.5         74.1            racy improvements in mathematical reasoning\n                                                         tasks. Specifically, it achieves a 1.2% increase on\nTable 4: Comparison of MAPO’s performance on math   AGIEval-Math and a 1.0% increase on GSM8K.\ndatasets.\n                                       5  Conclusion\n\nindicating that momentum does not compromise    In this work, we introduced Momentum-Aided\nmodel accuracy.                               Prompt Optimization (MAPO), a novel momentum-\n  However, there is a significant difference in con-   aided extension of natural language gradient de-\nvergence speed: on average, we observe a 54%    scent for prompt optimization in LLMs. Building\ndecrease in convergence time when incorporating   on ProTeGi, MAPO uses positive natural language\nmomentum into MAPO. This substantial improve-   gradients and momentum to refine prompts more\nment in convergence speed demonstrates that mo-    effectively, guiding optimization consistently and\nmentum effectively enhances the optimization pro-   avoiding local minima.\ncess by smoothing the loss landscape and helping    Momentum is crucial in overcoming ProTeGi’s\nthe model avoid getting trapped in local minima,    limitations such as local minima and oscillations.\nan issue that ProTeGi struggles with (Pryzant et al.,  By leveraging gradient history, MAPO ensures a\n2023). This is further evidenced by our smoother   more stable, directed search, resulting in faster con-\ntest set curves depicted in Figure 2, 3, and 4, which   vergence and a more robust optimization process.\ncontrast sharply with the oscillations observed in   MAPO  consistently  outperforms  ProTeGi,\nProTeGi’s data. The lack of significant fluctua-   achieving a 77.9% reduction in convergence time\ntions in our graphs confirms that our method not   while improving peak F1 scores with fewer API\nonly maximizes convergence speed but also pro-    calls and smoother convergence.\n\nLimitations                                     Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\n                                                  guang Zhu, and Michael Zeng. 2023.  Automatic\nWhile MAPO demonstrates promising results, our      Prompt Optimization with “Gradient Descent” and\nevaluation remains limited in several key aspects.    Beam Search. arXiv.org.\nFirstly, we assessed MAPO on a narrow set of    Pallavi Sahoo, Ankit Kumar Singh, Souvik Saha, Vipul\nbenchmarks and did not compare against a wide       Jain, Sourav Mondal, and Aditi Chadha. 2024. A\nrange of strong baselines, making it difficult to con-      Systematic Survey of Prompt Engineering in Large\n                                                 Language Models:  Techniques and Applications.textualize its performance relative to existing state-\n                                                              arXiv.org.\nof-the-art methods. Future work should include a\nmore extensive evaluation, including evaluations   Samuel Schulhoff, Madalina Ilie, Nihar Balepur, Kris-\n                                                                 tine Kahadze, Alison Liu, Cheng Si, Yichen Li,on diverse benchmarks and more comparative anal-\n                                               Ananya Gupta, Hyejun Han, Samuel Schulhoff,\nysis.                                                       Prashanth Dulepet, Suma Vidyadhara, Dong Ki,\n                                               Saksham Agrawal, Christopher Pham, Guy Kroiz,\n                                                        Fangfei Li, Hao Tao, Aditya Srivastava, and Philip\nReferences                                           Resnik. 2024. The Prompt Report: A Systematic\n                                                     Survey of Prompting Techniques. arXiv.org.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\n  Wang, Han Guo, Tianmin Shu, Meng Song, Eric P.   Taylor Shin, Yasaman Razeghi, Robert L Logan IV,\n  Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing       Eric Wallace, and Sameer Singh. 2020. AutoPrompt:\n   Discrete Text Prompts with Reinforcement Learning.       Eliciting Knowledge from Language Models with\n   arXiv.org.                                            Automatically Generated Prompts. arXiv.org.\n\n                                           KaShun Shum, Shizhe Diao, and Tong Zhang. 2023.Chrisantha  Fernando,  Dylan  Banarse,  Henryk\n                                                     Automatic Prompt Augmentation and Selection with   Michalewski, Simon Osindero, and Tim Rock-\n                                                      Chain-of-Thought from Labeled Data. arXiv.org.   täschel. 2023.  PromptBreeder:  Self-Referential\n   Self-Improvement via Prompt Evolution. arXiv.org.                                                  William Yang Wang. 2017. “Liar, Liar Pants on Fire”:\n                                   A New Benchmark Dataset for Fake News Detection.\nXiaohan Lin, Zhiqiang Dai, Anirudh Verma, Sufang                                                              arXiv.org.\n  Ng, Patrick Jaillet, and Bryan Kian Hsiang Low.\n   2024. Prompt Optimization with Human Feedback.   Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\n   arXiv.org.                                    Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023.\n                                                      Large Language Models as Optimizers. arXiv.org.\nYecheng Jason Ma, William Liang, Guanzhi Wang, De-\n  An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke    Eric Zelikman, Eliana Lorch, Lester Mackey, and\n  Zhu, Linxi Fan, and Anima Anandkumar. 2023. Eu-    Adam Tauman Kalai. 2023. Self-Taught Optimizer\n   reka: Human-Level Reward Design via Coding Large      (STOP): Recursively Self-Improving Code Genera-\n  Language Models. arXiv.org.                                 tion. arXiv.org.\n\n                                                      Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-Navaneeth Malingan. 2023. Momentum-Based Gradi-\n                                                       urmans, and Joseph E. Gonzalez. 2022. TEMPERA:   ent Descent - Scaler Topics.\n                                                      Test-Time Prompting via Reinforcement Learning.\n                                                              arXiv.org.\nChristopher D Manning, Prabhakar Raghavan, and Hin-\n   rich Schütze. 2008. Introduction to Information Re-                                              Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\n   trieval. Cambridge University Press.                                                      Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n                                                        Ba. 2022. Large Language Models are Human-Level\nIoannis Mollas, Zoi Chrysopoulou, Sergios Karlos, and      Prompt Engineers. arXiv.org.\n   Grigorios Tsoumakas. 2022. ETHOS: A Multi-Label\n  Hate Speech Detection Dataset. Complex & Intelli-  A  Prompts\n   gent Systems.\n                              MAPO uses two prompts. Prompt τ generates pos-\nOpenAI. 2022. Introducing ChatGPT. Accessed: 2024-                                                              itive gradients based on correct examples sampled\n  10-19.\n                                            from the minibatch of training data, and prompt α\nLukasz Pawlik. 2025. How the choice of llm and prompt    applies these gradients to refine the current prompt\n   engineering affects chatbot effectiveness. Electron-   and create child prompts.\n   ics, 14(5):888.\n                                                     tau = f\"\"\"\n                                                            I’m trying to write a zero-shot\nMohammad Taher Pilehvar and Jose Camacho-Collados.           {task_type} prompt.\n   2018. WiC: The Word-in-Context Dataset for Eval-\n   uating Context-Sensitive Meaning Representations.           My current prompt is:\n   arXiv.org.                                                \"{prompt}\"\n\nThis prompt gets the following examples\n       correct:\n       {correct_string}\n\n       In addition, consider the following\n       strengths of past\n       iterations of this prompt:\n       {positive_gradient_history}\n\n       Based on the above information, give\n       {num_gradients} reasons why the\n       prompt could have gotten these examples\n       correct.\n\n       Wrap each reason with <START> and <END>\n       \"\"\"\n\nalpha = f\"\"\"\n       I’m trying to write a zero-shot\n       {task_type} solver.\n\n       My current prompt is:\n       \"{prompt}\"\n\n       It gets the following examples correct:\n       {correct_str}\n\n       Based on these examples the strengths\n       with this current prompt are that\n       {positive_feedback_str}\n\n       Consider the following strengths\n       of past iterations of this prompt:\n       {positive_gradient_history}\n\n       Based on the above information,\n       modify and revise the current prompt to\n       create a new\n       prompt which improves upon the strengths\n       of the\n       original wording.\n       The new prompt is wrapped with <START>\n       and <END>.\n\n       The 1 new prompt is:\n       \"\"\"",
"headers": [
"arXiv:2410.19499v3  [cs.CL]  26 Jun 2025",
"Momentum-Aided Natural Language Gradient Descent for Prompt",
"Optimization"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2410.19499v3.pdf"
}