{
"text": "Meta-Prompt Optimization for LLM-Based Sequential Decision Making\n\n\n\n                             Mingze Kong 1 Zhiyong Wang 2 Yao Shu 3 Zhongxiang Dai 1\n\n\n                         Abstract                      (BO) (Yang et al., 2024) and reinforcement learning (RL)\n                                                                  (Dai et al., 2024).  Specifically, these methods often use\n              Large language models (LLMs) have recently                                                              an LLM to sequentially select the actions by providing it\n             been employed as agents to solve sequential                                                                  with a specially designed prompt, which we refer to as the\n              decision-making tasks such as Bayesian opti-                                                                   meta-prompt. The meta-prompt often contains several com-\n               mization and multi-armed bandits (MAB). These                                                                      ponents, such as the task description, the meta-instruction2025        works usually adopt an LLM for sequential ac-        (which is used to instruct the LLM to select an action in\n                tion selection by providing it with a fixed, man-                                                                      every step), the history of interactions with the environment,\n                ually designed meta-prompt. However, numer-                                                      among others. The previous methods have all adopted aFeb        ous previous works have found that the prompt          fixed, manually designed meta-prompt for the LLM-based\n              has a significant impact on the performance of2                                                               agent throughout the entire sequential decision-making pro-\n                the LLM, which calls for a method to automati-                                                                              cess. However, numerous previous works have highlighted\n                cally optimize the meta-prompt for LLM-based                                                                                 that the output text generated by LLMs is heavily dependent\n                agents.  Unfortunately, the non-stationarity in                                                          on its input prompt (Zhou et al., 2023). Therefore, using\n                the reward observations during LLM-based se-                                                                              fixed, manually designed meta-prompt may significantly\n                quential decision-making makes meta-prompt op-                                                                             limit the performance of the LLM-based agents, because\n                timization highly challenging. To address this[cs.LG]                                                              handcrafted prompts are often far from optimal (Lin et al.,\n                challenge, we draw inspirations from adversar-                                                                   2024b). This naturally begs the question: can we automat-\n                  ial bandit algorithms, which are inherently capa-                                                                               ically optimize the meta-prompt for LLM-based agents to\n                ble of handling non-stationary reward observa-                                                               enhance their performance?\n                 tions. Building on this foundation, we propose\n              our EXPonential-weight algorithm for prompt       The sensitivity of LLM-generated text to its input prompt\n               Optimization (EXPO) to automatically optimize        has given rise to many recent works on automated prompt\n                the task description and meta-instruction in the         optimization, among which a representative line of works\n              meta-prompt for LLM-based agents. We also ex-        have adopted the method of multi-armed bandits (MAB) to\n               tend EXPO to additionally optimize the exemplars         automatically optimize the prompt (Lin et al., 2024b; Wu\n                      (i.e., history of interactions) in the meta-prompt to           et al., 2024; Lin et al., 2024a). Unfortunately, the problem of\n                 further enhance the performance, hence introduc-        meta-prompt optimization for LLM-based agents presents\n               ing our EXPO-ES algorithm. We use extensive          significant challenges compared to traditional prompt op-\n              experiments to show that our algorithms signif-         timization. This is mostly due to the non-stationarity in\n                icantly improve the performance of LLM-based         the observed rewards during the LLM-based sequentialarXiv:2502.00728v1         sequential decision-making.                           decision-making process. Specifically, as the LLM-based\n                                                                   agent engages in more interactions with the environment,\n                                                                                             its state in the environment changes, making its observed\n                                                                 rewards non-stationary. For example, in MAB (Krishna-          1. Introduction\n                                                               murthy et al., 2024) and BO (Yang et al., 2024), the observed\n        The strong capabilities of LLMs have spurred significant    rewards in later iterations (i.e., after the agent has accumu-\n           recent interests in adopting them as agents to solve sequen-    lated significant experience in the environment) tend to be\n              tial decision-making problems, such as multi-armed bandits    higher than those obtained in initial iterations. Similarly, in\n        (MAB) (Krishnamurthy et al., 2024), Bayesian optimization   RL (Dai et al., 2024), rewards are typically dependent on\n                                                                  both the state and action. However, since the state of the\n             1The Chinese University of Hong Kong, Shenzhen 2The Chi-                                                         LLM-based agent evolves across iterations, this also results\n           nese University of Hong Kong 3Guangdong Lab of AI and Digital\n                                                                           in non-stationarity in the observed rewards. As a conse-         Economy (SZ). Correspondence to: Zhongxiang Dai <daizhongx-\n          iang@cuhk.edu.cn>.                                     quence of the non-stationarity, for the same meta-prompt\n                                                                                      (e.g., the same task description and meta-instruction), its\n            Preprint\n\n\n                                                         1\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\ncorresponding observed reward is highly likely to be dy-    to additionally use a separate adversarial bandit method to\nnamically changing across different iterations. This is in    optimize the exemplars (i.e., the interaction history) in the\nstark contrast to classical prompt optimization, in which    meta-prompt for LLM-based agents, and hence introduce\nthe reward or score for a prompt remains stationary across    our EXPO with Exemplar Selection (EXPO-ES) algorithm.\niterations. As a result, this renders the previous works on\n                                       We use extensive experiments to show that our EXPO algo-\nprompt optimization (such as those based on MAB (Lin\n                                                          rithm significantly improves the performance of the LLM-\net al., 2024b; Wu et al., 2024; Lin et al., 2024a)) inapplica-\n                                                        based BO algorithm from Yang et al. (2024) (Sec. 4.1) and\nble, and hence calls for novel algorithmic designs to solve\n                                                              the LLM-based MAB algorithm from Krishnamurthy et al.\nthe problem of meta-prompt optimization for LLM-based\n                                                          (2024) (Sec. 4.2). Furthermore, in tasks where the exemplars\nagents. To this end, we draw inspirations from the field of\n                                                         provide crucial information for the LLM-based agent, our\nadversarial bandits (Lattimore & Szepesv´ari, 2020).\n                                          EXPO-ES algorithm further enhances the performance of\nIn adversarial bandits, for each arm, the reward observations   EXPO via automated exemplar selection (Sec. 4.1). We also\nwhen the arm is pulled are chosen by an adversary, i.e., they    perform ablation study to unveil other interesting insights\nare allowed to change in an arbitrary way across different    about our algorithms in Sec. 5.\niterations. Therefore, the reward observations can be signifi-\ncantly non-stationary. This is considerably different from    2. Problem Setting\nclassical stochastic MAB, in which the reward observations                                                     Throughout our work, we use arms to represent meta-\nfor an arm are sampled from a fixed stationary distribu-                                                        prompts, and use actions to denote the actions selected\ntion. Therefore, the ability of adversarial bandits to handle                                                   by an LLM-based agent.\nnon-stationary reward observations makes it an ideal candi-\ndate for meta-prompt optimization for LLM-based agents.    Consider an algorithm which uses an LLM to perform a\nSpecifically, drawing inspirations from the EXP3 algo-    sequential decision-making task by sequentially instructing\nrithm for adversarial bandits, we introduce our EXPonential-    the LLM to select an action in every iteration. A repre-\nweight algorithm for prompt Optimization (EXPO) to opti-    sentative example of such algorithms is the Optimization\nmize the task description and meta-instruction in the meta-   by PROmpting (OPRO) algorithm from Yang et al. (2024).\nprompt of an LLM-based agent.1                  OPRO aims to solve an optimization problem, i.e., to find\n                                   x∗= arg minxf(x). To achieve this, in every iteration t,\nIn addition to the task description and meta-instruction, the                                      OPRO uses an LLM to select a batch of B input queries\nhistory of interactions with the environment (which we also                                                                 {xt,1, . . . , xt,B}, after which their corresponding scores\nrefer to as the exemplars) is also a crucial component in the                                                                    {st,1, . . . , st,B} are observed. When instructing the LLM\nmeta-prompt which exerts a considerable impact on the per-                                                                 to select the input queries, the meta-prompt Q given to the\nformance of LLM-based agents. Existing works often adopt                                   LLM contains a number of important components, includ-\nsimple heuristic approaches to decide how to incorporate the                                                          ing a fixed task description D, a fixed meta-instruction I,\nexemplars into the meta-prompt, including which subset of                                                    and a sequence of exemplars Et corresponding to a subset\nexemplars is included and their ordering in the meta-prompt.                                                              of the observations (i.e., pairs of input queries and observed\nPrevious works on in-context learning (ICL) have found that                                                               scores) collected so far. The same paradigm of LLM-based\nin addition to their contents, the ordering of the exemplars                                                               sequential decision-making has also been adopted by other\nalso has a significant impact on the performance of LLMs                                                         works, such as the LLM-based MAB algorithm from Krish-\n(Lu et al., 2022). Therefore, in addition to optimizing the                                                     namurthy et al. (2024) (more details in Sec. 4.2).\ntask description and meta-instruction, we also extend our\nEXPO algorithm to additionally optimize both the subset    In this work, our first algorithm, EXPO (Sec. 3.1), dynami-\nof exemplars included in the meta-prompt and their order-    cally optimize the task description D and meta-instruction\ning. However, the optimization of the task description and   I (i.e., selects a new Dt and It in every iteration t), in order\nmeta-instruction in every iteration in our EXPO makes the     to improve the efficiency and effectiveness of optimization.\noptimization of exemplars non-stationary as well. Specifi-  We also extend our EXPO to derive the EXPO-ES algorithm\ncally, for the same subset of exemplars with a fixed ordering,    (Sec. 3.2), which additionally optimizes the sequence of ex-\ntheir reward observations are usually non-stationary, be-    emplars Et to further improve the optimization performance.\ncause the task description and meta-instruction selected by   We use g(·) to denote a pre-trained embedding function,\nour EXPO algorithm are highly likely to vary across differ-   which maps some input text to its corresponding continuous\nent iterations. To this end, we extend our EXPO algorithm    representation. We separately obtain the embeddings of\n                                                             the task description g(Dt), the meta-instruction g(It) and\n   1Note that although here we only consider optimizing the task    the exemplar sequence g(Et). Based on the embeddings, in\ndescription and meta-instruction, the other components contained                                                          every iteration, we use the current history of selected meta-\nin the meta-prompt (e.g., some information from previously com-\n                                                     prompts and their scores to train a neural network (NN),pleted related tasks) can also be optimized in a similar fashion.\n\n\n                                                2\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n                     Qt = ( Dt , I t , Et' )           ǒƱM AȰʑnɢ                          1 LLM-Based Action Selection\n                            Now you will help me minimize a\n                                        function with two input variables w, b.\n                                                                      I                                     have some                                                           (w,                                                            b)                                                                pairs and\n                                       the                                             function                                                 values                                                                at                                                         those                                                                        points.                                                                                   t             t\n                                       The pairs are ...\n                                                                                                                                             Evaluator                     Input       Input\n                                                                    ...\n                                              input:                                               Agent\n                                   w=18, b=15                                                Observe the score                               Embedding\n                                                                                                                                                                                             ...                                          value:                                                                                             update                                                                                                              the                                    model                                                                         A new                                                                                                                         (w,                                                                                                                       b)                                                                                                                                  pair is              and                                  10386334                                                                                                                           [17,                                                                                                                   18]                                                                                          exemplar                                                                                                                 set                                                                    ...\n                                                                                                                     st     xt )                                    Give                              me a                                      new                                                            (w,                                                             b) pair                                                                        that is           The LLM agent                                                                                                                       g ( t )  ⊕   g (  t )                                                different                                              from                                                                            all                                                              pairs                                                          above,                                                                                      t 1  t   {( xt , st )}                                                                                             will                                                                              select                                                                               the                                  and has                                         a                                                      function                                                         value                                                               lower                                                                                                 St 1  St   {( g ( Dt )  g ( I t )), st }                                                                     next                                                                               action                                      than                                       any                                                       of                                                    the above.                                             Do                                                                  not ...\n                                                                           xt = f (t )\n\n                    3 Randomized Meta-Prompt Selection     2 Score Estimation\n                                                                                             Predict the arms’ scores using the NN       Update parameters of the NN\n                                               Calculate the sampling distribution:                                                                                                                                                                                          t 1  argmin LMSE (  ; St 1 )\n                                                           exp(  s i( t 1) )\n                                      Pt [i ]     k                 ( t 1)  ,   i  {1,    , k }\n                                                            exp(  s j   )\n             1        2                   k                                                  j 1\n                                          Use the distribution                                                                                    Update the cumulative                                                           to sample a new arm:                                                                                               predicted scores of the arms:\n                                                                             ( Dt +1 , I t +1 ) ~ tP\n                                                                                                                   s(i t 1),   i  {1,    , k }\n\n\nFigure 1. Illustration of our EXPO algorithm. We use purple to denote the task description and blue to represent the meta-instruction.\n\n\nwhich can then be used to predict the scores of every meta-    in the training set. This simple design helps our algorithms\nprompts in the domain. We denote this NN as M(g(·); θ),    achieve strong performance in our experiments (Sec. 4).\nin which θ represents the NN parameters.\nAdversarial Bandits. In adversarial bandits, the goal is    3. Algorithms\nto compete against the best arm in hindsight (Lattimore &\n                                                                    3.1. The EXPO Algorithm (Algo. 1)\nSzepesv´ari, 2020). Consider an MAB problem with k arms\n(i.e., meta-prompts). For each arm i = 1, . . . , k, denote    Our EXPO is used to dynamically optimize the task descrip-\nits corresponding sequence of rewards (i.e., scores) in T     tion D and the meta-instruction I in the meta-prompt.\niterations as {rt,i}t=1,...,T . The best arm in hindsight is\n                                            Domain Generation. At the beginning of our algorithm,\nthen defined as i∗= arg maxi=1,...,k PTt=1 rt,i. Then, the                                           we start by generating the domain of task descriptions and\ngoal of an adversarial bandit algorithm (which selects arm\n                                                                meta-instructions. Following the previous works on prompt\nAt in iteration t) is to minimize the following definition of                                                             optimization (Zhou et al., 2023; Lin et al., 2024a;b), we use\nregret: RT = PTt=1 rt,i∗−PTt=1 rt,At.                                                      an LLM to rephrase an initial task description D0 (resp. ini-\nAdversarial Bandits for LLM-Based Agents. LLM-based      tial meta-instruction I0) to generate a domain of k1 task\nsequential decision-making methods often aim to maximize    descriptions (resp. k2 meta-instructions). This results in a\neither (a) the cumulative rewards (e.g., the LLM-based    domain size of k = k1 × k2. We defer more details on\nMAB algorithm from Krishnamurthy et al. (2024)) or (b)    domain generation to App. B.1. We treat the combination of\nthe final reward (e.g., OPRO from Yang et al. (2024)). In the    a task description D and a meta-instruction I in the domain\nformer case of cumulative reward maximization, the overall    as an arm, i.e., our adversarial bandit problem has k arms.\nrewards/scores for the best arm i∗are higher than the other    In addition to jointly optimizing D and I, we have also eval-\narms. In the latter case, we implicitly assume that the arm    uated the performance of optimizing them separately. The\nwith the largest final reward after T iterations also has large     results show that jointly optimizing these two components\nrewards across all iterations in general. As a result, in both    leads to better performance.\ncases, the observed rewards of an arm (i.e., the observed                                           1⃝LLM-Based Action Selection (lines 3-7 of Algo. 1). At\nscores of a meta-prompt) in every iteration are indicative\n                                                              the beginning of every iteration t, we firstly use the current\nof the quality of the arm (i.e., the meta-prompt). So, when\n                                                              task description Dt, meta-instruction It and exemplar se-\ntraining the NN M(g(·); θ) (for score prediction) using the\n                                                       quence E′t selected at the end of the last iteration t−1 (more\nhistory of the selected meta-prompts and their observed\n                                                                     details below) to construct a meta-prompt Qt = (Dt, It, E′t)\nscores, we simply use the scores (i.e., rewards) as the labels\n                                                                    (line 3). Then, we use Qt as the input prompt to the LLM\n\n\n                                                3\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nf(·) to select the next action xt and collect its score st (lines    cuss more details on this, as well as the extension of our\n4-5). After that, we update the set of exemplars Et and the   EXPO algorithm to automatically select E′t+1, in Sec. 3.2.\nmeta-prompt-score set St (lines 6-7).\n2⃝Score Estimation (lines 8-9). In the classical EXP3 al-   Algorithm 1 EXPO\ngorithm for adversarial bandits with a finite number of arms,    input  : Initial task description D0, initial meta-instruction\nthe cumulative sum of the observed rewards of every arm        I0.\nis used to construct the arm sampling distribution through       1: Initialize the exemplar set E0 = ∅, and the subset E′0 =\nan exponential-weight mechanism (Lattimore & Szepesv´ari,         ∅, meta-prompt-score set S0 = ∅, and cumulative score\n2020). However, in problems where the number of arms is        estimates ˆs(0)i = 0 for all i ∈{1, . . . , k}.\nexcessively large (e.g., our problem of meta-prompt opti-      2: for iteration t = 0, 1, . . . , T −1 do\nmization), the reward observations for many arms are not       3:   Construct meta-prompt Qt = (Dt, It, E′t).\navailable. Therefore, the cumulative sum of the estimated       4:   Query the LLM f(·) using the meta-prompt Qt to\nrewards of every arm is often used instead to construct            select the next action xt: xt = f(Qt).\nthe sampling distribution (Lattimore & Szepesv´ari, 2020).      5:   Observe the score st for xt using the task-specific\nTherefore, we firstly estimate the scores of all k arms (i.e.,           evaluator: st = ξ(xt).\nmeta-prompts) in the domain and then use these score esti-      6:   Update the exemplar set Et+1 = Et ∪{(xt, st)}.\nmates to derive an arm sampling distribution for our EXPO.      7:   Update the meta-prompt-score set St+1 = St ∪\nA number of recent works have shown that using a neu-          {( g(Dt) ⊕g(It)  , st)}, where g(·) denotes the em-\nral network (NN) (which takes the pre-trained embedding          bedding function and ⊕denotes concatenation.\ng(·) as input) for score/reward estimation leads to powerful       8:   Update the parameters θ of the neural network (NN)\nprompt optimization algorithms (Lin et al., 2024a;b; Wu         M(g(·); θ) by using the updated St+1 as the training\net al., 2024). Therefore, we also adopt an NN M(g(·); θ)            set to minimize the MSE loss, yielding θt+1.\nfor score estimation in our EXPO. Specifically, in every it-      9:   Update the cumulative score estimates  ˆs(t)i   for\neration t, we use the history of selected meta-prompts and             all  arms  i  using  the  predicted  scores  from\ntheir scores, denoted as St+1 (line 7 of Algo. 1), to train an         M(g(·); θt+1):\nNN by minimizing the mean-squared error (MSE) loss (line\n8 of Algo. 1). The trained NN with parameters θt+1 can                  ˆs(t+1)i   = ˆs(t)i + M( g(Di) ⊕g(Ii)  ; θt+1)\nthen be used to estimate the score of every arm (i.e., every\n                                                                          ∀i ∈{1, . . . , k}.                    (1)\ncombination of task description and meta-instruction) in the\ndomain. For every arm, its estimated score is then added to                                                                  10:   Compute the sampling distribution Pt over all arms:\nits corresponding cumulative sum of score estimates ˆs(t+1)i\n(line 9 of Algo. 1). Note that every term in the cumula-                        exp(ηˆs(t+1)i    )\ntive sum ˆs(t+1)i     represents our score estimate for arm i in              Pt[i] = Pkj=1 exp(ηˆs(t+1)j    ) ,   ∀i ∈{1, . . . , k}\na particular iteration t, i.e., our estimated score for arm                                                                                                                           (2)\ni from an NN trained using the observation history up to                                                                  11:   Sample  an arm   (i.e.,  the  combination  of  a\niteration t. The updated cumulative sums of score estimates                                                                    task description and a meta-instruction) from Pt:\nˆs(t+1)i     for all k arms are then used for randomized arm (i.e.,         (Dt+1, It+1) ∼Pt.\nmeta-prompt) selection, which we discuss next.                 12:    Select a sequence of exemplars E′t+1 from Et+1 fol-\n3⃝Randomized Meta-Prompt Selection (lines 10-12).         lowing a pre-defined heuristic method.\nAfter the cumulative sum ˆs(t+1)i     of every arm i is updated,\nwe follow the EXP3 algorithm (Lattimore & Szepesv´ari,\n2020) and use the cumulative sums to construct a distribu-    Exploitation vs. Exploration. Our EXPO algorithm is able\ntion following Equation (2). Then, we use this distribution     to achieve a principled balance between exploitation and\nto randomly sample the next arm, i.e., the next task descrip-    exploration. The use of powerful pre-trained embedding and\ntion Dt+1 and meta-instruction It+1 (line 11 of Algo. 1).   NNs allows us to achieve accurate score estimates. There-\nRandomization is a key principle in adversarial bandits (Lat-     fore, the cumulative score estimate ˆs(t+1)i     (line 9 of Algo. 1)\ntimore & Szepesv´ari, 2020), and the randomization involved    provides a reliable assessment of the quality of every arm\nin our arm selection strategy is crucial for the ability of our     i (i.e., every combination of task description and meta-\nEXPO to deal with non-stationary reward observations. The     instruction). This ensures that an arm with a large score\nheuristic to select a sequence of exemplars E′t+1 (line 12     is given a large weight in the sampling distribution Pt (line\nof Algo. 1) is often specified by the LLM-based sequential    10) and hence leads to reliable exploitation. Meanwhile,\ndecision-making algorithm (Yang et al., 2024). We dis-    the inherent randomness in our randomized arm selection\n                                                                  strategy ensures that enough exploration is performed in the\n\n\n                                                4\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\ndomain of meta-prompts.                                      the domain, we need to obtain its cumulative score estimate\n                                                                (similar to line 9 of Algo. 1). Unfortunately, due to the\nBatch Action Selection.   In the description of our\n                                                          time-varying nature of the domain of exemplar sequences\nEXPO (Algo. 1), although we select one action xt in every\n                                                          (due to the addition of new exemplars and random sampling\niteration t, this can be easily generalized to select a batch of\n                                                              of exemplar sequences), we are no longer able to constantly\nactions. For example, when applying our EXPO to improve\n                                                         maintain a cumulative score estimate for every exemplar\nOPRO (Yang et al., 2024) (Sec. 4.1), we follow the practice\n                                                      sequence (i.e., arm) and update it in an incremental way.\nof OPRO to select a batch of 8 actions/queries in every iter-\n                                                To this end, we save the parameters of the trained NN in\nation (i.e., step 4 of Algo. 1) and set the temperature of the\n                                                         every iteration in history; then for each sampled exemplar\nLLM to 1 to ensure the diversity of the selected actions. In\n                                                        sequence in the domain, we obtain its score estimates from\norder to obtain a noiseless and reliable score to assess the\n                                                                           all NNs in the history and use their sum as the cumula-\nquality of the meta-prompt Qt, we set the temperature to\n                                                                     tive score estimate for this exemplar sequence (lines 9-14\n0 when selecting the last action and use its corresponding\n                                                            of Algo. 2). Next, the cumulative score estimates for all\nobserved score as the score st of Qt (line 5 of Algo. 1).\n                                                      exemplar sequences are used to compute the sampling dis-\n                                                                    tribution, from which the next exemplar sequence E′t+1 is\n3.2. EXPO with Exemplar Selection (EXPO-ES)                                                    sampled and used to the meta-prompt in the next iteration\nPrevious works on LLM-based for sequential decision mak-    (lines 15-16).\ning often select the sequence of exemplars E′t+1 included in\nthe meta-prompt Qt using a fixed pre-defined heuristic (line    4. Experiments\n12 of Algo. 1). For example, OPRO includes the 20 exem-\nplars with the highest observed scores in the meta-prompt,   We firstly apply our algorithms to improve the performance\narranging them in descending order based on their scores    of OPRO in the Linear Regression (LR) and traveling sales-\n(Yang et al., 2024); the LLM-based MAB method from Kr-   man problem (TSP) tasks, adopting the same experimental\nishnamurthy et al. (2024) either includes all exemplars (or-    setting as Yang et al. (2024) (Sec. 4.1). Then, we use our\ndered by their iteration sequence) in the prompt or includes    algorithms to enhance the performance of the LLM-based\na summarized representation of all exemplars. However,  MAB algorithm from Krishnamurthy et al. (2024).\nnumerous previous works have reported that both the subset\nof exemplars and their ordering have significant impacts on     4.1. Linear Regression and Traveling Salesman Problem\nthe performance of LLM (Wu et al., 2024). Therefore, here\n                                                       For both tasks here, we adopt GPT-3.5-Turbo as the LLM.\nwe further extend our EXPO (Algo. 1) to additionally opti-\nmize the sequence of exemplars E′t+1 (i.e., to replace line 12    Linear Regression (LR). In the LR task, our goal is to find\nof Algo. 1 by an automated method to select E′t+1), hence    the optimal LR coefficients, w and b, that best fit a set of\nintroducing our EXPO-ES algorithm (Algo. 2, App. A).       given noisy observations. We firstly choose the groundtruth\n                                       LR coefficients wtrue and btrue, and use them to generate\nAs a result of the dynamically changing task description and\n                                                           noisy observations for 50 inputs x which are randomly and\nmeta-instruction, the optimization of exemplar sequences\n                                                        uniformly selected within [−1, 1].  Specifically, for each\nbecomes non-stationary as well. Therefore, we also dynam-\n                                                            input x, we generate its noisy observation as y = wtruex +\nically optimize the exemplar sequence based on the EXP3\n                                                                                    btrue + ϵ where ϵ is a Gaussian noise. We adopt the two most\nalgorithm for adversarial bandits. That is, in every iteration\n                                                            challenging choices of coefficients from Yang et al. (2024):\nof our EXPO-ES algorithm (Algo. 2), we firstly optimize\n                                                                (1) wtrue = 2, btrue = 30 and (2) wtrue = 36, btrue = −1. In\nthe task description and meta-instruction (i.e., following\n                                                                     this task, OPRO aims to find the optimal w and b which\nlines 3-11 of Algo. 1), and then optimize the exemplar se-\n                                                       minimizes the regression error (i.e., mean squared error).\nquence E′t+1 in a similar way to Algo. 1.\n                                                        Traveling Salesman Problem (TSP). In the classical TSP\nDetails of EXPO-ES (Algo. 2). Specifically, after the task\n                                                    problem (J¨unger et al., 1995), given a set of n nodes with\ndescription and meta-instruction are optimized (i.e., after\n                                                                    their coordinates, the objective is to find the shortest route\nlines 3-11 of Algo. 1), we firstly extract the embedding of\n                                                                  that starts from a given node, traverses all nodes exactly\nthe exemplar sequence E′t used in this iteration: g(E′t), and                                                             once, and finally returns to the starting node. Therefore, our\nadd  g(E′t), st  to the exemplar training set Tt+1 (line 4 of                                                          goal is to solve a discrete optimization problem in which\nAlgo. 2). Next, the updated dataset Tt+1 is used to train an\n                                                               the input variable is a trajectory and the goal is to minimize\nNN with parameters θESt+1 (line 5), which is able to estimate                                                               the total distance of the trajectory. We adopt TSP instances\nthe score of any exemplar sequence. Subsequently, we ran-\n                                                          with 10, 15, and 20 randomly generated nodes, respectively,\ndomly sample kES exemplars sequences, each containing L\n                                                    which represent increasing levels of difficulty.\nexemplars, to be used as our domain of exemplar sequences\n(line 8). Next, for every candidate exemplar sequence in   The results for both tasks are shown in Fig. 2, which plot the\n\n\n                                                5\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n                                                                                                                                     OPRO                                                                                                                                            140                                                                                                  OPRO                                                                                                      70                                                                                                                                                                                OPRO                                                                      35                        OPRO                                                              OPRO   175                                     300                                                                                                                                     OPRO                                                                                                                                                                             (Enhanced)                                                                                                  OPRO                                                                                                                                                                                OPRO                                                                                                                                                                                                                                  (Enhanced)                        OPRO                                    (Enhanced)                                                              OPRO                                                                                                                                            120                                                                                                                                (Enhanced)  (%)                                                                                                      60                                                                                    (Enhanced)  (%)                                                                      30                                                                                                                                                          (%)                                                                                                                                              EXPO   150                                                                                                        EXPO                                                                                                                                                                                           EXPO                                     250                         EXPO                                                                  EXPO\n                                                                        EXPO-ES        Gap25                     EXPO-ES        Gap50                     EXPO-ES        Gap100                     EXPO-ESError125                     EXPO-ES              Error200\n   100                                                                 20                               40                                 80\n                                     150                               15                               30                                 60    75\n                                     100                                                                                                      20                                                                      10                                                                                                                                             40Regression 50                                                                                                                                  Regression                                      50                                                                                                          Optimality                                                                                                      10                                                                                                               Optimality                                                                       5                                                                                                            Optimality    25                                                                                                                                             20\n                                                                                                       0                                                                       0     0                                       0\n                                                                         0     25    50    75    100                                                                                                          0     50    100   150           0 0   50  100 150 200 250       0    10   20   30   40   50                                         0    10   20   30   40   50\n                      Iteration                                      Iteration                                    Iteration                                    Iteration                                      Iteration\n    Linear Regression      Linear Regression         TSP              TSP              TSP\n    (w = 2, b = 30)      (w = 36, b = −1)         (10 Nodes)            (15 Nodes)            (20 Nodes)\n\n  Figure 2. Results of different algorithms (mean ± standard error) in the Linear Regression and TSP task (Sec. 4.1). Lower is better.\n\n  regression error (i.e., mean squared error) for the LR tasks     for the TSP tasks are displayed in Fig. 16 in App. C.4.\n and optimality gap (i.e., the difference between the total dis-\n  tance of the discovered route and that of the optimal route)     4.2. LLM-Based Multi-Armed Bandits (MAB)\n  for the TSP tasks (lower is better for both tasks). Of note,\n                                                  The work of Krishnamurthy et al. (2024) has used an LLM  in addition to the standard OPRO (pink curves) (Yang et al.,\n                                                                    to sequentially select the arms/actions in MAB and proposed  2024), we have also proposed an enhanced variant of OPRO\n                                                       methods to manually design the meta-prompt. Their prompt (green curves) in which we added some further clarifications\n                                                           design consists of 5 components with each having 2 possi-  to the task description (see App. B.2.5 for more details). The\n                                                               ble choices, which gives rise to a total of 25 = 32 possible enhanced variant consistently improves the performance of\n                                                          prompts. Here we show that our algorithms can be used to  the standard OPRO (Fig. 2).2 More importantly, the re-\n                                                              automatically optimize their manually designed prompts to  sults in Fig. 2 show that in all tasks, our EXPO algorithm\n                                                                    further enhance their performance. Specifically, we adopt 2 (blue curves) significantly and consistently outperforms\n                                                                of their prompt designs: BSSND and BSSCD, and apply our OPRO, including both standard OPRO and its enhanced\n                                           EXPO and EXPO-ES algorithms to optimize the important  variant. This demonstrates that our meta-prompt optimiza-\n                                                     components in these prompt designs. Following Krishna-  tion approach, grounded in adversarial bandits, leads to\n                                                      murthy et al. (2024), we use two MAB instances: easy and more efficient (i.e., faster convergence) and more effective\n                                                            hard. We adopt GPT-4-Turbo as the LLM here. More de-  (i.e., improved final performance) LLM-based sequential\n                                                                              tails on the experimental design are deferred to App. B.3.1.  decision-making.\n                                                 The results for the 4 experimental settings (i.e., 2 prompt\n Meanwhile, our EXPO-ES algorithm, which is addition-    designs × 2 MAB instances) are shown in Fig. 4, which\n  ally equipped with automated exemplar selection, consid-    demonstrate that our EXPO and EXPO-ES algorithms are\n  erably improves the performance of EXPO in the LR tasks    able to significantly reduce the cumulative regret of MAB\n  yet performs on par with EXPO in the TSP tasks. This is     in this task across different prompt desings and MAB in-\n  likely because the exemplars play a more important role in     stances. We illustrate the comparison between the original\n  the LR tasks than the TSP tasks. Specifically, in LR, the    meta-prompt and the one optimized by our EXPO in Figs. 17\n  input-output exemplars provide important information for    and 18 in App. C.4.\n  identifying the optimal LR coefficients (Wu et al., 2024).\n Therefore, selecting better exemplars (via our EXPO-ES)\n                                                          5. Ablation Study\n  brings significant performance boost. On the other hand, in\n  the TSP tasks, due to the challenging nature of the tasks, it is   Only Optimizing Task Description or Meta-Instruction.\n  difficult for the LLM to infer crucial and useful information   Our EXPO jointly optimize the task description D and the\n from the exemplars. Therefore, the other components in the    meta-instruction I. Here we evaluate the performance of\n meta-prompt (i.e., the task description and meta-instruction)    optimizing either D or I alone. The results in Fig. 5 show\n provide more useful information in the TSP tasks. As a re-    that jointly optimizing them indeed leads to significantly\n  sult, selecting better exemplars does not lead to noticeable     better performance. However, optimizing these components\n performance gains in the TSP tasks. Fig. 3 provides an illus-    alone still consistently outperforms OPRO.\n  tration of the original task description and meta-instruction\n                                                Comparison with Stochastic MAB Algorithms: Upper used by OPRO and those discovered by our EXPO algorithm\n                                                      Confidence Bound. Classical stochastic MAB algorithms,  for the LR tasks, whereas the corresponding meta-prompts\n                                                          such as those based on upper confidence bound (UCB), have\n    2As discussed in the last paragraph of Sec. 3.1, we have slightly    been applied to prompt optimization in a number of recent\n  modified OPRO to select the last action in the batch using a tem-   works (Lin et al., 2024a;b; Wu et al., 2024) and yielded\n  perature of 0. We empirically show that this leads to comparable\n                                                             strong performance. However, as we have discussed in\n  performance with the original OPRO which uses a temperature of\n 1 to choose all 8 actions (see Fig. 14 in App. C.2).                 Sec. 1, in meta-prompt optimization for LLM-based sequen-\n                                                                                   tial decision-making, the non-stationary reward observations\n\n\n                                                  6\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n          OPRO                                    EXPO\n   Now you will help me minimize a function with     We will collaborate to optimize a function involving two parameters, \\(w\\) and \\(b\\). I possess a set of\n   two input variables w, b.  I have some (w, b)        data points, each consisting of \\((w, b)\\) pairs and their corresponding function values. These pairs are\n    pairs and the function values at those points.        systematically organized in reverse order, starting from the greatest to the smallest function values. Essentially,\n   The pairs are arranged in descending order based        the lower the function value, the more optimal or preferable the pair. Consequently, our goal is to identify\n   on their function values, where lower values are       and analyze the \\((w, b)\\) pair that manifests the lowest function value, as this represents the perspective of\n     better.                                       optimum efficacy.\n\n   {EXEMPLARS}                        {EXEMPLARS}\n\n   Give me a new (w, b)  pair  that  is  differ-      To enhance the quality and expand on the existing instructions, follow these improved guidelines\n    ent from all pairs above, and has a function value          vis-`a-vis designing a new and distinctive numerical pair: ensure the selected (w, b) combination diverges from\n    lower than any of the above. Do not write code.        prior examples and secures a function output lower than preceding values. Key details on methodology or\n   The output must end with a pair [w, b], where         calculations are not required—just ensure clarity in presenting a returned value that closes with the specific\n  w and b are numerical values.                        format [w, b], where both w and b are distinct numerical figures.\n\nFigure 3. The task description and meta-instruction used by OPRO (left) and optimized by our EXPO (right) in a Linear Regression task.\n\n                                                                                 300                                                                                                                                                       Neural UCB        175                             Neural UCB\n                                                                                 250                        EXPO            150                        EXPO\n            BSSND                                                              BSSCD    10                                                                                                                                                                                                                            Error125                                                                                                                                                    Error200             EXPO                                                               EXPO\n                                                                    EXPO-ES     8                                                                                                                        100   Regret                                                                                 150              EXPO-ES                                                              Regret108\n     6                                                                                                                    75\n                                               6                                          100                                                                                                                         50     4                                          4                                                                                                                                                   Regression                                                                                                                                     Regression                                                                                  50                                    25    Cumulative 2                                                                                                                                               Cumulative 2\n                                                                                   0                                     0\n     00      20     40     60     80     100     00      20     40     60     80     100               0    10   20   30   40   50           0    10   20   30   40   50                           Iteration                                                 Iteration                                                   Iteration                                          Iteration\n     BSSND (easy)         BSSCD (easy)                    Linear Regression               Linear Regression\n                                                          (w = 36, b = −1)            (w = 2, b = 30)    10            BSSND                               10      BSSCD\n   Regret 8      EXPOEXPO-ES                                                              Regret 8      EXPOEXPO-ES                        Figure 6. Comparison of our EXPO with NeuralUCB (i.e., a repre-\n     6                                          6\n                                                                        sentative stochastic MAB algorithm) in the LR tasks.\n     4                                          4    Cumulative 2                                                                                                                                               Cumulative 2\n                                                             the impact of the degree of exploration, i.e., the value of     00      20     40     60     80     100     00      20     40     60     80     100\n                           Iteration                                                 Iteration             η (see line 10 of Algo. 1). The results (Fig. 7) show that\n     BSSND (hard)         BSSCD (hard)          an excessively large degree of exploration (i.e., a small\n                                                   η = 10) or an overly small degree of exploration (i.e., a largeFigure 4. Cumulative regret of different algorithms in the LLM-\nbased MAB experiments (Sec. 4.2). Lower is better.             η = 1000) both deteriorate the performance. Moreover,\n                                                             the results also demonstrate that in easier tasks (i.e., TSP\n                                OPRO                                                                                        OPRO    175                                 EXPO                                                   (Task-Description                                                                     only)        300                                                                                              EXPO                                                                                                                                   (Task-Description                                                                                                                                                      only)      with 10 nodes), imposing a smaller degree of exploration\n                                 EXPO                                                   (Meta-Instruction only)                                                                                              EXPO                                                                                                                                   (Meta-Instruction only)    150                                           250                                 EXPO                                                                                              EXPO\n   Error125                                                              Error200                                     (i.e., η = 1000) leads to better performance compared to\n    100                                           150                       η = 10, because it allows our EXPO to quickly converge to     75\n                                           100                            the optimal solution. On the other hand, in more challenging       Regression 50                                                                                                                           Regression\n     25                                    50                            tasks (i.e., TSP with 20 nodes), more exploration (i.e., η =\n      0                                     0\n         0    10   20   30   40   50           0    10   20   30   40   50     10) results in better performance (than η = 1000), because\n                         Iteration                                           Iteration\n                                                                                           it makes it easier for our EXPO to escape local optimum.\n     w = 2, b = 30         w = 36, b = −1\n                                                    Experiments With Other LLMs. To evaluate the effective-\nFigure 5. Results of our EXPO when only optimizing the task                                                           ness of our approach when combined with different LLMs,\ndescription or the meta-instruction.\n                                                           here we adopt the challenging TSP task with 20 nodes and\n                                                            replace the GPT-3.5-Turbo model used in our original ex-\nrender these stochastic MAB methods unsuitable. Here we    periments (Sec. 4.1) by the more advanced GPT-4-Turbo\nverify this by comparing our EXPO with the NeuralUCB    model. The results in Fig. 7 (bottom right) show that the\nalgorithm adopted by Lin et al. (2024b); Wu et al. (2024).    use of the more advanced GPT-4-Turbo model significantly\nThe results for the Linear Regression tasks are displayed    improves the performance of both OPRO and our EXPO.\nin Fig. 6, which show that NeuralUCB indeed significantly   More importantly, as visualized more clearly in Fig. 13 in\nunderperforms in the problem of meta-prompt optimiza-   App. C.1, when both adopting GPT-4-Turbo, our EXPO still\ntion for LLM-based agents. The results for the TSP tasks     significantly outperforms OPRO. The results show that our\nare consistent with the results here (Fig. 19 in App. C.5).   EXPO can effectively improve the performance of LLM-\nThese results provide further justifications for our proposed    based agents across different LLMs.\nadversarial bandit-based algorithms.\n                                                          Effectiveness of the Optimal Prompt Discovered by\nImpact of the Degree of Exploration. Here we examine   EXPO. To further verify the ability of our EXPO to iden-\n\n                                                7\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n                                                        6. Related Work\n   35                           EXPOEXPO (eta=1000)(eta=100)      70                           EXPOEXPO (eta=1000)(eta=100)\n   30                           EXPO (eta=10)      60                           EXPO (eta=10)     Prompt Optimization. The field of prompt optimization\n(%)                                           (%)   25                                       50                           has been gaining significant popularity recently.  Earlier\nGap                                           Gap\n   20                                       40                         works on this topic have focused on optimizing the prompt\n   15                                       30                               for white-box LLMs (Shin et al., 2020; Shi et al., 2023; Optimality10                                                                                                                                         Optimality20\n                                                            Lester et al., 2021; Li & Liang, 2021; Zhong et al., 2021;    5                                       10\n    0                                        0                      Deng et al., 2022). More recently, a number of works\n      0     20    40                         60    80    100       0       50                                                              100     150          have developed prompt optimization methods for black-box                                                                                  Iteration                          Iteration\n     TSP (10 nodes)         TSP (15 nodes)        LLMs (Chen et al., 2023; Zhou et al., 2023; Fernando et al.,\n   140                           EXPO (eta=1000)     140                 OPRO (Enhanced, GPT-4-turbo)     2023; Guo et al., 2024; Hu et al., 2024; Lin et al., 2024b;\n                                  EXPO                                              (eta=100)\n                                  EXPO   120                                              (eta=10)      120                  EXPOOPRO (GPT-4-turbo)(Enhanced)         Zhan et al., 2024; Juneja et al., 2024; Wang et al., 2023a;\n(%)                                                (%)100                  EXPO   100                                             Kong et al., 2024; Schneider et al., 2024; Shi et al., 2024).Gap                                                Gap 80    80                                                            In addition, some recent works have focused on automati-\n                                              40                               cally selecting the exemplars for in-context learning (Wang Optimality 6040                                                                                                                                         Optimality 60\n                                              20                                et al., 2023b; Chang & Jia, 2023; Li & Qiu, 2023; Zhang    20\n                                               0                                et al., 2022; Nguyen & Wong, 2023; Albalak et al., 2024;\n       0    50   100  150  200  250  300        0     20    40    60    80\n                            Iteration                                                Iteration           Ye et al., 2023; Liu et al., 2022; Gao et al., 2024; Rubin\n     TSP (20 nodes)    TSP (20 Nodes, GPT-4-Turbo)     et al., 2022; Ye et al., 2023; Levy et al., 2023; Gupta et al.,\n                                                            2023), whereas a few methods have been proposed to jointlyFigure 7. First three figures: ablation study on impact of explo-\nration parameter η. Bottom right: results using GPT-4-Turbo.       optimize the prompt and select the exemplars (Opsahl-Ong\n                                                                     et al., 2024; Wan et al., 2024; Wu et al., 2024). However,\n                                                                to the best of our knowledge, our algorithm is the first ap-\ntify effective meta-prompts, here we replace the original                                                       proach that is able to efficiently optimize the meta-prompt\ntask description and meta-instruction in an LLM-based se-                                                                 for LLM-based agents in sequential decision-making tasks.\nquential decision-making algorithm (e.g., OPRO) by the\noptimal ones discovered by our EXPO. For example, for   LLM-Based Sequential Decision-Making. Some recent\nORPO, we firstly run our EXPO to completion, and then use    works have proposed to leverage the strong capability of\nthe final meta-prompt selected by our EXPO as the meta-   LLMs to solve sequential decision-making tasks, such as\nprompt to execute OPRO again. The results in Fig. 8 show    Bayesian optimization (Yang et al., 2024), multi-armed ban-\nthat fixing the meta-prompt to be the one optimized by our     dits (Krishnamurthy et al., 2024; Xia et al., 2024; Chen\nEXPO leads to dramatic performance boost to LLM-based     et al., 2024; Mukherjee et al., 2024), and reinforcement\nsequential decision-making.                                  learning (Dai et al., 2024; Monea et al., 2024; Wang et al.,\n                                                          2024a). However, these works often provide a fixed manu-\n   10           BSSND                               10      BSSCD                         ally designed meta-prompt to the LLM, and are hence un-\n            EXPO                                              EXPO\n    8      BSSND (Prompt Optimized by EXPO)              8      BSSCD (Prompt Optimized by EXPO)           able to fully unleash the potential of LLM-based sequential\n                                                            decision-making. The field of LLM-based agents has seen a Regret 6                                                                                      Regret 6\n                                                             surging interest recently, for which a number of benchmarks\n    4                                          4 Cumulative                                                                                                                                                      Cumulative                           have been proposed (Liu et al., 2023; Wu et al., 2023; Xi\n    2                                          2                                  et al., 2024). We defer more a comprehensive discussion of\n    00      20     40     60     80     100     00      20     40     60     80     100    LLM-based agents to recent surveys on this topic (Cheng\n                          Iteration                                                 Iteration                  et al., 2024; Wang et al., 2024b; Xi et al., 2023).\n     BSSND (hard)         BSSCD (hard)\n   350                                             140                                                                   OPRO                    OPRO\n                                                                   OPRO                                                                                         (Enhanced)            7. Conclusion                    OPRO                               (Enhanced)   300                                             120                                                                       EXPO                     EXPO\n   250              OPRO (Prompt Optimized by EXPO)   (%)100              OPRO (Prompt Optimized by EXPO)      In this work, we have proposed our EXPO algorithm to Error\n   200                                         Gap 80                            automatically optimize the meta-prompt for LLM-based\n   150                                        60                             sequential decision-making tasks. We further extend our Regression100                                                                                                                                         Optimality 40                    EXPO to derive the EXPO-ES algorithm, which additionally\n    50                                        20                                                            optimizes the exemplars in the meta-prompt. Our algorithms\n     0                                         0       0     10     20     30     40     50        0    50   100  150  200  250        use neural networks to estimate the scores of different meta-                            Iteration                                                Iteration\n                                                       prompts and sequentially selects the meta-prompts in a ran-     Linear Regression            TSP\n    (w = 36, b = −1)            (20 Nodes)            domized fashion based on adversarial bandits. We use exten-\n                                                                sive experiments to show that our algorithms considerably\nFigure 8. Results achieved by fixing the meta-prompt to be the\n                                                    and consistently improve the performance of LLM-based\noptimal one discovered by our EXPO (gray curves).\n                                                               sequential decision-making.\n\n\n                                                8\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nImpact Statements                               Hu, W., Shu, Y., Yu, Z., Wu, Z., Lin, X., Dai, Z., Ng, S.-\n                                                               K., and Low, B. K. H. Localized zeroth-order prompt\nThis paper presents work whose goal is to advance the field                                                                optimization. In Proc. NeurIPS, 2024.\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be    Juneja, G., Natarajan, N., Li, H., Jiao, J., and Sharma, A.\nspecifically highlighted here.                               Task facet learning: A structured approach to prompt\n                                                                optimization. arXiv preprint arXiv:2406.10504, 2024.\nReferences                                                      J¨unger, M., Reinelt, G., and Rinaldi, G.  The traveling\nAlbalak, A., Elazar, Y., Xie, S. M., Longpre, S., Lambert,      salesman problem. Handbooks in operations research\n   N., Wang, X., Muennighoff, N., Hou, B., Pan, L., Jeong,     and management science, 7:225–330, 1995.\n  H., Raffel, C., Chang, S., Hashimoto, T., and Wang,\n                                                   Kong, W., Hombaiah, S. A., Zhang, M., Mei, Q., and Ben-\n  W. Y. A survey on data selection for language models.\n                                                                  dersky, M. Prewrite: Prompt rewriting with reinforcement\n  arXiv:2402.16827, 2024.                                                                   learning. arXiv preprint arXiv:2401.08189, 2024.\nChang, T.-Y. and Jia, R. Data curation alone can stabilize\n                                                        Krishnamurthy, A., Harris, K., Foster, D. J., Zhang, C.,\n   in-context learning. In Proc. ACL, pp. 8123–8144, 2023.\n                                                      and Slivkins, A. Can large language models explore\nChen, D., Zhang, Q., and Zhu, Y. Efficient sequential deci-      in-context? arXiv preprint arXiv:2403.15371, 2024.\n  sion making with large language models. arXiv preprint\n                                                            Lattimore, T. and Szepesv´ari, C. Bandit algorithms. Cam-\n  arXiv:2406.12125, 2024.\n                                                              bridge University Press, 2020.\nChen, L., Chen, J., Goldstein, T., Huang, H., and Zhou, T.\n                                                                   Lester, B., Al-Rfou, R., and Constant, N. The power of scale   InstructZero: Efficient instruction optimization for black-\n                                                                    for parameter-efficient prompt tuning. In Proc. EMNLP,  box large language models. arXiv:2306.03082, 2023.\n                                                               pp. 3045–3059, 2021.\nCheng, Y., Zhang, C., Zhang, Z., Meng, X., Hong, S., Li,\n                                                        Levy, I., Bogin, B., and Berant, J. Diverse demonstrations  W., Wang, Z., Wang, Z., Yin, F., Zhao, J., et al. Ex-\n                                                         improve in-context compositional generalization. In Proc.  ploring large language model based intelligent agents:\n                                                   ACL, pp. 1401–1422, 2023.   Definitions, methods, and prospects.  arXiv preprint\n  arXiv:2401.03428, 2024.                                                                   Li, X. and Qiu, X. Finding support examples for in-context\n                                                                   learning. In Proc. EMNLP, pp. 6219–6235, 2023.Dai, Z., Tomasi, F., and Ghiassian, S. In-context exploration-\n   exploitation for reinforcement learning. arXiv preprint                                                                  Li, X. L. and Liang, P. Prefix-tuning: Optimizing continu-\n  arXiv:2403.06826, 2024.                                                        ous prompts for generation. In Proceedings of the 59th\nDeng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T.,      Annual Meeting of the Association for Computational Lin-\n  Song, M., Xing, E., and Hu, Z. RLPrompt: Optimizing       guistics and the 11th International Joint Conference on\n   discrete text prompts with reinforcement learning.  In      Natural Language Processing (Volume 1: Long Papers),\n  Proc. EMNLP, pp. 3369–3391, 2022.                         pp. 4582–4597, 2021.\n\nFernando, C., Banarse, D., Michalewski, H., Osindero, S.,    Lin, X., Dai, Z., Verma, A., Ng, S.-K., Jaillet, P., and Low, B.\n  and Rockt¨aschel, T. Promptbreeder: Self-referential self-     K. H. Prompt optimization with human feedback. arXiv\n  improvement via prompt evolution. arXiv:2309.16797,       preprint arXiv:2405.17346, 2024a.\n  2023.\n                                                              Lin, X., Wu, Z., Dai, Z., Hu, W., Shu, Y., Ng, S.-K., Jaillet,\nGao, L., Chaudhary, A., Srinivasan, K., Hashimoto, K., Ra-        P., and Low, B. K. H. Use your INSTINCT: Instruction\n  man, K., and Bendersky, M. Ambiguity-aware in-context       optimization using neural bandits coupled with transform-\n   learning with large language models. arXiv:2309.07900,        ers. In Proc. ICML, 2024b.\n  2024.\n                                                               Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen,\nGuo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu,     W. What makes good in-context examples for GPT-3? In\n   G., Bian, J., and Yang, Y. Connecting large language mod-      Proc. DeeLIO: Deep Learning Inside Out, pp. 100–114,\n   els with evolutionary algorithms yields powerful prompt      2022.\n   optimizers. In Proc. ICLR, 2024.\n                                                             Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y.,\nGupta, S., Gardner, M., and Singh, S. Coverage-based ex-      Ding, H., Men, K., Yang, K., et al. Agentbench: Evalu-\n  ample selection for in-context learning. In Proc. EMNLP,       ating llms as agents. arXiv preprint arXiv:2308.03688,\n  pp. 13924–13950, 2023.                                   2023.\n\n                                                9\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nLu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp,   Wang, X., Li, C., Wang, Z., Bai, F., Luo, H., Zhang, J., Jojic,\n   P. Fantastically ordered prompts and where to find them:      N., Xing, E. P., and Hu, Z. Promptagent: Strategic plan-\n  Overcoming few-shot prompt order sensitivity. In Proc.      ning with language models enables expert-level prompt\n  ACL, pp. 8086–8098, 2022.                                   optimization. arXiv preprint arXiv:2310.16427, 2023a.\n\nMonea, G., Bosselut, A., Brantley, K., and Artzi, Y. Llms   Wang, X., Zhu, W., Saxon, M., Steyvers, M., and Wang,\n  are in-context reinforcement learners. 2024.              W. Y. Large language models are latent variable mod-\n                                                                          els: Explaining and finding good demonstrations for in-\nMukherjee, S., Hanna, J. P., Xie, Q., and Nowak, R. Pre-                                                              context learning. In Proc. NeurIPS, pp. 15614–15638,\n   training decision transformers with reward prediction for                                                         2023b.\n  in-context multi-task structured bandit learning. arXiv\n  preprint arXiv:2406.05064, 2024.                    Wu, Y., Tang, X., Mitchell, T. M., and Li, Y. Smartplay: A\n                                                     benchmark for llms as intelligent agents. arXiv preprint\nNguyen, T. and Wong, E. In-context example selection with      arXiv:2310.01557, 2023.\n   influences. arXiv:2302.11042, 2023.\n                                             Wu, Z., Lin, X., Dai, Z., Hu, W., Shu, Y., Ng, S.-K., Jail-\nOpsahl-Ong, K., Ryan, M. J., Purtell, J., Broman, D., Potts,        let, P., and Low, B. K. H.  Prompt optimization with\n   C., Zaharia, M., and Khattab, O.  Optimizing instruc-    EASE? efficient ordering-aware automated selection of\n   tions and demonstrations for multi-stage language model      exemplars. In Proc. NeurIPS, 2024.\n  programs. arXiv preprint arXiv:2406.11695, 2024.\n                                                            Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B.,\nRubin, O., Herzig, J., and Berant, J. Learning to retrieve      Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and\n  prompts for in-context learning.  In Proc. NAACL, pp.       potential of large language model based agents: A survey.\n  2655–2671, 2022.                                         arXiv preprint arXiv:2309.07864, 2023.\n\nSchneider, L., Wistuba, M., Klein, A., Golebiowski, J., Zap-    Xi, Z., Ding, Y., Chen, W., Hong, B., Guo, H., Wang, J.,\n   pella, G., and Merra, F. A. Hyperband-based bayesian op-     Yang, D., Liao, C., Guo, X., He, W., et al. Agentgym:\n   timization for black-box prompt selection. arXiv preprint      Evolving large language model-based agents across di-\n  arXiv:2412.07820, 2024.                                    verse environments. arXiv preprint arXiv:2406.04151,\n                                                          2024.\nShi, C., Yang, K., Yang, J., and Shen, C. Best arm identifi-\n  cation for prompt learning under a limited budget. arXiv    Xia, F., Liu, H., Yue, Y., and Li, T. Beyond numeric awards:\n  preprint arXiv:2402.09723, 2024.                             In-context dueling bandits with llm agents. arXiv preprint\n                                                           arXiv:2407.01887, 2024.\nShi, W., Han, X., Gonen, H., Holtzman, A., Tsvetkov, Y.,\n  and Zettlemoyer, L.  Toward human readable prompt    Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D.,\n  tuning: Kubrick’s the shining is a good movie, and a      and Chen, X. Large language models as optimizers. In\n  good prompt too? In Proc. EMNLP, pp. 10994–11005,      Proc. ICLR, 2024.\n  2023.                                                           Ye, J., Wu, Z., Feng, J., Yu, T., and Kong, L. Compositional\n                                                         exemplars for in-context learning. In Proc. ICML, pp.Shin, T., Razeghi, Y., IV, R. L. L., Wallace, E., and Singh,\n                                                       39818–39833, 2023.  S. Eliciting knowledge from language models using au-\n  tomatically generated prompts.  In Proc. EMNLP, pp.   Zhan, H., Chen, C., Ding, T., Li, Z., and Sun, R. Unlock-\n  4222–4235, 2020.                                          ing black-box prompt tuning efficiency via zeroth-order\n                                                               optimization. In Findings of the Association for Com-Wan, X., Sun, R., Nakhost, H., and Arik, S. O.  Teach\n                                                                putational Linguistics: EMNLP 2024, pp. 14825–14838,   better or show smarter?  on instructions and exem-\n                                                          2024.   plars in automatic prompt optimization. arXiv preprint\n  arXiv:2406.15708, 2024.                              Zhang, Y., Feng, S., and Tan, C. Active example selection\n                                                                     for in-context learning. In Proc. EMNLP, pp. 9134–9148,\nWang, J., Blaser, E., Daneshmand, H., and Zhang, S. Trans-\n                                                          2022.\n  formers learn temporal difference methods for in-context\n  reinforcement learning. arXiv preprint arXiv:2405.13861,   Zhong, Z., Friedman, D., and Chen, D. Factual probing is\n  2024a.                                          [MASK]: Learning vs. learning to recall. In Proc. NAACL,\n                                                               pp. 5017–5033, 2021.\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J.,\n  Chen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on    Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S.,\n   large language model based autonomous agents. Frontiers      Chan, H., and Ba, J. Large language models are human-\n   of Computer Science, 18(6):186345, 2024b.                    level prompt engineers. In Proc. ICLR, 2023.\n\n                                                10\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nA. Our EXPO-ES Algorithm to Additionally Optimize the Exemplar Sequences\n\n\nAlgorithm 2 EXPO-ES\ninput Initial task description D0, initial meta-instruction I0.\n   Maximum number L of exemplars in the meta-prompt, the number kES of exemplar sequences in the domain.\n  1: Initialize the exemplar set E0 = ∅, and the subset E′0 = ∅, meta-prompt-score set S0 = ∅, and cumulative score estimates\n      ˆs(0)j   for all j ∈{1, . . . , kES}.\n     Initialize the history of NN parameters Θhistory = ∅, and the exemplar training set T0 = ∅.\n  2: for iteration t = 0, 1, . . . , T −1 do\n  3:   Lines 3-11 of Algo. 1.\n  4:   Compute the embedding g(E′t) of the selected exemplar sequence E′t, and add g(E′t) and its score st to the exemplar\n       training set: Tt+1 ←Tt ∪{(g(E′t), st)}.\n  5:   Update the parameters θES of the NN MES(g(·); θES) by using the updated Tt+1 as the training set to minimize the\n    MSE loss, yielding θESt+1.\n  6:   Add the updated parameters to the history: Θhistory ←Θhistory ∪{θESt+1}.\n  7:     if |Et+1| > L then\n  8:     Randomly generate kES sequences of L exemplars from the exemplar set Et+1: {E1t+1, E2t+1, . . . , EkESt+1}, in which\n         every Ejt+1 represents an ordered set of L exemplars from Et+1.\n  9:       Initialize cumulative score estimates ˆs(0)j = 0 for all j ∈{1, . . . , kES}.\n10:      for each Ejt+1 in {E1t+1, . . . , EkESt+1} do\n11:          Initialize cumulative score ˆs(0)j = 0.                                                       cumulative\n                                                                                                          score\n12:        for each historical model parameter θESi  ∈Θhistory do\n                                                                                                               estimates\n13:         Update the cumulative score for Ejt+1: ˆs(i)j = ˆs(i−1)j  + MES(g(Ejt+1); θESi  ).\n14:     Compute the final cumulative score estimates: ˆs(final)j  = ˆs(|Θhistory|)j          ,  ∀j ∈{1, . . . , kES}.\n15:     Compute the sampling distribution PtES over the k exemplar sequences:\n\n                                                              exp(ηˆs(final)j    )                                        ES                        Pt  [j] =                              ,  ∀j ∈{1, . . . , kES}.                            (5)\n                                      PkESl=1 exp(ηˆs(final)l    )\n\n16:     Sample an exemplar sequence E′t+1 ∼PtES .\n\n\n\nOur complete EXPO-ES algorithm is described in Algo. 2. As we have discussed in Sec. 3.2, there are two major differences\ncompared to the way in which our EXPO algorithm optimizes the task description and meta-instruction (Algo. 1). Firstly,\nour domain of kES arms (i.e., every arm corresponds to a randomly sampled exemplar sequence) changes in every iteration\n(line 8). Secondly, as a result of the time-varying domains, we need to save a copy of the parameters of the NN trained in\nevery iteration in order to compute the cumulative score estimates (lines 9-14).\n\nSimplified Variant of Our EXPO-ES Algorithm.  When applying our EXPO-ES algorithm to the LLM-based MAB\nalgorithm in Krishnamurthy et al. (2024) (Sec. 4.2), we have adopted a simplified variant of our EXPO-ES. This is because\nin the problem setting from Krishnamurthy et al. (2024), the number of arms is small. Therefore, instead of including a\nsubset of the history of exemplars in the prompt, their algorithm has instead included a summarized observation history. An\nexample of such summarized observation history with 5 arms (represented by 5 buttons with different colors) is given in\nFig. 9 below. Therefore, here we aim to optimize the format of the summarized observation history. Specifically, we optimize\nthe order of the arms in the summarized history, and our domain of arms consist of all cyclically shifted variants of the fol-\nlowing sequence of buttons: {blue button, green button, red button, yellow button, purple button}. For example, some other\narms (button sequences) in our domain include: {green button, red button, yellow button, purple button, blue button} and\n{red button, yellow button, purple button, blue button, green button}. As a result, unlike our original EXPO-ES algorithm\ndescribed in Sec. 3.2, here we do not suffer from the issue of time-varying domain of arms.\n\nTherefore, when applying our EXPO-ES algorithm to improve the LLM-based MAB method from Krishnamurthy et al.\n\n                                                11\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n     ...\n\n   blue button: pressed 2 times with average reward 0.5\n   green button: pressed 1 times with average reward 0.0\n   red button: pressed 1 times with average reward 1.0\n   yellow button: pressed 0 times\n   purple button: pressed 1 times with average reward 0.0\n\n     ...\n\n\nFigure 9. An example of the summarized observation history used by the LLM-based MAB algorithm from Krishnamurthy et al. (2024).\n\n\n(2024) (Sec. 4.2), we make two modifications to our standard EXPO-ES algorithm described in Algo. 2. Firstly, instead of\nrandomly sampling kES exemplar sequences to form our domain of exemplar sequences, here our domain remains fixed\nacross different iterations, i.e., all cyclically shifted variants of the arms. Secondly, since here we do not suffer from the\nissue of time-varying domain of arms (i.e., exemplar sequences), we can resort to the incremental update of the cumulative\nreward estimates adopted by our EXPO algorithm (line 9 of Algo. 1). As a result, we do not need to save a copy of the\nparameters of the NN trained in every iteration.\n\nB. More Details on Our Experimental Settings\n\nB.1. More Details on the Generation of the Domain of Task Description and Meta-Instruction\n\nHere we describe the details about how we generate the domain of task descriptions and meta-instructions. Below we\nprovide the prompt we have used to instruct the LLM to generate every prompt in the domain.\n\n   Example Query: Meta-Prompt Instruction Rephrasing Template\n\n   To achieve a more effective TASK description and INSTRUCTION and convey its core essence more clearly, please enhance the\n    content in the quote by rephrasing and changing some information: ”{INITIAL META-PROMPT}”\n    Please return directly the modified description without additional description.\n   The modified description:\n\n\n\nGeneration of the Domain. To effectively generate task-specific prompts, we utilized an initial prompt to guide the LLM in\ncreating diverse task descriptions and meta-instructions. For each task, the LLM was prompted 100 times to rephrase the\ntask description and meta-instruction separately, resulting in 100 unique rephrased prompts for each. Combined with the\ninitial prompt, this process produced a total of 101 × 101 combinations of task descriptions and meta-instructions for each\ntask.\n\nTo optimize computational efficiency, we pre-compute the embeddings of all task descriptions and meta-instructions in\nthe domain using the embedding model g(·) and store the results to prevent redundant calculations during subsequent\nexperiments.\n\nFor the rephrasing process, we employed the GPT-4 model with a temperature setting of 1.3, ensuring diverse and high-quality\nrephrased prompts for both task descriptions and meta-instructions.\n\n\nB.2. More Details on OPRO for the Linear Regression and Traveling Salesman Problem (Sec. 4.1)\n\nB.2.1. TASK SETTING.\n\nLinear Regression. We conduct experiments on Linear Regression by selecting two challenging ground truth weight-bias\n(w, b) pairs. The experiments follow the OPRO framework, which requires warm-starting the LLM with initial exemplars.\nUsing a fixed random seed, we first generate 50 random data points uniformly distributed within the range [−1, 1], which\nperfectly satisfy the ground truth wtrue, btrue pairs, ensuring that these data points can serve as the foundation for evaluating\nthe LLM’s ability to model the relationships. Additionally, 5 w, b pairs with corresponding scores, sampled within the\nrange [10, 20], are generated using another fixed random seed to serve as the initial exemplars. At each iteration, the LLM\nis prompted 8 times (consisting of 1 inference with a temperature setting of T = 0 and 7 inferences with a temperature\n\n                                                12\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nsetting of T = 1) using the current exemplars, and the prompt is updated based on the generated outputs. The exemplars are\ndynamically updated to include the top 20 w, b pairs and their associated scores from all historical records across iterations,\nensuring the LLM is always guided by the best-performing examples. The total number of iterations is set to 50, and each\nground truth configuration is repeated 5 times for consistency.\n\nTraveling Salesman Problem (TSP). For the TSP task, experiments are conducted on three problem sizes defined by the\nnumber of nodes: 10, 15, and 20. For each TSP instance, the problem is defined by randomly generating n = 10, 15, 20\nnodes, where the x and y coordinates of each node are sampled uniformly from the range [−100, 100]. For each configuration,\na specific TSP instance is generated using a fixed random seed, and a single random seed is used to generate warm-start\nexemplars to initialize the LLM prompts. To initialize the optimization process, we randomly sample 5 different TSP routes\nalong with their corresponding total distances. These routes and their lengths are used as the initial exemplars for the LLM.\nEach iteration consists of 8 prompt calls to the LLM, followed by an update of the exemplars based on the generated results.\nMore specifically, during each iteration, the GPT-3.5-turbo is prompted 8 times using the same prompt, consisting of 1\ninference with a temperature setting of T = 0 to ensure stability and 7 inferences with a temperature setting of T = 1 to\nencourage exploration. Similar to the Linear Regression task, the exemplars for TSP are updated to include the top 20\nhistorical solutions with the best scores, ensuring the prompt leverages the most effective examples. The number of iterations\nis set to 100, 200, and 300 for 10-node, 15-node, and 20-node TSP problems, respectively, to account for the increasing\ncomplexity of the tasks. Each node configuration is repeated 3 times to ensure consistency and reliability.\n\nB.2.2. EVALUATION METRICS.\n\nLinear Regression. In the Linear Regression task, the performance of the algorithms is evaluated using the Mean Squared\nError (MSE) metric. Given a set of N one-dimensional input data points x ∈R and their corresponding ground truth labels\ny ∈R, the MSE is computed as:\n                                           1                    2\n                          MSE =    y −(w · x + b)    ,\n                           N\nwhere w ∈R and b ∈R are the weight and bias parameters inferred by the LLM, and N is the total number of data points.\n\nTraveling Salesman Problem (TSP). For the TSP task, the performance of the LLM-generated solutions is evaluated based\non the total Euclidean distance of the TSP tour. Given a set of two-dimensional points {(xi, yi)}Ni=1, where N is the total\nnumber of nodes, the length of a proposed TSP tour P = [π(1), π(2), . . . , π(N), π(1)] is computed as:\n\n                           N r                   2                      2\n                          Length = X    xπ(i+1) −xπ(i)  +  yπ(i+1) −yπ(i)    ,\n                                       i=1\n\nwhere π represents the permutation of nodes in the proposed tour, and π(N + 1) = π(1) ensures the tour returns to the\nstarting node.\n\nTo evaluate the convergence and effectiveness of the agents, we use the Optimality Gap metric, which quantifies the deviation\nof the solver’s best-found solution from the true optimal solution. It is defined as:\n\n                                               SolverOptimal −Optima\n                               Optimality Gap =              × 100%,\n                                                  Optima\n\nwhere:\n\n\n   • SolverOptimal denotes the shortest tour length found by the solver up to the current iteration.\n\n   • Optima is the length of the known optimal TSP tour.\n\n\nB.2.3. DESIGN OF PROMPT SCORE.\n\nIn both the Linear Regression and TSP tasks, optimal solutions are characterized by lower evaluation scores. To align with\nthe requirements of the algorithm and ensure more stable learning, we define the Prompt Score using the formula:\n\n                                                  −Evaluation Score + b\n                                 Prompt Score =                                   ,\n                                                                b\n\n                                                13\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nwhere b > 0 is a stabilizing constant. This formulation ensures that lower evaluation scores correspond to higher prompt\nscores, which better facilitates the optimization process and contributes to steady algorithmic learning.\n\nFor the Linear Regression task, the Evaluation Score is defined as the Mean Squared Error (MSE) of the weight-bias (w, b)\npairs proposed by the algorithm at each iteration under a Temperature=0 stable inference. The MSE is computed based on\nthe provided one-dimensional data points.\n\nFor the TSP task, the Evaluation Score corresponds to the total Euclidean distance (Length) of the TSP tour proposed by the\nalgorithm at each iteration, also under a Temperature=0 stable inference.\n\nB.2.4. DETAILS ABOUT THE MODELS AND PARAMETERS IN OUR ALGORITHMS\n\nLLM Agents and Embedding Model. In our experiments, the primary LLM agent used is GPT-3.5-Turbo. For embedding\ngeneration, we utilized OpenAI’s text-embedding-3-large model, which outputs embeddings of dimensionality\n3072. These embeddings were used to represent both the task description and meta-instruction in the EXPO framework.\nThe embeddings were also employed to represent the exemplars in the EXPO-ES framework. During each iteration of\ninference, the LLM agent performed 1 prediction with a temperature setting of T = 0 to provide a stable solution and 7\nadditional predictions with a temperature setting of T = 1 to encourage exploration.\n\nNeural Network Parameters. For the EXPO, the input to the neural network consists of the concatenated embeddings of\nthe task description and meta-instruction, resulting in an input dimensionality of 3072 + 3072 = 6144. The neural network\nemploys a single hidden layer with a width of 1536 and produces a single scalar output. The training objective is to minimize\nthe Mean Squared Error (MSE) loss function.\n\nFor the EXPO-ES, the exemplar selection process differs depending on the iteration count. During the initial iterations,\nwhen fewer than 20 optimal historical records are available, we use all available exemplars. As the iteration count\nincreases, exemplars are selected from the top min(total exemplar records, 30) historical optimal records. From this pool,\n257 exemplars are constructed, consisting of 256 randomly selected exemplars and 1 heuristic exemplar generated from a\ncombination of 20 best historical records. The neural network for EXPO-ES operates on an input dimensionality of 3072,\ncorresponding to the embedding of a single exemplar. It employs a single hidden layer with a width of 512 and produces a\nsingle scalar output. The training objective is to minimize the Mean Squared Error (MSE) loss.\n\nEXP3 Learning Rate. In the EXPO , the learning rate parameter ηdesc is set to 100 for selecting task descriptions and\nmeta-instruction combinations. In the EXPO-ES, ηdesc is also set to 100 for selecting task descriptions and meta-instruction\ncombinations, while ηexemplar is set to 10 for selecting exemplars.\n\nB.2.5. ENHANCED OPRO\n\nHere, we describe how we have enhanced the original algorithm (Yang et al., 2024) by modifying its prompts.\n\nDuring initial experiments with the meta-prompts provided by the original OPRO algorithm (Yang et al., 2024) for task\ndescription rephrasing, we observed that the LLM often misinterprets the descending order semantics described in the\noriginal design. In tasks like TSP and Linear Regression, where better solutions correspond to lower evaluation scores,\ndescending order is intended to arrange solutions from high evaluation scores to low. However, the LLM frequently\nmisunderstands this as a descending order of solution quality, interpreting higher-ranked solutions as better and lower-ranked\nones as worse, which is contrary to the intended meaning.\n\nTo address this issue, we enhance the orginal meta-prompts by explicitly clarifying the semantics of descending order in the\ncontext of evaluation scores. This modification ensures that the LLM accurately understand the intended instructions. When\ntested with the enhanced prompts, the problem was resolved, and the LLM is able to consistently generate correct rephrased\ntask descriptions. For a clearer illustration, we provide below the original OPRO meta-prompt (Fig. 10) and our enhanced\nOPRO meta-prompt (Fig. 11).\n\n\n\n\n\n                                                14\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n   The task description in the original OPRO prompt\n\n   You are given a list of points with coordinates below: {POINTS}.\n   Below are some previous traces and their lengths. The traces are arranged in descending order based on their lengths, where\n    lower values are better.\n       ......\n\n\n\n                                  Figure 10. The task description in the original OPRO prompt.\n\n\n\n   The task description in our enhanced OPRO prompt\n\n   You are given a list of points with coordinates below: {POINTS}.\n   Below are some previous traces and their lengths. The traces are arranged in descending order based on their lengths, where\n    smaller lengths indicate better solutions. Therefore, the traces are listed from the largest length to the smallest, the trace with the\n    smallest length is considered the most optimal.\n       ......\n\n\n\n          Figure 11. The task description in the enhanced OPRO prompt. The texts we have modified are highlighted in red.\n\n\nB.3. More Details on the LLM-Based Multi-Armed Bandits Task (Sec. 4.2)\n\nB.3.1. EXPLANATION OF BSSCD AND BSSND\n\nWe provide a detailed explanation and demonstration of prompt designs for both BSSCD and BSSND (Krishnamurthy et al.,\n2024), highlighting their key components and structures. Figure 12 illustrates a complete example of a BSSCD prompt\ndesigned for the MAB problem under the hard difficulty setting. It showcases the structure and color-coded components of\nthe prompt in detail.\n\n\n   • Button scenario and Suggestive framing, providing the foundational task scenario, clarifying the role of the agent, and\n     framing the objective of the task in a suggestive manner to guide decision-making.\n\n   • Description of the multi-armed bandit problem, offering the agent a detailed task description, including comprehensive\n     information about the task’s objectives, constraints, and operational details.\n\n   • Summarized history, presenting a condensed version of historical decisions and reward feedback to the agent, instead\n     of providing step-by-step decision and reward feedback.\n\n   • Chain-of-thought or No CoT, indicating whether to encourage the agent to engage in step-by-step reasoning for\n     decision-making.\n\n   • Distribution over actions, encouraging the agent to generate a probability distribution over the arms of the bandit,\n     instead of making deterministic decisions.\n\nWhen we use our EXPO algorithm to optimize the task description and meta-instruction, the upper section with light purple\nbackground corresponds to the Task Description, where as the section below it with light blue background represents the\nMeta-Instruction. In other words, our EXPO algorithm is used to optimize the text in these two sections.\n\nB.3.2. TASK SETTING\n\nThe experiments are conducted for both the BSSND and BSSCD prompts under two pre-defined difficulty levels: hard\nand easy. For the hard setting, the MAB instance consists of K = 5 arms, where the best arm has a mean reward of\nµ⋆= 0.5 + ∆/2 with ∆= 0.2, and all other arms have a mean reward of µ = 0.5 −∆/2. For the easy setting, the MAB\ninstance consists of K = 4 arms with a larger gap ∆= 0.5 between the best arm and the suboptimal arm. We set the blue\nbutton as the optimal arm in experiments, corresponding to the arm with the highest expected reward. Each configuration\nis tested using two fixed random seeds, with experiments repeated 3 times for each seed, resulting in a total of 2 × 3 = 6\n\n                                                15\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n   The setting of the prompt in MAB\n\n   [SYSTEM]\n   You are a bandit algorithm in a room with 5 buttons labeled blue, green, red, yellow, purple. Each button is associated\n    with a Bernoulli distribution with a fixed but unknown mean; the means for the buttons could be different. For each\n    button, when you press it, you will get a reward that is sampled from the button’s associated distribution. You have\n   100 time steps and, on each time step, you can choose any button and receive the reward. Your goal is to maximize\n    the total reward over the 100 time steps.\n   At each time step, I will show you a summary of your past choices and rewards. Then you must make the next choice.\n   You may output a distribution over the 5 buttons formatted EXACTLY like ”blue:a,green:b,red:c,yellow:d,purple:e”.\n    Let’s think step by step to make sure we make a good choice.\n   You must provide your final answer within the tags <Answer>DIST<\\Answer> where DIST is the distribution in\n    the format specified above.\n\n   [USER]\n   So far you have played 5 times with your past choices and rewards summarized as follows:\n    blue button: pressed 2 times with average reward 0.5\n    green button: pressed 1 times with average reward 0.0\n    red button: pressed 1 times with average reward 1.0\n    yellow button: pressed 0 times\n    purple button: pressed 1 times with average reward 0.0\n   Which button will you choose next? Remember, YOU MUST provide your final answer within the tags <An-\n   swer>DIST<\\Answer> where DIST is formatted like ”blue:a,green:b,red:c,yellow:d,purple:e”.\n\n\n\nFigure 12. A complete example of the prompt in MAB. The different components in the prompt are explained in detail in App. B.3.1.\n\n\nruns per setting. Each experiment consists of 100 iterations, with the LLM-based agents making decisions and updating\nprompts iteratively to optimize performance. The work of Krishnamurthy et al. (2024) has reported that GPT-3.5 models\nencounter exploration failures in MAB tasks, making them unsuitable as agents for solving such problems. In contrast,\nGPT-4 demonstrates the capability to effectively handle the exploration-exploitation trade-off inherent in MAB settings.\nTherefore, we adopt GPT-4-turbo as the LLM agent for this experiment.\n\nB.3.3. EVALUATION METRIC.\n\nIn the LLM-based Multi-Armed Bandit (MAB) task (Sec. 4.2), the performance of the LLM agent is assessed using the\nCumulative Regret metric. At each iteration, the LLM agent outputs a probability distribution over the arms, representing\nthe likelihood of sampling each arm.\n\nFormally, let there be K arms, each associated with an expected reward µ1, µ2, . . . , µK, where µ∗= maxk∈{1,...,K} µk\ndenotes the expected reward of the optimal arm. At iteration t, we sample an arm at ∈{1, . . . , K}, which is determined by\nthe probability distribution provided by the LLM agent. The instantaneous regret for iteration t is then defined as:\n\n                                                         rt = µ∗−µat,\n\nwhere µat represents the expected reward of the selected arm at at iteration t.\n\nThe cumulative regret after T iterations is computed as:\n\n                                             T       T\n                             RT = X rt = X (µ∗−µat) .\n                                               t=1      t=1\n\nB.3.4. DESIGN OF PROMPT SCORE.\n\nThe score of the prompt is designed to quantify the expected reward of the LLM agent’s sampling strategy at each iteration. At\niteration t, the LLM agent outputs a sampling probability distribution {p1, p2, . . . , pK}, where pi represents the probability\n\n                                                16\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nof selecting arm i (i = 1, 2, . . . , K, with K being the total number of arms). Simultaneously, the historical records from the\nfirst (t −1) iterations allow us to compute an unbiased estimate of the Bernoulli reward parameter for each arm, ˆµi, based\non the observed rewards and sampling counts.\n\nFor arm i, the Bernoulli parameter ˆµi is estimated as:\n\n                       \n                                                       0,              if ni = 0,                       \n                                                              ˆµi =   Pt−1j=1 Ri,j\n                                                                                           ,   if ni > 0.                           ni\nwhere Pt−1j=1 Ri,j denotes the cumulative reward obtained from arm i during the first (t −1) iterations, and ni represents\nthe total number of times arm i was sampled during the same period.\nThe LLM agent’s expected reward ˆRexpected at iteration t is then calculated by weighting the estimated Bernoulli parameters\n{ˆµ1, ˆµ2, . . . , ˆµK} with the sampling probabilities {p1, p2, . . . , pK} provided by the LLM:\n\n                                     K\n                                                              ˆRexpected = X pi · ˆµi.\n                                                         i=1\n\nThis expected reward ˆRexpected serves as the score of the prompt.\n\nMotivation for the Score Design.  The design of the prompt score is driven by the objective of guiding the LLM agent to\nfavor arms with higher expected rewards, represented by µi. Since the true values of µi are not available, the prompt score\nis designed to estimate this quantity based on observed data. Specifically, the higher the value of µi, the higher the sampling\nprobability pi should be assigned to arm i, reflecting the optimal choice. Conversely, arms with lower values of µi should be\nassigned lower probabilities.\n\nThe original score with the Bernoulli parameters:\n\n                                      K\n                                                        Rexpected = X piµi.\n                                                          i=1\n\nIn the absence of the true µi, we rely on the unbiased estimates ˆµi:\n\n                                      K\n                                                               ˆRexpected = X piˆµi.\n                                                          i=1\n\nThis design is justified because, for most of iterations, the score PKi=1 piˆµi is an unbiased estimate of the true expected\nreward PKi=1 piµi, and we proceed to formally establish this unbiasedness.\n\nProof of Unbiasedness.  For iteration t, where ni > 0 for all i, we aim to show that the score PKi=1 piˆµi is an unbiased\nestimate of the true expected reward PKi=1 piµi. Since ˆµi is an unbiased estimate of µi, we have:\n                                E [ˆµi] = E [µi] ,\n\nThus, by the linearity of expectation, we obtain:\n\n                              K       K\n                           E              X piˆµi = X piE [ˆµi]                             \n                                              i=1         i=1\n\n                                      K\n                               = X piE [µi]\n                                                          i=1\n                                        K    \n                               = E X piµi                                            \n                                                              i=1\n\nThis shows that the score ˆRexpected is an unbiased estimate of the true expected reward Rexpected.\n\n                                                17\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nB.3.5. DETAILS ABOUT THE MODELS AND PARAMETERS IN OUR ALGORITHMS\n\nLLM Agents and Embedding Model. For the MAB tasks, the primary LLM agent is GPT-4-Turbo and the fixed inference\ntemperature is set to T = 0. For embedding generation, we employed OpenAI’s text-embedding-3-large model,\nwhich outputs embeddings with a dimensionality of 3072. These embeddings are utilized to represent the prompts provided\nto the LLM agent during the experiments. At each iteration, the LLM is prompted once using the designed prompt.\n\nNeural Network Parameters. For the EXPO, the input to the neural network consists of the concatenated embeddings of\nthe task description and meta-instruction, resulting in an input dimensionality of 3072 + 3072 = 6144. The neural network\nemploys a single hidden layer with a width of 1536 and produces a scalar output. The model is trained by minimizing the\nMean Squared Error (MSE) loss function.\n\nFor the EXPO-ES, the neural network is designed to process K exemplars, where K is determined by the total number of\navailable summaries. To ensure fairness, K distinct exemplar combinations are generated at each iteration using a cyclic\nrotation mechanism. This mechanism ensures that each summary occupies every possible position within the exemplar\nsequence. Formally, given K summaries indexed as {e0, e1, . . . , eK−1}, the i-th exemplar combination is defined as:\n\n                                           (ei, e(i+1)  mod K, . . . , e(i+K−1)  mod K).\n\nThis guarantees that each summary appears in every position across all K combinations.\n\nEach exemplar is embedded into a 3072-dimensional vector using the embedding model, and these embeddings are processed\nindividually by the neural network. The neural network consists of a single hidden layer with a width of 512 and produces a\nscalar output. Like EXPO, the training objective is to minimize the Mean Squared Error (MSE) loss function.\n\nEXP3 Learning Rate. For the EXPO, the learning rate parameter ηdesc is set to 10 for selecting task descriptions and\nmeta-instruction combinations. In the EXPO-ES, two learning rate parameters are used: ηdesc is set to 10 for selecting task\ndescription and meta-instruction combinations, and ηexemplar is set to 10 for selecting exemplars.\n\nB.4. Improving Numerical Stability\n\nTo prevent numerical overflow during the computation of exponentials in our algorithms, a translation constant C(t) is\nintroduced at each iteration t. This constant stabilizes the computation by shifting the cumulative scores, ensuring the\nalgorithm operates reliably until convergence without altering the resulting probability distribution. The translation constant\nis defined as:\n                                                 C(t) = max S(t)j   .                                                   (3)\n                                                                           j\n\nThe translated scores are:\n                                                                ˜S(t)i = S(t)i  −C(t).                                                  (4)\n\nThe probability distribution after translation is:\n                                              exp η ˜S(t)i                                                            ˜Pt[i] =                             .                                              (5)\n                              Pkj=1 exp η ˜S(t)j\n\nSubstituting ˜S(t)i = S(t)i  −C(t):\n                                          exp η S(t)i  −C(t)                                                     ˜Pt[i] =                                           .                                         (6)\n                           Pkj=1 exp η S(t)j  −C(t)\n\nUsing exp(a −b) = exp(a)exp(b) :\n                                         exp ηS(t)i  / exp ηC(t)                                                 ˜Pt[i] =                                                     .                                     (7)\n                         Pkj=1  exp ηS(t)j  / exp ηC(t)\n\nSimplifying:\n                                              exp ηS(t)i                                                            ˜Pt[i] =                             .                                              (8)\n                              Pkj=1 exp ηS(t)j\n\n                                                18\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\nThus, the probabilities remain unchanged:\n                                                                     ˜Pt[i] = Pt[i].                                                     (9)\n\nC. More Experimental Results\n\nC.1. Results of GPT-4-turbo for TSP\n\nFig. 13 shows a zoomed version of Fig. 7 (bottom right) in the main paper. It shows that when GPT-4-Turbo is used as the\nLLM, our EXPO is still able to significantly outperform OPRO.\n\n\n                                                        OPRO (Enhanced, GPT-4-turbo)                                 10                                                           EXPO (GPT-4-turbo)\n\n                                   (%)\n                                   Gap\n                                                                                                                    Optimality 5\n\n\n                                  0                                     0     20    40    60    80\n                                                                    Iteration\n\n\n                        Figure 13. Ablation study results using GPT-4-turbo in the TSP task with 20 nodes.\n\n\n\nC.2. Results of Other Variants of OPRO\n\nAs we have discussed in Sec. 3.1 and Sec. 4.1, the original OPRO uses a temperature of 1 to choose all 8 actions in a batch,\nwhile we have made a slight modification such that we choose the last action in the batch with a temperature of 0. Here\nwe show that this has a minimal impact on the performance of OPRO (Fig. 14). Specifically, in Fig. 14, the orange curves\nrepresent the original OPRO (using a temperature of 1 for all 8 actions) and the pink curves correspond to our modified\nversion. We have also compared the performances of the enhanced variants (see Sec. 4.1 for details) for both the original\n(purple) and modified OPRO (green). The results show that setting the temperature to 0 while selecting the last action\nhas negligible impact on the performance of OPRO. Importantly, our EXPO and EXPO-ES algorithms consistently and\ndramatically outperform all variants of OPRO.\n\nC.3. Impact of Adding Exemplar Embedding to the NN in EXPO\n\nRecall that in every iteration of our EXPO (Algo. 1), we need to train a neural network (NN) M(g(·); θ) to estimate the\nscores of the task descriptions and meta-instructions in the domain (line 8 of Algo. 1). Note that the training set used to\ntrain this NN is {( g(Di) ⊕g(Ii)  , si)}i=1,...,t+1 (line 7 of Algo. 1). However, it is also important to note that in our\nEXPO algorithm, the set of exemplars included in the meta-prompt E′t changes in every iteration and hence may also affect\nthe scores si’s. Therefore, one may naturally wonder whether including the embedding of E′t in the input of the NN can\nfurther improve the performance of the trained NN and, consequently, the performance of the overall EXPO. We conduct\nan ablation study to validate this hypothesis, and the results are shown in Fig. 15. The results demonstrate that including\nthe embedding of the exemplars in the input of the NN does not lead to better performance than our standard approach\nof excluding it (Algo. 1). This is likely due to the significantly increased dimensionality of the input to the NN, which\nmakes training the NN more challenging. Therefore, these results suggest that the benefit of additionally accounting for the\nchanging exemplars is outweighed by the drawback of the significantly increased dimensionality of the input to the NN.\n\n\nC.4. More Illustrations of the Discovered Task Description and Meta-instruction\n\nHere we provide more illustrations regarding the comparison of the original task description and meta-instruction adopted\nby the original LLM-based sequential decision-making algorithm (i.e., OPRO or the LLM-based MAB algorithm from\n\n                                                19\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n                            200                      OPRO (Original)                                            OPRO (Original)                                                                 300                      OPRO                                                                                                                                                                                                      (Original, Enhanced)                            175                      OPROOPRO (Original, Enhanced)                                                                                                                           OPRO\n                                                Error150                      OPROEXPO (Enhanced)                Error250                      OPROEXPO (Enhanced)                            125                         EXPO-ES               200                         EXPO-ES\n                            100                                                                 150\n                             75\n                                                                 100                                                                                               Regression 50                                                                                                                        Regression\n                             25                                   50\n                              0                                    0\n                                0    10   20   30   40   50          0    10   20   30   40   50\n                                                         Iteration                                          Iteration\n                               Linear Regression                 Linear Regression\n                          (w = 2, b = 30)               (w = 36, b = −1)\n\n                                                                                                                                                                                                                                            (Original)                                                                                    140                      OPRO\n                                                                                                                                                      OPRO                                                                                                                                                                                                                                                (Original, Enhanced)        35            OPRO (Original)              70               OPROOPRO (Original)(Original, Enhanced)\n                                                                                                                                                      OPRO                                                                        OPRO                   (%)120                                                                                                                                                      OPRO (Enhanced)       (%)30            OPROOPRO (Original, Enhanced)     (%)60\n                                                                                                                                                               EXPO\n                                                                                                                                                                             EXPO-ES       Gap25            OPRO (Enhanced)            Gap50               OPROEXPO (Enhanced)           Gap100\n        20            EXPO                     40                 EXPO-ES                   80                           EXPO-ES\n        15                                  30                                     60\n        10                                  20                                     40                                                                                                                                                                                                                                                                                                                Optimality                                                                                                                                                             Optimality                      Optimality                                            10         5                                                                                     20\n         0                                   0                                      0            0     25    50    75    100         0     50    100   150                 0   50  100 150 200 250\n                             Iteration                                        Iteration                                          Iteration\n               TSP                       TSP                       TSP\n                (10 Nodes)                       (15 Nodes)                        (20 Nodes)\n\n\n\n\nFigure 14. Results of different algorithms in the Linear Regression task and TSP task (Sec. 4.1). We have additionally included the\noriginal OPRO (which selects all 8 actions using a temperature of 1), as well as its enhanced variant. Lower is better.\n\nKrishnamurthy et al. (2024)) and those optimized by our EXPO algorithm. We include the comparisons for the TSP task\n(Fig. 16), and the two different prompt designs for the LLM-based MAB task in Sec. 4.2 (Fig. 17 and Fig. 18).\n\n\nC.5. More Results on the Ablation Study Regarding Comparison with the Stochastic MAB Algorithm of NeuralUCB\n\nHere we provide the additional ablation study results comparing the performance of our EXPO algorithm with the stochastic\nMAB algorithm of NeuralUCB, using the TSP task. The results are shown in Fig. 19, which, together with Fig. 6,\ndemonstrate that our EXPO algorithm based on adversarial bandits significantly and consistently outperforms the stochastic\nMAB method of NeuralUCB.\n\n\n\n\n\n                                                20\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n\n\n\n\n                                       (With                                         exemplars                                                embedding)                                       300               EXPO                                                                                                             (With                                                                                                         exemplars                                                                                                              embedding)                                                                                                              300              EXPO-ES                                                                                                                                                                                                                                                         (With                                                                                                                                                                                                                                          exemplars                                                                                                                                                                                                                                         embedding)    175               EXPO\n                          EXPO                                       (Without                                            exemplars                                                   embedding)                                                                                 EXPO                                                                                                            (Without                                                                                                            exemplars                                                                                                                 embedding)     175              EXPO-ESEXPO-ES (With(Withoutexemplarsexemplarsembedding)embedding)                                                                                                                                                                                                             EXPO-ES                                                                                                                                                                                                                                                       (Without                                                                                                                                                                                                                                             exemplars\n    150                                250                                150                                250                                       embedding)\n   Error125                                                        Error200                                                        Error125                                                        Error200\n    100                                                                          100                                150                                       150\n     75                                                                    75                                                                                                              100                                       100      Regression 50                                                                                                               Regression                                                                                                                                                                                                                                                                              Regression 50                                                                                                               Regression\n     25                                 50                                 25                                 50\n      0                                                                     0                                  0\n        0    10   20   30   40   50        0    10   20   30   40   50        0    10   20   30   40   50        0    10   20   30   40   50\n                         Iteration                                      Iteration                                      Iteration                                      Iteration\n\n                                                                                                                                                                                                                    (With                                                                                                                                                                                                        exemplars                                                                                                                                                                                                         embedding)                     40               EXPOEXPO (With(Withoutexemplarsexemplarsembedding)embedding)    70               EXPOEXPO (With(Withoutexemplarsexemplarsembedding)embedding)    140               EXPOEXPO                                                                                                                                                                                                                  (Without                                                                                                                                                                                                           exemplars\n                    (%)35                                 (%)60                                  (%)120                                       embedding)                     30\n                    Gap25                                 Gap50                                  Gap100                                                       40                                  80                     20\n                     15                                30                                  60\n                     10                                20                                  40                                                                    Optimality                      5                                                                                                                                                                                                   Optimality10                                                                                                                  Optimality 20\n                      0                                 0\n                        0     25    50    75             0     50    100   150              0   50  100 150 200 250\n                                              Iteration                                     Iteration                                       Iteration\n\nFigure 15. Convergence curves of our EXPO with and without exemplars embedding across different tasks: Linear Regression (top row)\nand TSP with 10, 15, and 20 nodes (bottom row).\n\n\n\n\n\n             OPRO                                    EXPO\n  You are given a list of points with coordinates     You are provided with a dataset containing a list of coordinates labeled as\n   below: {POINTS}.                           {POINTS}.\n   Below are some previous traces and their lengths.     The dataset also includes a series of previously calculated routes, with\n  The traces are arranged in descending order      associated lengths that are ordered from longest to shortest. However, it’s\n   based on their lengths, where lower values are      key to note that shorter routes are more desirable. Despite the presentation\n    better.                                                 order, understand that the optimal route is identified by the smallest total\n                                                           length.\n\n  {EXEMPLARS}                        {EXEMPLARS}\n\n   Give me a new trace that  is different from      Provide a unique  trace  that  is  distinct from any  previous  traces\n    all traces above, and has a length lower than     and shorter in length.  Ensure that this trace visits each point exactly\n   any of the above. The trace should traverse all      once and adhere to the specified format by starting with <trace> and\n   points exactly once. The trace should start with      concluding with </trace>.\n  <trace> and end with </trace>.\n\nFigure 16. The task description (top) and meta-instruction (bottom) used by OPRO (left) and optimized by our EXPO (right) in a TSP\ntask.\n\n\n\n\n\n                                                21\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n\n\n\n             BSSND                                    EXPO\n  You are a bandit algorithm in a room with 5     You are presented as a bandit algorithm, located in an environment\n   buttons labeled blue, green, red, yellow, purple.      offering five distinct buttons, each emblazoned with colors such as\n  Each button  is associated with a Bernoulli       blue, green, red, yellow, and purple. Each button acts a vessel tied to\n   distribution with a fixed but unknown mean;      a non-variable yet undisclosed Bernoulli distribution mean which isn’t\n   the means for the buttons could be different.      subjected to be uniformly distributed across buttons. In this mechanism,\n   For each button, when you press  it, you will      every button acts as a yielder of a capricious reward, constructed from the\n   get a reward that is sampled from the button’s      associated distribution of the respective button. With access to total life\n   associated distribution. You have 100 time steps      -encompassing around 100 temporal stages - your voluntary element grants\n   and, on each time step, you can choose any     you control towards opting the button insertion at each such progressive\n   button and receive the reward. Your goal is to      phase. Precisely summoning your approach could perpetually provide you\n   maximize the total reward over the 100 time      with a regulatory provision , the aptitude - is flexibly dwelling within its\n   steps.                                              underlining motive- aiming at optimizing total accumulated cashbacks\n                                                     during several phases of these 100 spatial temporalities.\n\n\n  At each time step,  I will show you a sum-     During every step of the process, a recap highlighting your previ-\n   mary of your past choices and rewards. Then you      ous selections and the prizes received will be presented to you. Then, it’ll\n   must make the next choice. You may output a dis-     now be incumbent upon you to proceed with the new decision-making pro-\n   tribution over the 5 buttons formatted EXACTLY       cess. For your ease, a well-structured distribution comprising five buttons in\n   like ”blue:a,green:b,red:c,yellow:d,purple:e”.          assorted colours such as ”blue”, ”green,”, ”red”, ”yellow”, and ”purple” will\n                                                be exhibited before you. Make sure to structure your output accordingly;\n                                                                 this might look something akin to ”blue:a,green:b,red:c,yellow:d,purple:e”.\n\n\nFigure 17. The suggestive framing (corresponding to the task description) and MAB problem description (corresponding to the meta-\ninstruction) used by BSSND hard (left) and optimized by our EXPO (right) in an LLM-based MAB task.\n\n\n\n\n\n              BSSCD                                    EXPO\n  You are a bandit algorithm in a room with 5 buttons     You are an algorithm designed to function as a bandit, positioned within\n   labeled blue, green, red, yellow, purple. Each button      an environment that features five distinct buttons, each colored blue,\n    is associated with a Bernoulli distribution with a       green, red, yellow, and purple. These buttons are intricately connected to\n   fixed but unknown mean; the means for the buttons       individual Bernoulli distributions which possess unique and undisclosed\n   could be different. For each button, when you press     mean probabilities. When a button is pressed,  it delivers a reward\n     it, you will get a reward that is sampled from the      based on its specific distribution. Granted with 100 opportunities to act,\n   button’s associated distribution.  You have 100      your objective is to strategically press these buttons in a manner that\n   time steps and, on each time step, you can choose      optimizes the accrued total reward throughout these attempts. Make\n   any button and receive the reward. Your goal is      your selections wisely to maximize the gains from this stochastic setup.\n   to maximize the total reward over the 100 time steps.\n\n  At each time  step,  I  will show you a sum-      In each phase, a concise recap of your previous decisions and\n  mary of your past choices and rewards. Then you      received rewards will be presented. Your task is to make a subsequent\n   must make the next choice.  You may output a      choice based on this data. It is essential to output your selection in an\n   distribution over the 5 buttons formatted EXACTLY      exact format defined as ”blue:a, green:b, red:c, yellow:d, purple:e”,\n   like ”blue:a,green:b,red:c,yellow:d,purple:e”. Let’s      where ’a’, ’b’, ’c’, ’d’, ’e’ represent specific z-score values for each color\n   think step by step to make sure we make a good      accompanied by the decision choice letter(s). The process is designed to\n   choice.                                                     refine our strategy progressively with each move, ensuring an informed\n                                                  and impactful outcome.\n\n\nFigure 18. The suggestive framing (corresponding to the task description) and MAB problem description (corresponding to the meta-\ninstruction) used by BSSCD hard (left) and optimized by our EXPO (right) in an LLM-based MAB task.\n\n\n\n\n\n                                                22\n\nMeta-prompt optimization for LLM-based Sequential Decision Making\n\n\n\n\n\n    40                             Neural UCB      70                             Neural UCB      140                             Neural UCB\n    35                        EXPO                                                                                                                      EXPO (%)                                         (%)60                        EXPO         (%)120    30                                       50                                   100 Gap25                                   Gap                                      Gap                                       40                                    80    20\n                                       30                                    60    15\n    10                                  20                                    40    Optimality                                                                                                                                          Optimality                                       10                                                                                                                        Optimality     5                                                                              20\n     0                                   0\n       0     25    50    75               0     50    100   150                0   50  100 150 200 250\n                       Iteration                                       Iteration                                         Iteration\n               TSP                              TSP                              TSP\n                 (10 Nodes)                                 (15 Nodes)                                 (20 Nodes)\n\nFigure 19. Comparison of our EXPO with NeuralUCB (i.e., a representative stochastic MAB algorithm) in the TSP tasks.\n\n\n\n\n\n                                                23",
"headers": [
"arXiv:2502.00728v1  [cs.LG]  2 Feb 2025",
"Meta-Prompt Optimization for LLM-Based Sequential Decision Making",
"Abstract",
"1. Introduction",
"2. Problem Setting",
"3. Algorithms",
"4. Experiments",
"5. Ablation Study",
"6. Related Work",
"7. Conclusion",
"Impact Statements",
"References",
"A. Our",
"EXPO-ES",
"Algorithm to Additionally Optimize the Exemplar Sequences",
"B. More Details on Our Experimental Settings",
"C. More Experimental Results"
],
"tables": [
"|100 n|200 n|Col3|20 G|Col5|40 G|Col7|80 G|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n|0<br>10<br>20<br>30<br>40<br>50<br>Iteration<br>0<br>25<br>50<br>75<br>100<br>|0<br>10<br>20<br>30<br>40<br>50<br>Iteration<br>0<br>50<br>100<br>150<br>Regressio||0<br>25<br>50<br>75<br>100<br>Iteration<br>0<br>5<br>10<br>15<br>20<br>Optimality||0<br>50<br>100<br>150<br>Iteration<br>0<br>10<br>20<br>30<br><br>Optimality||0<br>50<br>100<br>150<br>200<br>250<br>Iteration<br>0<br>20<br>40<br>60<br>80<br>Optimality||||\n|0<br>10<br>20<br>30<br>40<br>50<br>Iteration<br>0<br>25<br>50<br>75<br>100<br>|0<br>10<br>20<br>30<br>40<br>50<br>Iteration<br>0<br>50<br>100<br>150<br>Regressio||0<br>25<br>50<br>75<br>100<br>Iteration<br>0<br>5<br>10<br>15<br>20<br>Optimality||0<br>50<br>100<br>150<br>Iteration<br>0<br>10<br>20<br>30<br><br>Optimality||0<br>50<br>100<br>150<br>200<br>250<br>Iteration<br>0<br>20<br>40<br>60<br>80<br>Optimality||0<br>200<br>250<br>ion|0<br>200<br>250<br>ion|",
"|OPRO<br>will help me minimize a function with<br>variables w, b. I have some (w, b)<br>the function values at those points.<br>are arranged in descending order based<br>nction values, where lower values are<br>LARS}<br>a new (w, b) pair that is differ-<br>ll pairs above, and has a function value<br>n any of the above. Do not write code.<br>t must end with a pair [w, b], where<br>re numerical values.|EXPO<br>We will collaborate to optimize a function involving two parameters, \\(w\\) and \\(b\\). I possess a set of<br>data points, each consisting of \\((w, b)\\) pairs and their corresponding function values. These pairs are<br>systematically organized in reverse order, starting from the greatest to the smallest function values. Essentially,<br>the lower the function value, the more optimal or preferable the pair. Consequently, our goal is to identify<br>and analyze the \\((w, b)\\) pair that manifests the lowest function value, as this represents the perspective of<br>optimum efficacy.<br>{EXEMPLARS}<br>To enhance the quality and expand on the existing instructions, follow these improved guidelines<br>vis-a`-vis designing a new and distinctive numerical pair: ensure the selected (w, b) combination diverges from<br>prior examples and secures a function output lower than preceding values. Key details on methodology or<br>calculations are not required—just ensure clarity in presenting a returned value that closes with the specific<br>format [w, b], where both w and b are distinct numerical figures.|\n|---|---|",
"|ES|Col2|10 EXPO Regret<br>EXPO-<br>8<br>Cumulative<br>6<br>4<br>2<br>0|ES|\n|---|---|---|---|",
"|200 Er<br>Regression<br>150<br>100<br>50<br>0<br>0|10 20 30 40 5|\n|---|---|",
"|125 Er<br>100 Regression<br>75<br>50<br>25<br>0<br>0|10 20 30 40|\n|---|---|",
"|10|Col2|\n|---|---|\n|0<br>20<br>0<br>2<br>4<br>6<br>8<br>10<br>Cumulative Regret<br>BSSC<br>EXPO<br>EXPO~~-~~|D<br>ES|\n|0<br>20<br>0<br>2<br>4<br>6<br>8<br>10<br>Cumulative Regret<br>BSSC<br>EXPO<br>EXPO~~-~~|40<br>60<br>80<br>1<br>Iteration|",
"|BSSND<br>You are a bandit algorithm in a room with 5<br>buttons labeled blue, green, red, yellow, purple.<br>Each button is associated with a Bernoulli<br>distribution with a fixed but unknown mean;<br>the means for the buttons could be different.<br>For each button, when you press it, you will<br>get a reward that is sampled from the button’s<br>associated distribution. You have 100 time steps<br>and, on each time step, you can choose any<br>button and receive the reward. Your goal is to<br>maximize the total reward over the 100 time<br>steps.<br>At each time step, I will show you a sum-<br>mary of your past choices and rewards. Then you<br>must make the next choice. You may output a dis-<br>tribution over the 5 buttons formatted EXACTLY<br>like ”blue:a,green:b,red:c,yellow:d,purple:e”.|Col2|EXPO<br>You are presented as a bandit algorithm, located in an environment<br>offering fvie distinct buttons, each emblazoned with colors such as<br>blue, green, red, yellow, and purple. Each button acts a vessel tied to<br>a non-variable yet undisclosed Bernoulli distribution mean which isn’t<br>subjected to be uniformly distributed across buttons. In this mechanism,<br>every button acts as a yielder of a capricious reward, constructed from the<br>associated distribution of the respective button. With access to total life<br>-encompassing around 100 temporal stages - your voluntary element grants<br>you control towards opting the button insertion at each such progressive<br>phase. Precisely summoning your approach could perpetually provide you<br>with a regulatory provision , the aptitude - is flexibly dwelling within its<br>underlining motive- aiming at optimizing total accumulated cashbacks<br>during several phases of these 100 spatial temporalities.<br>During every step of the process, a recap highlighting your previ-<br>ous selections and the prizes received will be presented to you. Then, it’ll<br>now be incumbent upon you to proceed with the new decision-making pro-<br>cess. For your ease, a well-structured distribution comprising fvie buttons in<br>assorted colours such as ”blue”, ”green,”, ”red”, ”yellow”, and ”purple” will<br>be exhibited before you. Make sure to structure your output accordingly;<br>this might look something akin to ”blue:a,green:b,red:c,yellow:d,purple:e”.|\n|---|---|---|",
"|BSSCD<br>You are a bandit algorithm in a room with 5 buttons<br>labeled blue, green, red, yellow, purple. Each button<br>is associated with a Bernoulli distribution with a<br>fixed but unknown mean; the means for the buttons<br>could be different. For each button, when you press<br>it, you will get a reward that is sampled from the<br>button’s associated distribution. You have 100<br>time steps and, on each time step, you can choose<br>any button and receive the reward. Your goal is<br>to maximize the total reward over the 100 time steps.<br>At each time step, I will show you a sum-<br>mary of your past choices and rewards. Then you<br>must make the next choice. You may output a<br>distribution over the 5 buttons formatted EXACTLY<br>like ”blue:a,green:b,red:c,yellow:d,purple:e”. Let’s<br>think step by step to make sure we make a good<br>choice.|Col2|EXPO<br>You are an algorithm designed to function as a bandit, positioned within<br>an environment that features fvie distinct buttons, each colored blue,<br>green, red, yellow, and purple. These buttons are intricately connected to<br>individual Bernoulli distributions which possess unique and undisclosed<br>mean probabilities. When a button is pressed, it delivers a reward<br>based on its specific distribution. Granted with 100 opportunities to act,<br>your objective is to strategically press these buttons in a manner that<br>optimizes the accrued total reward throughout these attempts. Make<br>your selections wisely to maximize the gains from this stochastic setup.<br>In each phase, a concise recap of your previous decisions and<br>received rewards will be presented. Your task is to make a subsequent<br>choice based on this data. It is essential to output your selection in an<br>exact format defined as ”blue:a, green:b, red:c, yellow:d, purple:e”,<br>where ’a’, ’b’, ’c’, ’d’, ’e’ represent specific z-score values for each color<br>accompanied by the decision choice letter(s). The process is designed to<br>refine our strategy progressively with each move, ensuring an informed<br>and impactful outcome.|\n|---|---|---|",
"|40<br>35 (%)<br>30<br>Gap<br>25<br>20 ptimality<br>15<br>10|Neural UCB<br>EXPO|\n|---|---|\n|0<br>25<br>50<br>75<br>Iteration<br>0<br>5<br>O|0<br>25<br>50<br>75<br>Iteration<br>0<br>5<br>O|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2502.00728v1.pdf"
}