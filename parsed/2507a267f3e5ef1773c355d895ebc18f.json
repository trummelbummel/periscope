{
"text": "Learning from Prompt itself: the Hierarchical Attribution Prompt\n                                 Optimization\n\n        Dongyu Chen, Jian Ma, Xianpeng Zhang, Lei Zhang, Haonan Lu, Chen Chen,\n                           Chuangchuang Wang and Kai Tang\n\n\n\n\n\n                          Abstract                       modern productivity.  Traditional optimization, how-\n                                                                           ever, is grounded in continuous mathematical processes,\n            Optimization  is fundamental across numerous disci-        whereas prompt refinement operates in a fundamentally\n              plines, typically following an iterative process of re-                                                                          discrete semantic space, necessitating tailored mecha-\n              fining an  initial  solution  to enhance performance.2026                                                              nisms.\n            This  principle  is  equally  critical  in prompt  engi-\n             neering, where designing effective prompts for large          Current automated prompt optimization strategies\n            language models constitutes a complex optimization        often address this challenge by generating new promptsJan              challenge.  A structured optimization approach  re-        or applying edits based on performance feedback. How-\n6       quires automated or semi-automated procedures to de-         ever, these methods  suffer from several limitations.\n             velop improved prompts, thereby reducing manual ef-         First, they frequently induce prompt drift, where iter-\n               fort, improving performance, and yielding an inter-         ative refinements fix prior failures but degrade perfor-\n             pretable process. However, current prompt optimiza-       mance on tasks the prompt previously handled success-\n             tion methods often induce prompt drift, where new                                                                                       fully. In addition, generating prompts from scratch can\n           prompts fix prior failures but impair performance on\n                                                            compromise interpretability, obscuring the rationale be-\n             previously successful tasks.  Additionally, generating\n                                                                hind the changes, and making the optimization process[cs.AI]      prompts from scratch can compromise interpretabil-\n                 ity.  To address these limitations,  this study pro-       a black box. These issues highlight a gap in approaches\n            poses the Hierarchical Attribution Prompt Optimiza-        that can refine prompts in a controlled, transparent,\n             tion (HAPO) framework, which introduces three in-       and stable manner.\n             novations: (1) a dynamic attribution mechanism tar-        To address these limitations, we propose the Hi-\n             geting error patterns in training data and prompt-         erarchical Attribution Prompt Optimization (HAPO)\n             ing history, (2) semantic-unit optimization for edit-       framework ‚Äì a novel, dynamic attribution mechanism\n             ing functional prompt segments, and (3) multimodal-                                                                              for prompt optimization. Unlike prior approaches that\n              friendly progression supporting both end-to-end LLM\n                                                                              statically correlate performance with benchmark scores\n           and LLM-MLLM workflows. Applied in contexts like\n                                                                     or case-feedback, our method dynamically attributes in-             single/multi-image QA  (e.g., OCRV2) and complex\n             task analysis  (e.g., BBH), HAPO demonstrates en-         fluence based on the prompt‚Äôs own semantic features\n           hanced optimization efficiency, outperforming compa-       and its iterative performance history. By integrating\n             rable automated prompt optimization methods and es-        these attributes as dynamic variables within a gradient-\n             tablishing an extensible paradigm for scalable prompt        influenced framework, and supplementing them with\n             engineering.                                              task-expectation grading to approximate loss, we en-\n                                                                      able a more nuanced and efficient path to better promptarXiv:2601.02683v1                                                                  design.\n                    Introduction                                                  Dynamic Attribution Optimization.  This pa-\n        As a result of the rapid boost of modern technology,      per presents a novel dynamic attribution mechanism\n         the frequency usage of language models can be viewed       for prompt optimization. Unlike prior approaches that\n          as a criterion of high efficiency and convenience([14],       statically correlate performance with benchmark scores\n             [9], [30]). However, their generalized functionality of-      or case-feedback, our method dynamically attributes in-\n         ten remains inaccessible to nonexpert users, as effec-      fluence based on the prompt‚Äôs own semantic features\n           tive interaction typically requires specialized knowl-     and its iterative performance history. By integrating\n          edge. Prompt engineering is essential for unlocking the      these attributes within a gradient-influenced framework\n         advanced capabilities of large language models (LLMs)     and employing task-expectation grading to approxi-\n           for non-expert users, serving as a critical bridge between     mate loss, our method enables a nuanced and efficient\n       human intent and model performance.  Consequently,      optimization path that mitigates prompt drift.\n         the development of methods to automatically and ef-       Semantic-unit   Hierarchical  Segmentation.\n           ficiently optimize prompts is paramount to enhancing      Also, compared to peer works, our method put more\n\nweight at the modification on the discrete semantic      high-quality prompts.\nspace, enabling targeted, interpretable edits to func-\ntional prompt segments.  To  follow the procedure    Prompt quality distinguishing and refining\nof prompting complex tasks, we designed a process    through natural language instructions\nto change the prompt hierarchically, which is also a    A recent line of research explores methodologies that\nsimulation of the learning rate in machine learning. We      leverage natural language feedback within prompts to\nalso applied the Upper Confidence Bound algorithm     enhance LLM performance, demonstrating  effective-\n(UCB) [12] to optimize the location and tendency of      ness in mitigating weakness among prompting proce-\nmodification for long prompts.                              dure.  The authors of StraGo [23] joined both the\n  Generalized Multimodal Adaptation.  In addi-     good and bad cases to summarize the pro/cons through\ntion, with the aim of generalization in a modern envi-     a self-trained LLM; other methodologies include task\nronment, we attempted to deploy this mechanism into      reasoning (PromptWizard[1]), mutated word replace-\na wider range of multimodal tasks and models. We     ment (EvoPrompt [11]), task referencing (TAPO[18]),\nmanaged to apply such a strategy on tasks involving     and graph optimization adapted for domain-knowledge\ntext‚Üîimage, image‚Üîtext, etc. And our method also     (EGO-Prompt[28]).  In particular, TAPO applied a\nachieves an obvious improvement in these tasks and      task-aware evaluation strategy that connects output\nbenchmarks.                                          words towards task requirement scoring and optimiza-\n  In brief, our method demonstrates compelling SoTA      tion reasoning. This could lead to a more detailed attri-\nefficacy, achieving advanced performance in 11/12 of      bution process, but lacks generalizability in that such\nthe evaluated scenarios while delivering a consistent av-      word-level evaluation may only be meaningful in text\nerage accuracy advantage of +7.21% over the common       tasks, rather than in thinking or other complex tasks.\nbaseline. This robust and generalized superiority across        In contrast to these approaches, our method imple-\ndiverse reasoning and multimodal benchmarks conclu-     ments a hierarchical framework that fully leverages the\nsively validates it as a highly effective, model-agnostic      attention mechanism to consistently capture instruc-\nframework for instruction optimization.                       tions.  Furthermore, through iterative refinement to-\n                                                     wards prompts, our approach maintains proximity to\n            Related work                         viable candidate responses.\nAutomatic Prompt Optimization for                                  MLLM‚Äôs instruction-following capability\nvarious tasks\n                                               To address deficiencies in instruction-following capa-\nRecent prior work has developed automated methods        bilities within MLLMs, several prior studies have de-\nfor optimizing task-specific prompts to address the lim-      veloped  novel  methodologies.   For example, some\nitations of manual prompt engineering, such as APE     works improved this capability by implementing visual-\n[29], which generates candidates via forward/reverse      modality token compression and cross-modality atten-\nLLM sampling, selects high-scoring prompts and  it-      tion inhibition to mitigate image redundancy ([26]),\neratively resamples using Monte Carlo search; APO      while other approaches have incorporated image-based\n[21], using textual ‚Äúgradients‚Äù from error analysis and     prompting skills and optimization([3],[17]). In addition,\nbandit selection for efficient refinement; and OPRO       [19] attempted to implement specific image consistency\n[25], employing metaprompts to guide LLMs in gen-      metrics that focus only on the instruction compliance\nerating iterative improvements. There are also evolu-      capacity of the image generator model.\ntionary approaches such as PromptBreeder [7] and Evo-        In contrast, our method introduces a generalized\nPrompt [11], evolving prompts via genetic algorithms;      strategy that circumvents the limitations  of visual-\nand frameworks like DSPy [15], TextGrad [27], and Au-      information loss and narrow task specialization. This\ntomatic Prompt Engineering for Long Prompts [11],      linguistically-grounded approach ensures robust perfor-\nwhich treat prompts as differentiable parameters for     mance across diverse task modules by addressing the\nbatched optimization. These methods consistently out-      core of the instruction-following problem, all while rig-\nperform manual engineering (e.g., +4‚Äì60% on bench-      orously preserving the original structure and fidelity of\nmarks like GSM8K and TruthfulQA) but primarily tar-      the input image.\nget short prompts in constrained settings, leaving com-\nplex, multi-constraint real-world applications underex-                Method\nplored. Evaluation typically relies on task-specific met-\n                                        Problem Formulation\nrics (accuracy, F1) or LLM-based self-assessment.\n                                                       Let D = {(xi, y‚àói )}Ni=1 be the task dataset with N sam-  Compared to their approaches, our optimization pro-\n                                                                    ples, where xi represents input instances and y‚àói repre-cess hierarchically incorporates prompt outcome scores,\n                                                              sents the desired outputs.weakness locations in complex prompt body, and corre-\n                                                The objective of prompt optimization is to find thesponding optimizing suggestion; and by plugging in the\n                                                     improved prompt p‚àóthat minimizes the expected loss:meta-prompt, this approach enables the LLM-MLLM\noptimizer to follow a more step-like gradient descent\n                                             p‚àó= arg min                                                                               E(x,y‚àó)‚àºD [L(f(p, x), y‚àó)] ,     (1)process, resulting in clustering of common patterns for                                                                   p‚ààP\n\nwhere:                                         Task Result Generation. We will use the training-\n                                                                   free LLMs experimented before to produce the task re-\n ‚Ä¢ P is the space of all possible prompts\n                                                                  sults f(p, x) given prompt p and input x.  To avoid\n ‚Ä¢ f(p, x) is the LLM/MLLM response given prompt p       irrelevant influences, the hyperparameters remain de-\n  and input x                                                    fault: temperature:  1.0, Topp:  1.0, Presence Penalty\n ‚Ä¢ L(¬∑, ¬∑) is the loss function that measures the discrep-     and Frequency Penalty: 0.0.\n   ancy between generated and desired outputs\n                                               Semantic Text Segmentation. We segment p into\n  Analogous to gradient descent in optimization, our      semantically coherent units using a two-stage proce-\nprocess iteratively refines the prompt through gradient-      dure: (i) rule-based splitting by discourse markers, sec-\nbased updates in the discrete linguistic space:               tion headers, list items, and delimiters; and (ii) model-\n                                                                assisted refinement with a frozen instruction parser Œ†\n              pt+1 = pt ‚àíŒ∑ ¬∑ ‚àálingp J (pt),            (2)      that merges overly short fragments and splits run-on\n                                                                 clauses. Formally, S(p) = {uk}Kk=1 = Œ†(RuleSplit(p)).where:\n                                          Dynamic  Attribution Mechanism.  Let  Et ‚äÇ ‚Ä¢ pt is the prompt at iteration t\n                                                                 Dtrain denote the mispredicted examples in iteration t.\n ‚Ä¢ Œ∑ is the learning rate in the linguistic space                                      We estimate per-unit contribution scores by counterfac-\n ‚Ä¢ ‚àálingp   represents the linguistic gradient operator          tual occlusion with exponential smoothing:\n ‚Ä¢ J (p) = N1 PNi=1 L(f(p, xi), y‚àói ) is the empirical risk\n                                                                            1\n                            X  h L M(x; p \\ uk), y‚àó  The linguistic gradient is computed through prompt        s(t)k = Œªs(t‚àí1)k  + (1 ‚àíŒª) ¬∑                                                                                                         |Et|analysis  and  response  evaluation  on  the  task-                                        (x,y‚àó)‚ààEt\nexpectation grading:                                                                                                           i                                                           ‚àíL M(x; p), y‚àó    ,\n                        ‚àÇL  ‚àÇf                                                                         (4)\n          ‚àálingp J (p) = E(x,y‚àó)‚àºD         ¬∑        ,       (3)                            ‚àÇf  ‚àÇp                where L is a surrogate loss (e.g., 0‚Äì1 loss) and p \\ uk\n                                                  masks uk with a neutral arm. We augment with history-       ‚àÇf\nwhere ‚àÇp represents the functional derivative of prompt     aware decay:\nperformance with respect to the LLM expectation rat-\n         ‚àÇf                                                                      1\ning, and ‚àÇp represents that of the LLM response with               Àús(t)k = Œ±ts(t)k + (1 ‚àíŒ±t) ¬∑  X  ‚àÜŒ≥t‚àíœÑ,   (5)\nrespect to prompt variations.                                                       |Hk| (œÑ,‚àÜ)‚ààHk\n  On a linguistic scale, this derivative could be ex-\nplained as an attribution analysis of the impact exerted     where Hk stores past improvements attributed to edits\nby the prompt on the LLM output. The attribution     on uk and Œ≥ ‚àà(0, 1) is a temporal decay. Top-m units\nprocess links the model‚Äôs loss to specific prompt com-     by Àús(t)k  form the actionable set.\nponents, which directly enables a hierarchical optimiza-\ntion strategy:  high-attribution elements are modified     Selector Phase\nfirst (e.g., task structure), followed by fine-grained re-\n                                                       This phase runs a UCB process for the selection of thefinements (e.g., word choice), guiding a targeted search\n                                                     improved feedback package.for a better prompt. The generalized workflow is shown\nin Figure 1; the pseudocode is Algorithm 1.                                          UCB-based Edit Selection. We model each arm\n                                                            as an edit candidate a = (k, o) over actionable units.Initialization Phase.  The process begins with the\n                                                       Executing a yields an updated prompt p‚Ä≤ and a scalarinitialization of a meta-prompt, into which task require-\n                                                      reward on a holdout dev split:ments are directly embedded. These requirements may\nbe professionally refined by human experts, transform-                                                                        rt(a) = Acc(p‚Ä≤; Ddev) ‚àíAcc(p; Ddev).      (6)\ning preliminary rough project descriptions into formats\nmore amenable to comprehension by specific large-scale    We maintain empirical means ÀÜ¬µa and counts na.  In\nmodels. This step ensures clarity and alignment with       iteration t, we choose\nmodel capabilities. We also randomly selected a very\nsmall potion from the benchmark dataset as the train                s                                                                                                      ln t\nset Dtrain.                                                                at ‚ààarg max ÀÜ¬µa + c                      (7)                                                                                  a          max(1, na).\nAttributor Phase                                              The procedure employs a warm-start initialization by\nThis phase deals with segmentation and the hierarchical      pulling each arm once, followed by the elimination of\nattribution process of the prompt performance, gener-     arms whose rewards are non-positive. Given the well-\nating feedback packages for the top m segments. We      separated reward distributions of the arms, a maximum\nset m = 4 here.                                                iteration count tmax = 100 is sufficient.\n\nOutput\n\n                                               Segmented                                                                                                                                        Prompt             ü§ñ   Sample            Input & ü§ñ                                                                                                   Best                                                                                                  Opt  ü§ñ                                                  Prompt                                     Attributor                                                                                    Selector                                                                                                                                  Optimizer              Repository         Initializer                                                                                             Package\n                                                    Opt-feedback\n                                                 Packages\n\n  ü§ñ            ü§ñ\n                                  Ensure all open parentheses,                                 UCB Algorithm\n                               brackets, and braces are systematically and\n                                   accurately matched and closed. ‚Ä¶\n\n\n                              Semantic Text               Execute Prompt                                                                                                   Best opt-feedback package\n                              Segmentation                       for f(p,x)                                     Package 1   Package 2    Package 3   Package 4             Where: Ensure all open\n                                                                                                                                                                                                 parentheses, brackets, and\n                                                                                                                                                                     braces‚Ä¶\n                                                                                                                                                                   Why:                                                                                                                                                                            The                                                                                                                                                                                   head                                                                                                                                                                                                                                  part needs to                                                  ......Ensureareare                                  systematicallysystematicallyall open                                                                                                                                                                          be                                                                                                                                                                     more                                                                                                                                                                                                                                         specific.\n                      andand                                                                                                                                                                   How:                                                                                                                                                                                                                    Highlight                                                                                                                                                                                                                      the                                   matchedmatched         ErrorErrorErrorCasesCasesCases                               accuratelyparentheses,accurately\n                           brackets,andandandclosedclosedbraces ‚Ä¶                                                                                                                               completion of the sequence.\n\n\n                                              Attribution\n\n\n\n                                  Opt-feedback Packages\n\n                                     Package 1\n                 Where 1: Ensure all open parentheses, brackets, and braces‚Ä¶                           Selector: UCB for Best Opt Package\n               Why 1: The head part needs to be more specific.\n                                                                                                                                                                      Segment                                                                                                                                                                                                               Optimization               How 1: Highlight the completion of the sequence.    ü§ñ                                                                                                                                                                          and Prompt                                                                                                                                                                                                                            Edition     Original Prompt with opt-segment colored                                                                                                                          Best opt-feedback package\n\n                                    Package 2                                                    Where: Ensure all open                        Ensure all open parentheses, brackets, and braces are\n                                                                                                                                parentheses, brackets, and                       systematically and accurately matched and closed. ‚Ä¶\n                 Where 2: systematically and accurately matched and closed...                       braces‚Ä¶\n               Why 2: The body part could possibly affect the output to some                      Why:to be moreThe headspecific.part needs\n                       extent.                                                                               How: Highlight the                              Optimizeded Prompt with new segment\n               How 2: Add more details to actual matching process                                     completion of the\n                                                                                                                         sequence.                                 The requirement is to make sure that all open structural\n                                    Package 3                                                                                                               symbols: parentheses (), brackets [], and braces {} are\n                 Where 3: ...                                                                                                                                                                systematically and accurately matched and closed. ‚Ä¶\n                                      ...\n                                                                                         Optimizer: Optimization & Edition\n\n                Attributer: Hierarchically Reasoning & Suggesting\n\n\n                                      Figure 1: Workflow of HAPO.\n\n\nEdit Operators. We define a compact set of edit op-      suggestions and reasons into structured modules.  In\nerators O applied to a target unit uk: (i) Replace; (ii)      the end, to maintain consistency, the same model will\nInsert; (iii) Delete; (iv) Reorder; (v) Refine. An edit     be served with this meta-prompt to generate the new\ncandidate is a = (k, o) ‚àà{1, . . . , K} √ó O that produces      candidate in the next iteration.\np‚Ä≤ = Ek,o(p).                                             Measuring Prompt Drift. We quantify  drift as\nMultimodal Pipeline.  For MLLM settings, the in-      degradation on previously solved items.  Let St‚àí1 =\nputs include images {Ij} and question text x. We ex-      {i   : M(xi; pt‚àí1) correct} and Ft =  {i ‚ààSt‚àí1   :\ntract the base64 value of the local image data as part     M(xi; pt) incorrect}. We define retention and drift:\nof the MLLM request.\n                                                                            |St‚àí1 \\ Ft|\n  The optimizer constructs a joint meta-prompt that      Retention(t) =                  ,   Drift(t) = 1‚àíRetention(t).\npreserves the original template‚Äôs core structure, popu-                         |St‚àí1|\nlating it with the specific task requirements and con-                                              The  global  drift up  to  t  is the average  Drift =\nstraints while applying an identical process of attribu-       1                                        PtœÑ=1 Drift(œÑ). We also trigger protective actions iftion and UCB selection. Its sole modification is an ex-          t\n                                                                 Drift(t) exceeds a threshold for S‚Ä≤ consecutive itera-\nplicit annotation of the task‚Äôs multimodal background\n                                                                   tions.\nat the end of the meta-prompt.\n                                                 Early Stopping and Check-pointing. We stop\nOptimizer Phase                               when (i) the upper-limit iteration number S (we set\nThis phase receives the improved opt feedback package     20 here) is reached; (ii) no positive reward more than\nand will split and incorporate it into the meta prompt     0.5% for consecutive iterations of S‚Ä≤ (we set 3 here); or\nto produce the next candidate. The meta prompt will        (iii) the drift risk exceeds a threshold (Sect.  ; we set\ndirectly include the last round‚Äôs candidate, highlight        it at 10% here). We keep the improved development\nthe modification location in its linguistic layer, and give      checkpoint and evaluate once in the test split.\n\nAlgorithm 1 HAPO: the Hierarchical Attribution      capability of HAPO across distinct, cross-modal tasks.\nPrompt Optimization.\n                                                From each task branch, a fixed subset of 3% was 1: Initialize improved prompt p0, history scores {s(0)k  },\n                                                   randomly sampled. This subset was utilized through-   arm stats {ÀÜ¬µa, na}, calls C ‚Üê0, t ‚Üê1\n                                                       out the optimization procedure, facilitating the compu- 2: repeat\n                                                             tation of task accuracy at intermediate steps.  These 3:   Run  through  the  training  set  Dtrain  =\n                                                        accuracy metrics provide an estimate of performance    {(xi, y‚àói )}Ni=1 for f(p, xtrain)\n                                                   on the complete evaluation sets, thus balancing assess-\n 4:    {uk}Kk=1 ‚ÜêSeg(pt‚àí1)         ‚ñ∑Segmentation                                                 ment cost with a reliable proxy for general capability.\n 5:    Update Àús(t)k  via Eqs. (4)‚Äì(5)                   Upon completion of the optimization, the final instruc-\n 6:     Et ‚Üêmispredicted items on a small train slice        tions were evaluated on the full held-out portion of each\n 7:    Build candidate arms At = {(k, o)} over top-m     benchmark.\n    units\n              q    ln t\n 8:     at ‚Üêarg maxa‚ààAt ÀÜ¬µa + c   max(1,na)\n 9:     p‚Ä≤ ‚ÜêEat(pt‚àí1); evaluate reward rt on dev\n10:    Update ÀÜ¬µat, nat; update improved of {pt‚àí1, p‚Ä≤}                                                   Baseline Methods.  To rigorously evaluate HAPO,   by dev score\n                                               we compare it against six competitive baseline methods,11:   C ‚ÜêC + calls used; t ‚Üêt + 1\n                                                          organized into three principal categories:12: until Early stopping, or iteration limit for S rounds\n13: return improved prompt\n                                                              1.  Template-Based Methods. We include the\n                                                        Zero-Shot CoT prompting with prompt ‚ÄúThink step by\n                                                                   step‚Äù, and a Two-Shot CoT with two randomly selected\n            Experiments                       samples in each branch [16].\nImplementation and Experiment setup\n                                                              2. Auto-Generation Methods. In this category,Models.  Evaluation was conducted on three training-\n                                               we compare with APE [29] and OPRO[25], which lever-free large language models: Gemini 2.5 Pro Preview 06-\n                                                       age LLMs as optimizers to automatically generate or it-05 (Gemini) [5], GPT-4o (2025-03-26) [20], and Qwen3-\n                                                                eratively refine prompts, though they often depend onVL-Plus (2025-09-23) [24]. These models were selected\n                                                                  task-specific demonstrations or meta-prompts.for evaluation based on three principal considerations.\nFirst, their performance over time represents contem-\n                                                              3.   Gradient-Based Methods.  We encom-porary performance and stability, suggesting strong po-\n                                                          pass TextGrad [27] and EGO-Prompt[28], which ap-tential for nuanced linguistic analysis and prompt op-\n                                                           ply gradient-informed updates or evolutionary searchtimization. Second, each model natively supports mul-\n                                                            to navigate the discrete prompt space, albeit with con-timodal inputs including text and images, allowing for\n                                                               siderations for computational cost or sample efficiency.our experiment aim. Finally, their respective APIs are\nengineered for high-throughput parallel computation,\nenabling efficient processing of large-scale benchmarks      Multimodal Adjustment.  The majority of these\nwithin a feasible timeframe.                             methods, while not originally designed, were modified\n                                                            to process images during prompt evaluation. However,\nBenchmarks.  Our benchmarks included BBH [22],      although TextGrad can indirectly process images by\nGSM8K   [4],  OCRBench V2  (OCRV2)   [8],  and      integrating external MLLMs, its prompt optimization\nVQA2017(VQA)[10]. BBH constitutes 23 challenging      process introduces additional text-based task-loading\ntext-based branches for which prior language model     and evaluation, inherently mismatched for multimodal\nevaluations did not exceed average human performance;       tasks. Thus, TextGrad was excluded from multimodal\nGSM8K is a dataset of 8.5K high-quality, linguistically     benchmarks such as VQA and OCRV2.\ndiverse grade school math problems requiring multi-\nstep reasoning; OCRV2 represents a large-scale bilin-\ngual text-centric benchmark of 31 diverse scenarios to\nevaluate visual text localization and reasoning; and\nVQA is a dataset containing open-ended image-to-text     Evaluation.  For evaluation, we  will use an LLM\nquestions in 13 major types, demanding comprehension      grader following the benchmarks‚Äô grading rules.  To\nof vision, language, and commonsense knowledge.           avoid issues such as language patterns that affect the\n BBH and GSM8K serve as representative text-to-text      score performance of the grader, we chose to use a dif-\nbenchmarks, involving long-chain logical analysis and       ferent LLM than the previous three companies for scor-\nadvanced mathematical problems. OCRV2 and VQA       ing; we chose Deepseek-V3 [6]; since the benchmarks all\nare both image-to-text benchmarks comprising high-      have a standard target or answer, we will fit the task,\nquality visual question-answering tasks. We selected      the target, and the model‚Äôs output in a meta-prompt\nthese four benchmarks to demonstrate the generalized       like :\n\nYou are a professional question-answering assess-       Method     BBH  GSM8K VQA  OCRV2\n ment expert. You will be given a question descrip-        Gemini\n tion (including the question itself and the answer re-                                                           Zero-Shot CoT   70.23     62.45     39.68     50.06\n quirements), a standard answer, and an answer; you                                                      Two-Shot CoT   71.61     63.81     41.29     50.48\n  will use this to evaluate the quality of the answer.                                        APE             74.18     64.88     46.24     58.86\n                                      OPRO           86.95     82.75     44.10     59.34\n Question description: {task}                                                     TextGrad       90.19     76.36          -            -\n Reference answer: {target}                                                EGO-Prompt    89.94     75.61     41.71     55.68\n Answer: {output}                                                 Our Method      89.76    84.81    48.40   61.45\n                                              GPT-4o Try to learn and understand the task description,\n                                                           Zero-Shot CoT   77.52     69.73     44.81     38.64 and score the specific answer generated based on the\n                                                      Two-Shot CoT   77.08     70.17     43.25     39.01 task description and the reference answer to reflect\n                                        APE             80.63     72.04     52.06     44.39 whether the answer perfectly meets the question re-\n                                      OPRO           82.86     78.16     58.94     47.81 quirements in terms of steps and results, with a max-\n                                                     TextGrad        84.55     81.88          -            - imum score of 100.\n                                                EGO-Prompt    83.91     79.54     55.26     45.60\n  An open-source, minimal, runnable prototype (in-                                                 Our Method    85.94    83.41    60.17   48.79\ncluding a hierarchical attributor, UCB selector, meta-\nhint template library, logging, and checkpoints) will be     Qwen\nprovided in our GitHub repository, along with a scaffold         Zero-Shot CoT   64.01     68.02     32.18     46.20\nfor reproducing experiments.                              Two-Shot CoT   64.46     69.58     33.63     45.85\n                                        APE             61.53     65.07     32.16     54.18\nExperiment Results                        OPRO           73.33     79.35     43.09     56.15\n                                                     TextGrad        73.65     72.11          -            -\nThe comprehensive experimental results are summa-       EGO-Prompt    74.45     73.27     39.57     52.38\nrized in Table 1, with our method establishing an ad-       Our Method    75.70    80.79    45.19   58.45\nvantage, achieving an improved score in 11 out of 12\nmodel-benchmark combinations.  It delivers an aver-      Table 1: Mean Performance Across Benchmarks.\nage performance gain of +13.28% over the Zero-Shot     Bold numbers indicate the improved among four meth-\nCoT baseline across  all tasks and models.   Specifi-      ods in each benchmark. Notice that TextGrad was ex-\ncally,  it outperforms the robust OPRO optimizer by      cluded from multimodal input.\na notable margin in multimodal reasoning, such as a\n+2.54% and +1.80% percentage point advantage on\nthe VQA benchmark (48.71% vs. 51.25%) and OCRV2\nbenchmark (54.43% vs.  56.23%), respectively. While      the entire training set. TextGrad shows significant task-\nTextGrad show relatively good performance in BBH     dependent variance, averaging 2,365.31 calls on nor-\nbenchmark, and EGO-Prompt, specialized for knowl-     mal tasks, but 31,419.33 on long dataset like GSM8K.\nedge graph tasks through text-based expert learning,      For EGO-Prompt, total calls range from approximately\nshows competitive results on BBH but underperforms      3,440 for typical tasks such as BBH/VQA to 180,274\non visual datasets like VQA (e.g., 41.71% for Gemini vs.       for large-scale benchmarks like GSM8K. Finally, our\nour 48.40%, a 16% relative improvement), our method      proposed method, HAPO, occupies an efficient position\nexhibits generalized improvement. This is epitomized      within this methodological spectrum, averaging 6.71 it-\nby its scores on GSM8K, achieving 84.81% with Gem-      erations and 2,080.10 calls per branch, achieving a rela-\nini (a +2.06% lead over the second method, OPRO),       tive balance between performance and resource expen-\n83.41% with GPT-4o and 80.79% with Qwen, thereby       diture.\nrobustly validating its versatility and superior capabil-\nity as a model-agnostic framework for prompt optimiza-\ntion.\n                                      Prompt Optimization Case Study\nModel Call Analysis\n\nTo compare the computational efficiency of the eval-     Case 1: BBH - Date Understanding.\nuated prompt optimization techniques, we conducted\n                                                         This task branch requires temporal reasoning by ap-\na comparative analysis of model calls, a primary de-\n                                                           plying a time adjustment to a given date and select-\nterminant of operational cost in API-dependent envi-\n                                                            ing the correct formatted output from multiple choices.\nronments. Beginning with the baseline, the relatively\n                                              The optimization path shows a language to enforce a\nbasic APE algorithm requires an average of 453.17\n                                                 more rigorous and error-resistant reasoning process.\ncalls per branch.  Meanwhile, OPRO exhibits a sub-\nstantially higher overhead of 49,054.87 calls, a conse-        In the second iteration, the prompt received a score\nquence of its per-sample evaluation mechanism across       of 95.40 and is like the following:\n\nCarefully analyze the given date information, apply          1. Ensure the generated text strictly matches one of\n the specified time adjustment, and select the calcu-         the specified target options without introducing any\n lated date in MM/DD/YYYY format with the pro-         unlisted alternatives.\n vided choices.                                                     2. Avoid redundant phrasing and maintain precision\n                                                                 in alignment with the scoring rules.\n  In the fourth iteration with score of 96.00 it is like:\n                                                      **Optimized Example:**\n Carefully analyze the given date information, ap-                                                           Question: How many times does the character ‚Äòe‚Äô\n ply the specified time adjustment step-by-step, ver-                                                       appear in the picture?\n  ify the calculated date in MM/DD/YYYY format,                                                    Image description: An billboard showing ‚ÄúTimes\n and select the correct option by matching it precisely                                                          Square‚Äù\n with the provided choices.                                                      Answer: [‚Äò2‚Äô, ‚Äòtwo‚Äô, ‚Äòtwice‚Äô]\n\n  In the seventh iteration, the prompt that receives a\n                                                  **Key Notes:**\nscore of 99.20 is like the following:\n                                                                                  - The output must strictly adhere to the specified\n                                                        format and options. Accurately interpret the given date context, per-\n                                                                                  - Examples should be concise, precise, and directly form the required time adjustment through metic-\n                                                             aligned with the task requirements. ulous step-by-step computation, validate the result-\n                                                                                  - Avoid introducing any additional explanations or ing date in MM/DD/YYYY format, and identify the\n                                                               unlisted alternatives in the output. correct answer by exact alignment with the provided\n options.\n                                                The optimization focused on output constraint and\n                                                       exemplar-based learning. The progression from a sim-  In this case, by incrementally enforcing a structured,\n                                                               ple command to a detailed specification with illustrativemulti-step computational process and demanding exact\n                                                      examples and explicit guardrails (e.g., ‚Äústrictly match,‚Äùverification, the prompt guides the model to emulate\n                                                             ‚Äúavoid unlisted alternatives‚Äù) provided the model witha more reliable and deterministic algorithm, which is\n                                                          the necessary context and constraints to align its out-crucial for tasks requiring high numerical and logical\n                                                        puts precisely with the task‚Äôs evaluation criteria. Theaccuracy.\n                                                      performance leap is dramatic, moving from a failing to\n                                                     a passing grade, achieved by evolving from a terse in-\nCase 2: OCRV2 - Text Counting.\n                                                              struction to a richly specified prompt with demonstra-\n  This task branch involves counting and outputting       tions.\ntextual elements in an image.\n  In the third iteration, the prompt, with an average                Ablation Study\ngrade of 28.83, is\n                                      We conducted an in-depth ablation study to evalu-\n Output the exact number as a numeral without any        ate the impact of various components in our proposed\n additional explanation.                               method. All experiments were performed using GPT-4o\n                                                            as the base model with default parameters unless other-\n  In the fifth, the prompt, with average grade 43.75, is:      wise specified. The evaluation encompasses two distinct\n                                                           task types: BBH‚Äôs sports understanding (text-to-text\n Please output the exact number without any addi-                                                           reasoning) and OCRV2‚Äôs reasoning VQA en (image-to-\n tional explanation.                                                             text reasoning). These datasets were selected to rep-\n                                                              resent both mathematical and non-mathematical rea-\n Example:                                                         soning challenges while aligning with contemporary re-\n Question: How many times does the character ‚Äòe‚Äô                                                           search on AI evaluation methodologies.\n appear in the picture?                                                         For evaluation metrics, we used primarily accuracy\n Image description: An billboard showing ‚ÄúTimes                                                                   for the final output assessment to measure the stability\n Square‚Äù                                                                  of model performance under varying prompt conditions.\n Answer: [‚Äò2‚Äô, ‚Äòtwo‚Äô, ‚Äòtwice‚Äô]\n\n                                                   Meta-Prompt Config          SU    RVE Key note:\n  - Ensure the generated text strictly matches one of          Full (Prioritizing + Reasoning)   76.8    65.7\n the specified target options without introducing any         w/o. Prioritizing Weak Elements   71.9     60.2\n unlisted alternatives.                                   w/o. Structured Reasoning        73.5     62.1\n  - Avoid introducing any additional explanations or         w/o. Both Components            68.4     56.8\n unlisted alternatives in the output.\n                                                       Table 2: Ablation Study of Meta-Prompt Components.\n  And in the seventh iteration the prompt, with aver-     Here SU means sports understanding, RVE means rea-\nage grade 67.89, is:                                       soning VQA en.\n\nThe Impact of Meta-Prompt Design                ages, with prompt ‚ÄúBriefly describe the image.‚Äù) against\n                                                                 original images and enhanced visual representations (byThe meta-prompt‚Äôs construction is critical for prompt\n                                                      adding a red box to each input image hinting the tar-optimization. Our default design integrates two compo-\n                                                            get answer), on the subset, reasoning VQA en, of thenents: Prioritizing Weak Elements (focusing on under-\n                                                  benchmark OCRV2.performance) and Structured Reasoning (explicit anal-\n                                                       Results.   Table 4 shows that the enhanced visualysis of in-context examples). We performed an ablation\n                                                              features produce the highest accuracy (65.7%), outper-study to quantify each component‚Äôs contribution.\n                                                       forming original images (61.5%) and text-only (58.9%).  Objective. This part is designed to isolate the per-\n                                              The 6.8% gap between text-only and original imagesformance impact of the two core meta-prompt strate-\n                                                          confirms visual information  is indispensable for thisgies.\n                                                                task, supporting findings from visual mathematical rea-  Setup. We systematically ablated each component\n                                                         soning research.from the full meta-prompt and evaluated performance\non BBH sports understanding and OCRV2 reasoning\n                                                                Strategy                  AccuracyVQA en.\n                                                                              (%)  Analysis of Outcomes.   As shown in Table 2,\nthe full meta-prompt achieved the highest precision.           Text-Only                  58.9\nAblating Prioritizing Weak Elements caused substan-            Original Image              61.5\ntial drops (4.9% on BBH, 5.5% on reasoning VQA           w/. Enhanced Features     65.7\nen), demonstrating its critical role. Removing Struc-\ntured Reasoning led to significant but smaller reduc-      Table 4: Impact of input representation on reasoning\ntions (3.3% on sports understanding, 3.6% on reason-    VQA en performance.\ning VQA en). Removing both components caused the\nmost severe performance degradation (8.4% on sports\nunderstanding, 8.9% on reasoning VQA en), revealing                Reproducibility\na synergistic effect.                          We will release anonymized code, prompts, and logs\n  The importance of the component varied by task; Pri-      including:  (i) minimal working examples per dataset,\noritizing Weak Elements was relatively more crucial for        (ii) meta-prompt templates for the optimizer, (iii) con-\nthe text-based BBH task, while both components con-      figuration files (T, M), (iv) checkpoints for improved\ntributed more evenly to the complex visual reasoning     prompts and intermediate trajectories, and (v) exact\nin reasoning VQA en.                                      preprocessing for OCR/VQA (OCR engine, box for-\n  Our analytical selection strategy (utilizing the full      mats, resolution). All experiments use fixed seeds; we\nmeta-prompt) also converged faster and more stably       will report the versions of OS, driver, and libraries as\nthan a randomized baseline, achieving higher final pre-      well as the endpoints of the models.  Dataset/model\ncision (76.8% vs. 70.2% on sports understanding; 65.7%       licenses are respected and sensitive content is filtered.\nvs. 58.3% on reasoning VQA en) in fewer iterations (4\nvs. 8) with lower variance (¬±1.8% vs. ¬±5.2%), shown                  Conclusion\nin Table 3.\n                                      We  introduced HAPO,  a  hierarchical  attribution\n                                                    framework  for prompt  optimization  that combines\n  Strategy     SU      RVE        IConvergence                                                                  unit-level attribution, a compact edit operator  set,\n  Analytical     76.8       65.7      4                                                  and UCB-based selection, and extends naturally to\n  Randomized   70.2        58.3       8                                                     multimodal  pipelines.   In  fair comparisons, HAPO\n                                                                 yields consistent gains across text and vision-language\nTable 3: Performance comparison of prompt selection                                                   benchmarks while explicitly controlling prompt drift.\nstrategies. Here SU means sports understanding, RVE                                      We expect HAPO to serve as a practical, extensible\nmeans reasoning VQA en, and IConvergence means iter-                                                    paradigm  for  scalable prompt  engineering and  to\nations to convergence.                                                                inspire further work on attribution-driven optimization\n                                                                in discrete semantic spaces.\nThe Impact of Input Representation\nWe also noticed that the representation of visual in-     Discussion.  The current HAPO method  is com-\nformation requires careful design to support complex      putationally intensive, limiting its real-time use.  Its\nreasoning tasks, as models struggle to extract relevant      evaluation also requires broader validation in profes-\ninformation from raw visual inputs.                           sional technical domains, while  its generalization to\n  Objective. This ablation experiment is designed to      other AI models needs further study.  Future work\nassess how visual input representation forms affect OCR      could  focus on improving  efficiency with  adaptive\nperformance, given known AI limitations in visual rea-     prompting and  early  stopping,  refining  causal  at-\nsoning.                                                       tribution  methods,  generalizing on  domain-specific\n                                                      benchmarks, and expanding cross-model testing with  Setup. We compare text-only descriptions (made\n                                                         formal drift controls.by aimlessly prompting GPT-4o to caption task im-\n\nReferences                         and Yujiu Yang. Evoprompt: Connecting llms with\n                                                                evolutionary algorithms yields powerful prompt [1] Eshaan Agarwal,  Joykirat  Singh,  Vivek Dani,\n                                                                  optimizers. 2025.    Raghav Magazine,  Tanuja Ganu, and Akshay\n    Nambi. Promptwizard: Task-aware prompt opti-       [12] Qiyang Han,  Koulik Khamaru,  and Cun-Hui\n    mization framework. 2024.                              Zhang. Ucb algorithms for multi-armed bandits:\n [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang,           Precise regret and adaptive inference. 2024.\n    Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,       [13] Cho-Jui Hsieh, Si Si, Felix X. Yu, and Inderjit S.\n     Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi           Dhillon. Automatic engineering of long prompts.\n    Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan,          2023.\n     Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu,\n                                                                     [14] Shaohan Huang, Li Dong, Wenhui Wang, Yaru\n    Jiabo Ye, and Xi Zhang et al. Qwen2.5-vl technical\n                                                         Hao, Saksham Singhal, Shuming Ma, Tengchao\n     report. 2025.\n                                                            Lv, Lei Cui, Owais Khan Mohammed, Barun Pa-\n [3] Yumin Choi, Dongki Kim, Jinheon Baek, and            tra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan\n    Sung Ju Hwang. Multimodal prompt optimization:           Bjorck, Vishrav Chaudhary, Subhojit Som, Xia\n   Why not leverage multiple modalities for mllms.          Song, and Furu Wei. Language is not all you need:\n     2025.                                                     Aligning perception with language models. 2023.\n [4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-                                                                     [15] Omar Khattab, Arnav Singhvi, Paridhi Mahesh-\n     ian, Mark Chen, Heewoo Jun, Lukasz Kaiser,                                                                   wari, Zhiyuan Zhang, Keshav Santhanam,  Sri\n    Matthias Plappert, Jerry Tworek, Jacob Hilton,                                                      Vardhamanan,  Saiful Haq,  Ashutosh Sharma,\n     Reiichiro Nakano, Christopher Hesse, and John                                                 Thomas T. Joshi, Hanna Moazam, Heather Miller,\n    Schulman.  Training verifiers to solve math word                                                         Matei Zaharia, and Christopher Potts.   Dspy:\n    problems. 2021.                                                         Compiling declarative language model calls into\n [5] Gheorghe Comanici, Eric Bieber, Mike Schaek-           self-improving pipelines. 2023.\n    ermann, Ice Pasupat, Noveen Sachdeva, Inderjit                                                                     [16] Takeshi Kojima, Shixiang Shane Gu, Machel Reid,\n     Dhillon, Marcel Blistein, and Ori Ram et al. Gem-                                                        Yutaka Matsuo, and Yusuke Iwasawa. Large lan-\n      ini 2.5: Pushing the frontier with advanced reason-                                                         guage models are zero-shot reasoners. 2023.\n     ing, multimodality, long context, and next genera-\n     tion agentic capabilities. 2025.                             [17] Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin,\n                                                      Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao\n [6] DeepSeek-AI, Aixin Liu,  Bei Feng, Bing Xue,\n                                                           Wei, Hao Cheng, Wanggui He, Fangxun Shu, Hao\n    Bingxuan Wang, Bochao Wu, Chengda Lu, and\n                                                                  Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, and\n    Chenggang Zhao et al. Deepseek-v3 technical re-\n                                                           Yueting Zhuang. Boosting private domain under-\n     port, 2025.\n                                                              standing of efficient mllms: A tuning-free, adap-\n [7] Chrisantha  Fernando,  Dylan  Banarse,  Henryk             tive,  universal prompt optimization framework.\n    Michalewski, Simon Osindero, and Tim Rock-          2025.\n     t√§schel.   Promptbreeder:   Self-referential  self-\n                                                                     [18] Wenxin Luo, Weirui Wang, Xiaopeng Li, Weibo    improvement via prompt evolution. 2023.\n                                                         Zhou, Pengyue Jia, and Xiangyu Zhao.  Tapo:\n [8] Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin                                                               Task-referenced adaptation for prompt optimiza-\n    Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi                                                                        tion. 2025.\n    Luo, Xinyu Wang, Hao Lu, Zhang Li, Guozhi\n                                                                     [19] Oscar Ma√±as, Pietro Astolfi, Melissa Hall, Can-    Tang, Bin Shan, Chunhui Lin, Qi Liu, Binghong\n                                                           dace Ross, Jack Urbanek, Adina Williams, Aish-   Wu, Hao Feng, Hao Liu, Can Huang, Jingqun\n                                                        warya Agrawal,  Adriana Romero-Soriano, and    Tang, Wei Chen, Lianwen Jin, Yuliang Liu, and\n                                                           Michal Drozdzal. Improving text-to-image consis-    Xiang Bai. Ocrbench v2: An improved benchmark\n                                                             tency via automatic prompt optimization. 2024.     for evaluating large multimodal models on visual\n     text localization and reasoning. 2025.                     [20] OpenAI and  :  and Aaron Hurst, Adam Lerer,\n [9] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong       Adam  P.  Goucher,  Adam  Perelman,  Aditya\n    Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wen-         Ramesh, Aidan Clark, AJ Ostrow, Akila Weli-\n    wei Zhang, Ping Luo, and Kai Chen. Multimodal-           hinda,  Alan  Hayes,  Alec Radford,  and  Alek-\n     gpt: A vision and language model for dialogue with          sander MƒÖdry et al. Gpt-4o system card. 2024.\n    humans. 2023.                                               [21] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee,\n[10] Yash Goyal, Tejas Khot, Douglas Summers-Stay,         Chenguang Zhu, and Michael Zeng.  Automatic\n    Dhruv Batra, and Devi Parikh. Making the V in         prompt optimization with \"gradient descent\" and\n   VQA matter: Elevating the role of image under-        beam search. 2023.\n    standing in Visual Question Answering. 2017.                                                                     [22] Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli,\n[11] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li,          Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\n    Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,         Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi,\n\nDenny Zhou, and Jason Wei.  Challenging big-\n    bench  tasks and whether chain-of-thought can\n     solve them. 2022.\n[23] Yurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng\n    Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou,\n    Zhiming Ding, and Linjun Yang. Strago: Harness-\n     ing strategic guidance for prompt optimization.\n     2024.\n[24] An Yang, Anfeng  Li, Baosong Yang,  Beichen\n    Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\n    Gao, and Chengen Huang et al. Qwen3 technical\n     report, 2025.\n[25] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao\n     Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.\n    Large language models as optimizers. 2024.\n[26] Te Yang, Jian Jia, Xiangyu Zhu, Weisong Zhao,\n   Bo Wang, Yanhua Cheng, Yan  Li, Shengyuan\n     Liu, Quan Chen, Peng Jiang, Kun Gai, and Zhen\n     Lei. Enhancing instruction-following capability of\n     visual-language models by reducing image redun-\n    dancy. 2024.\n[27] Mert Yuksekgonul, Federico Bianchi, Joseph Boen,\n    Sheng Liu, Zhi Huang, Carlos Guestrin, and James\n    Zou.  Textgrad: Automatic \"differentiation\" via\n     text. 2024.\n[28] Yang Zhao, Pu Wang, and Hao Frank Yang. How\n     to auto-optimize prompts for domain tasks? adap-\n     tive prompting and reasoning through evolutionary\n    domain knowledge adaptation. 2025.\n[29] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen\n    Han, Keiran Paster, Silviu Pitis, Harris Chan, and\n   Jimmy Ba. Large language models are human-level\n    prompt engineers. 2023.\n[30] Zixuan Zhou,  Xuefei Ning, Ke Hong, Tianyu\n    Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning\n    Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan,\n    Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, and\n   Yu Wang. A survey on efficient inference for large\n    language models. 2024.",
"headers": [
"arXiv:2601.02683v1  [cs.AI]  6 Jan 2026",
"Learning from Prompt itself: the Hierarchical Attribution Prompt",
"Optimization",
"Dongyu Chen, Jian Ma, Xianpeng Zhang, Lei Zhang, Haonan Lu, Chen Chen,",
"Chuangchuang Wang and Kai Tang",
"Introduction",
"Method",
"Related work",
"Experiments",
"Prompt Optimization Case Study",
"Ablation Study",
"Reproducibility",
"Conclusion",
"References",
"ü§ñ"
],
"tables": [
"|Seman<br>Segme|tic Text<br>ntation|\n|---|---|",
"|Execut<br>for|e Prompt<br>f(p,x)|\n|---|---|",
"|Method|BBH GSM8K VQA OCRV2|\n|---|---|",
"|Zero-Shot CoT<br>Two-Shot CoT<br>APE<br>OPRO<br>TextGrad<br>EGO-Prompt<br>Our Method|70.23 62.45 39.68 50.06<br>71.61 63.81 41.29 50.48<br>74.18 64.88 46.24 58.86<br>86.95 82.75 44.10 59.34<br>90.19 76.36 - -<br>89.94 75.61 41.71 55.68<br>89.76 84.81 48.40 61.45|\n|---|---|",
"|Zero-Shot CoT<br>Two-Shot CoT<br>APE<br>OPRO<br>TextGrad<br>EGO-Prompt<br>Our Method|77.52 69.73 44.81 38.64<br>77.08 70.17 43.25 39.01<br>80.63 72.04 52.06 44.39<br>82.86 78.16 58.94 47.81<br>84.55 81.88 - -<br>83.91 79.54 55.26 45.60<br>85.94 83.41 60.17 48.79|\n|---|---|",
"|Zero-Shot CoT<br>Two-Shot CoT<br>APE<br>OPRO<br>TextGrad<br>EGO-Prompt<br>Our Method|64.01 68.02 32.18 46.20<br>64.46 69.58 33.63 45.85<br>61.53 65.07 32.16 54.18<br>73.33 79.35 43.09 56.15<br>73.65 72.11 - -<br>74.45 73.27 39.57 52.38<br>75.70 80.79 45.19 58.45|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2601.02683v1.pdf"
}