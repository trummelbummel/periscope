{
"text": "AutoPDL: Automatic Prompt Optimization for LLM Agents\n\n                 Claudio Spiess1 Mandana Vaziri2 Louis Mandel2 Martin Hirzel2\n               1UC Davis\n                2IBM Research\n\n                  Abstract The performance of large language models (LLMs) depends on how they are prompted,\n                             with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct,\n                      ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Man-\n                                 ually tuning this combination is tedious, error-prone, and specific to a given LLM and task.\n                                Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM\n                              agent configurations. Our approach frames this as a structured AutoML problem over a2025                               combinatorial space of agentic and non-agentic prompting patterns and demonstrations, us-\n                               ing successive halving to efficiently navigate this space. We introduce a library implement-\n                               ing common prompting patterns using the PDL prompt programming language. AutoPDLNov                          solutions are human-readable, editable, and executable PDL programs that use this library.\n                              This approach also enables source-to-source optimization, allowing human-in-the-loop re-3\n                             finement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B\n                              parameters) show consistent accuracy gains (9.21 Â± 15.46 percentage points), up to 67.5pp,\n                           and reveal that selected prompting strategies vary across models and tasks.\n[cs.LG]       1 Introduction\n                Large language models (LLMs) and LLM-based agents excel at a variety of tasks, including question\n                answering, math word problems, and programming. The performance of an LLM depends heavily\n              on how it is prompted, and there are a variety of popular prompting patterns. These include\n                 zero-shot or few-shot (Brown et al. 2020) prompting with chain-of-thought (CoT) (Wei et al. 2022),\n                 zero-shot CoT (Kojima et al. 2022), as well as agentic patterns such as ReAct (Yao et al. 2023) or\n           ReWOO (Xu et al. 2023). However, given a dataset ğ·test with a loss function L, e.g., error rate,\n                         it is not clear which pattern will do best. Furthermore, besides the pattern ğ´, performance also\n               depends on the prompt ğ‘, including few-shot samples and instructions. The problem is thus to find\n                a combination ğ´âˆ—ğ‘of a pattern along with an optimized prompt that minimizes L. This is usually\n               done via manual prompt engineering, but that is tedious and has to be repeated if a new LLM comes\n                 along. Therefore, this paper explores how to find ğ´âˆ—ğ‘using automated machine learning (AutoML).\n            And for users to trust the result or to tweak it further, ğ´âˆ—ğ‘itself should be easy to read and edit.arXiv:2504.04365v5             Agent frameworks, such as CrewAI (Moura 2023) or AutoGen (Q. Wu et al. 2023), contain pre-\n                   built agent patterns with prompts optimized for proprietary frontier models and common tasks.\n                 Unfortunately, their prompts are deeply buried (Schluntz et al. 2024), making them hard to modify\n              and adapt to non-frontier models or novel tasks. Moreover, the prompting pattern is fixed to a\n                  variation of ReAct, limiting flexibility in customizing the prompt structure. Prompt optimizers,\n               such as DSPy (Khattab et al. 2024), optimize few-shot samples for in-context learning (ICL) and/or\n                  instructions in the prompt ğ‘. Unfortunately, they do not automatically select the prompting pat-\n                 tern ğ´and do not return human-readable code.\n             We formulate the problem of finding a good pattern and corresponding prompt by defining and\n                then exploring a combined search space. We were inspired by the AutoML literature on combined\n                 search spaces of machine-learning algorithms and their hyperparameters (Thornton et al. 2013),\n                except that (i) instead of discrete or continuous hyperparameters, we explore textual ICL samples,\n                  instructions, and prompting patterns; (ii) instead of classification or regression tasks, we tackle\n\n\n                 An earlier version of this paper was published at AutoML 2025.\n                       This version adds missing standard deviations in Table 1.     1    Â© 2025 the authors, released under CC BY 4.0\n\ngenerative tasks; and (iii) instead of model training or fine-tuning, we focus on in-context learning.\n  We assume a dataset with a validation set ğ·valid, test set ğ·test, as well as an example bank ğ·train for\n  few-shot samples. ğ·valid is used during the optimization process to evaluate the performance of\n  a configuration, whereas ğ·test is used once upon completion, to evaluate the performance of the\n   best performing configuration. As usual, to avoid over-fitting, we assume these are disjoint from\n  each other. The problem statement is to find ğ´âˆ—ğ‘= argmin L(ğ´ğ‘, ğ·valid), where:\n                                           ğ´ğ‘âˆˆAP\n   â€¢ ğ´âˆˆA = {Zero-Shot, CoT, ReWOO, ReAct} is the prompting pattern, and\n\n   â€¢ ğ‘= âŸ¨ğ‘›,ğ‘‘train, instrâŸ©âˆˆP is the prompt, comprising a number ğ‘›â‰¤|ğ·train| of few-shot samples,\n    the actual few-shot samples ğ‘‘train âˆˆ(ğ·train)ğ‘›, and an instruction instr âˆˆI. A concrete example\n     of P is provided in Figure A1 and Figure A2.\n\n  To avoid getting stuck in local minima while saving compute and finding a solution with a low\n   loss, we explore the search space AP using successive halving (Jamieson et al. 2016). To make\n   the initial search space AP user-interpretable, and the final solution ğ´âˆ—ğ‘both human readable\n  and executable, we express them in a YAML-based prompting language, PDL (Vaziri et al. 2024).\n  PDLâ€™s structured format makes it easy to modify both the initial search space and the optimized\n  program, and ensures the final solution remains directly executable. We introduce a library for\n  PDL that implements each of the common prompting patterns in A. The initial search space AP\n   is a YAML file with various choice points for AutoML to decide. And the solution ğ´âˆ—ğ‘is a custom-\n   tailored PDL program optimized for the given task, as given by the dataset and loss function. The\n  developer can read or even tweak either or both as desired.\n    We evaluate our optimizer on three tasks (question answering, math, and programming), using\n  seven LLMs sized between 3B and 70B parameters. We find that the optimizer often gives accuracy\n  boosts in the 6â€“30% range, in some cases higher. Given the same task, different patterns ğ´âˆˆA do\n   best for different models. Conversely, given the same model, different patterns do best for different\n   tasks. Besides this variability in the chosen pattern ğ´, our experiments also revealed variability\n   in the optimized prompts ğ‘= âŸ¨ğ‘›,ğ‘‘train, instrâŸ©. We also found that when training data for a task is\n   missing, data from a related but different dataset can help. Also, while most of our experiments use\n  moderately-sized open models, we also show our optimized solutions can benefit frontier models.\n      This paper makes three primary contributions:\n\n   1. Jointly searching pattern and prompt: prior work in prompt optimization has not investigated\n     searching joint search spaces, including agentic patterns.\n\n   2. No one size fits all: we find that different models sometimes have differing optimal prompt pat-\n     terns for the same benchmark, suggesting that there is not one single optimal prompt pattern.\n\n   3. Source-to-source optimization: we propose the first source-to-source optimizer for LLM prompt\n     programs, where both the initial search space and the final solution are prompt programs in the\n    same language, making the final solution both human-readable and executable.\n\n       Overall, this paper shows how to apply AutoML to automatically discover agentic or non-\n   agentic LLM prompts and patterns optimized for a given task. We make our AutoPDL implemen-\n   tation available at https://github.com/IBM/prompt-declaration-language, and release the repro-\n  duction package used for the experiments in this work to the community.\n\n2 Background\n\n  This paper uses PDL (Vaziri et al. 2024) as a representation for exploring the search space of pro-\n  grams. PDL programs are declarative and combine human readability with ease of execution. They\n   represent the composition of calls to LLMs and tools, abstracting away the plumbing necessary for\n\n\n\n                                                                                               2\n\n1  text:\n                         2 - role: tools\n                         3    text: ${ tools }\n                         4 - \"Out of 1400 participants, 400 passed the test. What percentage is that?\\n\"\n                         5 - def: actions\n                         6    model: replicate/ibm-granite/granite-3.1-8b-instruct\n                         7    parser: json\n                         8    spec: [{ name: str, arguments: { expr: str }}]\n                         9 -  if: ${ actions[0].name == \"calc\" }\n                        10    then:\n                        11      lang: python\n                        12      code: result = ${ actions[0].arguments.expr }\n\n\n                                 Figure 1: Basic example of a PDL program.\n\n  such compositions. The output of the optimizer is also a PDL program, rather than simple textual\n  prompts, so it is fully executable and could be further refined by a developer.\n      Figure 1 shows a simple PDL program that uses a tool to answer a query. PDL is based on the\n  premise that interactions with an LLM are mainly for the purpose of generating data. So, it allows\n   users to specify the shape of data to be generated in a declarative way (in YAML), and is agnostic\n   of any programming language. The first line of Figure 1 specifies that we are creating some text.\n   Next, the first block in the itemized list defines the tools prompt. Line 3 contains a use of variable\n   tools, expressed as a Jinja expression. This variable is defined as the JSON Schema specification\n   of a calculator tool (not shown in this figure, for the full program see Appendix 8.2). Line 4 is the\n  user query prompt. We do not specify the role explicitly as user is the default role for prompts.\n  Lines 5 through 8 show a model call. In PDL, the background context is accumulated implicitly,\n  so the output of all blocks executed so far will be passed to the LLM as a list of input messages.\n  The result of the model call is assigned to the variable actions (line 5). The model to be called\n   is specified on line 6 (PDL is based on LiteLLM,1 so this is a LiteLLM model id). Finally, lines 7\n  and 8 say that we parse the output of the model as JSON and type-check it according to the type\n  on line 8. Furthermore, when the inferencing server supports it, model calls with a schema use\n  constrained decoding (Willard et al. 2023), enforcing syntactically correct JSON.2\n    On line 9, an if-statement checks whether the output of the LLM asks for the calculator tool. If\n   so, we use a Python code block to compute the requested tool call (lines 11 and 12). When we exe-\n   cute this program using the PDL interpreter, we obtain all the model inputs, followed by the model\n   output, and finally the output of the tool call. PDL has a rich set of control structures to allow writ-\n  ing a variety of prompting patterns, as well as functions to support libraries. For instance, Figure 3\n  shows a function call on line 4. In this paper, we consider the problem of automatically tuning\n  prompts and choosing prompting patterns for a given dataset. The following section explains our\n  approach in further detail.\n\n\n3 AutoPDL Approach\n\n   Figure 2 gives an overview of our approach. Referring to the numbers in the arrows:\n       (1) The input task is given by two disjoint datasets ğ·train and ğ·valid and a loss function L.\n  The datasets comprise âŸ¨ğ‘¥,ğ‘¦âŸ©instances, where ğ‘¥is a question, ğ‘¦is the corresponding answer, and\n  both are text strings. The loss function evaluates the quality of an answer ğ‘¦. (2) The search space\n   specification AP is a YAML file with the optimization variables and their possible values, along\n  with some hyperparameters. For example, num_demonstrations: [0, 3, 5] indicates that each candidate\n   will have zero, three, or five ICL samples randomly drawn from ğ·train. In the case of zero demon-\n   strations, this is equivalent to the zero-shot baseline. If zero is an option, we bias our candidate\n\n       1https://github.com/BerriAI/litellm\n     2PDL additionally makes use of the heuristic json-repair package.\n\n\n                                                                                               3\n\nSearch Space Specification\n       Input               variables:prompt_pattern: [cot, react, rewoo]                             Pattern Library\n      Task                   num_demonstrations:system_prompt: [granite_tools,[0, 3, 5]  llama3, granite_llama]\n                                initial_test_set_size: 16\n                                max_test_set_size: 1000\n                                num_candidates: 100                               Zero-\n                                parallelism: 5                                                    x                           y\n         Dtrain                                                     Shot\n                                     2\n                                                                                                    x1 y1\n                         Successive Halving Optimizer            â€¦                          tho                                                 CoT\n        Dvalid                                                                                     xn yn                         y\n                1                 c1         â„“1                    3                 x\n                  â€¦    â€¦\n                                                 Ap*\n          loss                             ck          â„“k                                                   traj1      tho\n       function                                 ReWOO  â€¦       act1      obs1       y\n                                                                                                             trajn   â€¦    â€¦                                     4                                                                                         x       actm     obsm\n                              defs:           Solution\n                                 prompt_pattern:                                                 react\n                                 num_demonstrations:                                                     5                                                      traj1\n                                 system_prompt:                                                granite_tools                     ReAct  â€¦       tho      obs       y                                 demonstrations:                                                                                                          act                                                                                                             trajn                                    data:         Dtest   5                                    - - question: Rita put a $120 elliptical machine [â€¦]                                                                                         x\n                                      - thought: The down payment Rita made was [â€¦]\n                                      - action: '{\"name\":\"Calculator\",\"arguments\":[â€¦]'\n                                      - observation: 60\n                                      - [â€¦]\n\n                                 Figure 2: Overview of our approach.\n\n\nsampling to always include one zero-shot candidate, just in case that baseline turns out to be the\nbest-performing configuration.\n    (3) The pattern library consists of four PDL functions. Zero-shot is a baseline that simply\nprompts the LLM with ğ‘¥and expects it to return ğ‘¦. CoT refers to chain-of-thought (Wei et al.\n2022) with in-context learning (Brown et al. 2020): the input includes a few ğ‘¥ğ‘–ğ‘¦ğ‘–pairs before the\nactual question ğ‘¥, and the output includes some reasoning thought tho before the actual answer ğ‘¦.\nReWOO (Xu et al. 2023) refers to reasoning without observations. Here, the few-shot samples are\ntrajectories trajğ‘–. A trajectory trajğ‘–consists of steps for a particular example problem instance in\nğ·train, e.g., in the case of ReWOO, tho and act steps and their contents. In contrast to ğ‘¥ğ‘–ğ‘¦ğ‘–pairs, trajğ‘–\nmay contain many tho, act and, depending on pattern, obs steps, before reaching the solution ğ‘¦ğ‘–.\nThe first LLM call in ReWOO generates one reasoning thought tho and multiple actions actğ‘–. The\nPDL code executes each of the actions as a tool call to obtain the corresponding observations obsğ‘–.\nA final model call generates the answerğ‘¦based on the observations. Finally, the ReAct pattern (Yao\net al. 2023) starts with few-shot trajectory examples trajğ‘–(brief example visible under Solution ğ´âˆ—ğ‘\nin Figure 2) and the question ğ‘¥, and then enters a TAO (thought, action, observation) loop. In each\nloop iteration, the LLM generates tho and act, then the PDL code executes the action as a tool call,\nand feeds the tool output back as an observation obs. A special Finish action breaks out of the loop\nto return the answer ğ‘¦.\n   Once inputs (1), (2), and (3) are in place, the suc-     1  text:\ncessive halving optimizer runs in a loop.    It starts     2   - include: ../../tools.pdl\n                                                                                              3   - include: ../../ReAct.pdl\nwith a small subset ğ·ğ‘£âŠ‚ğ·valid and many candidates     4   - call: ${ react }\nC = {ğ‘1, . . . ,ğ‘ğ‘˜} âŠ†AP sampled from the search space.     56      args:def: ANSWER\nEach iteration uses ğ·ğ‘£to evaluate the corresponding     7        task: \"Question: ${ question }\"\n                                                                                              8                                                                                    ${                                                                                       model                                                                                             }                                                                             model:losses â„“1, . . . , â„“ğ‘˜. Then each iteration keeps the ğ‘˜2 can-                                                                                              9                                                                                       tools                                                                                             }                                                                                  tools: ${\ndidates with the smallest L while doubling the size of    10        trajectories: ${ demonstrations }\n                                                                                             11   - \"\\nThe answer is ${ ANSWER.answer }\"the validation subset ğ·ğ‘£.  See Â§ 8.3 for the algorithm.\n(4) After the last iteration, the best remaining candidate\nis the solution ğ´âˆ—ğ‘. This solution is a set of PDL defini-  Figure 3: Basic example of PDL program\ntions with concrete values for the optimization variables,           using the ReAct pattern.\n\n\n\n                                                                                            4\n\ne.g., in Figure 2, num_demonstrations: 5 and demonstrations: ... a list of ReAct trajectories. (5) This pro-\n   gram can be used on the test set ğ·test. For instance, Figure 3 shows a call to the ReAct function\n    that passes ${demonstrations} from ğ´âˆ—ğ‘as an argument.\n\n 4 Methodology\n    This section describes the datasets used, the tools available to agents, and our experimental setup,\n    including how we construct agent trajectories that demonstrate tool use for each dataset.\n\n4.1 Datasets\n   We selected datasets that are widely used in the literature, span diverse tools and domains, and\n    are representative of tool categories frequently studied in prior work (e.g., calculator, search, code\n    execution). In our experiments, each dataset has three disjoint splits: ğ·train to sample few-shot\n    samples from, ğ·valid to evaluate candidates during optimization, and ğ·test to evaluate the final\n    chosen solution upon completion of optimization.\n\n   GSM8K. The Grade School Math (Cobbe et al. 2021) dataset consists of 8,792 grade school math\n    problems. We sample 1,024 problems without replacement from the train set to use as our ğ·valid\n     set, and 1,024 from the test set (consisting of 1,319 samples) to use as our ğ·test. This leaves a ğ·train\n    of 6,449 problems. Each problem consists of a word problem ğ‘¥such as â€œWhat is fifteen more than a\n    quarter of 48?â€, a sequence tho of reasoning steps, and finally a plain numeric answer ğ‘¦following\n    a special delimiter. For the CoT prompt pattern, we include these reasoning steps directly in the\n    demonstrations. We use a regular expression to extract the numerical answer from the model\n    solution, and define a correct solution as an exact match to the ground truth answer.\n\n   GSM-Hard. Gao et al. (2023) introduce a derivative of GSM8K with variables randomly changed to\n    large numbers, with 1,319 samples. Unfortunately, GSM-Hard had 132 samples where the ground\n    truth was incorrect, and hence we excluded those samples. We split the single set into equally\n    sized ğ·valid and ğ·test (ğ‘›= 594), and use the GSM8K training set (6,449 samples) described above\n    for ğ·train (cross transfer). We use the same correctness criterion as for GSM8K.\n\n   FEVER. The Fact Extraction and VERification dataset (Thorne et al. 2018) is a question-answering\n    dataset structured around fact-checking. The original dataset contained 185,445 claims that are\n     true, false, or unverifiable, and associated with human annotated supporting, refuting, or neu-\n     tral sentences and their Wikipedia article of origin. We follow the widely used derivative of this\n   benchmark in BIG-bench (Srivastava et al. 2023), which reformulates it into a true-or-false task by\n    removing unverifiable claims. We sample 1,024 claims from the train set as ğ·valid and 1,024 from\n    the test set as ğ·test. This leaves a ğ·train of 5,696 claims. BIG-bench also does not include the sup-\n    porting or refuting sentences, which we recover by joining on the original dataset, for use e.g., as\n   CoT demonstrations. To assess correctness, we check for the presence of â€œtrueâ€ and â€œfalseâ€ in the\n     final line of the model response. If neither or both are present, we deem the response as incorrect,\n   and otherwise the correctness is a direct match to the ground truth â€œtrueâ€ or â€œfalseâ€.\n\n   MBPP+. MBPP (Austin et al. 2021) is a dataset of 974 mostly basic Python problems, with each\n    example consisting of a natural language problem specification ğ‘¥for a self-contained Python func-\n    tionğ‘¦, along with a single test case. Each problem has an extended set of test cases used for evalua-\n     tion, which are not shown to the model. Liu et al. (2023) found that MBPP test cases are incomplete,\n    allowing proposed solutions to pass as correct, despite not matching the problem specification.\n    Therefore, our experiments are based on MBPP+, which contains a subset of the problems, but a\n   more complete set of test cases for each problem. We use these test cases to assess the correctness\n    of the proposed solutions. We use the 374 examples from the original MBPP dataset as ğ·train, and\n     split MBPP+ into ğ·test (224 samples) and ğ·valid (39 samples) based on which split they were in\n\n\n                                                                                                5\n\nMBPP. The discrepancy in number of MBPP+ samples is due to the exclusion of samples found in\n    the MBPP train set, to avoid data contamination. Our ReAct implementation follows Wang et al.\n     (2024). We do not implement ReWOO as it is not reactive, i.e., cannot include execution feedback.\n\n4.2 Tools\n   Our prompt library represents actions, i.e., tool calls, following the JSON tool calling schema (Ab-\n    delaziz et al. 2024). An action is represented as a JSON object with name and arguments mapping.\n    For example, {\"action\": \"Calc\", \"arguments\": {\"expr\": \"48/4\"}} represents a call to a calculator with the\n    expression to evaluate. The PDL functions implementing the patterns (see Figure 2) accept a list\n    of tool definitions as an argument. Each element of that list is itself a PDL function. As both the\n    agents and tools are implemented in PDL, the set of tools for a given task could itself be made a\n    search space dimension, which we leave to future work.\n\n    Calculator. For math datasets, we give the agentic approaches access to the Calc tool. This tool\n    evaluates a cleaned (e.g., replacement of ^ with âˆ—âˆ—) expression with SymPy, returning the result.\n    In case of error, the function returns a warning that the expression was invalid, which may help\n    the agent recover from invalid input.\n\n    Search. For fact verification, we provide access to the Search tool, which returns the summary\n    of the first Wikipedia search result for the query.  If no results are found, a hint to try again is\n    returned, or if the title is too ambiguous, a list of possible disambiguations.\n\n    Execute. We implement a programming agent for the code generation task, which can execute\n    arbitrary code surrounded in XML-style <execute> tags. This tool executes the code in a Python\n     shell, which returns the result of the final expression. This allows the agent to test its proposed\n    solution against the given test case before submitting its solution.\n\n    Finish. The most basic action is the Finish action, or <solution> tag for the coding agent. This\n    ends the agentâ€™s trajectory and results in the agent returning the value as the solution.\n\n4.3 Experimental Setup\n   We evaluate the efficacy of our approach by running an optimization process to completion for\n    each model & dataset pair, and subsequently comparing the task accuracy. As a baseline, we\n    evaluate each model in a zero-shot setting. This setting reflects the minimal effort approach by a\n    user or developer, where they do not include any demonstrations with their query or task. As no\n   model we investigate was specifically trained to create agentic trajectories in a zero-shot setting,\n      it is not feasible to create a zero-shot setting under ReAct or ReWOO.\n       For the optimization process, we used an initial candidate set C of 100 candidate prompt con-\n    figurations ğ‘ğ‘–= âŸ¨ğ´, ğ‘âŸ©âˆˆC per experiment. We fixed the size of the initial validation subset\n     ğ‘‘valid âŠ†ğ·valid to 16. We define L(ğ‘ğ‘–, ğ‘‘valid) = âˆ’Accuracy(ğ‘ğ‘–, ğ‘‘valid) for a given candidate ğ‘ğ‘–, where\n    accuracy is the fraction in ğ‘‘valid of correctly solved problems by ğ‘ğ‘–as per the dataset definition of\n    correctness. For each candidate ğ‘ğ‘–, ğ‘= âŸ¨ğ‘›,ğ‘‘train, instrâŸ©âˆˆP where ğ‘›few-shot samples or trajecto-\n     ries ğ‘‘train âˆˆ(ğ·train)ğ‘›are randomly sampled with replacement. The number of possible values for\n  P is combinatorially large and depends on the dataset ğ·train used. Finally, upon completion of an\n    optimization process, the optimal candidate is evaluated on ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡.\n\n    Constructing Trajectories. As we are also optimizing over agentic trajectories, we also need a\n    set of trajectories to sample few-shot samples from. To achieve this, we create a basic agentic\n    trajectory trajğ‘–for each training example âŸ¨ğ‘¥ğ‘–,ğ‘¦ğ‘–âŸ©, following a rule-based transformation. We design\n   and apply a template to each dataset, which is relatively simple and easy to implement for other\n    datasets (details are provided in Â§ 8.5). Prior work has introduced approaches to bootstrapping\n    trajectories e.g., in software engineering (Pan et al. 2024), tool use demonstrations (Li et al. 2025),\n\n\n                                                                                                6\n\nand reasoning paths (Zelikman et al. 2022), which could be applied to this problem. While we\n  acknowledge the shortcomings of manual template construction, we argue this approach has two\n   strengths:  it is generalizable in the sense that templates can be mixed and matched, and that\n   the trajectories are directly based on the datasets used. Additionally, we wanted to work with\n  commonly used datasets that cover a variety of tools and domains, rather than emerging datasets\n  containing trajectories or tool use demonstrations.\n\n  Models. We aim to study models of various abilities, e.g., natural language or code, various cre-\n   ators, and various sizes, ranging from single digit billions of parameters up to the edge of feasibility\n  on consumer hardware. We include seven models available on inference service IBM watsonx3 in\n  our study. We select three generalist natural language instruction models, Llama 3.1 8B, 3.2 3B,\n  and 3.3 70B from the open-source and widely studied LLaMa family (Dubey et al. 2024). We further\n   select three models from Mishra et al. (2024), which predate Dubey et al. (2024) by approximately\n  3 months. We select Granite 13B Instruct V2 as a generalist model, and Granite 20B and 34B\n  Code Instruct as code models. We select Granite 3.1 8B as an additional generalist model (Gran-\n   ite Team 2024). All of our experiments use greedy decoding, i.e., no sampling, to limit the impact of\n  hyperparameter choice. The number of models we evaluate is limited by cost in $/token terms and\n  execution time. By studying various models, we demonstrate the generalizability of our approach.\n\n  Alternative Setups. We evaluate two alternative experimental setups. First, to investigate low-\n  resource scenarios, we examine whether performance on one dataset can be improved by using\n  demonstrations ğ·train from a similar dataset, while optimizing w.r.t. ğ·valid. For this experiment, we\n   investigate whether performance on GSM-Hard can be improved by using demonstrations from\n  GSM8K, while optimizing w.r.t. GSM-Hard ğ·valid. Second, to explore saving optimization costs, we\n   assess whether optimized prompt programs of one model can transfer well to a frontier model.\n  The intuition is that while that might not be the best program for the frontier model, it might at\n   least improve somewhat over the baseline. To this end, we evaluate the optimized PDL programs\n   of LLaMa 3.1 70B on OpenAIâ€™s gpt-4o-mini-2024-07-18, for each dataset.\n\n5 Results\n  This section describes the results of our empirical study to evaluate our AutoPDL approach and\n  answer the following research questions:\n\n   RQ 1: To what extent does AutoPDL improve accuracy, and how much does the best solution vary by\n    task and model?\n\n     RQ1 asks to what degree our AutoPDL approach can improve model performance over their\n   zero-shot baseline across a variety of commonly used benchmarks. We also seek to identify trends,\n    if any, in optimal configurations, e.g., whether more few-shots is always better, or whether certain\n  prompt patterns are particularly suited to certain problem domains.\n\n   RQ 2: Can AutoPDL make up for a missing few-shot example bank for a given task by reusing the example\n   bank from a similar task?\n\n     RQ2 investigates whether optimizing on one dataset using demonstrations from another, re-\n   lated, dataset can result in higher performance than using no demonstrations (zero-shot). This RQ\n   addresses a low-resource scenario in which a limited pool of demonstrations exists in one dataset,\n  but a dataset from a similar domain has a large pool.\n\n   RQ 3: Do solutions found by AutoPDL improve performance on frontier models, even when optimized\n    for open-source models?\n\n      3https://www.ibm.com/watsonx\n\n\n\n                                                                                               7\n\nIt can be expensive to run optimization against commercial frontier-model APIs. RQ3 assesses\nwhether optimized prompt programs are transferable to different (and likely stronger) models than\nthose they were optimized with.\n    Table 1 reports the results of our optimization and evaluation procedure. We performed three\ncomplete optimization runs for FEVER, GSM8K, and MBPP+, and report mean accuracy, standard\ndeviation in percentage points (i.e., absolute, not relative, uncertainty), and the pattern of the\nhighest scoring run. Across models and datasets, we generally find some improvement over the\nzero-shot baseline with few-shot chain-of-thought, or agentic patterns ReAct or ReWOO.\n\n        Table 1: Model accuracies across datasets for baseline (zero-shot) and optimized versions.\n\n\n                                                 Accuracy\n            Dataset        Model                                           Best Pattern    Runtime\n                                                    Zero-Shot   Optimized          Delta\n\n                        Granite 3.1 8B               72.9 %   (76.4 Â± 3.3) %   +3.5pp  ReWOO (5 shot)         13:53\n                        Granite 13B Instruct V2       6.5 %   (74.0 Â± 1.4) %  +67.5pp  ReWOO (3 shot)         08:28\n                        Granite 20B Code            39.7 %   (63.1 Â± 1.6) %  +23.4pp  CoT (3 shot)             12:03\n           FEVER   Granite 34B Code            56.4 %   (62.6 Â± 3.8) %   +6.2pp  CoT (3 shot)             10:07\n                  LLaMA 3.1 8B               68.5 %   (77.5 Â± 0.8) %   +9.0pp  CoT (3 shot)             05:06\n                  LLaMA 3.2 3B               38.0 %   (66.3 Â± 0.9) %  +28.3pp  ReWOO (5 shot)         10:10\n                  LLaMA 3.3 70B              67.6 %   (78.1 Â± 0.6) %  +10.5pp  ReWOO (3 shot)         21:27\n                        Granite 3.1 8B               74.2 %   (74.2 Â± 0.6) %   +0.0pp  Zero-Shot (Baseline)     08:56\n                        Granite 13B Instruct V2      23.0 %   (30.9 Â± 1.0) %   +7.9pp  CoT (3 shot)             09:20\n                        Granite 20B Code            68.7 %   (68.7 Â± 0.1) %   +0.0pp  Zero-Shot (Baseline)     09:27\n          GSM8K   Granite 34B Code            72.1 %   (72.1 Â± 0.1) %   +0.0pp  Zero-Shot (Baseline)     08:52\n                  LLaMA 3.1 8B               78.4 %   (85.3 Â± 0.6) %   +6.9pp  CoT (5 shot)             08:48\n                  LLaMA 3.2 3B               71.8 %   (75.3 Â± 0.4) %   +3.5pp  CoT (3 shot)             16:36\n                  LLaMA 3.3 70B              85.5 %   (95.4 Â± 0.2) %   +9.9pp  CoT (3 shot)             07:50\n                        Granite 3.1 8B               62.9 %   (62.9 Â± 0.0) %   +0.0pp  Zero-Shot (Baseline)     02:14\n                        Granite 13B Instruct V2      10.7 %   (19.2 Â± 1.2) %   +8.5pp  ReAct (5 shot)           04:02\n                        Granite 20B Code            51.8 %   (51.8 Â± 0.4) %   +0.0pp  Zero-Shot (Baseline)     03:43\n          MBPP+   Granite 34B Code            48.7 %   (61.3 Â± 1.0) %  +12.6pp  ReAct (3 shot)           04:54\n                  LLaMA 3.1 8B               61.2 %   (62.8 Â± 4.0) %   +1.6pp  ReAct (5 shot)           01:45\n                  LLaMA 3.2 3B               58.0 %   (58.0 Â± 0.4) %   +0.0pp  Zero-Shot (Baseline)     02:01\n                  LLaMA 3.3 70B              71.4 %   (71.4 Â± 0.0) %   +0.0pp  Zero-Shot (Baseline)     02:27\n\nFEVER. We observed the minimum improvement in Granite 3.1 8B, with a 3.5 percentage\npoint (pp) improvement, and a maximal improvement of 67.5pp for Granite 13B Instruct V2.\nIn terms of prompt pattern, CoT and ReWOO are equally represented. ReAct was not the optimal\nfor any of the models. Interestingly, the largest model (LLaMa 3.3 70B) benefited by 10.5pp from\n3-shot ReWOO. FEVER runtimes are generally higher than the other benchmarks, likely due to the\nlarge number of tokens involved by including Wikipedia content.\n\nGSM8K. The highest improvement recorded (9.9pp) was for LLaMa 3.3 70B using 3-shot CoT,\nwhile the minimum improvement of 3.5pp was in LLaMa 3.2 3B using 3-shot CoT. ReAct and\nReWOO were not the optimal for any model. For Granite 3.1 8B, Granite 20B Code, and Gran-\nite 34B Code, no improvement over the zero-shot baseline was identified. This was somewhat\nsurprising, as generally including even some few-shot samples improves performance in LLMs.\n\nMBPP+. Several models benefited from execution feedback, as 3 out of 7 had ReAct as the opti-\nmal prompt pattern (ReWOO was excluded as described in Â§ 4.3). The greatest improvement of\n12.6pp was in Granite 34B Code, and 8.5pp in Granite 13B Instruct V2, likely due to its poor\nprogramming performance as a generalist, non-code model. In contrast, the smaller LLaMa 3.1\n8B model had high zero-shot performance of 61.2%, yet still improved by up to 6.2pp (1.6pp on\n\n\n\n                                                                                            8\n\naverage) with ReAct. No improvement was observed for Granite 3.1 8B, Granite 20B Code, or\n   the other LLaMa models.\n\n                 Table 2: Model accuracies on GSM-Hard for cross optimization experiment.\n\n\n                                                      Accuracy\n                 Dataset        Model                                              Best Pattern       Runtime\n                                                           Zero-Shot   Optimized         Delta\n\n                               Granite 3.1 8B               36.0 %   (37.8 Â± 0.8) %  +1.8pp  ReAct (5 shot, Granite Tools)     23:34\n                               Granite 13B Instruct V2       4.4 %   (6.2 Â± 0.7) %   +1.8pp  CoT (5 shot)                     10:26\n                               Granite 20B Code            28.8 %   (27.2 Â± 4.5) %  +0.0pp  Zero-Shot (Baseline)             11:14\n               GSM-Hard\n                               Granite 34B Code            27.9 %   (31.0 Â± 0.9) %  +3.0pp  CoT (3 shot)                     10:20\n                       LLaMA 3.2 3B               26.3 %   (26.8 Â± 0.6) %  +0.5pp  CoT (5 shot)                     17:08\n                       LLaMA 3.3 70B              47.3 %   (53.8 Â± 0.4) %  +6.5pp  CoT (5 shot)                     11:03\n\n\n                     Table 3: Model accuracy for GPT-4o-mini cross experiment results\n\n\n                                                Accuracy\n                     Dataset     Model                                      Best Pattern\n                                                     Zero-Shot   Optimized   Delta\n\n                  FEVER      GPT-4o-mini      83.7 %   87.7 %    +4.0pp  CoT (3 shot)\n\n                  GSM-Hard  GPT-4o-mini      45.6 %   54.9 %    +9.3pp  ReAct (5 shot, Granite LLaMa)\n\n                GSM8K     GPT-4o-mini      77.8 %   90.9 %   +13.1pp  CoT (5 shot)\n\n                 MBPP+     GPT-4o-mini      72.3 %   72.3 %    +0.0pp  Zero-Shot (Baseline)\n\n  Missing Few-Shot Example Bank. We optimized the PDL program for GSM-Hard, using GSM8K\n  demonstrations, three times and report results in Table 2. We found that in most cases, GSM8K\n  demonstrations were at least not harmful for models on GSM-Hard, with up to 6.5pp improvement\n   for LLaMa 3.3 70B using 5-shot CoT.\n\n  Commercial Frontier Model. To assess whether performance gains in one model can be\n  achieved in another, we evaluate the optimized PDL programs of LLaMa 3.1 70B on OpenAIâ€™s\n  gpt-4o-mini-2024-07-18 and report results in Table 3 (we did not use LLaMa 3.3 70B here be-\n  cause we did this experiment earlier and did not have the time and resources to repeat it for the\n   final version of this paper). For all dataset/prompt pattern pairs that resulted in improvement for\n  LLaMa 3.1 70B, we found a surprising improvement in GPT4o-mini of at least 4pp on FEVER us-\n  ing 3-shot CoT, 9.3pp on GSM-Hard (using GSM8K demonstrations) with 5-shot ReAct (Granite\n  LLaMa instructions), and up to 13.1pp on GSM8K using 5-shot CoT. This suggests that optimizing\n   for an open-source model can also benefit a closed-source model.\n\n\n6 Related Work\n  The closest related work is on prompt optimization. APE starts with an LLM-generated set of\n  candidate prompts, then performs rejection sampling based on evaluation on a subset of data (Zhou\n   et al. 2023). ZOPO incorporates a Neural Tangent Kernel-based derived Gaussian process into\n  standard zeroth-order optimization for an efficient search of a locally-optimal instruction (Hu et\n   al. 2024). Unlike our approach, neither APE nor ZOPO optimize few-shot samples. Aviary can also\n   jointly optimize over prompt pattern, instruction, and few-shot examples (Narayanan et al. 2024).\n  However, it would require the definition of a custom operator. CEDAR uses a demonstration pool,\n  from which it retrieves few-shot examples at query time (Nashid et al. 2023). Unlike our approach,\n   these few-shot samples are retrieved on a per-inference basis, not optimized ahead-of-time.\n     EASE leverages embeddings to represent few-shot examples, and uses a neural bandit algo-\n  rithm to find an ordered set that performs well for test queries from a given task (Z. Wu et al.\n   2024). An extension of their approach jointly optimizes demonstrations and instructions. How-\n   ever, the approach requires both an additional embedding model, and the training of a new model\n\n\n\n                                                                                               9\n\nto predict validation scores from embeddings. Unlike our approach, EASE does not optimize over\n   agentic patterns.\n     DSPy optimizes instructions and few-shot samples for a chain of LLM calls (Khattab et al.\n   2024) (not just a single call like APE or CEDAR). Also, DSPy takes away control over the exact\n  prompt from the programmer, which our approach preserves. Similarly to DSPy, TextGrad also\n  optimizes a chain of LLM calls, by using LLMs to back-propagate modifications to instructions\n   in prompts (Yuksekgonul et al. 2025). However, unlike our approach, neither of these optimize\n   agentic patterns.\n    BPO trains a sequence-to-sequence model on prompts augmented by an LLM incorporating\n  human preferences, producing a model that improves given input prompts (Cheng et al. 2024).\n  APOHF introduce a strategy to select a pair of prompts to query the user for preference feedback,\n  which they use to optimize LLM-generated instructions on a validation set (Lin et al. 2024). How-\n   ever, neither of these approaches explicitly optimize demonstrations or agentic patterns. EvoAgent\n  optimizes the instructions of a population of agents via crossover, mutation, and selection (Yuan\n   et al. 2024). It then forms an ensemble from the final, fittest, population. GPTSwarm represents\n  each agent as a graph, then freezes intra-agent edges and optimizes the placement of additional\n   inter-agent edges (Zhuge et al. 2024). Unlike our approach, neither EvoAgent nor GPTSwarm\n  optimize the agentic pattern inside individual agents, nor do they optimize few-shot samples.\n     Another closely related field of study is AutoML. Auto-sklearn (Feurer et al. 2015) used\n  Bayesian optimization to jointly perform both algorithm selection and hyperparameters of a scikit-\n   learn pipeline (Buitinck et al. 2013). While different, we see some analogy between algorithms and\n   agentic patterns, and between hyperparameters and few-shot samples. DAUB first evaluates many\n  candidate models on a small amount of data, then successively reduces candidates and increases\n  data to ultimately pick a strong model (Sabharwal et al. 2016). The successive-halving algorithm\n   takes a similar approach (Jamieson et al. 2016). Our approach is inspired by the same incremental\n  data allocation idea. While both randomized search and Bayesian optimization are popular in Au-\n  toML, there are also more intricate approaches. For instance, TPOT uses genetic algorithms (Olson\n   et al. 2016), and AlphaD3M uses Monte-Carlo tree search (Drori et al. 2018). We chose to start with\n  a simpler technique that depends less on a well-behaved optimization space. That said, exploring\n  more advanced AutoML optimizers could be fruitful future work for AutoPDL. Lale (Baudart et al.\n   2021) treats AutoML as a source-to-source optimization, similar to this paper, but unlike AutoPDL,\n    it has not been used to optimize agentic patterns or prompts.\n\n7 Conclusion\n  We present our AutoPDL approach for jointly optimizing prompting patterns and textual prompts\n   for large language models, addressing the challenges associated with manual prompt engineering.\n  By formulating the optimization as a discrete search over both agentic and non-agentic patterns,\n  combined with instructions and few-shot samples, we leveraged successive halving to efficiently\n  navigate this search space. Our evaluation across various datasets (FEVER, GSM8K, GSM-Hard,\n  and MBPP+) and multiple models (from the LLaMA, Granite, and GPT families) demonstrates sub-\n   stantial accuracy improvements, up to 67.5 percentage points, and affirms that no single prompting\n   strategy universally outperforms others across tasks and models. Additionally, generating code in\n  a YAML-based prompt programming language (PDL) makes it executable, easy to modify, and\n   readable by humans, supporting practical adoption and adaptation.\n\n\n\n\n\n                                                                                              10\n\nReferences\n\nAbdelaziz, I., Basu, K., Agarwal, M., Kumaravel, S., Stallone, M., Panda, R., Rizk, Y., Bhargav, G.,\n    Crouse, M., Gunasekara, C., Ikbal, S., Joshi, S., Karanam, H., Kumar, V., Munawar, A., Neelam,\n     S., Raghu, D., Sharma, U., Soria, A. M., Sreedhar, D., Venkateswaran, P., Unuvar, M., Cox, D.,\n   Roukos, S., Lastras, L., and Kapanipathi, P. (2024). Granite-Function Calling Model: Introducing\n    Function Calling Abilities via Multi-task Learning of Granular Tasks. url: https://arxiv.org/abs/\n    2407.00121 (cit. on p. 6).\n\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry,\n    M., Le, Q., and Sutton, C. (2021). Program Synthesis with Large Language Models. url: http:\n    //arxiv.org/abs/2108.07732 (cit. on p. 5).\n\nBaudart, G., Hirzel, M., Kate, K., Ram, P., Shinnar, A., and Tsay, J. (2021). â€œPipeline Combinators for\n   Gradual AutoMLâ€. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 19705â€“\n   19718 (cit. on p. 10).\n\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam,\n     P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,\n   Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,\n     S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.\n    (2020). Language Models are Few-Shot Learners. url: https://arxiv.org/abs/2005.14165 (cit. on\n    pp. 1, 4).\n\nBuitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer,\n     P., Gramfort, A., Grobler, J., Layton, R., VanderPlas, J., Joly, A., Holt, B., and Varoquaux, G.\n    (2013). API Design for Machine Learning Software: Experiences from the scikit-learn Project. url:\n    https://arxiv.org/abs/1309.0238 (cit. on p. 10).\n\nCheng, J., Liu, X., Zheng, K., Ke, P., Wang, H., Dong, Y., Tang, J., and Huang, M. (2024). â€œBlack-Box\n   Prompt Optimization: Aligning Large Language Models without Model Trainingâ€. In: Annual\n   Meeting of the Association for Computational Linguistics (ACL), pp. 3201â€“3219 (cit. on p. 10).\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton,\n      J., Nakano, R., Hesse, C., and Schulman, J. (2021). Training Verifiers to Solve Math Word Problems.\n    url: http://arxiv.org/abs/2110.14168 (cit. on p. 5).\n\nDrori, I., Krishnamurthy, Y., Rampin, R., Lourenco, R. d. P., Ono, J. P., Cho, K., Silva, C., and Freire, J.\n    (2018). â€œAlphaD3M: Machine Learning Pipeline Synthesisâ€. In: Workshop on Automatic Machine\n   Learning (AutoML) (cit. on p. 10).\nDubey, A. et al. (2024). The Llama 3 Herd of Models. url: http://arxiv.org/abs/2407.21783 (cit. on\n    p. 7).\n\nFeurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., and Hutter, F. (2015). â€œEfficient\n   and Robust Automated Machine Learningâ€. In: Conference on Neural Information Processing\n   Systems (NIPS), pp. 2962â€“2970 (cit. on p. 10).\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. (2023). â€œPAL:\n   Program-aided Language Modelsâ€. In: International Conference on Machine Learning (ICML),\n    pp. 10764â€“10799 (cit. on p. 5).\n\nGranite Team, I. (2024). Granite 3.0 Language Models (cit. on p. 7).\nHu, W., Shu, Y., Yu, Z., Wu, Z., Lin, X., Dai, Z., Ng, S.-K., and Low, B. K. H. (2024). â€œLocalized\n   Zeroth-Order Prompt Optimizationâ€. In: Conference on Neural Information Processing Systems\n    (NeurIPS), pp. 86309â€“86345 (cit. on p. 9).\n\n\n                                                                                           11\n\nJamieson, K. and Talwalkar, A. (2016). â€œNon-stochastic Best Arm Identification and Hyperparame-\n    ter Optimizationâ€. In: Conference on Artificial Intelligence and Statistics (AISTATS), pp. 240â€“248\n     (cit. on pp. 2, 10, 17).\n\nKhattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., A, S. V., Haq, S., Sharma,\n    A., Joshi, T. T., Moazam, H., Miller, H., Zaharia, M., and Potts, C. (2024). â€œDSPy: Compiling\n    Declarative Language Model Calls into Self-Improving Pipelinesâ€. In: International Conference\n   on Learning Representations (ICLR) (cit. on pp. 1, 10).\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). â€œLarge Language Models\n   Are Zero-Shot Reasonersâ€. In: Conference on Neural Information Processing Systems (NeurIPS),\n    pp. 22199â€“22213 (cit. on p. 1).\n\nLi, C., Xue, M., Zhang, Z., Yang, J., Zhang, B., Wang, X., Yu, B., Hui, B., Lin, J., and Liu, D. (2025).\n   START: Self-taught Reasoner with Tools. url: https://arxiv.org/abs/2503.04625 (cit. on pp. 6, 18).\nLin, X., Dai, Z., Verma, A., Ng, S.-K., Jaillet, P., and Low, B. K. H. (2024). Prompt Optimization with\n   Human Feedback. url: http://arxiv.org/abs/2405.17346 (cit. on p. 10).\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. (2023). â€œIs Your Code Generated by ChatGPT Really\n    Correct? Rigorous Evaluation of Large Language Models for Code Generationâ€. In: Conference\n   on Neural Information Processing Systems (NeurIPS), pp. 21558â€“21572 (cit. on p. 5).\nMishra, M., Stallone, M., Zhang, G., Shen, Y., Prasad, A., Soria, A. M., Merler, M., Selvam, P., Suren-\n    dran, S., Singh, S., Sethi, M., Dang, X.-H., Li, P., Wu, K.-L., Zawad, S., Coleman, A., White, M.,\n    Lewis, M., Pavuluri, R., Koyfman, Y., Lublinsky, B., de Bayser, M., Abdelaziz, I., Basu, K., Agar-\n    wal, M., Zhou, Y., Johnson, C., Goyal, A., Patel, H., Shah, Y., Zerfos, P., Ludwig, H., Munawar, A.,\n    Crouse, M., Kapanipathi, P., Salaria, S., Calio, B., Wen, S., Seelam, S., Belgodere, B., Fonseca, C.,\n    Singhee, A., Desai, N., Cox, D. D., Puri, R., and Panda, R. (7, 2024). Granite Code Models: A Family\n    of Open Foundation Models for Code Intelligence. Version 1. url: http://arxiv.org/abs/2405.04324\n     (cit. on p. 7).\n\nMoura, J. (2023). CrewAI: Framework for orchestrating role-playing, autonomous AI agents. url:\n    https://github.com/crewAIInc/crewAI (visited on 06/06/2025) (cit. on p. 1).\n\nNarayanan, S., Braza, J. D., Griffiths, R.-R., Ponnapati, M., Bou, A., Laurent, J., Kabeli, O., Wellawatte,\n    G., Cox, S., Rodriques, S. G., and White, A. D. (2024). Aviary: Training Language Agents on\n    Challenging Scientific Tasks. url: http://arxiv.org/abs/2412.21154 (cit. on p. 9).\nNashid, N., Sintaha, M., and Mesbah, A. (2023). â€œRetrieval-Based Prompt Selection for Code-Related\n   Few-Shot Learningâ€. In: International Conference on Software Engineering (ICSE), pp. 2450â€“2462\n     (cit. on p. 9).\n\nOlson, R. S., Urbanowicz, R. J., Andrews, P. C., Lavender, N. A., Kidd, L. C., and Moore, J. H. (2016).\n   â€œAutomating Biomedical Data Science Through Tree-Based Pipeline Optimizationâ€. In: Euro-\n   pean Conference on the Applications of Evolutionary Computation (EvoApplications), pp. 123â€“137\n     (cit. on p. 10).\n\nPan, J., Wang, X., Neubig, G., Jaitly, N., Ji, H., Suhr, A., and Zhang, Y. (2024). Training Software\n    Engineering Agents and Verifiers with SWE-Gym. url: http://arxiv.org/abs/2412.21139 (cit. on\n    p. 6).\n\nSabharwal, A., Samulowitz, H., and Tesauro, G. (2016). â€œSelecting Near-Optimal Learners via Incre-\n   mental Data Allocationâ€. In: Conference on Artificial Intelligence (AAAI), pp. 2007â€“2015 (cit. on\n    p. 10).\n\n\n\n\n\n                                                                                           12\n\nSchluntz, E. and Zhang, B. (2024). Building effective agents. url: https://www.anthropic.com/\n    research/building-effective-agents (visited on 06/06/2025) (cit. on p. 1).\n\nSrivastava, A. et al. (2023). â€œBeyond the Imitation Game: Quantifying and Extrapolating the Capa-\n     bilities of Language Modelsâ€. Transactions on Machine Learning Research (TMLR) (cit. on p. 5).\nThorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. (2018). FEVER: A Large-Scale Dataset\n    for Fact Extraction and VERification. url: http://arxiv.org/abs/1803.05355 (cit. on p. 5).\nThornton, C., Hutter, F., Hoos, H. H., and Leyton-Brown, K. (2013). â€œAuto-WEKA: Combined Selec-\n    tion and Hyperparameter Optimization of Classification Algorithmsâ€. In: Conference on Knowl-\n   edge Discovery and Data Mining (KDD), pp. 847â€“855 (cit. on p. 1).\nVaziri, M., Mandel, L., Spiess, C., and Hirzel, M. (2024). PDL: A Declarative Prompt Programming\n   Language. url: http://arxiv.org/abs/2410.19135 (cit. on p. 2).\nWang, X., Wang, Z., Liu, J., Chen, Y., Yuan, L., Peng, H., and Ji, H. (2024). MINT: Evaluating LLMs in\n    Multi-turn Interaction with Tools and Language Feedback. url: http://arxiv.org/abs/2309.10691\n     (cit. on pp. 6, 17, 18).\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. (2022).\n   â€œChain-of-Thought Prompting Elicits Reasoning in Large Language Modelsâ€. In: Conference on\n   Neural Information Processing Systems (NeurIPS), pp. 24824â€“24837 (cit. on pp. 1, 4).\nWillard, B. T. and Louf, R. (2023). Efficient Guided Generation for Large Language Models. url:\n    https://arxiv.org/abs/2307.09702 (cit. on p. 3).\n\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., Awadal-\n    lah, A. H., White, R. W., Burger, D., and Wang, C. (2023). AutoGen: Enabling Next-Gen LLM\n    Applications via Multi-Agent Conversation. url: https://arxiv.org/abs/2308.08155 (cit. on p. 1).\nWu, Z., Lin, X., Dai, Z., Hu, W., Shu, Y., Ng, S.-K., Jaillet, P., and Low, B. K. H. (2024). â€œPrompt\n   Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplarsâ€. In:\n    Conference on Neural Information Processing Systems (NeurIPS), pp. 122706â€“122740 (cit. on p. 9).\nXu, B., Peng, Z., Lei, B., Mukherjee, S., and Xu, D. (2023). ReWOO: Decoupling Reasoning from\n    Observations for Efficient Augmented Language Models. url: https://arxiv.org/abs/2305.18323\n     (cit. on pp. 1, 4).\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. (2023). â€œReAct: Syner-\n    gizing Reasoning and Acting in Language Modelsâ€. In: International Conference on Learning\n    Representations (ICLR) (cit. on pp. 1, 4).\nYuan, S., Song, K., Chen, J., Tan, X., Li, D., and Yang, D. (2024). EvoAgent: Towards Automatic Multi-\n   Agent Generation via Evolutionary Algorithms. url: https://arxiv.org/abs/2406.14228 (cit. on\n    p. 10).\n\nYuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Lu, P., Huang, Z., Guestrin, C., and Zou, J. (2025).\n   â€œOptimizing Generative AI by Backpropagating Language Model Feedbackâ€. Nature, 639(8055),\n    pp. 609â€“616 (cit. on p. 10).\n\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. D. (2022). â€œSTaR: Self-Taught Reasoner, Bootstrap-\n   ping Reasoning With Reasoningâ€. In: Conference on Neural Information Processing Systems\n    (NeurIPS) (cit. on p. 7).\nZhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2023). â€œLarge Language\n   Models are Human-Level Prompt Engineersâ€. In: International Conference on Learning Repre-\n    sentations (ICLR) (cit. on p. 9).\n\n\n\n                                                                                           13\n\nZhuge, M., Wang, W., Kirsch, L., Faccio, F., Khizbullin, D., and Schmidhuber, J. (2024). â€œGPTSwarm:\n   Language Agents as Optimizable Graphsâ€. In: International Conference on Machine Learning\n   (ICML) (cit. on p. 10).\n\n\n\n\n\n                                                                                           14\n\n8 Supplemental Material\n\n8.1 Concrete Prompt Example\n\n\n\n             1  defs:\n             2    prompt_pattern: cot\n             3   num_demonstrations: 2\n             4    demonstrations:\n             5      data:\n             6     - question: Tricia is a third of Amilia's age and Amilia is a quarter of Yorick's age. Yorick is twice\n             7         Eugene's age and Khloe is a third of Eugene's age. Rupert is 10 years older than Khloe but 2 years\n             8         younger than Vincent who is 22 years old. How old, in years, is Tricia?\n             9        reasoning: |-\n            10         Rupert is younger than Vincent by 2 years, so he is 22 years old - 2 years = <<22-2=20>>20 years old.\n            11         Khloe is 10 years younger than Rupert so she is 20 years old - 10 years = 10 years old.\n            12         Eugene is 3 times older than Khloe so he is 10 years old * 3 = <<10*3=30>>30 years old.\n            13         Yorick is twice Eugene's age so he is 30 years old * 2 = <<30*2=60>>60 years old.\n            14         Amilia is a quarter of Yorick's age so she is 60 years old / 4 = <<60/4=15>>15 years old.\n            15         Tricia is a third of Amilia's age so she is 15 years old / 3 = <<15/3=5>>5 years old.\n            16        answer: '5'\n            17     - question: Emmalyn decided to paint fences in her neighborhood for twenty cents per meter. If there were\n            18         50 fences in the neighborhood that she had to paint and each fence was 500 meters long, calculate\n            19         the total amount she earned from painting the fences.\n            20        reasoning: |-\n            21         The total length for the fifty fences is 50*500 = <<50*500=25000>>25000 meters.\n            22         If Emmalyn charged twenty cents to paint a meter of a fence, the total income she got from painting\n            23         the fences is $0.20*25000 =$5000\n            24        answer: '5000'\n            25    instruction: Answer the questions to the best of your abilities.\n            26  text:\n            27   - include: CoT.pdl\n            28   - \"${ instruction }\\n\\n\"\n            29   - call: ${ chain_of_thought }\n            30      args:\n            31        examples: ${ demonstrations }\n            32        question: ${ question }\n            33        model: ${ model }\n\n       Figure A1: Basic example of a concrete prompt configuration in PDL, using CoT pattern and two\n                  demonstrations.\n\n\n\n\n             1 Answer the questions to the best of your abilities.\n             2\n             3 Question: Tricia is a third of Amilia's age and Amilia is a quarter of Yorick's age. Yorick is twice Eugene's\n             4 age and Khloe is a third of Eugene's age. Rupert is 10 years older than Khloe but 2 years\n             5 younger than Vincent\n             6 who is 22 years old. How old, in years, is Tricia?\n             7 Answer: Let's think step by step. Rupert is younger than Vincent by 2 years, so he is 22 years\n             8 old - 2 years = <<22-2=20>>20 years old.\n             9 Khloe is 10 years younger than Rupert so she is 20 years old - 10 years = 10 years old.\n            10 Eugene is 3 times older than Khloe so he is 10 years old * 3 = <<10*3=30>>30 years old.\n            11 Yorick is twice Eugene's age so he is 30 years old * 2 = <<30*2=60>>60 years old.\n            12 Amilia is a quarter of Yorick's age so she is 60 years old / 4 = <<60/4=15>>15 years old.\n            13 Tricia is a third of Amilia's age so she is 15 years old / 3 = <<15/3=5>>5 years old.\n            14 The answer is 5\n            15\n            16 Question: Emmalyn decided to paint fences in her neighborhood for twenty cents per meter. If there were 50\n            17 fences in the neighborhood that she had to paint and each fence was 500 meters long, calculate the total\n            18 amount she earned from painting the fences.\n            19 Answer: Let's think step by step. The total length for the fifty fences is 50*500 = <<50*500=25000>>25000 meters.\n            20 If Emmalyn charged twenty cents to paint a meter of a fence, the total income she got from painting the\n            21 fences is $0.20*25000 =$5000\n            22 The answer is 5000\n            23\n            24 Question: ${ question }\n            25 Answer: Let's think step by step.\n\n       Figure A2: Corresponding rendered prompt configuration for Figure A1. With the exception of the\n                 non-rendered ${ question } variable, this is the input to the language model.\n\n       See Figure A1 and Figure A2.\n\n\n\n                                                                                               15\n\n8.2 Tool Calling Code\n\n\n\n                                   1  description: tool use\n                                   2  defs:\n                                   3    tools:\n                                   4      data:\n                                   5     - name: calc\n                                   6        description: Calculator function\n                                   7       arguments:\n                                   8         expr:\n                                   9           type: string\n                                  10            description: Arithmetic expression to calculate\n                                  11  text:\n                                  12 - role: system\n                                  13    text: You are Granite, developed by IBM. You are a helpful AI assistant\n                                  14     with access to the following tools. When a tool is required to answer\n                                  15     the user's query, respond with <|tool_call|> followed by a JSON list of\n                                  16     tools used. If a tool does not exist in the provided list of tools,\n                                  17     notify the user that you do not have the ability to fulfill the request.\n                                  18    contribute: [context]\n                                  19 - role: tools\n                                  20    text: ${ tools }\n                                  21    contribute: [context]\n                                  22 - \"Out of 1400 participants, 400 passed the test. What percentage is that?\\n\"\n                                  23 - def: actions\n                                  24    model: replicate/ibm-granite/granite-3.1-8b-instruct\n                                  25    parser: json\n                                  26    spec: [{ name: str, arguments: { expr: str }}]\n                                  27 - \"\\n\"\n                                  28 -  if: ${ actions[0].name == \"calc\" }\n                                  29    then:\n                                  30      lang: python\n                                  31      code: result = ${ actions[0].arguments.expr }\n\n\n                                  Figure A3: Basic example of a PDL program.\n\n       See Figure A3.\n\n\n8.3 Optimization\n\n\n    Algorithm 1 Successive Halving for PDL Optimization\n    Require: Program candidate set C, validation dataset ğ·valid, initial validation subset size ğ‘£min, max-\n      imum validation subset size ğ‘£max\n        1: ğ‘£â†ğ‘£min\n        2: while |C| > 1 do\n        3:    ğ‘‘valid â†first ğ‘£elements of ğ·valid s.t. ğ‘‘valid âŠ†ğ·valid and |ğ‘‘valid| = ğ‘£\n        4:    for each candidate ğ‘ğ‘–âˆˆC do\n        5:     Compute loss â„“ğ‘–â†L(ğ‘ğ‘–, ğ‘‘valid)\n        6:   end for\n        7:   C â†top âŒŠ|C|/2âŒ‹candidates with lowest loss\n        8:   ğ‘£â†min(ğ‘£max, 2 Â· ğ‘£)\n        9: end while\n      10: return Candidate in C with lowest loss\n\n\n       Figure A4: Illustration of the Successive Halving algorithm used to optimize the PDL program by\n                pruning poor candidates on progressively larger validation subsets.\n\n\n\n                                                                                               16\n\nFigure A4 describes our optimization algorithm, based on successive halving (Jamieson et al.\n    2016). The algorithm accepts a candidate set sampled from possible configurations and demon-\n     strations, a validation dataset to optimize against, an initial validation subset size, and a maximum\n    validation subset size. AutoPDL allows the user to specify these options in a YAML configuration\n      file, and ultimately saves its result as a PDL program. This source-to-source transformation en-\n    ables the user to modify both the search space and the resulting optimized PDL program, allowing\n    further modification and execution.\n\n8.4 Search Space\n   The search space is the Cartesian product of the following discrete variables, each taking one value\n    per candidate:\n\n     (1) ğ´âˆˆA = {Zero-Shot, CoT, ReWOO, ReAct}, i.e., the overall prompting pattern to apply.\n\n     (2) Number of demonstrations ğ‘›âˆˆ{0, 3, 5}. We selected these options as a representative sweep\n        across no supervision, moderate few-shot use, and an upper-end case (in terms of token win-\n       dow). We limited the search space to three options to avoid combinatorial explosion and limit\n       experiment cost.\n\n     (3)  If ğ´= ReAct, System prompt âˆˆ{Granite Tools, LLaMa  3, Granite LLaMa}. As the system\n       prompt instructs the model how to format tool calls, it only has an effect on benchmarks\n       with JSON tool calling (FEVER, GSM8K, and GSM-Hard) for candidates with the ReAct prompt\n        pattern. We note that only for MBPP+, ReWOO is not included as a prompt pattern, and that we\n       always include two trajectories displaying iterative refinement, i.e., an example of a solution\n         failing the example test case, followed by a passing solution, in line with Wang et al. (2024).\n       This effectively increases the number of trajectories to |ğ‘¡ğ‘Ÿğ‘ğ‘—| + 2.\n\n8.5 Agent Trajectory Construction\n   To optimize over agentic patterns and trajectories, we require a set of example trajectories to use\n    during optimization. For this purpose, we create a basic agentic trajectory trajğ‘–for each training\n    example âŸ¨ğ‘¥ğ‘–,ğ‘¦ğ‘–âŸ©, following a rule-based transformation outlined below.\n\n   GSM8K. To demonstrate tool use in ReAct, we instead derive a trajectory traj as follows. We\n    exploit the fact that there is at most one expression per reasoning step, by iterating through the\n     steps. At each step, we append a â€˜thoughtâ€™ to the trajectory, consisting of the text leading up to\n    the math expression, concatenated with a reflection â€˜I need to calculateâ€™. We append a calculator\n    tool call with the expression, and an â€˜observationâ€™, i.e., the result of the expression. Finally, we\n   append a thought â€˜The answer is  ...â€™, containing the ground truth answer, followed by the finish\n    action with the answer. We follow the same procedure to create ReWOO trajectories, except we\n    use slightly different wording, e.g., â€˜Calculate xyzâ€™ in place of â€˜I need to calculate xyzâ€™, and omit\n    the final thought and action.  Additionally, we use string substitution to replace any assumed\n    expression results in the trajectory with the corresponding variable.\n\n   FEVER. To produce agent trajectories, we iterate over each article associated with a claim, append\n    a thought â€˜I need to search for  ...â€™, followed by the action, an observation containing the article\n   summary, and finally a thought containing all the relevant sentences associated with that article\n    for that claim, which we repeat for each article associated with a claim. This procedure is not\n    ideal as there is no inherent order to the articles or sentences, even though there may be a natural\n    ordering following the annotatorâ€™s Wikipedia navigation. Finally, we append a thought â€˜The claim\n     is true/falseâ€™ and the finish action, both with the ground truth answer. For chain-of-thought, we\n    perform the same procedure except we only include the concatenated evidence sentences, as there\n     is no tool use.\n\n\n\n                                                                                               17\n\nMBPP+. To generate sample agent trajectories from the training set, we follow the agent pattern\n    (without feedback) in-context examples by Wang et al. (2024), which consists of the problem ğ‘¥, a\n    thought such as â€œThe intersection is elements that are in both listsâ€, an execute action that contains\n    proposed code and an assertion calling the proposed method with the test case input from the\n   prompt and comparing its output. This is then followed by an observation containing the execution\n     result, i.e., either â€œ[Executed Successfully with No Output]â€ or a stack traceback. This allows\n    the agent to iterate on solutions (up to five times in our implementation). We use the full MBPP\n    train set of 374 problems as ğ·train, and split the MBPP+ dataset into ğ·valid and ğ·test based on\n    problem id membership in MBPP, leaving 39 and 224 validation and test problems respectively.\n      To generate synthetic trajectories from the training set, we start with the natural language\n    specification and single test case (the prompt), append the thought â€œI should run a solution on\n    the test case before proposing a solution.â€, followed by the ground truth solution and substitute\n    in the prompt test case following the pattern [solution]res = ...;  assert res == ...,\n    â€Expected ... but got â€.format(res). Subsequently, we append the observation â€œ[Executed\n    Successfully with No Output]â€, the thought â€œThere is no more AssertionError.  I can\n    now submit the  solution.â€, and finally the solution action with the ground truth solution.\n    This naive approach allows us to provide demonstration trajectories, albeit simplistic ones that\n   assume the first solution is correct. Sampling a reflection or thought from a strong model may\n    be beneficial (Li et al. 2025), but we restrict our trajectories to rule based transformations. As\n   ReWOO is not reactive, i.e., without execution feedback, it does not make sense for MBPP. Hence,\n   we exclude it from our experiments.\n\n8.6 Results Plot\n\n                          FEVER         GSM8K         MBPP+         GSM-Hard\n                 Granite 3.1 8B\n         Granite 13B Instruct V2\n              Granite 20B Code\n              Granite 34B Code         Model               LLaMA 3.1 8B\n               LLaMA 3.2 3B\n              LLaMA 3.3 70B\n                           0  20 40 60 80 100 0  20 40 60 80 100 0  20 40 60 80 100 0  20 40 60 80 100\n\n       Figure A5: Comparison of optimized prompt program performance across models and datasets. Each\n                   barbell shows the accuracy improvement, if any, over the zero-shot baseline.\n\n        In Figure A5, we visualize the results from Table 1 and Table 2.\n\n\n\n\n\n                                                                                               18\n\n8.7 Accuracy vs. iterations\n    In Figure A6, we visualize the accuracy across candidates versus the iterations of the optimization\n    process, including a 95% confidence interval depicting the spread in accuracy across candidates.\n   The confidence interval is computed using mean and 1,000 bootstraps. As the iterations increase,\n    the number of candidates decreases, while the size of the validation set ğ·ğ‘£increases.\n\n\n\n\n\n                                                                                               19\n\nGSM8K                          FEVER                       MBPP+                         GSM-Hard\n   1.0\n\n   0.8\n   0.6                                                                                                                                                                                                                                                                                                                                          Granite\n                                                                                                                                                    20BAccuracy 0.4\n                                                                                                                                                                                                     Code\n   0.2\n\n   0.0\n\n   1.0\n\n   0.8\n   0.6                                                                                                                                                                                                                                            LLaMA\n                                                                                                                                                    3.1Accuracy 0.4\n                                                                                                  8B\n\n   0.2\n\n   0.0\n\n   1.0\n   0.8                                                                                                                                                                                                                                                                                                                                          Granite\n                                                                                                                                                    13BAccuracy 0.60.4                                                                                                                                                                                                                                                                                                                                                                                                          Instruct\n   0.2                                                                                              V2\n\n   0.0\n\n   1.0\n\n   0.8\n\n   0.6                                                                                                                                                                                                                                                                                                                                                        Granite\nAccuracy 0.4                                                                                                                                              3.1\n                                                                                                  8B\n\n   0.2\n\n   0.0\n\n   1.0\n\n   0.8\n   0.6                                                                                                                                                                                                                                                                                                                                          Granite\n                                                                                                                                                    34BAccuracy 0.4\n                                                                                                                                                                                                     Code\n   0.2\n\n   0.0\n\n   1.0\n\n   0.8\n   0.6                                                                                                                                                                                                                                            LLaMA\n                                                                                                                                                    3.2Accuracy 0.4\n                                                                                                  3B\n\n   0.2\n\n   0.0\n\n   1.0\n\n   0.8\n   0.6                                                                                                                                                                                                                                            LLaMA\n                                                                                                                                                    3.3Accuracy 0.4\n                                                                                                                                                    70B\n   0.2\n\n   0.0\n       0    1    2    3    4    5    0    1    2    3    4    5    0    1    2    3    4    5    0    1    2    3    4    5\n                        Iteration                                  Iteration                                  Iteration                                  Iteration\n\n          Figure A6: Accuracy vs. iterations with 95% confidence interval.\n\n\n\n\n                                                                                  20",
"headers": [
"arXiv:2504.04365v5  [cs.LG]  3 Nov 2025",
"AutoPDL: Automatic Prompt Optimization for LLM Agents"
],
"tables": [
"|Dataset|Model|Accuracy|Col4|Col5|Best Pattern|Runtime|\n|---|---|---|---|---|---|---|\n|Dataset|Model|Zero-Shot|Optimized|Delta|Delta|Delta|\n|FEVER<br>Granite 3.1 8B<br>72_._9 %<br>(76_._4 Â± 3_._3) %<br>+3.5pp<br>ReWOO (5 shot)<br>13:53<br>Granite 13B Instruct V2<br>6_._5 %<br>(74_._0 Â± 1_._4) %<br>+67.5pp<br>ReWOO (3 shot)<br>08:28<br>Granite 20B Code<br>39_._7 %<br>(63_._1 Â± 1_._6) %<br>+23.4pp<br>CoT (3 shot)<br>12:03<br>Granite 34B Code<br>56_._4 %<br>(62_._6 Â± 3_._8) %<br>+6.2pp<br>CoT (3 shot)<br>10:07<br>LLaMA 3.1 8B<br>68_._5 %<br>(77_._5 Â± 0_._8) %<br>+9.0pp<br>CoT (3 shot)<br>05:06<br>LLaMA 3.2 3B<br>38_._0 %<br>(66_._3 Â± 0_._9) %<br>+28.3pp<br>ReWOO (5 shot)<br>10:10<br>LLaMA 3.3 70B<br>67_._6 %<br>(78_._1 Â± 0_._6) %<br>+10.5pp<br>ReWOO (3 shot)<br>21:27|FEVER<br>Granite 3.1 8B<br>72_._9 %<br>(76_._4 Â± 3_._3) %<br>+3.5pp<br>ReWOO (5 shot)<br>13:53<br>Granite 13B Instruct V2<br>6_._5 %<br>(74_._0 Â± 1_._4) %<br>+67.5pp<br>ReWOO (3 shot)<br>08:28<br>Granite 20B Code<br>39_._7 %<br>(63_._1 Â± 1_._6) %<br>+23.4pp<br>CoT (3 shot)<br>12:03<br>Granite 34B Code<br>56_._4 %<br>(62_._6 Â± 3_._8) %<br>+6.2pp<br>CoT (3 shot)<br>10:07<br>LLaMA 3.1 8B<br>68_._5 %<br>(77_._5 Â± 0_._8) %<br>+9.0pp<br>CoT (3 shot)<br>05:06<br>LLaMA 3.2 3B<br>38_._0 %<br>(66_._3 Â± 0_._9) %<br>+28.3pp<br>ReWOO (5 shot)<br>10:10<br>LLaMA 3.3 70B<br>67_._6 %<br>(78_._1 Â± 0_._6) %<br>+10.5pp<br>ReWOO (3 shot)<br>21:27|FEVER<br>Granite 3.1 8B<br>72_._9 %<br>(76_._4 Â± 3_._3) %<br>+3.5pp<br>ReWOO (5 shot)<br>13:53<br>Granite 13B Instruct V2<br>6_._5 %<br>(74_._0 Â± 1_._4) %<br>+67.5pp<br>ReWOO (3 shot)<br>08:28<br>Granite 20B Code<br>39_._7 %<br>(63_._1 Â± 1_._6) %<br>+23.4pp<br>CoT (3 shot)<br>12:03<br>Granite 34B Code<br>56_._4 %<br>(62_._6 Â± 3_._8) %<br>+6.2pp<br>CoT (3 shot)<br>10:07<br>LLaMA 3.1 8B<br>68_._5 %<br>(77_._5 Â± 0_._8) %<br>+9.0pp<br>CoT (3 shot)<br>05:06<br>LLaMA 3.2 3B<br>38_._0 %<br>(66_._3 Â± 0_._9) %<br>+28.3pp<br>ReWOO (5 shot)<br>10:10<br>LLaMA 3.3 70B<br>67_._6 %<br>(78_._1 Â± 0_._6) %<br>+10.5pp<br>ReWOO (3 shot)<br>21:27|FEVER<br>Granite 3.1 8B<br>72_._9 %<br>(76_._4 Â± 3_._3) %<br>+3.5pp<br>ReWOO (5 shot)<br>13:53<br>Granite 13B Instruct V2<br>6_._5 %<br>(74_._0 Â± 1_._4) %<br>+67.5pp<br>ReWOO (3 shot)<br>08:28<br>Granite 20B Code<br>39_._7 %<br>(63_._1 Â± 1_._6) %<br>+23.4pp<br>CoT (3 shot)<br>12:03<br>Granite 34B Code<br>56_._4 %<br>(62_._6 Â± 3_._8) %<br>+6.2pp<br>CoT (3 shot)<br>10:07<br>LLaMA 3.1 8B<br>68_._5 %<br>(77_._5 Â± 0_._8) %<br>+9.0pp<br>CoT (3 shot)<br>05:06<br>LLaMA 3.2 3B<br>38_._0 %<br>(66_._3 Â± 0_._9) %<br>+28.3pp<br>ReWOO (5 shot)<br>10:10<br>LLaMA 3.3 70B<br>67_._6 %<br>(78_._1 Â± 0_._6) %<br>+10.5pp<br>ReWOO (3 shot)<br>21:27|FEVER<br>Granite 3.1 8B<br>72_._9 %<br>(76_._4 Â± 3_._3) %<br>+3.5pp<br>ReWOO (5 shot)<br>13:53<br>Granite 13B Instruct V2<br>6_._5 %<br>(74_._0 Â± 1_._4) %<br>+67.5pp<br>ReWOO (3 shot)<br>08:28<br>Granite 20B Code<br>39_._7 %<br>(63_._1 Â± 1_._6) %<br>+23.4pp<br>CoT (3 shot)<br>12:03<br>Granite 34B Code<br>56_._4 %<br>(62_._6 Â± 3_._8) %<br>+6.2pp<br>CoT (3 shot)<br>10:07<br>LLaMA 3.1 8B<br>68_._5 %<br>(77_._5 Â± 0_._8) %<br>+9.0pp<br>CoT (3 shot)<br>05:06<br>LLaMA 3.2 3B<br>38_._0 %<br>(66_._3 Â± 0_._9) %<br>+28.3pp<br>ReWOO (5 shot)<br>10:10<br>LLaMA 3.3 70B<br>67_._6 %<br>(78_._1 Â± 0_._6) %<br>+10.5pp<br>ReWOO (3 shot)<br>21:27|FEVER<br>Granite 3.1 8B<br>72_._9 %<br>(76_._4 Â± 3_._3) %<br>+3.5pp<br>ReWOO (5 shot)<br>13:53<br>Granite 13B Instruct V2<br>6_._5 %<br>(74_._0 Â± 1_._4) %<br>+67.5pp<br>ReWOO (3 shot)<br>08:28<br>Granite 20B Code<br>39_._7 %<br>(63_._1 Â± 1_._6) %<br>+23.4pp<br>CoT (3 shot)<br>12:03<br>Granite 34B Code<br>56_._4 %<br>(62_._6 Â± 3_._8) %<br>+6.2pp<br>CoT (3 shot)<br>10:07<br>LLaMA 3.1 8B<br>68_._5 %<br>(77_._5 Â± 0_._8) %<br>+9.0pp<br>CoT (3 shot)<br>05:06<br>LLaMA 3.2 3B<br>38_._0 %<br>(66_._3 Â± 0_._9) %<br>+28.3pp<br>ReWOO (5 shot)<br>10:10<br>LLaMA 3.3 70B<br>67_._6 %<br>(78_._1 Â± 0_._6) %<br>+10.5pp<br>ReWOO (3 shot)<br>21:27|FEVER<br>Granite 3.1 8B<br>72_._9 %<br>(76_._4 Â± 3_._3) %<br>+3.5pp<br>ReWOO (5 shot)<br>13:53<br>Granite 13B Instruct V2<br>6_._5 %<br>(74_._0 Â± 1_._4) %<br>+67.5pp<br>ReWOO (3 shot)<br>08:28<br>Granite 20B Code<br>39_._7 %<br>(63_._1 Â± 1_._6) %<br>+23.4pp<br>CoT (3 shot)<br>12:03<br>Granite 34B Code<br>56_._4 %<br>(62_._6 Â± 3_._8) %<br>+6.2pp<br>CoT (3 shot)<br>10:07<br>LLaMA 3.1 8B<br>68_._5 %<br>(77_._5 Â± 0_._8) %<br>+9.0pp<br>CoT (3 shot)<br>05:06<br>LLaMA 3.2 3B<br>38_._0 %<br>(66_._3 Â± 0_._9) %<br>+28.3pp<br>ReWOO (5 shot)<br>10:10<br>LLaMA 3.3 70B<br>67_._6 %<br>(78_._1 Â± 0_._6) %<br>+10.5pp<br>ReWOO (3 shot)<br>21:27|\n|GSM8K|Granite 3.1 8B|74_._2 %|(74_._2 Â± 0_._6) %|+0.0pp|Zero-Shot (Baseline)|08:56|\n|GSM8K|Granite 13B Instruct V2<br>23_._0 %<br>(30_._9 Â± 1_._0) %<br>+7.9pp<br>CoT (3 shot)<br>09:20|Granite 13B Instruct V2<br>23_._0 %<br>(30_._9 Â± 1_._0) %<br>+7.9pp<br>CoT (3 shot)<br>09:20|Granite 13B Instruct V2<br>23_._0 %<br>(30_._9 Â± 1_._0) %<br>+7.9pp<br>CoT (3 shot)<br>09:20|Granite 13B Instruct V2<br>23_._0 %<br>(30_._9 Â± 1_._0) %<br>+7.9pp<br>CoT (3 shot)<br>09:20|Granite 13B Instruct V2<br>23_._0 %<br>(30_._9 Â± 1_._0) %<br>+7.9pp<br>CoT (3 shot)<br>09:20|Granite 13B Instruct V2<br>23_._0 %<br>(30_._9 Â± 1_._0) %<br>+7.9pp<br>CoT (3 shot)<br>09:20|\n|GSM8K|Granite 20B Code|68_._7 %|(68_._7 Â± 0_._1) %|+0.0pp|Zero-Shot (Baseline)|09:27|\n|GSM8K|Granite 34B Code<br>72_._1 %<br>(72_._1 Â± 0_._1) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>08:52|Granite 34B Code<br>72_._1 %<br>(72_._1 Â± 0_._1) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>08:52|Granite 34B Code<br>72_._1 %<br>(72_._1 Â± 0_._1) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>08:52|Granite 34B Code<br>72_._1 %<br>(72_._1 Â± 0_._1) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>08:52|Granite 34B Code<br>72_._1 %<br>(72_._1 Â± 0_._1) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>08:52|Granite 34B Code<br>72_._1 %<br>(72_._1 Â± 0_._1) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>08:52|\n|GSM8K|LLaMA 3.1 8B|78_._4 %|(85_._3 Â± 0_._6) %|+6.9pp|CoT (5 shot)|08:48|\n|GSM8K|LLaMA 3.2 3B<br>71_._8 %<br>(75_._3 Â± 0_._4) %<br>+3.5pp<br>CoT (3 shot)<br>16:36|LLaMA 3.2 3B<br>71_._8 %<br>(75_._3 Â± 0_._4) %<br>+3.5pp<br>CoT (3 shot)<br>16:36|LLaMA 3.2 3B<br>71_._8 %<br>(75_._3 Â± 0_._4) %<br>+3.5pp<br>CoT (3 shot)<br>16:36|LLaMA 3.2 3B<br>71_._8 %<br>(75_._3 Â± 0_._4) %<br>+3.5pp<br>CoT (3 shot)<br>16:36|LLaMA 3.2 3B<br>71_._8 %<br>(75_._3 Â± 0_._4) %<br>+3.5pp<br>CoT (3 shot)<br>16:36|LLaMA 3.2 3B<br>71_._8 %<br>(75_._3 Â± 0_._4) %<br>+3.5pp<br>CoT (3 shot)<br>16:36|\n|GSM8K|LLaMA 3.3 70B|85_._5 %|(95_._4 Â± 0_._2) %|+9.9pp|CoT (3 shot)|07:50|\n|MBPP+<br>Granite 3.1 8B<br>62_._9 %<br>(62_._9 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:14<br>Granite 13B Instruct V2<br>10_._7 %<br>(19_._2 Â± 1_._2) %<br>+8.5pp<br>ReAct (5 shot)<br>04:02<br>Granite 20B Code<br>51_._8 %<br>(51_._8 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>03:43<br>Granite 34B Code<br>48_._7 %<br>(61_._3 Â± 1_._0) %<br>+12.6pp<br>ReAct (3 shot)<br>04:54<br>LLaMA 3.1 8B<br>61_._2 %<br>(62_._8 Â± 4_._0) %<br>+1.6pp<br>ReAct (5 shot)<br>01:45<br>LLaMA 3.2 3B<br>58_._0 %<br>(58_._0 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:01<br>LLaMA 3.3 70B<br>71_._4 %<br>(71_._4 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:27|MBPP+<br>Granite 3.1 8B<br>62_._9 %<br>(62_._9 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:14<br>Granite 13B Instruct V2<br>10_._7 %<br>(19_._2 Â± 1_._2) %<br>+8.5pp<br>ReAct (5 shot)<br>04:02<br>Granite 20B Code<br>51_._8 %<br>(51_._8 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>03:43<br>Granite 34B Code<br>48_._7 %<br>(61_._3 Â± 1_._0) %<br>+12.6pp<br>ReAct (3 shot)<br>04:54<br>LLaMA 3.1 8B<br>61_._2 %<br>(62_._8 Â± 4_._0) %<br>+1.6pp<br>ReAct (5 shot)<br>01:45<br>LLaMA 3.2 3B<br>58_._0 %<br>(58_._0 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:01<br>LLaMA 3.3 70B<br>71_._4 %<br>(71_._4 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:27|MBPP+<br>Granite 3.1 8B<br>62_._9 %<br>(62_._9 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:14<br>Granite 13B Instruct V2<br>10_._7 %<br>(19_._2 Â± 1_._2) %<br>+8.5pp<br>ReAct (5 shot)<br>04:02<br>Granite 20B Code<br>51_._8 %<br>(51_._8 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>03:43<br>Granite 34B Code<br>48_._7 %<br>(61_._3 Â± 1_._0) %<br>+12.6pp<br>ReAct (3 shot)<br>04:54<br>LLaMA 3.1 8B<br>61_._2 %<br>(62_._8 Â± 4_._0) %<br>+1.6pp<br>ReAct (5 shot)<br>01:45<br>LLaMA 3.2 3B<br>58_._0 %<br>(58_._0 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:01<br>LLaMA 3.3 70B<br>71_._4 %<br>(71_._4 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:27|MBPP+<br>Granite 3.1 8B<br>62_._9 %<br>(62_._9 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:14<br>Granite 13B Instruct V2<br>10_._7 %<br>(19_._2 Â± 1_._2) %<br>+8.5pp<br>ReAct (5 shot)<br>04:02<br>Granite 20B Code<br>51_._8 %<br>(51_._8 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>03:43<br>Granite 34B Code<br>48_._7 %<br>(61_._3 Â± 1_._0) %<br>+12.6pp<br>ReAct (3 shot)<br>04:54<br>LLaMA 3.1 8B<br>61_._2 %<br>(62_._8 Â± 4_._0) %<br>+1.6pp<br>ReAct (5 shot)<br>01:45<br>LLaMA 3.2 3B<br>58_._0 %<br>(58_._0 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:01<br>LLaMA 3.3 70B<br>71_._4 %<br>(71_._4 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:27|MBPP+<br>Granite 3.1 8B<br>62_._9 %<br>(62_._9 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:14<br>Granite 13B Instruct V2<br>10_._7 %<br>(19_._2 Â± 1_._2) %<br>+8.5pp<br>ReAct (5 shot)<br>04:02<br>Granite 20B Code<br>51_._8 %<br>(51_._8 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>03:43<br>Granite 34B Code<br>48_._7 %<br>(61_._3 Â± 1_._0) %<br>+12.6pp<br>ReAct (3 shot)<br>04:54<br>LLaMA 3.1 8B<br>61_._2 %<br>(62_._8 Â± 4_._0) %<br>+1.6pp<br>ReAct (5 shot)<br>01:45<br>LLaMA 3.2 3B<br>58_._0 %<br>(58_._0 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:01<br>LLaMA 3.3 70B<br>71_._4 %<br>(71_._4 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:27|MBPP+<br>Granite 3.1 8B<br>62_._9 %<br>(62_._9 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:14<br>Granite 13B Instruct V2<br>10_._7 %<br>(19_._2 Â± 1_._2) %<br>+8.5pp<br>ReAct (5 shot)<br>04:02<br>Granite 20B Code<br>51_._8 %<br>(51_._8 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>03:43<br>Granite 34B Code<br>48_._7 %<br>(61_._3 Â± 1_._0) %<br>+12.6pp<br>ReAct (3 shot)<br>04:54<br>LLaMA 3.1 8B<br>61_._2 %<br>(62_._8 Â± 4_._0) %<br>+1.6pp<br>ReAct (5 shot)<br>01:45<br>LLaMA 3.2 3B<br>58_._0 %<br>(58_._0 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:01<br>LLaMA 3.3 70B<br>71_._4 %<br>(71_._4 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:27|MBPP+<br>Granite 3.1 8B<br>62_._9 %<br>(62_._9 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:14<br>Granite 13B Instruct V2<br>10_._7 %<br>(19_._2 Â± 1_._2) %<br>+8.5pp<br>ReAct (5 shot)<br>04:02<br>Granite 20B Code<br>51_._8 %<br>(51_._8 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>03:43<br>Granite 34B Code<br>48_._7 %<br>(61_._3 Â± 1_._0) %<br>+12.6pp<br>ReAct (3 shot)<br>04:54<br>LLaMA 3.1 8B<br>61_._2 %<br>(62_._8 Â± 4_._0) %<br>+1.6pp<br>ReAct (5 shot)<br>01:45<br>LLaMA 3.2 3B<br>58_._0 %<br>(58_._0 Â± 0_._4) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:01<br>LLaMA 3.3 70B<br>71_._4 %<br>(71_._4 Â± 0_._0) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>02:27|",
"|Granite 13B Instruct V2|6.5 %|(74.0 Â± 1.4) %|+67.5pp|ReWOO (3 shot)|08:28|\n|---|---|---|---|---|---|",
"|Granite 34B Code|56.4 %|(62.6 Â± 3.8) %|+6.2pp|CoT (3 shot)|10:07|\n|---|---|---|---|---|---|",
"|LLaMA 3.2 3B|38.0 %|(66.3 Â± 0.9) %|+28.3pp|ReWOO (5 shot)|10:10|\n|---|---|---|---|---|---|",
"|Granite 13B Instruct V2|10.7 %|(19.2 Â± 1.2) %|+8.5pp|ReAct (5 shot)|04:02|\n|---|---|---|---|---|---|",
"|Granite 34B Code|48.7 %|(61.3 Â± 1.0) %|+12.6pp|ReAct (3 shot)|04:54|\n|---|---|---|---|---|---|",
"|LLaMA 3.2 3B|58.0 %|(58.0 Â± 0.4) %|+0.0pp|Zero-Shot (Baseline)|02:01|\n|---|---|---|---|---|---|",
"|Dataset|Model|Accuracy|Col4|Col5|Best Pattern|Runtime|\n|---|---|---|---|---|---|---|\n|Dataset|Model|Zero-Shot|Optimized|Delta|Delta|Delta|\n|GSM-Hard<br>Granite 3.1 8B<br>36_._0 %<br>(37_._8 Â± 0_._8) %<br>+1.8pp<br>ReAct (5 shot, Granite Tools)<br>23:34<br>Granite 13B Instruct V2<br>4_._4 %<br>(6_._2 Â± 0_._7) %<br>+1.8pp<br>CoT (5 shot)<br>10:26<br>Granite 20B Code<br>28_._8 %<br>(27_._2 Â± 4_._5) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>11:14<br>Granite 34B Code<br>27_._9 %<br>(31_._0 Â± 0_._9) %<br>+3.0pp<br>CoT (3 shot)<br>10:20<br>LLaMA 3.2 3B<br>26_._3 %<br>(26_._8 Â± 0_._6) %<br>+0.5pp<br>CoT (5 shot)<br>17:08<br>LLaMA 3.3 70B<br>47_._3 %<br>(53_._8 Â± 0_._4) %<br>+6.5pp<br>CoT (5 shot)<br>11:03|GSM-Hard<br>Granite 3.1 8B<br>36_._0 %<br>(37_._8 Â± 0_._8) %<br>+1.8pp<br>ReAct (5 shot, Granite Tools)<br>23:34<br>Granite 13B Instruct V2<br>4_._4 %<br>(6_._2 Â± 0_._7) %<br>+1.8pp<br>CoT (5 shot)<br>10:26<br>Granite 20B Code<br>28_._8 %<br>(27_._2 Â± 4_._5) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>11:14<br>Granite 34B Code<br>27_._9 %<br>(31_._0 Â± 0_._9) %<br>+3.0pp<br>CoT (3 shot)<br>10:20<br>LLaMA 3.2 3B<br>26_._3 %<br>(26_._8 Â± 0_._6) %<br>+0.5pp<br>CoT (5 shot)<br>17:08<br>LLaMA 3.3 70B<br>47_._3 %<br>(53_._8 Â± 0_._4) %<br>+6.5pp<br>CoT (5 shot)<br>11:03|GSM-Hard<br>Granite 3.1 8B<br>36_._0 %<br>(37_._8 Â± 0_._8) %<br>+1.8pp<br>ReAct (5 shot, Granite Tools)<br>23:34<br>Granite 13B Instruct V2<br>4_._4 %<br>(6_._2 Â± 0_._7) %<br>+1.8pp<br>CoT (5 shot)<br>10:26<br>Granite 20B Code<br>28_._8 %<br>(27_._2 Â± 4_._5) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>11:14<br>Granite 34B Code<br>27_._9 %<br>(31_._0 Â± 0_._9) %<br>+3.0pp<br>CoT (3 shot)<br>10:20<br>LLaMA 3.2 3B<br>26_._3 %<br>(26_._8 Â± 0_._6) %<br>+0.5pp<br>CoT (5 shot)<br>17:08<br>LLaMA 3.3 70B<br>47_._3 %<br>(53_._8 Â± 0_._4) %<br>+6.5pp<br>CoT (5 shot)<br>11:03|GSM-Hard<br>Granite 3.1 8B<br>36_._0 %<br>(37_._8 Â± 0_._8) %<br>+1.8pp<br>ReAct (5 shot, Granite Tools)<br>23:34<br>Granite 13B Instruct V2<br>4_._4 %<br>(6_._2 Â± 0_._7) %<br>+1.8pp<br>CoT (5 shot)<br>10:26<br>Granite 20B Code<br>28_._8 %<br>(27_._2 Â± 4_._5) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>11:14<br>Granite 34B Code<br>27_._9 %<br>(31_._0 Â± 0_._9) %<br>+3.0pp<br>CoT (3 shot)<br>10:20<br>LLaMA 3.2 3B<br>26_._3 %<br>(26_._8 Â± 0_._6) %<br>+0.5pp<br>CoT (5 shot)<br>17:08<br>LLaMA 3.3 70B<br>47_._3 %<br>(53_._8 Â± 0_._4) %<br>+6.5pp<br>CoT (5 shot)<br>11:03|GSM-Hard<br>Granite 3.1 8B<br>36_._0 %<br>(37_._8 Â± 0_._8) %<br>+1.8pp<br>ReAct (5 shot, Granite Tools)<br>23:34<br>Granite 13B Instruct V2<br>4_._4 %<br>(6_._2 Â± 0_._7) %<br>+1.8pp<br>CoT (5 shot)<br>10:26<br>Granite 20B Code<br>28_._8 %<br>(27_._2 Â± 4_._5) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>11:14<br>Granite 34B Code<br>27_._9 %<br>(31_._0 Â± 0_._9) %<br>+3.0pp<br>CoT (3 shot)<br>10:20<br>LLaMA 3.2 3B<br>26_._3 %<br>(26_._8 Â± 0_._6) %<br>+0.5pp<br>CoT (5 shot)<br>17:08<br>LLaMA 3.3 70B<br>47_._3 %<br>(53_._8 Â± 0_._4) %<br>+6.5pp<br>CoT (5 shot)<br>11:03|GSM-Hard<br>Granite 3.1 8B<br>36_._0 %<br>(37_._8 Â± 0_._8) %<br>+1.8pp<br>ReAct (5 shot, Granite Tools)<br>23:34<br>Granite 13B Instruct V2<br>4_._4 %<br>(6_._2 Â± 0_._7) %<br>+1.8pp<br>CoT (5 shot)<br>10:26<br>Granite 20B Code<br>28_._8 %<br>(27_._2 Â± 4_._5) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>11:14<br>Granite 34B Code<br>27_._9 %<br>(31_._0 Â± 0_._9) %<br>+3.0pp<br>CoT (3 shot)<br>10:20<br>LLaMA 3.2 3B<br>26_._3 %<br>(26_._8 Â± 0_._6) %<br>+0.5pp<br>CoT (5 shot)<br>17:08<br>LLaMA 3.3 70B<br>47_._3 %<br>(53_._8 Â± 0_._4) %<br>+6.5pp<br>CoT (5 shot)<br>11:03|GSM-Hard<br>Granite 3.1 8B<br>36_._0 %<br>(37_._8 Â± 0_._8) %<br>+1.8pp<br>ReAct (5 shot, Granite Tools)<br>23:34<br>Granite 13B Instruct V2<br>4_._4 %<br>(6_._2 Â± 0_._7) %<br>+1.8pp<br>CoT (5 shot)<br>10:26<br>Granite 20B Code<br>28_._8 %<br>(27_._2 Â± 4_._5) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>11:14<br>Granite 34B Code<br>27_._9 %<br>(31_._0 Â± 0_._9) %<br>+3.0pp<br>CoT (3 shot)<br>10:20<br>LLaMA 3.2 3B<br>26_._3 %<br>(26_._8 Â± 0_._6) %<br>+0.5pp<br>CoT (5 shot)<br>17:08<br>LLaMA 3.3 70B<br>47_._3 %<br>(53_._8 Â± 0_._4) %<br>+6.5pp<br>CoT (5 shot)<br>11:03|\n|GSM-Hard<br>Granite 3.1 8B<br>36_._0 %<br>(37_._8 Â± 0_._8) %<br>+1.8pp<br>ReAct (5 shot, Granite Tools)<br>23:34<br>Granite 13B Instruct V2<br>4_._4 %<br>(6_._2 Â± 0_._7) %<br>+1.8pp<br>CoT (5 shot)<br>10:26<br>Granite 20B Code<br>28_._8 %<br>(27_._2 Â± 4_._5) %<br>+0.0pp<br>Zero-Shot (Baseline)<br>11:14<br>Granite 34B Code<br>27_._9 %<br>(31_._0 Â± 0_._9) %<br>+3.0pp<br>CoT (3 shot)<br>10:20<br>LLaMA 3.2 3B<br>26_._3 %<br>(26_._8 Â± 0_._6) %<br>+0.5pp<br>CoT (5 shot)<br>17:08<br>LLaMA 3.3 70B<br>47_._3 %<br>(53_._8 Â± 0_._4) %<br>+6.5pp<br>CoT (5 shot)<br>11:03|LLaMA 3.3 70B|47_._3 %|(53_._8 Â± 0_._4) %|+6.5pp|CoT (5 shot)|11:03|",
"|Granite 13B Instruct V2|4.4 %|(6.2 Â± 0.7) %|+1.8pp|CoT (5 shot)|10:26|\n|---|---|---|---|---|---|",
"|Granite 34B Code|27.9 %|(31.0 Â± 0.9) %|+3.0pp|CoT (3 shot)|10:20|\n|---|---|---|---|---|---|",
"|Dataset|Model|Accuracy|Col4|Col5|Best Pattern|\n|---|---|---|---|---|---|\n|Dataset|Model|Zero-Shot|Optimized|Delta|Delta|\n|FEVER<br>GPT-4o-mini<br>83_._7 %<br>87_._7 %<br>+4.0pp<br>CoT (3 shot)|FEVER<br>GPT-4o-mini<br>83_._7 %<br>87_._7 %<br>+4.0pp<br>CoT (3 shot)|FEVER<br>GPT-4o-mini<br>83_._7 %<br>87_._7 %<br>+4.0pp<br>CoT (3 shot)|FEVER<br>GPT-4o-mini<br>83_._7 %<br>87_._7 %<br>+4.0pp<br>CoT (3 shot)|FEVER<br>GPT-4o-mini<br>83_._7 %<br>87_._7 %<br>+4.0pp<br>CoT (3 shot)|FEVER<br>GPT-4o-mini<br>83_._7 %<br>87_._7 %<br>+4.0pp<br>CoT (3 shot)|\n|GSM-Hard|GPT-4o-mini|45_._6 %|54_._9 %|+9.3pp|ReAct (5 shot, Granite LLaMa)|\n|GSM8K<br>GPT-4o-mini<br>77_._8 %<br>90_._9 %<br>+13.1pp<br>CoT (5 shot)|GSM8K<br>GPT-4o-mini<br>77_._8 %<br>90_._9 %<br>+13.1pp<br>CoT (5 shot)|GSM8K<br>GPT-4o-mini<br>77_._8 %<br>90_._9 %<br>+13.1pp<br>CoT (5 shot)|GSM8K<br>GPT-4o-mini<br>77_._8 %<br>90_._9 %<br>+13.1pp<br>CoT (5 shot)|GSM8K<br>GPT-4o-mini<br>77_._8 %<br>90_._9 %<br>+13.1pp<br>CoT (5 shot)|GSM8K<br>GPT-4o-mini<br>77_._8 %<br>90_._9 %<br>+13.1pp<br>CoT (5 shot)|\n|MBPP+|GPT-4o-mini|72_._3 %|72_._3 %|+0.0pp|Zero-Shot (Baseline)|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2504.04365v5.pdf"
}