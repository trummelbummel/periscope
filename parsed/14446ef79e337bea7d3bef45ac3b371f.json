{
"text": "MOPROMPT: MULTI-OBJECTIVE SEMANTIC EVOLUTION FOR\n                     PROMPT OPTIMIZATION\n\n\n\n\n                    Sara Câmara3, Eduardo Luz1, Valéria Carvalho1, Ivan Meneghini2 and Gladston Moreira1\n                1Computing Department, Universidade Federal de Ouro Preto, Ouro Preto, 35402-136, Minas Gerais, Brazil.\n                                           2Federal Institute of Minas Gerais, Ibirité-MG, Brazil.\n                            3Postgraduate Program in Computer Science, Federal University of Ouro Preto, Brazil.\n                                              gladston@ufop.edu.br\n2025\n\n                                     ABSTRACTAug              Prompt engineering is crucial for unlocking the potential of Large Language Models (LLMs). Still,\n3                 since manual prompt design is often complex, non-intuitive, and time-consuming, automatic prompt\n                        optimization has emerged as a research area. However, a significant challenge in prompt optimization\n                             is managing the inherent trade-off between task performance, such as accuracy, and context size.\n                   Most existing automated methods focus on a single objective, typically performance, thereby failing\n                         to explore the critical spectrum of efficiency and effectiveness. This paper introduces the MOPrompt,\n                      a novel Multi-objective Evolutionary Optimization (EMO) framework designed to optimize prompts\n                         for both accuracy and context size (measured in tokens) simultaneously. Our framework maps the[cs.CL]                       Pareto front of prompt solutions, presenting practitioners with a set of trade-offs between context\n                         size and performance – a crucial tool for deploying Large Language Models (LLMs) in real-world\n                         applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese, using Gemma-2B\n                     and Sabiazinho-3 as evaluation models. Our findings show that MOPrompt substantially outperforms\n                        the baseline framework. For the Sabiazinho model, MOPrompt identifies a prompt that achieves the\n                   same peak accuracy (0.97) as the best baseline solution, but with a 31% reduction in token length.\n\n           Keywords Multi-objective optimization · Prompt Evalution · Decision space diversity · Region of Interest.\n\n\n          1  Introduction\n\n            The emergence of powerful LLMs like GPT-4 [1] and Gemini [2] has revolutionized the field of Natural Language\n              Processing (NLP). The effective use of these models is highly dependent on prompt engineering, the process of\n              designing effective instructions to guide the model’s output [3]. However, manually crafting optimal prompts is aarXiv:2508.01541v1         significant bottleneck; it is often described as a “dark art” that requires extensive trial and error.\n             Automating prompt optimization is a promising avenue to address this challenge [4]. In [5], the authors presented a\n             framework called EvoPrompt, which combines evolutionary algorithms (EAs) with large language models (LLMs) to\n              optimize prompts, with significant performance improvements over human-designed prompts and existing automated\n            prompt generation methods. Yet, in [6], the authors proposed an iterative prompt evolution method to optimize the\n             model’s performance on toxic content classification in social media. The current automatic methods concentrate\n               exclusively on maximizing task-specific performance metrics [5], overlooking the context size, which is measured in\n             tokens—the fundamental units of text processed by the model. While larger context windows can improve performance,\n              they also require more computational resources and may lead to slower processing times.\n\n            To tackle this issue, this work introduces MOPrompt, an automatic prompt optimization framework that addresses a\n               multi-objective problem by optimizing prompts to achieve maximum accuracy and minimum context size (token length),\n              simultaneously. As in [5, 6], we utilize LLMs to act as evolutionary operators to generate new prompts, executing\n              semantic crossover and mutation operations to develop a diverse and contextually rich population of candidate prompts.\n\n         We pose the following research questions:\n\nMOPrompt: Multi-objective Semantic Evolution for Prompt Optimization\n\n\n\n        • RQ1: Can a multi-objective evolutionary approach find prompts that are both more accurate and more\n         token-efficient than those found by a single-objective optimization?\n\n        • RQ2: What is the impact of different prompting strategies, specifically zero-shot versus few-shot, on the\n         multi-objective optimization process?\n\nWe conduct a set of experiments on a Portuguese sentiment classification task. We compare our approach against a\nbaseline framework, EvoPrompt [5], across two distinct open-source evaluation models (Gemma-2B and Sabiazinho-3)\nand two prompting strategies (zero-shot and few-shot). The results demonstrate rapid convergence of MOPrompt. Using\na few-shot strategy with the Sabiazinho model, the MOPrompt framework identified a prompt that achieves the same\npeak accuracy (0.97) as the best baseline solution, while reducing the token length by 31%.\n\nOur main contributions are:\n\n        • A novel EMO framework, MOPrompt, for automatic prompt optimization that effectively balances task\n        accuracy and token length (context size).\n\n        • An empirical study on a non-English language (Portuguese) that validates our approach and provides actionable\n         insights into the accuracy-cost trade-off in practical prompt engineering.\n\n2  Related Work\n\nOur work is situated at the intersection of automatic prompt engineering, evolutionary algorithms, and the emerging\nparadigm of using LLMs as optimizers.\n\nAutomatic Prompt Engineering  The field has rapidly moved from manual prompt design to automated methods\n[7, 8]. Early approaches focused on discrete textual prompts, often using gradient-free optimization or search algorithms\nto find the best instruction from a predefined set [4]. These methods, while effective, typically optimize for a single\nperformance metric. In contrast, our work explicitly models the trade-off between performance and cost, a crucial\naspect for real-world applications that is often overlooked in academic research.\n\nEvolutionary Algorithms for Prompt Optimization  Evolutionary Algorithms (EAs) has proven to be a powerful\ntool for prompt optimization due to its gradient-free nature and ability to explore complex search spaces. [5] introduced\nEvoPrompt to optimize discrete prompts, which connects LLMs with evolutionary algorithms. Specifically, EvoPrompt\nutilizes LLMs to generate new candidate prompts based on evolutionary operators (semantic operators), inspired by\nthe genetic algorithm and differential evolution. In [9], the authors proposed an evolutionary multi-objective (EMO)\napproach tailored explicitly for prompt optimization called EMO-Prompts, using sentiment analysis capabilities as a\nmaximization problem of the score of an emotion pair. Our work differs in a key aspect: we focus on the fundamental\ntrade-off between accuracy and context size. Similarly, we use an LLM itself as the core engine for performing\nevolutionary operations, a significant departure from using traditional, heuristic-based genetic operators.\n\nLLMs as Optimizers and Operators A recent trend involves using LLMs not just as the target of optimization\nbut as active components within the optimization task. Works, such as Automatic Prompt Engineer (APE) [4] and\nPromptbreeder [10], have demonstrated that LLMs can generate and refine prompts for various tasks iteratively. Also\nin [11], the authors propose SPELL, a semantic prompt evolution method considering a LLM as a prompt generator.\nHowever, it operates in a self-referential, single-objective manner. Our proposed framework integrates the idea of\nan LLM-driven evolution into a formal EMO context, utilizing the LLM to execute guided crossover and mutation\noperations, thereby explicitly navigating the accuracy-context size trade-off.\n\n3  Background\n\nTo understand our method, we first introduce the foundational concepts of LLMs, evolutionary algorithms, and\nmulti-objective optimization.\n\n\n3.1  Large Language Models and Prompting\n\nLLMs are deep neural networks trained on vast amounts of text data, enabling them to understand and generate\nhuman-like text [3]. Their behavior is steered through prompts, which are natural language instructions. Two common\nprompting strategies are:\n\n\n                                               2\n\nMOPrompt: Multi-objective Semantic Evolution for Prompt Optimization\n\n\n\n        • Zero-shot prompting: The LLM is given a direct instruction to perform a task without any examples.\n\n        • Few-shot prompting: The prompt includes a few examples (demonstrations) of the task to guide the model’s\n         output more effectively.\n\n\n3.2  Evolutionary Algorithms\n\nEAs is a family of population-based metaheuristic optimization algorithms inspired by biological evolution [12]. They\nmaintain a population of candidate solutions (individuals) that evolve over generations. Each generation involves\nevaluating the fitness of individuals, selecting the best ones for reproduction, and applying genetic operators like\ncrossover (combining two parents to create offspring) and mutation (introducing small, low-frequency random changes)\nto create a new generation.\n\n\n3.3  Multiobjective Optimization\n\nMany real-world problems involve the simultaneous optimization of multiple, often conflicting, objectives [13]. A\nMulti-objective Optimization Problem (MOP) can be mathematically stated as follows:\n\n                             min F(x) = (f1(x), f2(x), ..., fm(x))                                      (1)\n                              x∈X\n\nwhere x is the vector of decision variables (or a solution) belonging to the feasible solution set X, and F(x) is the\nvector of m objective functions to be minimized.\n\nUnlike in single-objective optimization, there is typically no single solution that is best for all objectives. The goal,\ninstead, is to find a set of solutions representing the best possible trade-offs. The Pareto dominance principle formalizes\nthis concept. A solution xa dominates a solution xb (denoted as xa ≺xb) if and only if fi(xa) ≤fi(xb) for every\nobjective i ∈{1, ..., m} and there is at least one objective j ∈{1, ..., m} for which fj(xa) < fj(xb). A solution\nx∗∈X is called Pareto-optimal if no other solution x ∈X dominates it.\n\n        • The set of all Pareto-optimal solutions is called the Pareto set (XE).\n\n        • The image of the Pareto set in the objective space, f(XE), is called the Pareto front (YN).\n\nThe Pareto front represents the optimal trade-offs among the conflicting objectives. Algorithms such as Nondominated\nSorting Genetic Algorithm II (NSGA-II) [12] are designed to find a well-distributed and convergent approximation of\nthis front.\n\n\n4  The MOPrompt Framework\n\nWe now detail our MOPrompt framework, starting with the multi-objective formulation problem, the core LLM-based\ngenetic operators, our single-objective baseline, and concluding with the complete multi-objective approach.\n\n\n4.1  Problem Formulation\n\nWe formally define the multi-objective prompt optimization problem. Let p be a prompt from the space of all possible\ntext-based prompts P. We aim to find a set of Pareto-optimal prompts P ∗⊂P that solves the following bi-objective\noptimization problem:\n                               min F(p) = (fcost(p), ferror(p))                                         (2)\n                                  p∈P\n\nwhere the two objective functions to be minimized are:\n\n        • Cost: fcost(p) = ftokens(p), the number of tokens in prompt p. This function measures the computational\n         efficiency of the prompt.\n\n        • Error: ferror(p) = 1 −facc(p, D, M, S), the classification error rate, where facc is the accuracy on a dataset D\n        using an evaluator model M and a prompting strategy S.\n\nThis formulation aligns with standard minimization problems in EMO frameworks, such as NSGA-II.\n\n\n                                               3\n\nMOPrompt: Multi-objective Semantic Evolution for Prompt Optimization\n\n\n\n4.2  LLM-based Genetic Operators\n\nOur framework employs a generator LLM, MG (GPT-4o mini [1]), to execute genetic operations. Diverging from\ntraditional heuristic-based operators that manipulate text superficially, we leverage the semantic understanding of a\nLLM. We’ve defined a unified genetic function, GALLM, which performs a crossover between two parent prompts\n(pra, prb) and subsequently mutates the result to produce a single offspring. This entire sequence is executed via a\nstructured requisition to MG’s public Application Programming Interface (API), utilizing the template presented in\nTable 1. This approach facilitates the creation of new prompts that are not only syntactically valid but also semantically\ncoherent and contextually relevant to the optimization task.\n\n\nTable 1: LLM template for genetic operations. The generator LLM (MG) is instructed to act as a prompt optimizer and\nperform crossover and mutation.\n\nTemplate for Generator LLM (Crossover + Mutation)\n\nsystem:\nVocê é um otimizador de prompts para classificação de sentimentos (positivo ou negativo). Seu papel é melhorar instruções para\nmodelos de linguagem, gerando prompts curtos, diretos e eficazes. Gere apenas o prompt, sem explicações ou comentários adicionais:\n\nuser_crossover:\nPrompt A: \"{prompt_a}\"\nPrompt B: \"{prompt_b}\"\nRealize uma combinação dos dois prompts, como em uma operação de crossover, mantendo clareza, coerência e o propósito original.\n\nuser_mutation:\nAssim como uma mutação que introduz variedade, gere uma variação deste prompt mantendo seu objetivo de classificar sentimentos\ncom precisão: \"{prompt}\" A variação pode incluir reformulação, troca de termos ou reorganização sintática.\n\n\n\n4.3  Multi-objective Approach: MOPrompt\n\nOur proposed method, MOPrompt, extends this framework to a multi-objective context. The algorithm MOPrompt\naims to solve the bi-objective problem formulated in the previous section, which involves minimizing both token count\nand classification error.\n\nThe key difference lies in the selection phase. Instead of roulette wheel selection, MOPrompt employs the NSGA-II\nalgorithm. At each generation, the combined population of parents and offspring is sorted into non-dominated fronts.\nThe next generation is then populated with individuals from the best fronts. To maintain diversity along the Pareto front,\na crowding distance metric is used as a tie-breaker, favoring solutions in less-crowded regions of the objective space.\nThis process allows MOPrompt to explore the trade-off between the prompt’s accuracy and context size, returning a\nprompt’s Pareto-optimal set for the user to choose from.\n\n5  Experimental Setup\n\n5.1  Baseline Framework: EvoPrompt\n\nTo establish a strong baseline, we implement a version of the Evo-Prompt framework [5]. Here, the Evo-Prompt version\nperforms the same genetic operations in Table 1. This algorithm focuses solely on maximizing accuracy (facc). It\nemploys a standard EA structure where selection is performed using the roulette wheel method. The probability of a\nprompt being selected for reproduction is proportional to its accuracy score. While this method is effective at finding\nhigh-accuracy prompts, it inherently overlooks prompt length, often resulting in verbose and costly solutions.\n\n5.2  Task and Dataset\n\nWe evaluate our methods on a binary sentiment analysis task. The dataset used is “maritaca-ai/imdb_pt”, a Portuguese\ntranslation of the classic Internet Movie Database (IMDb) movie review dataset [14]. For computational efficiency,\nwe conduct our experiments on a fixed, randomly sampled subset of 100 reviews, balanced with 50 positive and 50\nnegative examples. This same subset is used across all experimental runs to ensure fair comparison.\n\n5.3  Models\n\nOur framework utilizes two types of models:\n\n\n                                               4\n\nMOPrompt: Multi-objective Semantic Evolution for Prompt Optimization\n\n\n\n        • Generator LLM (MG): We use GPT-4o mini [1] as the engine for our evolutionary algorithm. It is responsible\n         for generating the initial population of prompts and for executing the crossover and mutation operations as\n         described in Table 1.\n\n        • Evaluator LLMs (M): To assess the fitness (accuracy) of the generated prompts, we use two distinct,\n        open-source models:\n\n         – Gemma-2B: A lightweight model from Google based on Gemini technology [2].\n         – Sabiazinho-3: An efficient model from Maritaca AI, specifically trained for Brazilian Portuguese [15].\n\n\n5.4  Evaluation\n\nWe investigate four primary experimental scenarios, covering both the baseline framework, Evo-Prompt, and our\nproposed framework, MOPrompt, each one applied to the ‘Gemma-2B’ and ‘Sabiazinho-3’ evaluator models. Within\neach scenario, we explore both ‘zero-shot’ and ‘few-shot’ prompting strategies.  In our experiments, we set the\npopulation size to 10 individuals, and each evolutionary run lasted for 10 generations.\n\n\n5.5  Metrics\n\nThe performance of the prompts is measured using two primary metrics:\n\n        • Accuracy: The proportion of correctly classified sentiment labels in our 100-review test set.\n\n        • Token Count: In LLMs, the whole input, including the prompt, is processed as a sequence of tokens, which\n         are the fundamental units of text. The maximum number of tokens that a model can process at once is known\n         as its context size or context window [16]. The cost of using commercial LLMs and the computational load\n         of open-source models are directly proportional to the total number of tokens processed [17]. Therefore,\n        minimizing the prompt’s token count is a critical objective for creating efficient, cost-effective, and scalable\n         applications. To ensure a model-agnostic and consistent measure of prompt cost, we calculate the number\n         of tokens for each prompt using the ‘TreebankWordTokenizer‘ from the NLTK library. This provides a\n         standardized measure of prompt verbosity.\n\n6  Results and Discussion\n\nThe evolutionary dynamics of the MOPrompt framework are illustrated in Figure 1(a) and Figure 1(b), which maps the\nprogression of the Pareto front across three key generations (0, 5, and 10) for both the Sabiazinho and Gemma models\nusing a few-shot strategy. The visualizations confirm that the optimization process demonstrates a clear pattern of rapid\nconvergence towards more optimal solutions.\n\nFor the Sabiazinho model, Figure 1(a), the initial prompts are scattered across a wide range of context sizes and have\nrelatively high error rates. The evolutionary process quickly drives the population towards a more optimal state over the\ngenerations. A similar trend is observed with the Gemma model Figure 1(b), although its final Pareto front reveals a\nmore pronounced trade-off. The initial prompts in Generation 0 also start with high error rates. As the generations\nprogress, MOPrompt successfully pushes the front toward lower error and cost.\n\nWe now present our findings, structured around the research questions posed in the introduction. More details of the\ncode and results are available on the repository of Github project.\n\n\n6.1  RQ1: Multiobjective vs. Single-Objective Performance\n\nTable 2 provides a quantitative summary of our results. The data clearly answers our first research question: the multi-\nobjective approach, MOPrompt, consistently finds prompts that are superior to those discovered by the single-objective\nevoPrompt.\n\nFor the Gemma and Sabiazinho models, there is a minimal variation in accuracy between the MOPrompt and EVO-\nPrompt baseline and a significant difference in the reduction of the context size of prompts. For example, for the\nGemma model in the zero-shot configuration, the best baseline prompt achieves an accuracy of 0.87 with 19 tokens.\nAt the same time, MOPrompt finds a solution with a comparable accuracy of 0.85, but using only 12 tokens — a cost\nreduction of 37%. Another interesting observation is the difference in accuracy between the tested models. The average\naccuracy for both models and strategies (MOPrompt and EVO-prompt, Few-shot and Zero-shot) using the Gemma\nmodel was 83.75, while the same measurement was 96.5 for the Sabiazinho model. We believe this difference stems\n\n\n                                               5\n\nMOPrompt: Multi-objective Semantic Evolution for Prompt Optimization\n\n\n\n\n\n                        (a) Sabiazinho model                                              (b) Gemma model\n\nFigure 1: Pareto Front evolution for 0, 5, and 10 generations, running the MOPrompt framework using a \"few-shot\"\nstrategy.\n\n\nTable 2: Performance summary comparing the MOPromot and the baseline (Evo-Prompt) framework for all scenarios.\n\n                 Model     Strategy   Framework   Highlight          Accuracy   Tokens\n\n                                  MOPrompt   Max Acc (Front)        0.8100       13\n                         Few shot   MOPrompt   Min Tokens (Front)      0.7600       10\n                                             Baseline       Best Accuracy          0.8200       20\n              Gemma\n                                  MOPrompt   Max Acc (Front)        0.8500       12\n                              Zero shot  MOPrompt   Min Tokens (Front)      0.7900       10\n                                             Baseline       Best Accuracy          0.8700       19\n\n                                  MOPrompt   Max Acc (Front)        0.9700       11\n                         Few shot   MOPrompt   Min Tokens (Front)      0.9600       10\n                                             Baseline       Best Accuracy          0.9700       16\n                   Sabiazinho\n                                  MOPrompt   Max Acc (Front)        0.9600       11\n                              Zero shot  MOPrompt   Min Tokens (Front)      0.9500       10\n                                             Baseline       Best Accuracy          0.9600       18\n\n\n\nfrom the language used. All tokens were written in Brazilian Portuguese, and the Sabiazinho model is specifically\ndesigned for this language, unlike the Gemma model.\n\nTo understand why MOPrompt is more effective, we qualitatively analyzed the generated prompts, as shown in Table 3.\nThe MOPrompt framework proposed favors concise and direct instructions, while the Baseline framework (Evo-Prompt),\nlacking the pressure to reduce token count, often generates more verbose and less efficient prompts. For example, the\nbest zero-shot prompt for Gemma from Evo-Prompt is: “Classifique a opinião como positiva ou negativa, identificando\nse o autor demonstra aprovação ou desaprovação.” (19 tokens). The MOPrompt that achieves a similar performance\nis much more direct: “Determine se a opinião apresentada é positiva ou negativa.” (12 tokens), effectively removes\nredundant clauses (the \"identificando...\" part) without harming performance.\n\nCollectively, the evidence from both models shows that the MOPrompt methodology is promising. It actively refines\nand simplifies prompts over generations, validating the framework’s ability to find optimal solutions that strike a balance\nbetween performance and cost.\n\n\n6.2  RQ2: Impact of Prompting Strategy\n\nOur second research question concerns the role of few-shot examples. Our results confirm that providing examples\ntypically improves performance. However, the magnitude of this benefit varies significantly between models.\n\n\n                                               6\n\nMOPrompt: Multi-objective Semantic Evolution for Prompt Optimization\n\n\n\n        Table 3: Qualitative analysis of the prompt solutions presented in Table 2, obtained by Frameworks.\nModel       Strategy  Framework  Prompt Highlight\n\n                   MOPrompt    \"Classifique esta resenha de filme como sendo positiva ou negativa.\"\n           Few shot  MOPrompt    \"Avalie este comentário como positivo ou negativo.\"\n                         Baseline       \"Classifique esta resenha de filme como ’positiva’ (favorável)\nGemma\n                                  ou ’negativa’ (desfavorável).\"\n\n                   MOPrompt    \"Determine se a opinião apresentada é positiva ou negativa.\"\n             Zero shot  MOPrompt    \"Classifique este sentimento como positivo ou negativo.\"\n                         Baseline       \"Classifique a opinião como positiva ou negativa,\n                                           identificando se o autor demonstra aprovação ou desaprovação.\"\n\n                   MOPrompt    \"Determine se a avaliação é positiva ou negativa.\"\n           Few shot  MOPrompt    \"Classifique este sentimento como positivo ou negativo.\"\n                         Baseline       \"Identifique se o sentimento expressado a seguir é ’positivo’ ou ’negativo’.\"\nSabiazinho\n                   MOPrompt    \"Determine se este comentário é positivo ou negativo.\"\n             Zero shot  MOPrompt    \"Classifique a crítica como positiva ou negativa.\"\n                         Baseline      \"Determine se a análise a seguir reflete uma opinião positiva ou\n                                          negativa sobre o filme.\"\n\n\n\nFor the Sabiazinho model, the gap between the zero-shot and few-shot front’s is modest. This indicates that Sabiazinho\nis an inherently strong zero-shot reasoner for this task, and the examples serve as a fine-tuning mechanism to reach\npeak performance. For the Gemma model, the difference is much more pronounced, suggesting a greater dependency\non in-context examples to optimize the accuracy-cost trade-off. This highlights a crucial interaction between the\noptimization algorithm and the model’s capabilities: the value of a prompting strategy is model-dependent.\n\n\n6.3  Limitations\n\nDespite the promising results, we acknowledge some limitations. First, while the LLM-based genetic operator is\npowerful, we observed that it can sometimes converge to similar prompt structures, leading to sparse Pareto fronts in\nsome experimental runs. Future work could focus on techniques to keep greater diversity in the generated prompts.\nSecond, our study focused on a single task (sentiment analysis) and utilized a specific set of models. Validating the\ngeneralizability of these findings across a broader range of tasks and LLMs is an essential next step.\n\n\n7  Conclusion\n\nIn this paper, we addressed the critical challenge of balancing performance and cost in prompt engineering for LLMs.\nWe introduced MOPrompt, a novel EMO framework that simultaneously optimizes prompts for accuracy and token\nefficiency. Our approach utilizes a large language model as a semantic operator to perform genetic crossover and\nmutation, employing Pareto dominance to evolve prompts.\n\nOur experiments, conducted on a sentiment analysis task in Portuguese, demonstrate the clear superiority of our\nmulti-objective approach over a strong single-objective baseline. By exploring the Pareto front of solutions, MOPrompt\ndiscovered prompts that offer significant cost reductions – up to 31% – without compromising peak accuracy. Our anal-\nysis revealed that MOPrompt achieves this by generating more concise and direct instructions, effectively discovering\nthe optimal “language” for each target model.\n\nThis work suggests empirical evidence for the effectiveness of EMO in prompt engineering and highlights its crucial\nrole in optimizing the accuracy-context size trade-off. For future work, we plan to apply MOPrompt to a broader array\nof tasks and models, investigate methods to enhance prompt diversity, and explore its integration with more complex\nprompting techniques such as Chain-of-Thought (CoT).\n\n\nAcknowledgments\n\nThe authors would like to thank the Fundação de Amparo a Pesquisa do Estado de Minas Gerais (FAPEMIG, grant\nAPQ-01647-22), Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq, grants 307151/2022-0,\n\n\n                                               7\n\nMOPrompt: Multi-objective Semantic Evolution for Prompt Optimization\n\n\n\n152613/2024-2) and Instituto Federal de Educação e Tecnologia de Minas Gerais (IFMG, grant 030/2024) for supporting\nthe development of this study.\n\nReferences\n\n [1]  J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman,\n      S. Anadkat, et al., Gpt-4 technical report, preprint arXiv:2303.08774.\n [2] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love,\n      et al., Gemma: Open models based on gemini research and technology, preprint arXiv:2403.08295.\n [3] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, A. Chadha, A systematic survey of prompt engineering in\n      large language models: Techniques and applications, preprint arXiv:2402.07927.\n [4] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, J. Ba, Large language models are human-level\n     prompt engineers, in: The Eleventh International Conference on Learning Representations, 2022.\n [5] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, Y. Yang, Connecting large language models\n     with evolutionary algorithms yields powerful prompt optimizers, in: International Conference on Learning\n     Representations (ICLR), 2024.\n [6] A. Oliveira, P. H. Silva, V. Santos, G. Moreira, V. L. Freitas, E. J. Luz, Toxic text classification in portuguese: Is\n     llama 3.1 8b all you need?, in: Symposium in Information and Human Language Technology, 2024, pp. 57–66.\n [7] P. Korzynski, G. Mazurek, P. Krzypkowska, A. Kurasinski, Artificial intelligence prompt engineering as a\n    new digital competence: Analysis of generative ai technologies such as chatgpt, Entrepreneurial Business and\n     Economics Review 11 (3) (2023) 25–37.\n [8] G. Marvin, N. Hellen, D. Jjingo, J. Nakatumba-Nabende, Prompt engineering in large language models, in:\n      International conference on data intelligence and cognitive informatics, 2023, pp. 387–402.\n [9]  J. Baumann, O. Kramer, Evolutionary multi-objective optimization of large language model prompts for balancing\n      sentiments, in: International Conference on the Applications of Evolutionary Computation (Part of EvoStar), 2024,\n     pp. 212–224.\n[10] C. Fernando, D. Banarse, H. Michalewski, S. Osindero, T. Rocktäschel, Promptbreeder: self-referential self-\n     improvement via prompt evolution, in: Proceedings of the 41st International Conference on Machine Learning,\n     2024.\n[11] Y. B. Li, K. Wu, Spell: Semantic prompt evolution based on a llm, preprint arXiv:2310.01260.\n[12] K. Deb, A. Pratap, S. Agarwal, T. Meyarivan, A fast and elitist multiobjective genetic algorithm: Nsga-ii, IEEE\n      transactions on evolutionary computation 6 (2) (2002) 182–197.\n[13] G. Moreira, L. Paquete, Guiding under uniformity measure in the decision space, in: 2019 IEEE Congress on\n     Evolutionary Computation (CEC), 2019, pp. 1536–1542.\n[14] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, C. Potts, Learning word vectors for sentiment analysis,\n      in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language\n     Technologies, 2011, pp. 142–150.\n   URL http://www.aclweb.org/anthology/P11-1015\n[15] R. Pires, H. Abonizio, T. S. Almeida, R. Nogueira, Sabiá: Portuguese large language models (2023). arXiv:\n    2304.07880[cs], doi:10.1007/978-3-031-45392-2_15.\n   URL http://arxiv.org/abs/2304.07880\n[16] S. Pawar, S. M. T. I. Tonmoy, S. M. M. Zaman, V. Jain, A. Chadha, A. Das, The what, why, and how of context\n     length extension techniques in large language models – a detailed survey, preprint arXiv:2401.07872.\n[17] N. Martin, A. B. Faisal, H. Eltigani, R. Haroon, S. Lamelas, F. Dogar, Llmproxy: Reducing cost to access large\n     language models, preprint arXiv:2410.11857.\n\n\n\n\n\n                                               8",
"headers": [
"arXiv:2508.01541v1  [cs.CL]  3 Aug 2025",
"MOP",
": M",
"-",
"S",
"E",
"P",
"O",
"ROMPT",
"ULTI",
"OBJECTIVE",
"EMANTIC",
"VOLUTION FOR",
"PTIMIZATION",
"A",
"1",
"Introduction",
"2",
"Related Work",
"3",
"Background",
"4",
"The MOPrompt Framework",
"5",
"Experimental Setup",
"6",
"Results and Discussion",
"7",
"Conclusion",
"Acknowledgments",
"References"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2508.01541v1.pdf"
}