{
"text": "MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a\n                                     Posteriori Inference\n\n          Zheyuan Zhang1, Lin Ge3, Hongjiang Li3, Weicheng Zhu3, Chuxu Zhang2, Yanfang Ye1†\n                          1University of Notre Dame, 2University of Connecticut, 3Amazon\n                                           †Corresponding Author\n                                    {zzhang42, yye7}@nd.edu,\n\n\n\n                          Abstract\n\n                Large language models (LLMs) have demon-\n                    strated remarkable capabilities across diverse\n                    tasks, and LLM-based agents further extend\n                  these abilities to various practical workflows.2025\n               While recent progress shows that multi-agent\n                 systems (MAS) can outperform single agents\n               by  coordinating  specialized  roles,  design-Oct\n                  ing effective MAS remains difficult due to\n8\n                prompt sensitivity and the compounded in-\n                     stability MAS creates.  To cope with the\n                   challenge, recent efforts in automated prompt\n                 design have reduced manual effort.  How-\n                     ever, multi-agent prompt optimization remains\n                   largely unexplored. Challenges like exponen-[cs.CL]\n                      tially expanding search space and ambiguous\n                     credit assignment together make systematic de-\n                  sign intractable without principled methods.\n                  Therefore, we introduce Multi-Agent PRompt        Figure 1: Prompt quality governs agent reliability:\n                 Optimization (MAPRO), a four-stage frame-         (top-left) vague, manually revised prompts are error-\n               work that first formulates MAS prompt opti-       prone and costly; (top-right) automated prompt opti-\n                  mization as a Maximum a Posteriori (MAP) in-       mization search and produce correct answers; (bottom-\n                   ference problem and solves it using a language-          left) the optimizer explores and selects among candidate\n                 guided variant of max-product belief propaga-        rewrites; (bottom-right) performance improves over it-\n                   tion algorithm. To address credit assignment         erations, surpassing hand-tuned prompting.\n                and updates the system iteratively, MAPRO\n                                                                  tasks that require reasoning, comprehension, and\n                employs a topology-aware refinement mech-\n                                                                         text generation. Their rapid progress has reshaped                anism that integrates execution feedback and\n                downstream blames to selectively update agent       both research and practice across domains rang-arXiv:2510.07475v1           prompts. Through this process, MAPRO pro-       ing from scientific discovery to software develop-\n                   gressively converges to a coordinated set of      ment (Kojima et al., 2022; Ouyang et al., 2022).\n                   agent-specific prompt policies. Across bench-       Building upon this foundation, LLM-based agents\n                marks in various tasks, MAPRO achieves state-                                                         have gained prominence for their ability to au-\n                     of-the-art performance, consistently surpassing\n                                                          tonomously plan, interact, and solve complex prob-\n                 manually engineered baselines and recent auto-\n                                                          lems with minimal human supervision. Such agents               mated alternatives. Beyond performance, our\n              MAP-based formulation also delivers general       extend the reach of LLMs into practical work-\n                   guidelines for building more reliable and prin-       flows, enabling applications such as program syn-\n                   cipled multi-agent systems in the future 1.            thesis and debugging, retrieval-augmented genera-\n                                                                           tion, data-centric analysis and interactive decision-\n          1  Introduction                           making (Jimenez et al., 2024; Singh et al., 2025;\n                                              Guo et al., 2024b; Li et al., 2024).  While sin-\n            Large language models (LLMs) have emerged as                                                                  gle agents are useful, orchestrating multiple LLM\n            powerful general-purpose learners, excelling at                                                              agents in a coordinated system has shown even\n                1Work done during internship at Amazon.                 greater promise (Hong et al., 2024a; Zhong et al.,\n\n\n                                                    1\n\n2024; Wang et al., 2024b). Multi-agent systems   polynomial time complexity. Furthermore, to ad-\n(MAS) leverage diverse perspectives and roles,   dress the inherent ambiguity in credit assignment,\nsuch as critics, verifiers, or debaters, to collec-  MAPRO introduces a topology-aware refinement\ntively outperform single-agent counterparts in rea-   procedure that maintains distinct prompt policies\nsoning, exploration, and robustness (Shinn et al.,    for each agent rather than collapsing the system\n2023; Qian et al., 2025; Wang et al., 2025). Yet,    into a single global policy. By distributing credit by\nconstructing effective MAS is far from straightfor-   incorporating the blames from downstream agents,\nward. A recurring difficulty lies in prompt sensi-  MAPRO progressively converging toward a set\ntivity, where small variations in instructions can    of coordinated yet agent-specific prompt policies\ndrastically alter behavior and degrade performance    that enhance overall system robustness and perfor-\n(Zhou et al., 2024). In multi-agent settings, where   mance. Through iterative optimization, MAPRO\noutputs cascade across agents, such fragility may   produces multi-agent systems that achieve state-of-\nbe compounded, amplifying instability across the    the-art performance, surpassing both manually en-\nsystem (Zhou et al., 2025).                         gineered MAS baselines and automatically gener-\n  To mitigate these challenges, recent work has ex-   ated alternatives in single- and multi-agent settings.\nplored various forms of automated prompt design   These improvements are consistently demonstrated\nand system adaptation. Broadly, these approaches    across diverse tasks, including mathematical rea-\naim to reduce reliance on manual engineering by    soning, question answering, and code generation.\nalgorithmically refining prompts, adjusting agent   Our contributions can be summarized as follows:\nroles, or restructuring interaction patterns (Khattab\net al., 2024; Hu et al., 2025). However, despite        • MAP Inference Formulation. To our best\nthese advances, the problem of prompt optimiza-        knowledge, we are the first to cast multi-agent\ntion in multi-agent settings remains largely under-       prompt optimization as a Maximum a Posteri-\nexplored. This gap arises from two core challenges:         ori (MAP) inference problem. This formula-\n(1) the search space grows combinatorially as each         tion provides a principled objective for navi-\nagent maintains its own set of prompt candidates,        gating the combinatorial search space, enables\nmaking it extremely difficult to navigate efficiently          efficient approximation of globally optimal\nand leaving the system vulnerable to suboptimal       prompt sets, and offers general guidelines for\nlocal choices rather than coordinated global im-        systematic prompt optimization design.\nprovement; (2) credit assignment is highly uncer-\n                                                                 • Topology-aware Credit Assignment. We\ntain, since it is rarely clear which agent’s prompt\n                                                   propose a novel refinement mechanism that\nshould be targeted for refinement, how it should be\n                                                              integrate execution feedback and downstream\nmodified, or whether adjustments at the individual\n                                                     blames, which alleviate the challenge of am-\nlevel will even translate into system-wide gains.\n                                                    biguous credit assignment, enabling targeted\n  To  tackle  these  challenges,  in  this  paper,\n                                                  improvements to specific agents with distinct\nwe propose Multi-Agent PRompt Optimization\n                                                 prompt policies.\n(MAPRO), a four-stage framework that jointly ex-\nplores the multi-agent prompt space, propagates                                                                 • State-of-the-Art Performance. On diverse\nfeedback signals, and iteratively refines prompt                                                  benchmarks—including mathematical reason-\npolicy of each agent. By grounding optimization in                                                             ing, question answering, and code genera-\na principled inference process, MAPRO provides                                      tion—MAPRO consistently surpasses man-\na structured approach for navigating the otherwise                                                          ually engineered MAS baselines and recent\nintractable combinatorial landscape of MAS de-                                                    automated alternatives, establishing new state-\nsign.  Specifically, to cope with the exponential                                                             of-the-art results in multi-agent prompt opti-\nsearch space, we formalize multi-agent prompt                                                         mization.\noptimization as a Maximum a Posteriori (MAP)\ninference problem over Directed Acyclic Graphs   2  Preliminary\n(DAGs), and develop a language-guided variant\n                                                    2.1  Multi-agent System as Directed Graphof the max-product belief propagation (MPBP) al-\ngorithm.  This design leverages agent-level and  We study a multi-agent system (MAS) composed of\ninteraction-level reward models to efficiently ap-  N large-language-model agents that collaborate on\nproximate globally optimal prompt assignments in   a shared code-generation workflow. Let the index\n\n\n                                         2\n\nset of agents be A = {1, . . . , N}. For every agent    practice, for agent i, there will be K agent scores,\ni ∈A, the agent-specific prompt candidate pool is   and the same logic applies for the edge scores as\ndefined as                                            well, so for (i, j), there will be K2 edge scores.\n                                                    Therefore, the objective of MAS prompt optimiza-\n          Pi =   p1i , p2i , . . . , pKi    ,         (1)    tion can be defined as:\n\nwhere p(k)i    is the k-th candidate prompt (k =       P ∗= argmaxP∈P1×···×PN T (P).     (3)\n1, . . . , K) and K is the uniform pool size.  For\nclarity, we denote by p∗i the optimal prompt can-   Equation (3) can be viewed as the posterior likeli-\ndidate, while ˜pi represents a selected prompt can-   hood that the entire system completes the evalua-\ndidate drawn from the candidate pool. Because    tion batch without error, conditioned on the hidden\ncollaboration unfolds through directed hand-offs   prompt set P.  Indeed, if we regard each agent\nof textual outputs, we encode these dependencies   outcome and each edge hand-off as independent\nas a directed graph G = (V, E), V = A, in which    Bernoulli events given P, then under a uniform\neach vertex i ∈V corresponds to agent i, and a    prior over prompt sets, maximizing T (P ) is thus\ndirected edge (i, j) ∈E signifies that the output    equivalent to the classical maximum-a-posteriori\nof agent i is consumed as (part of) the input of   (MAP) problem (Proved in Appendix-C.1)\nagent j. This graph abstraction concisely captures                                Why is this problem challenging?\nthe information-flow topology that underpins the\n                                      As can be seen, the brute-force search spacesubsequent optimization problem.\n                                          P1 × · · · × PN contains KN discrete combina-\n2.2 MAS Prompt Optimization                     tions.  Moreover, the factors are highly interde-\nA prompt set ˜P = (˜p1, . . . , ˜pN) is considered suc-   pendent: changing a single prompt pi can affect\n                                      many downstream agents, making greedy or lo-cessful if the entire workflow executed successfully\n                                                       cal strategies prone to failure. As the objective isand correctly. Unlike single-agent settings, fail-\n                                                non-convex, discontinuous, and combinatorial, ef-ures in MAS stem from two sources: 1) Agent In-\n                                                        fective optimization must exploit additional struc-competence—producing incorrect code even from\n                                               ture—here, the acyclic topology of the graph Gwell-formed input, thereby propagating errors; and\n                                                  (with a prescribed iteration limit)—to prune the2) Defective Interaction — an upstream agent\n                                                   search space and assign credit correctly among in-returning semantically irrelevant text that blocks\n                                                       teracting prompts. In the next section, we presentdownstream progress. Both hazards need to be\n                                             an algorithm that leverages these properties to ap-properly addressed to achieve good performance.\n                                                proximate P ∗in polynomial time.  To make these notions quantitative, we define the\nagent score to record the empirical quality of the k-\n                                       3  Methodology\nth prompt of agent i as g(pki ), and the edge score to\nmeasure the reliability of the corresponding hand-  Now that we have formalized the optimization ob-\noff between the k-th prompt of agent i and the l-th                                                        jective for multi-agent prompt optimization and\nprompt of agent j as g(pki , plj). Both measures    highlighted the challenges, we proceed to detail\nlie in [0, 1], where the value 1 denotes flawless   our proposed framework, Multi-Agent PRompt\nbehavior. To maximize the system performance   Optimization (MAPRO). MAPRO addresses the\nand to reflect that the overall workflow is only as    Multi-agent System (MAS) prompt-optimization\nreliable as its weakest link, we propose the Joint   problem by formulating it as a discrete maximum-\nQuality Score for the multi-agent system as:          a-posteriori (MAP) inference over the joint prompt\n                                                space and solving it via an iterative, LLM-guided          N\n  T (˜P) = Y g(˜pi) Y  g(˜pi, ˜pj)  .   (2)    algorithm. In particular, our method comprises four\n                                                       stages: (1) Initialization of prompt candidates and               i=1          (i,j)∈E\n                                              reward models; (2) Language-based MAP selec-\nNote that we put ˜p in the equation here and omit    tion, which employs LLM-based reward models to\nk and l for simplicity. Intuitively, the performance    conduct max-product belief-propagation algorithm\nT (˜P) of a MAS is good when every agent is com-    to efficiently find the optimal prompt combination\npetent and every hand-off is clean, because a single    (Figure-2 c.1); (3) Preference-based prompt policy\nfailure at any node or edge derails the execution. In    update, which updates the prompt pools and reward\n\n\n                                         3\n\nSelect & Update                                                           Update                    Node\n                                                                                 Reward                                                                                                                                     (N2)  N3                                                                                                                             g(P1 )                                                TPE Select                                                                                                                  m2→1 (N1)   m3→2\n                                                                                                           N1           N2\n                                                                                                                                                g(P1, P2 )\n                                                                                       Minibatch    Edge      …           m4→2                                                                                                                                     (N2)   N4      Isolate Improvement  Fitness     Sub-Optimal  Prompt Sets        Fitness      Results    Reward        …\n        a.1) Direct Optimization                     a.2) TPE Optimization               Inputs     Scoring    Max-Product Belief Propagation\n                                                                                                          c.1) Language-based MPBP Selection           a) Existing Methods for MAS Prompt Optimization\n                                                                                                                Update\n                                                                                                             Failure Summary\n                        initialize               Prompt Sets                              Global Feedback\n    Agent                                                    c.2)                                 Downstream Blame        Updated\n    Node                                                                        Topology-aware                        Demos\n                                                     Update                   Local Feedback    Mutation Strategy\n   Prompt\n                           initialize                                c.1)                                                            Optimized\n   Demo                                                                                                           Prompt Sets\n            Initialization                                           Select        Fitness               c.2) Preference-based Policy Update\n            b) Our Method: The Overall Framework of MAPRO             c) Our Method: Detailed Modules of MAPRO\n\nFigure 2: The Overall Framework of MAPRO. Specifically, a) shows the existing methods of prompt optimization\nfor MAS and their drawbacks; b) shows the overall framework of MAPRO compared with existing methods; and c)\ndemonstrate the detailed modules used in MAPRO.\nmodels based on multi-level feedback (Figure-2    collect their full interaction traces if the entire task\nc.2); and (4) Termination, which defines stopping     is solved correctly end-to-end on B Each such suc-\ncriteria and yields the final optimized prompt set    cessful trace serves as the initial positive exemplars\nfor downstream left-out testing. We next describe    - the prompt pi used is considered d+ and the output\neach stage in detail.                             produced can be recorded as a desirable response\n                                                       for that agent; likewise, for every edge, the mes-\n3.1   Initialization                                                  sage serves as an initial example of a good hand-off.\nPrompt Candidate Pool Setup. MAPRO is de-   To obtain complementary failure example prompts,\nsigned as a plug-and-play setting atop any given   we then generate synthetic negatives by perturbing\nMAS. We assume an established MAS as G =    the successful prompts as d−. Thus we obtain, for\n(V, E) (as defined in the preliminaries) and an ini-   each agent and edge, a pool of preference prompt\ntial set of base prompts P 0 = {p01, . . . , p0N} for   responses D = {d+, d−}. The detailed input of\nthe N agents. The first step is to construct a di-   the reward model are provided in the section below.\nverse prompt candidate pool Pi for each agent i\n                                                    3.2  Language-based MAP Selectionby mutating its original prompt to K candidates\nfollowing standard practice (Wang et al., 2024c;   Given the MAP formulation of prompt optimiza-\nXiang et al., 2025), yielding semantically similar    tion from above, during the action selection phase,\nvariants Pi =  p1i , p2i , . . . , pKi    .                  our goal is to efficiently find the prompt assign-\n                                           ment P ∗=   p∗1, p∗2, . . . , p∗N   that maximizesPreference Demonstration Pool Setup.  Inspired\n                                                    the joint quality score. As previously discussed,\nby the reward model design of TPO (Li et al.,\n                                                        directly searching the exponentially large space\n2025), which demonstrate human preferences can\n                                          P1 × · · · × PN is intractable. Therefore, we exploit\nbe aligned during inference without retraining and\n                                                    the factorized structure of the multi-agent system\nachieve comparable results, we instantiate a reward\n                                     (MAS) and introduce LLM-guided Max-Product\nmodel R and seed the reward model with a set of\n                                                       Belief Propagation (LMPBP), which consists of\naccepted (positive) and rejected (negative) exam-\n                                           two steps, specifically, reward model scoring and\nple prompt as few-shot preference demonstrations\n                                                  optimal prompt searching.\nto guide it to generate scalar scores for each agent\nnode and edge. Intuitively, it will judge the quality   Reward Model Scoring.  In the first stage, we\nof an agent’s output in isolation, and the quality of   prompt the reward model R to assign numerical\na hand-off between agents.                         scores between 0 to 1. For each agent i, the re-\n  To initialize these examples, we first run the   ward model Ri will rank each prompt pki ∈Pi\nMAS on a mini-batch of training tasks B using a   and evaluates how well the prompt would enable\nfew random draws from the prompt pool Pi. and    agent i to fulfill its role. This evaluation is condi-\n\n\n                                         4\n\ntioned on the preference demonstrations Di and the   Algorithm 1 MAPRO Overall Process\ncorresponding input xi and desirable response yi:       1: Initialization: Set up prompt pools P, and\n                                                     demonstration preferences D.\n          g(pki ) = Ri(xi; yi; Di; Pi),         (4)      2: while termination condition not met, do\n                                                                     3:       // Language-based MAP Selection\nSimilarly, for each directed edge (i →j), the                                                                     4:     Retrieve reward scores g(pi) and g(pi, pj).\nreward model Rij produces a score g(pki , plj) re-      5:    Upward pass to retrieve localized optimal\nflecting how well agent i’s output under prompt pi                                                             score mi→j(pj) at each node.\nwould set up agent j for success. Concretely, we                                                                     6:    Downward pass to assign best prompt p∗\nhave:                                                           given parents’ choices.\n          g(pki , plj) = Rij(yi, Dj; Pj),        (5)      7:    Run with P ⋆on task B; update score S(t).\n                                                                     8:       // Preference-guided Policy UpdateThis way, we have obtained the reward scores for\n                                                                     9:    Update D ←Critic(D; P; g(P)).factors required in the searching step.\n                                                                                                         ∗                                                              10:    Get P ← Mutate(M(P ∗), fg, fl), P\nOptimal Prompt Searching  After the reward     11:       if improvements ≤ε over T steps then\nscores are secured,  the second stage applies     12:       break\nLMPBP to find the global optimum T (P) exactly     13:    end if\nin the DAG by passing local messages that aggre-    14:      t ←t + 1\ngate optimal sub-solutions. For MAS with multiple     15: end while\nparent dependencies, we convert the structure to     16: Inference: Freeze P ⋆; test on held-out tasks.\nequivalent tree-structured via a junction-tree trans-\nformation (Implementation details and equivalence\n                                             prompt for the root:proof in Appendix-C.2). The message-passing pro-\ncess works as follows: First, it goes through a leaf-                                                            p∗r = arg max βr(pr).           (7)\nto-root pass. (Note here the notations have different\nmeanings) Assume agent i receives messages from       Finally, we perform a downward pass to fix the\nall of its children (downstream agents for which i is   prompts of the remaining agents based on the root\nan input), and then sends an aggregated message up    decision. We visit each child i of the root and\nto its own parent j. Specifically, for each possible   choose the prompt that attained the maximum in\nprompt choice of its parent, agent i computes       mi→r(pr):\n\n                                                  p∗i = arg maxh g(pi) g(pi, p∗r) Y mk→i(pi)i .                hmi→j(pj) = max                   g(pi)g(pi, pj) Y mk→i(pi)i .                                       k∈Child(i)\n                                    k∈Child(i)                                                               (8)\n                                                 (6)   This gives the optimal prompt for agent i assuming\n  Here Child(i) denotes the set of agents that de-   the root was p∗r. We then recursively select their\npend on i’s output.  Intuitively, mi→j(pj) repre-   best prompt given p∗i , and so on, until all agents\nsents the best achievable joint score of the entire    in the graph have an assigned prompt. This back-\nsubtree rooted at i, given that i’s parent j is fixed    tracking procedure propagates the optimal choices\nto prompt pj. In other words, i considers all its   down the tree, yielding the globally optimal prompt\nown prompt options and those of its descendants,    set P ∗(Proved in Appendix-C.3). This selected\nand encapsulates the optimal outcome (in terms of   prompt set will next be used in the refinement stage\nproduct of local scores) in a message indexed by pj.    to collect feedback and further improve the prompt\nOnce the upward messages reach the designated    pools and reward models.\nroot agent r (the entry point of MAS), we calcu-\n                                                    3.3  Preference-based Policy Updatelate the root belief and that agent can evaluate the\ntotal score for each of its prompt candidates using   The MAP selection phase yields the global optimal\nequation-6.                                   prompt set P ∗, along with an explicit assessment\n  This combines r’s own goodness score with the    of each agent prompt and hand-offs via the reward\nmessages from all its children (each of which al-    scores. In the prompt policy refinement phase, we\nready accounts for the best configuration of that    leverage this information, together with actual ex-\nchild’s subtree). We then select the highest-belief    ecution feedback or diffs on tasks, to update and\n\n\n                                         5\n\nimprove the prompt pools and reward models. The    3.4  Termination\nkey idea is to incorporate feedback from multiple                                  We iterate the select–update loop until the improve-\nlevels: (i) global-wise execution results, (ii) down-                                            ments in the joint reward have saturated, indicat-\nstream agent blames, and (iii) controlled prompt                                                  ing convergence to an optimal prompt policy. To\nmutation strategy to force small edits. By inte-                                                  formalize the stopping criterion, let S(t) denote\ngrating the multi-level feedback, we can introduce                                                     the joint validation score (e.g., pass rate) obtained\ntargeted prompt variations to explore new parts of                                          by the best prompt set at iteration t. We define\nthe search space. After refinement, the MAS is                                     ∆St = S(t) −S(t−1) as the improvement in reward\nready to perform another round of MAP-based se-                                           compared to the previous iteration. We choose a\nlection with updated components. We detail the                                                     fixed patience window size T (e.g., T = 3) and a\nfeedback collection and update steps below.                                                 small tolerance ε ≥0. After each iteration t ≥T,\nReward and Expected Output Update. We    collect the recent gains {∆St−T+1, . . . , ∆St}. and\nevaluate the performance given P ∗on a set of rep-  we terminate the optimization loop when\nresentative tasks (e.g., the training question batch\nB) and we update each agent’s outputs yi as the            max  ∆St−i+1 ≤ε,         (11)                                                                                       i=1,...,T\nnew desirable response for next cycle of updates.\n                                             which means no improvement exceeding ε hasWe then use the reward scores as standards to up-\n                                             been observed in the  last T  iterations.   Thisdate the accepted and rejected prompt responses\n                                                       rule halts exactly when the system has shownfor each agent. Specifically, we use a critic LLM\nmodel to judge if for agent i, the prompt pki should   no progress over the specified window, ensuring\nbe updated as d+ or d−. Formally,                     that computation stops once the prompt policy has\n                                                    plateaued. After termination, we obtain the final\n        Di ←Critic(Di; Pi; g(Pi)).        (9)   optimized prompt set P ∗for test-time inference\n                                           on unseen tasks. By locking in P ∗, we ensure the\nThis process make sure the preference demonstra-                                                      efficiency of the system that no additional time is\ntions are continually updated so that the reward                                                    required during inference. The time complexity of\nscores are more closely align with actual task suc-                                                        training phase is analyzed in Appendix-C.4\ncess.\n                                       4  Experiments\nPrompt Pool Refinement. We improve  the\nprompt candidate pool by generating new varia-   4.1  Experimental Setup\ntions using a mutate LLM model with innovative\n                                         Benchmarks. We conduct experiments on an ex-\nfeedback design from three aspects: global feed-\n                                                       tensive collection of tasks: HumanEval-ET, MBPP-\nback fg indicating the final execution feedback;                                                 Plus and CodeContest for code generation task,\nlocal feedback fl which takes the reversed topol-                                   NewsQA and WebQuesion for question answering\nogy and ask each agent to generate blames to it\n                                                         task, and MATH and GSK8K for math reasoning\nupstream agents based on their generated input and\n                                                         task. Since we are focusing on the prompt opti-\nfg, achieving fine-grained credit assignment; and                                                mization and have a training scheme, we discuss\na predefined mutation strategy set with small ed-\n                                                    the split of sets with other details including cita-\nits M which mimics the idea of trust region in\n                                                        tions of these benchmarks in Appendix-B.1.\nMAP policy optimization, keep the prompt varia-\n                                                     Baselines. We consider the following types of base-\ntions from drifting afar. Formally, we invoke an\n                                                           lines: 1) Single agents without prompt optimiza-\nLLM-based prompt mutation function to produce a\n                    new                              tion, including the raw model, and the most classi-refined prompt pools P     that modifies P ∗:\n                                                         cal baselines CoT and ReAct; 2) Single agents with\n    new                                     prompt optimization, including two most recent  P i  =  Mutate(M(p∗i ), fg, fl), p∗i   .   (10)\n                                  SOTA baselines EvoPrompt, and PromptBreeder.\nThrough such prompt augmentation, the MAS ex-   3) Classical Multi-agent baselines without prompt\nplores new prompts that are informed by past fail-   optimization. While there are many MAS, we hope\nures yet remain close to proven good prompts,    to choose the ones that are designed for general\nthereby continuously improving robustness.  Fi-    tasks, recent SOTA and covering as many types\nnally, the updated prompt pools are then used in    of common topologies as possible, therefore we\nthe next iteration of MAP-based selection.          choose Chain Design, DMAD, and the \"swarm\"\n\n\n                                         6\n\nBackbone: Claude Haiku 3.5                 Code Generation                   Question Answering         Math Reasoning\n\n     Model     MAS  Optimized  HumanEval-ET  MBPP-Plus  CodeContest   NewsQA    WebQuestion   MATH    GSM8K\n      Raw        ✗       ✗         69.38 ± 3.24    70.93 ± 0.34   20.36 ± 2.29   49.12 ± 0.11   33.50 ± 0.41   59.54 ± 1.10  88.57 ± 0.53\n      CoT        ✗       ✗         70.31 ± 1.91    71.98 ± 0.39   22.91 ± 1.46   54.44 ± 0.30   33.22 ± 0.32   60.25 ± 0.56  90.71 ± 0.34\n      ReAct        ✗       ✗         72.19 ± 1.71    71.02 ± 0.31   21.21 ± 0.61   58.72 ± 0.30   33.60 ± 0.34   61.29 ± 1.03  91.50 ± 0.39\n    EvoPrompt      ✗     ✓        75.63 ± 2.10    73.97 ± 0.48   22.18 ± 1.26   60.44 ± 0.74   34.99 ± 0.39   60.81 ± 1.66  92.37 ± 0.31\n   PromptBreeder     ✗     ✓        75.31 ± 0.70    74.13 ± 0.22   21.45 ± 1.47   60.76 ± 0.17   35.12 ± 0.51   60.43 ± 0.52  92.24 ± 0.18\n       Chain      ✓       ✗         71.88 ± 1.10    74.34 ± 1.14   28.85 ± 2.29   60.88 ± 0.72   34.85 ± 0.40   62.82 ± 0.89  92.06 ± 0.27\n     w/t Direct     ✓     ✓        73.96 ± 2.30    74.87 ± 0.53   29.70 ± 0.61   62.20 ± 1.31   34.25 ± 0.21   63.80 ± 0.21  92.76 ± 0.14\n      w/t TPE     ✓     ✓        75.00 ± 1.56    75.22 ± 0.40   29.90 ± 0.93   63.80 ± 0.20   34.01 ± 0.13   62.81 ± 0.37  92.72 ± 0.21\n    w/t MAPRO    ✓     ✓        80.21 ± 0.90    76.54 ± 0.67   31.52 ± 0.61   64.00 ± 0.35   34.65 ± 0.30   64.30 ± 0.59  93.48 ± 0.42\n    DMAD     ✓       ✗         72.19 ± 1.31    73.02 ± 0.37   36.77 ± 0.93   60.40 ± 0.37   34.43 ± 0.44   61.08 ± 0.39  90.39 ± 0.46\n     w/t Direct     ✓     ✓        73.44 ± 1.56    74.07 ± 0.27   38.79 ± 0.61   62.20 ± 0.20   34.91 ± 0.07   62.85 ± 0.11  91.19 ± 0.36\n      w/t TPE     ✓     ✓        72.92 ± 2.39    73.54 ± 0.53   37.58 ± 0.61   61.80 ± 0.35   35.22 ± 0.13   62.81 ± 0.37  91.87 ± 0.29\n    w/t MAPRO    ✓     ✓        77.08 ± 1.81    74.60 ± 0.27   38.99 ± 1.95   62.93 ± 0.12   35.50 ± 0.33   63.33 ± 0.46  91.96 ± 0.58\n ChatEval (Swarm)  ✓       ✗         73.44 ± 1.10    72.60 ± 0.34   38.79 ± 1.05   60.36 ± 0.26   33.33 ± 0.34   62.62 ± 0.97  91.59 ± 0.89\n     w/t Direct     ✓     ✓        74.48 ± 2.38    73.19 ± 0.46   38.18 ± 0.61   61.80 ± 0.20   34.17 ± 0.29   63.83 ± 0.84  91.42 ± 0.47\n      w/t TPE     ✓     ✓        76.04 ± 0.90    73.28 ± 0.26   40.61 ± 0.61   62.53 ± 0.31   34.35 ± 0.18   62.68 ± 0.58  91.31 ± 0.62\n    w/t MAPRO    ✓     ✓        78.13 ± 1.56    73.98 ± 0.15   41.41 ± 0.93   62.67 ± 0.31   34.52 ± 0.20   63.13 ± 0.84  91.73 ± 0.41\n\nTable 1: Performance results with baseline methods on Claude Haiku 3.5. We report the mean and standard deviation\nfor all results. The best performance is bolded and runner-ups are underlined.\n\n Prompt Optimization Example\n\n Base instruction:\n You are a Python programmer. Write pure, runnable Python\n  code that solves the task.\n\n Adding:\n You are a Python programmer. Write pure, runnable Python\n code that solves the task. Ensure the solution is a single\n  function named solution with robust input validation,\n  direct implementation, and no type hints. Handle edge\n  cases explicitly and provide clear, executable code.\n\n Replacement:\n You are a Python programmer. Write pure, runnable Python\n code that solves the task. Ensure the solution is a single\n  function named solution with robust input validation us-\n                                                     Figure 3:  Optimization trajectories on the MBPP+  ing isinstance() checks, type conversion fallbacks, and\n  comprehensive error handling. Use try-except blocks with     benchmark. We report the first ten optimization iter-\n  specific exception types, provide default values for edge     ations using the chain MAS framework. MAPRO ex-\n  cases, . . .                                                    hibits a more consistent and steady improvement com-\n                                                     pared to alternative methods.\nTable 2: \"Adding\" appends guidance to the prompt\n                                                    4.2  Main Resultswhile \"Replacement\" rewrites previous parts with better\ninstructions. Colors highlight the modified texts.      We present the main results of MAPRO against\n                                                    baselines in Table 1 and Table 3.  Across all\n                                              benchmarks, MAPRO consistently achieves su-version of ChatEval, which is Simultaneous-Talk-\n                                                     perior performance, often setting the best resultswith-Summarizer. 4) Prompt optimization for MAS\n                                                  within the same MAS, underscoring the strengthbaselines.  Since we only consider prompt opti-\nmization2, we adopt the direct optimization method    of our approach.   Several additional insights\n                                               emerge. In terms of MAS structure, while topol-from GPTSwarm and Tree-structured Parzen Esti-\n                                          ogy exerts a stronger influence on overall accu-mator (TPE) methods used in MASS and MIPRO\n                                                  racy than prompts, our plug-and-play design of-to each of the MAS we described above to make a\n                                                         fers unmatched flexibility and extensibility, avoid-comprehensive comparison. More details including\n                                                  ing the heavy cost of topology optimization andthe citations of the baselines are in Appendix-B.2.\n                                                 enabling efficient deployment. As for task char-\n                                                            acteristics, MAPRO delivers the largest gains\n                                           on reasoning-intensive tasks (e.g., WebQuestions,\n    2In this paper, we consider the plug-and-play settings an   MBPP-Plus) compared to knowledge-heavy ones\nunique advantage and don’t update the topology because this\n                                                               (e.g., NewsQA, CodeContest), highlighting theis more friendly to industry scenarios where teams already\nhave their developed MAS implemented and in production.     unique advantage of prompt optimization in com-\n\n\n                                         7\n\nBackbone: Llama 3.3-70b                  Code Generation                   Question Answering         Math Reasoning\n\n     Model     MAS  Optimized  HumanEval-ET  MBPP-Plus  CodeContest   NewsQA    WebQuestion   MATH    GSM8K\n      Raw        ✗       ✗         67.81 ± 0.86    68.04 ± 0.68   19.76 ± 2.08   58.65 ± 0.96   33.15 ± 0.50   67.56 ± 1.44  91.71 ± 0.37\n      CoT        ✗       ✗         68.44 ± 1.31    68.04 ± 0.22   21.09 ± 1.31   60.56 ± 0.35   34.35 ± 0.47   69.01 ± 0.66  92.06 ± 0.29\n      ReAct        ✗       ✗         69.06 ± 0.70    68.15 ± 0.44   20.36 ± 0.69   62.06 ± 0.28   35.84 ± 0.54   69.26 ± 0.84  92.34 ± 0.27\n    EvoPrompt      ✗     ✓        72.19 ± 1.71    69.74 ± 0.64   20.85 ± 1.33   64.37 ± 0.34   35.47 ± 0.41   71.62 ± 0.38  93.12 ± 0.19\n   PromptBreeder     ✗     ✓        71.88 ± 1.10    69.10 ± 0.29   20.85 ± 1.40   64.53 ± 0.43   35.77 ± 0.16   71.01 ± 0.44  93.05 ± 0.23\n       Chain      ✓       ✗         70.00 ± 1.31    68.68 ± 0.14   27.52 ± 2.08   63.20 ± 0.45   35.24 ± 0.28   71.45 ± 1.04  93.71 ± 0.25\n     w/t Direct     ✓     ✓        71.35 ± 1.80    69.58 ± 0.46   28.08 ± 0.70   63.62 ± 0.27   36.16 ± 0.18   70.37 ± 0.57  93.89 ± 0.31\n      w/t TPE     ✓     ✓        71.88 ± 1.56    70.28 ± 0.40   28.69 ± 0.93   63.80 ± 0.21   36.04 ± 0.03   71.64 ± 0.62  93.42 ± 0.22\n    w/t MAPRO    ✓     ✓        75.00 ± 1.56    72.31 ± 0.55   30.10 ± 0.70   63.82 ± 0.54   36.22 ± 0.30   71.87 ± 0.35  93.56 ± 0.34\n    DMAD     ✓       ✗         70.94 ± 0.86    70.37 ± 0.46   34.06 ± 0.90   63.82 ± 0.54   35.56 ± 0.11   69.85 ± 0.42  94.12 ± 0.19\n     w/t Direct     ✓     ✓        71.88 ± 1.56    70.90 ± 0.26   35.35 ± 0.70   64.12 ± 0.35   36.02 ± 0.20   70.99 ± 0.33  94.93 ± 0.33\n      w/t TPE     ✓     ✓        71.35 ± 1.80    70.55 ± 0.40   34.55 ± 0.61   64.30 ± 0.29   36.14 ± 0.22   71.32 ± 0.26  94.67 ± 0.28\n    w/t MAPRO    ✓     ✓        73.96 ± 0.90    71.60 ± 0.31   35.96 ± 1.53   65.10 ± 0.24   36.22 ± 0.28   72.99 ± 0.33  95.91 ± 0.30\n ChatEval (Swarm)  ✓       ✗         71.25 ± 0.86    71.43 ± 0.19   34.91 ± 1.01   63.73 ± 0.43   36.44 ± 0.32   71.32 ± 0.26  93.14 ± 0.31\n     w/t Direct     ✓     ✓        72.92 ± 0.90    70.99 ± 0.31   35.15 ± 0.61   64.02 ± 0.26   36.36 ± 0.27   71.73 ± 0.57  92.58 ± 0.37\n      w/t TPE     ✓     ✓        72.40 ± 2.39    71.78 ± 0.31   36.36 ± 0.61   64.18 ± 0.31   36.41 ± 0.27   71.48 ± 0.80  92.45 ± 0.34\n    w/t MAPRO    ✓     ✓        75.00 ± 1.56    72.22 ± 0.26   37.17 ± 0.93   65.45 ± 0.09   36.55 ± 0.29   72.26 ± 0.71  92.38 ± 0.40\n\nTable 3: Performance results with baseline methods on Llama 3.3-70b. We report the mean and standard deviation\nfor all results. The best performance is bolded and runner-ups are underlined.\n\n Method    HumanEval-ET  MBPP-Plus  CodeContest   NewsQA    WebQuestion   MATH    GSM8K\n\n MAPRO      80.21 ± 0.90    76.54 ± 0.67   31.52 ± 0.61   64.00 ± 0.35   34.65 ± 0.30   64.30 ± 0.59  93.48 ± 0.42\n w/o demos    76.04 ± 0.90    75.22 ± 0.31   29.70 ± 0.61   62.33 ± 0.31   34.20 ± 0.35   63.87 ± 0.23  92.86 ± 0.15\n\n Drop (%)       5.20%         1.72%        5.78%        2.61%        1.30%        0.67%       0.66%\n\nTable 4: Ablation study results showing the performance drop when removing demonstration-guided reward.\nNumbers are reported with mean ± standard deviation, and relative drops are given in percentage.\n\nplex reasoning. For LLM backbones, the results    the-loop process and bring in more reliability and\nreaffirm general trends—Haiku excels in code,   robustness to the system.\nwhereas Llama is stronger in reasoning—but also\nshow that MAPRO adapts well across both. No-   4.4  Reward Model Analysis\ntably, the optimal MAS under Llama shifted to-   To assess the incremental gains of the reward mod-\nward more sophisticated designs, suggesting that     els, we conducted an ablation study across all tasks.\nstronger reasoning models further amplify the ben-  As shown in Table 4, the results underscore the\nefits of our framework. Overall, these results val-    critical role of demonstration-guided reward, con-\nidate MAPRO as both more effective and more    sistent with TPO (Li et al., 2025). A key insight\nversatile than existing methods, with significant     is that the contribution of demonstrations varies\npotential for even greater gains on future LLMs.     across tasks, likely due to the relative simplicity\n                                                   of certain benchmarks such as those in the math\n4.3  Optimization Trajectory                   domain. We also examined the consistency of the\n                                                  scoring process: under a low temperature setting,\nWe  visualize  the  optimization  trajectory  of\n                                                    the selection procedure produced nearly identical\nMAPRO as shown in Figure-3. MAPRO’s tra-\n                                              outcomes across tasks and MAS configurations, so\njectory demonstrates a more steady trend of opti-\n                                    we omit ablation on that front. This robustness,\nmization that gradually improves the validation per-\n                                                     together with the ablation results, demonstrate the\nformance towards better prompt sets, whereas we\n                                                       efficacy of our reward model design.\nobserve more fluctuations when it comes to other\noptimization methods, as they have a hard time cap-                                       5  Conclusion\nturing complicate interplays between agents. We\nfurther inspect an example of optimized prompt  We introduced MAPRO, a principled framework\ntrajectory of an agent node in Table-2. As can    that first recasts multi-agent prompt optimization as\nbe seen, the prompt evolves overtime with more   a MAP inference problem and resolves it through\nprecise instructions that provides task-specific in-   language-guided belief propagation and topology-\nsights. These insights, especially the repeatedly   aware refinement.  Across diverse downstream\noccurring refinements, in practice, can be ingested    tasks, MAPRO consistently surpasses all types of\ninto knowledge base which facilitates human-in-    baselines, demonstrating its effectiveness and gen-\n\n\n                                         8\n\nerality. Beyond strong empirical gains, MAPRO   Limitations\ndelivers a plug-and-play setting that balances accu-\n                                                   In this section, we discuss the limitations of our\nracy with flexibility. This flexibility, together with\n                                           work and outline promising directions for future re-\nthe interpretability provided by the optimization\n                                                       search. First, our study focuses exclusively on opti-\ntrajectories, making MAPRO especially practical\n                                               mizing prompts for MASs while keeping the agent\nfor real-world MAS deployment and improvement.\n                                                 topology fixed. The results suggest that additional\n                                                  gains could be achieved through more deliberate\n                                                  choices of MAS topology. However, updating the\n                                                topology requires reconfiguring the entire system,\n                                            which is substantially more complex and resource-\n                                                       intensive. In many industrial applications where\n                              MAS designs are already deployed or constrained\n                                            by fixed requirements, such re-establishment is im-\n                                                         practical.  This reflects a fundamental trade-off\n                                            between flexibility, extensibility, efficiency, and\n                                                performance.  Nevertheless, extending MAPRO\n                                                      to jointly optimize both prompts and topologies\n                                            would be an exciting avenue for future exploration.\n                                                Second, while we employed LLM-based agents as\n                                                reward models and demonstrated their efficacy and\n                                                       consistency, it would be valuable to investigate fine-\n                                                tuned alternatives. In particular, approaches such\n                                                   as Max a Posteriori Policy Optimization (Abdol-\n                                               maleki et al., 2018) offer a principled framework\n                                                        that could replace our current reward mechanism\n                                              and integrate more seamlessly into the overall opti-\n                                                 mization process. Exploring such directions could\n                                                      further enhance the robustness and generality of\n                                                 our approach.\n\n\n                                          References\n\n                                             Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval\n                                                              Tassa, Remi Munos, Nicolas Heess, and Martin Ried-\n                                                                    miller. 2018. Maximum a posteriori policy optimisa-\n                                                                    tion. In ICLR.\n\n                                                      Jonathan Berant, Andrew Chou, Roy Frostig, and Percy\n                                                        Liang. 2013.  Semantic parsing on freebase from\n                                                         question-answer pairs. In EMNLP.\n\n                                                Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,\n                                                Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.\n                                                        2024. Chateval: Towards better llm-based evaluators\n                                                        through multi-agent debate. In ICLR.\n\n                                                   Jingchang Chen, Hongxuan Tang, Zheng Chu, Qiang-\n                                                       long Chen, Zekun Wang, Ming Liu, and Bing Qin.\n                                                      2024a. Divide-and-conquer meets consensus: Un-\n                                                          leashing the power of functions in code generation.\n                                                           In NeurIPS.\n\n                                               Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang,\n                                                       Nicholas Roy, and Chuchu Fan. 2024b. Prompt op-\n                                                            timization in multi-step tasks (promst): Integrating\n                                             human feedback and heuristic-based sampling. In\n                                          EMNLP.\n\n\n                                         9\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,    Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu\n  Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias      Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang,\n   Plappert, Jerry Tworek, Jacob Hilton, Reiichiro        Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al.\n  Nakano, et al. 2021. Training verifiers to solve math      2024b.  Metagpt: Meta programming for a multi-\n  word problems. arXiv preprint arXiv:2110.14168.        agent collaborative framework. In ICLR.\n\nWendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun,   Shengran Hu, Cong Lu, and Jeff Clune. 2025. Auto-\n  Damien Lopez, Kamalika Das, Bradley A Malin, and      mated design of agentic systems. In ICLR.\n   Sricharan Kumar. 2025. Automatic prompt optimiza-\n   tion via heuristic search: A survey. arXiv.             Haitao Jiang, Lin Ge, Yuhe Gao, Jianian Wang, and Rui\n                                                      Song. 2024. Llm4causal: Democratized causal tools\n                                                                for everyone via large language model. In CoLM.Nicola Dainese, Matteo Merler, Minttu Alakuijala, and\n  Pekka Marttinen. 2024. Generating code world mod-\n                                                     Carlos E Jimenez, John Yang, Alexander Wettig,   els with large language models guided by monte carlo\n                                                Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R   tree search. In NeurIPS.\n                                                     Narasimhan. 2024. Swe-bench: Can language mod-\n                                                                   els resolve real-world github issues? In ICLR.\nYihong Dong, Jiazheng Ding, Xue Jiang, Ge Li, Zhuo\n   Li, and Zhi Jin. 2025. Codescore: Evaluating code                                         Omar Khattab, Arnav Singhvi, Paridhi Maheshwari,\n   generation by learning code execution. ACM Trans-                                                   Zhiyuan Zhang, Keshav Santhanam, Saiful Haq,\n   actions on Software Engineering and Methodology.                                                     Ashutosh Sharma, Thomas T Joshi, Hanna Moazam,\n                                                      Heather Miller,  et  al. 2024.  Dspy:  Compiling\nXidong Feng, Bo Liu, Yan Song, Haotian Fu, Ziyu                                                               declarative language model calls into state-of-the-art\n  Wan, Girish A Koushik, Zhiyuan Hu, Mengyue                                                                pipelines. In ICLR.\n  Yang, Ying Wen, and Jun Wang. 2024.  Natural\n  language reinforcement learning.  arXiv preprint    Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\n  arXiv:2411.14251.                                       taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\n                                                    guage models are zero-shot reasoners. NeurIPS.\nChrisantha  Fernando,  Dylan  Banarse,  Henryk\n  Michalewski, Simon Osindero, and Tim Rock-   Chao Lei, Yanchuan Chang, Nir Lipovetzky, and\n   täschel. 2024.   Promptbreeder:   self-referential       Krista A Ehinger. 2025. Planning-driven program-\n  self-improvement via prompt evolution. In ICML.        ming: A large language model programming work-\n                                                                flow. ACL.\nLin Ge, Hengrui Cai, Runzhe Wan, Yang Xu, and Rui\n  Song. 2025. A review of causal decision making.   Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang,\n  arXiv preprint arXiv:2502.16156.                  Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony\n                                                          Lee, Li Erran Li, Ruohan Zhang, et al. 2024. Embod-\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao       ied agent interface: benchmarking llms for embodied\n  Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu       decision making. In NeuralIPS.\n  Yang. 2024a.  Connecting large language models\n                                                 Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, and  with evolutionary algorithms yields powerful prompt\n                                            Yu Cheng. 2025. Test-time preference optimization:   optimizers. In ICLR.\n                                                          On-the-fly alignment via iterative textual feedback.\n                                                           In ICML.Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\n  Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\n                                                         Yujia Li, David Choi, Junyoung Chung, Nate Kushman,\n   jiu Yang. 2025. Evoprompt: Connecting llms with\n                                                             Julian Schrittwieser, Rémi Leblond, Tom Eccles,\n   evolutionary algorithms yields powerful prompt opti-\n                                                  James Keeling, Felix Gimeno, Agustin Dal Lago,\n   mizers. arXiv.\n                                                                    et al. 2022. Competition-level code generation with\n                                                         alphacode. Science.\nSiyuan Guo, Cheng Deng, Ying Wen, Hechang Chen,\n  Yi Chang, and Jun Wang. 2024b. Ds-agent: auto-                                                         Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\n  mated data science by empowering large language                                                 ming Zhang. 2023. Is your code generated by chatgpt\n  models with case-based reasoning. In ICML.                                                                  really correct? rigorous evaluation of large language\n                                                   models for code generation. NeuralIPS.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\n  Arora, Steven Basart, Eric Tang, Dawn Song, and   Xiangyan  Liu, Bo Lan, Zhiyuan Hu, Yang  Liu,\n  Jacob Steinhardt. 2021.  Measuring mathematical      Zhicheng Zhang, Fei Wang, Michael Shieh, and Wen-\n  problem solving with the math dataset. In NeuralIPS.     meng Zhou. 2025a. Codexgraph: Bridging large lan-\n                                                   guage models and code repositories via code graph\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu       databases. NAACL.\n  Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang,\n   Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al.   Yexiang Liu, Jie Cao, Zekun Li, Ran He, and Tieniu Tan.\n  2024a.  Metagpt: Meta programming for a multi-      2025b. Breaking mental set to improve reasoning\n   agent collaborative framework. In ICLR.                through diverse multi-agent debate. In ICLR.\n\n\n                                         10\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler   Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Hao-\n   Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,       tian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and\n  Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,      Zhiting Hu. 2024c. Promptagent: Strategic planning\n   et al. 2023.  Self-refine: Iterative refinement with      with language models enables expert-level prompt\n   self-feedback. NeurIPS.                                  optimization. In ICLR.\n\nKrista Opsahl-Ong, Michael Ryan, Josh Purtell, David    Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n  Broman, Christopher Potts, Matei Zaharia, and Omar     Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\n   Khattab. 2024. Optimizing instructions and demon-       et al. 2022. Chain-of-thought prompting elicits rea-\n   strations for multi-stage language model programs.      soning in large language models. NeuralIPS.\n   In EMNLP.\n                                              Zhaoxuan Wu,  Xiaoqiang  Lin,  Zhongxiang  Dai,\n                                               Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet,Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\n                                                    and Bryan Kian Hsiang Low. 2024. Prompt optimiza-   Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n                                                              tion with ease? efficient ordering-aware automated  Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n                                                               selection of exemplars. In NeurIPS.  2022. Training language models to follow instruc-\n   tions with human feedback. NeuralIPS.                                                        Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng,\n                                                          Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin Wu,\nSiru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhi-                                                  and Yuyu Luo. 2025. Self-supervised prompt opti-\n  han Zhang, Mengzhao Jia, Jiawei Han, Hongming                                                           mization. arXiv.\n  Zhang, and Dong Yu. 2025. Repograph: Enhancing\n   ai software engineering with repository-level code   Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\n   graph. ICLR.                                           Shafran, Karthik Narasimhan, and Yuan Cao. 2023.\n                                                           React: Synergizing reasoning and acting in language\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan      models. In ICLR.\n  Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\n   Su, Xin Cong, et al. 2024. Chatdev: Communicative    Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng\n   agents for software development. In ACL.             Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong\n                                                   Chen, and Dawei Cheng. 2025a.  G-designer: Ar-\nChen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun       chitecting multi-agent communication topologies via\n  Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize      graph neural networks. ICML.\n  Chen, Cheng Yang, et al. 2025. Scaling large lan-\n  guage model-based multi-agent collaboration.  In   Zheyuan Zhang, Tianyi Ma, Zehong Wang, Yiyang Li,\n  ICLR.                                                Shifu Hou, Weixiang Sun, Kaiwen Shi, Yijun Ma,\n                                              Wei Song, Ahmed Abbasi, et al. 2025b. Llms4all:\nNoah Shinn, Federico Cassano, Ashwin Gopinath,    A review on large language models for research and\n  Karthik Narasimhan, and Shunyu Yao. 2023. Re-       applications in academic disciplines. arXiv preprint\n   flexion: Language agents with verbal reinforcement      arXiv:2509.19580.\n   learning. NeurIPS.\n                                               Zheyuan Zhang, Kaiwen Shi, Zhengqing Yuan, Ze-\n                                                  hong Wang, Tianyi Ma, Keerthiram Murugesan, Vin-Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Ta-\n                                                           cent Galassi, Chuxu Zhang, and Yanfang Ye. 2025c.   laei Khoei. 2025. Agentic retrieval-augmented gen-\n                                                         Agentrouter: A knowledge-graph-guided llm router   eration: A survey on agentic rag. arXiv.\n                                                              for collaborative multi-agent question answering.\nFeifan Song, Yuxuan Fan, Xin Zhang, Peiyi Wang, and       arXiv.\n  Houfeng Wang. 2025. Instantly learning preference\n                                            Kunhao Zheng, Juliette Decugis, Jonas Gehring, Taco\n  alignment via in-context dpo. In NAACL.\n                                                   Cohen, Gabriel Synnaeve, et al. 2025. What makes\n                                                             large language models reason in (multi-turn) codeAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris,\n                                                          generation? In The Thirteenth International Confer-  Alessandro Sordoni, Philip Bachman, and Kaheer\n                                                      ence on Learning Representations.  Suleman. 2016. Newsqa: A machine comprehension\n   dataset. arXiv.                                                    Li Zhong, Zilong Wang, and Jingbo Shang. 2024. De-\n                                                  bug like a human: A large language model debugger\nJunlin Wang, WANG Jue, Ben Athiwaratkun, Ce Zhang,                                                              via verifying runtime execution step by step. In Find-\n  and James Zou. 2025. Mixture-of-agents enhances                                                           ings of ACL.\n   large language model capabilities. In ICLR.\n                                          Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu,\nRuochen Wang, Sohyun An, Minhao Cheng, Tianyi        Jilin Chen, Katherine A Heller, and Subhrajit Roy.\n  Zhou, Sung Ju Hwang, and Cho-Jui Hsieh. 2024a.      2024. Batch calibration: Rethinking calibration for\n  One prompt is not enough: automated construction       in-context learning and prompt engineering. In ICLR.\n   of a mixture-of-expert prompts. In ICML.\n                                          Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi,\nXingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang,      Shariq Iqbal, Ivan Vuli´c, Anna Korhonen, and Ser-\n  Yunzhu Li, Hao Peng, and Heng Ji. 2024b.  Exe-      can Ö Arık. 2025. Multi-agent design: Optimizing\n   cutable code actions elicit better llm agents. In ICML.      agents with better prompts and topologies. arXiv.\n\n\n                                         11\n\nMingchen  Zhuge,  Wenyi  Wang,  Louis  Kirsch,  A  Related Work\n  Francesco Faccio, Dmitrii Khizbullin, and Jürgen\n  Schmidhuber. 2024. Gptswarm: language agents as   A.1  Prompt Optimization for MAS\n   optimizable graphs. In ICML.\n                                               Recent progress in large language models (LLMs)\n                                                 has enabled multi-agent systems (MAS), in which\n                                                 cooperating agents consistently outperform sin-\n                                                    gle agents on demanding reasoning and software-\n                                                 engineering tasks (Zhang et al., 2025c,b; Zhong\n                                                             et al., 2024; Wang et al., 2024b). This performance,\n                                                however, depends on labor-intensive prompt engi-\n                                                    neering: each agent needs carefully crafted role\n                                                        instructions, and this effort grows rapidly as the\n                                           number of agents increases, because aligning coor-\n                                                     dination between them becomes increasingly com-\n                                                        plex. To ease this burden, many studies now frame\n                                             prompt design as an optimization problem and use\n                                                       heuristic search algorithms to explore and refine\n                                              prompts with minimal human effort and oversight.\n                                             Prompt optimization in LLMs generally falls\n                                                      into two main categories: soft-prompt tuning in\n                                                  continuous space and discrete prompt optimization\n                                                       in text space. Soft tuning supports gradient-based\n                                                  updates but sacrifices transparency and portability,\n                                                   as its learned vectors are opaque, model-specific,\n                                            and require gradient access that most black-box\n                                          APIs do not provide (Cui et al., 2025). To work\n                                                around this, researchers approximate gradients with\n                               LLM feedback and develop gradient-like strategies\n                                                       suited for non-differentiable settings. For example,\n                                        some works apply beam search for step-wise re-\n                                                 finement (Chen et al., 2024b; Wang et al., 2024c),\n                                                 while others explore alternative optimization strate-\n                                                         gies, such as evolutionary algorithms (Guo et al.,\n                                                2025; Fernando et al., 2024) and other heuristic al-\n                                                 gorithms (Opsahl-Ong et al., 2024; Li et al., 2025),\n                                                         to adapt prompts iteratively. Another related line of\n                                           work focuses on prompt selection, searching a pool\n                                                   of variants to pick the best one (Wu et al., 2024;\n                                     Wang et al., 2024a; Song et al., 2025).\n                                              However, most existing methods target a sin-\n                                                    gle agent, while prompt optimization for MAS\n                                                   as a whole remains under-explored. Among the\n                                           few efforts in this area, Mass (Zhou et al., 2025)\n                                         warms each agent’s role prompt, prunes the interac-\n                                                       tion graph, and then jointly fine-tunes all prompts,\n                                              showing that layered optimization boosts group per-\n                                               formance. GPTSwarm (Zhuge et al., 2024) treats\n                                                  agents as a graph and updates node-level prompts\n                                             and edge connections together, letting prompts co-\n                                                 evolve with coordination patterns. We argue that\n                                                    the importance of proper prompt design has been\n                                                        significantly under-studied in these prior works.\n\n\n                                         12\n\nSpecifically, current approaches still overlook two   prompt optimization (Zhang et al., 2025a; Zhou\nkey issues: even minor lexical edits upstream can    et al., 2025) to unlock more reliable and general-\nshift the distributions seen by downstream agents,    izable agent-collaboration schemes for real-world\nand, to the best of our knowledge, none of the meth-   coding tasks.\nods searches for a globally optimal set of prompts\nfor the full system.                   B  Implementation Details\n\nA.2 LLM Agents for Code Generation Tasks    B.1  Benchmarks\n\nCode generation has emerged as a core appli-  HumanEval-ET (Dong et al., 2025) is an extended\ncation for large language model (LLM) agents   benchmark for evaluating code generation. It builds\nbecause it links natural-language reasoning with   upon the original HumanEval dataset by introduc-\nconcrete, testable outputs and promises to auto-   ing more challenging variations and refined eval-\nmate sizable portions of software engineering and    uation protocols, particularly emphasizing error\ndata-science workflows.  Early studies tackled    tolerance and execution-based correctness. The\nthis task with single-agent or minimally interac-   dataset is specifically designed to better capture\ntive pipelines—such as Self-Refine (Madaan et al.,   the robustness of large language models (LLMs)\n2023), Reflexion (Shinn et al., 2023), and CoT-   under real-world coding scenarios, where multi-\nZero (Kojima et al., 2022)—that plan, execute, and    ple correct implementations may exist and minor\niteratively repair their own code until unit tests    deviations from reference solutions should not nec-\npass. These works showed that even simple agent    essarily be penalized. By incorporating these re-\ninteractions can improve reliability when the agent    finements, HumanEval-ET provides a more reliable\ncan inspect failures and revise its output, a process   and nuanced measure of code generation quality.\nthat loosely aligns with causal reasoning (Ge et al.,   Since this dataset doesn’t provide a train-test split,\n2025; Jiang et al., 2024): the model infers poten-  we used the first 100 records for optimization and\ntial sources of error and adjusts its generation in    the rest 64 for zero-shot left-out testing.\nresponse.                                 MBPP-Plus (Liu  et  al., 2023) extends the\n  As multi-agent systems advance, the trend has   \"Mostly Basic Python Problems\" (MBPP) dataset\nshifted toward complex MAS with carefully de-    into a larger and more diverse collection. While\nsigned role-play interactions.  Frameworks like  MBPP was originally created to evaluate basic pro-\nCodeAct (Wang et al., 2024b), MetaGPT (Hong   gramming competency using short Python func-\net al., 2024b), and ChatDev (Qian et al., 2024)    tions, MBPP-Plus expands both the scale and vari-\nassign specialized roles—planner, coder, tester,   ety of tasks to cover more intricate programming\nreviewer—and let agents converse in plain lan-   constructs, edge cases, and multi-step logic. This\nguage, mimicking real software teams. These or-   augmentation addresses the limitations of the orig-\nchestrated exchanges boost division of labor and    inal dataset by providing a broader set of prob-\nhelp solve coding problems that demand sophisti-   lems that better reflect practical coding challenges,\ncated reasoning, though they impose substantial    thereby serving as a more comprehensive bench-\noverhead in prompt design.  Building on these   mark for evaluating code generation models.\nframeworks, recent work explores several direc-     CodeContest (Li et al., 2022) is a benchmark de-\ntions. Some studies enhance individual modules    rived from real competitive programming problems,\nthrough richer planning (Lei et al., 2025; Chen    representing a significant increase in difficulty com-\net al., 2024a), stronger verification (Dainese et al.,   pared to synthetic or basic coding datasets. It con-\n2024; Zheng et al., 2025), or improved knowledge    tains tasks sampled from programming competi-\nbases (Ouyang et al., 2025; Liu et al., 2025a). Oth-    tions, where problems are designed to require al-\ners introduce supervised signals into the framework,   gorithmic reasoning, data structure manipulation,\nsuch as reinforcement learning (Feng et al., 2024),   and efficiency considerations. The inclusion of\nto guide agent behavior.                                     strict input–output constraints and hidden test cases\n  However, across all phases, prompt design re-   makes CodeContest a rigorous benchmark that\nmains a bottleneck: each role prompt must be    challenges LLMs to go beyond template-based so-\ncarefully crafted, and even minor edits can rip-    lutions and demonstrate genuine problem-solving\nple through the workflow. Consequently, a grow-    ability. Given the large volume of this dataset’s\ning body of research now investigates automatic    training set, we sample the same records as the test\n\n\n                                         13\n\nset from training for optimization.                Each problem is designed to require multi-step rea-\n  NewsQA (Trischler et al., 2016) is a large-scale    soning with arithmetic operations, testing a model’s\nquestion answering dataset constructed from CNN    ability to parse natural language descriptions, trans-\nnews articles. It consists of over 100,000 human-    late them into formal reasoning steps, and compute\ngenerated questions paired with answers derived    the correct answer. The dataset emphasizes chain-\nfrom corresponding news passages. Unlike earlier    of-thought reasoning and has become a standard\nQA datasets that focus on simple fact extraction,   testbed for evaluating LLMs’ ability to perform\nNewsQA emphasizes reasoning, inference, and    reliable symbolic reasoning in relatively simple but\nsynthesis across multiple sentences within an arti-   compositional tasks. Its structured design and mod-\ncle. Its design introduces ambiguity, unanswerable    erate difficulty level make GSM8K complementary\nquestions, and multi-sentence reasoning, making    to more advanced datasets like MATH. Given the\nit a challenging benchmark for evaluating reading    large volume of this dataset’s training set, we sam-\ncomprehension and open-domain question answer-   ple the same records as the test set from training\ning systems. Given the large volume of this dataset,    for optimization.\nwe sample the first 500 records to use as optimiza-\n                                              B.2  Baselinestion and left-out testing.\n  WebQuestions (Berant et al., 2013) is a bench-   Chain-of-Thought (CoT) (Wei et al., 2022) is\nmark dataset for semantic parsing and knowledge-   a prompting paradigm that encourages large lan-\nbase question answering. It contains around 6,000   guage models (LLMs) to generate intermediate\nnatural language questions paired with answers    reasoning steps before arriving at final answers.\nsourced from Freebase, covering a diverse range of   Unlike direct-answer prompting, CoT exposes the\ntopics. The dataset is notable for requiring models   model’s latent reasoning process, which has been\nto bridge the gap between natural language queries   shown to substantially improve performance on\nand structured knowledge graph representations,    tasks requiring multi-step deduction such as arith-\nthereby testing a system’s ability to perform en-   metic, commonsense inference, and symbolic rea-\ntity linking, relation extraction, and logical reason-   soning. The introduction of CoT has established a\ning. As one of the earliest large-scale QA datasets   new standard for eliciting reasoning from LLMs,\ngrounded in knowledge bases, WebQuestions has   making it a fundamental baseline in subsequent\nbeen widely adopted as a standard benchmark for    research. Its effectiveness also highlights a broader\nsemantic parsing and open-domain QA research.    principle: structured prompting can significantly\nGiven the large volume of this dataset, we sam-   extend the reasoning capability of LLMs without\nple the first 500 records to use as optimization and    the need for additional training.\nleft-out testing.                              ReAct (Yao et al., 2023) builds upon CoT by\n MATH (Hendrycks et al., 2021) is a dataset    integrating reasoning with acting. Specifically, Re-\nspecifically designed to evaluate advanced math-   Act enables agents to interleave chain-of-thought\nematical reasoning in LLMs. It contains approxi-   reasoning with concrete actions, such as query-\nmately 12,000 competition-style problems, ranging    ing external knowledge sources, interacting with\nfrom high school mathematics to Olympiad-level    environments, or calling tools. This synergy al-\nchallenges, with step-by-step solutions provided.   lows models to dynamically refine their reason-\nUnlike arithmetic-focused datasets, MATH covers    ing based on external feedback, thereby reduc-\na broad spectrum of topics including algebra, geom-   ing hallucinations and improving factual ground-\netry, number theory, and calculus, requiring multi-    ing.  ReAct has been validated across diverse\nstep reasoning and symbolic manipulation. Its com-   tasks including knowledge-intensive QA, fact ver-\nplexity makes it one of the most rigorous bench-    ification, and embodied agent settings, where its\nmarks for assessing the capacity of LLMs to han-   reasoning-and-acting paradigm consistently outper-\ndle formal reasoning and mathematical problem-   forms reasoning-only or acting-only strategies. As\nsolving. Given the large volume of this dataset’s   a baseline, ReAct represents an important step to-\ntraining set, we sample the same records as the test   ward interactive and tool-augmented LLM systems.\nset from training for optimization.                EvoPrompt (Guo et al., 2024a) frames prompt\n GSM8K (Cobbe et al., 2021) (Grade School    optimization as an evolutionary search process,\nMath 8K) is a benchmark comprising 8.5k carefully   where a population of prompts is iteratively mu-\ncrafted grade-school-level math word problems.   tated and recombined to generate stronger candi-\n\n\n                                         14\n\ndates. The method relies on large language mod-   evaluation framework that leverages structured di-\nels themselves as operators for variation, while   alogue among diverse LLM agents to produce\nselection mechanisms ensure gradual improvement.   more reliable judgments. In our study, we adopt\nThis makes EvoPrompt effective for black-box    the Simultaneous-Talk-with-Summarizer variant,\nsingle-agent prompt optimization. However, its de-   which we call SWARM, where agents contribute in\nsign remains confined to evolving isolated prompts,    parallel and a summarizer condenses their discus-\nand it does not extend naturally to multi-agent set-   sion into a concise shared history. In this setting,\ntings where inter-agent coordination and topology   each agent interacts with each other in a dense\nplay central roles.                                  format, making this baseline a typical and repre-\n  PromptBreeder (Fernando et al., 2024) extends    sentative topology type, as it emphasizes on the\nevolutionary prompt optimization by introducing    richness of multi-agent discussion with strong rea-\nself-referential mutation. In this framework, not    soning and expressiveness.\nonly task-prompts but also the mutation-prompts    GPTSwarm (Zhuge et al., 2024) frames lan-\nthat generate them are evolved, enabling the system   guage agents as computational graphs, where each\nto adapt its own optimization strategy over time.   node corresponds to an operation such as an LLM\nThis self-referential design yields a flexible and    query, and edges capture the flow of information\nautomated process for refining prompts in single-   across agents. This framework is intended for op-\nagent contexts. Nevertheless, PromptBreeder is in-   timizing both topology and prompts. To enable\nherently tailored to optimizing individual prompts   a fair comparison, we focus solely on the prompt\nand does not address the complexities of scaling to    optimization parts of this work. Within this frame-\nmulti-agent systems.                            work, Direct Optimization is employed to refine\n  Chain (Shinn et al., 2023) represents the sim-   the prompts associated with each node individu-\nplest combination topology of MAS. In our study,    ally, using input–output histories and iterative up-\nit combines the reasoning-and-acting paradigm of    dates to improve local performance.  This strat-\nReAct with a self-reflection module. After complet-   egy allows each operation to self-improve in iso-\ning a task, the agent revisits its reasoning trajectory,    lation, but it treats prompts largely as independent\nidentifies mistakes, and integrates corrective feed-    units and does not account for the interdependen-\nback into subsequent attempts. By incorporating    cies across the wider agent graph. In contrast, our\nthe simple reflection module into the loop, Chain  MAPRO framework explicitly models prompt op-\nimproves both robustness and sample efficiency. In    timization as a joint inference problem over the\nour experiments, we adopt this variant as a base-    entire MAS topology, propagating credit and de-\nline to capture the benefits of the effectiveness of   pendencies across nodes and edges. Thus, while\nsimple MAS, compared with other MAS choices.   GPTSwarm provides a strong formulation of node-\n DMAD (Liu et  al., 2025b) (Diverse Multi-    level direct optimization, our approach generalizes\nAgent Debate) is a recent state-of-the-art frame-    this idea to coordinated optimization across multi-\nwork designed to overcome the inherent limita-   agent systems, addressing the limitations of local-\ntions of multi-agent debate (MAD). Traditional    only updates\nMAD setups often fall prey to a mental set, where    TPE Optimization (Zhou et al., 2025; Opsahl-\nagents—even if assigned different personas—rely   Ong et al., 2024) applies a Tree-structured Parzen\non similar reasoning strategies, limiting their abil-   Estimator (TPE)–based Bayesian search strategy to\nity to explore alternative solutions. DMAD ex-   optimize prompts in multi-agent systems. In these\nplicitly addresses this by requiring each agent to   frameworks, prompts (instructions and demonstra-\nemploy a distinct reasoning method (e.g., Chain-    tions) are treated as discrete parameters, and TPE\nof-Thought, Step-Back Prompting, Program-of-    is used to model the joint contribution of differ-\nThought), thereby fostering genuine diversity in    ent parameter settings to downstream performance.\nproblem-solving. This represents an intermediate-   This surrogate-based approach efficiently explores\ncomplexity MAS topology that balances complex-   the search space by prioritizing promising configu-\nity and expressiveness. Given its robustness and    rations from past evaluations. Unlike Direct Op-\nstate-of-the-art results, we consider DMAD an es-   timization, which treats node prompts indepen-\nsential MAS base structure for evaluating MAS-    dently, TPE can partially capture dependencies\nlevel optimization.                             between variables through its probabilistic mod-\n  ChatEval (Chan et al., 2024) is a multi-agent    eling. However, TPE optimization still operates\n\n\n                                         15\n\nover a fixed pool of candidate proposals, limit-     Since we do not assume one prompt set is in-\ning its ability to propagate credit across agents    herently better than another (we have no prior\nor adapt proposals dynamically. In contrast, our   knowledge). The most neutral choice is to use\nMAPRO framework formulates prompt optimiza-   a uniform prior, and under a uniform prior, every\ntion as a joint inference problem across the entire  P ∈P1 × · · · × PN is assigned the same positive\nMAS topology, explicitly propagating dependen-    probability. Thus π(P) = c for some constant\ncies and credit signals across nodes and edges. This    c > 0 independent of P. Since multiplying by a\nenables coordinated optimization beyond the local    constant does not affect an arg max, we have\nor surrogate-based updates employed by TPE meth-\n                                                     arg max f(P) c = arg max f(P).\nods, addressing their limitations in capturing the              P               P\nfull structure of multi-agent interactions.\n                                                   Therefore, given T (P) = Pr(S | P),\nB.3  Training Protocol                                                arg max Pr(P  | S) = arg max Pr(S | P) π(P)\n                                                 P                 P\nWe limit the number of preference demonstrations\n                                       = arg max Pr(S | P) c\nto 3 and candidates to 5. We limit the agent num-                           P\nber smaller than 10. We set model temperature at                                       = arg max Pr(S | P)\n0.2, maximum output tokens at 2048. We imple-                           P\nment the same LLM backbone as both evaluator             = arg max T (P).\n                                                                    P\nand executors in all phases. The optimized MAS                                                (14)\nis reported on the held-out test set over three runs,   Thus, maximizing the Joint Quality Score is exactly\nwhile other baselines over five runs. Given our mis-   a MAP estimate of P.\nsion to optimize the prompts, we didn’t spend too\n                                            C.2  Proof of Junction Tree MAPmuch effort on prompt engineering, which mimics\nthe real-life scenarios where a general prompt is   Max-product belief propagation (MPBP) is guar-\nadopted to a specific downstream tasks. The spe-   anteed to compute the exact MAP assignment\ncific prompt designs can be seen in Appendix-D.     only on tree-structured factor graphs. For a DAG\n                           G = (V, E), the factorization (T (P)) generally in-\nC  Proof                                               duces cycles, since a node j with multiple parents\nC.1  Proof of MAP Equivalence                 couples the variables {pi  : (i, j) ∈E} together.\n                                                  Formally, one first moralizes and triangulates the\nBy Bayes’ rule, the classic MAP estimate chooses\n                                DAG to ensure a chordal structure admitting a junc-\nthe hypothesis P that maximizes the posterior:\n                                                        tion tree.\n    bPMAP ∈arg maxP  Pr(P  | S)                 The junction-tree construction converts  this\n                                            (12)  DAG  factorization  into  an  equivalent  tree-\n      = arg max Pr(S | P) π(P),\n                P                                structured form. The procedure groups variables\n                                                       into clusters C ⊆V, each associated with a poten-\nwhere S is the observed event and π(P) is the\n                                                                    tial ψC defined as\nprior on P. This way MAP finds the most prob-\nable explanation (the most likely hidden variable      ψC(PC) := Y g(pi) Y g(pi, pj),     (15)\nassignment) given what one observed.                              i∈C      (i,j)∈E\n   In our case, P = (p1, . . . , pN) ∈P1 ×· · ·×PN                              {i,j}⊆C\nis a joint prompt assignment and S denote the event                                           where PC = {pi  : i ∈C}. In words, every fac-\nthat the system succeeds on the batch. By construc-                                                          tor is assigned to exactly one cluster that contains\ntion of the node/edge success scores g(·), g(·, ·),                                                                   its variables. Clusters are arranged in a tree TJT\n         N                                      satisfying the running intersection property: if a\nPr(S | P) = Y Pr(Xi=1 | P) Y Pr(Yij=1 | P)  variable pi appears in two clusters C1, C2, then it\n             i=1                 (i,j)∈E                appears in every cluster on the unique path between\n         N                            C1 and C2 in TJT.\n      = Y g(pi) Y g(pi, pj)                The resulting representation is an exact refactor-\n             i=1      (i,j)∈E                            ization:\n        =: T (P).                            T (P) = Y ψC(PC) / Y ψs(Ps),     (16)\n                                            (13)              C∈C          s∈S\n\n                                         16\n\nwhere S denotes the separator sets (intersections    realizes the global maximum (ties broken arbitrar-\nof adjacent clusters). Here each separator potential     ily).\nψs is defined as the product of factors assigned to s,     Remark (junction tree). In the clique/sepset form,\nensuring no double counting. The division by sepa-   replace nodes i by cliques C, parent j by neighbor\nrators ensures that no factor is double-counted and   D, pi by PC, and g by ψ; messages are\nthat the product reproduces exactly T (P). Since\n                                   mC→D(PSCD)= max ψC(PC) Y mB→C(PSBC) ,this is equivalent to the original joint score T (P)                                                            PC\\SCD                                                                         B∈nb(C)\\{D}\nbut expressed on a tree-structured factor graph, ap-\nplying MPBP to {ψC} on the junction tree yields    and the same induction establishes exact MAP for\n                                                     the original DAG via the established equivalence.\n             Q C∈C ψC(PC)  arg max T (P) = arg max                        .                                            C.4  Time Complexity Analysis       P              P  Q s∈S ψS(Ps)\n                                            (17)  MPBP on a tree. Let N = |V| be the number of\nwhich recovers the exact MAP assignment of P.   agents, E = |E| the number of edges, and K =\nHence, the junction-tree transformation converts a   maxi |Pi| the maximum size of any agent’s prompt\ngeneral DAG into a tree-structured model where    pool. On a tree-structured factor graph, each edge\nMPBP can be applied directly and exactly. This    passes one message in each direction. Updating a\nratio form follows from the junction-tree theorem,    single message requires comparing all prompt pairs\nwhich guarantees that clique potentials multiplied    (pi, pj), which costs O(K2). Since there are O(E)\nand corrected by separator terms reproduce the   such messages in total, the overall complexity is\nexact joint distribution.\n                                                    O(EK2).\nC.3  Proof of MAP Global Optimality                                               Junction-Tree MAP (general DAG). For a DAG,\nLemma (optimal-subtree property). Assume the   we convert the graph into a junction tree whose\nreward factorization is finite and has no negative    clique set is denoted C.  Let w be the induced\nfactors, for any edge i →j and any pj, mi→j(pj)    treewidth, i.e. the size of the largest clique mi-\nequals the maximum of the product of factors con-   nus one. Each message update involves marginal-\ntained in the subtree rooted at i, conditioned on   izing/maximizing over a clique  table of  size\npj.                                     O(Kw+1), and there are O(|C|) such cliques (at\n   Proof. Assume i is a leaf agent node, (6) reduces   most linear in N). Thus the complexity is\nto maxpi g(pi)g(pi, pj), the best score of the leaf                                                              O(|C| Kw+1),\nedge given pj. Assume the claim holds for all chil-\ndren k ∈Child(i). Then the product inside (6)   with storage requirements of the same order.\nequals, for each fixed pi, the optimal contributions     The treewidth w reflects how many agents must\nof all child sub-trees consistent with pi; maximiz-   be grouped into a single clique to remove cycles.\ning over pi yields the optimal value of the entire   For instance, if two agents both depend on the same\nsubtree at i given pj.                                  parent, they may be merged into a summary clique\n                                                     of size three, so w = 2. If a node has three parents,\n  Theorem (global MAP optimality).  Let p∗r ∈                                                 then a clique containing all four variables may be\narg maxpr βr(pr). Then there exists an assignment\n                                                needed, giving w = 3. In general, sparse MAS\nP ⋆obtained by the standard downward backtrack-\n                                                graphs usually have small w (often 2 or 3), so the\ning that satisfies P ∗∈arg maxP T (P),                                                  exponential factor Kw+1 remains modest. This\n   since the root collects optimal contributions from\n                                           means that in practical multi-agent settings, where\nall disjoint subtrees. Thus\n                                              each agent only interacts with a few neighbors,\n                                                      junction-tree MAP is efficient and scales nearly\n        max βr(pr) = max T (P).\n               pr           P                          linearly with N once w is bounded.\n                                                     In summary, we control both selection and up-\nDuring the upward pass, for every edge i →j                                                   date phase in polynomial time complexity and it\nand parent value pj, the maximizer(s) achieving                                                     scales well the increase of the density of interac-\n(6) define a witness choice p⋆i (pj). Starting from                                                       tions as well as the size of candidate pools, which\np∗r ∈arg max βr(pr) and recursing p⋆i  p∗j  along   emphasize the scalability and efficiency of our\nedges away from r yields a full assignment P ∗that  MAPRO framework.\n\n\n                                         17\n\nD  Prompt Designs\n\n\n\n\n\n                                         18\n\nNode-Level Reward Model (Header + Prefix)\n\n   node_header:\n   You are a *reward model* for evaluating the competence, clarity of candidate **role prompts**.\n   Based on the input, output and prefernece examples,\n   you should first rank the candidate prompts with the good and bad examples,\n   Then you will give each a distinct two-decimal quality score between (0.00, 1.00) based on the\n   standard and alignment with the good examples.\n   You should be severely harsh and the score difference should be ranged from 0.4 - 0.8 and each\n    differs more than 0.05 with each other.\n    Finally, return exactly a score each line corresponding to the **prompt’s original position**. (Not\n    the sorted score)\n   Note that your output should contain only the numeric scores (e.g., 0.62). Nothing else.\n\n\n   agent_reward_prefix:\n  You are an evaluation LLM. Given {input} and the agent’s response {output}, rate how well\n   the response accomplishes the agent’s role on a scale 0–1 (higher is better).Use the preference\n   demonstrations below as reference.Return ONLY the floating-point score.\n\n   === Preference Demonstrations ===\n  {demo}\n   === End Demonstrations ===\n\n\n   Edge-Level Reward Model (Header + Prefix)\n\n   edge_header:\n   You are a *reward model* for assessing **communication quality** from\n   an upstream agent to a downstream agent. Consider information completeness, format,\n    clarity, and alignment with demonstrations.\n   Based on the input, output and prefernece examples,\n   you should first rank the candidate prompts with the good and bad examples,\n   Then you will give each a distinct two-decimal quality score between (0.00, 1.00) based on the\n   standard and alignment with the good examples.\n   You should be severely harsh and the score difference should be ranged from 0.4 - 0.8 and each\n    differs more than 0.05 with each other.\n    Finally, return exactly a score each line corresponding to the **prompt’s original position**. (Not\n    the sorted score)\n   Note that your output should contain only the numeric scores (e.g., 0.62). Nothing else.\n\n\n   edge_reward_prefix:\n  You are an evaluation LLM. Judge whether a message produced by agent {i} helps agent {j}\n   perform its next step. Rate on a 0–1 scale. Use the demonstrations for guidance. Return ONLY the\n    floating-point score.\n\n   === Preference Demonstrations ===\n  {demo}\n   === End Demonstrations ===\n\nFigure 4: Unified Reward Modeling Prompts for MAPRO: node-level (left) and edge-level (right), merging each\nmodule’s header and reward prefix verbatim.\n\n\n\n\n                                         19\n\nFeedback and Mutation Strategy Prompts\n\n   global_feedback_sys:\n   You are an experienced prompt engineer and failure-analysis specialist.\n   Given multiple examples of runtime *error messages* produced by the given LLM-generated\n   code,\n    identify the three most recurring but easy to solve root-cause patterns or missing constraints **in\n   the prompts** that lead to the errors. Produce a short **specific and actionable** list of fix\n   suggestions an author can apply.\n   Note 1: Output each fix as a bullet starting with numbers. Do NOT quote full stack traces; mention\n   key function names only if essential.\n   Note 2: You should focus on the pragmatism and cleaniness of code rather than if it’s easy to read,\n    for example, if the a module doesn’t have package ‘List‘, instead of asking to properly import the\n   package, you should emphasize it should write code without any type hints or annotations.\n\n\n   local_feedback_sys:\n   You are a experienced prompt engineer and failure-analysis specialist. You are given:\n   1) The global overall feedback list that the system is currently facing.\n    2) Blame statements from downstream agents suggesting how the current module can be improved\n   (may be empty).\n   3) The prompt this module is currently using.\n   Based on the roles of the current module, your task is to generate a *local feedback* list, focusing\n   on give specific, actionable fix suggestions specifically for this current module to take to avoid\n   downstream errors and satisfy the overall fix suggestions. Each line starts with ‘•’.\n\n\n   mutation_strategy_sys:\n   You are a experienced prompt engineer and failure-analysis specialist.\n   You are given the original <prompt> of a module plus two feedback blocks:\n   One overall fix feedback suggesting the errors the system currently experience and one optional\n    local feedback suggesting what this current modules can focus on to improve to benefit the system.\n   Your task is to modify, improve, and explode the original prompt by outputing exactly {n} JSON\n    strings as prompt variations with specific and detailed improvement.\n   Note:\n   1) You should focus on the pragmatism and cleaniness of the prompts (You shouldn’t acutally\n   write any code), so **always emphasize** the code should be executable, wrapped in one function,\n   without any type hints or annotations, and named as solution if no other names are provided.\n   2) You are only allowed to make relatively small edits. You must choose exactly one action item\n    in the following: a) adding one sentence from the feedback. b) replacing one senetence from the\n   feedback to existing edits. c) Re-organize, rewrite or clean the current prompt to make it logically\n    consistent. d) delete one redundant sentence in the current prompt.\n   3) You should ALWAYS respond with ONLY the VALID JSON array – You should return No\n   headings, no prose such as </prompt>, no markdown fences such as “‘, no trailing commas, no\n   escape codes, or unclosed parenthesis. Each string must be valid UTF-8. Escape all newlines as \\n.\n  No raw newlines inside JSON strings. Example (node, n = 2): [\"Prompt variant 1\",\"Prompt variant\n    2\"].\n\n\nFigure 5: Feedback system prompts in MAPRO (for coding tasks): global feedback, local feedback, and mutation\nstrategy.\n\n\n\n\n                                         20\n\nVariation Prompt\n\n   variation:\n   You are a prompt-engineering assistant.\n   The user will give you an original prompt TEMPLATE inside <prompt></prompt>.\n   Produce {n} diverse textual prompt variants (NOT solution, but the prompts) that keep the same\n    intent but differ in wording, ordering, or tone. Note that you should generate the prompt for the\n   agent not generate solution.\n   Don’t write code here and Return **only** a JSON array of strings.\n   Respond on a single line only. Do not emit any raw line breaks.\n\n\n   Negative Variation Prompt\n\n   neg_variation:\n   You are a prompt-mutation helper.\n   The user will give you a JSON object with:\n   good_examples : list[str] # 3 GOOD prompt templates (node) *or* 3 GOOD upstream-downstream\n    pairs\n   mode : \"node\"|\"edge\" # mutation type\n   n : int # number of BAD variants requested\n   Produce exactly {n} sligthly BAD variants:\n    • For \"node\": each string could omit some key instructions, introduce contradictions, or add\n    irrelevant text that reduces agent quality.\n    • For \"edge\": each string code be a JSON array [\"bad_upstream\", \"good_downstream\"] where\n   bad_upstream makes the pair incompatible.\n    • Note that your generation should be obviously worse than good examples, but not too absurd or\n    entirely off the topic.\n   Remember, Return nothing except one valid JSON array.\n    - For mode = \"node\" →[\"str\", \"str\", . . .]\n    - For mode = \"edge\" →[[\"str\",\"str\"], [\"str\",\"str\"], ...]\n  You should ALWAYS respond with ONLY the VALID JSON array – You should return No\n   headings, no prose such as </prompt>, no markdown fences such as “‘, no trailing commas, no\n   escape codes, or unclosed parenthesis.\n   Each string must be valid UTF-8. Escape all newlines as \\n. No raw newlines inside JSON strings.\n\n\nFigure 6: Initialization prompts in MAPRO: variation (left) for diverse positive variants and neg_variation (right)\nfor intentionally degraded variants.\n\n\n\n\n\n                                         21\n\nCoding Prompts and Notes\n\n   raw:\n  You are a reasoning agent and coding expert. Solve the task by outputting only executable\n   Python code as a single function. Do not print any prose, comments, or markdown fences. If\n   preprocessing or postprocessing is needed to conform to the specified input/output format, perform\n     it inside the function. Follow all constraints in note exactly.\n\n    cot:\n   You are a reasoning agent and coding expert. First reason step by step silently (do not print thoughts\n   or a plan). Then output only executable Python code as a single function that solves the task\n   with the correct input/output format. Do not include comments, explanations, tags, or markdown\n    fences. Follow note exactly.\n\n   react:\n  You are a reasoning agent. First analyze the problem and form a plan silently (do not print it).\n   Then use that plan to produce the final answer as code only — a single Python function with\n   no comments, prose, or markdown fences. If FEEDBACK is provided, revise the code only when\n   the feedback is correct and improves conformance to the task or note; otherwise keep the best\n   previous solution. Follow note exactly.\n\n    reflect:\n   You are a coding critic. Be conservative — revise only if there are concrete mistakes. Evaluate the\n   submitted answer against:\n   1) It is only executable Python code with no markdown, tags, or comments, and has no obvious\n   syntax errors.\n   2) It implements any required preprocessing/postprocessing so the input and output formats are\n    correct.\n   3) It satisfies all constraints in note.\n    If everything is correct, reply exactly: ACCEPT\n   Otherwise, reply exactly: REVISE: <concise, actionable fixes required>\n  Do not include code blocks, bullets, or extra text beyond the required format.\n\n   note:\n   1) No type annotations or return-type hints.\n   2) Output only executable Python code, with no tags (e.g., </...>), no markdown fences (“‘), and\n   no explanations or comments.\n   3) Wrap the solution in exactly one function. If the function name is specified in the problem or\n  PUBLIC TEST, use it and keep the exact parameter list. Otherwise, name the function solution.\n   4) Match the task’s input format. If examples indicate inputs arrive as strings, accept a string\n   parameter and parse internally. Likewise, format outputs exactly as required (e.g., print vs return).\n   5) Use only Python’s standard library; do not rely on network access, external files, or third-party\n   packages.\n   6) Ensure deterministic behavior and avoid unnecessary randomness or system calls.\n   7) Prefer clear, robust algorithms; handle edge cases implied by the task (empty inputs, boundary\n   values) when reasonable.\n\n\nFigure 7: Unified coding prompt suite for coding solutions. The five roles (RAW, CoT, ReAct, Reflect, Note) ensure\nsilent planning, strict code-only output, conservative review, and precise conformance to I/O constraints.\n\n\n\n\n\n                                         22\n\nMath Prompts and Notes\n\n   raw:\n   You are a competition mathematician. Solve the problem with clear step-by-step reasoning, using\n    exact symbolic forms (prefer fractions to decimals when appropriate). End with a single final line:\n   ⟨answer⟩. Follow note exactly.\n\n    cot:\n    First think step by step silently (do not print your thoughts); then present a concise, logically\n   ordered solution. Use exact forms; avoid decimals unless requested. End with the single final line:\n   ⟨answer⟩. Follow note.\n\n    react:\n   Analyze the problem and form a plan silently (do not print it); then present a full worked solution\n   with clear steps and exact forms. If FEEDBACK is provided, revise the solution only when it is\n    correct and improves adherence to note; otherwise keep the best prior solution. Finish with the\n    final line: ⟨answer⟩.\n\n    reflect:\n   You are a competition-math critic. Assess the submitted solution on:\n   1) Mathematical correctness of the result and reasoning.\n    2) Output formatting (final line exactly boxed).\n    3) Adherence to all constraints in note.\n    If everything is correct, reply exactly: ACCEPT\n   Otherwise, reply exactly: REVISE: <concise, actionable fixes required>\n  Do not include any extra text beyond the required format.\n\n   note:\n  MATH Canonicalization (Top Priority)\n   1) Final line only: ⟨answer⟩.\n    2) Exact forms: reduce a/b; simplify radicals; use π; avoid decimals unless asked.\n    3) Numbers: no commas anywhere (9901 not 9,901; 448/15625 not 2,240/78,125).\n   4) Expressions: canonical, no spaces, use ^ for exponents (x3 + 3x −6). Do not prepend variables\n    or ’=’ (prefer 5 over x = 5).\n    5) MCQ: box the single capital letter only.\n    6) Tuples/Sets: (a, b, . . . ) and {a, b, . . . } with simplified components.\n   7) Units: match the problem; use k◦for degrees; default radians.\n    8) Sanity: respect domains; drop extraneous roots; include a quick plug-back check.\n\nFigure 8: Unified math prompt suite for exact, well-formatted solutions. The five roles (RAW, CoT, ReAct, Reflect,\nNote) enforce silent planning, precise symbolic work, and a canonical boxed final answer. Different from coding\nprompts, prompts for question answering are rather similar to math prompts as they both relate to reasoning, thus\nwe skip the demonstration here.\n\n\n\n\n\n                                         23",
"headers": [
"arXiv:2510.07475v1  [cs.CL]  8 Oct 2025",
"MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a",
"Posteriori Inference"
],
"tables": [
"|Select & Update<br>Isolate Improvement Fitnes<br>a.1) Direct Optimization|Update<br>TPE Select<br>s Sub-Optimal Prompt Sets Fitness<br>a.2) TPE Optimization|\n|---|---|",
"|w/t MAPRO|✓|✓|80.21 ± 0.90|76.54 ± 0.67|31.52 ± 0.61|64.00 ± 0.35|34.65 ± 0.30|64.30 ± 0.59|93.48 ± 0.42|\n|---|---|---|---|---|---|---|---|---|---|",
"|w/t MAPRO|✓|✓|77.08 ± 1.81|74.60 ± 0.27|38.99 ± 1.95|62.93 ± 0.12|35.50 ± 0.33|63.33 ± 0.46|91.96 ± 0.58|\n|---|---|---|---|---|---|---|---|---|---|",
"|w/t MAPRO|✓|✓|78.13 ± 1.56|73.98 ± 0.15|41.41 ± 0.93|62.67 ± 0.31|34.52 ± 0.20|63.13 ± 0.84|91.73 ± 0.41|\n|---|---|---|---|---|---|---|---|---|---|",
"|w/t MAPRO|✓|✓|75.00 ± 1.56|72.31 ± 0.55|30.10 ± 0.70|63.82 ± 0.54|36.22 ± 0.30|71.87 ± 0.35|93.56 ± 0.34|\n|---|---|---|---|---|---|---|---|---|---|",
"|w/t MAPRO|✓|✓|73.96 ± 0.90|71.60 ± 0.31|35.96 ± 1.53|65.10 ± 0.24|36.22 ± 0.28|72.99 ± 0.33|95.91 ± 0.30|\n|---|---|---|---|---|---|---|---|---|---|",
"|w/t MAPRO|✓|✓|75.00 ± 1.56|72.22 ± 0.26|37.17 ± 0.93|65.45 ± 0.09|36.55 ± 0.29|72.26 ± 0.71|92.38 ± 0.40|\n|---|---|---|---|---|---|---|---|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2510.07475v1.pdf"
}