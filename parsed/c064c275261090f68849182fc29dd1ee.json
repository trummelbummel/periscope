{
"text": "promptolution: A Unified, Modular Framework for Prompt Optimization\n\n                     Tom Zehle1,2,∗, Timo Heiß3,4,∗, Moritz Schlager5,∗,\n                              Matthias Aßenmacher3,4, Matthias Feurer6,7\n            ∗Equal contribution 1ELLIS Institute, Tübingen, Germany 2 University of Freiburg, Germany\n           3LMU Munich, Germany 4Munich Center for Machine Learning (MCML), Germany\n                   5Technical University of Munich, Germany 6TU Dortmund University, Germany\n             7Lamarr Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany\n                       Correspondence: tom.zehle@tue.ellis.eu, moritz.schlager@tum.de, timo.heiss@stat.uni-muenchen.de\n\n\n                          Abstract                     through in-context learning, i.e., simply by a tex-\n                                                                       tual instruction and optionally few-shot examples\n               Prompt optimization has become crucial for                                                              provided to the LLM as input (Brown et al., 2020).\n                 enhancing the performance of large language\n                                                            Since this input (referred to as prompt) steers the\n                models (LLMs) across a broad range of tasks.\n                                                              output of the LLM (Karmaker Santu and Feng,2025           Although many research papers show its effec-\n                     tiveness, practical adoption is hindered as exist-       2023; White et al., 2023), the LLM’s performance\n                  ing implementations are often tied to unmain-      on a given task highly depends on it – in terms ofDec            tained and isolated research codebases. To ad-        quality, formulation, and the choice and order of\n                   dress this, we introduce promptolution, a uni-      examples (Zhao et al., 2021; Lu et al., 2022; Zhou2\n                    fied and modular open-source framework that                                                                           et al., 2023). Table 1 illustrates this with two simi-\n                  provides all components required for prompt\n                                                                           lar prompts for the GMS8K math dataset (Cobbe\n                  optimization within a single extensible system\n                                                                         et al., 2021). Despite their strong semantic sim-                   for both practitioners and researchers. It inte-\n                                                                              ilarity (matching parts in the same color), their                   grates multiple contemporary discrete prompt\n                  optimizers while remaining agnostic to the un-      performances differ substantially. This sensitiv-[cs.CL]\n                   derlying LLM implementation.                        ity highlights the potential of optimizing prompts\n                                                                    for specific tasks, much like how hyperparameter\n          §  AutoML/Promptolution                                                              tuning boosts performance in classical machine\n         Å  System Demonstration               learning (Kohavi and John, 1995).  Just as man-\n                                                                 ual hyperparameter search, crafting and refining\n          1  Introduction\n                                                     good prompts by hand (manual prompt engineer-\n           Modern large language models (LLMs) exhibit im-    ing) is tedious and unreliable (Jiang et al., 2020; Liu\n              pressive general-purpose capabilities, being able to    et al., 2023). Similar to how AutoML automates the\n             solve a wide variety of tasks (Radford et al., 2019;   search over large hyperparameter spaces (Feurer\n           Ouyang et al., 2022; Bai et al., 2023; Touvron et al.,   and Hutter, 2019), automatic prompt optimiza-\n             2023). They can be adapted to solve specific tasks    tion recently emerged to systematically search thearXiv:2512.02840v1\n\n\n\n\n\n              Figure 1: Overview of the promptolution framework. promptolution takes a dataset (dev set + few-shot\n              examples), token budget constraints, a description of the task, and optionally initial prompts as input. In an iterative\n               process, a user-selected prompt optimizer refines the prompt(s) by evaluating the LLM’s prediction performance on\n               the task’s development set and adapting the prompts accordingly (e.g., through another LLM). Detailed logging and\n               callbacks enable tracking the entire process. The optimized prompts are returned and can be evaluated on test data.\n\n\n                                                    1\n\nPrompt                                 Accuracy    ple LLM interfaces, NLP tasks, and contempo-\n Tackle this elementary math problem by         37.6%      rary discrete prompt optimization methods, which\n breaking it into logical steps. When you reach             can be used interchangeably or replaced by cus-\n the solution, enclose the final answer with              tom implementations of the components. While\n <final_answer> and </final_answer>                                           promptolution facilitates single-prompt optimiza-\n markers for clarity.\n                                                      tion for practitioners, a low abstraction level and\n Assist with solving the elementary or grade      53.8%\n                                                   helpers for systematic, reproducible experiments\n school level math problem that requires multiple\n steps and provide the solution within                       also make it geared toward researchers. Figure 1\n <final_answer> </final_answer> tags for             shows an overview of how the framework operates.\n easy identification.\n                                                 Outline. We position our work within the land-\nTable 1: Example prompts and their test set accuracy on   scape of automatic prompt optimization tools and\nGSM8K with Llama-3.3-70B (colors = similar phrases).    libraries (§2), describe the design of our framework\n                                                         (§3), evaluate its performance compared to unopti-\n                                           mized prompts and other libraries to demonstrate\ncombinatorial space of prompts (Li et al., 2025).                                                                   its utility and competitiveness (§4), and outline\nEspecially nowadays, when multi-agent systems                                                      future directions of the framework (§5).\nhave become central in both industry and research,\nLLMs specialized for individual tasks are increas-   2  Background & Related Works\ningly important (He et al., 2025; Li et al., 2024).\n                                       Prompt Optimization.  Automatic prompt opti-\nWhile fine-tuning entire models is expensive and\n                                                  mization refers to systematically exploring prompt\ndata-intensive, automatic prompt optimization is a\n                                                   spaces using various automated optimization strate-\nlightweight alternative and potential addition, of-\n                                                          gies, which may optimize both the instruction and\nten compatible with black-box LLMs (Cheng et al.,\n                                                      the few-shot example components of a prompt (Cui\n2024). A diverse collection of prompt optimizers\n                                                           et al., 2025; Li et al., 2025; Wan et al., 2024).\nhas thus been introduced (see Cui et al., 2025; Li\n                                           Prompt optimization is commonly categorized by\net al., 2025). However, several hurdles arise when\n                                                     the nature of the prompt space into continuous (Li\napplying these optimizers in practice. Each opti-\n                                              and Liang, 2021; Lester et al., 2021; Qin and Eisner,\nmizer lives in its own research repository. Trying\n                                             2021) and discrete approaches (Guo et al., 2024;\nand comparing several optimizers requires juggling\n                                           Yang et al., 2024; Zhou et al., 2023). For overviews\nwith multiple code bases and conflicting require-\n                                                   of the field, we refer to Li et al. (2025) and Cui\nments. Additionally, these repositories are typically\n                                                             et al. (2025). promptolution focuses on the LLM-\nnot actively maintained, and the research code of-\n                                                   agnostic and interpretable discrete prompt opti-\nten lacks proper software tests, documentation, and\n                                                    mization, which iteratively refines textual prompts\nrobustness. Moreover, their setups are inflexible,                                                            directly, often using another “meta”-LLM1 to gen-\nand deviations from their use cases (often limited to\n                                                        erate improved candidates. The following optimiz-\nsimple classification tasks) and LLM deployments\n                                                        ers are implemented in promptolution:\nrequire considerable effort. While existing libraries\n                                                          1. OPRO (Yang et al., 2024) uses LLMs as optimiz-\nand tools for prompt optimization partly address\n                                                          ers by providing a task description, examples,\nthese issues, they are either commercial and closed-\n                                              and previously scored candidates to the meta-\nsource (Amazon Bedrock, 2025; Anthropic, 2025;\n                                       LLM, which then proposes refined instructions.\nJina AI, 2025; Lee and Nardini, 2024), only imple-\n                                                          2. EvoPrompt (Guo et al., 2024) optimizes instruc-\nment a single optimizer (Adalflow, 2025; Agarwal\n                                                            tions using evolutionary algorithms, based on (a)\net al., 2024; Hinthorn and Nishimi, 2025; Yuksek-\n                                                    a genetic algorithm (GA) and (b) on differential\ngonul et al., 2024), or have a high abstraction level\n                                                      evolution (DE). In both cases, the meta-LLM\ndesigned mainly for end-to-end AI application de-\n                                                   performs crossover and mutation.\nvelopment (Khattab et al., 2024; Kulin et al., 2025).\n                                                          3. CAPO (Zehle et al., 2025) is a recent GA-based\n                                                           alternative that leverages AutoML techniquesContribution.  We introduce promptolution, a\n                                                            to improve cost-efficiency and jointly optimizesmodular, lightweight, and extensible open-source\n                                                  both instructions and few-shot examples, out-framework for automatic prompt optimization in\n                                                   performing the discrete optimizers above.Python, providing thoroughly tested and stable\nimplementations. Our library implements multi-      1Can be the same as the one we optimize prompts for.\n\n\n                                         2\n\nLow      Multiple        tegrates prompt optimization as only one com-\n   Framework     Extensible\n                               Abstraction Optimizers      ponent within a high-level program compiler,\n   promptolution   ✓      ✓      ✓         promptolution solely focuses on prompt opti-\n   DSPy             (✓)       ✗      ✓                                                 mization with relatively low-level abstraction, ex-   CoolPrompt       (✓)       ✗      ✓\n   promptomatix      ✗        ✗      ✓           tensible design, and support for systematic scien-\n   prompt-ops       ✗      ✓      ✓              tific benchmark studies and is thus more geared\n   TextGrad         ✗      ✓        ✗\n                                                toward researchers and advanced practitioners than    adalflow         ✗      ✓        ✗\n   PromptWizard     ✗      ✓        ✗          end-to-end AI application development (see also\n   PromptIM        ✗      ✓        ✗                                           Appendix A.1).  promptomatix (Murthy et al.,\n                                               2025) builds on DSPy through its structured prompt\n  Table 2: Comparison of open-source frameworks.\n                                                 compilation backend while also offering a lighter\n                                              meta-prompt optimizer.\nOther  promising  prompt  optimizers  not  yet     CoolPrompt (Kulin et al., 2025) is a very recent\nimplemented    in   promptolution   include   LLM-agnostic prompt optimization framework\nGEPA (Agrawal et al., 2025), TextGrad (Yuksek-    that supports multiple optimizers and emphasizes\ngonul et al., 2025), MIPRO (Opsahl-Ong et al.,   “zero-configuration” in contrast to promptolution,\n2024), and PromptWizard (Agarwal et al., 2024).    where researchers maintain close control over the\n                                                      setup. Table 2 summarizes how promptolution\nExisting Libraries, Frameworks & Tools.  Most    differs from all these open-source tools.\noptimizers above are implemented in siloed re-     Other related tools with a slightly different focus\nsearch repositories with hard-coded experimental    include PromptBench (Zhu et al., 2024), which tar-\nsetups. Oftentimes not actively maintained, lack-   gets LLM evaluation supporting only simple opti-\ning software tests and proper documentation, they    mization techniques, and OpenPrompt (Ding et al.,\nare inherently difficult to use for both scientific    2022), designed for prompt learning for language\nbenchmark experiments with other optimizers and   models predating modern LLMs.\npractical use cases with specific requirements. In   Closed-Source. Proprietary tools range from com-\nthe prompt optimization landscape, many actively   mercial web-platforms like PromptPerfect (Jina\nmaintained libraries and tools have emerged, in-   AI, 2025), cloud integrations such as Google\ncluding both open- and closed-source solutions:     Cloudâ˘A´Zs Vertex AI Prompt Optimizer (Lee and\nOpen-Source.  Several libraries only implement    Nardini, 2024) and the AWS Bedrock Prompt En-\na single prompt optimizer, including TextGrad    gineering Playground (Amazon Bedrock, 2025),\n(Yuksekgonul et al., 2024) using the method from    to vendor-specific solutions like Anthropic Claude\nYuksekgonul et al. (2025), AdalFlow (Adalflow,   Prompt Tools (Anthropic, 2025).\n2025) with LLM-AutoDiff (Yin and Wang, 2025),\n                                                 Positioning of promptolution.  Our library isMicrosoft’s PromptWizard (Agarwal et al., 2024)\n                                             an open-source, LLM-agnostic, and highly modu-based on  the eponymous  optimization  algo-\n                                                            lar framework.  It focuses exclusively on promptrithm, and PromptIM (Hinthorn and Nishimi,\n                                                  optimization rather than constructing full LLM2025) with an own iterative optimization strategy.\n                                                       pipelines. It already includes relevant LLM inter-prompt-ops (Meta-Llama, 2025) implements two\n                                                      faces and NLP tasks, along with evaluation metrics,optimizers, but focuses on prompt optimization for\n                                            and provides contemporary discrete prompt opti-Llama models. In contrast, promptolution is a\n                                                 mizers in a single, unified system. The frameworkframework with multiple optimizers that can be\n                                                                is highly customizable, offering fine-grained con-used interchangeably for arbitrary LLMs.\n                                                                trol over optimizers, tasks, logging, evaluation, and  DSPy  is arguably the most popular existing\n                                                 experiment configuration. As a result, it is suitableframework in prompt optimization (Khattab et al.,\n                                                         for both practitioners performing single-prompt op-2024) for building modular,  declarative LLM\n                                                    timization and researchers conducting systematicpipelines, and includes an embedded prompt op-\n                                             and reproducible large-scale benchmark studies.timization component.  It encompasses multiple\noptimizers like MIPROv2 (building on Opsahl-Ong                                       3  System Design\net al., 2024) or GEPA (Agrawal et al., 2025), and\ncan also combine LLM training with prompt op-   promptolution is designed as a modular and ex-\ntimization (Soylu et al., 2024). While DSPy in-    tensible framework consisting of four key com-\n\n\n                                         3\n\nTo solve this problem, we need to calculate the total number of\n                                                                        fish in the fishbowls at all the tables. First, [...] Then, we add\n                                                                the 3 fish from the table that has 3 fish: 62 fish + 3 fish = 65\n                                                                              fish. <final_answer> 65 </final_answer>\n\n                                                     Table 3: Example of the MarkerBasedPredictor ex-\n                                                              traction from a LLM response for a GSM8K sample.\n\n\n                                                    3.3  Task\n\nFigure 2: Core components of promptolution. The   The Task component holds the dataset and other\nupper part of each box lists the component implemen-    task-related information, including a textual de-\ntations, the lower part important functions. Arrows    scription of the task, and defines how prompts\nbetween components indicate conceptual connections.    are evaluated. It controls how subsampling is per-\n                                                 formed, which is crucial for efficiency, as not every\n                                            prompt needs evaluation on the full data for a rea-\nponents (see Figure 2).  All components follow\n                                                  sonable performance estimate.2 We implement the\na unified interface defined through corresponding\n                                                  following tasks relevant to NLP:\nBase-classes, ensuring that implementations can\n                                                          1. ClassificationTask targets discrete class la-be used interchangeably, remain fully compati-\n                                                        bels and evaluates predictions using standardble with the framework, and automatically inherit\n                                                            classification metrics. In our example in List-shared functionality. While each component can\n                                                      ing 3 (L. 7–13), the task is specified using accu-be configured individually (see §3.1–3.4), a sepa-\n                                                      racy as metric. The task is easily created from arate ExperimentConfig together with associated\n                                              Pandas DataFrame by specifying the input andhelper functions enables convenient parameteriza-\n                                                           label columns, along with a task description.tion of all components in a single object (see §3.5).\n                                                          2. JudgeTask is based on LLM-as-a-judge (Zheng\n3.1 LLM                                                  et al., 2023), where another LLM scores the\nThe LLM component provides an interface for ob-      quality of the output according to the task de-\n                                                            scription. This enables optimizing for subjec-taining responses from any LLM implementation.\n                                                                 tive, creative tasks, both supervised and unsu-Its base class enables parallelization and monitors\n                                                        pervised, as ground-truth labels are optional.token usage. Three classes are implemented:\n                                                          3. RewardTask allows optimization w.r.t. custom1. APILLM enables calls to LLMs hosted via an\n                                                   reward functions. The reward can represent any   API, covering common vendors such as OpenAI\n                                                 measurable objective, such as code execution   and Anthropic. Listing 3 (L.1–5) illustrates the\n                                                  time or business metrics.  Users define their   setup through the DeepInfra API.\n                                         own reward functions that compute a score (“re-2. LocalLLM allows using a local model using\n                                                 ward”) directly from a predictorâ˘A´Zs output.  transformers (Wolf et al., 2020).\n3. VLLM integrates the vllm library (Kwon et al.,\n                                                    3.4  Optimizer\n   2023) for efficient high-throughput inference\n   and serving, and deployment on GPU clusters.   The Optimizer component combines all other\n                                            components by using a Predictor and a Task\n3.2  Predictor                                       to determine the optimal prompt for the speci-\nThe Predictor component defines how predic-    fied setup.  Depending on the chosen optimiza-\ntions are extracted from the LLM’s output. For clas-    tion method, it may also rely on a (meta-) LLM for\nsification tasks, we extract only the predicted class,   prompt alteration. As the core of the framework, it\nwhereas for math reasoning tasks, we extract the    iteratively evaluates LLM predictions for a given\nfinal solution. The FirstOccurrencePredictor    task and refines the prompt(s) according to the re-\nsearches for the first occurrence of any possible    spective optimization strategy.\nclass label in the response and extracts it. The more     promptolution currently implements four es-\nrobust MarkerBasedPredictor (Listing 3, L.6) ex-\n                                                         2Depending on the subsampling strategy, evaluation is\ntracts predictions between predefined HTML-like                                                                    either performed on randomly drawn samples, a slice of the\nmarkers (see Table 3).                                       dataset (block), or the full dataset.\n\n\n                                         4\n\ntablished prompt optimizers: OPRO (Yang et al.,   not specified resort to carefully chosen defaults.\n2024), both EvoPromptDE and EvoPromptGA (Guo   The associated helper functions allow users to run\net  al., 2024), and the current SOTA discrete    the optimization process according to the config-\nprompt  optimizer CAPO  (Zehle  et  al.,  2025).   uration (run_optimization) and to evaluate the\npromptolution  caches  previously  evaluated   prompts on unseen test data (run_evaluation).\nprompts out of the box, and constrains prompt   The run_experiment() function (Listing 4, L. 8)\nevaluation during optimization to a subset of the   combines both steps, aiming to support researchers\navailable data, making algorithms faster and more   who want to perform extensive benchmark studies\nefficient. In Listing 3, we set up CAPO (L. 14–22)    across multiple datasets and optimizers, such as for\nand let it optimize prompts for 12 steps (L. 23).     newly proposed prompt optimizers.\n  Furthermore, new prompt optimizers can be\nadded with minimal effort by inheriting from the                                                      config = ExperimentConfig(\nbase optimizer class and implementing a custom         optimizer=\"capo\",\n_step() method that defines the iterative optimiza-         task_description=\"The␣task␣is...\",\n                                                          n_steps=12,\ntion scheme. This makes our framework particu-                                                          api_url=\"api.deepinfra.com/v1/...\",\nlarly useful for researchers developing and bench-         model_id=\"google/gemma-3-27b-it\",\nmarking new optimization algorithms.                 )\n                                                      prompts = run_experiment(df, config)\n\n\n  llm = APILLM(\n     api_url=\"api.deepinfra.com/v1/...\",            Figure   4:    Running  an  experiment  via  the\n     model_id=\"google/gemma-3-27b-it\",             ExperimentConfig abstraction.\n     api_key=\"...\",\n  )\n  predictor = MarkerBasedPredictor(llm=llm)\n  task = ClassificationTask(                        3.6  Supporting Modules and Utilities\n     df,\n                                            Exemplar Selection: Some prompt optimization     task_description=\"The␣task␣is...\",\n     x_column=\"text\",                              algorithms (e.g., OPRO or EvoPrompt) do not con-\n     y_column=\"label_text\",                          sider few-shot examples. However, they can sub-\n     metric=accuracy_score,\n                                                            stantially improve LLM performance (Brown et al.,  )\n  optim = CAPO(                                    2020), even with simple selection strategies (Wan\n     predictor,                                         et al., 2024). promptolution offers post-hoc ex-\n     task,\n     meta_llm=llm,                              emplar selection in an additional module, imple-\n     init_prompts=[                             menting random selection and random search3 to\n         \"Classify␣the␣text␣based␣on...␣\",         add few-shot examples to a fixed instruction.\n         # ...\n     ],                                                        Initial Prompt Creation: Many prompt optimiz-\n  )                                                    ers require an initial pool of prompts to start with.\n  prompts = optim.optimize(n_steps=12)                                  We offer functions to automatically create prompts\n                                            from a task description, a base prompt (following\n                                         Zhou et al., 2023), or samples from a dataset.\nFigure 3: Setup of promptolution’s core components.\n                                                      Callbacks: The base class for the optimizers sup-\n                                                     ports callbacks, allowing for easy tracking of the\n                                                  optimization progress. Callbacks can access the3.5  Experiment Configuration & Helper\n                                                          state of the optimizer at every optimization step,\npromptolution not only supports optimizing   and optionally terminate the process. Important im-\nprompts for a single specific setup, but also pro-   plementations include the TokenCountCallback,\nvides an ExperimentConfig framework that en-   which tracks the accumulated token budget and\nables a convenient and structured configuration of    terminates optimization if a specified threshold is\nlarger benchmark experiments. Additional helper   exceeded, and the FileOutputCallback, which\nfunctions enable the running and evaluation of such    writes prompts and their scores to a file, enabling\nexperiments with just a few lines of code.           easy post-hoc analysis of the process.\n   Listing 4 (L. 1–7) illustrates how the previ-\n                                                   3Random selection selects exemplars at random, whereas\nous example (Listing 3) can be expressed with                                                   random search generates multiple sets of random examples,\na single configuration class. Arguments that are     evaluates them, and selects the best performing set.\n\n\n                                         5\n\n4  Evaluation                              mented in promptolution, underscoring the com-\n                                                      petitiveness of our framework compared to other\nSetup.  To evaluate promptolution and contex-\n                                             prompt optimization tools. Although not every op-\ntualize its performance relative to other prompt\n                                                      timizer included in promptolution performs opti-\noptimization tools, we perform prompt optimiza-\n                                               mally on every task (e.g., OPRO on GSM8K), the\ntion on the popular GSM8K (grade school math\n                                                          library consistently provides at least one strong op-\nword problems; Cobbe et al., 2021) and SST-5\n                                                      timizer per task. Combined with its modular design,\ndataset (sentiment classification; Socher et  al.,\n                                                           this allows users to switch easily to an alternative\n2013).  We use gemma-3-27B  instruction\n                                                    optimizer if the current one yields unsatisfying per-\ntuned (Kamath et al., 2025) as the downstream\n                                               formance. We further emphasize that comparing\nLLM, and for optimizers that require one also as the\n                                                  optimizers within promptolution required mini-\nmeta-LLM. Further details on datasets and imple-\n                                           mal manual effort due to its dedicated support for\nmentation choices are provided in Appendix A.2.\n                                                   systematic benchmark experiments.\n  For our comparison, we employ the optimiz-\ners CAPO, EvoPromptGA, and OPRO from the\npromptolution library, and additionally evaluate   5  Conclusion & Future Directions\nthe leading optimizers from two other frameworks\nAdalFlow (LLM-AutoDiff) and DSPy (GEPA). To    In this work, we introduced promptolution, a uni-\nassess the impact of prompt optimization itself, we    fied and modular open-source Python framework\nalso evaluate three manually created prompts per    for automatic prompt optimization, designed for\ndataset and report their average performance. We   both practitioners performing single-task prompt\nuse the default parameterization for each optimizer    optimization and for researchers conducting large-\nto ensure practical comparability. We further re-   scale benchmark studies.  We highlighted the\nstrict the token budget to at most one million in-   unique position of our framework within the land-\nand output tokens combined, which corresponds    scape of prompt optimization tools, emphasizing its\nto a cost below $0.15 for this LLM. All frame-    extensible, modular design and its low abstraction\nworks are initialized from a single task description    level, focused specifically on optimizing prompts.\nwithout providing any initial prompts to test the   Furthermore, we demonstrated the role of each\nfull automation workflow. Both the unoptimized   component in the framework and how they interact\nprompts and best prompts per optimizer (based on    to support effective prompt optimization. Finally,\nvalidation performance) are evaluated on a test set.   through a comparative evaluation, we verified the\n                                                               utility and competitiveness of promptolution.Results. A summary of the results is presented\nin Table 4. With the exception of GEPA on SST-     Looking ahead, we plan to develop interoperabil-\n5 and OPRO on GSM8K, all optimizers substan-    ity interfaces with higher-level frameworks such\ntially outperform the unoptimized baseline with    as DSPy, enabling a combination of strengths from\nimprovements of up to 15%p in accuracy (CAPO on   both ecosystems. To further simplify the manage-\nGSM8K). This demonstrates the utility of prompt   ment of complex, large-scale experimental setups,\noptimization in general and of our framework in   we intend to introduce an interface to configuration\nparticular. The best-performing optimizer on both   management frameworks such as hydra (Yadan,\ndatasets, CAPO, as well as each runner-up, is imple-   2019). We also aim to improve accessibility by\n                                               implementing a lightweight graphical interface for\n                                                     real-time experiment tracking and visual analysis\n     Framework    Optimizer GSM8K SST5\n                                                   of the optimization process. In light of the recent\n       Baseline        unoptimized   78.1    44.6                                                           rise of multi-agent systems, we plan to support\n      AdalFlow       AutoDiff      88.7    55.7          the optimization of system prompts and interac-\n     DSPy       GEPA         84.7    42.0\n                                                      tion protocols across multiple agents. Addition-\n               OPRO        69.7    56.0            ally, inspired by AutoML, we intend to explore\n      promptolution EvoPrompt    91.0    53.3\n               CAPO        93.7    56.3         ensembling strategies for prompt optimizers, akin\n                                                      to ELPO (Zhang et al., 2025). More broadly, the\nTable 4: Test set accuracy of optimized prompts using    extensibility of our framework ensures that we can\nGemma3-27B-it. Bold values indicate the best, under-   continue to incorporate new state-of-the-art opti-\nlined values the second-best performance per dataset.     mization methods as the field evolves.\n\n\n                                         6\n\nBroader Impact                                        International Conference on Advances in Neural In-\n                                                       formation Processing Systems (NeurIPS’20), pages\nBy unifying multiple prompt optimization meth-     1877–1901. Curran Associates.\nods that were previously scattered across sepa-\n                                                                  Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning\nrate research repositories, promptolution makes\n                                               Wang, Yuxiao Dong, Jie Tang, and Minlie Huang.\nthe benefits of prompt optimization for improv-      2024.  Black-box prompt optimization: Aligning\ning LLM performance broadly accessible, allow-      large language models without model training. In\ning practitioners to leverage these capabilities in      Proceedings of the 62nd Annual Meeting of the As-\n                                                           sociation for Computational Linguistics (Volume 1:real-world industry applications. At the same time,\n                                                 Long Papers), pages 3201–3219, Bangkok, Thailand.\nits modular and extensible design, combined with                                                         Association for Computational Linguistics.\nexperiment-friendly implementations, makes the\nlibrary a powerful tool for researchers benchmark-   Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\n                                             Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthiasing new prompt optimization algorithms. Since\n                                                             Plappert, Jerry Tworek, Jacob Hilton, Reiichiro\nreimplementing competing methods and setting up                                                  Nakano, Christopher Hesse, and John Schulman.\nrigorous benchmark experiments is typically time-      2021. Training verifiers to solve math word prob-\nconsuming, promptolution offers the potential to       lems. arXiv:2110.14168 [cs.LG].\naccelerate research progress in prompt optimiza-\n                                             Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun,\ntion and to enhance methodological comparability                                              Damien Lopez, Kamalika Das, Bradley A. Malin,\nacross the field.                                     and Sricharan Kumar. 2025. Heuristic-based search\n                                                        algorithm in automatic instruction-focused prompt\n                                                           optimization: A survey.  In Findings of the Asso-\nReferences                                               ciation for Computational Linguistics: ACL 2025,\n                                                     pages 22093–22111, Vienna, Austria. Association\nAdalflow. 2025. Build and Optimize LM Workflows.       for Computational Linguistics.\n   Last accessed: 11/30/2025.\n                                              Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nEshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav      Zhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022.\n  Magazine, Tanuja Ganu, and Akshay Nambi. 2024.     OpenPrompt: An open-source framework for prompt-\n  PromptWizard:  Task-aware prompt optimization       learning. In Proceedings of the 60th Annual Meet-\n  framework. arXiv:2405.18369 [cs.CL].                   ing of the Association for Computational Linguistics:\n                                                      System Demonstrations, pages 105–113, Dublin, Ire-\nLakshya A. Agrawal, Shangyin Tan, Dilara Soylu,       land. Association for Computational Linguistics.\n  Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Ar-\n  nav Singhvi, Herumb Shandilya, Michael J. Ryan,    Matthias Feurer and Frank Hutter. 2019. Hyperparame-\n  Meng Jiang, Christopher Potts, Koushik Sen, Alexan-        ter optimization. In Frank Hutter, Lars Kotthoff, and\n  dros G. Dimakis, Ion Stoica, Dan Klein, Matei Za-      Joaquin Vanschoren, editors, Automated Machine\n   haria, and Omar Khattab. 2025.  GEPA: Reflec-      Learning: Methods, Systems, Challenges, pages 3–\n   tive prompt evolution can outperform reinforcement       33. Springer International Publishing.\n   learning. arXiv:2507.19457 [cs.CL].\n                                                Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\nAmazon Bedrock. 2025.  User Guide: Optimize a      Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu\n  prompt. Last accessed: 11/25/2025.                    Yang. 2024. Connecting large language models with\n                                                          evolutionary algorithms yields powerful prompt op-\nAnthropic. 2025. Prompt engineering: Use our prompt       timizers. In The Twelfth International Conference\n  improver to optimize your prompts. Last accessed:     on Learning Representations (ICLR’24). ICLR. Pub-\n  11/25/2025.                                               lished online: iclr.cc.\n\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,   Junda He, Christoph Treude, and David Lo. 2025. LLM-\n  Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei      based multi-agent systems for software engineering:\n  Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,       Literature review, vision, and the road ahead. ACM\n  Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,      Transactions on Software Engineering and Method-\n  Keming Lu, and 29 others. 2023. Qwen technical       ology, 34(5):1–30.\n   report. arXiv:2309.16609 [cs.CL].\n                                                  William F. Hinthorn and Masahiro Nishimi. 2025.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,      Promptim. Last accessed: 11/25/2025.\n   P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\n  A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,   Zhengbao Jiang, Frank Xu, Jun Araki, and Graham\n   T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,      Neubig. 2020. How can we know what language\n  C. Winter, and 12 others. 2020. Language models      models know? Transactions of the Association for\n   are few-shot learners.  In Proceedings of the 33rd      Computational Linguistics, 8:423–438.\n\n\n                                         7\n\nJina AI. 2025. PromptPerfect: AI Prompt Optimizer.   Xiang Li and Percy Liang. 2021. Prefix-tuning: Opti-\n   Last accessed: 11/25/2025.                           mizing continuous prompts for generation. In Pro-\n                                                       ceedings of the 59th Annual Meeting of the Asso-\nAishwarya Kamath, Johan Ferret, Shreya Pathak, Nino       ciation for Computational Linguistics and the 11th\n   Vieillard, Ramona Merhej, Sarah Perrin, Tatiana       International Joint Conference on Natural Language\n   Matejovicova, Alexandre RamÃl’, Morgane RiviÃ´lre,      Processing (Volume 1: Long Papers), pages 4582–\n  Louis Rouillard, Thomas Mesnard, Geoffrey Cideron,      4597. Association for Computational Linguistics.\n   Jean-bastien Grill, Sabela Ramos, Edouard Yvinec,\n  Michelle Casbon, Etienne Pot, Ivo Penchev, GaÃ´nl    Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang.\n   Liu, and 196 others. 2025. Gemma 3 technical report.      2024. A survey on LLM-based multi-agent sys-\n  arXiv:2503.19786 [cs.CL].                             tems: workflow, infrastructure, and challenges. Vici-\n                                                          nagearth, 1(1):9.\nShubhra Karmaker Santu and Dongji Feng. 2023.\n  TELeR: A general taxonomy of LLM prompts for    Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\n  benchmarking complex tasks. In Findings of the As-      Hiroaki Hayashi, and Graham Neubig. 2023. Pre-\n   sociation for Computational Linguistics: EMNLP        train, prompt, and predict: A systematic survey of\n  2023, pages 14197–14203. Association for Computa-      prompting methods in natural language processing.\n   tional Linguistics.                        ACM Computing Surveys, 55(9):195:1–195:35.\n\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,   Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\n  Zhiyuan Zhang, Keshav Santhanam,  Sri Vard-     and Pontus Stenetorp. 2022. Fantastically ordered\n  hamanan, Saiful Haq, Ashutosh Sharma, Thomas      prompts and where to find them: Overcoming few-\n   Joshi, Hanna Moazam, Heather Miller, Matei Za-      shot prompt order sensitivity. In Proceedings of the\n   haria, and Christopher Potts. 2024. DSPy: Compil-      60th Annual Meeting of the Association for Compu-\n   ing declarative language model calls into state-of-the-       tational Linguistics (Volume 1: Long Papers), pages\n   art pipelines. In The Twelfth International Confer-      8086–8098. Association for Computational Linguis-\n  ence on Learning Representations (ICLR’24). ICLR.        tics.\n  Published online: iclr.cc.\n                                                 Meta-Llama. 2025.   prompt-ops: An open-source\nRon Kohavi and George H. John. 1995. Automatic pa-       tool for LLM prompt optimization. Last accessed:\n  rameter selection by minimizing estimated error. In      11/30/2025.\n  Proceedings of the Twelfth International Conference\n                                                      Rithesh Murthy, Ming Zhu, Liangwei Yang, Jielin Qiu,  on Machine Learning (ICML’95). Morgan Kaufmann\n                                                          Juntao Tan, Shelby Heinecke, Caiming Xiong, Silvio   Publishers.\n                                                            Savarese, and Huan Wang. 2025. Promptomatix: An\nNikita Kulin, Viktor Zhuravlev, Artur Khairullin, Alena      automatic prompt optimization framework for Large\n   Sitkina, and Sergey Muravyov. 2025. CoolPrompt:     Language Models. arXiv:2507.14241 [cs.CL].\n  Automatic prompt optimization framework for Large\n                                                           Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David  Language Models. In Proceedings of the 38th Confer-\n                                                   Broman, Christopher Potts, Matei Zaharia, and Omar  ence of Open Innovations Association FRUCT, Issue\n                                                          Khattab. 2024. Optimizing instructions and demon-  1 (Full papers), pages 158–166, Helsinki, Finland.\n                                                                strations for multi-stage language model programs.  FRUCT Oy.\n                                                            In Proceedings of the 2024 Conference on Empirical\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying      Methods in Natural Language Processing (EMNLP),\n  Sheng, Lianmin Zheng, Cody Yu, Joseph Gonzalez,      pages 9340–9366. Association for Computational\n  Hao Zhang, and Ion Stoica. 2023. Efficient memory       Linguistics.\n  management for large language model serving with\n                                             Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-   pagedattention. In Proceedings of the 29th Sympo-\n                                                                      roll L. Wainwright, Pamela Mishkin, Chong Zhang,  sium on Operating Systems Principles (SOSP ’23),\n                                                       Sandhini Agarwal, Katarina Slama, Alex Ray, John  pages 611–626. Association for Computing Machin-\n                                                     Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,   ery.\n                                                Maddie Simens, Amanda Askell, Peter Welinder,\nGeorge Lee and Ivan Nardini. 2024. Announcing Pub-      Paul Christiano, Jan Leike, and Ryan Lowe. 2022.\n   lic Preview of Vertex AI Prompt Optimizer.  Last       Training language models to follow instructions with\n   accessed: 11/25/2025.                           human feedback. In Proceedings of the 35th Inter-\n                                                            national Conference on Advances in Neural Informa-\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.       tion Processing Systems (NeurIPS’22). Curran Asso-\n  The power of scale for parameter-efficient prompt        ciates.\n   tuning. In Proceedings of the 2021 Conference on\n  Empirical Methods in Natural Language Processing   Guanghui Qin and Jason Eisner. 2021. Learning how\n  (EMNLP), pages 3045–3059. Association for Com-       to ask: Querying LMs with mixtures of soft prompts.\n   putational Linguistics.                                   In Proceedings of the 2021 Conference of the North\n                                                  American Chapter of the Association for Computa-\nWenwu Li, Xiangfeng Wang, Wenhao Li, and Bo Jin.       tional Linguistics: Human Language Technologies,\n  2025. A survey of automatic prompt engineering: An      pages 5203–5212. Association for Computational\n   optimization perspective. arXiv:2502.11560 [cs.AI].       Linguistics.\n\n\n                                         8\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,      International Conference on Learning Representa-\n  Dario Amodei, and Ilya Sutskever. 2019. Language       tions (ICLR’24). ICLR. Published online: iclr.cc.\n  models are unsupervised multitask learners. OpenAI\n   blog, 1(8):9.                                       Li  Yin  and  Zhangyang  Wang.  2025.   LLM-\n                                                        AutoDiff:  Auto-differentiate any LLM workflow.\nRichard Socher, Alex Perelygin, Jean Wu, Jason      arXiv:2501.16673 [cs.CL].\n  Chuang, Christopher Manning, Andrew Ng, and\n   Christopher Potts. 2013. Recursive deep models for   Mert Yuksekgonul, Federico Bianchi, Joseph Boen,\n   semantic compositionality over a sentiment treebank.     Sheng Liu, Zhi Huang, Carlos Guestrin, and James\n   In Proceedings of the 2013 Conference on Empiri-      Zou. 2024. TextGrad: Automatic “Differentiation”\n   cal Methods in Natural Language Processing, pages      with Text. Last accessed: 11/30/2025.\n  1631–1642. Association for Computational Linguis-\n                                                Mert Yuksekgonul, Federico Bianchi, Joseph Boen,   tics.\n                                                Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin,\n                                                   and James Zou. 2025. Optimizing generative AI byDilara Soylu, Christopher Potts, and Omar Khattab.\n                                                       backpropagating language model feedback. Nature,  2024. Fine-tuning and prompt optimization: Two\n   great steps that work better together.  In Proceed-      639(8055):609–â˘A¸S616.\n   ings of the 2024 Conference on Empirical Methods\n                                       Tom Zehle, Moritz Schlager, Timo Heiß, and Matthias   in Natural Language Processing (EMNLP), pages\n                                                            Feurer. 2025. CAPO: Cost-aware prompt optimiza-  10696–10710. Association for Computational Lin-\n                                                                    tion. In 4th International Conference on Automated   guistics.\n                                                  Machine Learning.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n                                              Qing Zhang, Bing Xu, Xudong Zhang, Yifan Shi,   Martinet, Marie-Anne Lachaux, TimothÃl’e Lacroix,\n                                               Yang Li, Chen Zhang, Yik Chung Wu, Ngai Wong,   Baptiste RoziÃ´lre, Naman Goyal, Eric Hambro,\n                                                              Yijie Chen, Hong Dai, Xiansen Chen, and Mian   Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\n                                                    Zhang. 2025.  ELPO: Ensemble learning based  Edouard Grave,  and Guillaume Lample. 2023.\n                                                  prompt optimization for Large Language Models.  LLaMA: Open and efficient foundation language\n                                                      arXiv:2511.16122 [cs.CL].  models. arXiv:2302.13971 [cs.CL].\n                                                  Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nXingchen Wan, Ruoxi Sun, Hootan Nakhost, and Ser-\n                                                Sameer Singh. 2021. Calibrate before use: Improv-\n  can Arik. 2024. Teach better or show smarter? on\n                                                         ing few-shot performance of language models. In\n   instructions and exemplars in automatic prompt op-\n                                                      Proceedings of the 38th International Conference\n   timization. In Proceedings of the 37th International\n                                                on Machine Learning (ICML’21), volume 139 of\n  Conference on Advances in Neural Information Pro-\n                                                      Proceedings of Machine Learning Research, pages\n   cessing Systems (NeurIPS’24), pages 58174–58244.\n                                                   12697–12706. PMLR.\n  Curran Associates.\n                                                Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nJules White, Quchen Fu, Sam Hays, Michael Sand-                                                   Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\n   born, Carlos Olea, Henry Gilbert, Ashraf Elnashar,                                                Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\n   Jesse Spencer-Smith, and Douglas Schmidt. 2023.                                                     Joseph E. Gonzalez, and Ion Stoica. 2023. Judging\n A prompt pattern catalog to enhance prompt engi-                                                     LLM-as-a-judge with MT-bench and Chatbot Arena.\n   neering with ChatGPT. In Proceedings of the 30th                                                            In Proceedings of the 36th International Conference\n  Conference on Pattern Languages of Programs, PLoP                                                 on Advances in Neural Information Processing Sys-\n   ’23, pages 1–31, USA. The Hillside Group.                                                      tems (NeurIPS’23), pages 46595–46623. Curran As-\n                                                                 sociates.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\n  Chaumond, Clement Delangue, Anthony Moi, Pier-                                                        Y. Zhou, A. Ioan Muresanu, Z. Han, K. Paster, S. Pitis,\n   ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-                                                    H. Chan, and J. Ba. 2023. Large language models are\n   icz, Joe Davison, Sam Shleifer, Patrick von Platen,                                                      human-level prompt engineers. In The Eleventh In-\n  Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,                                                              ternational Conference on Learning Representations\n  Teven Le Scao, Sylvain Gugger, and 3 others. 2020.                                                        (ICLR’23). ICLR. Published online: iclr.cc.\n   Transformers: State-of-the-art Natural Language Pro-\n   cessing. In Proceedings of the 2020 Conference on    Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and\n  Empirical Methods in Natural Language Process-     Xing Xie. 2024. PromptBench: A unified library for\n   ing: System Demonstrations, pages 38–45, Online.      evaluation of Large Language Models. Journal of\n  Association for Computational Linguistics.             Machine Learning Research, 25(254):1–22.\n\nOmry Yadan. 2019. Hydra - a framework for elegantly\n   configuring complex applications.  Last accessed:\n  11/30/2025.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao\n   Liu, Quoc Le, Denny Zhou, and Xinyun Chen. 2024.\n  Large language models as optimizers. In The Twelfth\n\n\n                                         9\n\nA  Appendix                                    gested prompt.\n                                                For evaluation, we route every LLM API call\nA.1  Delimitation from DSPy                                                 through the same interface using langchain to en-\nOne might argue that the existing framework    sure a fair comparison of the optimized prompts,\nDSPy (Khattab et al., 2024) is already very sim-   and compare exact matches between the predicted\nilar to promptolution, as it is also open-source   and true labels (allowing for differences in cap-\nand modular, supports prompt optimization, imple-    italization).  In the case of GEPA, we had to\nments multiple optimizers and allows for several   manually clean the LLM outputs because they\nLLM implementations, and provides additional util-   did not follow the required output format stated\nities. However, there are several important differ-    in the task description (encapsulating the predic-\nences, both in underlying purpose and in concrete    tion within <final_answer> tags).  Specifically,\ndesign, that clearly distinguish the two frameworks:  we had to remove the “[[ ## target ## ]]”\nDifferent focus  : DSPy includes prompt optimiza-   and “[[  ##  completed  ##  ]]” tags.  For\n     tion only as one component within a broader   AdalFlow, we had to intercept API calls due to\n     compilation workflow tightly coupled to a pro-    faulty extraction of system and user prompts. The\n    gram structure, whereas promptolution is   system-prompt extraction mechanism relied on\n     not a full application framework and instead    tags, but these were not forwarded correctly (e.g.,\n     focuses exclusively on the prompt optimiza-   “<START_OF_USER_PROMPT>” was expected instead\n     tion stage.                                    of the provided “<START_OF_USER>”).  These\nAbstraction level  : promptolution provides a   changes are documented in the experiment reposi-\n     relatively low-level abstraction, emphasizing    tory.\n      full user control over each component and\n     transparency via detailed logging and call-   Dataset          Huggingface ID   ndev ntest\n    back hooks. In contrast, DSPy hides much of\n                                       SST5               SeetFit/sst5       500 300\n     the prompt construction behind a high-level\n                                GSM8K          openai/gsm8k     500 300\n     declarative interface (though it remains in-\n     spectable).                                                       Table 5: Overview of the utilized HuggingFace datasets.\nTarget user base  :   Thus,  promptolution  is\n     geared toward researchers and advanced ML   The utilized datasets and sample sizes are detailed\n      practitioners, while DSPy primarily targets AI    in Table 5. The dev set is used for optimization, the\n     application developers building end-to-end    test set for holdout evaluation of the final prompts.\n    workflows such as agents or RAG systems.     Few-shot examples, if considered by the optimizer,\nExperiments  : promptolution makes it easy to    are also taken from the dev set.\n    implement custom optimizers, tasks, and other    We make  our  complete  experiment  code\n     components, and is particularly suited for sys-   publicly   available   for    full   reproducibil-\n     tematic large-scale benchmark experiments    ity    at   https://github.com/finitearth/\n    due to the integrated config framework. Its ex-   prompt-optimization-framework-comparison.\n      tensibility explicitly encourages contributions\n                                            A.3  Quality Standards     of new algorithms and tasks.\nCompatibility  : promptolution integrates seam-   To ensure high code quality, we adopt established\n     lessly into existing inference pipelines, since    software engineering best practices throughout our\n     the result of optimization is a single string,   package. We maintain a comprehensive test suite\n    whereas DSPy essentially requires substituting    that automatically verifies expected behavior after\n     parts of its program representation.            code changes. All tests must pass before a release is\n                                                     published, ensuring users encounter no issues when\nA.2  Experiment Details                                                     updating. The main branch is protected, and all con-\nWe evaluate our framework against DSPy and    tributions must be submitted via pull requests, each\nAdalFlow.  Since neither provides a straightfor-   reviewed by another main contributor. We addition-\nward way to restrict compute budgets based on    ally employ pre-commit hooks to automatically\ntoken counts, we enforce the token limit by raising   check code formatting, documentation, and other\nan exception inside the respective LLM wrappers    basic quality issues before commits are made, im-\nonce the limit is exceeded, returning the last sug-   proving readability and maintainability. We also\n\n\n                                         10\n\nmaintain strict documentation standards and do not          Yours␣sincerely,␣Tom␣Zehle.\"\naccept poorly documented pull requests. Dedicated\nCI and CI/CD pipelines enable an automated build,\n                                We then instantiate the underlying LLM. In this\ntest, and release of the package and documentation.\n                                               example, the APILLM wrapper connects to the\n                                           Llama-3.3-70B-Instruct-Turbo model via theA.4  Documentation & Tutorials\n                                               DeepInfra API, which serves as the downstream\nAlongside  the  open-source  software  pack-                                     LLM, the judge, and the meta-LLM.\nage,  we  provide  extensive  documentation\navailable    at   https://automl.github.io/\n                                                      llm = APILLM(\npromptolution/. It covers the major components                                                          model_id=\"meta-llama/Llama-3.3-70B-\"\\\nand their functionality, and also includes tuto-                 \"Instruct-Turbo\",\nrials that guide users through their first prompt         api_url=\"https://api.deepinfra.com/\"\\\n                                                                 \"v1/openai\",\noptimization use case. This further enhances the         api_key=open(\"token.txt\").read(),\naccessibility and usability of our framework.           )\n\nA.5  System Demonstration\n                                         To initialize the optimization search space, a set\nThe system demonstration under https://youtu.                                                   of candidate prompts is derived directly from the\nbe/gySdgjEhsZA uses the following code exam-                                                      task description using the framework’s utility func-\nple, through which we guide step-by-step. The                                                            tion. These serve as the starting population for the\nimplementation requires the installation of the                                                   optimizer we use later.\npromptolution package with API support en-\nabled. The subsequent imports establish the nec-\n                                                      initial_prompts =\nessary components for the LLM wrapper, CAPO,          create_prompts_from_task_description(\nand the evaluation task definitions.                        task_description,\n                                                          llm,\n                                                      )\n  ! pip install promptolution[api]\n\n                                              Given the subjective nature of the email generation\n                                                     task (lacking a ground truth), a JudgeTask is con-\n  from promptolution.llms import APILLM\n  from promptolution.optimizers import CAPO          figured. This setup utilizes an LLM-as-a-Judge ap-\n  from promptolution.tasks import JudgeTask         proach to evaluate the semantic quality of the gener-\n  from promptolution.predictors import\n                                                    ated outputs against the input instructions. A great      MarkerBasedPredictor\n  from promptolution.utils import                     alternative could be the ClassificationTask in\n      create_prompts_from_task_description          cases where ground truth labels are available, or the\n  import pandas as pd\n                                           RewardTask when the user can define an objective\n                                                    function that scores the outputs.\nThe dataset, containing raw email-generation in-\nstructions, is loaded from a CSV file using pandas:                                                      task = JudgeTask(\n                                                          df_emails,\n                                                          judge_llm=llm,\n  df_emails = pd.read_csv(\"emails.csv\")                                                          task_description=task_description,\n                                                          x_column=\"instruction\",\n                                                      )\nAfter loading the data, we define a task description\nthat specifies the desired persona, tone, and format-\n                                                 Next, we instantiate the CAPO optimizer with ating constraints (including the closing signature).\n                                               marker-based predictor. The optimization routineThis serves as the reference specification for the\n                                                                  is executed for six iterations (n_steps=6) to refineentire workflow.\n                                                     the prompt candidates iteratively.\n\n  task_description = \"Write␣concise,␣polite,␣\n      professional␣academic␣emails␣for␣me␣as␣a␣       optimizer = CAPO(\n      PhD␣student,␣asking␣clarifying␣questions␣           task=task,\n      when␣my␣instructions␣are␣vague,␣avoiding␣           predictor=MarkerBasedPredictor(llm),\n      cliche␣openings,␣and␣always␣ending␣with␣            meta_llm=llm,\n\n\n\n                                         11\n\ninitial_prompts=initial_prompts,\n     check_fs_accuracy=False\n  )\n  final_prompts = optimizer.optimize(n_steps=6)\n\n\nAfter optimization completes, the final optimized\nprompts are returned and ready for use in email\ngeneration.\n\n\n\n\n\n                                         12",
"headers": [
"arXiv:2512.02840v1  [cs.CL]  2 Dec 2025",
"promptolution",
": A Unified, Modular Framework for Prompt Optimization"
],
"tables": [
"|Tackl|e this|Col3|ele|mentar|y math p|rob|Col8|lem|by|\n|---|---|---|---|---|---|---|---|---|---|\n|break|ing|it i|nto|logica|l steps.|l steps.|l steps.|l steps.|l steps.|\n|the solution|the solution|the solution|, en|close t|he final|an|swer|swer|wit|\n|<fin|al_answer> and|al_answer> and|al_answer> and|al_answer> and|</final_answer>|</final_answer>|</final_answer>|</final_answer>|</final_answer>|\n|mark|ers|for|cla|<br>  rity.|<br>  rity.|<br>  rity.|<br>  rity.|<br>  rity.|<br>  rity.|",
"|Assis|t with|Col3|sol|ving th|e eleme|ntary|Col8|or|grade|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|schoo|l level|l level|math pro|math pro|blem tha|t re|quires|quires|quires|mul|ti|ple|\n|steps|and|pr|ovide the|ovide the|solution|within|within|within|within|within|within|within|\n|<fin|al_answer> </f|al_answer> </f|al_answer> </f|al_answer> </f|inal_answer>|inal_answer>|inal_answer>|inal_answer>|tag|s for|s for|s for|\n|<br>easy|<br> iden|<br> tif|<br> icat|<br> ion.|<br> ion.|<br> ion.|<br> ion.|<br> ion.|<br> ion.|<br> ion.|<br> ion.|<br> ion.|",
"|Framework|Extensible|Low<br>Abstraction|Multiple<br>Optimizers|\n|---|---|---|---|",
"|promptolution<br>DSPy<br>CoolPrompt<br>promptomatix<br>prompt-ops<br>TextGrad<br>adalflow<br>PromptWizard<br>PromptIM|✓<br>(✓)<br>(✓)<br>✗<br>✗<br>✗<br>✗<br>✗<br>✗|✓<br>✗<br>✗<br>✗<br>✓<br>✓<br>✓<br>✓<br>✓|✓<br>✓<br>✓<br>✓<br>✓<br>✗<br>✗<br>✗<br>✗|\n|---|---|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2512.02840v1.pdf"
}