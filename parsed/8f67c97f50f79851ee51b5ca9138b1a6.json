{
"text": "Under Review\n\n\n          REWARD-AGNOSTIC PROMPT OPTIMIZATION FOR\n          TEXT-TO-IMAGE DIFFUSION MODELS\n\n\n                   Semin Kim  Yeonwoo Cha  Jaehoon Yoo  Seunghoon Hong\n                  KAIST\n                    {seminkim, ckdusdn03, wogns98, seunghoon.hong}@kaist.ac.kr\n\n\n\n                                       ABSTRACT\n\n                    We investigate a general approach for improving user prompts in text-to-image\n                                (T2I) diffusion models by finding prompts that maximize a reward function speci-\n                                   fied at test-time. Although diverse reward models are used for evaluating image2025\n                                 generation, existing automated prompt engineering methods typically target specific\n                              reward configurations. Consequently, these specialized designs exhibit suboptimal\n                             performance when applied to new prompt engineering scenarios involving differ-Sep                                ent reward models. To address this limitation, we introduce RATTPO (Reward-\n29                        AgnosticapplicableTest-Timeacross variousPromptrewardOptimization),scenariosa withoutflexible test-timemodification.optimizationRATTPOmethoditera-\n                                    tively searches for optimized prompts by querying large language models (LLMs)\n                                without requiring reward-specific task descriptions. Instead, it uses the optimization\n                                  trajectory and a novel reward-aware feedback signal (termed a \"hint\") as context.\n                              Empirical results demonstrate the versatility of RATTPO, effectively enhancing\n                                user prompts across diverse reward setups that assess various generation aspects,\n                             such as aesthetics, general human preference, or spatial relationships between[cs.LG]\n                                  objects. RATTPO surpasses other test-time search baselines in search efficiency,\n                             running 4.8 times faster than naive reward-agnostic test-time search baseline on\n                                average. Furthermore, with sufficient inference budget, it can achieve comparable\n                             performance to learning-based baselines that require reward-specific fine-tuning.\n\n\n                1  INTRODUCTION\n\n                     Recent advancements in text-to-image (T2I) diffusion models have enabled the generation of high-\n                        quality and diverse images from user prompts (Rombach et al., 2022; Ramesh et al., 2022; Podell\n                            et al., 2024; Black Forest Labs, 2024). Despite the success, their output is heavily reliant on the input\n                     prompts (Liu & Chilton, 2022; Oppenlaender, 2024; Diab et al., 2022), often exhibiting noticeable\n                         fluctuations in generation quality in response to subtle changes in prompts (Mahajan et al., 2024;\n                  Cao et al., 2023) that do not alter the underlying semantics. To improve the generation process to\n                       follow user intent and yield desired visual outputs, one needs to iteratively improve the initial promptarXiv:2506.16853v2                through a trial-and-error loop, a process commonly referred to as prompt engineering.\n                    Prompt engineering refines prompts to guide a T2I model to produce better images. The quality of\n                    images is often measured by diverse reward models evaluating aspects like human preference (Xu\n                           et al., 2023; Schuhmann, 2022; Kirstain et al., 2023; Wu et al., 2023), text-to-image alignment (Rad-\n                       ford et al., 2021; Huang et al., 2025; Hu et al., 2023; Cho et al., 2024), and their implicit or explicit\n                     combinations (Hao et al., 2023; Ma et al., 2025; Huang et al., 2025; Sun et al., 2024; Chen et al.,\n                      2023). Based on these reward models, several works (Yun et al., 2025; Hao et al., 2023; Mo et al.,\n                      2024; Mañas et al., 2024; Wang et al., 2024) have proposed automated prompt engineers that rewrite\n                       user prompts into enhanced versions capable of generating high-reward images.\n\n                    However, many of the aforementioned automated prompt engineering techniques (Yun et al., 2025;\n                 Hao et al., 2023; Mañas et al., 2024; Mo et al., 2024) are tailored for particular reward models.\n                       Consequently, these specialized methods often exhibit suboptimal performance when applied to new\n                        scenarios involving different reward functions, which limits their general applicability. This limitation\n                        highlights the need for a versatile prompt optimization technique that can adapt to diverse reward\n                        functions at test-time without requiring retraining or manual, reward-specific adjustments.\n\n\n                                                           1\n\nUnder Review\n\n\n\n\n\nTo address this challenge, we introduce Reward-Agnostic Test-Time Prompt Optimization (RATTPO),\na flexible test-time optimization approach for reward-agnostic automated prompt engineering. By\navoiding reward-specific designs, RATTPO remains reward-agnostic and can optimize user prompts\nwith respect to reward functions that are only defined at test-time. The method iteratively refines an\ninitial user prompt by querying large language models (LLMs), without relying on explicit reward-\nspecific task descriptions that are often human-crafted for better guiding the LLMs.  Instead, it\nconditions the LLM-based optimizer on the historical trajectory of previously attempted prompts and\ntheir corresponding reward scores, along with a novel reward-aware feedback signal we term a \"hint.\"\nEach hint is a concise textual strategy for increasing the reward, analogous to a manually written\ntask description, but generated on-the-fly by an independent LLM during optimization. This design\nremoves the need for manual rewriting while still providing reward-aware guidance to the optimizer.\n\nOur contributions are threefold:  First, we propose RATTPO, a training-free and gradient-free\nautomated prompt engineer that is readily applicable to diverse reward setups without requiring\nreward-specific adjustments or training. Second, we introduce \"hint\", a novel, reward-aware self-\nfeedback mechanism to guide the prompt engineering process. The hint offsets the absence of a\nreward-specific design by estimating strategy for improving reward from optimization trajectory.\nThird, we empirically demonstrate RATTPO’s effectiveness and versatility across various reward\nsettings, including human preference, text-to-image consistency, and holistic evaluation using a\nmultimodal LLM. Our extensive experiments demonstrate that RATTPO can effectively improve the\ninitial prompt with respect to diverse rewards, and shows higher search efficiency compared to other\ntest-time search baselines.\n\n\n2  RELATED WORK\n\nLLM-Based Optimization  Recent works have leverage the strong instruction-following (Ouyang\net al., 2022; Sanh et al., 2022; Qin et al., 2024) and in-context learning capabilities (Brown et al.,\n2020; Wei et al., 2022; Dong et al., 2022) of LLMs for various optimization tasks (Yang et al., 2024;\nDu et al., 2024; Mañas et al., 2024; He et al., 2024; Zhang et al., 2023; Liu et al., 2023). Among these,\nOPRO (Yang et al., 2024) is closely related to our approach, proposing an optimization-by-prompting\nframework that iteratively queries LLMs using previous optimization history as context. RATTPO\nextends this concept by constructing a dual-LLM optimization loop with a novel feedback mechanism,\ntackling the unique problem of building a reward-agnostic prompt engineer for T2I models.\n\n\nAutomated Prompt Engineering for Diffusion Models  The output generated by diffusion models\noften deviates from user intention or preference, as captured by diverse reward models assessing\nhuman preference (Xu et al., 2023; Schuhmann, 2022; Kirstain et al., 2023; Wu et al., 2023), text-\nto-image consistency (Radford et al., 2021; Huang et al., 2025; Hu et al., 2023; Cho et al., 2024),\nand other criteria (Hao et al., 2023; Ma et al., 2025; Huang et al., 2025; Sun et al., 2024; Chen et al.,\n2023). To bridge this gap without manual prompting, several studies have focused on automated\nprompt engineering for a given reward, employing either training-based or test-time approaches.\n\nLearning-based methods like Promptist (Hao et al., 2023) define a heuristic reward model and train\na language model via reinforcement learning (RL). Similarly, Mo et al. (2024) propose finetuning\na language model using RL to utilize specialized prompt formats, while PAG (Yun et al., 2025)\nfocuses on the diversity of resulting prompts. While effective for trained rewards, these methods\nincur considerable training costs (e.g., approximately 4 GPU days in Yun et al. (2025)). We also\nempirically observe that their transferability to novel reward scenarios is limited (see Sec. 4.2).\n\nInstead of training language models, a few works seek alternatives that utilize test-time computation.\nDPO-Diff (Wang et al., 2024) constrains a search space with LLM-generated synonyms or antonyms\nand optimizes the negative prompt via gradient descent employing several optimization tricks. While\nthis approach makes the search tractable, its performance is inherently limited as the reduced search\nspace may exclude optimal prompt candidates. OPT2I (Mañas et al., 2024) is similar to our approach\nin that it also employs an LLM to optimize user prompts at test time. Specifically, OPT2I constructs\nan iterative loop in which an LLM refines the initial prompt based on history from previous iterations\nto enhance text-to-image consistency. Despite being a test-time approach, the LLM in OPT2I is tied\nwith human-crafted query prompts and thus they are tailored for and evaluated under two specific\nchoices of reward functions, leaving its applicability to unseen, diverse rewards unanswered.\n\n\n                                       2\n\nUnder Review\n\n\n\n\n\nAlgorithm 1 RATTPO: Reward-Agnostic Test-Time Prompt Optimization\n  1: Input: User prompt p0, T2I model G, reward model R, optimization iterations N\n  2: Initialize: history ←∅, hint ←null\n  3: for t = 1 to N do\n  4:     contexto ←SampleTrajectories(history)\n  5:    candidate_prompts ←Lo(p0, contexto, hint)         ▷Optimizer LLM proposes prompts\n  6:     for each p in candidate_prompts do\n  7:        score = EI∼G(p)R(I, p0)                    ▷Generate images and compute score\n  8:         history ←history ∪{p, score}                               ▷Update history\n  9:    end for\n10:     hint ←Lh(SampleTrajectories(history))            ▷Hint-generator LLM generates hint\n11: end for\n12: Return: Best prompt ˆp                       ▷Select the best-scoring prompt from history\n\n3  METHOD\n\n3.1  PROBLEM SETUP\n\nWe consider the problem of improving user prompts for text-to-image (T2I) generative models.\nGiven an initial prompt p0, a T2I generative model G, and a reward function R, prompt engineering\naims to produce an enhanced prompt ˆp that maximizes the expected reward of the generated images\nwhile preserving the semantics of the original prompt. Formally, this can be cast as the following\noptimization problem over the set of prompts that preserve the semantics S(p0):\n                                              ˆp = arg max EI∼G(p)[R(I, p0)].                               (1)\n                                     p∈S(p0)\n\nAutomating the prompt engineering process has often been considered under predefined reward setups.\nA prominent example is finetuning a language model with respect to reward R during training (Hao\net al., 2023; Mo et al., 2024; Yun et al., 2025), resulting in a dedicated prompt engineer for the specific\nreward and T2I model it was trained on. While it is technically possible to use them for different\nreward models at test-time without reward-specific re-training, they show suboptimal performance\nwhen there is a large shift in the target reward. On the other hand, test-time approaches offer greater\nflexibility by adapting to new rewards on-the-fly, but previous methods either limit the search space\nfor tractability (Wang et al., 2024), which weakens performance, or are tailored for specific reward\nchoices (Mañas et al., 2024), necessitating manual configuration for application to a new reward.\n\nIn contrast, we focus on the more challenging scenario of building a reward-agnostic automated\nprompt engineer that is capable of solving Eq. 1 for a broad range of unknown reward functions R,\nso it can be directly applied to diverse prompt engineering scenarios without any further modification.\nOur aim is to handle real-world application scenarios, where reward models with different evaluation\nrubrics, potentially personalized ones, are continually developed and deployed for capturing fine-\ngrained preferences. In such application scenarios, a principled reward-agnostic prompt engineer can\nbe trained or designed once and later used for downstream reward models in general, avoiding the\nneed to train separate dedicated prompt engineers again.\n\n3.2  BUILDING REWARD-AGNOSTIC PROMPT OPTIMIZER\n\nTo build a reward-agnostic yet effective automated prompt engineer, we choose a test-time search\napproach equipped with LLMs for efficient exploration of the large search space S(p0). Our\noptimization iteration consists of two LLMs, one for proposing prompt candidates and another for\nproviding reward-specific feedback. At each iteration, the optimizer LLM Lo first proposes a set\nof promising prompt candidates that are used to generate images. The generated images are then\nevaluated using reward model R to score the proposed prompts. The hint-generator LLM Lh is used\nto provide feedback to the optimizer LLM by describing the reward function based on optimization\nhistory. Below, we describe each component and its design choices (the complete algorithm is\npresented in Alg. 1; prompt templates for querying LLMs can be found in App. B).\n\nThe Optimizer LLM  Motivated by the successful application of LLMs to various optimization\nproblems (Yang et al., 2024; Du et al., 2024; Mañas et al., 2024; He et al., 2024; Zhang et al., 2023;\n\n\n                                       3\n\nUnder Review\n\n\n\n\n\nLiu et al., 2023), we exploit the strong in-context learning (ICL) capabilities of LLMs (Brown et al.,\n2020; Wei et al., 2022; Dong et al., 2022) to solve Eq. 1. Specifically, we query the optimizer LLM\nto enhance the initial prompt in several distinct ways by using top-k history from previous iterations\nand feedback from the hint-generator LLM. By prompting the optimizer LLM with a handful of\noptimization history examples, we bias its output prompt candidates towards more promising regions\nof the search space. This in-context policy shift uses examples generated on-the-fly and therefore does\nnot require reward-specific finetuning, aligning with our goal of building a reward-agnostic automated\nprompt engineer. Furthermore, the optimization process is both training-free and gradient-free,\nmaking it applicable to diverse reward models that are often non-differentiable (e.g., visual question\nanswering models) or only available via forward API call (e.g., proprietary vision-language models).\n\nGuiding the Optimizer LLM with the Hint-Generator LLM  The optimizer LLM is controlled\nby a meta-prompt, an instruction used for querying it (Yang et al., 2024). Previous work (Yang et al.,\n2024; Fernando et al., 2023; Du et al., 2024; He et al., 2024; Mañas et al., 2024) has typically relied\non predefined task descriptions and objectives to instruct the optimizer LLM (Yang et al., 2024).\nWhile removing such descriptions is necessary for our purposes, it can also reduce optimization\nperformance. Therefore, we keep the task description general and concise, and instead augment it\nwith another type of reward-specific optimization signal.\n\nInspired by successful self-feedback techniques in NLP literature (Madaan et al., 2023; Wang et al.,\n2023a; Shinn et al., 2023), we introduce the hint-generator LLM to compensate for the potential loss\nin search efficiency. To be specific, we ask hint-generator LLM to response with how we can improve\nthe score, given the context of optimization history (see Fig. 4 for example). When constructing the\ncontext, we stick to a simple design of also using the search history obtained from the optimizer LLM,\nsince it already contains information about the optimization objective. To help the hint-generator\nLLM better identify the reward function, we provide random subset of histories as context that include\nboth good and bad examples. Besides the empirical gain in search efficiency, the hint is formatted as\nnatural language feedback and is therefore human-interpretable. This transparency is often beneficial,\nsince human prompt engineers can review the generated hints and later use them for manual prompt\nengineering.\n\n4  EXPERIMENT\n\n4.1  EXPERIMENTAL SETTING\n\nReward Models  To evaluate the effectiveness of RATTPO across diverse scenarios, we consider\nvarious reward models as optimization targets, primarily categorized into human preference, text-to-\nimage consistency, and holistic assessment using multimodal LLMs (MLLMs).\n\n• Human Preference: We consider the Promptist Reward (Hao et al., 2023) and ImageReward (Xu\n   et al., 2023). Promptist Reward combines Aesthetic Score (Schuhmann, 2022) and CLIP Score (Rad-\n  ford et al., 2021) to balance image aesthetics with faithfulness to the prompt. ImageReward is\n  trained on a human-annotated preference dataset to capture general human preferences.\n• Text-to-Image Consistency: We consider reward models with different scoring mechanisms. First,\n DSG (Cho et al., 2024) generates atomic questions with a dependency graph from a given prompt,\n  and then utilizes a visual question answering model for scoring. Secondly, we adopt three scorers\n  from T2I-CompBench++ (Huang et al., 2025) to assess 2D/3D spatial relationships and numeracy\n  (UniDet2D, UniDet3D, and UniDetNumeracy, respectively) using an object detection model.\n• Holistic MLLM Assessment: We assess image quality using LLMGrader, following the procedure\n  of Ma et al. (2025). LLMGrader evaluates an overall score along with five sub-scores: accuracy\n  to prompt, creativity and originality, visual quality and realism, consistency and cohesion, and\n  emotional or thematic resonance. We use the overall score as our optimization target.\n\nDatasets  We consider two collections of simplified user prompts from the Lexica (Lexica, 2023)\nwebsite and DiffusionDB (Wang et al., 2023b) for Promptist Reward, ImageReward and LLMGrader.\nWe use the evaluation split used in Yun et al. (2025). For the DSG score, we use a subset of\nPartiPrompt (Yu et al., 2022) to follow the experimental setup of Mañas et al. (2024). We use a\none-third subset of the evaluation prompts from each category (2D, 3D, and numeracy) in T2I-\nCompBench++ (Huang et al., 2025) for the UniDet evaluator1.\n\n   1UniDet depends on T2I-CompBench++ dataset as it extracts desired image compositions from the prompt.\n\n\n                                       4\n\nUnder Review\n\n\n\n\n\n                (a) Experiments on human preference rewards (Promptist Reward, ImageReward)\n\n\n\n\n\n          (b) Experiments on text-to-image consistency rewards (DSG, UniDet2D / 3D / Numeracy)\nFigure 1: Optimization curves for human preference and text-to-image consistency rewards. Each\ncurve shows how the reward changes as the number of generated prompts increases.\n\nTable 1: Experimental results on LLMGrader reward in Lexica and DiffusionDB datasets. Test-time\nsearch methods are evaluated at the budget of 160 generated prompts. Full results in Tab. 14.\n\n   Dataset       Method     Accuracy   Originality   Visual   Consistency   Emotional   Overall\n\n                       Initial         67.85       62.84      80.11      83.93        69.59      72.49\n   Lexica         Paraphrase     69.73       67.24      83.94      86.40        73.92      89.01\n\n              Ours          75.11       72.44      85.96      88.44        77.97      89.69\n\n                       Initial         68.99       63.18      81.24      84.58        69.48      73.12\n   DiffusionDB   Paraphrase     69.65       66.56      84.35      86.81        72.75      88.25\n\n              Ours          73.39       70.66      86.40      88.30        76.14      88.99\n\n\n\nBaselines  We compare RATTPO with both learning-based and test-time search-based methods. For\nthe learning-based methods, we consider Promptist (Hao et al., 2023) and PAG (Yun et al., 2025)\nwhich train language models to directly enhance user-provided prompts. Following Yun et al. (2025),\nwe generate sixteen responses from their official checkpoint by beam search and report the best\none. Evaluations on reward setups other than the Promptist Reward are performed by applying them\nwithout retraining (i.e., in a zero-shot manner).\n\nFor test-time search-based methods, we consider DPO-Diff (Wang et al., 2024), OPT2I (Mañas et al.,\n2024), and additionally two naive best-of-N baselines that modify the initial prompt by using LLM to\nparaphrase (denoted as Paraphrase) or in a rule-based manner (Rule-Based). Compared to RATTPO,\nthe Paraphrase baseline does not use history or hint. The Rule-Based baseline randomly appends\nmodifier words that are known to improve image aesthetics, similar to the heuristic used in Hao et al.\n(2023). Note that OPT2I is a reward-specific prompt engineer designed for text-to-image consistency\nscores, and thus the comparison is done only for the supported setup (DSG on PartiPrompt).\n\nImplementation Details We report averaged results across three runs with different seeds. Due\nto space constraints, we present the full results with standard deviations in App. F. We use the\ninstruction-tuned Gemma 3 27B (Team et al., 2025) for all LLM components. We follow either\nPAG (Yun et al., 2025) or OPT2I (Mañas et al., 2024) for diffusion sampling hyperparameters. We\nconduct an ablation study on the choice of different-sized LLMs and different hyperparameters,\nsuch as diffusion sampler or history construction strategy for the hint-generator LLM, in App. D. To\nreduce the computational burden, we select two representative setups in our additional experiments:\nPromptist Reward and UniDet2D. We provide more implementation details in App. A.\n\n\n                                       5\n\nUnder Review\n\n\n\n\n\n                                                  Ours (1.20)                                    Promptist (0.01)\n      Human Preference                                      Highly detailed digital painting of a                                dictator duck. highly detailed, digital painting,\n                Initial (-0.77)                                    duck dictator, golden epaulettes,                                  artstation, concept art, smooth, sharp focus,\n                                                                           resolute gaze, ArtStation.                                             illustration, art by artgerm and greg rutkowski\n                                                                                                                and alphonse mucha, 8 k\n\n\n\n                                                    Paraphrase (0.79)                              DPO-Diff (-0.42)\n                                                                             Hyperrealistic portrait of a duck                               Negative Prompt:\n                                                                                  dictator, wearing a military                                 democrat, swan\n               Dictator duck.                                        uniform and helmet, intense gaze,\n                                                                             8k.\n\n\n\n          Text-to-Image                            Ours (1.00)                                    Promptist (0.55)\n          Consistency                                          Highly detailed oil painting in the                       Masamune Shirow, raccoon wearing formal\n                Initial (0.67)                                               style of traditional Chinese masters:                            clothes, wearing a tophat, holding a cane,\n                                                               a formally dressed raccoon, tophat,                            holding a garbage bag. Oil painting, in the style\n                                                                     cane, holding a garbage bag—                                    of traditional Chinese. Trending on Artstation.\n                                                                   comical juxtaposition.                                           8k, UHD. Highly detailed. Art by (…)\n\n\n                                                    Paraphrase (0.78)                            OPT2I (0.78)\n                                                             Raccoon in formal wear, tophat,                                  Traditional Chinese painting, oil on canvas: a\n                                                                     cane, holding a garbage bag.                                    formally dressed raccoon with a top hat, cane,\n   A raccoon wearing formal clothes, wearing a                           Chinese oil painting.                                   and a garbage bag.\n     tophat and holding a cane. The raccoon is\n  holding a garbage bag. Oil painting in the style of\n             traditional Chinese painting.\n\n          LLMGrader                             Ours (92.0)                                    Promptist (51.0)\n                                                                          Miniature Japanese village,                                 Japanese village inside a snowball. By\n                Initial (46.0)                                     encased in a crystal snow globe.                          Makoto Shinkai, Stanley Artgerm Lau,\n                                                                   High detail.                                       WLOP, Rossdraws, James Jean,\n\n\n\n                                                    Paraphrase (90.0)                       PAG (50.0)\n                                                                                   Intricate snow globe featuring                            an anthropomorphic japanese village inside a\n                                                               a traditional Japanese village.                                 snowball. hyperdetailed, hyperrealistic, 8 k,\n         A Japanese village inside a                                                                                                     trending on artstation by wlop and greg\n                  snowball.                                                                                                         rutkowski and alphonse mucha and ross tran\n                                                                                                                and wlop and (…) fantasy art. hyper detailed. no\n\nFigure 2: Qualitative results on diverse setups. As a reward-agnostic prompt engineer, RATTPO can\nenhance user prompts with respect to a broad range of reward functions.\n\n4.2  MAIN RESULT\n\nEffectiveness of RATTPO for Diverse Rewards  As shown in Fig. 1 and Tab. 1, RATTPO\nconsistently improves the initial prompts, with performance gains scaling with the inference budget.\nAt an inference budget of 160 prompts, RATTPO achieves a significant improvement over the\ninitial scores across all rewards and datasets. The qualitative results in Fig. 2 also showcase that\nRATTPO-optimized prompts can generate both aesthetically pleasing and correctly composed images\ndepending on the target reward. Beyond human preference and image composition, RATTPO is also\ncapable of improving user prompts to generate desired images on complex rubrics like originality\nand emotional resonance, when optimized against the overall LLMGrader score (Tab. 1).\n\nAs a test-time gradient-free prompt engineer, RATTPO effectively handles both differentiable and\nnon-differentiable rewards. While previous learning-based methods and some test-time methods\n(e.g., DPO-Diff) often rely on gradient-based optimization, many real-world rewards, such as the\nLLMGrader, are non-differentiable. Taken together, these results indicate that RATTPO is a reward-\nagnostic yet effective automated prompt engineer, capable of enhancing user prompts for a wide\nrange of rewards.\n\nComparison to Learning-Based Baselines  With sufficient inference budgets (~100 prompts),\nFig. 1 shows that RATTPO can match the performance of learning-based methods on Promptist\nReward without requiring expensive reward-specific training (which requires ~4 GPU days in Yun\net al. (2025)). The result suggests a well-designed reward-agnostic prompt engineer can be as effective\nas its reward-specific counterparts, without the substantial cost of reward-specific training.\n\nWhen we directly apply learning-based methods to unseen rewards, we observe significant perfor-\nmance drops. For instance, the results for ImageReward (Fig. 1a) show that their performance is\nonly comparable to the simple Paraphrase baseline at the same inference cost, despite the correlation\n\n\n                                       6\n\nUnder Review\n\n\n\n\nTable 2: Results for the ablation study on hint.                Promptist Reward - Lexica              UniDet2D\n                                                                                                                                                        0.16\nAdd. Hist. denotes the variant using extra his-              0.00                                         0.15\ntory instead of hint, and PR denotes Promptist               -0.10                                         0.14\nReward. RATTPO outperforms w/o Hint vari-                                                           0.13\nants.                                                                                                       -0.20                                         0.12\n\n                                                                                                                                                        0.11\n  Method            PR    UniDet2D                     -0.30\n                                                                                                                                                        0.10\n  RATTPO              0.683     0.416                         0    25   50   75   100  125  150     0    25   50   75   100  125  150\n                                                                                        Ours       No Hint + Additional History       No Hint\n  w/o Hint              0.565     0.395\n                                              Figure 3: Per-iteration average reward plots for the  w/o Hint + Add. Hist.   0.522     0.387\n                                                   hint ablation study.\n\nbetween the trained and target rewards. The performance degradation is more severe in text-to-image\nconsistency setups, where the correlation is weaker. These results indicate that the performance\nof learning-based methods drops quickly as the test reward diverges from the trained reward. This\nhighlights the need for reward-agnostic prompt engineers: learning-based reward-specific prompt en-\ngineers require re-training for optimal performance, while reward-agnostic approaches like RATTPO\ncan handle diverse reward functions that are defined and deployed at test-time.\n\nComparison to Test-Time Search Baselines  As shown in the optimization curve in Fig. 1,\nRATTPO outperforms other test-time search methods (DPO-Diff, OPT2I, Paraphrase, and Rule-\nBased) in terms of search efficiency. With an inference budget of 160 generated prompts, RATTPO\nsurpasses all baselines across all experiment setups. When compared to the Rule-Based method,\nRATTPO shows a substantial performance advantage on ImageReward, while the advantage is\nsmaller for Promptist Reward. This difference arises from the Rule-Based method’s heuristics being\nspecifically tailored for the Aesthetic Score component used in Promptist Reward. Consequently, the\nRule-Based method performs only as well as simple paraphrasing on ImageReward, which highlights\nits limited generalizability across different reward. In contrast, RATTPO outperforms the Rule-Based\nmethod without relying on reward-specific heuristics, by progressively evolving prompt candidates\nusing in-context learning from its optimization history.\n\nWhen compared to OPT2I that employs an LLM as an optimizer with reward-specific meta-prompt\n(Fig. 1b), we observe that RATTPO achieves competitive scores at the early optimization stages but\nshows rapid improvement over time, leading to much higher reward at the end. We conjecture that the\nearly saturation of OPT2I is due to heuristics in its design, which may contain an implicit inductive\nbias towards local optima. In contrast, RATTPO saturates slowly and achieves higher rewards,\npossibly by replacing reward-specific task description with hints, which are generated automatically\nfrom the optimization history and thus evolve as the optimization proceeds.\n\nTo further analyze the search efficiency in practice, we  Table 3: Search efficiency of RATTPO con-\nmeasure wall-clock time speedup of RATTPO over  sidering wall-clock time.\nParaphrase, the reward-agnostic baseline.  Despite\nthe overhead from an additional hint-generator LLM,                     PR    UniDet2D\nRATTPO improves upon the Paraphrase baseline in    Search cost at win        24        24\nterms of search efficiency, achieving up to 6.46×    Time, Paraphrase        447s      300s\nwall-clock speedup as detailed in Tab. 3. On aver-   Time, RATTPO at win    69s       51s\nage, RATTPO is 4.81× faster in end-to-end wall-clock                                                    Speedup              6.46×    5.90×\ntime across our eight experimental setups, although\nspecific speedups vary. Full results are in Tab. 8.\n\n\n4.3  ANALYSIS\n\nEffectiveness of Hint  To validate our hint mechanism, we compare RATTPO against two variants:\none without hint and another replacing the hint generator with additional random history context.\nAs shown in Tab. 2, removing hints degrades performance on both rewards, and simply adding\nmore history alone does not compensate for this performance loss. Also, Fig. 3 depicts that variants\nwithout the hint achieve lower per-iteration average score. Based on the results, we claim that\nthe hint mechanism in RATTPO effectively guides the optimizer LLM to produce better prompts\nevery iteration, thereby improving the final reward. The performance gain from hint is not simply a\nconsequence of having more history in the context window, but rather the result of an well-designed\nmechanism for aggregating history to explicitly guide the search.\n\n\n                                       7\n\nUnder Review\n\n\n\n\nTable 4: Cross-reward experiment results. Underlined numbers    Table  5:  Inter-reward  hint\nindicate that RATTPO was not optimized for the evaluated reward.     transfer results.\n\n   Optimization Target             Promptist Reward   UniDet2D        Method     PR     IR\n\n    Initial (No Opt.)                      -0.311          0.159        RATTPO     0.683   1.132\n  RATTPO, optimize PR               1.021           0.164          w/o Hint     0.565   1.081\n  RATTPO, optimize UniDet2D        -0.017          0.461          w/ IR Hint   0.579       -\n                                                               w/ PR Hint       -     1.065\n\n   Initial Prompt                                           Initial Prompt\n  playing guitar                           (-0.648)      a couch on the left of a dog              (0.197)\n\n  Example Context for Hint-Generator LLM       Example Context for Hint-Generator LLM\n   Soulful, full-body portrait of a guitarist immersed    A detailed living room scene: a couch on the left,\n   in performance, highly detailed hands expertly      a dog on the right.                      (0.281)\n   playing, (...) emotive expression.         (0.172)    A comfortable couch is to the left of a happy dog.\n  Young adult playing electric guitar, stage lights,       (0.0)\n  dynamic pose.                          (-0.660)\n                                               Generated Hints\n  Generated Hints                                Focus on detailed descriptions of the living room,\n  Focus  on  full-body shots,   (...)    and  con-      couch, and dog (breed,  color, texture) while\n   sistently  include  \"intricate hand positions\"  or      maintaining clear left/right positioning and em-\n   \"detailed hands on fretboard\" alongside (...)           phasizing realism/quality.\n\n  Example Output from Optimizer LLM          Example Output from Optimizer LLM\n   Passionate guitarist, full-body, intricate hand po-      Realistic living room: a linen sectional couch\n   sitions, Artgerm, WLOP, (...).            (0.335)        (left) and a relaxed, cream-colored Labrador re-\n A 26-year-old guitarist, full-body, intensely pas-       triever (right), soft lighting.             (0.303)\n   sionate performance,  detailed hands, Artgerm,     Cozy living room, left: a velvet teal couch, right:\n  WLOP, dramatic lighting, 8k.           (-0.148)      a golden retriever.                       (0.229)\n\n      (a) Human Preference (Promptist Reward)             (b) Text-to-Image Consistency (UniDet2D)\nFigure 4: Case study of generated hints. Numbers in parentheses indicate reward for corresponding\nprompts. The Hint-generator LLM summarizes the search history to generate a \"hint\" that instructs\nthe optimizer LLM. We highlight the relevant parts and omit (...) some words for better presentation.\n\nTo further evaluate whether hint contains reward-specific optimization signal, we design an inter-\nreward hint transfer experiment. Specifically, we consider variants that use pre-generated hints from\nother reward setup instead of generating hints during optimization. For instance, w/IR Hint variant\noptimizes for Promptist Reward using hints generated by RATTPO during ImageReward optimization.\nSince hint is generated per-prompt, we consider the transfer within the same dataset (Lexica). As\nshown in Tab. 5, these variants show degraded performance, and do not meaningfully improve upon\nRATTPO without the hint. Thus, we claim that hints indeed contain reward-specific information,\nthanks to our explicit design that queries the hint-generator LLM about optimization strategy.\n\nLastly, we conduct a case study on generated hints, exploiting their human interpretability (Fig. 4,\nsee App. H for more examples). As can be seen, the hint-generator LLM is capable of recognizing\nhigh-scoring patterns and summarizing them as textual descriptions. For instance, highlighted parts\nin Fig. 4a shows that the hint-generator LLM instructs the optimizer LLM to focus on \"full-body\nportrait\" and \"detailed hand positions\", which can be inferred by comparing high-scoring prompts\nwith low-scoring prompts. Also, by comparing the generated hints for different rewards, we observe\nthat the hint-generator can capture the information about the underlying reward function. The hint-\ngenerator instructs the optimizer LLM to add fine-grained details for improving image aesthetics\n(Fig. 4a), and to explicitly mention both ‘left’ and ‘right’ keywords for better drawing images with\nspatial object relationships (Fig. 4b). Hints are not only an effective way to capture reward-aware\noptimization signals, but they also provide a means to analyze the optimization process.\n\nCross-Reward Evaluation  While we extensively verify the effectiveness of RATTPO in optimizing\ntarget rewards, it would be undesirable if the improved reward comes at the cost of overall image\nquality. To validate that RATTPO does not fall into such an over-optimization scenario, we conduct\na cross-reward experiment. Specifically, we optimize prompts for human preference (Promptist\nReward) and evaluate them on an uncorrelated reward for spatial composition (UniDet2D), and vice\nversa. In Tab. 4, we observe that the non-optimized rewards (underlined) remain at a similar level\nor even slightly improved compared to their initial scores. This result suggests that RATTPO does\n\n\n                                       8\n\nUnder Review\n\n\n\n\nTable 6: Experimental results with various diffusion backbones. Asterisk denotes the baselines trained\nfor Promptist Reward. Test-time search methods are evaluated with a 160-prompt budget.\n\n                     Human Preference                     Text–to-Image Consistency\n\n   Method     SD1.4   SD2.1   SDXL-Turbo  FLUX   SD1.4   SD2.1  SDXL-Turbo  FLUX\n\n     Initial        -0.325   -0.373      -0.327      -0.480   0.123    0.133       0.162       0.240\n    Promptist*   0.591    0.345       0.257       0.104    0.255    0.273       0.322       0.413\n   PAG*        0.545    0.219       0.193       -0.013   0.275    0.326       0.327       0.439\n   DPO-Diff    0.066   -0.037            -               -           -           -               -               -\n    Paraphrase   0.372    0.261       0.415       0.138    0.384    0.316       0.419       0.536\n\n   Ours        0.683    0.503       0.487       0.445    0.396    0.416       0.454       0.578\n\n\nTable 7: Results of RATTPO combined\nwith test-time alignment (DAS (Kim et al.,\n2025)). PR denotes Promptist Reward.\n\n\n       Method          PR\n\n           Initial               -0.325                                                           (a) Initial           (b) DAS      (c) DAS+RATTPO\n      +DAS              0.411\n      +DAS + RATTPO   1.006          Figure 5: Qualitative examples of combining RATTPO\n                                        with DAS (Kim et al., 2025). Prompts are in App. E\n\nnot over-optimize prompts for the target reward. We hypothesize that RATTPO is robust to such\nover-optimization for two main reasons. First, our optimization is performed in a discrete text space\nwithout gradients, which may act as an implicit form of regularization. Second, we explicitly instruct\nthe LLM to restrict its search space to prompts that preserve the semantics of the initial prompt. This\nconstraint makes over-optimization scenarios with degraded faithfulness less feasible.\n\nRobustness to Diffusion Backbone  As a test-time search method, RATTPO is also agnostic to the\nchoice of T2I models. To validate this claim, we experiment with various T2I diffusion backbones\nand report the results in Tab. 6. Specifically, we consider SDXL-Turbo (Sauer et al., 2024) and\nFLUX.1 Schnell (Black Forest Labs, 2024) in addition to Stable Diffusion 1.4 and 2.1 that are\nused in our main experiments. The results show that RATTPO is robust to the choice of diffusion\nbackbone, consistently improving the initial prompt by a large margin. We also note that not all\ntest-time methods are agnostic to diffusion backbone, exemplified by DPO-Diff (Wang et al., 2024).\nDPO-Diff optimizes a negative prompt for improving image, where negative prompts are often not\nused in timestep-distilled models (SDXL-Turbo and FLUX.1 Schnell). In contrast, RATTPO directly\noptimizes the user prompt and is applicable to these models.\n\nRATTPO with Test-Time Alignment  Since RATTPO is agnostic to denoising process, it is\ncompatible with test-time alignment methods that tweak sampling steps. For instance, RATTPO can\nbe combined with DAS (Kim et al., 2025), a sampling-as-alignment approach that effectively aligns\na diffusion model without training. We report the experimental results on Promptist Reward in the\nLexica dataset in Tab. 7 and Fig. 5. The results demonstrate that our automated prompt engineer can\nbe easily combined with other test-time alignment methods to generate images with higher rewards.\n\n\n5  CONCLUSION\n\nWe introduce RATTPO, a reward-agnostic prompt optimization framework that can be applied to\ndiverse reward functions without task-specific modifications. Our approach employs an iterative opti-\nmization process using two complementary LLMs: the optimizer LLM to propose enhanced prompts\nand the hint-generator LLM to provide contextual feedback based on reward signals. RATTPO is a\nreward-agnostic yet effective prompt engineer, thanks to replacing the reward-specific task description\nin the meta-prompt with a hint that is automatically generated by an LLM on-the-fly. We validate the\neffectiveness of RATTPO in various reward setups, categorized by human preference, text-to-image\nconsistency, and holistic MLLM assessment. Experimental results demonstrate that RATTPO is\napplicable to a wide range of rewards and surpasses other test-time approaches in terms of search\nefficiency.\n\n\n                                       9\n\nUnder Review\n\n\n\n\n\nAcknowledgments  This work was in part supported by the National Research Foundation of\nKorea (RS-2024-00351212 and RS-2024-00436165) and the Institute of Information & communica-\ntions Technology Planning & Evaluation (IITP) (RS-2022-II220926, RS-2024-00509279, RS-2021-\nII212068, RS-2022-II220959, and RS-2019-II190075) funded by the Korea government (MSIT).\n\nETHICS STATEMENT\n\nWe have carefully reviewed the Code of Ethics and confirm that we adhere to the principles. To the\nbest of our knowledge, this work raises no ethical concerns. The use of LLMs in this paper is clarified\nin App. I.\n\nREPRODUCIBILITY STATEMENT\n\nWe have made our best efforts to ensure the reproducibility of our experiments. We include the code\nin our submission to enable others to replicate our results. We report all implementation details\n(App. A) for the experiments, including meta-prompts used for LLMs (App. B). We expect the\nnumbers in main results to be reproducible, as we report the average of three different runs. Except\nthe LLMGrader experiment, we use publicly available datasets and open-sourced models. For dataset,\nexact lists/splits and preprocessing scripts for the datasets are included in our attached code.\n\n\n\n\n\n                                       10\n\nUnder Review\n\n\n\n\nREFERENCES\n\nBlack Forest Labs. Announcing Black Forest Labs. Blog post on Black Forest Labs website, August\n  2024. URL https://blackforestlabs.ai/announcing-black-forest-labs/.\n  Accessed: 2025-05-14.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n  Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n  few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nTingfeng Cao, Chengyu Wang, Bingyan Liu, Ziheng Wu, Jinhui Zhu, and Jun Huang. Beautiful-\n  prompt: Towards automatic prompt engineering for text-to-image synthesis. In Proceedings of\n  the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp.\n  1–11, 2023.\n\nYixiong Chen, Li Liu, and Chris Ding. X-iqe: explainable image quality evaluation for text-to-image\n  generation with visual large language models. arXiv preprint arXiv:2305.10843, 2023.\n\nJaemin Cho, Yushi Hu, Jason M Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit\n  Bansal, Jordi Pont-Tuset, and Su Wang.  Davidsonian scene graph: Improving reliability in\n  fine-grained evaluation for text-to-image generation. In ICLR, 2024.\n\nMohamad Diab, Julian Herrera, Bob Chernow, and Coco Mao. Stable diffusion prompt book, 2022.\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu,\n  Zhiyong Wu, Tianyu Liu, et al. A survey on in-context learning. arXiv preprint arXiv:2301.00234,\n  2022.\n\nYingjun Du, Wenfang Sun, and Cees Snoek. Ipo: Interpretable prompt optimization for vision-\n  language models.  Advances in Neural Information Processing Systems, 37:126725–126766,\n  2024.\n\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rock-\n   täschel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint\n  arXiv:2309.16797, 2023.\n\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation.\n  Advances in Neural Information Processing Systems, 36:66923–66939, 2023.\n\nYutong He, Alexander Robey, Naoki Murata, Yiding Jiang, Joshua Nathaniel Williams, George J\n  Pappas, Hamed Hassani, Yuki Mitsufuji, Ruslan Salakhutdinov, and J Zico Kolter.  Auto-\n  mated black-box prompt engineering for personalized text-to-image generation. arXiv preprint\n  arXiv:2403.19103, 2024.\n\nYushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A\n  Smith.  Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question\n  answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n  20406–20417, 2023.\n\nKaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench++:\n  An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE\n  Transactions on Pattern Analysis and Machine Intelligence, 2025.\n\nSunwoo Kim, Minkyu Kim, and Dongmin Park. Test-time alignment of diffusion models without\n  reward over-optimization. In The Thirteenth International Conference on Learning Representations,\n  2025.\n\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-\n   a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural\n  Information Processing Systems, 36:36652–36663, 2023.\n\nLexica. The state of the art ai image generation engine. https://lexica.art/, 2023. Accessed:\n  2024-11-15.\n\n\n                                       11\n\nUnder Review\n\n\n\n\n\nLuping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on\n  manifolds. In International Conference on Learning Representations, 2022.\n\nTennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela van der Schaar.  Large language\n  models to enhance bayesian optimization. In The Twelfth International Conference on Learning\n  Representations, 2023.\n\nVivian Liu and Lydia B Chilton. Design guidelines for prompt engineering text-to-image generative\n  models. In Proceedings of the 2022 CHI conference on human factors in computing systems, pp.\n  1–23, 2022.\n\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast\n  ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural\n  Information Processing Systems, 35:5775–5787, 2022.\n\nNanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang,\n  Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond\n  scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025.\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\n  Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\n  with self-feedback. Advances in Neural Information Processing Systems, 36:46534–46594, 2023.\n\nShweta Mahajan, Tanzila Rahman, Kwang Moo Yi, and Leonid Sigal. Prompting hard or hardly\n  prompting: Prompt inversion for text-to-image diffusion models. In Proceedings of the IEEE/CVF\n  Conference on Computer Vision and Pattern Recognition, pp. 6808–6817, 2024.\n\nOscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya\n  Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving text-to-image consistency\n  via automatic prompt optimization. CoRR, 2024.\n\nWenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, and Qing Yang. Dynamic prompt\n  optimizing for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition, pp. 26627–26636, 2024.\n\nJonas Oppenlaender. A taxonomy of prompt modifiers for text-to-image generation. Behaviour &\n  Information Technology, 43(15):3763–3776, 2024.\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\n  Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\n   instructions with human feedback. Advances in neural information processing systems, 35:27730–\n  27744, 2022.\n\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe\n  Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\n   synthesis. In The Twelfth International Conference on Learning Representations, 2024.\n\nYiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng\n  Wu, Fei Liu, Pengfei Liu, and Dong Yu. InFoBench: Evaluating instruction following ability in\n  large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of\n  the Association for Computational Linguistics: ACL 2024, pp. 13025–13048, Bangkok, Thailand,\n  August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.772.\n  URL https://aclanthology.org/2024.findings-acl.772/.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n  Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n  models from natural language supervision. In International conference on machine learning, pp.\n  8748–8763. PmLR, 2021.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n  conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\n\n                                       12\n\nUnder Review\n\n\n\n\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n   resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\n  ence on computer vision and pattern recognition, pp. 10684–10695, 2022.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\n  Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\n  Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V.\n  Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,\n  Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,\n  Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan,\n  Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask\n  prompted training enables zero-shot task generalization. In ICLR. OpenReview.net, 2022.\n\nAxel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach.  Adversarial diffusion\n   distillation. In European Conference on Computer Vision, pp. 87–103. Springer, 2024.\n\nChristoph  Schuhmann.     Improved  aesthetic  predictor.    https://github.com/\n  christophschuhmann/improved-aesthetic-predictor,   2022.      Accessed:\n  2025-05-13.\n\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\n  Language agents with verbal reinforcement learning. Advances in Neural Information Processing\n  Systems, 36:8634–8652, 2023.\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR.\n  OpenReview.net, 2021.\n\nJiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles\n  Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning text-to-image\n  generation with image understanding feedback. In Synthetic Data for Computer Vision Workshop@\n  CVPR 2024, 2024.\n\nGemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett\n  Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal\n  understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\n\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej,\n  Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical\n   report. arXiv preprint arXiv:2503.19786, 2025.\n\nRuochen Wang, Ting Liu, Cho-Jui Hsieh, and Boqing Gong. On discrete prompt optimization for\n  diffusion models. In Proceedings of the 41st International Conference on Machine Learning, pp.\n  50992–51011, 2024.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\n  Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\n  models. In The Eleventh International Conference on Learning Representations, 2023a.\n\nZijie Wang, Evan Montoya, David Munechka, Haoyang Yang, Benjamin Hoover, and Polo Chau.\n  Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models. In Annual\n  Meeting of the Association for Computational Linguistics, 2023b.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\n  Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\n  neural information processing systems, 35:24824–24837, 2022.\n\nXiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score:\n  Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF\n  International Conference on Computer Vision, pp. 2096–2105, 2023.\n\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.\n  Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances\n   in Neural Information Processing Systems, 36:15903–15935, 2023.\n\n\n                                       13\n\nUnder Review\n\n\n\n\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\n  Chen. Large language models as optimizers. In The Twelfth International Conference on Learning\n  Representations, 2024.\n\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\n  Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\n   rich text-to-image generation. Trans. Mach. Learn. Res., 2022.\n\nTaeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and\n  diverse prompts for text-to-image generation. arXiv preprint arXiv:2502.11477, 2025.\n\nMichael R Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, and Jimmy Ba. Using large language\n  models for hyperparameter optimization. In NeurIPS 2023 Foundation Models for Decision Making\n  Workshop, 2023.\n\nXingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Simple multi-dataset detection. In Proceed-\n  ings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7571–7580,\n  2022.\n\n\n\n\n\n                                       14\n\nUnder Review\n\n\n\n\nAPPENDIX\n\nA  IMPLEMENTATION DETAILS\n\nSampling from Diffusion Backbone  Following PAG (Yun et al., 2025), we use Stable Diffusion\n1.4 (Rombach et al., 2022) with DPM solver (Lu et al., 2022) as a default diffusion sampling setup.\nThe inference is done in half precision (fp16). For text-to-image consistency reward experiments, we\nfollow OPT2I (Mañas et al., 2024) and use Stable Diffusion 2.1 (Rombach et al., 2022) as a diffusion\nbackbone. We set the number of inference steps to 20 by default. Furthermore, for the diffusion\nbackbone robustness experiment, SDXL-Turbo (Sauer et al., 2024) and FLUX.1 Schnell (Black Forest\nLabs, 2024) are timestep distilled models, and thus we use the PNDM (Liu et al., 2022) solver and\nEuler solver with the number of inference steps set to one and four, respectively. For the experiment\ncombining RATTPO with test-time alignment method, DAS (Kim et al., 2025), we use DDIM (Song\net al., 2021) solver, as the official implementation1 is provided only for DDIM sampler. We also\nevaluate RATTPO with various diffusion samplers (Tab. 11) and find that it is robust to different\nsampler choices.\n\n\nHyperparameters  For RATTPO, we perform 20 iterations where each iteration proposes 8 prompts.\nWe select the 8 best prompts from the history for the context of the optimizer LLM, and 20 random\nprompts for the hint-generator LLM. We use default hyperparameters when querying the LLM. For\nbaselines, we use the default setup if possible. To match the inference cost (i.e., 160 generated\nprompts), we increase the length of the evolutionary search phase in DPO-Diff, resulting in 20\niterations of gradient-based optimization and 140 iterations of evolutionary search. The evolutionary\nsearch phase may terminate early if the search space contains fewer than 140 candidates. For\nOPT2I (Mañas et al., 2024), we increase the iteration, resulting in 32 optimization iterations where\neach iteration generates 5 prompts. For the experiment combining RATTPO with DAS (Kim et al.,\n2025), the hyperparameters for DAS are set to default (γ = 0.008, α = 0.005).\n\n\nRewards  In our experiment, we consider various rewards that can be categorized into (1) Human\nPreference Reward (2) Text-Image Consistency Reward (3) Holistic MLLM Assessment. Except\nfor the LLMGrader used for the last category, all other models are publicly available. We employ\nwidely adopted reward models for each category, namely Promptist Reward (Hao et al., 2023),\nImageReward (Xu et al., 2023), DSG Score (Cho et al., 2024), and UniDet (Zhou et al., 2022) scorer\nfrom T2I-Compbench (Huang et al., 2025) benchmark. For LLMGrader, we follow the setup in Ma\net al. (2025); the prompt used for querying LLM is shown in Fig. 6. When calculating rewards, we\nfollow Yun et al. (2025) to estimate the expectation in Eq. 1 by averaging rewards over generated\nimages from three different initial diffusion noise samples. The initial noises are shared across\nmethods and iterations.\n\n\nDatasets  For the human preference and LLMGrader experiments, we directly use the evaluation\nsplits of Lexica (Lexica, 2023) and DiffusionDB (Wang et al., 2023b) used in PAG (Yun et al., 2025).\nThe datasets consist of 64 and 256 preprocessed prompts, respectively, obtained by extracting the main\ncontent from human-engineered prompts. For PartiPrompt (Yu et al., 2022) dataset used for DSG (Cho\net al., 2024) reward setup, we prepare the dataset following (Mañas et al., 2024). Specifically, we use\nthe first 50 prompts from four categories (\"Properties and Positioning”, “Quantity”, “Fine-grained\nDetail”, “Complex”). As the category \"Properties and Positioning\" contains only 35 prompts, the\nresulting dataset has 185 initial prompts in total.  Lastly, for UniDet-based evaluators, we use\naccompanying datasets used in T2I-Compbench++ (Huang et al., 2025) benchmark. Each validation\nset for 2D, 3D and Numeracy category includes 300 prompts, and we use subset of 100 prompts in\neach category.\n\n\nBaselines  We provide some additional details for baselines used in our experiments.\n\n• DPO-Diff (Wang et al., 2024) baseline in our experiment uses the default setting recommended\n  in the original paper. Specifically, we optimize negative prompts with hybrid (gradient-based\n\n    1https://github.com/krafton-ai/DAS\n\n\n                                       15\n\nUnder Review\n\n\n\n\n\n  You are a multimodal large-language model tasked with evaluating images generated by a text-to-image\n  model.\n  Your goal is to assess each generated image based on specific aspects and provide a detailed critique, along\n  with a scoring system.\n  The final output should be formatted as a JSON object containing individual scores for each aspect and an\n   overall score.\n  Below is a comprehensive guide to follow in your evaluation process:\n   1. Key Evaluation Aspects and Scoring Criteria:\n  For each aspect, provide a score from 0 to 100, where 0 represents poor performance and 100 represents\n   excellent performance.\n  For each score, include a short explanation or justification (1-2 sentences) explaining why that score was\n   given.\n  The aspects to evaluate are as follows:\n   a) Accuracy to Prompt\n  Assess how well the image matches the description given in the prompt.\n  Consider whether all requested elements are present and if the scene, objects, and setting align accurately\n  with the text.\n  Score: 0 (no alignment) to 100 (perfect match to prompt).\n   Creativity and Originality\n  Evaluate the uniqueness and creativity of the generated image.\n  Does the model present an imaginative or aesthetically engaging interpretation of the prompt?\n   Is there any evidence of creativity beyond a literal interpretation?\n  Score: 0 (lacks creativity) to 100 (highly creative and original).\n   c) Visual Quality and Realism\n  Assess the overall visual quality, including resolution, detail, and realism.\n  Look for coherence in lighting, shading, and perspective.\n  Even if the image is stylized or abstract, judge whether the visual elements are well-rendered and visually\n  appealing.\n  Score: 0 (poor quality) to 100 (high-quality and realistic).\n  d) Consistency and Cohesion\n  Check for internal consistency within the image.\n  Are all elements cohesive and aligned with the prompt?\n  For instance, does the perspective make sense, and do objects fit naturally within the scene without visual\n  anomalies?\n  Score: 0 (inconsistent) to 100 (fully cohesive and consistent).\n   e) Emotional or Thematic Resonance\n  Evaluate how well the image evokes the intended emotional or thematic tone of the prompt.\n  For example, if the prompt is meant to be serene, does the image convey calmness?\n   If it’s adventurous, does it evoke excitement?\n  Score: 0 (no resonance) to 100 (strong resonance with the prompt’s theme).\n   2. Overall Score\n  After scoring each aspect individually, provide an overall score, representing the model’s general perfor-\n  mance on this image.\n  This should be a weighted average based on the importance of each aspect to the prompt or an average of\n   all aspects.Now grade the image based on the above criteria.\n  Below is the prompt:\n  Prompt: {prompt}\n\n\nFigure 6: The prompt used for LLMGrader reward. We follow the prompt used in Ma et al. (2025).\n\n\n\n  optimization with evolutionary search) search. Note that this setup is not applicable to both (1) non-\n  differentiable rewards in text-to-image consistency and MLLM score, and (2) diffusion backbones\n  like SDXL-Turbo (Sauer et al., 2024) or FLUX.1 Schnell (Black Forest Labs, 2024) that do not use\n  negative prompt. We exclude DPO-Diff baseline for such setups.\n\n• OPT2I (Mañas et al., 2024) baseline in our experiment is implemented by using the exact hyperpa-\n  rameters and meta-prompt following the original paper. We use the same LLM (Gemma 3 27B)\n  as RATTPO for fair comparison. While the paper claims that OPT2I can be used for arbitrary\n  text-to-image consistency score, it uses different meta-prompts for each choice of the score. Among\n  the experimental setups used in our paper, the DSG reward on PartiPrompt is the only setup for\n  which the meta-prompt for OPT2I is provided, and thus we include OPT2I as a baseline only in that\n  setup. As Gemma 3 does not use a system role, we include the system prompt in the user prompt.\n\n\n                                       16\n\nUnder Review\n\n\n\n\n\n  As an expert prompt engineer for text-to-image generation, rewrite the original prompt in 8 distinct ways to\n  improve the visual quality of the resulting images.\n  To aid you in this task, you will be also given 8 history prompts that are already tried before. For each\n   history prompt, its score is given as number. Higher score indicates that the prompt is better.\n   (Hint: {hint})\n  You can use the scores to guide your rewriting process and thus improve the visual quality of the generated\n  image, but your response should be different from histories.\n   Histories:\n   1. Prompt: {prompt_1} (Score: {score_1})\n   2. Prompt: {prompt_2} (Score: {score_2})\n    (...)\n   8. Prompt: {prompt_8} (Score: {score_8})\n  Return exactly 8 variations, numbered 1 through 8, each on its own line and ordered from shortest to\n   longest.\n  Preserve the meaning of the original prompt and keep each variation under 70 words. Start your output\n  immediately with the numbered variations.\n   Original Prompt: {initial_prompt}\n\n\nFigure 7: Meta-prompt used for the optimizer LLM. We highlight the history-related part and hint-\nrelated part in colored texts.\n\n\n\n• The Rule-Based baseline is constructed in a similar way to the heuristic baseline used in\n  Promptist (Hao et al., 2023).  Specifically, we collect the top 15 most frequent words ap-\n  pearing in human-engineered prompts, and randomly append three of them to the initial\n  prompt. The word pool consists of: \"concept art\", \"highly detailed\", \"sharp\n  focus\", \"artstation\", \"digital painting\", \"intricate\", \"illustration\",\n \"trending on artstation\",   \"smooth\",   \"elegant\",  \"octane render\",\n  \"fantasy\", \"wlop\", \"digital art\", and \"8 k\".  As  this  heuristic  rule  is  tar-\n  geted for enhancing image aesthetics, we only include it for experiments that contain human\n  preference.\n\n\nComputational Resources  We conduct the experiments with our internal GPU servers that consist\nof two types of machines. We list their specifications below.\n\n      1. Intel Xeon Gold 6330 CPU and NVIDIA RTX A6000 GPU (with 48GB VRAM)\n\n      2. Intel Xeon Gold 6230 CPU and NVIDIA RTX 3090 GPU (with 24GB VRAM)\n\nFor LLM components, we either use (1) API provided by Google Generative AI service2\n(gemma-3-27b-it) or (2) self-hosted LLM server constructed with ollama3 (gemma3:27b)\nfor using Gemma. When implementing LLMGrader with Gemini-1.5-Flash (Team et al., 2024),\nwe use the official API4.\n\nTotal running time depends on the GPU machine and also LLM API response time. When using\nthe machine with A6000 GPU and using Google GenAI API for LLM inference, we expect a single\noptimization (20 iterations for one initial prompt) to take about 10 minutes.\n\n\nB  META-PROMPT\n\nMeta-prompts used for querying the optimizer LLM and the hint-generator LLM can be found\nin Fig. 7 and Fig. 8, respectively. Note that history and hint are not available for the first search\niteration in RATTPO. In this case, the meta-prompt for the optimizer LLM reduces to the prompt for\nParaphrase baseline, which uses neither optimization history nor hint. This corresponds to a prompt\nthat consists of uncolored texts in Fig. 7.\n\n   2https://ai.google.dev/gemma/docs/core/gemma_on_gemini_api\n   3https://ollama.com/library/gemma3\n    4https://ai.google.dev/gemini-api/docs/models#gemini-1.5-flash\n\n\n                                       17\n\nUnder Review\n\n\n\n\n\n  As an expert prompt engineer for text-to-image generation, you are trying to rewrite the original prompt to\n  improve the scores. Below are some histories of prompts you tried before. Based on the history prompts\n  with corresponding scores, guess how you can enhance the score.\n   Histories:\n  Prompt: {prompt_1} (Score: {score_1})\n  Prompt: {prompt_2} (Score: {score_2})\n    (...)\n  Prompt: {prompt_20} (Score: {score_20})\n\n  Now describe the way how we can increase the score in plain words. Simply output the way in a single line.\n\n\n                    Figure 8: Meta-prompt used for the hint-generator LLM.\n\n     Table 8: Wall-clock time analysis of search efficiency, compared to Paraphrase baseline\n\n\n                                     Lexica     DiffusionDB   Parti      UniDet (reward)\n\n                         PR   IR   PR    IR   DSG   2D   3D   Numeracy\n\n Average # of prompts at win          24    40    24     32     40     24    40       32\n Wall clock time, Paraphrase (s)       447   383   410    355    1151   300   328      310\n Wall clock time, RATTPO at win (s)   69    106    62     82     300    51    94       74\n\n Speedup (×)                         6.46   3.62   6.66    4.34    3.84    5.90   3.48      4.20\n\n           Table 9: Ablation study on LLM capacity. Bold denotes our default setting.\n\n\n                       Method          Promptist Reward   UniDet2D\n\n                                  Initial              -0.325 ±0.013     0.133 ±0.007\n                  Gemma 3 1B       0.558 ±0.019     0.401 ±0.002\n                  Gemma 3 12B      0.563 ±0.009     0.415 ±0.011\n                 Gemma 3 27B     0.683 ±0.090     0.416 ±0.015\n\n\nC  SEARCH EFFICIENCY OF RATTPO\n\nFor a practical analysis of search efficiency, we measure the wall-clock time it takes RATTPO to\nachieve the peak score of the Paraphrase baseline. We extend the analysis in Tab. 3 by averaging the\nresults across all eight experimental setups. As detailed in Tab. 8, RATTPO achieves a net wall-clock\nspeedup of 4.81× over the Paraphrase baseline on average.\n\n\nD  ADDITIONAL EXPERIMENT RESULTS\n\nAblation on LLM Size  RATTPO relies on the ICL ability of LLMs for prompt optimization, where\nlarger LLMs are often better at (Brown et al., 2020). To see how the optimization performance\nchanges with the choice of different-sized LLM, we experiment with 1B, 12B and 27B variants of\nGemma 3 (Team et al., 2025). Results in Tab. 9 shows that RATTPO, even with the smallest size 1B\nvariant, can yield meaningful improvements over the initial prompt. We use 27B model as default\nsetting, which works the best in both settings.\n\nAblation on Hint-Generator LLM Context  For the hint-generator LLM, we sample 20 random\nhistory prompts and scores to use for its context. Tab. 10 shows an ablation study for this choice. For\nthe history selection strategy, we evaluate three methods: (1) selecting random histories, (2) using a\nmix of the best and worst histories, and (3) selecting only the best histories. For each method, we also\nexperiment with varying the number of histories provided as context. As can be seen, our default\nsetting performs the best in both setups.\n\nAblation on Diffusion Sampler  As a test-time optimization method, RATTPO is robust to different\ndiffusion samplers. To empirically support this claim, we additionally evaluate the performance of\n\n\n                                       18\n\nUnder Review\n\n\n\n\n       Table 10: Ablation study on hint history selection. Bold denotes our default setting.\n\n\n                          Selection Strategy    Promptist Reward   UniDet2D\n\n                  Random              0.683 ±0.090     0.416 ±0.015\n                         Best                  0.614 ±0.045     0.411 ±0.013\n\n                    Number of History   Promptist Reward   UniDet2D\n\n                      0 (w/o hint)            0.565 ±0.033     0.395 ±0.038\n                      4                     0.674 ±0.030     0.409 ±0.006\n                      20                    0.683 ±0.090     0.416 ±0.015\n                          All                    0.557 ±0.023     0.406 ±0.002\n\n\nTable 11: Experiment results with various diffusion samplers. Asterisk denotes learning baselines\ntrained for Promptist Reward. Test-time search methods are evaluated with a 160-prompt budget.\n\n\n                                PR+Lexica              UniDet2D\n\n               Sampler   DPM  DDIM  PNDM  DPM  DDIM  PNDM\n\n                       Initial        -0.308   -0.339    -0.404   0.125    0.116    0.101\n                  Promptist*   0.540    0.571    0.518    0.279    0.272    0.272\n             PAG*        0.582    0.525    0.436    0.267    0.261    0.238\n                  Paraphrase   0.374    0.509    0.473    0.325    0.389    0.369\n\n              Ours        0.662    0.644    0.594    0.429    0.430    0.382\n\nTable 12: Full results for human preference reward experiments. Asterisk denotes learning baselines\ntrained for Promptist Reward. Test-time search methods are evaluated at the budget of 160 generated\nprompts.\n\n\n                                   Promptist Reward             ImageReward\n\n             Method         Lexica      DiffusionDB      Lexica     DiffusionDB\n\n                    Initial        -0.325 ±0.013   -0.375 ±0.001   0.049 ±0.143   0.052 ±0.096\n                Promptist*   0.591 ±0.034    0.609 ±0.020   0.714 ±0.114   0.686 ±0.108\n            PAG*        0.545 ±0.014    0.581 ±0.002   0.657 ±0.144    0.623±0.103\n              DPO-Diff    0.066 ±0.009    0.070 ±0.006   0.783 ±0.024   0.775 ±0.060\n                Paraphrase   0.372 ±0.004    0.365 ±0.013   0.880 ±0.170   0.850 ±0.062\n\n             Ours         0.683±0.090    0.663±0.020    1.132±0.049    1.121±0.036\n\n\n\nRATTPO using two different samplers implemented in the diffusers library (DDIM and PNDM).\nAs shown in Tab. 11, our method is robust to the choice of diffusion sampler, effectively improving\nuser prompts across various sampler configurations. As a test-time reward-agnostic prompt engineer,\nRATTPO is compatible with both a broad set of rewards and diverse generation setups, including\ndifferent diffusion models (Tab. 6) and samplers (Tab. 11).\n\n\nE  FULL PROMPTS IN DAS EXPERIMENT\n\nFor the qualitative sample in Fig. 5, we use a prompt from Lexica dataset. The initial prompt is \"Duck\nand dog crossbreed\", and the RATTPO-optimized prompt is \"Captivating digital art: a duck-dog\nhybrid with flowing fur transitioning to feathers, bathed in golden light, emphasizing an intelligent,\ninquisitive gaze\".\n\n\nF  FULL MAIN RESULTS TABLE WITH STANDARD DEVIATION\n\nAs we omit the standard deviations in main text (due to spatial constraint), we present the full results\nhere. See Tab. 12-15.\n\n\n                                       19\n\nUnder Review\n\n\n\n\nTable 13: Full results for text-to-image consistency reward experiments. Asterisk denotes learning\nbaselines trained for Promptist Reward. Test-time search methods are evaluated at the budget of 160\ngenerated prompts.\n\n\n                      DSG                     UniDet\n\n              Method           Parti        2D        3D       Numeracy\n\n                      Initial       0.625 ±0.013   0.133 ±0.007   0.324 ±0.005   0.481 ±0.004\n                 Promptist*   0.743 ±0.010   0.273 ±0.008   0.405±0.013    0.555±0.007\n            PAG*       0.693 ±0.005   0.269 ±0.002   0.397±0.007    0.569±0.004\n                 Paraphrase   0.791 ±0.007   0.316 ±0.008   0.465 ±0.009   0.666±0.004\n\n              Ours        0.842 ±0.005   0.416 ±0.015   0.539 ±0.009   0.741 ±0.006\n\n\nTable 14: Full results for LLMGrader reward experiments in Lexica dataset. Asterisk denotes learning\nbaselines trained for Promptist Reward. Test-time search methods are evaluated at the budget of 160\ngenerated prompts.\n\n\n      Method      Accuracy    Originality     Visual     Consistency   Emotional    Overall\n\n         Initial        67.8 ±1.12    62.8±1.52    80.1±0.41     83.9±0.66     69.6±1.39    72.5±0.85\n       Promptist*    60.5 ±1.94   64.7 ±1.29   85.3 ±0.52    85.9 ±0.18    68.9 ±0.85   85.8 ±1.57\n     PAG*        57.7 ±2.03   64.2 ±2.46   84.7 ±0.96    84.8 ±0.39    68.2 ±1.56   85.2 ±1.61\n      Rule-Based   68.4 ±2.21   66.0 ±2.68   83.8 ±1.97    85.4 ±1.56    71.5 ±2.61   88.4 ±0.87\n       Paraphrase    69.7 ±2.68   67.2 ±1.24   83.9 ±0.70    86.4 ±1.23    73.9 ±2.22   89.0 ±0.44\n\n      Ours         75.1±2.49    72.4 ±1.52   86.0 ±0.65    88.4 ±0.09     78.0±1.30    89.7±0.40\n\n\nTable 15: Full results for LLMGrader reward experiments in DiffusionDB dataset. Asterisk denotes\nlearning baselines trained for Promptist Reward. Test-time search methods are evaluated at the budget\nof 160 generated prompts.\n\n\n      Method      Accuracy    Originality     Visual     Consistency   Emotional    Overall\n\n         Initial         69.0±0.13    63.2±0.16    81.2±0.12     84.6±0.02     69.5±0.22    73.1±0.13\n       Promptist*    59.1±1.69    65.9±1.37    86.4±0.84     86.3±0.32     68.8±1.04    84.6±1.13\n     PAG*         52.5±0.38    62.9±0.22    85.7±0.51     84.7±0.13     64.2±0.22    83.6±0.04\n      Rule-Based   68.1±1.95    67.0±2.41    84.9±0.96     86.6±0.83     71.3±2.13    87.2±0.58\n       Paraphrase    69.7 ±1.00   66.6 ±0.77   84.3 ±0.46    86.8 ±0.53    72.8 ±0.86   88.3 ±0.31\n\n      Ours         73.4±1.44    70.7±1.33    86.4 ±0.51    88.3±0.40    76.1 ±1.15    89.0±0.33\n\n\nG  ADDITIONAL QUALITATIVE RESULTS\n\nAdditional qualitative examples, for all combinations of rewards and datasets in our main experiment,\ncan be found in Fig. 9-13.\n\nH  ADDITIONAL HINT CASE STUDY\n\nAdditional hint case study, for all combinations of rewards and datasets in our main experiment,\ncan be found in Fig. 14-23. Similar to the figure in the main text, We highlight the relevant parts.\nAlso note that the hint sometimes suggests avoiding phrases that have negative effect, visualized by\nunderlined text. Numbers in parentheses indicate reward for corresponding prompts.\n\nI  USE OF LLMS\n\nWe utilize LLMs to check grammar and improve the clarity of the sentences in this paper. The authors\ndraft all the original content, and the role of the LLMs is strictly limited to language refinement. All\nfinal contents are carefully verified by the authors.\n\n\n                                       20\n\nUnder Review\n\n\n\n\n\n                                     Initial ( 1.49)                                                              Initial ( 0.43)\n                           Duck and dog crossbreed                                       a huddle of astronauts\n                                                                                                           dissecting a minion from\n                                                                                                     despicable me.\n\n\n\n\n\n                          Ours (1.44)                                              Ours (0.60)\n                              Immersive digital painting:                                                 Ultra-detailed: astronauts\n                         A realistic duck-dog hybrid                                          performing a precise\n                                  peacefully stands in a misty                                             dissection of a Minion in a\n                                wetland, bathed in the warm                                                  sterile laboratory, dramatic\n                              glow of golden hour. Focus                                            volumetric lighting,\n                            on detailed textures,                                                      ArtStation.\n                               atmospheric perspective, and\n                                        soft, natural lighting.\n\n\n\n                           Paraphrase (0.96)                                         Paraphrase (0.47)\n                                 Highly detailed digital                                                 Highly detailed image:\n                                  painting of a duck-dog                                                astronauts dissecting a\n                                   hybrid, combining canine and                                          Minion, sterile lab\n                                avian features, dramatic                                            environment.\n                                      lighting, artstation.\n\n\n\n\n\n                       PAG (0.41)                                         PAG (-3.40)\n                           Duck and dog crossbreed,                                                  hyperrealistic,\n                                     hyperrealistic, cinematic,                                              hyperdetailed, 3 d render, 8\n                               dramatic lighting, by wlop                                         k resolution, by wlop and\n                                                                                              greg rutkowski\n\n\n\n\n\n                            Promptist (-0.54)                                            Promptist (-0.06)\n                           Duck and dog crossbreed,                                      an a huddle of astronauts\n                                  painting by Craig Mullins,                                                dissecting a minion from\n                                  4k. trending on artstation.                                            despicable me, by greg\n                                                                                                         rutkowski, by wlop, by namek\n\n\n\n\n\n                            DPO-Diff (-1.84)                                             DPO-Diff (-0.67)\n                               Negative Prompt:                                                  Negative Prompt:\n                                  lion,but,snake,separate                                                 separate,for,passengers,buil\n                                                                                                                ding,individual,to,her\n\n\n\n\n\n                           Rule-Based (0.79)                                          Rule-Based (-0.10)\n                           Duck and dog crossbreed,                                       a huddle of astronauts\n                                        illustration, fantasy,                                                     dissecting a minion from\n                            smooth                                                             despicable me, 8 k, sharp\n                                                                                                             focus, elegant.\n\n\n\n\n\n             (a) Promptist Reward - Lexica                        (b) Promptist Reward - DiffusionDB\n\nFigure 9: Additional qualitative results for Promptist Reward in Lexica (left) and DiffusionDB (right)\ndatasets.\n\n\n\n                                       21\n\nUnder Review\n\n\n\n\n\n                                     Initial ( 0.27)                                                              Initial ( 0.25)\n                            an intricate concept                                               halloween witch girl with\n                                       illustration of bird landing                                       making witchcraft.\n\n\n\n\n\n                          Ours (1.61)                                              Ours (1.80)\n                                 Highly detailed concept art:                                       Halloween witch girl\n                            a small bird, wings                                                    meticulously crafting a\n                                      partially folded, landing                                            magical potion, vibrant\n                                 gently on a vibrant meadow                                              ingredients, atmospheric and\n                                    of wildflowers, rendered in                                 warm lighting, detailed\n                            a painterly style with                                                           digital painting.\n                               dramatic golden hour\n                                      lighting.\n\n\n\n                           Paraphrase (0.69)                                         Paraphrase (1.10)\n                         A breathtaking concept                                            Atmospheric Halloween scene:\n                                       illustration of a bird                                            a young witch girl intensely\n                                 landing  highly detailed,                                           focused on making\n                                   painterly style, dramatic                                                   witchcraft, glowing potion,\n                                      lighting.                                                          dark fantasy, intricate\n                                                                                                                       details, cinematic lighting.\n\n\n\n\n                       PAG (1.31)                                         PAG (0.75)\n                            an intricate concept                                               halloween witch girl with\n                                       illustration of bird                                             making witches. hyper-\n                                   landing, by wlop, by wlop,                                                 detailed, 8k, HD. by wlop.\n                            by ilyakuvshinov, by greg\n                                rutkowski\n\n\n\n\n\n                            Promptist (0.86)                                            Promptist (0.31)\n                            an intricate concept                                              Halloween witch girl with\n                                       illustration of bird                                             making witches. , digital\n                                   landing, by greg rutkowski,                                               Art , WLOP, Rossdraws, James\n                                     artstation, cgsociety,                                                     Jean,\n                               dramatic lighting, highly\n                                    detailed, incredible\n                                      quality, trending on\n                                    artstation\n\n\n\n                            DPO-Diff (0.32)                                              DPO-Diff (0.04)\n                               Negative Prompt:                                                  Negative Prompt:\n                                  none,simple,fact,abstraction                                              saint,elder,without,destroyi\n                                    ,none,reptile                                                            ng,purity\n\n\n\n\n\n                           Rule-Based (1.02)                                          Rule-Based (0.98)\n                            an intricate concept                                               halloween witch girl with\n                                       illustration of bird                                             making witchcraft, digital\n                                   landing, wlop, fantasy,                                                           art, octane render, digital\n                                       digital art                                                                 painting.\n\n\n\n\n\n               (a) ImageReward - Lexica                            (b) ImageReward - DiffusionDB\n\nFigure 10: Additional qualitative results for ImageReward in Lexica (left) and DiffusionDB (right)\ndatasets.\n\n\n\n                                       22\n\nUnder Review\n\n\n\n\n\n                                 Initial (0.33)                                                                Initial (0.00)\n                       A punk rock frog in a studded                                    a cup on the right of a dog\n                               leather jacket shouting into a\n                          microphone while standing on a\n                             boulder\n\n\n\n\n\n                        Ours (1.00)                                              Ours (1.00)\n                                  Hyperrealistic digital art: a                                       A friendly Golden Retriever\n                                     fiercely passionate punk frog                                            gazes forward, a rustic\n                                   vocalist in a studded leather                                             stoneware mug sits to its\n                                    jacket, aggressively                                                                right on a wooden surface.\n                               performing into a vintage\n                             microphone atop a colossal,\n                             weathered boulder, dramatic\n                               rim lighting, intricate\n                                  textures, capturing raw energy\n                           and emotion.\n\n                        Paraphrase (0.89)                                          Paraphrase (0.00)\n                                  Intricate illustration of a                                     A dog and a cup, cup\n                         punk rock frog vocalist                                                 positioned right.\n                           wearing a black studded\n                               leather jacket, passionately\n                             shouting into a vintage\n                          microphone while standing\n                               confidently on a large,\n                          weathered boulder, stage\n                                    lights, high detail.\n\n\n                     PAG (0.00)                                          PAG (0.00)\n                           hyper realistic, dramatic,                                        a cup on the right of a dog,\n                                   intricate, elegant, highly                                                    hyperrealistic, hyper\n                                detailed, digital painting,                                                  detailed, 8 k resolution, by\n                           concept art, matte, sharp                                         wlop and greg rutkowski                               focus, illustration, art by\n                           wlop and greg rutkowski\n\n\n\n                         Promptist (0.22)                                             Promptist (0.00)\n                         a punkrock frog in a studded                                 A cup on the leftof of a dog,\n                               leather jacket shouting into a                                    a fantasy digital painting by                          microphone while standing on a\n                               boulder,                                                      Greg Rutkowski and James                                                                                              Gurney, trending on\n                                                                                                              Artstation, highly detailed\n\n\n\n\n                       OPT2I (0.33)\n                       A screaming punk rock frog\n                            atop a boulder, microphone in\n                            hand, wearing a black leather\n                               jacket with studs.\n\n\n\n\n\n                 (a) DSG - PartiPrompt                                                                               (b) UniDet2D\n\n          Figure 11: Additional qualitative results for DSG (left) and UniDet2D (right),\n\n\n\n\n\n                                       23\n\nUnder Review\n\n\n\n\n\n                                     Initial (0.26)                                                                Initial (0.75)\n                            a suitcase in front of a horse                                    one person and three cats\n\n\n\n\n\n                          Ours (0.66)                                              Ours (1.00)\n                            A majestic horse, dappled gray                                    A cinematic portrait of a\n                                     or chestnut, pauses before a                                             person and three cats, a warm\n                                   vintage suitcase in a sunlit                                              atmosphere, soft focus, and\n                             meadow, emphasizing                                                         highly detailed textures, 8k\n                                       photorealistic detail and                                                          resolution.\n                                           soft, cinematic lighting.\n\n\n\n\n                           Paraphrase (0.46)                                         Paraphrase (0.75)\n                         A brown leather suitcase in a                                          Intimate portrait: one person\n                                          field, facing a stunning                                        and three cats, natural light,\n                                  horse, 8k, photorealistic.                                                detailed fur.\n\n\n\n\n\n                       PAG (0.25)                                         PAG (0.75)\n                            a suitcase in front of a                                          a single person and three\n                                  horse, hyper realistic, 8 k,                                                  cats, hyperrealistic,\n                            by wlop and greg rutkowski                                            hyperdetailed, 8 k realistic,\n                                                                                        by wlop and greg rutkowski\n\n\n\n\n\n                            Promptist (0.53)                                            Promptist (0.50)\n                            a suitcase in front of a                                         one person and three cats.\n                                  horse, by greg rutkowski,                                                      intricate, elegant, highly\n                                       digital art, realistic                                                        detailed, digital painting,\n                                   painting, fantasy, very                                                      artstation, concept art, sharp\n                                    detailed, trending on                                                     focus, illustration, by justin\n                                    artstation                                                          gerard and artgerm, 8 k\n\n\n\n\n                      (a) UniDet3D                                        (b) UniDetNumeracy\n\n    Figure 12: Additional qualitative results for UniDet3D (left) and UniDetNumeracy (right),\n\n\n\n\n\n                                       24\n\nUnder Review\n\n\n\n\n\n                                     Initial (90.00)                                                              Initial (70.00)\n                            a hyperrealistic illustration                                      a fantasy hybrid creature that\n                                    of a monster covered in sand.                                     has the body of a grizzly bear\n                                                                                      and the head of an owl\n\n\n\n\n\n                          Ours (94.40)                                             Ours (90.00)\n                                    Hyperrealistic: ancient,                                                      Intricate fantasy hybrid:\n                                   colossal monster *unfurling*                                       muscular grizzly bear and\n                              from the desert, sand                                                         intelligent owl, seamless\n                                 cascading, hyperdetailed                                               blend, dramatic god rays,\n                                    scales, dramatic volumetric                                           trending on ArtStation.\n                                      lighting, dust.\n\n\n\n\n                           Paraphrase (92.60)                                        Paraphrase (87.40)\n                                   Hyperrealistic illustration of                                                 Intricate and highly detailed\n                            a monster completely covered                                        fantasy creature  a grizzly\n                                      in desert sand, intricate                                            bear s robust body elegantly\n                                      details.                                                      combined with the head of a\n                                                                                              barn owl, atmospheric\n                                                                                                                       lighting, digital art.\n\n\n\n\n                       PAG (90.00)                                        PAG (49.00)\n                                    hyperrealistic illustration of                                           fantasy hybrid creature that\n                            a monster covered in sand,                                            looks like a grizzly bear and\n                                 hyperdetailed, 8 k realistic,                                          the head of an owl, hyper\n                            by wlop and greg rutkowski                                               detailed, digital art,\n                                                                                                      trending in artstation,\n                                                                                                    cinematic lighting, by wlop\n\n\n\n\n                            Promptist (90.00)                                           Promptist (71.00)\n                            a hyperrealistic illustration                                          Fantasy hybrid creature that\n                                    of a monster covered in sand,                                     has the body of a grizzly bear\n                            by greg rutkowski and thomas                                  and the head of an owl, by\n                                 kinkade, trendingrete                                           Greg Rutkowski and Raymond\n                                     artstation.                                                       Swanland, Trending on\n                                                                                                                  Artstation, ultra realistic\n                                                                                                                          digital art\n\n\n\n                           Rule-Based (92.60)                                         Rule-Based (48.00)\n                            a hyperrealistic illustration                                      a fantasy hybrid creature that\n                                    of a monster covered in sand,                                     has the body of a grizzly bear\n                                       digital art, concept art,                                        and the head of an owl,\n                                      intricate                                                                elegant, intricate, fantasy.\n\n\n\n\n\n                (a) Lexica - LLMGrader                             (b) DiffusionDB - LLMGrader\n\nFigure 13: Additional qualitative results for LLMGrader in Lexica (left) and DiffusionDB (right)\ndatasets.\n\n\n\n\n\n                                       25\n\nUnder Review\n\n\n\n\n\n   Initial Prompt\n  An ultradetailed illustration of cthulu destroying a fleet of battleships                            (-0.044)\n\n  Example Context for Hint-Generator LLM\n A stunning, ultradetailed illustration depicting Cthulhu rising from the abyssal depths to obliterate a fleet of\n   battleships. Intense, dramatic lighting, Lovecraftian horror, colossal scale.                       (0.160)\n  Hyperdetailed  Cthulhu,   colossal  tentacles  crushing  battleships  in  a  bioluminescent  abyss.\n   Dramatic, volumetric lighting. Lovecraftian horror.                                             (0.088)\n\n  Generated Hint\n  Focus on highly detailed, cinematic descriptions emphasizing scale, lighting, and Lovecraftian horror,\n  while maintaining a balance between descriptive length and conciseness.\n\n  Example Output from Optimizer LLM\n  An intensely detailed, cinematic illustration of Cthulhu unleashing his wrath upon a fleet of battle-\n   ships. Bioluminescent abyss, colossal scale, intricate tentacle details, dramatic, god-like lighting, pure\n   Lovecraftian horror.                                                                          (0.323)\n  An ultradetailed digital painting of Cthulhu’s attack: immense tentacles rise from a bioluminescent abyss,\n  destroying a fleet of battleships. Volumetric lighting, Lovecraftian horror aesthetic.              (0.273)\n\n\n    Figure 14: Additional case study of generated hints for Promptist Reward in Lexica dataset.\n\n\n\n\n\n   Initial Prompt\n   illustration of a colorful melting human head                                                    (-0.083)\n\n  Example Context for Hint-Generator LLM\n  Vibrant, dripping colors cascade over a melting human head – a highly detailed digital painting with\n   flowing, bioluminescent textures – surreal and captivating.                                    (1.362)\n A surreal and captivating illustration: a melting human head, rendered with hyperdetailed, dripping paint\n  and bioluminescent textures. Dreamlike, liquid light.                                           (1.016)\n\n  Generated Hint\n  Combine strong keywords like \"highly detailed,\" \"surreal,\" \"bioluminescent,\" and \"dripping\" with de-\n   scriptions of flowing, organic textures and captivating ambiance, emphasizing it as an illustration or\n   digital painting.\n\n  Example Output from Optimizer LLM\n   Captivating digital painting: a melting human head, enveloped in bioluminescent, dripping hues and\n   flowing, organic textures – a surreal portrait with intricate details and a mesmerizing ambiance.  (1.390)\n A highly detailed illustration of a melting human head, dripping with vibrant bioluminescent colors,\n  showcasing flowing, organic forms and a surreal, captivating atmosphere.                      (0.903)\n\n\n Figure 15: Additional case study of generated hints for Promptist Reward in DiffusionDB dataset.\n\n\n\n\n\n                                       26\n\nUnder Review\n\n\n\n\n\n   Initial Prompt\n  Anthropomorphic mushroom.                                                                   (-0.380)\n\n  Example Context for Hint-Generator LLM\n  Full body portrait of a joyful anthropomorphic mushroom character, radiating positive energy, detailed\n   clothing adorned with glowing moss and sparkling spores, lush magical forest, Artgerm and Ghibli style,\n  8k resolution, highly detailed.                                                                 (0.492)\n  8k full body: A benevolent anthropomorphic mushroom, radiating positive energy, wearing intricately\n  woven clothing with glowing moss, lush magical forest environment, Artgerm style.            (0.748)\n\n  Generated Hint\n  Focus on  \"benevolent\" or \"joyful\" anthropomorphic mushroom spirits/people with  intricately wo-\n  ven, glowing moss attire in a lush, sun-dappled magical forest, specifying 8k full body portrait and\n   Artgerm/Ghibli inspiration.\n\n  Example Output from Optimizer LLM\n A joyful anthropomorphic mushroom character with a warm expression, detailed clothing of glowing moss\n  and woven fibers, standing in a magical forest, 8k, Artgerm and Ghibli aesthetics.              (0.744)\n   Intricate 8k full body portrait of a benevolent mushroom person, radiating joy, with exquisitely woven\n   attire of glowing moss, bathed in sunlight within a lush, magical forest – Artgerm & Ghibli inspired.\n  (0.574)\n\n\n     Figure 16: Additional case study of generated hints for ImageReward in Lexica dataset.\n\n\n\n\n\n   Initial Prompt\n  a character wearing a white costume                                                            (-0.861)\n\n  Example Context for Hint-Generator LLM\n   Full body, white feathered costume, sculpted ice accents, WLOP style, volumetric light, detailed tex-\n   tures.                                                                                         (0.110)\n   Full body illustration of a powerful character in a breathtaking white costume of sculpted ice, iridescent\n   feathers, and shimmering silk, dramatic volumetric lighting.                                   (0.744)\n\n  Generated Hint\n  Focus on detailed descriptions of materials (ice, feathers, silk) combined with strong lighting and artistic\n   style references (WLOP, Greg Rutkowski, Artgerm) within a full-body illustration.\n\n  Example Output from Optimizer LLM\n   Full body  illustration of a powerful character wearing a breathtaking white costume of sculpted\n   ice,  iridescent  feathers,  and  shimmering  frost,  dynamic  pose,  dramatic volumetric lighting,\n  Artgerm & Greg Rutkowski, intricate details, cinematic composition.                           (0.869)\n   Full body  portrait:  a character adorned in a white costume –  ice, feathers, silk – dynamic pose,\n  Artgerm & Greg Rutkowski, intricate details, rim lighting.                                     (0.720)\n\n\n   Figure 17: Additional case study of generated hints for ImageReward in DiffusionDB dataset.\n\n\n\n\n\n                                       27\n\nUnder Review\n\n\n\n\n\n   Initial Prompt\n  two baseballs to the left of three tennis balls                                                    (0.400)\n\n  Example Context for Hint-Generator LLM\n  Two baseballs, three tennis balls – arranged left to right.                                        (0.667)\n A close-up of two baseballs positioned to the left of three tennis balls, high detail, realistic materials,\n  8k, sharp focus, natural lighting.                                                               (0.333)\n\n  Generated Hint\n  Keep   the  prompt   concise,   focusing  on   quantity  and   left-to-right   arrangement,   and\n  avoid overly descriptive or artistic terms\n\n  Example Output from Optimizer LLM\n  Two baseballs positioned left, followed by three tennis balls.                                    (0.867)\n  Baseball x2, tennis balls x3, left to right.                                                       (0.667)\n\n\n       Figure 18: Additional case study of generated hints for DSG in PartiPrompt dataset.\n\n\n\n\n\n   Initial Prompt\n  a butterfly on the right of a balloon                                                             (0.000)\n\n  Example Context for Hint-Generator LLM\n   Swallowtail butterfly, right of a crimson balloon.                                              (0.330)\n  Monarch butterfly, right of lavender balloon, detailed wings.                                   (0.272)\n\n  Generated Hint\n  Focus on simpler prompts with key elements – butterfly type, balloon color, and \"right of\" – avoiding\n  excessive detail or artistic jargon.\n\n  Example Output from Optimizer LLM\n A blue Morpho butterfly to the right of a yellow balloon.                                      (0.331)\n  Monarch butterfly right of red balloon.                                                       (0.329)\n\n\n               Figure 19: Additional case study of generated hints for UniDet2D.\n\n\n\n\n\n   Initial Prompt\n  a wallet in front of a person                                                                    (0.060)\n\n  Example Context for Hint-Generator LLM\n   Realistic photo of a person with a wallet visibly held.                                          (0.172)\n  Warm, candid portrait of a person, a wallet visible in foreground, shallow depth of field, photorealistic.\n  (0.184)\n\n  Generated Hint\n  Focus on warm, candid portraits with the wallet visible in the foreground, emphasizing photorealism and\n   shallow depth of field.\n\n  Example Output from Optimizer LLM\n   Photorealistic, warm-toned portrait, wallet visible, focused on hands, candid pose.              (0.204)\n   Intimate, candid portrait of a person, wallet held naturally in foreground, soft, diffused light, high detail,\n   photorealistic rendering, shallow depth of field.                                                (0.102)\n\n\n               Figure 20: Additional case study of generated hints for UniDet3D.\n\n\n\n\n                                       28\n\nUnder Review\n\n\n\n\n\n   Initial Prompt\n   three bicycles, two frogs, two keys, two helmets and two pigs                                   (0.167)\n\n  Example Context for Hint-Generator LLM\n A vibrant and detailed illustration: three bicycles, two emerald frogs, two golden keys, two scarlet helmets,\n  and two rosy pigs – storybook aesthetic, trending on Artstation.                                 (0.100)\n  Three bicycles with two frogs, keys, helmets, and pigs.                                          (0.300)\n\n  Generated Hint\n  Focus on concise descriptions of the subjects and prioritize quantity of subjects over excessive detail or\n   artistic style requests.\n\n  Example Output from Optimizer LLM\n  Two frogs, two keys, two helmets, two pigs, and three bicycles.                                  (0.300)\n A grouping of three bicycles, two frogs, two keys, two helmets, and two pigs.                    (0.267)\n\n\n            Figure 21: Additional case study of generated hints for UniDetNumeracy.\n\n\n   Initial Prompt\n  Painting of group of hungry alien snails homeless at dinner in a garbage dump.                   (53.00)\n\n  Example Context for Hint-Generator LLM\n  Highly detailed painting of forlorn alien snails sharing a meager meal within a realistically decaying\n   garbage dump, volumetric lighting, melancholic mood.                                        (79.53)\n  Highly detailed digital painting of desperately hungry, homeless alien snails gathered for a somber dinner\n  amongst the refuse of a vast, decaying garbage dump.                                          (66.33)\n\n  Generated Hint\n  Focus on combining \"highly detailed,\" \"dramatic,\" \"realistic rendering/decaying,\" and \"volumetric/rim\n   lighting\" with clear emotional descriptors like \"forlorn,\" \"desperate,\" or \"melancholic\" while specifying\n  \"dinner\" or \"meal\" within the \"garbage dump\" setting.\n\n  Example Output from Optimizer LLM\n  Highly detailed, atmospheric painting:  desperately hungry, homeless alien snails gathered for a\n   melancholic dinner amongst the refuse of a sprawling, realistically decaying garbage dump, dramatic\n   lighting.                                                                                      (86.73)\n   Realistic rendering of forlorn alien snails sharing a sparse meal in a sprawling garbage dump, highly\n   detailed textures, dramatic rim lighting, melancholic mood.                                     (76.13)\n\n\n  Figure 22: Additional case study of generated hints for LLMGrader Reward in Lexica Dataset.\n\n\n   Initial Prompt\n  a phoenix                                                                                     (91.33)\n\n  Example Context for Hint-Generator LLM\n A breathtaking phoenix, intensely detailed plumage, swirling flames, digital painting, art by Artgerm.\n  (93.67)\n A phoenix reborn from ashes, highly detailed plumage, dramatic lighting, digital painting, Artgerm\n   inspired.                                                                                      (93.13)\n\n  Generated Hint\n  Combine strong artistic references (Artgerm & Rutkowski are consistently helpful), emphasize intricate\n   detail in plumage/feathers, and use evocative descriptions of fire/light (molten gold, god rays, swirling\n  embers) with dramatic lighting and epic scale.\n\n  Example Output from Optimizer LLM\n  Majestic phoenix with feathers of shimmering gold and crimson, dynamic pose, dramatic lighting,\n  Rutkowski, swirling embers, highly detailed fantasy art.                                       (94.33)\n A breathtaking phoenix, intensely detailed iridescent plumage, engulfed in swirling flames, digital paint-\n   ing, dramatic lighting.                                                                        (94.00)\n\n\nFigure 23: Additional case study of generated hints for LLMGrader Reward in DiffusionDB dataset.\n\n\n\n                                       29",
"headers": [
"arXiv:2506.16853v2  [cs.LG]  29 Sep 2025",
"R",
"-A",
"P",
"O",
"T",
"-",
"-I",
"D",
"M",
"EWARD",
"GNOSTIC",
"ROMPT",
"PTIMIZATION",
"FOR",
"EXT",
"TO",
"MAGE",
"IFFUSION",
"ODELS",
"A",
"1",
"I",
"2",
"W",
"3",
"4",
"E",
"5",
"C",
"S",
"B",
"RATTPO",
"F",
"DAS",
"G",
"Q",
"H",
"U",
"LLM"
],
"tables": [
"|Col1|dripping|Col3|\n|---|---|---|\n||bioluminescent|bioluminescent|\n|surreal|and|captivating|",
"|magical forest,|Col2|8k,|Artgerm and Ghibli|\n|---|---|---|---|\n|magical forest,|benevolent|benevolent|benevolent|",
"|materials (ice, feathers, silk)|Col2|\n|---|---|\n|materials (ice, feathers, silk)|full-body|",
"|right of|lavender|\n|---|---|",
"|right of|red|\n|---|---|",
"|Col1|orlorn|Col3|\n|---|---|---|\n|volumetric lighting,|volumetric lighting,|melancholic|",
"|decaying|garbage dump|\n|---|---|",
"|realistically decaying|garbage dump|\n|---|---|",
"|plumage,|swirling|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2506.16853v2.pdf"
}