{
"text": "Evolutionary Pre-Prompt Optimization for Mathematical\n     Reasoning\n\n     MATHURIN VIDEAU, Meta AI & TAU, INRIA and LISN (CNRS & Univ. Paris-Saclay) , France\n     ALESSANDRO LEITE, TAU, INRIA and LISN (CNRS & Univ. Paris-Saclay), France\n     MARC SCHOENAUER, TAU, INRIA and LISN (CNRS & Univ. Paris-Saclay), France\n      OLIVIER TEYTAUD, Meta AI, France2024\n        Recent advancements have highlighted that large language models (LLMs), when given a small set of task-\n          specific examples, demonstrate remarkable proficiency, a capability that extends to complex reasoning tasks.\n         In particular, the combination of few-shot learning with the chain-of-thought (CoT) approach has been pivotalDec          in steering models towards more logically consistent conclusions [Wei et al. 2022b]. This paper explores the\n5  optimization of example selection for designing effective CoT pre-prompts and shows that the choice of the\n         optimization algorithm, typically in favor of comparison-based methods such as evolutionary computation,\n          significantly enhances efficacy and feasibility. Specifically, thanks to a limited exploitative and overfitted\n         optimization, Evolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the naive few-shot\n        approach exceeding 10 absolute points in exact match scores on benchmark datasets such as GSM8k and\n       MathQA. These gains are consistent across various contexts and are further amplified when integrated with\n          self-consistency (SC).[cs.CL]\n       CCS Concepts: â€¢ Computing methodologies â†’Reasoning about belief and knowledge.\n\n         Additional Key Words and Phrases: Evolutionary algorithms, large language model, pre-prompt optimization,\n        mathematical reasoning\n\n     ACM Reference Format:\n        Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud. 2024. Evolutionary Pre-Prompt\n         Optimization for Mathematical Reasoning. 1, 1 (December 2024), 29 pages. https://doi.org/10.1145/nnnnnnn.\n       nnnnnnn\n\n       1  INTRODUCTION\n       Large language models (LLMs) [Anil et al. 2023; Brown et al. 2020; Hoffmann et al. 2022; Team\n         et al. 2023; Touvron et al. 2023] have emerged as a transformative force, demonstrating exceptional\n         capabilities across a spectrum of tasks. However, despite their size and complexity, these models\n           still face challenges in multi-step reasoning, particularly in tasks that require arithmetic, logic,\n        and/or mathematical reasoning [Cobbe et al. 2021; Rae et al. 2021].arXiv:2412.04291v1\n         To address this limitation, recent works have focused on enhancing the reasoning abilities of LLMs.\n     A significant advancement in this direction is the chain-of-thought (CoT) prompting method [Wei\n         et al. 2022b]. This approach involves guiding LLMs to articulate intermediate reasoning steps in a\n       manner akin to human thought processes, leading to more accurate and interpretable solutions.\n        This method has shown substantial improvements on complex tasks, including mathematics and\n       commonsense reasoning [Lu et al. 2022b; Suzgun et al. 2022; Wei et al. 2022b].\n        The advancement of the CoT prompting has opened new pathways in the design of effective\n      CoT prompts [Fu et al. 2022; Jiang et al. 2023; Kojima et al. 2022; Zhou et al. 2022]. A crucial aspect\n\n          Authorsâ€™ addresses: Mathurin Videau, Meta AI & TAU, INRIA and LISN (CNRS & Univ. Paris-Saclay), France, mvideau@\n         meta.com; Alessandro Leite, TAU, INRIA and LISN (CNRS & Univ. Paris-Saclay), France, alessandro.leite@inria.fr; Marc\n          Schoenauer, TAU, INRIA and LISN (CNRS & Univ. Paris-Saclay), France, marc.schoenauer@inria.fr; Olivier Teytaud, Meta\n           AI, France, oteytaud@meta.com.\n\n           2024. This is the authorâ€™s version of the work. It is posted here for your personal use. Not for redistribution. The definitive\n          Version of Record was published in , https://doi.org/10.1145/nnnnnnn.nnnnnnn.\n\n\n\n                                                                                                                                            , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n2                                     Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\n                                  Look up   Demonstration                                                                      Provide fewshot CoT\n                                          Data\n\n                                                                                                                               Q: There are 15 treeâ€¦                   Example Array                                                        A: There are 15 treesâ€¦\n                                           [ 3, 7, ...]\n                                                                                                                               Q: Olivia has $23.Sheâ€¦\n                                                                                                                               A: Olivia had 23 dollarsâ€¦\n                   Ask                                       Questions    Q: Janet's ducks lay 16â€¦\n                                                                                                                               A:\n\n                                             Training                 Optimiser                              LLM\n                                          Data\n\n\n                                                                                                                               Janetâ€™s ducks lay 16 eggs\n                                                                                                                               per day. She eats 3 for\n                                                                                                                               breakfast every morning.\n                                                                                                                               She bakes muffins for her\n                                                                                                                               friends every day with 4.\n                                                            True answers   Soâ€¦\n\n                                   Tell                                    LLM answers\n                                         Score\n\n\n\n                          Fig. 1. Overview of the proposed CoT optimization process.\n\n\nof this research involves the strategic use of complex examples in prompts for a solution. Fu et al.\n[2022] demonstrate that using such intricate examples in few-shot prompts could improve LLM\nperformance in reasoning tasks, highlighting the importance of not only the content but also the\nstructure of the prompts in enhancing the reasoning process. Another effective technique in CoT\nprompting is the inclusion of directive phrases, such as â€œlet us think step by step,â€ which leads to\nmore organized reasoning generation [Kojima et al. 2022]. Overall, these developments highlight\nthe need for carefully selecting and structuring the prompts, as they directly impact the efficiency\nof LLMs in tackling complex reasoning tasks.\n  In line with previous works, we investigate the effect of few-shot prompting through automated\nexample selection. However, we use these examples as a pre-prompt for the whole downstream task\nat hand: once constructed, these few examples remain fixed for the given task (here, mathematical\nmodeling [Bundy 1983; Feigenbaum et al. 1963]). This differs from various in-context learning\nmethods (see Section 7) in which the selection of the prompts depends on each specific instance. Fur-\nthermore, since most reasoning benchmarks provide a training set too small for proper training [Liu\net al. 2023; Ni et al. 2023], we focus on evolutionary algorithms, which can be comparison-based and\ntherefore only need a few bits as feedback, paving the way to information-theoretic generalization\nguarantees. We analyze mathematically the risk of overfitting or exploitative behavior of our\nevolutionary approach, and prove generalization bounds that do not exist for other approaches\naccessing fine-grain data with gradient-based methods.\n  Such an Evolutionary Pre-Prompt Optimization strategy (termed EPPO in the following) effectively\nidentifies a concise set of examples (called for short few-shot in the following) that notably enhance\nperformance when used together as pre-prompt. Additionally, considering the current limited\nunderstanding of few-shot mechanisms [Min et al. 2022], the insights gained from the selected\nexamples enable us to identify new ways to improve LLMs performances, not only helping in the\npractical use of LLMs for complex tasks but also contributing to a better understanding of the\nnuances of few-shot prompting at large.\n  The paper is organized the following way: Section 2 introduces the different components of\nEPPO and details the algorithm itself. Section 3 proposes information-theoretic results and proves\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                             3\n\n\nsome generalization bounds (Section 3.2) for EPPO thanks to the limited information it requires to\noptimize the pre-prompt. Section 4 presents the experimental validation of EPPO with an emphasis\non overfitting, while Section 5 digs deeper inside its working details. Section 6 discusses these\nresults, and Section 7 puts them in perspective with other related published works. Finally, Section 8\nsummarizes and concludes the paper.\n\n2 METHODS\n\nThis work focuses on optimizing the performance of a given LLM on a given complex downstream\ntask (e.g., mathematical reasoning) through few-shot CoT pre-prompting. Our primary goal is to\nidentify the most impactful small pre-prompt that can significantly boost the LLM performances\non the task at hand, i.e., to select a small set of prompts from a given demonstration dataset D that,\nwhen used as pre-prompt for all further queries, improve the efficiency and effectiveness of the\nLLM on that task.\n\n2.1  Few-shot optimization\nFor a given training set T and its corresponding demonstration set D made of CoT prompt examples,\nor simply CoT prompts (see Section 2.2), we aim to find out the best performing small subset, referred\nto as few-shot pre-prompt in the following, or simply pre-prompt. To do so, we formulate this problem\nas a combinatorial optimization problem, as follows (Figure 1):\n\n   â€¢ Representation of CoT prompt: Each example in the demonstration set D is assigned\n     a unique index in {1, . . . , |D|}. This transforms the demonstration set into a lookup table.\n      Instead of searching in Dğ‘ (with ğ‘ the size of the few-shot prompts we are looking for), we\n      search in {1, 2, . . . , |D|}ğ‘ .\n   â€¢ Construction of the few-shot pre-prompt: A few-shot pre-prompt is hence represented\n     by a (small) list of ğ‘ integers (typically 2 â‰¤ğ‘ â‰¤16), each representing a CoT prompt from the\n     demonstration dataset D.\n   â€¢ Optimization: The objective is to identify the combination of integers (i.e., of CoT prompts)\n      that maximizes the LLM performance for the task at hand on the training dataset. This\n      involves varying the integers of the list (and possibly the order and the size of the list). This\n        is amenable to black-box optimization (see Section 2.3).\n   â€¢ Evaluations: Every few-shot pre-prompt proposed by the optimization algorithm is tested\n     on the dataset: The objective function for the black-box optimization is the performance of\n      the LLM for the tasks at hand, which measures how well the chosen combination of CoT\n     prompts works when used as a pre-prompt for the whole task.\n\n2.2  Datasets construction\nIn our research setup, we work with training datasets that typically contain several thousand\nexamples. However, optimizing various few-shot across the entire dataset is cost-prohibitive due to\ncomputational limitations and the high cost of running LLMs. Addressing this challenge, hence,\nrequired to employ a sub-sampling strategy. When datasets are categorized with various difficulty\nlevels, we ensure a balanced approach by layered sub-sampling, i.e., uniformly sub-sampling across\nall categories and levels. This method guarantees a diverse range of examples in the reduced dataset,\nwhich is crucial for a comprehensive model evaluation.\nIn scenarios where the datasets lack such detailed categorization, we pivot to a strategy based on\nthe uncertainty of the LLM responses, specifically when using LLaMA2-70B [Touvron et al. 2023].\nWe generate ğ‘›different answers for each example, using a predefined temperature setting ğœ, that\ninfluences the diversity of the model responses. In order to gauge the confidence in an answer, we\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n4                                     Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\nanalyze the frequency of the correct answer within these multiple responses. The idea is that the\nmore frequently a correct answer appears, the higher the confidence of the model in that answer.\nBased on this confidence measure, we perform uniform sub-sampling across all levels of uncertainty.\nIn practice, in our experiments, we take ğ‘›= 10 and take the same number of examples for each\nfrequency of correct answers (in each bracket between 10,0   10,1   . . . 10,9  10).10  This method allows us to\nselect a representative subset of examples that captures a wide range of the model certainty levels\nas a proxy for the example difficulty.\nThe same strategy is used to build the demonstration datasets, starting from the remaining examples\nof the original training set. To further increase the demonstration diversity, we also add correct\ndemonstrations generated by the model with the baseline CoT. As a result, the demonstration\ndataset D contains both hand-annotated and automatically generated demonstrations, limiting the\nneed for human intervention.\n\n\n2.3  Evolutionary optimization\nThe goal is to optimize an array of integers (the list of indices of query/answer pairs from the\ndemonstration set): This is amenable to a classical black-box optimization scenario, allowing us\nto use standard optimization libraries like Nevergrad [Rapin and Teytaud 2018]. The simplest\noptimization algorithms for doing so consist of mutating the variables one or a few at a time, with\npossibly Tabu lists (though not used here). More sophisticated methods include selecting the vari-\nables or the number of variables to modify. This last option is frequent in recent works and included\nin Nevergrad algorithms such as â€œLogNormalâ€ [Kruisselbrink et al. 2011] and â€œLenglerâ€ [Doerr\net al. 2019; Einarsson et al. 2019]. In a nutshell (see e.g., [Rapin and Teytaud 2018] for details), the\nfollowing optimization algorithms are used in our experiments Section 4:\n\n   â€¢ The simple Discrete (1 + 1)-ES mutates each variable with probability 1/ğ‘‘, in dimension ğ‘‘\n      (repeat if no variable is mutated): The mutated point is used as a new reference if its objective\n      value is not worse than the previous best.\n   â€¢ The Portfolio method [Dang and Lehre 2016] replaces ğ‘= 1/ğ‘‘with a uniform random choice\n      of the mutation probability ğ‘in [0, 1].\n   â€¢ The DoubleFastGA method is an adaptation in [Rapin and Teytaud 2018] of the Fast GA[Doerr\n       et al. 2017] to non-binary discrete variables.\n   â€¢ In the Lengler method, the mutation probability ğ‘decreases over time, with a schedule that\n       is mathematically derived for optimal performance on artificial test functions [Einarsson\n      et al. 2019]. Variants are included with variations of the critical hyperparameters: We refer\n      to [Rapin and Teytaud 2018] for details.\n   â€¢ The LogNormal method [Kruisselbrink et al. 2011] uses a self-adaptation mechanism that\n      modifies the mutation probability ğ‘, that is itself subject to log-normal mutation.\n   â€¢ Some variants of these algorithms are also tested (their name contains â€œRecombiningâ€ or\n      â€œCrossoverâ€) with a crossover operator. The different crossover operators include (a) one-point\n      crossover, (b) two-point crossover, and (c) recombining as in differential evolution, i.e., each\n      variable is independently copied from a random parent. We do not check for duplicates\n      in the list after the crossover, which is unlikely to happen thanks to the large size of the\n     demonstration set D compared the few-shot size ğ‘ .\n   â€¢ We also compare EPPO with random search, in which all variables are repeatedly and\n      uniformly drawn in their domain, retaining, in the end, the overall best-encountered solution.\n\n  The discrete algorithms discussed above are generally presented in the context of binary variables,\nbut the adaptation to categorical variables is straightforward: The mutation operator replaces the\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                             5\n\n\ncurrent value with a value that is randomly drawn in the domain of this variable (excluding the\nprevious value).\n\n2.4  Evolutionary Pre-Prompt Optimization\n\nAlgorithm 1 EPPO global view. ğ´ğ‘™ğ‘”ğ‘œ(specified by ğ‘ğ‘ ğ‘˜, ğ‘¡ğ‘’ğ‘™ğ‘™, ğ‘Ÿğ‘’ğ‘ğ‘œğ‘šğ‘šğ‘’ğ‘›ğ‘‘) can be anything provided\nthat ğ‘¡ğ‘’ğ‘™ğ‘™accepts a first argument ğ‘ğ‘ğ‘’ğ‘ ğ‘¡in {1, . . . ,ğœ…}.\nRequire: ğ´ğ‘™ğ‘”ğ‘œ       âŠ²a comparison-based optimization algorithm with ask-and-tell interface\nRequire: ğœ”, ğ‘                                       âŠ²Random seed and budget\n  1: ğœ…â†ğ´ğ‘™ğ‘”ğ‘œ.ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™ğ‘–ğ‘§ğ‘’(ğœ”,ğ‘, . . . )                    âŠ²ğœ…: number of values returned by ask()\n  2: ğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’â†âˆ…\n  3: for step ğ‘ in 1, . . . ,ğ‘do\n  4:     ğ‘1, ğ‘2, . . . , ğ‘ğœ…â†ğ´ğ‘™ğ‘”ğ‘œ.ğ‘ğ‘ ğ‘˜()              âŠ²the ğœ…pre-prompts to evaluate and compare\n  5:     ğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’.ğ‘ğ‘‘ğ‘‘({ğ‘1, . . . , ğ‘ğœ…})                                 âŠ²store them in the archive\n  6:     ğ‘ğ‘’ğ‘ ğ‘¡(ğ‘ ) = ğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’(ğ‘1, . . . , ğ‘ğœ…)            âŠ²Run the LLM with each pre-prompt and\n  7:                                                         âŠ²return the index of the best\n  8:     ğ´ğ‘™ğ‘”ğ‘œ.ğ‘¡ğ‘’ğ‘™ğ‘™(ğ‘ğ‘’ğ‘ ğ‘¡(ğ‘ ), ğ‘1, . . . , ğ‘ğœ…)           âŠ²Only feedback: the result of the comparison\n  9: end for\n 10: return ğ‘Ÿ= ğ´ğ‘™ğ‘”ğ‘œ.ğ‘Ÿğ‘’ğ‘ğ‘œğ‘šğ‘šğ‘’ğ‘›ğ‘‘(ğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’)             âŠ²return the best guess from the archive\n\n\n  A global overview of EPPO algorithm is given in Figure 1 and its pseudo-code in Algorithm 1. Its\nmain parameter is a comparison-based combinatorial optimization algorithm ğ´ğ‘™ğ‘”ğ‘œ, that handles\ncategorical variables and uses the ask-and-tell interface (or a slight variant of it to take into\naccount that it is comparison-based): It is initialized (Line 1) with the random seed, the available\nbudget, and any other useful information related to the problem at hand (e.g., number of variables\nwith their domain of definition, constraints, etc). An archive of all visited points (points are pre-\nprompts here) is maintained (Line 2). In the main loop, ğ´ğ‘™ğ‘”ğ‘œis asked for new points to evaluate\n(Line 4). These points are added to the ğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’(Line 5) and compared (Line 6), the index of the\nbest one being the only feedback given to ğ´ğ‘™ğ‘”ğ‘œ(Line 8). When the budget ğ‘is exhausted, the best\nrecommendation (usually the best pre-prompt from ğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’, but possibly another one, for instance,\non noisy problems) is returned as the proposed optimal solution (Line 10).\n  The formalization using ğœ…departs from the usual formalism of Evolutionary Algorithms but aims\nto cover most possible cases. It covers at least all the algorithms used in this work and is described\nin Section 2.3. It will be necessary for the derivation of the theoretical results in Section 3, based\non information theory. It represents the number of points that ğ´ğ‘™ğ‘”ğ‘œwill send for comparisons. In\nparticular, it can include or not the previous current point of the algorithm and can also include a\nfull population. For instance, for (1 + ğœ†)-ES, it is ğœ…= 1 + ğœ†while for (1, ğœ†)-ES, it is ğœ…= ğœ†.\n   Finally, ğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’(ğ‘1, ğ‘2, . . . , ğ‘ğœ…) (Line 6) runs the LLM on the training data using each ğ‘ğ‘–in turn1,\nas can be visualized on the right part of Figure 1. It then typically (though our proof does not\nassume anything except Equation (5)) returns its estimation of the best-performing pre-prompt.\nTypically, EPPO can be seen as a repeated A/B pre-prompt testing scenario of an online LLM, but\nwithout ensuring that the same questions are sent to both alternatives nor that we have access to a\ndetailed ground truth answer for each question.\n  Next, Section 3 analyzes in more depth the properties of EPPO and derives bounds on the\ngeneralization error in the case of few-shot pre-prompts (i.e., containing only a small number of\n\n1We simplify the discussion by considering training data, but A/B testing or whatever process (provided that the output is\nin {1, . . . ,ğœ…}) is ok for our algorithm and analysis.\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n6                                     Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\nexamples from the demonstration set). Our key result is that, thanks to information theory and the\nnature of our algorithms (which all verify an equation of the form Equation (5) as they are based\non comparisons and not on detailed losses or gradients), we can derive generalization bounds. Liu\net al. [2023] has emphasized how hard it is to train an LLM on a small training set such as GSM8k,\neven with a small LLM. We show that we can do such a training even with a LLaMA2-70B model.\n\n3  INFORMATION-THEORETIC ANALYSIS\nIn modern deep learning, it is increasingly common for test datasets to be inadvertently contami-\nnated by data from the training phase: LLMs being pre-trained on almost the entire Internet, it\nis possible, and even probable, that the pre-training data leaks some of the test data [Li 2024].\nFurthermore, fine-grain data can lead to various issues, such as over-exploitative behaviors [Zheng\net al. 2024].\n  Therefore, overfitting becomes critical [Mirzadeh et al. 2024; Yang et al. 2024], and making\nprogress in the direction of reasoning might require new types of learning focused on generaliza-\ntion [Berglund et al. 2024; Dziri et al. 2023; McCoy et al. 2023; Mitchell et al. 2023]. We will now\ndiscuss how EPPO is a priori less prone to overfitting than its competitors by requiring feedback\nwith limited information (specifically, using only comparisons from possibly big datasets).\n\n3.1  Data size and usage\n\nVarious approaches have been proposed for improving LLMs. Some examples include Reinforcement\nLearning from Human Feedback (RLHF) [Abramson et al. 2022; Jain et al. 2015; Lee et al. 2023];\ndirect usage of the human feedback [Kim et al. 2023; Videau et al. 2023]; reward modeling [Adams\net al. 2022] to reduce data usage; and tuning only a small part of the models [Burkholz 2024;\nGiannou et al. 2023; Lu et al. 2022a]. On the opposite, EPPO uses little (binary, or more generally\nğœ…-ary) information that is coarse-grain (aggregated over an entire dataset and not on mini-batches)\nin order to tune a small pre-prompt (array of prompts from the demonstration set, typically, 2 to 16\nindices in [0, 999]). Table 1 summarizes the size and use of data of the different approaches listed\nabove to train and improve LLMs.\n  Indeed, let us consider in Algorithm 1 the simplest case where ğœ…= 2 (e.g., ğ´ğ‘™ğ‘”ğ‘œis a variant of\na (1+1)-ES), and let us compute the size of the feedback sent to the optimization algorithm ğ´ğ‘™ğ‘”ğ‘œ\nduring one run of EPPO. When ğœ…= 2, at each iteration, one comparison is performed by the\nğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’routine (Line 6), and there are ğ‘iterations (see Figure 2). Hence the entire data flow used\nas feedback is exactly ğ‘bits. The general case for ğœ…will be discussed in the forthcoming Section 3.2.\n  Furthermore, EPPO never makes any backward passes, which does bring a lot of information\nto the model, nor does it send data as mini-batches, which would also multiply the information\nintegrated in the model. Paradoxically, whereas classical deep learning methods try to avoid\nunderfitting by integrating as much information from the dataset as possible into the model, EPPO\nfocuses on decreasing overfitting by using a limited data flow.\n  In next Section 3.2, we will theoretically validate the use of such a limited data bandwidth in\nEPPO by deriving the resulting mathematical bounds on the generalization error, while the results\nof Section 4 will provide its experimental validation.\n\n3.2  Mathematics of overfitting\nFollowing [Fournier and Teytaud 2011], this section derives proven upper bounds on the general-\nization error of EPPO. Let us first define what we mean by generalization error.\n\nNotations: For a given ğœ€> 0, let us denote ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘) the event that the difference between the\nempirical risk (loss computed on the whole training set) and the risk in generalization (expected\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                             7\n\n\n\n\n\nFig. 2. EPPO from the information-theoretical point of view in the case ğœ…= 2: The loop is used ğ‘times, and 1\nbit (the result of a comparison) is returned and used at each iteration. The total information flow in the top\nleftarrow is hence ğ‘bits.\n\nTable 1. Information used for various approaches. For LLM training (resp. fine-tuning), ğ‘›(resp ğ‘›â€²) is the\nnumber of prompts in the training (resp. fine-tuning) set, ğ‘ their typical length, ğ‘–the typical input length, ğ‘œ\nthe typical output length (ğ‘–and ğ‘œin the dozens or hundreds of scalars); RLHF, uses tens of thousands of\nhuman answers times the input length; For EPPO (Algorithm 1), ğ‘is the budget, and ğœ…the â€œpopulationâ€ size\nof the optimizer: In the case ğœ…= 2, these ğ‘scalars are Booleans.\n\n\n    Context         Type of             Data           Orders of      Generalization\n                        data            volume         magnitude        bound\n  LLM training   Complete Seq2Seq    ğ‘›Ã— ğ‘ chars         â‰ƒ1012                  -\n   Fine-tuning   Complete Seq2Seq   ğ‘›â€² Ã— (ğ‘–+ ğ‘œ) chars  â‰ƒ1ğ‘’6 for GSM8K           -\n                     Questions    RLHF                             ğ‘›â€² Ã— (ğ‘–+ 1) chars    frequently 106\n               and selected answer                                                           -\n                     Scores of ğ‘                                  ğ›¿ğ¸ğ‘ƒğ‘ƒğ‘‚= ğ‘‚(ğœ…ğ‘)    EPPO                              ğ‘scalars         â‰ƒ100\n                   pre-prompts                                           (Equation (7))\n\n\nloss on the full space, of course unreachable) is greater than ğœ€:\n  ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘) = {|(ğ‘’ğ‘¥ğ‘ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘™ğ‘œğ‘ ğ‘ ğ‘œğ‘“ğ‘ğ‘–ğ‘›ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘›) âˆ’(ğ‘’ğ‘šğ‘ğ‘–ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ğ‘™ğ‘œğ‘ ğ‘ ğ‘œğ‘“ğ‘ğ‘œğ‘›ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡)| > ğœ€}\n  We are interested here in bounding, for any ğ‘returned by EPPO, the probability of this event,\naka a large deviation of the generalization error of pre-prompt ğ‘.\n\nHypothesis: We assume that for all ğœ€> 0, there exists ğ›¿> 0 such that for all ğ‘âˆˆDğ‘ ,\n                                       ğ‘ƒ(ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘)) â‰¤ğ›¿.                                            (1)\nThis bound is non-trivial only if ğ›¿< 1, and further results will depend on ğ›¿.\n  An example of such a deviation bound is the case where the evaluation of a pre-prompt is based\non averaging ğ‘‡distinct, independent answers of the model, where ğ‘‡is the size of the training\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n8                                     Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\ndataset T: For each question, the answer receives a score in [0, 1]. Then by Hoeffdingâ€™s inequality,\nwe have\n                  ğ›¿= ğ‘ƒ(ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘)) â‰¤2 exp(âˆ’2ğ‘‡ğœ€2).                                      (2)\n\nBut we do not need this specific bound and only suppose here Equation (1).\n\nGoals: The challenge is to extend Equation (1) to one of the following settings:\n   â€¢ for the pre-prompt ğ‘Ÿrecommended by EPPO, which is unknown a priori (in the same spirit as\n     [Vapnik 1995]), we look for some ğ›¿ğ¸ğ‘ƒğ‘ƒğ‘‚> 0 such that:\n                                      ğ‘ƒ(ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘Ÿ)) â‰¤ğ›¿ğ¸ğ‘ƒğ‘ƒğ‘‚.                                          (3)\n     The difficulty is that ğ‘Ÿis not known in advance, so Equation (1) does not immediately\n      provide Equation (3).\n   â€¢ A useful extension consists in bounding the risk that at least one of the pre-prompts considered\n      in Algorithm 1 (all those stored in ğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’) has a deviation of more than ğœ€by some ğ›¿ğ‘¢ğ‘›ğ‘–ğ‘“> 0,\n         i.e.,\n                           ğ‘ƒ(âˆƒğ‘âˆˆğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’,ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘)) â‰¤ğ›¿ğ‘¢ğ‘›ğ‘–ğ‘“.                                  (4)\n     The advantage of this extension is that we not only guarantee a deviation for the final\n     recommended ğ‘Ÿbut also for all pre-prompts considered during the run. And because the\n      optimization algorithm ğ´ğ‘™ğ‘”ğ‘œand the tool ğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’use ğœ…Ã—ğ‘different pre-prompts, which are\n      not chosen a priori, Equation (4) can not be immediately derived by a Bonferroni correction.\nPreliminary:\n   â€¢ Everything else being equal (training and demonstration sets, ğ´ğ‘™ğ‘”ğ‘œand budget ğ‘in Algo-\n      rithm 1), a given ğ‘only depends on the random seed ğœ”that was used in the run of Algorithm 1\n      that returned it. We will hence start reasoning for a given ğœ”, then generalize to all possible ğœ”\n      to get bounds on the generalization of EPPO as a whole.\n   â€¢ The key feature here is that the function ğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’() (Line 6 in Algorithm 1) is based on an\n      evaluation of the pre-prompts and returns an index in {1, . . . ,ğœ…}:\n                             ğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’(. . . ) âˆˆ{1, . . . ,ğœ…}.                                     (5)\n\nProof: We now mathematically prove, following [Videau et al. 2024], that, in terms of statistical\nrisk, this leads to a bound on the generalization error.\n   First, consider a specific random seed ğœ”. We can apply the Bonferroni correction [Bonferroni\n1936]: Consider a list L of ğ‘€pre-prompts. Assuming Equation (1), the risk of deviation greater\nthan ğœ€for at least one of those pre-prompts is at most ğ‘€Â· ğ›¿instead of ğ›¿:\n                             ğ‘ƒ(âˆƒğ‘âˆˆL,ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘)) â‰¤ğ‘€ğ›¿.                                      (6)\n\n  Consider now the complete list L of pre-prompts that could be returned by Algorithm 1 (Line 10).\nğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’(. . . ) has only ğœ…possible values (Equation (5)). Hence the pre-prompt chosen at the end\nis in the list L of ğœ…ğ‘pre-prompts2 â€“ and this list only depends on the random seed ğœ”. Then,\napplying Equation (6) leads to\n                                       ğ‘ƒ(ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘Ÿ)) â‰¤ğœ…ğ‘ğ›¿,                                           (7)\nwhere ğ‘Ÿis the value returned by Algorithm 1. This solves our quest for an equation of the form\nof Equation (3): we have proved that EPPO returns a pre-prompt ğ‘with a limited generalization\nerror, i.e., the empirical evaluation should not be too far from the performance in generalization. â– \n\n\n2This is far smaller than the number of possible values for ğ‘Ÿ, which is  | D|ğ‘ (ğ‘ the size of the pre-prompts), typically 100012.\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                             9\n\n\nDiscussion: One should be aware that a lower bound on the generalization error of an algorithm\nhas no connection with its accuracy. Indeed, consider, for instance, the case of pure random search,\nin which a randomly drawn few-shot (made of ğ‘ examples) is drawn from Dğ‘ at each iteration,\nregardless of the past. ğ´ğ‘™ğ‘”ğ‘œ.ğ‘ğ‘ ğ‘˜() always returns one independently randomly drawn pre-prompt,\nand ğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’(. . .) is not called at all: We can consider that ğœ…= 1. However, for the last step, all\nthose randomly drawn pre-prompts are compared once, and the best one is returned: For that step,\nğœ…= ğ‘there. Therefore, the total number of possible recommended pre-prompts is 1ğ‘âˆ’1 Ã—ğ‘, hence ğ‘,\nfor Random Search:\n                                       ğ‘ƒ(ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘Ÿ)) â‰¤ğ‘ğ›¿,                                            (8)\nCompare this to ğœ…ğ‘in Equation (7): EPPO with Random Search overfits less than EPPO with\nevolutionary search. On the other hand, of course, it performs worse than evolutionary based\nvariants in terms of finding empirically good pre-prompts. We discuss this in Section 4.4: A\nstraightforward next step (discussed in [Videau et al. 2024]) would be to search for a trade-off\nbetween the search effectiveness of evolutionary algorithms and the generalization ability of\nRandom Search by using evolution with a much larger population.\n  We now consider two straightforward extensions of Equation (7).\n\nExtension with stochasticity: We derive the previous results in the context of a fixed random\nseed ğœ”. If the algorithm is randomized (i.e., ğœ”is actually randomly chosen), this bound is still valid:\nThe risk of deviation by more than ğœ€(averaged over all these random outcomes) is the average\nof the different risks corresponding to the different random outcomes (in mathematical terms,\nğ‘ƒ(ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘Ÿ)) = Eğœ”ğ‘ƒ(ğ‘’ğ‘Ÿğ‘Ÿğœ€(ğ‘Ÿ)|ğœ”)), and the same bound applies to the average.\n\nBound over more pre-prompts: Instead of proving a bound valid for all ğ‘in the possibly recom-\nmended pre-prompts (and therefore valid for the ğ‘Ÿreturned by Algorithm 1), we could consider\na bound valid uniformly over all ğ‘considered in all the ğ¶ğ‘œğ‘šğ‘ğ‘ğ‘Ÿğ‘’(. . . ) of the algorithm, i.e., in\nall ğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’. This implies that the optimization algorithm works on correctly estimated few-shot.\nGiven that our bound is computed by uniformity over all possible outcomes âˆˆL, the extension to\nall ğ´ğ‘Ÿğ‘â„ğ‘–ğ‘£ğ‘’is straightforward: We need to slightly modify the constant in Section 3.2 and replace\nğœ…ğ‘by ğœ…ğ‘+1. This solves our quest for an equation of the form Equation (4).              â– \n\n  We observe that the risk of a deviation greater than ğœ€is limited by ğœ…ğ‘Â· ğ›¿, which increases when\nthe computational budget ğ‘increases. Also, the risk decreases with ğ›¿, i.e., if the precision for each\nmodel individually increases, e.g., if the training set size ğ‘‡increases: Overall, assuming a bound as\nin Equation (2), we get a risk as in Equation (9), or, in other words, a precision ğœ€as in Equation (10):\n                                 ğ›¿  â‰¤  2ğœ…ğ‘exp(âˆ’2ğ‘‡ğœ€2),                                        (9)\n                                      âˆšï¸‚\n                                         ğœ€  â‰¤    âˆ’ln(ğœ…âˆ’ğ‘ğ›¿/2) .                                   (10)\n                                              2ğ‘‡\n\n4  EXPERIMENTAL RESULTS\n\nThis section presents the experimental validation of EPPO on several mathematical reasoning tasks.\n\n\n4.1  Experimental settings\n\nDatasets: EPPO is evaluated on four mathematical reasoning tasks. Each task comes from a different\ndataset and they have heterogeneous complexity. The datasets used here are GSM8k [Cobbe\net al. 2021], SVAMP [Patel et al. 2021], MathQA [Amini et al. 2019], and MATH [Hendrycks et al.\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n10                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\nTable 2. LLaMA2-70B: 3 baselines at the top, then 2-shots, 4-shots and 8-shots optimization and the best\nprompt of the 7B transferred to 70B. We observe that (i) 4-shots is best (ii) transferring the preprompt from\n7b does work, though few-shots optimization is better. For MathQA, without parentheses, the exact match\nfor the numerical answer and choice of the right option between parentheses.\n\n\n                              GSM8k        SVAMP          MathQA         MATH\n          Prompt Type\n                                  # Tokens   Acc.   # Tokens   Acc.   # Tokens      Acc.      # Tokens   Acc.\n\n            CoT                  762      56.8     762      73.1     602      25.4 (35.8)      671      13.9\n           Long CoT               2838      66.5     2838      74.7     2019      21.9 (29.9)     1435      14.2\n           Resprompt               2470      65.7     2470      71.0     1367       9.9 (13.0)          -           -\n\n          EPPO 2                 412      62.5     412      75.0     406      26.6 (35.6)      843      14.0\n          EPPO 4                1297     68.2     1297     78.3     906     34.1 (41.0)     1587     15.4\n          EPPO 8                1651      67.6     1651      77.3     1651      30.5 (39.1)     1634      14.7\n           7B â†’70B                809      64.7     809      76.4     888      27.2 (31.4)      641      14.3\n\n\n\n2021]. GSM8k and SVAMP focus on real-world mathematical problems. While GSM8k offers a\ncomprehensive training set, SVAMP does not have a corresponding training set. To tackle this\nissue, we will test the transferability of EPPO by evaluating on SVAMP the same few-shot pre-\nprompts found for GSM8k. MathQA offers a wide range of mathematical problems of varying\ndifficulties. In contrast, MATH poses the greatest challenge, targeting advanced mathematics\ntypically encountered in late high school and beyond. Also, notice that MathQA has very noisy\nannotations, and hence provides a test for the resilience of optimization strategies. For an in-depth\nunderstanding of each of these datasets, including specifics about their training, demonstration,\nand test components, please refer to Table 7 in the Appendix.\nEvaluation metrics: For each dataset, our primary metric is the exact match (EM) with the\nground truth. Specifically, we focus on the final output generated by the model, which is typically\na numerical value, and we compare it directly with the ground truth. In the case of the MathQA\ndataset, which involves multiple-choice questions, we use two distinct EM scores: one for the\nnumerical answer and another for the correct multiple-choice option (identified simply by its letter).\nThis dual-scoring approach is particularly beneficial for evaluating smaller/less capable models\nthat do not effectively correlate with the numerical answer.\nLanguage models: We use the LLaMA family of models (i.e., LLaMA2 [Touvron et al. 2023]).\nA key advantage of these models is their open-source nature, which significantly helps conduct\ncost-effective and reproducible research.\nBaselines: The baselines used for comparison are: (a) the original CoT prompt [Wei et al. 2022b],\nas detailed in the publication and sourced from their appendix; (b) the Long CoT prompt [Fu et al.\n2022] that includes examples with more steps, targeting complex reasoning tasks. These prompts\nwere obtained from the official GitHub repository; and (c) the Resprompt approach [Jiang et al.\n2023] that suggests following a reasoning graph and adding residual connections between the\nnodes of the graph. This technique is designed to reduce errors in multi-step reasoning tasks by\nLLMs. We sourced the Resprompt prompts from the paper appendix.\n  We do not include in-context learning such as [Gupta et al. 2023; Rubin et al. 2022; SU et al. 2023;\nWu et al. 2022; Ye et al. 2023; Zhang et al. 2023a], which optimize a pre-prompt on a per-example\nbasis. Likewise, they are usually tested on heterogeneous and older models without considering\nmathematical reasoning tasks. Moreover, their source codes are barely available.\n  For MATH tasks, which are not covered by any of the baselines, we took the base CoT prompt\nfrom Minerva [Lewkowycz et al. 2022]. For the Long CoT, we created prompts using the most\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            11\n\n\nTable 3. LLaMA2-7B: 3 baselines at the top, then 2-shots, 4-shots, and 8-shots optimization, and the best\npre-prompt of the 70B transferred to 7B. We observe that (i) 4-shots is best, (ii) no improvement when\ntransferring the pre-prompt from 70B to 7B.\n\n\n                              GSM8k        SVAMP          MathQA         MATH\n         Pre-prompt Type\n                                  # Tokens   Acc.   # Tokens   Acc.   # Tokens      Acc.      # Tokens   Acc.\n\n            CoT                  762      14.1     762      39.6     602      10.1 (19.8)      671       3.3\n           Long CoT               2838     17.5     2838      35.9     2019       9.4 (17.5)      1435      3.0\n           Resprompt               2470      16.9     2470      30.4     1367       6.7 (13.8)          -           -\n\n         EPPO 2-shot               582      16.4     582      39.6     442       9.9 (17.6)      460      4.5\n         EPPO 4-shot               809      15.5     809     41.6     888     14.7 (19.4)     641       3.9\n         EPPO 8-shot              2088      16.5     2088      39.4     1945      12.3 (16.7)     1254      3.7\n       EPPO 70B â†’7B             412      12.4     412      40.5     888       8.5 (14.5)      843       2.6\n\n\n\ndetailed and complex answers from the training set. All these baselines prompt are 8-shot for\nGSM8k and SVAMP, 4-shot for MathQA and MATH. To ensure a fair comparison, we run all these\nbaseline prompts in our own codebase. Our scores are consistent with the original papers, except\nin the case of Resprompt for which we get worse results on MathQA.\nHyperparameters: For each task, except for SVAMP, which lacks a training set, we create a smaller\ntraining set containing roughly 500 samples by downsampling the original one, and assemble a\ndemonstration dataset of about 1000 samples, as described in Section 2.2. Precise details about each\ndataset can be found in Table 7 in the Appendix. In Algorithm 1, the optimization algorithm ğ´ğ‘™ğ‘”ğ‘œ\nis chosen among those described in Section 2.3, in their Nevergrad implementation [Rapin and\nTeytaud 2018]; The budget ğ‘is set to 100 for both the LLaMA2-7B and LLaMA2-70B models; The\nnumber ğ‘ of prompts from the demonstration set D is taken in {2, 4, 8, 12, 16}, and such pre-prompts\nare denoted for short â€œğ‘ -shot pre-promptsâ€.\n\n4.2  Results on Few-Shot Optimization\nOur results, detailed in Tables 2 and 3, showcase the performance of EPPO across different tasks\nfor both the LLaMA2-7B and LLaMA2-70B models. We evaluate our approach on different ğ‘ -shot\nsetup with ğ‘ âˆˆ2, 4, 8, to understand its effectiveness in various contexts. As one can see in both\ntables, the results consistently indicate that EPPO outperforms all baselines. This improvement\nis noteworthy not only in terms of accuracy but also in terms of efficiency, as indicated by the\nsimilar or even reduced number of tokens in the few-shot pre-prompts used. This reduced number\nof tokens is directly reflected in a reduced inference cost, both in memory and computation time.\nFurthermore, an important aspect of their results is the versatility of EPPO. It is effective across\nmodels of different sizes, from the smaller LLaMA2-7B to the larger LLaMA2-70B. Interestingly, we\nobserve more pronounced gains in performance with the larger model. This suggests that EPPO\nscales well and can leverage the increased capacity of larger models.\nPer example complexity results: This analysis, detailed in Figure 7, examines the number of\ncorrect solutions across different levels of problem complexity for LLaMA2-70B: EPPO consistently\noutperforms baseline methods across numerous complexity levels. This is particularly noteworthy\nfor more complex problems, where EPPO delivers better results, even though it was not specifically\ndesigned for such challenging scenarios. The most pronounced improvements are observed for\nthe GSM8k and MATH tasks, especially on multi-step examples, which are typically harder to\nenhance [Fu et al. 2022]. These gains are achieved without any deliberate focus on such difficult\nproblems during either the optimization process or within the demonstration dataset. This outcome\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n12                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\ndemonstrates the strong generalization ability of EPPO in adapting to and addressing complex\nchallenges without specifically targeting them beforehand.\nTask Transfer: As said, our approach to SVAMP, in the absence of a specific training set, involves\napplying few-shot pre-prompts optimized for GSM8k. This strategy leads to a notable and consistent\nimprovement over the existing baselines across all few-shot settings. Notably, while methods like\nResprompt, designed to enhance multi-step reasoning, struggle to adapt to the simpler SVAMP\ntask, EPPO demonstrates more flexibility. It successfully transfers to SVAMP and simultaneously\nimproves performance in multi-step reasoning tasks. Further details on task transferability for\nLLaMA2-70B, using GSM8k optimized 4-shot pre-prompts, are available in Table 9 in the appendix.\nModel transfer: We investigate the transfer of optimized pre-prompts between two models,\nLLaMA2-7B and LLaMA2-70B, on the same task. We optimize pre-prompts with EPPO on one of\nthese LLMs, and test it on the other one. For LLaMA2-70B, we use optimized 4-shot pre-prompts,\nand for LLaMA2-7B, we choose the best from either 4-shot or 2-shot pre-prompts. The results of\nthis experiment are presented in Tables 2 and 3. Table 3 shows the performance on the LLaMA2-7B\nmodel using pre-prompts optimised for LLaMA2-70B (indicated as 70b â†’7b), and Table 2 displays\nthe results on the LLaMA2-70B model of pre-prompts optimised for LLaMA2-7B (indicated as 7b\nâ†’70b). From these results, we observe that pre-prompts optimized for the larger LLaMA2-70B\nmodel do not effectively transfer to the smaller LLaMA2-7B model. Interestingly, the opposite\nscenario â€“ transferring pre-prompts from LLaMA2-7B to LLaMA2-70B â€“ resulted in a substantial\nperformance increase, surpassing some of the baselines. Nevertheless, pre-prompts that were\nspecifically optimized for one model always outperform pre-prompts that are transferred from the\nother model.\nEffect of the number of shots on performance: Each model is tested with few-shot optimized\npre-prompts containing 2, 4, and 8 shots. We observe that using more than four examples does not\nlead to better results in the downsampled case. In fact, it appears that employing more than four\nexamples in a pre-prompt can slightly but steadily degrade the performance of the LLM. This is a\ncounter-intuitive finding, as one might intuitively assume that providing more examples in the\nfew-shot pre-prompt would lead to broader coverage of the task, thereby improving the LLMâ€™s\nability to generate task-specific responses: hence the investigation below in terms of overfitting.\nEffect of the number of shots on overfitting. In order to understand why we get better results\nwith a small number of shots, whereas 8 shots are usual, we investigate the performance of LLaMA2-\n70B after running EPPO on the training set using 2, 4, and 8 shots for each task, scrutinizing the\noptimizer logs of the train error along optimization. Our findings are illustrated in Figure 3, showing\nstatistics gathered over the full EPPO run, and in Table 5 presenting test errors. There is a noticeable\nand consistent performance gap between the 2-shot prompts and those with more than 4 shots\nin Figure 3. This indicates that increasing the number of shots from 2 to 4 yields most of the\n(train error) benefits. Also, with an increase in the number of shots, there was a corresponding\nincrease in the median performance (median over the EPPO run) and in the performance of the\nworst-performing prompts (typically randomly generated at the beginning, before overfitting can\ntake place). When considering the test error, Table 5 and Figure 4 show that increasing the number\nof shots, in the downsampled case, leads to a clear overfitting. We conclude that the problem of large\nfew-shot is an overfitting issue, particularly visible in the downsampled case. This is confirmed\nby Table 6, which shows good results for larger few-shot when EPPO uses the full training set\n(without downsampling).\n\n4.3  Experiments on the downsampled GSM8k\n\nTable 4 presents a sanity-check comparison against pure Random Search of different optimization\nalgorithms used within EPPO, applied to the downsampled GSM8k training set (1/16 of the complete\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            13\n\n\n                                                                                                               dataset                      50                                                                                        GSM8k\n                                                                                             MathQA\n                                                                                      MATH\n\n                      40\n\n\n\n                     (%) 30\n                                    Match\n                                    Exact 20\n\n\n\n                      10\n\n\n\n                       0\n                             2        4        8        2        4        8        2        4        8\n                                                        number of shots k\n\n\nFig. 3. Comparison between 2-shot, 4-shot, and 8-shot: training score over the EPPO run for LLaMA2-70B:\nEach boxplot represents the loss values observed during the whole run. The numbers represent the percentage\nof exact matches on the training set. Typically, for each number of shots, the bottom part (low EM) corresponds\nto the beginning of the run, similar to random search, and the performance of these initial few-shot increases\ngreatly with the number of shots.\n\n\nTable 4. Comparison between exact match accuracies (without SC) obtained with different discrete opti-\nmization methods with budget 100 on the training set. We observe a significant performance improvement\ncompared to random search. Red for the 2 worst performances. Additional results in Table 5.\n\n\n                                        LLaMA2-7B                            LLaMA2-70B\n\n                                          GSM8k num-shots\n\n     Optimizer              8-shot                  4-shot                  2-shot                  8-shot\n\n    RandomSearch           13.0                   13.0                   13.3                   46.6\n LogNormal(1 + 1)           12.8                   13.5                   12.8                   45.1\n    DoubleFastGA            15.0                     14.8                     14.3                     51.0\n    Portfolio(1 + 1)            15.5                     14.5                   14.8                     49.7\n   Discrete(1 + 1)           15.8                   15.5                   14.8                   51.7\n     Lengler(1 + 1)            14.8                     14.8                   14.8                     50.3\n\n\nset, see Section 2.2 for details). We observe a gap between the train (Table 4) and test (Table 6, Table 5,\nFigure 4) scores of Llama-70B on GSM8k, primarily due to the downsampling procedure, which leads\nto the different difficulty levels being more uniformly represented in the training set than in the test\nset. As said, we use the algorithms described in Section 2.3 in their Nevergrad implementation [Rapin\nand Teytaud 2018]: LogNormal [Kruisselbrink et al. 2011], DoubleFastGA [Doerr et al. 2017],\nPortfolio [Dang and Lehre 2016], the basic Discrete ES, and Lengler [Einarsson et al. 2019]; We\nrefer to [Doerr et al. 2019] for a theoretical analysis including the classical Discrete (1 + 1)-ES.\n  Most algorithms (except LogNormal) clearly outperform random search (in red), while Discrete\n(1+1)-ES performs best for all small-shot sizes (in bold): Given the high computational cost associated\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n14                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\n\n\n\n                  4, 8*, 12 or 16 shots,                                   4, 8*, 12 or 16 shots\n       Downsampled (1/16th) GSM8k                             Full GSM8k\n\n\nFig. 4. Results of Llama 70B, Exact Matches on the test set without improvement by Self-Consistency. Legends\nare the Nevergrad algorithm names pre-pended with \"s#\" (*ğ‘ = 8 is the default few-shot size ğ‘ , when not\nmentioned). For all algorithms, ğœ…= 2, so the x-axis is exactly the number of binary comparisons between\npre-prompts, i.e., budget ğ‘in Algorithm 1. Figure 5 presents the results of the transfer to SVAMP.\nLeft: Downsampled GSM8k. For ğ‘ âˆˆ{12, 16}, we observe a clear overfitting (confirmed with more ex-\nperiments in Figure 12): The test performance decreases when the budget increases, consistently with\nthe mathematical analysis. We also observe no overfitting for Random Search, which grows steadily with\nğ‘ âˆˆ{4, 8, 12, 16}. Note that the longest run used 16 GPUs during â‰ƒ30 hours.\nRight: Full GSM8k. As only one bit of information is used per iteration (comparison with the best so far),\nwe observe no overfitting until budget 150. Note that the longest run here used 160 GPUs during â‰ƒ48 hours.\n\n\nwith an LLM inference, choosing an efficient optimizer plays an important role in managing\ncomputational costs more effectively.\n\n4.4  Extension to the full training set: reducing the overfitting\nLiu et al. [2023] pointed out how learning on the training set of GSM8k is hard due to the moderate\nsize. We investigate in the present section to which extent EPPO is relevant for the training set of\nGSM8k. We mathematically proved in Section 3.2 that the overfitting should be low thanks to the\nlimited access of EPPO to fine-grain data, but can be further limited by increasing the training set\nsize ğ‘‡(Section 3.2). Figure 4 presents experimental results validating these theoretical findinds, and\nextending this claim with results both on one-sixteenth of the training set (downsampled GSM8k\nas in Section 4.3) on the left and on the entire training set (though still using only a single scalar\nindicator per few-shot pre-prompt) on the right: We emphasize positive results on GSM8k with\npositive transfer on SVAMP with just 50 to 150 bits of information from the data.\n\nGeneralization error: Random search vs other methods: Whereas Table 4 shows poor results for\nrandom search (in terms of training error), we observe (Table 5, left) that in generalization, when we\nhave downsampled the GSM8k training set, overfitting matters more, and random search becomes\ncompetitive. This is consistent with the limited overfitting predicted by Equation (8).\n\n4.5  Combining with Self-Consistency\n\nIn the context of problem-solving with LLMs, multiple pathways often lead to a solution. Recogniz-\ning this, Wang et al. [2022] introduced the concept of self-consistency (SC) for LLMs. SC operates\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            15\n\n\n\n\n\nFig. 5. Transfer of pre-prompts optimized on full GSM8k (see caption of Figure 4-Right) to SVAMP. We\nobserve a good transfer in this context.\n\n\nTable 5. Random Search vs other optimizers, for budget ğ‘= 50 and 100. Bold for the best performances,\nred for worst. +/- denotes cases in which budget 100 outperforms (resp. underperforms) budget 50 (i.e., no\noverfitting). Observations: (1) the downsampled case is more prone to overfitting (much more â€œ-â€) (2) Random\nSearch never overfits (consistently with predictions in Section 3.2) whereas other methods do, to the point\nrandom search is sometimes the best in the downsampled case (3) few-shot sizes 12, 16 are more prone to\noverfitting than 4, 8. The few missing values are on-going computations at the time of submission.\n\n\n                              Downsampled GSM8k                   Full GSM8k\n          Few-shot size â†’      4        8       12       16        8       12       16\n           Algorithm â†“\n                        Budget 50: no overfitting & random search is weak\n          RandomSearch     64.13    64.68    64.13    62.16     66.41     66.33    60.72\n               Portfolio         64.80     64.80     65.57    65.54     67.62    66.07\n             Disc1+1          65.50    64.70     66.94     63.00    67.70    67.47\n          DiscLengler1+1      66.71     66.71    67.70    65.04    65.73    66.98\n        RecombDiscLengler   66.75   66.75    65.45               65.88     66.48    63.45\n                   Budget 100: Overfitting matters & random search is competitive\n                                         in the downsampled case.\n          RandomSearch     66.33+   65.76+   66.33+   64.44+    65.95    67.09   62.69+\n               Portfolio       67.46+  67.46+   65.23-   62.62-   68.00+   67.36+\n             Disc1+1        66.18+  65.30+   66.07-   66.18+    67.53-   67.53+    65.69\n          DiscLengler1+1     66.77+   66.77+    65.95-    62.01-             67.09+\n        RecombDiscLengler   66.43-    66.43-    65.40-             67.85+   67.39+   65.95+\n\n\n\nby generating a range of potential solutions with the same LLM. Diverse prompts for the problem\nat hand are generated by adding some random parts and controlling the level of randomness with a\nspecific parameter ğœcalled temperature. Each of these generated pathways leads to diverse answers\nthat are then aggregated by a majority voting [Wang et al. 2022], effectively using the collective\noutput of the model to determine the most likely correct answer.\n  We examine now the interaction between self-consistency, known for improving LLM perfor-\nmance, and pre-prompt optimization. The aim is to ascertain if the enhancements seen in single-path\ndecoding also carry on to a self-consistency framework. Indeed, results presented in Table 6 show\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n16                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\nTable 6. Self-consistency improvement over optimized prompt and CoT baseline: error in test. The model\ntested here is Llama-2-70B with SC over 8 generations (maj@8) and temperature ğœ= 0.6.\n\n\n                       Prompt Type      GSM8k  SVAMP   MathQA  MATH\n\n                        Cot                     56.8       73.1     25.4 (35.8)     13.9\n                    CoT + SC               68.3       80.0     35.8 (44.9)     19.1\n                                    Downsampled training set\n                    EPPO 4-shot           68.2       78.3      34.1(41.0)     15.4\n                    EPPO 4-shot + SC      76.9      85.7    46.7 (50.4)    21.3\n                                                        Full training set\n                    EPPO 4-shot + SC      77.1       82.4            -             -\n                    EPPO 8-shot + SC     79.1      85.5            -             -\n                    EPPO 12-shot + SC     78.8       85.4            -             -\n\n\n\n\n\n                            GSM8k                                                      MathQA                                                  MATH\n        600                                                Prompt                                                                         Prompt                   1400                                                Prompt\n                                                        CoT                                                                      CoT                                                                      CoT\n        500                                               LongOptimCoT4-shots               800                                               LongOptimCoT4-shots              1200                                               LongOptimCoT4-shots\n        400                                                                       600                                                                      1000\n          Count 300                                                                                                                               Count                                                                                                                                    Count 800\n                                                                                  400                                                                       600\n        200\n                                                                                                                                                            400\n                                                                                  200\n        100                                                                                                                                                 200\n\n          0   0.0     2.5     5.0     7.5    10.0    12.5    15.0    17.5                    0   0.0     2.5     5.0     7.5    10.0    12.5    15.0    17.5                    0   0.0     2.5     5.0     7.5    10.0    12.5    15.0    17.5\n                           Number of steps in generation                                               Number of steps in generation                                               Number of steps in generation\n\n\nFig. 6. Comparison between the number of steps in the CoT, Long Cot, and EPPO for GSM8k (4-shot) and\nMathQA (8-shot) for LLaMA2-70B. The variance is higher for EPPO, with few-shot resulting in more steps in\nthe output.\n\n\n\na clear benefit transferred from greedy decoding to self-consistency, at least with a temperature of\n0.6 (a robust reasonable value, though sensitivity analysis remains to be done).\n\n5  INSIGHT ANALYSES\nEffect of few-shot on LLMs generation: Figure 6 illustrate the number of steps generated by\nLLaMA2-70B across different tasks. As the number of steps to derive a solution can be hard to define,\nwe use a simple method as a proxy: The number of steps is the number of sentences generated\nby the LLM to derive the answers. Contrary to baselines CoT and Long CoT, few-shot found by\nEPPO provide more diverse outputs in terms of number of steps. In particular, optimized few-shot\npre-prompts tend to provide answers containing more steps. This extended number of steps might\nhelp to give better answers to multi-step examples (Figure 7).\nOperations on optimized pre-prompts: Here, we evaluate the robustness of the optimized few-\nshot prompts under various transformations: example permutations, removals, and combinations.\nThrough these experiments, we seek to gain deeper insights into how a good few-shot pre-prompt\nis constructed. We perform this evaluation on GSM8k and MathQA. In this case, MathQA has been\ndownsampled to 1000 samples to accelerate the evaluation.\nRobustness against permutation: Figure 8 presents an analysis of how the performance fluctuates\nwith changes in the order of few-shot prompts. To conduct this analysis, after running EPPO, we\nrandomly modified the order of the prompts in the optimized pre-prompts ten times and observed\nthe impact on the model performance in terms of Exact Matches.\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            17\n\n\n\n\n\nFig. 7. Exact Matches on LLaMA2-70B for CoT, Long Cot, and EPPO. X-axis: gold number of steps, i.e., number\nof steps in the ground truth or Level of the example for MATH: The x-axis is hence a measure of the complexity\nof problems. EPPO performs best in most cases.\n\n\n\n\n\n                            (a) LLaMA2-70B                              (b) LLaMA2-7B\n\n\nFig. 8. Differences of Exact Matches on LLaMA2-7B and LLaMA2-70B between the original pre-prompt\nreturned by EPPO and 10 randomly permuted variants: The order is not very important.\n\n\n  We found that such random permutations of the prompts can lead to minor variations in the\nresults, typically around one percent across all benchmarks and shots. This demonstrates the\nresilience of the method.\n8-shot are more resilient to prompt removal thanks to redundancy. To assess the sensitivity\nof optimized pre-prompts to removal, we randomly remove prompt examples from the few-shot pre-\nprompts (Figure 9 for LLaMA2-70B and Figure 10 for LLaMA2-7B). The following pattern is observed\nin both cases: High-performing 4-shot pre-prompts lead to a significant drop in performance when\nsome prompts are removed, as shown in Figure 9-(b) and -(d). In contrast, 8-shot pre-prompts,\nwhich initially perform almost on par with 4-shot, demonstrate a higher tolerance to removal.\nThese pre-prompts can be condensed in 6 or 7 shots without a notable decrease in performance, as\nindicated in Figure 9-(a) and -(c). This suggests a redundancy within the elements of the 8-shot\npre-prompts. This phenomenon is also visible in the MathQA dataset, despite the multiplicity of\ncategories of questions.\nComposition of prompt: Adding examples from an already good few-shot prompt seems like a\nstraightforward path toward building an effective few-shot prompt. As depicted in Figure 11, two\nstrong 4-shot pre-prompts are added together to produce an 8-shot pre-prompt. Three combination\nstrategies have been tested, and compared to the baseline (performance of the best): (a) Best first:\nThe most effective of the two 4-shot pre-prompts is prepended first; (b) Best last: The worst of\nthe two 4-shot pre-prompts is prepended first; (c) Alternate: Examples of both pre-prompts are\ninterleaved. Our findings reveal a slight preference of the LLM for the â€œbest firstâ€ strategy, aligning\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n18                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\n                                                                               30.0                                  35.0\n (%)68                              (%)\n                                   68                                (%)27.5                              (%)32.5\n                                                                                                                     30.0 Match66                                                             Match66                                                     Match25.0                                                   Match   64\n Exact62                                                   Exact64                                                     Exact22.5                                                   Exact27.525.0\n       2   3   4   5   6   7   8          2        3        4            2   3   4   5   6   7   8            2        3        4\n            Number of shots                  Number of shots                   Number of shots                   Number of shots\n\n      (a) GSM8k 8-shot.         (b) GSM8k 4-shot.         (c) Math-QA 8-shot.       (d) Math-QA 4-shot.\n\nFig. 9. Exact Matches for EPPO on LLaMA2-70B after randomly reducing the optimized few-shot to ğ‘˜shots\n(x-axis). Box-plots represent the distribution under 10 random permutations (as in Figure 8). Reducing 8-shot\nto 6 or 7 does not decrease the performance, whereas any reduction does for 4-shot.\n\n              GSM8k                   GSM8k                     MathQA                   MathQA\n                                     18                                                              16\n   (%)16                              (%)                                 (%)12.5                              (%)                                                                                                     14\n     Match14                                                   Match16                                                   Match10.0                                                  Match12\n     Exact12                                                   Exact14                                                   Exact 7.5                                                  Exact10\n                                                                                    5.0                              8\n         2   3   4   5   6   7   8          2        3        4            2   3   4   5   6   7   8          2        3        4\n              Number of shots                  Number of shots                   Number of shots                  Number of shots\n\n       (a) GSM8k 8-shot.        (b) GSM8k 4-shot.        (c) Math-QA 8-shot.      (d) Math-QA 4-shot.\n\nFig. 10. Exact Matches for EPPO on LLaMA2-7B after randomly reducing the optimized few-shot to ğ‘˜shots\n(x-axis). Box-plots represent the distribution under 10 random permutations (as in Figure 8). Reducing 8-shot\nto 6 or 7 does not decrease the performance, whereas any reduction does for 4-shot.\n\n\n\n                                                   70                                                                                                   Alternate\n                                                                                                                                                         Best First\n                                                                                                                                                         Best Last\n                                                   60                                                                                                Baseline\n\n\n                                                   50\n\n                                                      (%)\n                                                                                          Match 40\n                                                                                          Exact 30\n\n                                                   20\n\n\n                                                   10\n\n\n                                                    0\n                                                               GSM8k-Optim 4                GSM8k-Res+Long                MathQA-Optim 4\n                                                                                                Prompt Combination\n\n\nFig. 11. On LLaMA2-70B, test of fusion of two non-intersecting 4-shot pre-prompts obtained by EPPO (see\ntext for description of legends). Overall, fusion is ineffective, and adding more shots is risky.\n\n\nwith previous results [Zhao et al. 2021]. However, we observe that naÃ¯vely combining pre-prompts\nusually degrades performance.\nWhat constitutes a good pre-prompt? We observe that chosen examples (see â€œBest Few-Shot\nPromptâ€ in the appendix) frequently include: (a) Long, multi-step examples with long and intricate\nquestions. This is consistent with some classical heuristics, such as Long CoT [Fu et al. 2022].\n(b) Examples which link previous steps, especially the quantities, and start by stating very literally\neach element of the question; (c) Examples which are effective while being buggy in terms of\njustification. We observe that the style of answer matters more than its correctness. Examples of\neffective incorrect rationales can be found in the MATH task few-shot. However, these heuristics\nappear to be less effective as the difficulty of the task increases, as for MATH.\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            19\n\n\n6  DISCUSSION\n\nComputational cost. To show the effectiveness of EPPO and explore further the space of examples\nselection in few-shot pre-prompting, we use a relatively large demonstration dataset of around 1000\nsamples. Still, the computational cost of EPPO is reasonable (on the downsampled GSM8k, â‰ˆ3h of\n64 V100 GPU for LLaMA2-70B, less than one hour for LLaMA2-7B with float16 precision and a\nbudget of 50) compared to that of training LLaMA2. Our method is tractable, in particular thanks\nto a well-chosen optimization method. Moreover, it has a much smaller memory footprint than\napproaches based on backprop (i.e., fine-tuning), as there is no backward pass involved. Moreover,\na few-shot pre-prompt provided by humans can be used as an initial point for the optimization\nalgorithm.\nRequired data. EPPO also requires less data than fine-tuning: For instance, GSM8k lacks sufficient\ndata to do a classical fine-tuning [Liu et al. 2023; Ni et al. 2023] even for moderate sized LLMs (â‰ˆ1B).\nAlso, EPPO is less likely to overfit on a specific task, as indicated by the information-theoretic\ngeneralization bounds (Section 3.2), independently of the model size: We get positive results even\nwith a 70B model. Fine-grained token-level outputs are here unnecessary, and indeed increase the\nrisk of overfitting. For instance, with GSM8K, only the final result is used, unlike in fine-tuning\nmethods, where the detailed reasoning process is needed.\nVersatility. We have demonstrated the efficiency of EPPO on multiple datasets with various\nlevels of difficulty and demonstration datasets setup. In particular, for MathQA, the demonstration\ndataset is noisy, frequently with false, badly explained or incomplete rationales. This particular case\ndemonstrates the robustness of EPPO, which nevertheless found effective few-shot preprompts.\nFinding the data for our approach is feasible: When using a dataset as ground truth, our sources\nare the training data and the answers by LLaMA2 itself, so human intervention is limited. Also,\nour approach could be entirely integrated in an automatic A/B testing scenario. Last, EPPO works\nwhen heuristic methods used by humans do not work.\nCombination with other methods. Finally, our improvement can be combined with Self Consis-\ntency [Wang et al. 2022], and the improvements are additive: The simple and cheap EPPO approach\ncould also be added on top of other methods such as fine-tuning using augmented data by the\nLLM [Pang et al. 2024; Xu et al. 2023; Yu et al. 2023], bootstrapping from output of the LLM [Bai\net al. 2022; Huang et al. 2023; Pang et al. 2024].\n\n\n7  RELATED WORKS\n\nIn-context learning has emerged as a way to adapt LLMs to a task just from few-shot examples. This\nmethod is particularly effective as the scale of the models increases. Essentially, it allows LLMs to\nadjust to new tasks without modifying their underlying parameters [Brown et al. 2020; Hoffmann\net al. 2022; Rae et al. 2021]). As LLMs continue to grow, they show improved performance across a\nrange of tasks and exhibit the development of new skills, a phenomenon referred to as â€œemergent\nabilityâ€ [Wei et al. 2022a]. At the forefront of this area is the chain-of-thoughts (CoT) reasoning\napproach [Wei et al. 2022b]. As opposed to some forms of in-context learning, CoT modifies the\nprompt statically for a specific downstream task (e.g. mathematical reasoning) instead of on a\nper-example basis: Our work fits this â€œper taskâ€ framework and differs from â€œper-exampleâ€ works\nsuch as [Gupta et al. 2023; Rubin et al. 2022; SU et al. 2023; Wu et al. 2022; Ye et al. 2023; Zhang\net al. 2023a]. This method, which prompts LLMs to articulate intermediate reasoning steps, has\nbeen refined through different techniques [Fu et al. 2022; Jiang et al. 2023; Lewkowycz et al. 2022;\nWang et al. 2022]. Despite its effectiveness, CoT prompting often faces challenges with complex\nmulti-step tasks [Fu et al. 2022; Jiang et al. 2023; Zhou et al. 2022]. Various approaches have been\nexplored to handle this challenge. For instance, Wang et al. [2022] introduced self-consistency,\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n20                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\nwhich involves aggregating LLMsâ€™ reasoning across multiple reasoning paths. Zhou et al. [2022]\nproposed a â€œleast to mostâ€ prompting strategy, which breaks down problems into sub-problems\nand solves them sequentially. Jiang et al. [2023] developed Residual prompting, which follows a\nreasoning graph and incorporates residual connections into the prompt design. Finally, [Gao et al.\n2023] introduced PAL, a method that prompts LLMs to generate programs that solve the problem,\nwith the solution obtained by executing these programs.\nThere is a variety of methods regarding the selection of examples for the prompts. This task can be\nvery challenging due to the instability. Multiple works have shown that the performance is sensitive\nto prompt, task, dataset, and model changes [Lu et al. 2021; Zhao et al. 2021]. This sensitivity\ncontributes to the complexity of prompt engineering, making it a somewhat arduous task. In\nparticular, prompt engineering for LLMs often relies on community-wide collective trial and error\neffort (there is even a prompt marketplace named PromptBase). Despite these challenges, a line\nof work is to address this issue by introducing retrieval in few-shot learning [Liu et al. 2021; Wu\net al. 2023; Zhang et al. 2023b]. In this context, each few-shot examples are retrieved dynamically\nat test time for each presented example, meaning that a particular CoT is retrieved following some\nsimilarity measure on the test example and retrieved few-shot CoT: Our work shows, however, that\na retrieval performed once for the task is effective, robust and transferable.\n  EPPO focuses on pre-prompt optimization by example prompt selection for a given task and on\nmulti-step reasoning, searching in large datasets to demonstrate diverse ways of answering the\nquestions. Orthogonal to this approach, several methods [Pang et al. 2024; Sessa et al. 2024; Xu et al.\n2023; Yu et al. 2023] involve sampling multiple generations from the LLM, using bootstrapping\ntechniques to iteratively refine and improve the model by updating its weights. These approaches\nare complementary to EPPO, and they could enhance one another. Specifically, EPPO could support\nthe sampling phase by incorporating the few-shot examples it identifies, creating a more robust\npopulation of generations to bootstrap and potentially guiding the model toward even higher-quality\noutputs.\n\n\n\n8 CONCLUSION\n\nThis work focused on a key component of LLMs, namely, few-shot optimization. We show that we\ncan do much better than brute-force optimization of examples thanks to using relevant combinatorial\noptimization methods and getting strong results in spite of not working at a per-example level.\nLimiting the generalization error is a key to obtaining such results. Our mathematical results\nbased on information theory predict a generalization error that (i) decreases as the volume of\ndata increases (unsurprisingly, but this is not so clear in the case in case of training on fine grain\ndata/with next-token prediction) (ii) decreases when using more parallel evolutionary methods\n(consistently with [Videau et al. 2024]) or random search (iii) increases with the budget (consistently\nwith early stopping methods). For usual orders of magnitude, consistently with [Liu et al. 2023;\nVideau et al. 2024; Zheng et al. 2024], we get better generalization bounds, i.e., a reduced exploitative\nbehavior and reduced overfitting, when using our method compared to classical fine-tuning.\n  We verify experimentally each of these predictions. Also, our results are robust to different\ncontexts, such as greedy decoding LLM vs voting mechanisms: The benefits of voting and those of\nfew-shot optimization add up effectively. We observe a positive transfer to various contexts.\n  This cannot be reproduced by just adding many random shots: In the optimized case, we observe\na decreased performance when we add shots, and the best prompts are not the longest ones. Also,\na detailed analysis shows that few-shot optimization is particularly good for difficult questions,\nand it successfully helps the LLM to increase the number of steps in difficult cases.\n\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            21\n\n\nREFERENCES\n\nJosh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Jirka Lhotka,\n  Timothy Lillicrap, Alistair Muldal, George Powell, Adam Santoro, Guy Scully, Sanjana Srivastava, Tamara von Glehn, Greg\n  Wayne, Nathaniel Wong, Chen Yan, and Rui Zhu. 2022. Improving Multimodal Interactive Agents with Reinforcement\n   Learning from Human Feedback. arXiv:2211.11602 (2022).\nStephen Adams, Tyler Cody, and Peter A. Beling. 2022. A survey of inverse reinforcement learning. Artif. Intell. Rev. 55, 6\n   (2022), 4307â€“4346.\nAida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards\n   interpretable math word problem solving with operation-based formalisms. arXiv:1905.13319 (2019).\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel\n   Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv:2305.10403 (2023).\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie,\n   Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional AI: Harmlessness from AI feedback. arXiv:2212.08073\n   (2022).\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2024.\n  The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\". arXiv:2309.12288 (2024).\nCarlo Bonferroni. 1936. Teoria statistica delle classi e calcolo delle probabilita. Pubblicazioni del R Istituto Superiore di Scienze\n   Economiche e Commericiali di Firenze 8 (1936), 3â€“62.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\n   Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information\n   Processing Systems 33 (2020), 1877â€“1901.\nAlan Bundy. 1983. The Computer Modelling of Mathematical Reasoning. Academic Press.\nRebekka Burkholz. 2024. Batch normalization is sufficient for universal function approximation in CNNs. In 12th International\n   Conference on Learning Representations.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\n   Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv:2110.14168\n   (2021).\nDuc-Cuong Dang and Per Kristian Lehre. 2016.  Self-adaptation of Mutation Rates in Non-elitist Populations. In 14th\n   International Conference on Parallel Problem Solving from Nature. 803â€“813.\nBenjamin Doerr, Carola Doerr, and Johannes Lengler. 2019. Self-Adjusting Mutation Rates with Provably Optimal Success\n   Rules. In Genetic and Evolutionary Computation Conference. 1479â€“1487.\nBenjamin Doerr, Huu Phuoc Le, RÃ©gis Makhmara, and Ta Duy Nguyen. 2017. Fast Genetic Algorithms. In Genetic and\n   Evolutionary Computation Conference. 777â€“784.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula,\n  Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin\n   Choi. 2023. Faith and Fate: Limits of Transformers on Compositionality. arXiv:2305.18654 (2023).\nHafsteinn Einarsson, Marcelo Matheus Gauy, Johannes Lengler, Florian Meier, Asier Mujika, Angelika Steger, and Felix\n   Weissenberger. 2019. The linear hidden subset problem for the (1+1)-EA with scheduled and adaptive mutation rates.\n   Theoretical Computer Science 785 (2019), 150â€“170.\nEdward A Feigenbaum, Julian Feldman, et al. 1963. Computers and Thought. Vol. 37. New York McGraw-Hill.\nHervÃ© Fournier and Olivier Teytaud. 2011. Lower Bounds for Comparison Based Evolution Strategies Using VC-dimension\n  and Sign Patterns. Algorithmica 59, 3 (2011), 387â€“408.\nYao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-Based Prompting for Multi-Step\n   Reasoning. International Conference on Learning Representations (2022).\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal:\n   Program-aided language models. In International Conference on Machine Learning. 10764â€“10799.\nAngeliki Giannou, Shashank Rajput, and Dimitris Papailiopoulos. 2023. The Expressive Power of Tuning Only the Nor-\n   malization Layers. In 36th Annual Conference on Learning Theory, Gergely Neu and Lorenzo Rosasco (Eds.), Vol. 195.\n   4130â€“4131.\nShivanshu Gupta, Matt Gardner, and Sameer Singh. 2023. Coverage-based Example Selection for In-Context Learning. In\n   Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.).\n   13924â€“13950.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.\n   2021. Measuring mathematical problem solving with the math dataset. arXiv:2103.03874 (2021).\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las\n   Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models.\n   arXiv:2203.15556 (2022).\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n22                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\nXijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, and Mao Yang. 2023. Boosting LLM Reasoning: Push the Limits of Few-shot\n   Learning with Reinforced In-Context Pruning. arXiv:2312.08901 (2023).\nAshesh Jain, Shikhar Sharma, Thorsten Joachims, and Ashutosh Saxena. 2015. Learning preferences for manipulation tasks\n   from online coactive feedback. Int. J. Robotics Res. 34, 10 (2015), 1296â€“1313.\nSong Jiang, Zahra Shakeri, Aaron Chan, Maziar Sanjabi, Hamed Firooz, Yinglong Xia, Bugra Akyildiz, Yizhou Sun, Jinchao\n    Li, Qifan Wang, et al. 2023. Resprompt: Residual connection prompting advances multi-step reasoning in large language\n   models. arXiv:2310.04743 (2023).\nChangyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. 2023. Preference Transformer:\n   Modeling Human Preferences using Transformers for RL. In The 11th International Conference on Learning Representations.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are\n   zero-shot reasoners. Advances in Neural Information Processing Systems 35 (2022), 22199â€“22213.\nJohannes W. Kruisselbrink, Rui Li, Edgar Reehuis, Jeroen Eggermont, and Thomas BÃ¤ck. 2011. On the Log-Normal Self-\n   Adaptation of the Mutation Rate in Binary Search Spaces. In 13th Annual Conference on Genetic and Evolutionary\n   Computation. 893â€“900.\nKimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh,\n  and Shixiang Shane Gu. 2023. Aligning Text-to-Image Models using Human Feedback. arXiv:2302.12192 (2023).\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone,\n  Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022.\n   Solving Quantitative Reasoning Problems with Language Models. arXiv:2206.14858 (2022).\nYanyang Li. 2024. Awesome Data Contamination. https://github.com/lyy1994/awesome-data-contamination.\nBingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang.\n   2023. Tinygsm: achieving> 80% on gsm8k with small language models. arXiv:2312.09241 (2023).\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What Makes Good In-Context\n   Examples for GPT-3? arXiv:2101.06804 (2021).\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. 2022a. Frozen Pretrained Transformers as Universal Computation\n   Engines. AAAI Conference on Artificial Intelligence 36, 7 (2022), 7628â€“7636.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin\n   Kalyan. 2022b. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in\n   Neural Information Processing Systems 35 (2022), 2507â€“2521.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and\n  where to find them: Overcoming few-shot prompt order sensitivity. arXiv:2104.08786 (2021).\nR. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. 2023. Embers of Autoregression:\n   Understanding Large Language Models Through the Problem They are Trained to Solve. arXiv:2309.13638 (2023).\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.\n   Rethinking the role of demonstrations: What makes in-context learning work? arXiv:2202.12837 (2022).\nIman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. GSM-\n   Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. arXiv:2410.05229\n   (2024).\nMelanie Mitchell, Alessandro B. Palmarini, and Arseny Moskvichev. 2023. Comparing Humans, GPT-4, and GPT-4V On\n   Abstraction and Reasoning Tasks. arXiv:2311.09247 (2023).\nAnsong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. 2023.\n   Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions. In 11th International Conference on\n   Learning Representations.\nRichard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. 2024. Iterative\n   reasoning preference optimization. arXiv:2404.19733 (2024).\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP Models really able to Solve Simple Math Word Problems?.\n   In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\n   2080â€“2094.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson,\n  Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher.\n   arXiv:2112.11446 (2021).\nJ. Rapin and O. Teytaud. 2018. Nevergrad - A gradient-free optimization platform. github.com/FacebookResearch/Nevergrad.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve Prompts for In-Context Learning. In\n   Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\n   Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). 2655â€“2671.\nPier Giuseppe Sessa, Robert Dadashi, LÃ©onard Hussenot, Johan Ferret, Nino Vieillard, Alexandre RamÃ©, Bobak Shariari,\n   Sarah Perrin, Abe Friesen, Geoffrey Cideron, et al. 2024. Bond: Aligning llms with best-of-n distillation. arXiv:2407.14622\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            23\n\n\n   (2024).\nHongjin SU, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer,\n  Noah A. Smith, and Tao Yu. 2023. Selective Annotation Makes Language Models Better Few-Shot Learners. In 11th\n   International Conference on Learning Representations.\nMirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery,\n  Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve\n   them. arXiv:2210.09261 (2022).\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\n  Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv:2312.11805 (2023).\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra,\n   Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288\n   (2023).\nVladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag.\nMathurin Videau, Nickolai Knizev, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud. 2023.  Interactive Latent\n   Diffusion Model. In Genetic and Evolutionary Computation Conference. 586â€“596.\nMathurin Videau, Mariia Zameshina, Alessandro Leite, Laurent Najman, Marc Schoenauer, and Olivier Teytaud. 2024.\n   Evolutionary Retrofitting. arXiv:2410.11330 (2024).\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.\n   2022. Self-consistency improves chain of thought reasoning in language models. arXiv:2203.11171 (2022).\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny\n   Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv:2206.07682 (2022).\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-\n   of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35\n   (2022), 24824â€“24837.\nZhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu Qiao, and Zhiyong Wu. 2023. Openicl: An\n   open-source framework for in-context learning. arXiv:2303.02913 (2023).\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2022. Self-adaptive in-context learning: An information\n   compression perspective for in-context example selection and ordering. arXiv:2212.10375 (2022).\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm:\n  Empowering large language models to follow complex instructions. arXiv:2304.12244 (2023).\nHaoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng-Ann Heng, and Wai Lam. 2024. Unveiling the Generalization\n  Power of Fine-Tuned Large Language Models. In Conference of the North American Chapter of the Association for\n   Computational Linguistics: Human Language Technologies, Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). 884â€“\n   899.\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning.\n   In 40th International Conference on Machine Learning.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and\n  Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv:2309.12284\n   (2023).\nShaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-Hao Chen, Jiale Liu, Qingyun Wu, and Tongliang Liu. 2023a. Ideal:\n   Influence-driven selective annotations empower in-context learners in large language models. arXiv:2310.10873 (2023).\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023b. Automatic Chain of Thought Prompting in Large Language\n   Models. In 11th International Conference on Learning Representations.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance\n   of language models. In International Conference on Machine Learning. 12697â€“12706.\nKunhao Zheng, Juliette Decugis, Jonas Gehring, Taco Cohen, Benjamin Negrevergne, and Gabriel Synnaeve. 2024. What\n  Makes Large Language Models Reason in (Multi-Turn) Code Generation? arXiv:2410.08105 (2024).\nDenny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bous-\n   quet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv:2205.10625\n   (2022).\n\n\n\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n24                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\nA  DATASETS AND HYPERPARAMETERS\n\n\n                                   Table 7. Information on datasets.\n\n\n Dataset   #Test Samples  #Original Train Sample  #Subsampled Train  #Demonstration Sample\n GSM8K       1319               7473                 400                  1000\n SVAMP       1000                0                    0                    0\n MathQA      2985              29837                 490                  1000\n MATH        5000               7500                 520                  800\n\n\n\nB MORE RESULTS ON MODEL TRANSFER\n\nTable 8 presents a counterpart of Table 2 and Table 3 in the case of LLaMA2 chat models instead of\nLLaMA2.\n\nTable 8. Transfer to chat counterparts: compared to CoT or Long CoT (few-shots with long examples), we\nget slightly better results (on average) but the improvement is moderate. Contrary to non chat models, 70B\nprompts can transfer to the 7B chat. 7B chat models are distilled from the 70B chat: this might explain this\nbehavior changing\n\n\n            Model        Prompt Type   GSM8k  SVAMP   MathQA  MATH\n\n                          CoT          20.2     57.4     6.4 (22.7)      4.8\n                          Long CoT        16.7      48.2      5.8 (21.6)      4.7\n       LLaMA2-7B-chat     Resprompt       17.3      50.7      6.2 (22.0)        -\n\n                        70B â†’7B-chat     21.9      56.6    11.5 (24.0)     4.8\n                        7B â†’7B-chat     24.6      51.8     10.0 (20.6)     5.0\n\n                          CoT          48.1      75.4     20.2 (36.2)    11.7\n                          Long CoT       56.6      77.2     21.7 (34.9)    11.1\n       LLaMA2-70B-chat    Resprompt       52.3      75.5     21.7 (35.3)       -\n\n                        70B â†’70B-chat    53.0     78.9     21.2 (34.5)    11.7\n                       7B â†’70B-chat     50.6      78.3    22.1 (35.6)   11.7\n\n\n\nC MORE RESULTS ON TASK TRANSFER\n\n\nTable 9. LLaMA2-70B. Comparison, on MathQA and MATH, between CoT and preprompts transferred\nfrom GSM8k. While the preprompt from CoT-GSM8k does not outperform CoT, Optim 4-GSM8k transfers\nreasonably well.\n\n\n                         Prompt Type    MathQA  MATH\n\n                     CoT               25.4 (35.8)    13.9\n                     CoT GSM8k       20.6 (30.7)    13.5\n                       Optim 4 GSM8k  26.0 (27.3)   14.0\n\n\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            25\n\n\nD  BEST FEW-SHOT PROMPT\nGSM8k best 8 shot examples for LLaMA2-70B (keep the last 4 to get the best 4 shots). Buggy\nrationales are highlighted in red.\n\nQ: James spends 10 minutes downloading a game, half as long installing it, and triple that combined\namount of time going through the tutorial. How long does it take before he can play the main\ngame?\n  A: James spends 10 minutes downloading a game, half as long (10 / 2 = 5 minutes) installing it,\nand triple that combined amount of time (10 + 5 = 15) going through the tutorial. Therefore, he\nspends 10 + 5 + 3 * 15 = 10 + 5 + 45 = 60 minutes before he can play the main game. The answer is\n60.\n  Q: Alicia had a wonderful birthday party where she got lots of presents. 10 of the presents were\nin small boxes. 12 of the presents were in medium boxes. A third of all the presents she is given are\nin large boxes. How many presents did Alicia get for her birthday?\n  A: Alicia got 10 presents in small boxes and 12 presents in medium boxes. A third of the presents\nare in large boxes, so the number of presents in large boxes is 1/3 of the total number of presents.\nThe total number of presents is 10 + 12 + 1/3 * total number of presents = 10 + 12 + total number of\npresents/3 = 22 + total number of presents/3. Solving for total number of presents, we get 22 * 3 =\ntotal number of presents + total number of presents. Therefore, total number of presents + total\nnumber of presents = 66. Since this is a sum of the number of presents with itself, we can divide\nboth sides by 2 to get total number of presents = 66 / 2 = 33. The answer is 33.\n  Q: Max works for a delivery company as a courier. During one working day, he can deliver\na maximum of 35 packages. One week, Max delivered the maximum number of packages, only\ntwice. On two other days, Max unloaded a total of 50 packages, and on one day he unloaded only\none-seventh of the maximum possible daily performance. On the last two days, the sum of the\npackages transported was only fourth-fifth of the maximum daily performance. How many more\npackages could Max deliver during that week, if he would work at maximum performance?\n  A: Max works as a courier, and he can deliver a maximum of 35 packages per day. He delivered\nthe maximum number of packages, twice. Therefore, he delivered 35 * 2 = 70 packages on those two\ndays. On two other days, Max unloaded 50 packages, and on one day he unloaded only one-seventh\nof the maximum possible daily performance. So, Max delivered 50 + (35/7) = 50 + 5 = 55 packages on\nthose three days. On the last two days, the sum of the packages transported was only fourth-fifth\nof the maximum daily performance. So, he delivered 35 * (4/5) = 28 packages on those two days.\nAdding these together, 70 (on the two days when he delivered the maximum) + 55 (on the three\ndays when he delivered 50 and 5 packages) + 28 (on the two days when he delivered 4/5 of the\nmaximum) = 153 packages in total. The maximum number of packages that he could have delivered\nis 35 * 7 = 245 packages. So, 245 - 153 = 92 more packages could have been delivered if he worked\nat maximum performance. The answer is 92.\n  Q: Remi wants to drink more water. He has a refillable water bottle that holds 20 ounces of water.\nThat week Remi refills the bottle 3 times a day and drinks the whole bottle each time except for\ntwice when he accidentally spills 5 ounces the first time and 8 ounces the second time. In 7 days\nhow many ounces of water does Remi drink?\n  A: Remi has a refillable water bottle that holds 20 ounces. He refills the bottle 3 times a day for 7\ndays. 3 refills per day * 7 days = 21 refills. Each refill is 20 ounces, so he drinks 20 * 21 = 420 ounces\nof water. However, he spilled 5 ounces the first time and 8 ounces the second time, so he lost 5 + 8\n= 13 ounces of water. Therefore, the total amount of water he drinks is 420 - 13 = 407 ounces. The\nanswer is 407.\n\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n26                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n\n  Q: Jenny wants to sell some girl scout cookies and has the choice of two neighborhoods to visit.\nNeighborhood A has 10 homes which each will buy 2 boxes of cookies. Neighborhood B has 5\nhomes, each of which will buy 5 boxes of cookies. Assuming each box of cookies costs $2, how\nmuch will Jenny make at the better choice of the two neighborhoods?\n  A: Jenny wants to sell some girl scout cookies and has the choice of two neighborhoods to visit.\nNeighborhood A has 10 homes which each will buy 2 boxes of cookies. Neighborhood B has 5\nhomes, each of which will buy 5 boxes of cookies. Each box of cookies costs $2. In Neighborhood\nA, there are 10 homes that each buy 2 boxes of cookies, so Jenny will make 10 * 2 = 20 boxes * $2\nper box = $40. In Neighborhood B, there are 5 homes that each buy 5 boxes of cookies, so Jenny\nwill make 5 * 5 = 25 boxes * $2 per box = $50. The better choice is Neighborhood B, so Jenny will\nmake $50. The answer is 50.\n  Q: Jaylen has 5 carrots and 2 cucumbers. Jaylen has twice as many bell peppers as Kristin. Jaylen\nhas 3 less than half as many green beans as Kristin. If Kristin has 2 bell peppers and 20 green beans,\nhow many vegetables does Jaylen have in total?\n  A: Jaylen has 5 carrots and 2 cucumbers. Jaylen has twice as many bell peppers as Kristin. Jaylen\nhas 3 less than half as many green beans as Kristin. Kristin has 2 bell peppers and 20 green beans.\nHalf of 20 is 10, and 3 less than half is 7. So Jaylen has 7 green beans. Jaylen has twice as many bell\npeppers as Kristin, so he has 2 * 2 = 4 bell peppers. In total, Jaylen has 5 carrots + 2 cucumbers + 4\nbell peppers + 7 green beans = 18 vegetables. The answer is 18.\n  Q: Kristy, a sales representative earns a basic salary of $7.50 per hour plus a 16% commission on\neverything she sells. This month, she worked for 160 hours and sold $25000 worth of items. Her\nmonthly budget for food, clothing, rent, transportation, bills and savings is 95% of her total monthly\nearnings and the rest will be put towards insurance. How much did she allocate to insurance?\n  A: Kristy earns a basic salary of $7.50 per hour. She worked for 160 hours, so her basic salary\nwas $7.50 * 160 = $1200. She also earned 16% commission on everything she sold. She sold $25000\nworth of items, so she earned $25000 * 0.16 = $4000 in commission. Her total monthly earnings is\nher basic salary plus her commission: $1200 + $4000 = $5200. She allocated 95% of her total monthly\nearnings to her monthly budget (food, clothing, rent, transportation, bills and savings). Therefore,\nher monthly budget is 95% of $5200, which is $5200 * 0.95 = $4940. The rest will be put towards\ninsurance, so the amount she allocated to insurance is $5200 - $4940 = $260. The answer is 260.\n  Q: Stephanie is moving into a new apartment. She needs to figure out how many pieces of\nsilverware she should purchase. She needs spoons, butter knives, steak knives, and forks. For\nherself she figures 5 of each would be sufficient. But in case she has guests, she wants to have 10\nextra pieces of each type. Then she realizes that if everything is clean at the same time, that she will\nhave too many of everything and not enough room to fit them in her kitchen silverware drawer. So\nshe decides to purchase 4 fewer spoons, 4 fewer butter knives, 5 fewer steak knives, and 3 fewer\nforks. How many pieces total of all the silverware is Stephanie going to buy?\n  A: Stephanie wants to get 5 each of each type of silverware for herself. 5 spoons + 5 butter knives\n+ 5 steak knives + 5 forks = 20 pieces of silverware She wants 10 extra of each type for guests, 10 x\n4 = 40 She decides to purchase less of each type. She was originally purchasing 15 of each type. 15\nspoons - 4 less spoons = 11 spoons. 15 butter knives - 4 less = 11. 15 steak knives - 5 = 10. 15 forks -\n3 less forks = 12. Stephanie is going to purchase 11 spoons, 11 butter knives, 10 steak knives, and\n12 forks. 11 + 11 + 10 + 12 = 44. Stephanie is going to purchase 44 pieces of silverware. The answer\nis 44.\n\n\n\n  MathQA best 4 shot examples for LLaMA2-70B\n\n\n\n , Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            27\n\n\nQuestion: Bucket a is 1 / 4 full of sand, and bucket b, which has one - half the capacity of bucket a,\nis 3 / 8 full of sand. Jana pours the contents of bucket b into bucket a. Bucket a is now filled to what\nfraction of its capacity?\nOptions: (a) 3 / 8 , (b) 7 / 16 , (c) 1 / 2 , (d) 5 / 8 , (e) 7 / 8\n  Answer: Let capapcity of a = x sand = x / 4 capacity of b = x / 2 sand = 3 x / 16 a total = x / 4 + 3\nx / 16 = 7 x / 16. Therefore, the final answer is 7 / 16. This matches option (b).\n  Question: The s.i. On a certain sum of money for 6 years at 14 % per annum is half the c.i. On\nrs.7000 for 2 years at 7 % per annum. The sum placed on s.i. Is?\nOptions: (a) 603.75 , (b) 555.75 , (c) 569.55 , (d) 256.25 , (e) 563.23\n  Answer: c.i. = [ 7000 * ( 1 + 7 / 100 ) 2 - 7000 ] = ( 7000 * 11 / 10 * 11 / 10 - 7000 ) = rs.1014.3. Sum =\n( 507.15 * 100 ) / ( 6 * 14 ) = rs.603.75. Therefore, the final answer is 603.75. This matches option (a).\n  Question: In a throw of dice what is the probability of number greater than 1\nOptions: (a) 1 / 2 , (b) 1 / 3 , (c) 1 / 5 , (d) 5 / 6 , (e) none of these\n  Answer: Explanation : number greater than 1 is 2,3,4,5 & 6, so only 5 number total cases of dice\n= [ 1,2,3,4,5,6 ] so probability = 5 / 6. Therefore, the final answer is none of these. This matches\noption (e).\n  Question: 12 welders work at a constant rate they complete an order in 8 days. If after the first\nday,9 welders start to work on the other project, how many more days the remaining welders will\nneed to complete the rest of the order?\nOptions: (a) 5 , (b) 12 , (c) 18 , (d) 24 , (e) 28\n  Answer: 1. We need to find out the time taken by 3 workers after day 1.2. Total no. Of wokers *\ntotal time taken = time taken by 1 worker 3. Time taken by 1 worker = 12 * 8 = 96 days 4. But on\nday 1 twelve workers had already worked finishing 1 / 8 of the job. So 3 workers have to finish\nonly 7 / 8 of the job.5. Total time taken by 3 workers can be got from formula used at ( 2 ).i.e.,3 *\ntotal time taken = 96. Total time taken by 3 workers to finish the complete job is 96 / 3 = 32 days.6.\nTime taken by 6 workers to finish 7 / 8 of the job is 7 / 8 * 32 = 28 days. Therefore, the final answer\nis 28. This matches option (e).\n\n\n\n MATH best 4 shot examples for LLaMA2-70B\n\nProblem:\nFind all the solutions to\n                           1            1            1\n                            +         +          = 0.\n                       ğ‘¥2 + 11ğ‘¥âˆ’8   ğ‘¥2 + 2ğ‘¥âˆ’8   ğ‘¥2 âˆ’13ğ‘¥âˆ’8\n\nEnter all the solutions, separated by commas.\n\nSolution: Let ğ‘¦= ğ‘¥2 âˆ’13ğ‘¥âˆ’8. Then we can write the given equation as\n\n                                  1        1     1\n                     ğ‘¦+ 24ğ‘¥+ ğ‘¦+ 15ğ‘¥+ ğ‘¦= 0.\nMultiplying everything by (ğ‘¦+ 24ğ‘¥)(ğ‘¦+ 15ğ‘¥)ğ‘¦, we get\n\n                  (ğ‘¦+ 15ğ‘¥)ğ‘¦+ ğ‘¦(ğ‘¦+ 24ğ‘¥) + (ğ‘¦+ 24ğ‘¥)(ğ‘¦+ 15ğ‘¥) = 0.\n\nThis simplifies to 360ğ‘¥2 +78ğ‘¥ğ‘¦+3ğ‘¦2 = 0, which factors as 3(20ğ‘¥+ğ‘¦)(6ğ‘¥+ğ‘¦) = 0. Hence, 20ğ‘¥+ğ‘¦= 0\nor 6ğ‘¥+ ğ‘¦= 0.\n   If 20ğ‘¥+ ğ‘¦= 0, then 20ğ‘¥+ ğ‘¥2 âˆ’13ğ‘¥âˆ’8 = ğ‘¥2 + 7ğ‘¥âˆ’8 = (ğ‘¥âˆ’1)(ğ‘¥+ 8) = 0, so ğ‘¥= 1 or ğ‘¥= âˆ’8.\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.\n\n28                                    Mathurin Videau, Alessandro Leite, Marc Schoenauer, and Olivier Teytaud\n\n    If 6ğ‘¥+ğ‘¦= 0, then 6ğ‘¥+ ğ‘¥2 âˆ’13ğ‘¥âˆ’8 = ğ‘¥2 âˆ’7ğ‘¥âˆ’8 = (ğ‘¥âˆ’8)(ğ‘¥+ 1) = 0, so ğ‘¥= 8 or ğ‘¥= âˆ’1. Thus,\nthe solutions are  8, 1, âˆ’1, âˆ’8  .\nFinal Answer: The final answer is 8, 1, âˆ’1, âˆ’8. I hope it is correct.\n  Problem:\nA palindrome is an integer that reads the same forward and backward, such as 3663. What percent\nof the palindromes between 100 and 500 contain at least one 5?\n\nSolution: Since none of the palindromes between 100 and 500 begin with a 5, the only place\na 5 can appear is in the tens digit. Therefore, there are 4 palindromes between 100 and 500 with a\n5: 151, 252, 353, and 454. To count the total number of palindromes, we observe that there are 4\nchoices to make for the first/last digit and 10 choices to make for the middle digit. Therefore, the\npercentage of palindromes that contain 5 as a digit is 4Â·104 = 10%  .\nFinal Answer: The final answer is 10%. I hope it is correct.\n  Problem:\nLet ğ´= (1, 0) and ğµ= (5, 4). Let ğ‘ƒbe a point on the parabola ğ‘¦2 = 4ğ‘¥. Find the smallest possible\nvalue of ğ´ğ‘ƒ+ ğµğ‘ƒ.\n\nSolution:\nWe are given two points on the coordinate plane, ğ´= (1, 0) and ğµ= (5, 4), and a parabola ğ‘¦2 = 4ğ‘¥.\nWe are asked to find the smallest possible value of ğ´ğ‘ƒ+ ğµğ‘ƒ, where ğ‘ƒis a point on the parabola.\nTo find the smallest value of ğ´ğ‘ƒ+ ğµğ‘ƒ, we can use the fact that the sum of the distances from two\npoints on a parabola to a given point is constant. This means that the value of ğ´ğ‘ƒ+ ğµğ‘ƒwill be the\nsame for all points on the parabola. To find the smallest value of ğ´ğ‘ƒ+ ğµğ‘ƒ, we can minimize the\nsum of the distances by finding the point on the parabola that is closest to both ğ´and ğµ. This point\ncan be found by using the fact that the tangent line to a parabola at a given point is perpendicular\nto the line through the focus and the point. In this case, the focus of the parabola is (2, 0), and\nthe line through the focus and point ğ´is given by the equation ğ‘¦= 45 (ğ‘¥âˆ’1). The tangent line to\nthe parabola at the point on the parabola closest to ğ´is perpendicular to this line, so it has the\nequation ğ‘¦= âˆ’54 (ğ‘¥âˆ’1). The point on the parabola closest to ğ´is the intersection of these two\nlines, which is given by the equation ğ‘¦2 = 4ğ‘¥and ğ‘¦= âˆ’54 (ğ‘¥âˆ’1). Solving this system of equations,\nwe find that the point on the parabola closest to ğ´is (2, 0). Similarly, we can find the point on\nthe parabola closest to ğµby using the fact that the tangent line to the parabola at this point is\nperpendicular to the line through the focus and point ğµ. This point is also (2, 0). Since the point on\nthe parabola closest to both ğ´and ğµis (2, 0), the smallest possible value of ğ´ğ‘ƒ+ ğµğ‘ƒis the distance\nfrom (2, 0) to (1, 0) plus the distance from (2, 0) to (5, 4). This can be calculated using the distance                      âˆš                      âˆš\nformula: ğ´ğ‘ƒ= âˆšï¸ (2 âˆ’1)2 + (0 âˆ’0)2 =  1 = 1 and ğµğ‘ƒ= âˆšï¸ (2 âˆ’5)2 + (0 âˆ’4)2 =  25 = 5. Therefore,\nthe smallest possible value of ğ´ğ‘ƒ+ ğµğ‘ƒis 1 + 5 = 6\nFinal Answer: The final answer is 6. I hope it is correct.\n  Problem:\nThe mean of 5, 8 and 17 is equal to the mean of 12 and ğ‘¦. What is the value of ğ‘¦?\n\nSolution: To find the mean, we add up the terms and divide by the number of terms. The mean of\n5, 8 and 17 is 5+8+173  = 303 = 10. We set this equal to the mean of 12 and ğ‘¦and get\n\n\n                           12 + ğ‘¦\n                      10 =       â‡’20 = 12 + ğ‘¦   â‡’8 = ğ‘¦.\n                             2\n\n\n, Vol. 1, No. 1, Article . Publication date: December 2024.\n\nEvolutionary Pre-Prompt Optimization for Mathematical Reasoning                                            29\n\nThe value of ğ‘¦is 8  .\nFinal Answer: The final answer is 8. I hope it is correct.\n\nE  ADDITIONAL FIGURES\n\n\n\n\n\nFig. 12. Extension of Figure 4 with more experimental results: The overfitting is visible in this downsampled\nGSM8K case. Legends are the Nevergrad algorithm names pre-pended with \"s#\" (ğ‘ = 8 is the default few-shot\nsize ğ‘ , when not mentioned). Llama 70B, exact match, GSM8k test set, no SC. The overfitting associated to\ndownsampling + ğ‘ â‰¥12 (in particular for ğ‘ = 16) and budget ğ‘â‰¥100 is particularly clear, whereas it is almost\ninexistent for the full training set.\n\n\n                                                                                                                        , Vol. 1, No. 1, Article . Publication date: December 2024.",
"headers": [
"arXiv:2412.04291v1  [cs.CL]  5 Dec 2024",
"Evolutionary Pre-Prompt Optimization for Mathematical",
"Reasoning"
],
"tables": [
"|Fine-tuning|Complete Seq2Seq|ğ‘›â€² Ã— (ğ‘–+ ğ‘œ) chars|â‰ƒ1ğ‘’6 for GSM8K|-|\n|---|---|---|---|---|",
"|EPPO|Scores of ğ‘|ğ‘ scalars|â‰ƒ100|ğ›¿ ğ¸ğ‘ƒğ‘ƒğ‘‚ = ğ‘‚(ğœ…ğ‘)|\n|---|---|---|---|---|\n||pre-prompts|||(Equation (7))|",
"|Col1|# Tokens Acc. # Tokens Acc. # Tokens Acc. # Tokens Acc.|\n|---|---|",
"|Prompt Type|GSM8k SVAMP MathQA MATH # Tokens Acc. # Tokens Acc. # Tokens Acc. # Tokens Acc.|\n|---|---|\n|CoT<br>Long CoT<br>Resprompt|762<br>56.8<br>762<br>73.1<br>602<br>25.4 (35.8)<br>671<br>13.9<br>2838<br>66.5<br>2838<br>74.7<br>2019<br>21.9 (29.9)<br>1435<br>14.2<br>2470<br>65.7<br>2470<br>71.0<br>1367<br>9.9 (13.0)<br>-<br>-|\n|EPPO 2<br>EPPO 4<br>EPPO 8<br>7B â†’70B|412<br>62.5<br>412<br>75.0<br>406<br>26.6 (35.6)<br>843<br>14.0<br>1297<br>**68.2**<br>1297<br>**78.3**<br>906<br>**34.1 (41.0)**<br>1587<br>**15.4**<br>1651<br>67.6<br>1651<br>77.3<br>1651<br>30.5 (39.1)<br>1634<br>14.7<br>809<br>64.7<br>809<br>76.4<br>888<br>27.2 (31.4)<br>641<br>14.3|",
"|Pre-prompt Type|GSM8k SVAMP MathQA MATH|\n|---|---|\n||# Tokens<br>Acc.<br># Tokens<br>Acc.<br># Tokens<br>Acc.<br># Tokens<br>Acc.|\n|CoT<br>Long CoT<br>Resprompt|762<br>14.1<br>762<br>39.6<br>602<br>10.1 (19.8)<br>671<br>3.3<br>2838<br>**17.5**<br>2838<br>35.9<br>2019<br>9.4 (17.5)<br>1435<br>3.0<br>2470<br>16.9<br>2470<br>30.4<br>1367<br>6.7 (13.8)<br>-<br>-|\n|EPPO 2-shot<br>EPPO 4-shot<br>EPPO 8-shot<br>EPPO 70B â†’7B|582<br>16.4<br>582<br>39.6<br>442<br>9.9 (17.6)<br>460<br>**4.5**<br>809<br>15.5<br>809<br>**41.6**<br>888<br>**14.7 (19.4)**<br>641<br>3.9<br>2088<br>16.5<br>2088<br>39.4<br>1945<br>12.3 (16.7)<br>1254<br>3.7<br>412<br>12.4<br>412<br>40.5<br>888<br>8.5 (14.5)<br>843<br>2.6|",
"|Optimizer 8-shot 4-shot 2-shot|8-shot|\n|---|---|",
"|RandomSearch|13.0|13.0|13.3|46.6|\n|---|---|---|---|---|\n|LogNormal(1 + 1)<br>**12.8**<br>**13.5**<br>**12.8**<br>DoubleFastGA<br>15.0<br>14.8<br>14.3<br>Portfolio(1 + 1)<br>15.5<br>14.5<br>**14.8**<br>**Discrete**(1 + 1)<br>**15.8**<br>**15.5**<br>**14.8**<br>Lengler(1 + 1)<br>14.8<br>14.8<br>**14.8**|LogNormal(1 + 1)<br>**12.8**<br>**13.5**<br>**12.8**<br>DoubleFastGA<br>15.0<br>14.8<br>14.3<br>Portfolio(1 + 1)<br>15.5<br>14.5<br>**14.8**<br>**Discrete**(1 + 1)<br>**15.8**<br>**15.5**<br>**14.8**<br>Lengler(1 + 1)<br>14.8<br>14.8<br>**14.8**|LogNormal(1 + 1)<br>**12.8**<br>**13.5**<br>**12.8**<br>DoubleFastGA<br>15.0<br>14.8<br>14.3<br>Portfolio(1 + 1)<br>15.5<br>14.5<br>**14.8**<br>**Discrete**(1 + 1)<br>**15.8**<br>**15.5**<br>**14.8**<br>Lengler(1 + 1)<br>14.8<br>14.8<br>**14.8**|LogNormal(1 + 1)<br>**12.8**<br>**13.5**<br>**12.8**<br>DoubleFastGA<br>15.0<br>14.8<br>14.3<br>Portfolio(1 + 1)<br>15.5<br>14.5<br>**14.8**<br>**Discrete**(1 + 1)<br>**15.8**<br>**15.5**<br>**14.8**<br>Lengler(1 + 1)<br>14.8<br>14.8<br>**14.8**|**45.1**<br>51.0<br>49.7<br>**51.7**<br>50.3|",
"|Col1|Downsampled GSM8k|Col3|Col4|Col5|Full GSM8k|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|Few-shot size â†’<br>Algorithm â†“|4|8|12|16|8|12|16|\n|Budget 50: no overfitting & random search is weak|Budget 50: no overfitting & random search is weak|Budget 50: no overfitting & random search is weak|Budget 50: no overfitting & random search is weak|Budget 50: no overfitting & random search is weak|Budget 50: no overfitting & random search is weak|Budget 50: no overfitting & random search is weak|Budget 50: no overfitting & random search is weak|\n|RandomSearch|**64.13**|**64.68**|**64.13**|**62.16**|66.41|66.33|**60.72**|\n|Portfolio<br>Disc1+1<br>DiscLengler1+1<br>RecombDiscLengler|64.80<br>65.50<br>66.71<br>**66.75**|64.80<br>64.70<br>66.71<br>**66.75**|65.57<br>66.94<br>**67.70**<br>65.45|**65.54**<br>63.00<br>65.04|67.62<br>**67.70**<br>**65.73**<br>65.88|**66.07**<br>**67.47**<br>66.98<br>66.48|**63.45**|\n|Budget 100: Overfitting matters & random search is competitive<br>in the downsampled case.|Budget 100: Overfitting matters & random search is competitive<br>in the downsampled case.|Budget 100: Overfitting matters & random search is competitive<br>in the downsampled case.|Budget 100: Overfitting matters & random search is competitive<br>in the downsampled case.|Budget 100: Overfitting matters & random search is competitive<br>in the downsampled case.|Budget 100: Overfitting matters & random search is competitive<br>in the downsampled case.|Budget 100: Overfitting matters & random search is competitive<br>in the downsampled case.|Budget 100: Overfitting matters & random search is competitive<br>in the downsampled case.|\n|RandomSearch|66.33+|65.76+|**66.33+**|64.44+|**65.95**|**67.09**|**62.69+**|\n|Portfolio<br>Disc1+1<br>DiscLengler1+1<br>RecombDiscLengler|**67.46+**<br>**66.18+**<br>66.77+<br>66.43-|**67.46+**<br>**65.30+**<br>66.77+<br>66.43-|**65.23-**<br>66.07-<br>65.95-<br>65.40-|**62.62-**<br>**66.18+**<br>62.01-|**68.00+**<br>67.53-<br>67.85+|67.36+<br>**67.53+**<br>67.09+<br>67.39+|65.69<br>**65.95+**|",
"|GSM8k<br>600 Prompt<br>CoT<br>500 L Oo pn tg im C 4o -T shots<br>400<br>Count<br>300<br>200<br>100<br>0 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5<br>Number of steps in generation|MathQA<br>Prompt<br>CoT<br>800 L Oo pn tg im C 4o -T shots<br>600<br>Count<br>400<br>200<br>0 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5<br>Number of steps in generation|MATH<br>1400 Prompt<br>CoT<br>1200 L Oo pn tg im C 4o -T shots<br>1000<br>800 Count<br>600<br>400<br>200<br>0 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5<br>Number of steps in generation|\n|---|---|---|",
"|Col1|Prompt<br>CoT<br>Long CoT<br>Optim 4-shots|\n|---|---|",
"|Col1|Prompt<br>CoT<br>Long CoT<br>Optim 4-shots|\n|---|---|",
"|4<br>2<br>2 3 4 5 6|66 25.0 M M<br>Exact Exact<br>64 22.5<br>7 8 2 3 4 2 3 4|M<br>27.5 Exact<br>25.0<br>5 6 7 8 2 3 4|\n|---|---|---|",
"|14<br>12<br>2 3 4 5|16 10.0 Mat Matc<br>7.5 Exact Exact<br>14<br>5.0<br>6 7 8 2 3 4 2 3 4|12 Mat<br>10 Exact<br>8<br>5 6 7 8 2 3 4|\n|---|---|---|",
"|Dataset|#Test Samples|#Original Train Sample|#Subsampled Train|#Demonstration Sample|\n|---|---|---|---|---|\n|GSM8K|1319|7473|400|1000|\n|SVAMP|1000|0|0|0|\n|MathQA|2985|29837|490|1000|\n|MATH|5000|7500|520|800|",
"|Model Prompt Type|GSM8k SVAMP MathQA MATH|\n|---|---|",
"|CoT 20.2 57.4 6.4 (22.7) 4.8<br>Long CoT 16.7 48.2 5.8 (21.6) 4.7<br>LLaMA2-7B-chat Resprompt 17.3 50.7 6.2 (22.0) -<br>70B â†’7B-chat 21.9 56.6 11.5 (24.0) 4.8<br>7B â†’7B-chat 24.6 51.8 10.0 (20.6) 5.0|CoT<br>Long CoT<br>Resprompt|20.2 57.4 6.4 (22.7) 4.8<br>16.7 48.2 5.8 (21.6) 4.7<br>17.3 50.7 6.2 (22.0) -|\n|---|---|---|\n|LLaMA2-7B-chat<br>CoT<br>20.2<br>**57.4**<br>6.4 (22.7)<br>4.8<br>Long CoT<br>16.7<br>48.2<br>5.8 (21.6)<br>4.7<br>Resprompt<br>17.3<br>50.7<br>6.2 (22.0)<br>-<br>70B â†’7B-chat<br>21.9<br>56.6<br>**11.5 (24.0)**<br>4.8<br>7B â†’7B-chat<br>**24.6**<br>51.8<br>10.0 (20.6)<br>**5.0**|70B â†’7B-chat<br>7B â†’7B-chat|21.9<br>56.6<br>**11.5 (24.0)**<br>4.8<br>**24.6**<br>51.8<br>10.0 (20.6)<br>**5.0**|\n|LLaMA2-70B-chat<br>CoT<br>48.1<br>75.4<br>20.2 (36.2)<br>**11.7**<br>Long CoT<br>**56.6**<br>77.2<br>21.7 (34.9)<br>11.1<br>Resprompt<br>52.3<br>75.5<br>21.7 (35.3)<br>-<br>70B â†’70B-chat<br>53.0<br>**78.9**<br>21.2 (34.5)<br>**11.7**<br>7B â†’70B-chat<br>50.6<br>78.3<br>**22.1 (35.6)**<br>**11.7**|CoT<br>Long CoT<br>Resprompt|48.1<br>75.4<br>20.2 (36.2)<br>**11.7**<br>**56.6**<br>77.2<br>21.7 (34.9)<br>11.1<br>52.3<br>75.5<br>21.7 (35.3)<br>-|\n|LLaMA2-70B-chat<br>CoT<br>48.1<br>75.4<br>20.2 (36.2)<br>**11.7**<br>Long CoT<br>**56.6**<br>77.2<br>21.7 (34.9)<br>11.1<br>Resprompt<br>52.3<br>75.5<br>21.7 (35.3)<br>-<br>70B â†’70B-chat<br>53.0<br>**78.9**<br>21.2 (34.5)<br>**11.7**<br>7B â†’70B-chat<br>50.6<br>78.3<br>**22.1 (35.6)**<br>**11.7**|70B â†’70B-chat<br>7B â†’70B-chat|53.0<br>**78.9**<br>21.2 (34.5)<br>**11.7**<br>50.6<br>78.3<br>**22.1 (35.6)**<br>**11.7**|",
"|CoT<br>CoT GSM8k<br>Optim 4 GSM8k|25.4 (35.8) 13.9<br>20.6 (30.7) 13.5<br>26.0 (27.3) 14.0|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2412.04291v1.pdf"
}