{
"text": "IPO: Interpretable Prompt Optimization for\n                        Vision-Language Models\n\n\n\n\n                                       Yingjun Du1*, Wenfang Sun2∗, Cees G. M. Snoek1\n                     1AIM Lab, University of Amsterdam 2University of Science and Technology of China\n\n\n                                                 Abstract2024\n                                 Pre-trained vision-language models like CLIP have remarkably adapted to various\n                           downstream tasks. Nonetheless, their performance heavily depends on the speci-Oct                                ficity of the input text prompts, which requires skillful prompt template engineering.\n                                  Instead, current approaches to prompt optimization learn the prompts through gra-\n                                 dient descent, where the prompts are treated as adjustable parameters. However,20\n                                these methods tend to lead to overfitting of the base classes seen during training\n                           and produce prompts that are no longer understandable by humans. This paper\n                               introduces a simple but interpretable prompt optimizer (IPO), that utilizes large\n                             language models (LLMs) to generate textual prompts dynamically. We introduce\n                             a Prompt Optimization Prompt that not only guides LLMs in creating effective\n                             prompts but also stores past prompts with their performance metrics, providing rich[cs.LG]                          in-context information. Additionally, we incorporate a large multimodal model\n                     (LMM) to condition on visual content by generating image descriptions, which\n                            enhance the interaction between textual and visual modalities. This allows for\n                                 the creation of dataset-specific prompts that improve generalization performance,\n                             while maintaining human comprehension. Extensive testing across 11 datasets\n                                  reveals that IPO not only improves the accuracy of existing gradient-descent-based\n                            prompt learning methods but also considerably enhances the interpretability of the\n                                generated prompts. By leveraging the strengths of LLMs, our approach ensures that\n                                 the prompts remain human-understandable, thereby facilitating better transparency\n                            and oversight for vision-language models.\n\n\n                1  Introduction\n\n                      Vision-language models, trained on a diverse array of image-text pairs encapsulating a broad vo-\n                      cabulary of real-world concepts [1, 2, 3], have demonstrated notable adaptability across various\n                    downstream tasks [4, 5, 6, 7]. These models perform zero-shot image classification by filling inarXiv:2410.15397v1\n                      a predefined prompt template (e.g., “a photo of a [CLASS]”) with specific class names for the\n                          text encoder. Despite their effective generalization to new tasks, the performance can be influenced\n                   by minor changes in the wording of prompt templates [8].  Instead of manually creating hand-\n                        crafted prompts, recent developments in natural language processing [9, 10] and computer vision\n                           [8, 11, 12, 13] have proposed methods to learn a set of soft prompts with minimal labeled data.\n                       Despite the strides made in learning prompts, the current state of the art remains limited by its lack of\n                          interpretability and the overfitting problems on the base classes, which can be prohibitive in diverse\n                    and dynamic application environments. These limitations underscore the need for a more adaptable\n                    and user-friendly approach to prompt optimization in vision-language models.\n\n                    Drawing from recent advancements in using large language models (LLMs) as optimization tools [14],\n                     our paper, for the first time, incorporates these capabilities into vision-language modeling. Unlike\n                        gradient descent-based methods [8, 11, 13], which often fail to provide explanations for the generated\n\n                          ∗Equal contribution.\n\n\n                        38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n\nInitial prompt                              Gradient descent optimizer  Optimized prompt                  Default prompt              Interptretable prompt optimizer             Optimized prompt\n                                                              photo                                                                    of                                                                                                                     a breathtaking                                                                                                                                    and               <CLASS>.                                       <CLASS>.             A                                                                                                                                        Prompt Optimization Prompt                 Take\n                                                             a                                                               <CLASS>.                                                                                                                  flawlessly                                                                                                                             detailed\n                                                                                                        Task introduction                                                                                                                  photo of a {CLASS}.\n                                                                             +                                                                                             LLM                                                                                                    Image                                                                                                                                   description\n                                                                                                                                                                    Top-k prompts        CLIP       Loss                   CLIP      Scores            CLIP      Scores                                                                             +\n                                                                                                            Prompt history                               CLIP      Scores\n\n                                                                                                                    Scores\n                                                                     Loss:                                                                           0.25                             Loss:                                                                                                                 7.95        Image description\n                                                                                                            Acc: 0.65                                                                      Acc: 0.75\n                                                                                                                                                                                                   Loss: 0.11\n                                                                                                                                                                                                    Acc: 0.98\n                                                                      LMM\n\n\n     (a) Gradient-based prompt optimization.                               (b) Interpretable prompt optimization.\n\nFigure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt\noptimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11]\ntreat the text prompt as learnable parameters V . By minimizing the loss through gradient descent on the training\nset, an optimized prompt ˆV is obtained after I iterations, which is not interpretable by humans. In contrast, our\ninterpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I\niterations, the resulting optimized top prompt is effective and human-readable.\n\nprompts and tend to overfit on base classes, natural language-based methods enable LLMs to develop\nand refine solutions through continuous feedback iteratively. This approach improves interpretability\nin complex tasks like prompt optimization for vision-language models, making it easier for humans\nto understand the generated prompts. However, existing research on these methods [14, 15, 16]\nprimarily addresses language tasks and has not yet explored their potential for integrating prompt\noptimization with an LLM in vision-language models.\n\nTo address these challenges, this paper proposes an interpretable prompt optimizer (IPO) for vision-\nlanguage models that leverages the capabilities of LLMs to generate and refine text prompts dy-\nnamically.  First, we design a Prompt Optimization Prompt to prompt LLMs to generate more\neffective prompts that improve the accuracy of CLIP and reduce the loss in base classes. Our\nPrompt Optimization Prompt also stores past prompts along with their corresponding accuracy\nand loss as episodic memory, thereby providing richer in-context information to enable LLMs to\ngenerate more effective prompts. Second, to incorporate image information within the Prompt\nOptimization Prompt, we propose using a large multimodal model (LMM) to generate descrip-\ntions of images in base classes that can be added to the Prompt Optimization Prompt. This in-\ntegration facilitates a more intuitive interaction between the textual and visual modalities, which\nallows the Prompt Optimization Prompt to utilize image information, thereby generating dataset-\nspecific prompts to enhance the generalization performance of CLIP. The framework of our IPO,\nillustrated in Figure 1, showcases the comparison between traditional gradient-based prompt opti-\nmization and the proposed interpretable prompt optimization. Third, the prompts generated by\nour optimizer are human-interpretable.  For example, on the Food101 dataset [17], the initial\nprompt evolves from [“a photo of a [CLASS]”] to [“Categorize the image depicting a\ndelicious and appetizing <CLASS> with remarkable visual qualities.”]. Our gen-\nerated prompts perform 10.29% better in novel classes than the gradient-based method CoOP [8],\nreducing overfitting while maintaining interpretability.\n\nWe validated our IPO across 11 different datasets, demonstrating that it surpasses traditional gradient-\nbased state-of-the-art methods in accuracy and excels in interpretability. Our approach generates\nhuman-comprehensible prompts that can be seamlessly integrated into existing vision-language\nmodels to enhance performance. We conducted rigorous comparative experiments to quantify the\ninterpretability between gradient-based prompt learning and our method. We demonstrated the\nimportance of specific keywords in our generated prompts and revealed that not all tokens learned\nthrough traditional prompt learning methods are essential.\n\n2  Related work\n\nPrompt learning in vision-language models. Prompt learning, originally introduced in the natural\nlanguage processing community [18, 19, 20], involves applying a fixed function to input tokens\nto provide task instructions to the model. In the computer vision community, prompt learning has\nbeen explored in various forms, including textual prompt tuning [8, 11, 21, 22, 23, 24], and prefix\ntuning [13, 12, 25, 26, 27, 28, 29]. 1) Prompt tuning mainly involves treating text prompts as learnable\nparameters, using a small amount of data to fine-tune these parameters. As pioneered by CoOp [8]\n\n\n                                       2\n\nand CoCoOp [11], which both fine-tune a CLIP vision-language model [30] for few-shot transfer\nby optimizing a continuous set of prompt vectors within its language branch. Bayesian prompt\nlearning [21] formulated prompt learning as a variational inference problem and demonstrated its\nability for unseen class generalization. 2) Prefix tuning primarily involves adding learnable tokens to\nthe text encoder [31], vision encoder [12, 25], or both encoders [13, 27, 28, 32]. These tokens are\nfine-tuned using a small amount of data. Note that these methods do not optimize the initial text\nprompts. Instead, they focus on enhancing the model’s understanding capabilities by integrating these\nadditional, trainable tokens. Our method belongs to prompt tuning, but unlike previous approaches\nthat use gradient descent to optimize prompts, we propose using LLMs to optimize prompts. Our\nmethod leverages the natural language capabilities of LLMs to iteratively refine feedback-based\nprompts, aiming to enhance both the effectiveness and the explainability of the prompts.\n\nLLMs as prompt optimizers. Several recent works explore the role of LLMs as prompt optimizers\nfor NLP tasks. Some use LLMs to directly optimize the task instruction for in-context learning [33,\n34, 14]. Other studies use LLMs to mutate prompts for evolutionary algorithms [35, 36]. However, to\nthe best of our knowledge, no existing studies have investigated how LLMs could be used to optimize\ntext prompts within vision-language models. This approach could potentially open up new avenues\nfor integrating and enhancing the capabilities of vision-language models through more effective and\ncontextually appropriate text prompts.\n\nMeta-prompting. Suzgun and Kalai [37] introduce meta-prompting to transform a single LLM into\na versatile “conductor” capable of managing and integrating multiple independent LLM queries.\nBy using high-level instructions, meta-prompting guides the LLM in decomposing complex tasks\ninto smaller subtasks. The core of OPRO [14] involves designing a meta-prompt for LLMs to\noptimize prompts for each task. This meta-prompt includes two key pieces of information: previously\ngenerated prompts with their corresponding training accuracies and a description of the optimization\nproblem. Self-select [38] leverages meta-prompting to optimize instruction selection. It considers\na set of provided templates and chooses the most suitable template. Meta-prompting is related to\ninstruction tuning [39] as both techniques provide high-level guidance to improve the performance\nand adaptability of LLMs. However, while instruction tuning focuses on fine-tuning models with\na variety of tasks to improve generalization, meta-prompting offers the advantage of dynamically\nguiding the model to decompose and manage complex tasks in real-time. Liu et al. [40] proposes a\nmethod that utilizes LLMs as black-box optimizers for vision-language models, iteratively refining\nprompts based on in-context examples. Their approach focuses on leveraging ChatGPT to improve\nprompt templates for visual classification tasks. Mirza et al. [41] explores a different aspect of prompt\noptimization by focusing on zero-shot vision-language models. Our Prompt Optimization Prompt\nis akin to meta-prompting, it stores past prompts along with their corresponding accuracy and loss,\nthereby providing richer in-context information to enable LLMs to generate more effective prompts.\nDifferent from prior meta-prompting, our Prompt Optimization Prompt generates prompts beyond\nLLMs for vision-language models.\n\n3  Preliminaries\n\nContrastive Language-Image Pre-Training (CLIP). The goal of CLIP [30] is to develop an image\nencoder fI and a text encoder gT via contrastive pre-training with a large collection of paired images\nand captions. This process aims to map image-text pairs to a common semantic space. After the\npre-training phase, CLIP is able to perform zero-shot visual recognition by treating classification\nas a task of matching images to text. Specifically, the placeholder term “[CLASS]” is used within a\nprompt template (e.g., “a photo of a [CLASS]”) for the text encoder gT . Here, gT (Ti) denotes\nthe text features adapted for class i, and the probability of classifying class i from an image I is:\n\n                                     exp(⟨gT (Ti), fI(I)⟩/τ)\n                       p(y=i|I)=                                              ,                          (1)\n                  PKj=1 exp(⟨gT (Tj), fI(I)⟩/τ)\n\nwhere ⟨gT (Ti), fI(I)⟩represents the cosine similarity between the image feature fI(I) and the text\nfeature gT (Ti) specific to the i-th class, K is the total number of classes, and τ is the temperature\nparameter that is tuned during training.\n\nPrompt learning improves the adaptability of the CLIP model by eliminating the need for man-\nual prompt engineering. It facilitates the automatic generation of prompts using a limited number\nof examples from a downstream task. CoOp [8] presents a method where a set of M continu-\n\n\n                                       3\n\nous context vectors V ={v1, v2, . . . , vM} serve as the learnable prompt. The constructed prompt\nTi={v1, v2, . . . , vM, ci} merges these learnable context vectors V with the class-specific token\nembedding ci, which is then processed by the text encoder gT (·). In CoOp, the optimization of these\nstatic context vectors V aims to minimize the negative log-likelihood for the correct class token:\n\n                         LCE(V )= − X yi log p(Ti|I),                                (2)\n                                                                     i\n\nhere, yi represents the one-hot encoded ground-truth label for class i. In downstream applications,\nthe pre-trained model parameters are kept unchanged, which allows the learnable prompt vectors V\nto be optimized efficiently using only a small number of samples through the minimization of the\ncross-entropy loss.\n\n\n4  Methods\n\nFigure 1 depicts the comprehensive structure of our interpretable prompt optimizer. At each step of\nthe optimization, the LLM generates candidate prompts for the vision-language task by considering\nboth the description of the optimization problem and the feedback from previously evaluated prompts\nstored in the Prompt Optimization Prompt. These new prompts are then assessed and incorporated\ninto the Prompt Optimization Prompt for future optimization cycles. The optimization process\nconcludes either when the LLM can no longer generate prompts that improve the optimization scores,\nor when a predefined maximum number of optimization steps is reached. Next, we will detail the\ndesign of the Prompt Optimization Prompt and explain how image information is integrated into the\nPrompt Optimization Prompt.\n\nPrompt Optimization Prompt design  At the core of our optimizer is the design of the Prompt\nOptimization Prompt, which enhances the performance of the vision-language model by optimizing\nthe prompts through the prompt LLM. Figure 2 shows an example of our Prompt Optimization Prompt.\nOur Prompt Optimization Prompt consists of the following components: (1) Instructions: These\nguide the LLM by clearly defining its task to optimize the prompt for achieving better performance\nin classification tasks. (2) Textual descriptions of training images: These descriptions provide the\nLLM with detailed information about the images, enabling it to generate dataset-specific prompts.\n(3) Previously generated prompts and corresponding scores: This component supplies in-context\ninformation, including past prompts and their performance metrics, allowing the LLM to refine its\nprompt generation more accurately. By incorporating these elements, our approach leverages the\niterative refinement capabilities of LLMs to dynamically generate and optimize text prompts. The\ninstructions ensure that the LLM understands the optimization goal, the textual descriptions offer rich\nimage-related context, and the historical data aids in producing more effective and precise prompts.\n\nTextual descriptions of training images  For the textual descriptions of training images, we utilize\na large multimodal model (LMM) to generate text descriptions for each training image. Specifically,\nwe employ MiniCPM-V-2.0 [43] to generate descriptions of the content of images from base classes.\nIn the appendix, we provide content descriptions for some images from each dataset generated using\nMiniCPM-V-2.0. We denote the extracted image textual features as fM(·).\n\nAdditionally, we have attempted to directly optimize prompts using the LMM with the Prompt Opti-\nmization Prompt. Specifically, we input images from the base classes and the Prompt Optimization\nPrompt into the LMM, aiming for the LMM to generate better prompts. We experimented with six\ndifferent LMMs: BLIP-2 [44], Qwen-VL-Chat-9.6B [45], FUYU-8B [46], MiniCPM-V-2.0 [43], and\nllava-llama3-8B [47]. Unfortunately, all six models failed to understand our Prompt Optimization\nPrompt and generated new prompts that were merely descriptions of the images, not the universal\nprompts we desired. This failure might be due to the fact that the training of these LMMs did not\nconsider such a task. Note that image descriptions are not mandatory. In our 16-shot experiments,\nwe omitted the image descriptions in the Prompt Optimization Prompt due to the limited text input\nlength that the LLM can handle.\n\nEpisodic memory retrieval  We utilize an episodic memory mechanism to retrieve past prompts\nand their corresponding scores, which include metrics such as loss and accuracy. Here, we denote\nthe memory as M. During each iteration, we retrieve the top-20 prompts R(M), based on their\naccuracy from M and use them as the current memory, denoted as m. Moreover, we consistently\ninclude the prompt “a photo of <CLASS>” in our history at every step, as this is a frequently used\n\n\n                                       4\n\n# INPUT\n\n               Instruction 1\n\n              You need to perform image classification on the large-scale visual recognition dataset based on visual features.Here, <CLASS> represents\n              a class name from the large-scale visual recognition dataset, and it is essential to include <CLASS> in <INS>.\n\n               Below is the image description for each image:\n\n             Image descriptions\n\n                    Image 0: The image showcases a close-up view of the texture on an object with distinct stripes, buttons and stitching details.\n\n\n                    Image 1: The leaf has a yellowish background with darker spots and irregular black marks, indicating possible disease or damage.\n\n\n                                       (... more image examples. . . )\n\n                Instruction 2\n\n               Below are some previous prompts with their scores. The scores consist of loss and accuracy. The loss value is equal to or greater than 0,\n                  while the accuracy varies from 0 to 100.\n\n              Prompt history\n\n                    text prompt:\n\n              a photo of a <CLASS>.\n\n                 scores:\n\n                   loss: 1.6435546875, accuracy: 54.166664123535156\n\n\n                    text prompt:\n\n                    Identify the intricate details and unique patterns of the <CLASS> texture.\n\n                 scores:\n\n                   loss: 1.3935546875, accuracy: 70.83332824707031\n\n\n                           (... more previous prompts and scores . . . )\n\n                Instruction 3\n\n               Generate a prompt different from all the prompts <INS> above, with a lower loss and higher accuracy than all the prompts <INS> above. The\n               prompt should begin with <INS> and end with </INS>. The prompt should be concise, effective, and generally applicable to all problems above.\n\n\n        # OUTPUT\n                 Below are the prompts created according to your guidelines:\n\n                 (1) <INS> Analyze the intricate texture of <CLASS> with precision and attention to detail. </INS>\n\n                 (2) <INS> Thoroughly analyze and classify the intricate <CLASS> texture with meticulous precision. </INS>\n\n                 (3) <INS> Analyze, categorize, and classify the intricate <CLASS> texture with unparalleled precision and exceptional attention to detail. </INS>\n\n\n                        (... more output prompts examples. . . )\n\nFigure 2: An example of our Prompt Optimization Prompt with input and output on the DTD [42] dataset. The\nred text represents instructions given to the large language model, the blue text denotes the image descriptions\ngenerated by the large multimodal model, and the green text indicates the top-20 previously generated prompts\nretrieved from episodic memory along with their corresponding scores. yellow indicates the output prompt.\n\n\nand effective prompt within the CLIP framework [30]. Therefore, our optimization loss is defined as:\n\n\n                       LCE = − X yi log p(ˆTi|fM(I), m, I),                            (3)\n                                                          i\n\n\nwhere I indicates the our designed instruction for LLM, ˆT represents the new human-interpretable\ntext prompt optimized by the LLM. Note that our optimizer is parameter-free, which differentiates it\nfrom traditional gradient-based prompt learning methods. Instead, we leverage the LLM to optimize\nthe prompt, reducing LCE iteratively until convergence.\n\nThe input and output example in Figure 2 shows the structured information fed into the LLM, while\noutput demonstrates the optimized prompts generated by the LLM. For more detailed examples of\nPrompt Optimization Prompt input and output, please refer to the appendix.\n\n\n                                       5\n\n5  Experiments\n\n5.1  Experimental setup\n\nWe validate the effectiveness of our approach on the base-to-new generalization benchmark for\nevaluating prompt learning in vision-language models [8, 11]. Across all experiments, we benchmark\nthe models’ performance in a 1-shot and commonly used 16-shot setting. To ensure consistency, all\nresults from learning-based methods are averaged over three random seeds. We use the harmonic\nmean (H) as the average metric, which is a common approach in prompt learning for vision-language\nmodels.\n\nEleven Datasets. We follow CLIP [30] and CoOp [8] to use 11 image classification datasets, i.e.,\nImageNet [48] and Caltech101 [49] for generic object classification, OxfordPets [50], Stanford-\nCars [51], Flowers102 [52], Food101 [17] and FGVCAircraft [53] for fine-grained image recognition,\nEuroSAT [54] for satellite image classification, UCF101 [55] for action classification, DTD [42] for\ntexture classification, and SUN397 [56] for scene recognition.\n\nSix Baselines. To conduct a comparative evaluation, we utilize a number of established baselines\nincluding CLIP [30], Coop [8], CoCoOp [11], MaPLe [13], PromptSRC [28], and CoPrompt [32].\nNote that all methods do not present 1-shot results in their publications, so we perform 1-shot\nexperiments using their available code.\n\nTraining details. We use GPT-3.5 Turbo as our default optimizer, iterating 100 steps for each dataset\nto derive the final prompt. At each step, we generate five prompts and compare their accuracy with past\nprompts, storing the top-20 prompts in our history. Ultimately, we select the prompt with the highest\naccuracy as the final prompt. For generating image descriptions, we employ MiniCPM-V-2.0 [43]\nas the default LMM, using the prompt: “[Please generate the description in detail,\ndo not provide the class name in the description.]”. We added image descriptions\nto the 1-shot Prompt Optimization Prompt but not to the 16-shot version due to the character\ninput limitations of GPT-3.5 Turbo, which prevent adding detailed information for each class’s\nimages. All experiments were conducted on a GeForce RTX 3090. Our code is available at https:\n//github.com/lmsdss/IPO.\n\n\n5.2  Results\n\nInterpretable prompt analysis. To analyze the importance of specific words or phrases in text\nprompts, we display a comparative experiment on the Flower102 dataset. Specifically, in Table 1a,\nwe use “a photo of a <CLASS>” as the prompt, which is the most commonly utilized format. By\nemploying occlusion sensitivity analysis, we individually remove each word to test the importance of\nthe remaining words. We find that using just “<CLASS>” as the prompt performs only 0.89% lower\non the novel classes than “a photo of a <CLASS>”, indicating that the CLIP model can generate\nmore discriminative features using just the category name. Additionally, the prompt “a photo\n<CLASS>.” achieves the best performance. By comparing “<CLASS>” with “photo <CLASS>”, we\ndetermine that the word photo is particularly significant on the novel classes.\n\nIn Table 1b, we present the results of the CoOP and CoCoOP models when some learned prompt\ntokens are removed to analyze which token is most crucial. Surprisingly, for both models, performance\nimproves in novel classes when some or all tokens are removed. This indicates severe overfitting in\nbase classes by these models, where the learned tokens are only applicable to base classes. Removing\nall tokens allows the models to retain some of the original performance of CLIP on novel classes.\nAdditionally, these methods, which only learn tokens, make it challenging for humans to understand\nthe specific meanings of each token, complicating interpretation.\n\nIn Table 1c, we also demonstrate the final prompt produced by our model: “Identify the unique visual\nfeatures of the <CLASS> flower accurately,” which achieves a performance of 79.6%, surpassing the\noriginal CLIP by 2.9% on novel classes. When comparing this optimal prompt with others, removing\nany words from it results in worse performance than the original prompt. Specifically, comparing\n“Identify <CLASS>” with “<CLASS>” reveals that including \"Identify\" boosts performance by\n1.7% on novel classes. This highlights the importance of the word “Identify” in datasets like\nFlower102. We have included additional analyses on various datasets in the appendix.\n\nOverall, the comparative experiments demonstrate that our prompt can be more easily interpreted\nand understood by humans, while also providing insights into the significance of certain key words\n\n\n                                        6\n\nPrompts                  CLIP [30]                   Prompts           CoOP [8]          CoCoOP [11]\n                            Base   Novel   H                              Base   Novel   H    Base   Novel   H\n                                                                     Token 1, 2, 3, 4   71.47   72.47   71.97   73.67   75.50   74.57\n       a photo of a <CLASS>.   69.34   76.72   72.84           None            61.75   75.83   68.07   61.75   75.83   68.07\n     <CLASS>.              61.75   75.83   68.07            Token 1         76.02   73.38   74.68   75.83   76.92   76.37\n       a <CLASS>.            63.04   75.02   68.51            Token 2         70.07   74.26   72.10   76.75   76.98   76.86\n      photo <CLASS>.        64.72   76.74   70.22            Token 3         69.62   72.88   71.21   76.74   77.27   77.00\n                                                                     Token 4         70.85   67.75   69.27   76.51   77.64   77.07\n       of <CLASS>.           63.02   76.17   68.97            Token 1, 2       75.04   74.23   74.63   76.02   77.23   76.62\n       a photo <CLASS>.       66.52   77.25   71.48            Token 3, 4       70.13   66.75   68.40   76.07   78.75   77.39\n      photo of <CLASS>.      61.33   77.12   68.32            Token 1, 4       75.92   68.81   72.20   76.49   78.22   77.35\n       of a <CLASS>.          63.37   75.91   69.08            Token 1, 2, 3     72.55   74.04   73.29   76.51   75.53   76.02\n                                                                     Token 1, 2, 4     74.82   70.52   72.61   76.04   77.92   76.97\n       a photo of <CLASS>.    59.15   77.25   70.00            Token 1, 3, 4     76.25   66.15   70.84   75.53   78.37   76.92\n      photo of a <CLASS>.    69.16   76.72   72.74            Token 2, 3, 4     70.91   67.93   69.39   75.25   77.71   76.46\n\n                         (a) CLIP                                        (b) CoOP and CoCoOP.\n\n     Prompts                                                        IPO\n\n                                                                  Base   Novel   H\n\n       Identify the unique visual features of the <CLASS> flower accurately.   74.17   79.65   76.81\n     <CLASS>.                                                        61.75   75.83   68.07\n      Visual features of the <CLASS>.                                    64.42   74.64   69.15\n       Identify unique visual features <CLASS>.                            63.28   77.21   69.55\n       Identify the unique visual features of the <CLASS> accurately.         65.67   77.35   71.03\n     The unique visual features <CLASS> flower.                         66.42   76.67   71.18\n       Identify the unique <CLASS>.                                      67.22   77.73   72.09\n       Identify the <CLASS>.                                             68.88   76.35   72.42\n       Identify <CLASS>.                                                69.53   77.52   73.31\n      Features of the <CLASS> flower accurately.                          72.03   77.82   74.81\n      Visual features of the <CLASS> flower.                              71.93   78.55   75.09\n     The unique visual features of the <CLASS> flower accurately.          72.67   78.72   75.57\n       Identify the unique visual features of the <CLASS> flower.             73.82   79.11   76.37\n\n                                                 (c) This paper: IPO\nTable 1: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nFlower102 dataset [52]. The shaded areas in the table indicate the original performance of each method. Bold\nblue refers to the result of the best prompt for each model. The original CLIP model shows particular sensitivity\nto the word photo. In contrast, the tokens learned by CoOP and CoCoOP affect especially base class performance,\nwhile removing these learned tokens improves novel class performance. By contrast, with our interpretable\nprompt optimization, every word makes a meaningful contribution to both base and new classes. We provide\nresults for more datasets in the appendix.\n\n        Models          Base   Novel   H             Models              Base   Novel   H\n        Phi2-2.7B        71.15   75.43   73.22         FUYU-8B            70.98   75.45   73.14\n      PaLM 2-L       71.32   75.93   73.55           BLIP-2               71.95   76.52   73.16\n      PaLM 2-L-IT    71.13   76.16   73.56           Qwen-VL-Chat-9.6B   71.23   77.08   74.03\n        Phi3-7B         71.43   76.68   73.96           LLaVA-Llama-3-8B   73.17   75.24   74.19\n        GPT-3.5-turbo   71.76   77.00   74.29           MiniCPM-V-2-2.8B   71.76   77.00   74.29\n          (a) Impact of large language model.                 (b) Impact of large multimodal model.\n\nTable 2: Effect of LLM and LMM choice. We obtain best results with GPT-3.5-turbo as the LLM optimizer, and\nMiniCPM-V-2.0 for generating image descriptions for the Prompt Optimization Prompt.\n\nin the prompt. This understanding can guide us in identifying crucial words that enhance prompt\neffectiveness.\n\nEffect of LLM and LMM choice. In Table 2, we analyze the effect of the choice of the LLM\noptimizer and LMM for generating image descriptions as Prompt Optimization Prompt inputs. We\nevaluate various models and report their average performance across 11 datasets. Using GPT-3.5-\nturbo as the optimizer results in better prompt generation, especially improving performance on novel\nclasses. The training speed of our IPO is heavily influenced by the computational efficiency of LLMs\nused. Since PaLM and GPT-3.5 are not open-source, we rely on their respective APIs to generate\nprompts. Consequently, our training speed depends on the API call latency and the computational\ncomplexity of these models. Alternatively, Phi2 and Phi3 are open-source, allowing us to generate\nprompts directly using their weights. Therefore, for researchers and practitioners seeking faster and\nmore cost-effective training, we recommend utilizing open-source large models for prompt generation.\nWhen comparing different LMMs, we found that LLaVA-Llama-3-8B and MiniCPM-V-2.0 perform\n\n\n                                       7\n\nDataset          Best Prompt\n\n        ImageNet         Take a high-quality photo of a <CLASS>.\n        Caltech101         Categorize the <CLASS> shown in the image.\n                         Take a well-composed photo of a <CLASS> with optimal lighting, fo-\n        OxfordPets          cus, and minimal distractions. Capture the pet’s unique characteristics,\n                             including expression and posture, to ensure a clear and distinct image.\n       StanfordCars        Describe the distinguishing characteristics of the <CLASS> in the image.\n       Flowers102          Identify the unique visual features of the <CLASS> flower accurately.\n                               Identify the primary ingredient in the <CLASS> and describe its texture,\n        Food101\n                                color, and presentation.\n                           Capture a comprehensive range of well-lit, high-resolution images of an\n      FGVCAircraft     <CLASS> from various angles, meticulously showcasing its specific de-\n                              sign features with perfect clarity and precision for unparalleled accuracy\n                               in aircraft.\n       SUN397      A photo of a <CLASS>, a type of large-scale scene.\n       DTD             Classify the intricate <CLASS> texture.\n                         Analyze the <CLASS> vehicles in the satellite image with state-of-the-\n        EuroSAT\n                                  art algorithms for precise classification and optimal efficiency.\n                          Capture a high-quality, well-lit image of a person flawlessly demon-\n       UCF101            strating the <CLASS> action with impeccable visual representation to\n                            achieve unmatched.\n\n        Table 3: Interpretable prompts generated by our method for each dataset in 1-shot scenarios.\n\nalmost identically on average. However, MiniCPM-V-2.0 shows better performance on novel classes.\nBefore using LLMs, we first use LMMs to generate image descriptions, which are then used as\ninputs for the LLMs. In terms of computation cost, MiniCPM-V-2.0, with its smaller parameter size,\ngenerates descriptions more quickly. Therefore, we recommend this lightweight LMM as the image\ndescription generator for more efficient processing. In summary, the text comprehension capabilities\nof LLMs are crucial for determining the quality of the optimized prompts.\n\nInterpretable prompts generated per dataset. Table 3 showcases the diverse prompts generated by\nour optimizer for each dataset. Our method enhances model accuracy by concentrating on the most\nrelevant attributes, such as those in Flowers102 [52] and Food101 [17]. Consequently, it delivers\nhigh-quality text prompts that improve vision-language models. We encourage future researchers to\nleverage these interpretable prompts in their own downstream tasks.\n\nBenefit of image description. In Table 4, we assess the impact of incorporating image descriptions\ninto the Prompt Optimization Prompt on the performance of the optimizer. The inclusion of image\n descriptions enhances the model’s performance. This improve-\nment suggests that IPO, when generating new prompts, can ef-                                                                                  Base  New   H\nfectively integrate information from the images themselves. As\n                                                               CLIP       69.34   74.22   71.70\na result, the optimizer is able to produce prompts that are more\nspecific to the data, thereby increasing the relevance and accu-     w/o LMM   71.12   76.03   73.49\n                                                                    w/ LMM    71.76   77.00   74.29\nracy of the generated content. This highlights the importance\nof multimodal inputs in optimizing the prompting abilities of  Table 4: Benefit of image description.\nvision-language models.\n\nComparison with knowledge bank-based prompt learning methods. Our IPO leverages LLMs to\noptimize prompts, which conceptually aligns with previous bank-based prompt learning approaches.\nTo assess its relative effectiveness, we compared IPO to traditional bank-based methods, specifically\nusing L2P [57], which learns to dynamically prompt (L2P) a pre-trained model to learn tasks\nsequentially under different task transitions. We apply L2P within the visual prompt tuning (VPT) [12]\nframework, which also learns prompts in the visual space and applies them to few-shot vision-\n language model tasks.  Similar to our IPO, VPT +\nL2P learns prompts within the visual space and applies      Model               Base   Novel   H\nthem to few-shot VLM tasks. In this setup, VPT + L2P\n                                                   VPT [12]             69.34   74.22   71.70\ntrains a prompt bank, allowing test samples to query the     VPT [12] + L2P [57]   71.12   76.03   73.49\nbank for suitable prompts during testing. The table 5     VPT [12] + IPO       71.76   77.00   74.29\npresents a comparison of VPT + L2P and our method  Table 5: Comparison with knowledge bank-\nacross 11 datasets in the 16-shot setting. While L2P  based prompt learning methods.\ncontributes to enhanced VPT performance, confirming\n\n\n                                       8\n\nViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H\n      CLIP      69.34   74.22   71.70      CLIP      72.43   68.14   70.22      CLIP      96.84   94.00   95.40\n     CoOp     72.08   66.71   69.29     CoOp     73.20   67.43   70.20     CoOp     90.63   85.20   87.83\n    CoCoOp    72.85   72.17   72.51     CoCoOp    73.90   69.07   71.40     CoCoOp    96.37   93.13   94.72\n     MaPLe     70.85   71.57   71.21     MaPLe     74.03   68.73   71.28     MaPLe     96.40   94.10   95.24\n   PromptSRC   73.38   71.47   72.41    PromptSRC   73.27   68.87   71.00    PromptSRC   97.30   95.57   96.43\n    CoPrompt   70.44   70.11   70.27     CoPrompt   73.97   70.87   72.39     CoPrompt   97.60   95.57   96.57\n     IPO      71.76   77.00   74.29      IPO      74.09   69.17   71.54      IPO      96.53   95.39   95.95\n     (a) Average over 11 datasets.              (b) ImageNet                        (c) Caltech101\n\n\n     ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H\n      CLIP      91.17   97.26   94.12      CLIP      63.37   74.89   68.65      CLIP      72.08   77.80   74.83\n     CoOp     93.73   96.23   94.96     CoOp     61.80   68.33   64.90     CoOp     83.97   67.10   74.59\n    CoCoOp    93.47   96.27   94.85     CoCoOp    65.27   73.73   69.24     CoCoOp    75.57   77.00   76.28\n     MaPLe     90.83   96.00   93.34     MaPLe     66.00   73.67   69.62     MaPLe     77.10   76.97   77.03\n   PromptSRC   93.73   97.33   95.50    PromptSRC   67.93   73.73   70.71    PromptSRC   85.57   74.83   79.84\n    CoPrompt   92.37   96.37   94.33     CoPrompt   64.17   71.50   67.64     CoPrompt   72.90   72.93   72.91\n     IPO      94.48   97.93   96.43      IPO      63.83   75.45   69.16      IPO      74.17   79.65   76.81\n            (d) OxfordPets                      (e) StanfordCars                        (f) Flowers102\n\n\n     ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H\n      CLIP      90.10   91.22   90.66      CLIP      27.19   36.29   31.09      CLIP      69.36   75.35   72.23\n     CoOp     87.90   88.03   87.96     CoOp     27.77   27.60   27.68     CoOp     71.47   72.47   71.97\n    CoCoOp    88.73   89.60   89.16     CoCoOp    29.77   31.23   30.48     CoCoOp    73.67   75.50   74.57\n     MaPLe     89.13   90.67   89.89     MaPLe     28.33   29.00   28.66     MaPLe     74.33   76.37   75.34\n   PromptSRC   88.30   91.03   89.64    PromptSRC   10.93    6.73    8.33    PromptSRC   75.60   77.07   76.33\n    CoPrompt   88.40   90.60   89.49     CoPrompt   10.10    4.87    6.57     CoPrompt   76.37   78.77   77.55\n     IPO      89.78   91.59   90.67      IPO      31.43   36.32   33.70      IPO      72.25   77.53   74.80\n              (g) Food101                    (h) FGVCAircraft                           (i) SUN397\n\n\n     ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H\n      CLIP      53.24   59.90   56.37      CLIP      56.48   64.05   60.03      CLIP      70.53   77.50   73.85\n     CoOp     60.80   47.53   53.35     CoOp     69.13   50.33   58.25     CoOp     72.50   63.57   67.74\n    CoCoOp    58.70   52.70   55.54     CoCoOp    71.13   62.87   66.75     CoCoOp    74.73   72.80   73.75\n     MaPLe     58.20   54.17   56.11     MaPLe     50.20   51.20   50.70     MaPLe     74.83   76.43   75.62\n   PromptSRC   63.17   55.60   59.14    PromptSRC   73.27   67.00   70.00    PromptSRC   78.13   78.37   78.25\n    CoPrompt   62.77   60.40   61.56     CoPrompt   59.27   51.60   55.17     CoPrompt   76.93   77.73   77.33\n     IPO      55.45   62.47   58.75      IPO      64.97   82.13   72.54      IPO      72.43   79.35   75.73\n                  (j) DTD                         (k) EuroSAT                              (l) UCF101\n\nTable 6: Comparison with existing state-of-the-art methods for base-to-novel generalization using 1-shot\nlearning. Except for CLIP, the results for other methods are based on our reimplementation of their official code.\nOur proposed IPO exhibits robust generalization capability and achieves significant improvements on novel\nclasses across 11 datasets.\n\n\nL2P’s efficacy in the VLM domain, our IPO method still demonstrates superior results compared to\nVPT + L2P. This comparison, which we will include in the revised manuscript, reinforces that IPO\n’s effectiveness is not merely due to a memory retrieval mechanism but also benefits from prompt\noptimization through LLMs.\n\nComparison with state-of-the-art. Table 6 shows the comparative experiments of IPO against\nthe state-of-the-art across 11 datasets in a 1-shot setting. Our method excels in the novel classes,\n  surpassing  the second-best  performer,  the  original                                                                             Base   Novel   H\nCLIP [30], by 2.78% in average performance. Other meth-\n                                                      CLIP [30]         69.34   74.22   71.70ods do not perform as well as CLIP in novel classes, indicat-\n                                                CoOP [8]         82.69   63.22   71.66\ning overfitting to base classes. For instance, CoCoOP [11]                                                 CoCoOp [11]      80.47   71.69   75.83\nperforms better in base classes but falls behind the origi-   MaPLe [13]       82.28   75.14   78.55\nnal CLIP by 2.05% in novel classes. In particular, on the    PromptSRC [28]   84.26   76.10   79.97\nmost challenging FGVCAircraft [53], and EuroSAT [54]    CoPrompt [32]    84.00   77.23   80.48\ndataset, the current state-of-the-art model, CoPrompt [32],   IPO (1-shot)      71.76   77.00   74.29\n                                                 IPO (16-shot)     79.92   80.51   80.21\nperforms poorly. This is because methods based on prefix-\ntuning, like CoPrompt [32], PromptSRC [28], require  Table 7: Comparison with gradient-based\nsubstantial amounts of data for training to achieve ade-  prompt learning methods for 16-shots across\n                                                      11 datasets.quate generalization. Consequently, on more challenging\ndatasets, when data is scarce, it becomes difficult to fine-\n\n\n                                       9\n\ntune these prefixes effectively. In contrast, our model outperforms other methods, exceeding the\nsecond-best by 1.78% in harmonic mean. Additionally, in Table 7, we compared our method’s\nperformance with other traditional gradient-based prompt learning methods on a 16-shot setting\nacross all datasets, where our approach consistently performs well in novel classes. Demonstrating\nthat our approach can mitigate overfitting and generalize better to novel classes.\n\n6  Conclusion\n\nIn this paper, we presented a novel approach to prompt optimization for vision-language models,\naddressing the limitations of existing gradient-descent-based methods. By integrating large language\nmodels for dynamic text prompt generation and optimization, we introduced the IPO system. This\nsystem guides LLMs in crafting effective prompts while maintaining a record of past prompts and\ntheir performance metrics, offering valuable in-context information. Additionally, we incorporated\nlarge multimodal models to generate image descriptions, enhancing the synergy between textual and\nvisual modalities. Our comprehensive evaluation across 11 datasets demonstrated that our method\nimproves the initial accuracy of vision-language models compared to traditional gradient-descent-\nbased prompt learning methods. Most notably, our approach significantly enhances the interpretability\nof the generated prompts. By leveraging the strengths of LLMs, IPO ensures that the prompts remain\nhuman-understandable, thereby facilitating better transparency and oversight for vision-language\nmodels. This improvement in interpretability is crucial, as it allows for more effective and trustworthy\nhuman-AI collaboration, making vision-language systems more reliable and accessible.\n\nLimitation. Our IPO method is primarily designed for few-shot scenarios. However, when dealing\nwith large domain-specific datasets, the need to generate extensive image descriptions, which can\nlead to substantial computational costs due to the large text inputs required for LLMs. Currently, our\nmodel uses an input length of approximately 5,000 tokens. When scaled to larger datasets, the input\nlength may increase to around 50,000 tokens. Using GPT-4 with an 8k context length, the cost for\nour current input size (5,000 tokens) is approximately 0.15 dollars per input (0.03 dollars per 1,000\ntokens). For the expanded input size of 50,000 tokens, the cost would rise to approximately 3.00\ndollars per input. If we were to use GPT-4 with a 32k context length, the cost for the 50,000-token\ninput would be approximately 3.00 dollars for the first 32,000 tokens and an additional 1.08 dollars\nfor the remaining 18,000 tokens, totaling approximately 4.08 dollars per input. Since our IPO method\nrequires 100 iterations during training, the costs would multiply accordingly when scaled to large\ninputs. In future work, we aim to investigate methods for LMM fine-tuning to enable the direct input\nof both images and text, thereby generating even more sample-specific prompts.\n\nBroader Impact. This paper explores the use of an LLM as an optimizer for refining text prompts\nin vision-language models. We introduce a straightforward yet interpretable approach to prompt\noptimization, which holds potential for societal impact, particularly in vision-language tasks.\n\nAcknowledgment\n\nThis work is financially supported by the Inception Institute of Artificial Intelligence, the University\nof Amsterdam and the allowance Top consortia for Knowledge and Innovation (TKIs) from the\nNetherlands Ministry of Economic Affairs and Climate Policy.\n\n\n\n\n\n                                       10\n\nReferences\n\n [1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n     Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n    models from natural language supervision. In ICML, pages 8748–8763. PMLR, 2021.\n\n [2] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\n     Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\n     with noisy text supervision. In ICML, pages 4904–4916. PMLR, 2021.\n\n [3] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang,\n    Qi Tian, and Siliang Tang. Fine-grained semantically aligned vision-language pre-training.\n     NeurIPS, 35:7290–7303, 2022.\n\n [4] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\n      Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n     pages 740–755. Springer, 2014.\n\n [5] Mengze Li, Tianbao Wang, Haoyu Zhang, Shengyu Zhang, Zhou Zhao, Jiaxu Miao, Wenqiao\n     Zhang, Wenming Tan, Jin Wang, and Peng Wang. End-to-end modeling via information tree for\n     one-shot natural language spatial video grounding. In ACL, pages 8707–8717, 2022.\n\n [6] Mengze Li, Han Wang, Wenqiao Zhang, Jiaxu Miao, Zhou Zhao, Shengyu Zhang, Wei Ji, and\n     Fei Wu. Winner: Weakly-supervised hierarchical decomposition and alignment for spatio-\n     temporal video grounding. In CVPR, pages 23090–23099, 2023.\n\n [7] Wenqiao Zhang, Haochen Shi, Jiannan Guo, Shengyu Zhang, Qingpeng Cai, Juncheng Li, Sihui\n     Luo, and Yueting Zhuang. Magic: Multimodal relational graph adversarial inference for diverse\n     and unpaired text-based image captioning. In AAAI, pages 3335–3343, 2022.\n\n [8] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for\n     vision-language models. arXiv preprint arXiv:2109.01134, 2021.\n\n [9] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\n    prompt tuning. In EMNLP, 2021.\n\n[10] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\n     understands, too. arXiv preprint arXiv:2103.10385, 2021.\n\n[11] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning\n      for vision-language models. In CVPR, 2022.\n\n[12] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan,\n     and Ser-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.\n\n[13] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fa-\n    had Shahbaz Khan. Maple: Multi-modal prompt learning. In CVPR, pages 19113–19122,\n     2023.\n\n[14] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\n     Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.\n\n[15] Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, and\n     Minlie Huang. Black-box prompt optimization: Aligning large language models without model\n      training. arXiv preprint arXiv:2311.04155, 2023.\n\n[16] Hao Sun. Offline prompt evaluation and optimization with inverse reinforcement learning.\n     arXiv preprint arXiv:2309.06553, 2023.\n\n[17] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative\n    components with random forests. In ECCV, 2014.\n\n[18] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Auto-\n     prompt: Eliciting knowledge from language models with automatically generated prompts.\n     arXiv preprint arXiv:2010.15980, 2020.\n\n\n                                       11\n\n[19] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\n    models know? TACL, 8:423–438, 2020.\n\n[20] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\n      Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language\n     processing. ACM Computing Surveys, 55(9):1–35, 2023.\n\n[21] Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor G Turrisi da Costa,\n    Cees GM Snoek, Georgios Tzimiropoulos, and Brais Martinez. Bayesian prompt learning for\n     image-language model generalization. In ICCV, pages 15237–15246, 2023.\n\n[22] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution\n      learning. In CVPR, pages 5206–5215, 2022.\n\n[23] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient\n      for prompt tuning. In ICCV, pages 15659–15669, 2023.\n\n[24] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-\n     guided context optimization. In CVPR, pages 6757–6767, 2023.\n\n[25] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual\n     prompts for adapting large-scale models. arXiv preprint arXiv:2203.17274, 2022.\n\n[26] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. Anomalyclip: Object-\n     agnostic prompt learning for zero-shot anomaly detection. arXiv preprint arXiv:2310.18961,\n     2023.\n\n[27] Zheng Li, Xiang Li, Xinyi Fu, Xing Zhang, Weiqiang Wang, and Jian Yang.  Promptkd:\n     Unsupervised prompt distillation for vision-language models. arXiv preprint arXiv:2403.02781,\n     2024.\n\n[28] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan\n    Yang, and Fahad Shahbaz Khan.  Self-regulating prompts: Foundational model adaptation\n     without forgetting. In CVPR, pages 15190–15200, 2023.\n\n[29] Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, and Cees GM Snoek.\n     Any-shift prompting for generalization over distributions. arXiv preprint arXiv:2402.10099,\n     2024.\n\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n     Girish Sastry, Amanda Askell, Pamela Mishkin, and Jack Clark. Learning transferable visual\n    models from natural language supervision. In ICML, 2021.\n\n[31] Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, Luc Van Gool, and\n     Federico Tombari. Learning to prompt with text only supervision for vision-language models.\n     arXiv preprint arXiv:2401.02418, 2024.\n\n[32] Shuvendu Roy and Ali Etemad.  Consistency-guided prompt learning for vision-language\n     models. arXiv preprint arXiv:2306.01195, 2023.\n\n[33] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\n     and Jimmy Ba. Large language models are human-level prompt engineers. In ICLR, 2022.\n\n[34] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic\n     prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495,\n     2023.\n\n[35] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang\n     Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields\n     powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023.\n\n[36] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rock-\n      täschel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint\n     arXiv:2309.16797, 2023.\n\n\n                                       12\n\n[37] Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with\n     task-agnostic scaffolding. arXiv preprint arXiv:2401.12954, 2024.\n\n[38] Keshav Ramji and Alexander Kyimpopkin. Self-select: Optimizing instruction selection for\n     large language models. In NeurIPS 2023 Foundation Models for Decision Making Workshop,\n     2023.\n\n[39] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li,\n     Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey.\n     arXiv preprint arXiv:2308.10792, 2023.\n\n[40] Shihong Liu, Samuel Yu, Zhiqiu Lin, Deepak Pathak, and Deva Ramanan. Language models as\n     black-box optimizers for vision-language models. In CVPR, pages 12687–12697, 2024.\n\n[41] M Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, Mateusz Kozinski,\n     Hilde Kuhene, and Horst Possegger. Meta-prompting for automating zero-shot visual recognition\n     with llms. arXiv preprint arXiv:2403.11755, 2024.\n\n[42] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.\n     Describing textures in the wild. In CVPR, 2014.\n\n[43] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao\n    Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and\n    Maosong Sun. Large multilingual models pivot zero-shot multimodal learning across languages.\n     arXiv preprint arXiv:2308.12038, 2023.\n\n[44] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.  Blip-2: Bootstrapping language-\n    image pre-training with frozen image encoders and large language models. In ICML, pages\n    19730–19742. PMLR, 2023.\n\n[45] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\n    Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,\n      localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023.\n\n[46] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani,\n    and Sagnak Tasırlar.  Introducing our multimodal models, 2023. URL https://www. adept.\n     ai/blog/fuyu-8b, 2, 2023.\n\n[47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS,\n     36, 2023.\n\n[48] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n      hierarchical image database. In CVPR, pages 248–255, 2009.\n\n[49] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\n     examples: An incremental bayesian approach tested on 101 object categories. In CVPR, 2004.\n\n[50] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In\n    CVPR, 2012.\n\n[51] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.  3d object representations for\n     fine-grained categorization. In ICCV, 2013.\n\n[52] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large\n    number of classes. In ICVGIP, 2008.\n\n[53] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-\n     grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n\n[54] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel\n     dataset and deep learning benchmark for land use and land cover classification. IEEE Journal\n      of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n\n[55] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human\n     actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n\n\n                                       13\n\n[56] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\n     Large-scale scene recognition from abbey to zoo. In CVPR, 2010.\n\n[57] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su,\n     Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In\n    CVPR, pages 139–149, 2022.\n\n[58] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. A\n     simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language\n     model. In European Conference on Computer Vision, pages 736–753. Springer, 2022.\n\n[59] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting\n      clip for zero-shot semantic segmentation. In CVPR, pages 11175–11185, 2023.\n\n[60] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic\n     projection network for zero-and few-label semantic segmentation. In CVPR, pages 8256–8265,\n     2019.\n\n[61] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Pérez.  Zero-shot semantic\n     segmentation. NeurIPS, 32, 2019.\n\n[62] Zhangxuan Gu, Siyuan Zhou, Li Niu, Zihan Zhao, and Liqing Zhang. Context-aware feature\n     generation for zero-shot semantic segmentation. In ICMM, pages 1921–1929, 2020.\n\n[63] Jiaxin Cheng, Soumyaroop Nandi, Prem Natarajan, and Wael Abd-Almageed. Sign: Spatial-\n     information incorporated generative network for generalized zero-shot semantic segmentation.\n     In ICCV, pages 9556–9566, 2021.\n\n[64] Donghyeon Baek, Youngmin Oh, and Bumsub Ham. Exploiting a joint embedding space for\n     generalized zero-shot semantic segmentation. In ICCV, pages 9536–9545, 2021.\n\n[65] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmen-\n      tation. In CVPR, pages 11583–11592, 2022.\n\n[66] Yassine Ouali, Adrian Bulat, Brais Matinez, and Georgios Tzimiropoulos. Black box few-shot\n     adaptation for vision-language models. In ICCV, pages 15534–15546, 2023.\n\n[67] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang.\n      Plot: Prompt learning with optimal transport for vision-language models.  arXiv preprint\n     arXiv:2210.01253, 2022.\n\n\n\n\n\n                                       14\n\nViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H\n     CoOp     82.69   63.22   71.66     CoOp     76.47   67.88   71.92     CoOp     98.00   89.81   93.73\n    CoCoOp    80.47   71.69   75.83     CoCoOp    75.98   70.43   73.10     CoCoOp    97.96   93.81   95.84\n    MaPLe     82.28   75.14   78.55     MaPLe     76.66   70.54   73.47     MaPLe     97.74   94.36   96.02\n   PromptSRC   84.26   76.10   79.97    PromptSRC   77.60   70.73   74.01    PromptSRC   98.10   94.03   96.02\n     IPO      79.92   80.51   80.21      IPO      77.83   72.45   75.04      IPO      97.32   95.23   96.26\n     (a) Average over 11 datasets.              (b) ImageNet                        (c) Caltech101\n\n\n     ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H\n     CoOp     93.67   95.29   94.47     CoOp     78.12   60.40   68.13     CoOp     97.60   59.67   74.06\n    CoCoOp    95.20   97.69   96.43     CoCoOp    70.49   73.59   72.01     CoCoOp    94.87   71.75   81.71\n    MaPLe     95.43   97.76   96.58     MaPLe     72.94   74.00   73.47     MaPLe     95.92   72.46   82.56\n   PromptSRC   95.33   97.30   96.30    PromptSRC   78.27   74.97   76.58    PromptSRC   98.07   76.50   85.95\n     IPO      95.21   98.23   96.70      IPO      73.42   75.71   74.55      IPO      96.78   78.32   86.58\n            (d) OxfordPets                      (e) StanfordCars                        (f) Flowers102\n\n\n     ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H\n     CoOp     88.33   82.26   85.19      CoOp      40.44   22.30   28.75     CoOp     80.60   65.89   72.517\n    CoCoOp    90.70   91.29   90.99     CoCoOp    33.41   23.71   27.74     CoCoOp    79.74   76.86   78.27\n    MaPLe     90.71   92.05   91.38     MaPLe     37.44   35.61   36.50     MaPLe     80.82   78.70   79.75\n   PromptSRC   90.67   91.53   91.10    PromptSRC   42.73   37.87   40.15    PromptSRC   82.67   78.47   80.52\n     IPO      90.92   93.08   91.99      IPO      41.21   41.42   41.31      IPO      81.25   80.92   81.08\n             (g) Food101                    (h) FGVCAircraft                           (i) SUN397\n\n\n     ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H       ViT-B/16    Base   Novel   H\n     CoOp     79.44   41.18   54.24     CoOp     92.19   54.74   68.69     CoOp     84.69   56.05   67.46\n    CoCoOp    77.01   56.00   64.85     CoCoOp    87.49   60.04   71.21     CoCoOp    82.33   73.45   77.64\n    MaPLe     80.36   59.18   68.16     MaPLe     94.07   73.23   82.35     MaPLe     83.00   78.66   80.77\n   PromptSRC   83.37   62.97   71.75    PromptSRC   92.90   73.90   82.32    PromptSRC   87.10   78.80   82.74\n     IPO      82.14   66.81   73.69      IPO      94.25   80.11   86.61      IPO      85.32   80.92   83.06\n                  (j) DTD                         (k) EuroSAT                              (l) UCF101\n\nTable 8: Comparison with existing state-of-the-art methods for base-to-novel generalization using 16-shots\nlearning. Our proposed IPO exhibits robust generalization capability and achieves significant improvements on\nnovel classes across 11 datasets.\n\n             Source                                                         Target\n             ImageNet   Caltech101  OxfordPets   StanfordCars  Flowers102  Food101   Aircraft  SUN397  DTD  EuroSAT  UCF101  Average\n\n  CoOp       71.51      93.70      89.14      64.51       68.71     85.30    18.47    64.15   41.92    46.39    66.55    63.88\n  CoCoOp    71.02      94.43      90.14      65.32       71.88     86.06    22.94    67.36   45.73    45.37    68.21    65.74\n  IPO        72.15      94.34      90.96      66.10       72.75     86.75    25.14    67.97   47.01    48.56    69.23    67.36\n\nTable 9: Cross-dataset generalization. Accuracy (%) evaluation for prompts learned from the source dataset.\nOur IPO consistently outperforms existing prompt learning methods.\n\n\nA  Experiments on 16-Shots\n\nWe report the 16-shot performance of our IPO method across 11 datasets, providing detailed results\nfor the Base, Novel, and H metrics in Table 8. Our IPO method consistently outperforms all other\napproaches on the novel classes and the H metric, highlighting its effectiveness in reducing overfitting.\n\n\nB  Experiments on cross-dataset\n\nWe conducted a comprehensive cross-dataset experimental evaluation using the standard 16-shot\nsetting to assess the performance of our IPO method. The results, presented in Table 9, demonstrate\nthat IPO consistently outperforms previous gradient-based prompt learning approaches. By apply-\ning our task-agnostic, LLM-driven prompt optimization technique, IPO achieved superior results\nacross various datasets, showcasing its robustness and generalizability. These findings highlight the\neffectiveness of IPO in adapting to diverse tasks and domains, further reinforcing its advantage over\ntraditional gradient-based methods in few-shot learning scenarios.\n\n\n                                       15\n\nLLM           Params    LMM      Params Base  Novel   H\n\n     GPT-3.5-turbo    175B    MiniCPM-V-2      2.8B      71.76   77.00   74.29\n     GPT-4          175B    MiniCPM-V-2      2.8B      72.67   77.62   75.06\n     GPT-4-o        175B    MiniCPM-V-2      2.8B      72.91   78.13   75.42\n     GPT-3.5-turbo    175B       GPT-4o      500B ˜ 1T    72.78   77.92   75.26\n     GPT-4        500B ˜ 1T     GPT-4o      500B ˜ 1T    72.93   78.01   75.38\n     GPT-4-o       500B ˜ 1T     GPT-4o      500B ˜ 1T    73.41   78.93   76.06\n\n                             Table 10: Impact of large language model.\n\n\n            LLM             Params    Base   Novel   H\n\n                CLIP                       -       69.34   74.22   71.70\n                 w/o LMM                 -       71.12   76.03   73.49\n                w/MiniCPM-V-2     2.8B     71.76   77.00   74.29\n                 w/GPT-4o       500B ˜ 1T   72.78   77.92   75.26\n\n                             Table 11: Impact of large language model.\n\n\nC  Impact of LLM\n\nAs demonstrated in Table 10, upgrading the LLM capacity yielded a similarly positive impact on\nperformance. To explore how a more advanced LLM, like GPT-4, could generate more effective\nprompts for our model, we conducted additional experiments with both GPT-4 and GPT-4o. Specifi-\ncally, when we enhanced the LLM to GPT-4o and paired it with the GPT-4o LMM, we observed a\nsignificant overall increase in the H-score by 1.77% compared to the initial setup using GPT-3.5-turbo\nalongside MiniCPM-V-2. This improvement underscores the advantages of employing larger, more\ncapable models, as they facilitate greater task generalization and more robust performance. The\nfindings suggest that scaling up model capacity in both the LMM and LLM components can lead\nto substantial gains in prompt quality and adaptability across various tasks, indicating a promising\ndirection for further enhancing our model’s versatility and effectiveness.\n\nD  Experiments on Segmentation Task with IPO\n\nIn prompt-based vision tasks like segmentation and detection, the design of the text prompt plays\na pivotal role. Our task-agnostic method, IPO, can be seamlessly integrated into various vision\ntasks to optimize text prompts. For example, as shown in Table 13, we applied IPO to pre-trained\nsemantic segmentation models [58, 59], where the original prompt used was ’a photo of a [CLASS].’\nBy utilizing GPT-4o as both the LLM and LMM, we generated more effective, context-specific\ntext prompts tailored to the open-vocabulary semantic segmentation task, leading to significant\nperformance improvements. These results highlight the potential of IPO to enhance text prompt\ndesign in segmentation tasks, showcasing its adaptability and value. We plan to extend our exploration\nof IPO to other vision tasks in future studies, aiming to further validate its effectiveness in optimizing\nprompt construction across a broader range of applications.\n\nIPO with GPT-3.5 Turbo, indeed, does not show an improvement on the large-scale ImageNet. This\nis because ImageNet has a large number of classes and samples, which results in longer LLM input\nwhen generating descriptions for each sample. GPT-3.5 Turbo has limited performance in handling\nlong-text inputs. The table 12 shows the results on ImageNet when IPO uses GPT-4o, which has\nsuperior long-text understanding compared to GPT-3.5 Turbo. We found that IPO using GPT-4o leads\nto better performance improvements over other methods as well as a considerable improvement over\nIPO with GPT-3.5 Turbo.\n\nE  Experiments on segmentation task with IPO\n\nIn other prompt-based vision tasks, such as segmentation and detection, the design of the text prompt\nis crucial. Our method, being task-agnostic, can be easily embedded into any vision task to optimize\n\n\n                                       16\n\nModel           Base   Novel   H\n\n                     CLIP            72.43   68.14   70.22\n                   CoOp            73.20   67.43   70.20\n                   CoCoOp         73.90   69.07   71.40\n                   MaPLe           74.03   68.73   71.28\n                      CoPrompt        73.97   70.87   72.39\n                     IPO w/ GPT-3.5   74.09   69.17   71.54\n                     IPO w/ GPT-4o   76.14   72.13   74.09\n\n                        Table 12: Performance on large-scale generic datasets.\n\n\n\n                 Methods        pAcc  mIoU (S)  mIoU (U)  hIoU\n\n                SPNet [60]            -       78.0        15.6      26.1\n               ZS3 [61]               -       77.3        17.7      28.7\n               CaGNet [62]      80.7      78.4        26.6      39.7\n              SIGN [63]             -       75.4        28.9      41.7\n                     Joint [64]              -       77.7        32.5      45.9\n                  Zegformer [65]       -       86.4        63.6      73.3\n                  Zsseg [58]        90.0      83.5        72.5      77.5\n                ZegCLIP [59]     94.6      91.9        77.8      84.3\n                  Zsseg + IPO      91.2      84.7        73.2      78.6\n                ZegCLIP + IPO   95.3      92.7        78.7      85.1\n\n                           Table 13: Experiments on segmentation tasks.\n\n\n\nthe text prompt. For instance, as shown in Table 13, we incorporated IPO into pre-trained semantic\nsegmentation models [58, 59], where the original text prompt was \"a photo of a [CLASS].\" Using\nGPT-4o as the LLM and LMM, we crafted more effective text prompts specifically suited to the\nopen-vocabulary semantic segmentation task, leading to enhanced performance and demonstrating\nthe value of IPO in optimizing text prompts for this application. We intend to further investigate the\nuse of IPO in other vision tasks in future work.\n\n\nF  Effect of batch size\n\nThe Table 14 compares the performance of GPT-3.5 turbo and GPT-4o across different batch sizes. We\nobserved that as the batch size increases to 128, GPT-3.5 turbo’s performance begins to decline due to\nits limited capacity for handling longer input texts effectively. In contrast, GPT-4o maintains strong\nperformance even at larger batch sizes. However, using extremely large batch sizes with GPT-4o\nbecomes cost-prohibitive. Therefore, we selected a batch size of 128 for our experiments. Although\n\n\n\n                 Model           Batch size   Base   Novel   H\n\n                IPO w/ GPT-3.5      4       73.11   68.08   70.51\n                IPO w/ GPT-4o       4       74.32   67.98   70.55\n                IPO w/ GPT-3.5      16      73.42   68.43   70.82\n                IPO w/ GPT-4o      16      74.94   70.75   72.78\n                IPO w/ GPT-3.5      32      73.79   68.72   71.16\n                IPO w/ GPT-4o      32      75.01   70.93   72.91\n                IPO w/ GPT-3.5      64      74.09   69.17   71.54\n                IPO w/ GPT-4o      64      75.34   71.23   73.45\n                IPO w/ GPT-3.5     128      73.67   68.07   70.75\n                IPO w/ GPT-4o      128      76.14   72.13   74.09\n                IPO w/ GPT-3.5     256      73.11   67.81   70.36\n                IPO w/ GPT-4o      256      76.81   72.73   74.71\n\n                                  Table 14: Effect of batch size.\n\n                                       17\n\nHistory length   Base   Novel   H\n\n                        n = 0           69.15   75.20   72.04\n                        n = 1           70.25   75.43   72.74\n                        n = 5           70.95   76.21   73.49\n                        n = 10          71.23   76.41   73.72\n                        n = 20          71.76   77.00   74.29\n                        n = 50          71.81   76.81   74.23\n                        n = 100         72.02   76.81   74.33\n\n                             Table 15: Impact of prompt history length.\n\n\n\neven larger batch sizes could potentially improve performance further, the cost considerations become\na critical factor.\n\n\nG  Impact of prompt history length\n\nWe evaluated the effect of varying prompt history lengths on model performance, as shown in\nTable 15. Our findings indicate that without prompt history, performance declines due to the absence\nof contextual information, making it challenging for the LLM to converge. As the history length\nincreases, performance progressively improves, with convergence observed at n=20. Although using\nn=100 yields the highest average performance, the extended input length significantly raises API\ncosts. As a result, we selected n=20 for our IPO, balancing performance gains with cost efficiency.\n\n\n      Model     Base   Novel   H             Model       1-shot   16-shots\n\n     LFA [66]   83.62   74.56   78.83          PLOT [67]   65.45    76.20\n     IPO       79.92   80.51   80.21            IPO         74.29    80.21\n\n (a) Comparison with LFA across 11 datasets in 16-shot (b) Comparison with PLOT on average accuracy across\n scenarios.                                    11 datasets in 1-shot and 16-shot scenarios.\n\n                     Table 16: Comparison with recent prompt learning methods\n\n\n\nH  Comparison with recent prompt learning methods\n\n\nWe conducted a comparative evaluation with LFA [66] and PLOT [67] under the same experimental\nconditions, as shown in Tables 16. Our IPO method consistently outperforms both LFA and PLOT\nacross the benchmarks.\n\n\nI  Detailed Prompt Optimization Prompt\n\nIn Figure 3, 4, 5 and 6, 7, 8, we show detailed inputs and outputs of different training steps in our\nPrompt Optimization Prompt. We observed that each optimized prompt is unique, and both loss and\naccuracy exhibit a downward trend.\n\n\nJ  Loss and accuracy curve.\n\n\nTo demonstrate that IPO can indeed serve as optimizer for prompt learning in vision-language models,\nwe present the optimization process’s loss and accuracy on ImageNet in Figure 9. As the training\nsteps increase, the loss consistently decreases, and the accuracy gradually improves, proving the\noptimization capability of IPO. Additionally, IPO not only allows for interpretable prompt generation\nbut also reduces the risk of overfitting during training.\n\n\n                                       18\n\n# INPUT                                                Initial Instructions\n\n        Instruction 1\n\n       You need to perform image classification on the large-scale visual recognition dataset based on visual features.Here, <CLASS> represents\n       a class name from the large-scale visual recognition dataset, and it is essential to include <CLASS> in <INS>.\n\n        Below is the image description for each image:\n\n       Image descriptions\n\n             Image 0: The image showcases a plate of food featuring thinly sliced red meat, garnished with green herbs and possibly other spices\n                 or seasonings.\n\n\n             Image 1: The image showcases a mouthwatering dish consisting of crispy grilled ribs with sticky sauce, accompanied by juicy apple\n                  slices on the side.\n\n\n                         (... more image examples. . . )\n\n\n        Instruction 2\n\n       Below are some previous prompts with their scores. The scores consist of loss and accuracy. The loss value is equal to or greater than 0,\n        while the accuracy varies from 0 to 100.\n\n       Prompt history\n\n\n         text prompt:\n\n       a photo of a <CLASS>, a type of food.\n\n        scores:\n\n         loss: 0.78955078125, accuracy: 82.35294342041016\n\n        Instruction 3\n\n       Generate a prompt different from all the prompts <INS> above, with a lower loss and higher accuracy than all the prompts <INS> above. The\n       prompt should begin with <INS> and end with </INS>. The prompt should be concise, effective, and generally applicable to all problems above.\n\n\n\nFigure 3: An example of our Prompt Optimization Prompt with input with initial instruction on the Food101 [17]\ndataset.\n\n\nK  More generated prompts\n\nIn Table 17, we provide detailed prompts for each dataset in 16-shot format. We encourage future\nresearchers to utilize these interpretable prompts in their own downstream tasks.\n\n\nL  Image description with LMM\n\nIn Table 18, we provide descriptions of some training samples generated using Mini-CPM-V-2.0 on\neach dataset, which serve as input for image information in our Prompt Optimization Prompt. Note\nthat we did not use this image information in the 16-shot setup due to the context length limitations\nof the language model.\n\n\nM  Prompt Optimization Prompt design\n\nOur Prompt Optimization Prompt is a crucial component of our optimizer, serving to enhance the\nperformance of the vision-language model by optimizing the prompts through the prompt LLM.\nFigure 2 displays an example of our Prompt Optimization Prompt. Initially, the instruction in the\nfirst segment of the Prompt Optimization Prompt defines the role of the LLM. It introduces two\nessential tokens, <INS> and <CLASS>, which represent the prompt and category, respectively.\nThe primary function of this instruction is to inform the LLM of its role and the contents of the\nPrompt Optimization Prompt, enabling a more effective understanding of the Prompt Optimization\nPrompt. Note that in this section, our model does not involve image information, hence the Prompt\nOptimization Prompt here lacks statements like “Here is a description of some features of the flowers\n\n\n                                       19\n\n# INPUT                                STEP 1\n\n        Instruction 1\n\n       You need to perform image classification on the large-scale visual recognition dataset based on visual features.Here, <CLASS> represents\n       a class name from the large-scale visual recognition dataset, and it is essential to include <CLASS> in <INS>.\n\n       Below is the image description for each image:\n\n      Image descriptions\n\n             Image 0: The image showcases a plate of food featuring thinly sliced red meat, garnished with green herbs and possibly other spices\n                or seasonings.\n\n\n             Image 1: The image showcases a mouthwatering dish consisting of crispy grilled ribs with sticky sauce, accompanied by juicy apple\n                 slices on the side.\n\n\n                        (... more image examples. . . )\n\n\n       Instruction 2\n\n       Below are some previous prompts with their scores. The scores consist of loss and accuracy. The loss value is equal to or greater than 0,\n        while the accuracy varies from 0 to 100.\n\n      Prompt history\n\n\n         text prompt:\n\n      a photo of a <CLASS>, a type of food.\n\n        scores:\n\n         loss: 0.78955078125, accuracy: 82.35294342041016\n\n\n         text prompt:\n\n         Identify the <CLASS> in the image based on its unique visual features.\n\n        scores:\n\n         loss: 0.7109375, accuracy: 84.3137283325192\n\n\n         text prompt:\n\n         Identify the <CLASS> in the image and evaluate its visual features for classification.\n\n        scores:\n\n         loss: 0.69470703125, accuracy: 85.47058868408203\n\n\n            (... more previous prompts and scores . . . )\n\n\n      Instruction 3\n\n\n     Generate a prompt different from all the prompts <INS> above, with a lower loss and higher accuracy than all the prompts <INS> above. The\n     prompt should begin with <INS> and end with </INS>. The prompt should be concise, effective, and generally applicable to all problems above.\n\n\n\nFigure 4: An example of our Prompt Optimization Prompt with input at step 1 on the Food101 [17] dataset.\n\n\n\n\n\n                                      20\n\n# INPUT                               STEP 100\n\n        Instruction 1\n\n       You need to perform image classification on the large-scale visual recognition dataset based on visual features.Here, <CLASS> represents\n       a class name from the large-scale visual recognition dataset, and it is essential to include <CLASS> in <INS>.\n\n       Below is the image description for each image:\n\n      Image descriptions\n\n            Image 0: The image showcases a plate of food featuring thinly sliced red meat, garnished with green herbs and possibly other spices\n                or seasonings.\n\n\n            Image 1: The image showcases a mouthwatering dish consisting of crispy grilled ribs with sticky sauce, accompanied by juicy apple\n                 slices on the side.\n\n\n                        (... more image examples. . . )\n\n\n       Instruction 2\n\n      Below are some previous prompts with their scores. The scores consist of loss and accuracy. The loss value is equal to or greater than 0,\n        while the accuracy varies from 0 to 100.\n\n      Prompt history\n\n\n         text prompt:\n\n      a photo of a <CLASS>, a type of food.\n\n        scores:\n\n        loss: 0.78955078125, accuracy: 82.35294342041016\n\n        text prompt:\n\n        Identify the primary ingredient in the <CLASS> and describe its texture, color, and presentation.\n\n       scores:\n\n        loss: 0.49072265625, accuracy: 88.23529052734375\n\n\n\n        text prompt:\n\n       Train a state-of-the-art deep learning model for advanced <CLASS> image classification with exceptional accuracy and minimal loss.\n\n       scores:\n\n       loss: 0.4296875, accuracy: 90.19607543945312\n\n\n          (... more previous prompts and scores . . . )\n\n        Instruction 3\n\n\n       Generate a prompt different from all the prompts <INS> above, with a lower loss and higher accuracy than all the prompts <INS> above. The\n       prompt should begin with <INS> and end with </INS>. The prompt should be concise, effective, and generally applicable to all problems above.\n\n\n\nFigure 5: An example of our Prompt Optimization Prompt with input step 100 on the Food101 [17] dataset.\n\n\n\n\n\n                                      21\n\n# OUTPUT                        STEP 1\n\n         Below are the prompts created according to your guidelines:\n\n      (1) <INS> Classify the image based on visual features of a <CLASS>. </INS>\n\n      (2) <INS> Identify the <CLASS> in the image and evaluate its visual features for classification. </INS>\n\n      (3) <INS> Classify the image as a <CLASS> food item. </INS>\n\n      (4) <INS> Classify the image featuring a <CLASS> with precision and accuracy. </INS>\n\n      (5) <INS> Identify the <CLASS> in the image based on its unique visual features. </INS>\n\n      (6) <INS> A photo of a <CLASS>, displaying vibrant colors and textures. </INS>\n\n      (7) <INS> Identify the primary ingredient in the <CLASS> and describe its texture, color, and presentation. </INS>\n\n      (8) <INS> Identify the type of food shown in the image and classify it accurately. </INS>\n\n\n\n\n\nFigure 6: An example of our Prompt Optimization Prompt with output at step 1 on the Food101 [17] dataset.\n\n\n\n\n\n   # OUTPUT                         STEP 2\n         Below are the prompts created according to your guidelines:\n\n      (1) <INS> Accurately classify the <CLASS> in the image using advanced visual recognition techniques. </INS>\n\n      (2) <INS> Accurately classify the image for <CLASS> identification based on detailed visual analysis with\n               precision and minimal loss.</INS>\n\n      (3) <INS> Accurately classify the image to determine the <CLASS> based on distinctive visual cues. </INS>\n\n      (4) <INS> Analyze the visual composition of the image to accurately classify the <CLASS> featured with precision. </INS>\n\n      (5) <INS> Analyze the visual composition of the image to classify the <CLASS> accurately. </INS>\n\n      (6) <INS> Analyze the distinct visual elements of the <CLASS> to achieve precise classification. </INS>\n\n      (7) <INS> Analyze the unique visual features of the <CLASS> for precise classification with superior accuracy. </INS>\n\n      (8) <INS> Utilize cutting-edge visual recognition algorithms to categorize the image accurately and efficiently. </INS>\n\n\n\n\n\nFigure 7: An example of our Prompt Optimization Prompt with output at step 2 on the Food101 [17] dataset.\n\n\n\n\n\n                                      22\n\n# OUTPUT                       STEP 100\n\n          Below are the prompts created according to your guidelines:\n\n       (1) <INS> Train a state-of-the-art convolutional neural network for accurate and efficient classification of <CLASS> with\n                 exceptional precision and minimal loss. </INS>\n\n       (2) <INS> Implement state-of-the-art deep learning algorithms for superior <CLASS> image classification accuracy with\n                 unrivaled precision and minimal loss..</INS>\n\n       (3) <INS> Develop an advanced neural network model for precise and efficient <CLASS> image classification with\n                exceptional accuracy and minimal loss. </INS>\n\n       (4) <INS> Utilize state-of-the-art machine learning algorithms for precise and efficient <CLASS> image classification\n                 with exceptional accuracy and negligible loss. </INS>\n\n       (5) <INS> Develop an innovative neural network model for precise and efficient <CLASS> image classification with\n                exceptional accuracy and minimal loss. </INS>\n\n       (6) <INS> Employ state-of-the-art deep learning techniques for precise and efficient <CLASS> image classification with\n                 unparalleled accuracy and minimal loss. </INS>\n\n       (7) <INS> Efficiently classify <CLASS> using cutting-edge deep learning techniques for unparalleled accuracy. </INS>\n\n       (8) <INS> Identify the primary ingredient in the <CLASS> and describe its texture, color, and presentation. </INS>\n\n\n\n\n\n Figure 8: An example of our Prompt Optimization Prompt with output step 99 on the Food101 [17] dataset.\n\n\n\n\n\n                   (a) Training loss.                                    (b) Training accuracy.\n\nFigure 9: Training loss and accuracy on ImageNet. Each dot represents the average loss and accuracy across up\nto 8 generated prompts in the single strip, with the shaded region indicating the standard deviation. Our findings\ndemonstrate that IPO can effectively optimize prompt learning in vision-language models. Notably, the best\nperformance was achieved at step 85.\n\n\n\nin the image” and the subsequent description of the image. In the next section, we will discuss how\nto integrate image information into the Prompt Optimization Prompt.\n\nThe subsequent instruction concerns the history of the prompts and their associated scores, which\ninclude metrics such as loss and accuracy. Our task follows CoOP [8], focusing on base classes\nwith few samples. We calculate the loss and accuracy for these base classes using the generated\nprompts. Initially, we inform the LLM that the following details are past prompts along with their\nscores, and we clarify the range of values for loss and accuracy. What follows are the historical\nprompts and scores. This history functions similarly to episodic memory, to prevent the generation\nof suboptimal prompts while providing in-context information that enhances the creation of better\nprompts. Additionally, as the history of past prompts grows, we only retain the top 20 prompts based\non accuracy to avoid information overload. The selection of these top 20 prompts is determined by\ntheir accuracy scores. Moreover, we consistently include the prompt “a photo of <CLASS>” in our\nhistory at every step, as this is a frequently used and effective prompt within the CLIP framework [30].\n\n\n                                       23\n\nDataset          Best Prompt\n\n            ImageNet          TLet’s address problem, a photo of a <CLASS>.\n             Caltech101         Take an awe-inspiring photograph of <CLASS> that beautifully captures\n                                              its essence with exceptional clarity, vibrant colors, impeccable composi-\n                                        tion, and mesmerizing details.\n                                Capture a well-lit, high-resolution photo of the <CLASS> with optimal\n             OxfordPets         focus and minimal distractions. Ensure proper composition and framing\n                                     to highlight its unique characteristics. Emphasize the pet’s distinct traits\n                             by capturing its expression and posture accurately.\n            StanfordCars       Use a higher resolution camera to capture the vehicle photo <CLASS>..\n            Flowers102         Capture a high-resolution image of a <CLASS> in perfect lighting condi-\n                                        tions, ensuring precise focus and a clutter-free background for maximum\n                                       classification accuracy and to highlight the flower’s distinctive features.\n                                 Categorize the image depicting a delicious and appetizing <CLASS>\n            Food101\n                                with remarkable visual qualities.\n                                Capture a comprehensive set of high-resolution images of an <CLASS>\n           FGVCAircraft       from various angles, ensuring optimal lighting conditions and precise\n                                 focus for unparalleled accuracy in aircraft model recognition.\n           SUN397      A photo of a <CLASS>, a type of large-scale scene.\n          DTD            Capture an image depicting the distinct pattern of <CLASS>.\n                                 Construct a state-of-the-art deep learning model on the Sentinel-2 satel-\n            EuroSAT\n                                             lite dataset for <CLASS> leveraging cutting-edge techniques including\n                                     attention mechanisms, transfer learning, ensemble learning.\n                                Capture a high-quality, well-lit image of a person flawlessly demonstrat-\n           UCF101           ing the <CLASS> action, ensuring impeccable visual representation for\n                             unmatched results.\n\n      Table 17: Interpretable prompts generated by our method for each dataset in 16-shot scenarios.\n\n\nThe final paragraph of the instruction specifies the ultimate goal of the task: to generate improved\nprompts based on the aforementioned instructions and the history of past prompts. The aim is to\nachieve lower loss and higher accuracy in this vision-language task, and it also outlines the format\nfor the final generated prompt. These three components constitute the entire content of the Prompt\nOptimization Prompt, each playing a critical role for the LLM. It is important to note that in this\nPrompt Optimization Prompt, we do not consider the specific content of the images; we merely use\nimages to calculate scores, and the details of the images are overlooked. In the following section,\nwe will utilize a LMM to generate content from images and then incorporate it into the Prompt\nOptimization Prompt.\n\nN  More occlusion sensitivity analysis\n\nIn Tables 19-29, we present various prompts analyzed using occlusion sensitivity analysis across\ndifferent models and datasets. We found that CLIP is particularly sensitive to photos. However,\nCoOP and CoCoOP exhibit severe overfitting on base classes, leading to poor performance on base\nclasses when some tokens are removed, but improved performance on novel classes. In contrast, our\noptimized prompts show performance degradation to varying degrees when certain words or phrases\nare removed, indicating that the key words or phrases in our generated prompts are essential.\n\n\n\n\n\n                                       24\n\nDatasets         Images       Text description\n\n                              The cheetah’s fur has spots and stripes, its eyes are black with white pupils. It is\n Caltech101                        looking straight into the camera while standing against a clear blue sky background.\n\n                              The image prominently features a large white and red airplane with the Turkish flag\n                                      logo, flying against a backdrop of blue sky dotted with clouds.\n\n                              The image features a black Audi TT parked outside the dealership, with its silver\n StanfordCars                       rims and elegant design.\n\n                              The car in the image is a 2016 Acura TSX, distinguished by its sleek design and\n                                          distinctive tail lights.\n\n                               The flowers have a combination of pink and white colors with distinct leaves, creating\n Flowers102                      an attractive appearance.\n\n                              The flowers in the image are large, have a yellow center with dark spots and petals\n                                         that vary from pink to red. They appear vibrant due to their color contrast against\n                                       natural backgrounds like leaves or rocks on trees behind them.\n\n                              The cat in the image is a light-colored, possibly an orange tabby with big yellow\n OxfordPets                        eyes and white whiskers.\n\n                              The image features a small white puppy with brown spots, which is likely to be an\n                                American Pit Bull Terrier breed.\n\n                              The food in the image is a delicious pastry with meat and cheese filling, covered by\n  Food101                         caramelized topping.\n\n                              The image features a colorful and diverse salad with various ingredients such as\n                                     onions, corn kernels, lettuce leaves (cabbage), peanuts or pumpkin seeds.\n\n                              The aircraft in the image is a large commercial airplane, likely used for passenger\n                                        transportation. It has an orange and white color scheme with prominent windows\nFGVCAircraft\n                                   along its body to provide natural light inside during flights.\n\n                               The aircraft in the image is a Western Airlines Boeing 737, identified by its distinctive\n                                                 tail fin with \"Western\" written on it. It has various markings including numbers and\n                                            letters that are part of their registration or identification system used for aviation\n                                     purposes.\n\n                              The image showcases a commercial airplane cabin with rows of blue and white\n  SUN397                             chairs, each equipped with cup holders for passengers’ convenience.\n\n                               The image captures the grandeur of a monument with an elaborate arch, surrounded\n                               by fountains and illuminated buildings at night.\n\n                               The image showcases a close-up view of the texture on an object with distinct stripes,\n   DTD                            buttons and stitching details.\n\n                              The leaf has a yellowish background with darker spots and irregular black marks,\n                                       indicating possible disease or damage.\n\n                              The fish, characterized by its light yellow body and distinct blue fins with a black\n  ImageNet                          spot on the tail fin’s edge.\n\n                              The image features a white chicken with red comb and orange legs, standing on\n                              wooden planks surrounded by straw or grass.\n\n                              The image features a black car with four-wheeled design, possibly indicating that it\n  EuroSAT                                is an automobile.\n\n                              The image captures various cars, each with distinct features such as their color and\n                                    shape. For instance, one car is red in color while another has a unique design that\n                                     stands out from the rest of them.\n\n                      A baby in a yellow shirt and blue jeans is actively crawling on the tiled floor, while\n  UCF101                       an adult wearing red pants stands nearby.\n\n                     A female gymnast is skillfully performing on a balance beam, showcasing her\n                                          athletic abilities as she navigates the challenging obstacle.\n\n                Table 18: Image descriptions generated with Mini-CPM-V-2.0 [43].\n\n\n                                     25\n\nModels     Prompts                              Base   Novel   H\n\n                 a photo of a <CLASS>.                   72.43   68.14   70.22\n              <CLASS>.                              69.91   64.72   67.21\n                 a <CLASS>.                            70.72   66.44   68.51\n                 photo <CLASS>.                        68.15   62.32   65.10\n                  of <CLASS>.                            67.84   64.93   66.35\n   CLIP [30]\n                 a photo <CLASS>.                       69.91   64.15   66.91\n                 photo of <CLASS>.                      71.24   65.67   68.34\n                  of a <CLASS>.                          69.37   66.49   67.90\n                 a photo of <CLASS>.                    70.99   64.26   67.46\n                 photo of a <CLASS>.                    72.64   68.02   70.25\n\n               Token 1, 2, 3, 4                          73.20   67.43   70.20\n              None                                   69.91   64.72   67.21\n               Token 1                                 66.39   62.88   64.59\n               Token 2                                 66.77   62.18   64.39\n               Token 3                                 74.06   68.61   71.23\n               Token 4                                 66.96   64.07   65.48\n   CoOP [8]\n               Token 1, 2                               67.15   62.93   64.97\n               Token 3, 4                               73.92   69.18   71.47\n               Token 1, 4                               68.99   65.60   67.25\n               Token 1, 2, 3                            73.15   66.54   69.69\n               Token 1, 2, 4                            69.84   64.14   66.87\n               Token 1, 3, 4                            73.52   68.64   71.00\n               Token 2, 3, 4                            73.71   68.08   70.78\n\n               Token 1, 2, 3, 4                          73.90   69.07   71.40\n              None                                   69.91   64.72   67.21\n               Token 1                                 74.01   67.85   70.80\n               Token 2                                 74.00   68.25   71.01\n               Token 3                                 73.52   68.58   70.96\n CoCoOP [11]  Token 4                                 73.92   68.23   70.96\n               Token 1, 2                               73.29   67.45   70.25\n               Token 3, 4                               73.34   67.68   70.40\n               Token 1, 4                               73.66   67.68   70.54\n               Token 1, 2, 3                            71.45   65.57   68.38\n               Token 1, 2, 4                            72.95   65.57   69.06\n               Token 1, 3, 4                            72.52   66.87   69.58\n               Token 2, 3, 4                            72.52   66.74   69.51\n\n                Take a high-quality photo of a <CLASS>.   74.09   69.17   71.54\n              <CLASS>.                              69.92   64.71   67.21\n                Take a photo of a <CLASS>.              72.43   68.43   70.37\n                  High-quality photo of a <CLASS>.         73.64   68.24   70.84\n                Photo of a <CLASS>.                    72.65   68.08   70.29\n     IPO\n                Take a high-quality photo of a <CLASS>.   74.08   69.12   71.51\n                  Quality photo of a <CLASS>.             73.69   68.23   70.85\n                  High-quality <CLASS>.                  71.71   66.47   68.99\n                Take a photo of the <CLASS>.            71.42   67.72   69.52\n           A photo of a <CLASS>.                  72.48   68.13   70.24\n\nTable 19: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nImageNet dataset.\n\n\n\n\n\n                                       26\n\nModels     Prompts                                  Base   Novel   H\n\n                 a photo of a <CLASS>.                       96.84   94.00   95.40\n              <CLASS>.                                  90.47   92.19   91.32\n                 a <CLASS>.                                91.08   93.45   92.25\n                 photo <CLASS>.                            89.67   90.62   90.14\n                  of <CLASS>.                                91.38   92.72   92.05\n   CLIP [30]\n                 a photo <CLASS>.                           92.02   92.69   92.35\n                 photo of <CLASS>.                          91.93   93.32   92.62\n                  of a <CLASS>.                              96.14   94.72   95.42\n                 a photo of <CLASS>.                        91.32   94.45   92.86\n                 photo of a <CLASS>.                        97.05   94.36   95.69\n\n               Token 1, 2, 3, 4                              90.63   85.20   87.83\n              None                                       90.47   92.19   91.32\n               Token 1                                     89.29   90.41   89.85\n               Token 2                                     89.71   90.83   90.27\n               Token 3                                     87.18   92.76   89.88\n               Token 4                                     91.75   91.45   91.60\n   CoOP [8]\n               Token 1, 2                                   89.86   92.97   91.39\n               Token 3, 4                                   89.97   92.26   91.10\n               Token 1, 4                                   92.84   92.04   92.44\n               Token 1, 2, 3                                88.13   92.26   90.15\n               Token 1, 2, 4                                94.34   93.02   93.68\n               Token 1, 3, 4                                88.56   92.81   90.64\n               Token 2, 3, 4                                89.52   92.97   91.21\n\n               Token 1, 2, 3, 4                              96.37   93.13   94.72\n              None                                       90.47   92.19   91.32\n               Token 1                                     96.36   93.38   94.85\n               Token 2                                     96.02   93.12   94.55\n               Token 3                                     96.97   93.38   95.14\n CoCoOP [11]  Token 4                                     96.15   93.24   94.67\n               Token 1, 2                                   96.42   93.95   95.17\n               Token 3, 4                                   97.56   93.95   95.72\n               Token 1, 4                                   97.47   93.41   95.40\n               Token 1, 2, 3                                95.54   92.87   94.19\n               Token 1, 2, 4                                96.63   93.24   94.90\n               Token 1, 3, 4                                97.41   93.24   95.28\n               Token 2, 3, 4                                97.41   93.12   95.22\n\n                  Categorize the <CLASS> shown in the image.   96.53   95.39   95.95\n              <CLASS>.                                  90.45   92.16   91.30\n                  Categorize <CLASS>.                        91.92   93.78   92.84\n                  Categorize the <CLASS>.                     95.44   95.05   95.24\n             <CLASS> shown in the image.                97.08   94.15   95.59\n     IPO\n              Shown in the image: <CLASS>.               93.54   93.76   93.65\n                  Categorize the <CLASS> in the image.         96.64   95.74   96.19\n               The <CLASS> shown in the image.            97.05   95.06   96.04\n               The <CLASS> in the image.                  93.54   94.32   93.93\n                  Categorize the <CLASS> shown.              95.37   95.09   95.23\n               Image of a <CLASS>.                        97.26   94.84   96.03\n\nTable 20: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nCaltech101 dataset.\n\n\n\n\n\n                                       27\n\nModels     Prompts                                                        Base   Novel   H\n\n                 a photo of a <CLASS>.                                               89.42   96.81   92.97\n              <CLASS>.                                                          88.30   89.09   88.69\n                 a <CLASS>.                                                        87.77   93.96   90.76\n                 photo <CLASS>.                                                    84.69   90.66   87.57\n                  of <CLASS>.                                                        86.98   88.53   87.75\n   CLIP [30]\n                 a photo <CLASS>.                                                   87.19   91.16   89.13\n                 photo of <CLASS>.                                                  88.62   90.38   89.49\n                  of a <CLASS>.                                                      85.38   92.84   88.95\n                 a photo of <CLASS>.                                                88.94   91.39   90.15\n                 photo of a <CLASS>.                                                89.90   96.36   93.02\n\n               Token 1, 2, 3, 4                                                      93.73   96.23   94.96\n              None                                                               88.30   89.09   88.69\n               Token 1                                                             81.34   86.80   83.98\n               Token 2                                                             81.34   89.26   85.12\n               Token 3                                                             93.83   94.80   94.31\n               Token 4                                                             85.11   90.16   87.56\n   CoOP [8]\n               Token 1, 2                                                           81.77   85.79   83.73\n               Token 3, 4                                                           91.97   96.31   94.09\n               Token 1, 4                                                           87.99   91.72   89.82\n               Token 1, 2, 3                                                        93.62   97.37   95.46\n               Token 1, 2, 4                                                        88.46   88.09   88.27\n               Token 1, 3, 4                                                        91.92   97.32   94.54\n               Token 2, 3, 4                                                        93.35   97.37   95.32\n\n               Token 1, 2, 3, 4                                                      93.47   96.27   94.85\n              None                                                               88.30   89.09   88.69\n               Token 1                                                             94.26   95.64   94.94\n               Token 2                                                             94.42   95.64   95.03\n               Token 3                                                             94.84   96.14   95.49\n CoCoOP [11]  Token 4                                                             94.05   96.09   95.06\n               Token 1, 2                                                           94.15   95.75   94.94\n               Token 3, 4                                                           93.83   97.09   95.43\n               Token 1, 4                                                           94.05   96.48   95.25\n               Token 1, 2, 3                                                        93.67   95.30   94.48\n               Token 1, 2, 4                                                        93.14   95.92   94.51\n               Token 1, 3, 4                                                        93.35   96.59   94.94\n               Token 2, 3, 4                                                        93.25   96.70   94.94\n\n                Take a well-composed photo of a <CLASS> with optimal lighting, focus,\n                and minimal distractions. Capture the pet’s unique characteristics,         94.48   97.93   96.43\n                  including expression and posture, to ensure a clear and distinct image.\n              <CLASS>.                                                          88.30   89.09   88.69\n                Take a photo of a <CLASS>.                                          90.06   96.59   93.21\n                Well-composed photo of a <CLASS>.                                  89.90   96.70   93.18\n                Photo of a <CLASS> with optimal lighting.                             89.37   96.81   92.94\n             <CLASS> with optimal lighting, focus, and minimal distractions.          89.85   92.62   91.21\n                 Capture the <CLASS>’s unique characteristics.                          87.77   89.60   88.68\n                  Including expression and posture of the <CLASS>.                      86.98   88.98   87.97\n     IPO       Ensure a clear and distinct image of the <CLASS>.                      81.77   86.35   84.00\n                Unique characteristics of the <CLASS>.                                89.10   86.74   87.90\n                 Capture the expression of the <CLASS>.                               88.20   90.04   89.11\n                 Capture the posture of the <CLASS>.                                  87.83   91.11   89.44\n                 Expression of the <CLASS>.                                          87.56   90.32   88.92\n                  Posture of the <CLASS>.                                             89.79   92.67   91.21\n                Focus and minimal distractions for a <CLASS>.                         90.27   96.31   93.19\n                  Clear and distinct image of the <CLASS>.                              83.57   85.91   84.72\n\nTable 21: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nOxfordPets dataset.\n\n\n\n\n\n                                       28\n\nModels     Prompts                                                        Base   Novel   H\n\n                 a photo of a <CLASS>.                                               63.37   74.89   68.65\n              <CLASS>.                                                          62.72   73.58   67.72\n                 a <CLASS>.                                                        61.37   72.82   66.61\n                 photo <CLASS>.                                                    61.79   73.78   67.25\n                  of <CLASS>.                                                        62.04   73.98   67.49\n   CLIP [30]\n                 a photo <CLASS>.                                                   62.57   73.98   67.80\n                 photo of <CLASS>.                                                  63.09   74.75   68.43\n                  of a <CLASS>.                                                      59.25   71.82   64.93\n                 a photo of <CLASS>.                                                63.82   75.41   69.13\n                 photo of a <CLASS>.                                                63.04   74.45   68.27\n\n               Token 1, 2, 3, 4                                                      61.80   68.33   64.90\n              None                                                               62.72   73.58   67.72\n               Token 1                                                             61.42   72.05   66.31\n               Token 2                                                             58.77   69.74   63.79\n               Token 3                                                             62.37   68.31   65.21\n               Token 4                                                             60.24   70.19   64.84\n   CoOP [8]\n               Token 1, 2                                                           61.79   73.95   67.33\n               Token 3, 4                                                           59.57   68.46   63.71\n               Token 1, 4                                                           60.14   71.97   65.53\n               Token 1, 2, 3                                                        63.37   69.82   66.44\n               Token 1, 2, 4                                                        61.12   74.00   66.95\n               Token 1, 3, 4                                                        60.79   70.29   65.20\n               Token 2, 3, 4                                                        57.95   65.66   61.56\n\n               Token 1, 2, 3, 4                                                      65.27   73.73   69.24\n              None                                                               62.72   73.58   67.72\n               Token 1                                                             65.22   75.27   69.89\n               Token 2                                                             65.32   74.81   69.74\n               Token 3                                                             65.09   74.05   69.28\n CoCoOP [11]  Token 4                                                             65.54   74.50   69.73\n               Token 1, 2                                                           64.57   74.94   69.37\n               Token 3, 4                                                           64.67   73.93   68.99\n               Token 1, 4                                                           64.84   74.85   69.49\n               Token 1, 2, 3                                                        64.07   74.52   68.90\n               Token 1, 2, 4                                                        64.74   74.10   69.10\n               Token 1, 3, 4                                                        63.17   74.18   68.23\n               Token 2, 3, 4                                                        63.29   74.18   68.30\n\n                 Describe the distinguishing characteristics of the <CLASS> in the image.   63.83   75.45   69.16\n              <CLASS>.                                                          62.72   73.58   67.72\n                 Describe the <CLASS>.                                              62.67   75.34   68.42\n                  Distinguishing characteristics of the <CLASS>.                         63.52   74.40   68.53\n                   Characteristics of the <CLASS> in the image.                           63.72   74.35   68.63\n                 Describe the <CLASS> in the image.                                   62.97   74.82   68.39\n     IPO\n               The <CLASS> in the image.                                          63.97   74.94   69.02\n                 Describe distinguishing characteristics of the <CLASS>.                 63.97   74.15   68.68\n                  In the image, the <CLASS>.                                           63.67   75.19   68.95\n               The <CLASS>’s distinguishing characteristics.                          61.34   72.39   66.41\n               The distinguishing characteristics of the <CLASS> in the image.          64.07   74.72   68.99\n\nTable 22: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nStanfordCars dataset.\n\n\n\n\n\n                                       29\n\nModels     Prompts                                                      Base   Novel   H\n\n                 a photo of a <CLASS>.                                            69.34   76.72   72.84\n              <CLASS>.                                                       61.75   75.83   68.07\n                 a <CLASS>.                                                     63.04   75.02   68.51\n                 photo <CLASS>.                                                 64.72   76.74   70.22\n                  of <CLASS>.                                                     63.02   76.17   68.97\n   CLIP [30]\n                 a photo <CLASS>.                                                66.52   77.25   71.48\n                 photo of <CLASS>.                                               61.33   77.12   68.32\n                  of a <CLASS>.                                                   63.37   75.91   69.08\n                 a photo of <CLASS>.                                             59.15   77.25   70.00\n                 photo of a <CLASS>.                                             69.16   76.72   72.74\n\n               Token 1, 2, 3, 4                                                   71.47   72.47   71.97\n              None                                                            61.75   75.83   68.07\n               Token 1                                                          76.02   73.38   74.68\n               Token 2                                                          70.07   74.26   72.10\n               Token 3                                                          69.62   72.88   71.21\n               Token 4                                                          70.85   67.75   69.27\n   CoOP [8]\n               Token 1, 2                                                        75.04   74.23   74.63\n               Token 3, 4                                                        70.13   66.75   68.40\n               Token 1, 4                                                        75.92   68.81   72.20\n               Token 1, 2, 3                                                     72.55   74.04   73.29\n               Token 1, 2, 4                                                     74.82   70.52   72.61\n               Token 1, 3, 4                                                     76.25   66.15   70.84\n               Token 2, 3, 4                                                     70.91   67.93   69.39\n\n               Token 1, 2, 3, 4                                                   73.67   75.50   74.57\n              None                                                            61.75   75.83   68.07\n               Token 1                                                          75.83   76.92   76.37\n               Token 2                                                          76.75   76.98   76.86\n               Token 3                                                          76.74   77.27   77.00\n CoCoOP [11]  Token 4                                                          76.51   77.64   77.07\n               Token 1, 2                                                        76.02   77.23   76.62\n               Token 3, 4                                                        76.07   78.75   77.39\n               Token 1, 4                                                        76.49   78.22   77.35\n               Token 1, 2, 3                                                     76.51   75.53   76.02\n               Token 1, 2, 4                                                     76.04   77.92   76.97\n               Token 1, 3, 4                                                     75.53   78.37   76.92\n               Token 2, 3, 4                                                     75.25   77.71   76.46\n\n                   Identify the unique visual features of the <CLASS> flower accurately.   74.17   79.65   76.81\n              <CLASS>.                                                       61.75   75.83   68.07\n                   Identify <CLASS>.                                               69.53   77.52   73.31\n                   Identify the <CLASS>.                                            68.88   76.35   72.42\n                   Identify the unique <CLASS>.                                      67.22   77.73   72.09\n                   Identify unique visual features <CLASS>.                           63.28   77.21   69.55\n     IPO\n                  Visual features of the <CLASS>.                                   64.42   74.64   69.15\n               The unique visual features <CLASS> flower.                         66.42   76.67   71.18\n                  Features of the <CLASS> flower accurately.                          72.03   77.82   74.81\n                  Visual features of the <CLASS> flower.                             71.93   78.55   75.09\n                   Identify the unique visual features of the <CLASS> accurately.         65.67   77.35   71.03\n                   Identify the unique visual features of the <CLASS> flower.            73.82   79.11   76.37\n               The unique visual features of the <CLASS> flower accurately.          72.67   78.72   75.57\n\nTable 23: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nFlowers102 dataset.\n\n\n\n\n\n                                       30\n\nModels     Prompts                                                                            Base   Novel   H\n\n                 a photo of a <CLASS>.                                                                    89.44   90.68   90.06\n              <CLASS>.                                                                               89.25   89.44   89.34\n                 a <CLASS>.                                                                             88.48   89.12   88.80\n                 photo <CLASS>.                                                                         89.39   88.49   88.94\n                  of <CLASS>.                                                                            89.51   90.01   89.76\n   CLIP [30]\n                 a photo <CLASS>.                                                                       89.08   87.87   88.48\n                 photo of <CLASS>.                                                                       89.38   90.00   89.69\n                  of a <CLASS>.                                                                           89.31   90.17   89.74\n                 a photo of <CLASS>.                                                                     89.32   88.15   88.73\n                 photo of a <CLASS>.                                                                     89.17   90.59   89.87\n\n               Token 1, 2, 3, 4                                                                           87.90   88.03   87.96\n              None                                                                                    89.25   89.44   89.34\n               Token 1                                                                                 88.62   86.29   87.44\n               Token 2                                                                                 88.99   87.01   87.99\n               Token 3                                                                                 86.47   88.90   87.67\n               Token 4                                                                                 89.27   88.95   89.11\n   CoOP [8]\n               Token 1, 2                                                                               88.00   87.45   87.72\n               Token 3, 4                                                                               87.55   89.17   88.35\n               Token 1, 4                                                                               89.12   89.61   89.36\n               Token 1, 2, 3                                                                             86.38   88.91   87.63\n               Token 1, 2, 4                                                                             88.84   89.58   89.21\n               Token 1, 3, 4                                                                             87.73   89.67   88.69\n               Token 2, 3, 4                                                                             87.41   89.41   88.40\n\n               Token 1, 2, 3, 4                                                                           88.73   89.60   89.16\n              None                                                                                    89.25   89.44   89.34\n               Token 1                                                                                 88.77   89.55   89.16\n               Token 2                                                                                 89.31   90.45   89.87\n               Token 3                                                                                 89.57   90.83   90.20\n CoCoOP [11]  Token 4                                                                                 89.01   90.40   89.70\n               Token 1, 2                                                                               88.63   89.55   89.09\n               Token 3, 4                                                                               89.08   90.68   89.87\n               Token 1, 4                                                                               88.49   89.57   89.03\n               Token 1, 2, 3                                                                             88.40   89.86   89.12\n               Token 1, 2, 4                                                                             88.24   89.59   88.91\n               Token 1, 3, 4                                                                             88.37   90.15   89.25\n               Token 2, 3, 4                                                                             88.53   90.28   89.40\n\n                   Identify the primary ingredient in the <CLASS> and describe its texture, color, and presentation.   89.78   91.59   90.67\n              <CLASS>.                                                                               89.25   89.44   89.34\n                   Identify the <CLASS>.                                                                    89.08   90.80   89.93\n                 Primary ingredient in the <CLASS>.                                                        80.55   82.15   81.34\n                   Identify the primary ingredient in the <CLASS>.                                             88.99   90.44   89.71\n                 Describe the <CLASS>’s texture.                                                           89.32   90.39   89.85\n                 Describe the <CLASS>’s color.                                                            87.75   88.82   88.28\n     IPO\n                 Describe the <CLASS>’s presentation.                                                      88.48   89.07   88.77\n                  Texture of the <CLASS>.                                                                  88.86   91.15   89.99\n                 Color of the <CLASS>.                                                                   87.90   90.31   89.09\n                  Presentation of the <CLASS>.                                                             88.74   90.98   89.85\n               The <CLASS>’s primary ingredient.                                                        87.44   87.89   87.66\n                   Identify the primary ingredient in the <CLASS> and describe its texture.                        89.35   91.06   90.20\n                   Identify the primary ingredient in the <CLASS> and describe its color.                          88.56   90.60   89.57\n                   Identify the primary ingredient in the <CLASS> and describe its presentation.                   89.77   91.51   90.63\n\nTable 24: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nFood101 dataset.\n\n\n\n\n\n                                       31\n\nModels     Prompts                                                                Base   Novel   H\n\n                 a photo of a <CLASS>.                                                        27.73   33.17   30.21\n              <CLASS>.                                                                   25.81   31.43   28.34\n                 a <CLASS>.                                                                 25.81   30.89   28.12\n                 photo <CLASS>.                                                             22.51   30.77   26.00\n                  of <CLASS>.                                                                24.01   30.95   27.04\n   CLIP [30]\n                 a photo <CLASS>.                                                           26.71   31.37   28.85\n                 photo of <CLASS>.                                                           25.03   31.97   28.08\n                  of a <CLASS>.                                                               24.55   30.41   27.17\n                 a photo of <CLASS>.                                                         26.59   33.11   29.49\n                 photo of a <CLASS>.                                                         27.01   33.23   29.80\n\n               Token 1, 2, 3, 4                                                               27.77   27.60   27.68\n              None                                                                        25.81   31.43   28.34\n               Token 1                                                                     22.27   26.69   24.28\n               Token 2                                                                     13.21   19.38   15.71\n               Token 3                                                                     22.45   26.81   24.44\n               Token 4                                                                     23.41   28.31   25.63\n   CoOP [8]\n               Token 1, 2                                                                   12.91   12.90   12.90\n               Token 3, 4                                                                   27.61   33.11   30.11\n               Token 1, 4                                                                   23.65   30.17   26.52\n               Token 1, 2, 3                                                                 17.95   18.72   18.33\n               Token 1, 2, 4                                                                 18.49   20.82   19.59\n               Token 1, 3, 4                                                                 27.01   35.27   30.59\n               Token 2, 3, 4                                                                 27.67   28.73   28.19\n\n               Token 1, 2, 3, 4                                                               29.77   31.23   30.28\n              None                                                                        25.81   31.43   28.34\n               Token 1                                                                     25.93   33.17   29.11\n               Token 2                                                                     26.59   32.33   29.18\n               Token 3                                                                     25.75   30.65   27.99\n CoCoOP [11]  Token 4                                                                     29.23   33.17   31.08\n               Token 1, 2                                                                   28.45   32.87   30.50\n               Token 3, 4                                                                   30.43   33.65   31.96\n               Token 1, 4                                                                   31.33   35.21   33.16\n               Token 1, 2, 3                                                                 28.75   31.49   30.06\n               Token 1, 2, 4                                                                 31.33   35.81   33.42\n               Token 1, 3, 4                                                                 30.19   35.51   32.63\n               Token 2, 3, 4                                                                 30.13   35.45   32.57\n\n                 Capture a comprehensive range of well-lit, high-resolution images of an <CLASS>\n                from various angles, meticulously showcasing its specific design features with       31.43   36.32   33.70\n                   perfect clarity and precision for unparalleled accuracy in aircraft.\n              <CLASS>.                                                                   25.81   31.43   28.34\n                 Capture an <CLASS>.                                                        26.53   31.79   28.92\n               Range of images of an <CLASS>.                                              28.81   32.93   30.73\n                    Well-lit images of an <CLASS>.                                               26.53   32.75   29.31\n                  High-resolution images of an <CLASS>.                                        28.99   33.11   30.91\n                Images of an <CLASS> from various angles.                                    28.21   34.13   30.89\n     IPO       Comprehensive range of images of an <CLASS>.                                28.21   32.75   30.31\n                 Meticulously showcase the <CLASS>’s design features.                           25.03   30.83   27.63\n                   Specific design features of an <CLASS>.                                        28.27   34.01   30.88\n                   Perfect clarity and precision of the <CLASS>.                                   23.77   31.31   27.02\n                 Capture well-lit images of an <CLASS>.                                        27.31   30.71   28.91\n                 Capture high-resolution images of an <CLASS>.                                 30.07   32.81   31.38\n                 Capture images of an <CLASS> from various angles.                             27.79   32.63   30.02\n               Showcase the <CLASS> with clarity and precision.                               25.27   22.26   23.67\n                  Unparalleled accuracy of the <CLASS> in aircraft.                               28.69   33.05   30.72\n\nTable 25: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nFGVCAircraft dataset.\n\n\n\n\n\n                                       32\n\nModels     Prompts                                       Base   Novel   H\n\n                 a photo of a <CLASS>.                            69.36   75.35   72.23\n              <CLASS>.                                       65.91   72.74   69.16\n                 a <CLASS>.                                     68.75   66.65   67.68\n                 photo <CLASS>.                                 62.88   70.42   66.44\n                  of <CLASS>.                                     65.27   69.95   67.53\n   CLIP [30]\n                 a photo <CLASS>.                                65.01   75.73   69.96\n                 photo of <CLASS>.                               70.47   72.75   71.59\n                  of a <CLASS>.                                   69.19   75.47   72.19\n                 a photo of <CLASS>.                             69.76   75.55   72.54\n                 photo of a <CLASS>.                             69.52   70.62   70.07\n\n               Token 1, 2, 3, 4                                   71.47   72.47   71.97\n              None                                            65.91   72.74   69.16\n               Token 1                                          61.39   65.98   63.60\n               Token 2                                          61.14   66.43   63.68\n               Token 3                                          69.87   71.54   70.70\n               Token 4                                          63.72   66.22   64.95\n   CoOP [8]\n               Token 1, 2                                        64.76   68.34   66.50\n               Token 3, 4                                        69.51   71.57   70.52\n               Token 1, 4                                        65.23   68.66   66.90\n               Token 1, 2, 3                                     69.98   72.66   71.29\n               Token 1, 2, 4                                     66.27   72.07   69.05\n               Token 1, 3, 4                                     69.64   71.45   70.53\n               Token 2, 3, 4                                     69.62   72.22   70.90\n\n               Token 1, 2, 3, 4                                   73.67   75.50   74.57\n              None                                            65.91   72.74   69.16\n               Token 1                                          70.85   75.63   73.16\n               Token 2                                          70.52   75.25   72.81\n               Token 3                                          69.64   75.48   72.44\n CoCoOP [11]  Token 4                                          71.64   75.05   73.31\n               Token 1, 2                                        72.55   72.65   72.60\n               Token 3, 4                                        73.56   72.61   73.08\n               Token 1, 4                                        73.18   72.72   72.95\n               Token 1, 2, 3                                     72.53   69.01   70.73\n               Token 1, 2, 4                                     73.26   68.87   71.00\n               Token 1, 3, 4                                     73.46   68.75   71.03\n               Token 2, 3, 4                                     73.24   68.02   70.53\n\n           A photo of a <CLASS>, a type of large-scale scene.   72.25   77.53   74.80\n              <CLASS>.                                       65.91   72.74   69.16\n           A photo of a <CLASS>.                           69.36   75.35   72.23\n                Photo of a <CLASS>.                             69.52   70.62   70.07\n           A <CLASS>, a type of large-scale scene.            71.51   77.02   74.16\n     IPO\n                  Large-scale scene of a <CLASS>.                   71.03   76.63   73.72\n               Type of large-scale scene: <CLASS>.               69.97   76.08   72.90\n           A photo of the <CLASS>.                         71.74   76.71   74.14\n               The <CLASS>, a large-scale scene.                 69.83   74.83   72.24\n           A type of large-scale scene: <CLASS>.              70.92   76.64   73.67\n                Scene of a <CLASS>.                             70.55   74.95   72.68\n\nTable 26: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nSUN397 dataset.\n\n\n\n\n\n                                       33\n\nModels     Prompts                             Base   Novel   H\n\n                 a photo of a <CLASS>.                 54.63   59.18   56.81\n              <CLASS>.                            53.70   59.18   56.31\n                 a <CLASS>.                           53.70   60.27   56.80\n                 photo <CLASS>.                       43.63   44.93   44.27\n                  of <CLASS>.                          55.79   58.82   57.26\n   CLIP [30]\n                 a photo <CLASS>.                     43.17   41.43   42.28\n                 photo of <CLASS>.                    53.47   52.05   52.75\n                  of a <CLASS>.                        55.21   60.75   57.85\n                 a photo of <CLASS>.                   53.36   51.69   52.51\n                 photo of a <CLASS>.                   54.63   59.78   57.09\n\n               Token 1, 2, 3, 4                        60.80   47.53   53.35\n              None                                 53.70   59.18   56.31\n               Token 1                               50.93   52.66   51.78\n               Token 2                               49.88   48.07   48.96\n               Token 3                               59.03   44.93   51.02\n               Token 4                               54.05   55.43   54.73\n   CoOP [8]\n               Token 1, 2                             46.53   42.03   44.17\n               Token 3, 4                             55.90   46.74   50.91\n               Token 1, 4                             54.17   57.00   55.55\n               Token 1, 2, 3                           54.51   48.67   51.42\n               Token 1, 2, 4                           51.62   44.69   47.91\n               Token 1, 3, 4                           55.67   46.74   50.82\n               Token 2, 3, 4                           55.32   46.38   50.46\n\n               Token 1, 2, 3, 4                        58.70   52.70   55.54\n              None                                 53.70   59.18   56.31\n               Token 1                               57.41   51.45   54.27\n               Token 2                               57.64   50.60   53.89\n               Token 3                               59.03   53.38   56.06\n CoCoOP [11]  Token 4                               57.87   52.66   55.14\n               Token 1, 2                             55.44   52.05   53.69\n               Token 3, 4                             57.06   54.83   55.92\n               Token 1, 4                             56.37   53.62   54.96\n               Token 1, 2, 3                           56.02   49.28   52.43\n               Token 1, 2, 4                           55.44   52.29   53.82\n               Token 1, 3, 4                           57.29   52.90   55.01\n               Token 2, 3, 4                           57.18   52.78   54.89\n\n                   Classify the intricate <CLASS> texture.   55.45   62.47   58.75\n              <CLASS>.                            53.70   59.18   56.31\n                   Classify <CLASS>.                    53.01   57.97   55.38\n               The intricate <CLASS>.                54.17   58.70   56.34\n                     Intricate <CLASS> texture.              52.66   55.43   54.01\n                   Classify the <CLASS>.                 55.56   59.66   57.54\n     IPO\n                   Classify the texture of the <CLASS>.     53.70   62.44   57.74\n             <CLASS> texture.                      53.12   60.51   56.57\n                  Texture of the <CLASS>.               52.43   63.77   57.55\n               The <CLASS>’s intricate texture.        52.31   60.63   56.16\n                   Classify intricate <CLASS> texture.      54.40   58.70   56.47\n\nTable 27: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nDTD dataset.\n\n\n\n\n\n                                       34\n\nModels     Prompts                                                                  Base   Novel   H\n\n                 a photo of a <CLASS>.                                                         50.26   69.90   58.47\n              <CLASS>.                                                                    47.19   66.49   55.20\n                 a <CLASS>.                                                                   53.57   70.64   60.93\n                 photo <CLASS>.                                                               46.26   53.77   49.73\n                  of <CLASS>.                                                                  54.14   74.23   62.61\n   CLIP [30]\n                 a photo <CLASS>.                                                             48.14   62.95   54.56\n                 photo of <CLASS>.                                                            48.02   67.92   56.26\n                  of a <CLASS>.                                                                 62.43   71.31   66.58\n                 a photo of <CLASS>.                                                           49.62   67.97   57.36\n                 photo of a <CLASS>.                                                           48.43   67.38   56.35\n\n               Token 1, 2, 3, 4                                                                 69.13   50.33   58.25\n              None                                                                          47.19   66.49   55.20\n               Token 1                                                                       47.95   67.85   56.19\n               Token 2                                                                       58.83   66.03   62.22\n               Token 3                                                                       74.88   46.00   69.55\n               Token 4                                                                       57.30   71.59   63.65\n   CoOP [8]    Token 1, 2                                                                     57.36   67.23   61.90\n               Token 3, 4                                                                     76.57   46.23   57.65\n               Token 1, 4                                                                     54.76   68.77   60.97\n               Token 1, 2, 3                                                                   67.14   40.36   50.41\n               Token 1, 2, 4                                                                   61.67   65.51   63.53\n               Token 1, 3, 4                                                                   75.40   45.31   56.60\n               Token 2, 3, 4                                                                   79.14   44.77   57.19\n\n               Token 1, 2, 3, 4                                                                 71.13   62.87   66.75\n              None                                                                          47.19   66.49   55.20\n               Token 1                                                                       74.62   51.38   60.86\n               Token 2                                                                       72.83   50.15   59.40\n               Token 3                                                                       76.05   48.85   59.49\n               Token 4                                                                       71.21   50.10   58.82\n CoCoOP [11]  Token 1, 2                                                                     72.10   47.95   57.60\n               Token 3, 4                                                                     72.55   58.23   64.61\n               Token 1, 4                                                                     76.26   58.92   66.48\n               Token 1, 2, 3                                                                   74.67   40.21   52.27\n               Token 1, 2, 4                                                                   75.36   51.28   61.03\n               Token 1, 3, 4                                                                   70.40   51.08   59.20\n               Token 2, 3, 4                                                                   66.83   47.54   55.56\n\n                Analyze the <CLASS> vehicles in the satellite image with state-of-the-art algorithms\n                                                                                               64.97   82.13   72.54\n                    for precise classification and optimal efficiency.\n              <CLASS>.                                                                    47.19   66.49   55.20\n                Analyze <CLASS> vehicles.                                                     48.95   74.46   59.07\n                Analyze the <CLASS>.                                                         51.60   75.64   61.35\n             <CLASS> vehicles in the satellite image.                                          51.55   80.15   62.74\n     IPO        Vehicles in the satellite image: <CLASS>.                                         58.26   69.67   63.46\n                    State-of-the-art algorithms for <CLASS> classification.                             54.60   74.85   63.14\n                  Precise classification of <CLASS> vehicles.                                       61.71   76.59   68.35\n                Optimal efficiency in <CLASS> analysis.                                         56.12   79.26   65.71\n                Analyze the <CLASS> vehicles in the satellite image.                              58.17   78.82   66.94\n               Use state-of-the-art algorithms for <CLASS> classification.                         58.17   75.54   65.73\n                    Classification and efficiency of <CLASS> vehicles.                                 63.79   76.46   69.55\n\nTable 28: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nEuroSAT dataset.\n\n\n\n\n\n                                       35\n\nModels     Prompts                                                                               Base   Novel   H\n\n                 a photo of a <CLASS>.                                                                        68.15   75.07   71.44\n              <CLASS>.                                                                                   67.68   72.47   67.00\n                 a <CLASS>.                                                                                 68.25   69.39   68.82\n                 photo <CLASS>.                                                                             65.05   68.74   66.84\n                  of <CLASS>.                                                                                 68.87   72.31   70.55\n   CLIP [30]\n                 a photo <CLASS>.                                                                            64.74   71.98   68.17\n                 photo of <CLASS>.                                                                           69.70   77.23   73.27\n                  of a <CLASS>.                                                                               68.20   71.01   69.58\n                 a photo of <CLASS>.                                                                         68.61   76.10   72.16\n                 photo of a <CLASS>.                                                                         68.10   75.01   71.39\n\n               Token 1, 2, 3, 4                                                                               72.50   63.57   67.74\n              None                                                                                        67.68   72.47   67.00\n               Token 1                                                                                      65.21   71.55   68.23\n               Token 2                                                                                      64.32   70.90   67.45\n               Token 3                                                                                      74.20   71.39   72.77\n               Token 4                                                                                      65.15   70.63   67.78\n   CoOP [8]\n               Token 1, 2                                                                                    64.63   73.12   68.61\n               Token 3, 4                                                                                    72.75   69.17   70.91\n               Token 1, 4                                                                                    66.91   70.96   68.88\n               Token 1, 2, 3                                                                                 74.61   73.12   73.86\n               Token 1, 2, 4                                                                                 64.63   72.69   68.42\n               Token 1, 3, 4                                                                                 73.11   69.33   71.17\n               Token 2, 3, 4                                                                                 74.20   69.01   71.51\n\n               Token 1, 2, 3, 4                                                                               74.73   72.80   73.75\n              None                                                                                        67.68   72.47   67.00\n               Token 1                                                                                      72.60   72.53   72.56\n               Token 2                                                                                      71.92   71.82   71.87\n               Token 3                                                                                      71.20   70.96   71.08\n               Token 4                                                                                      71.77   70.52   71.14\n CoCoOP [11]\n               Token 1, 2                                                                                    73.73   72.42   73.07\n               Token 3, 4                                                                                    74.72   71.66   73.16\n               Token 1, 4                                                                                    75.54   72.58   74.03\n               Token 1, 2, 3                                                                                 74.61   72.20   73.39\n               Token 1, 2, 4                                                                                 75.08   71.12   73.05\n               Token 1, 3, 4                                                                                 76.27   72.58   74.38\n               Token 2, 3, 4                                                                                 76.01   72.28   74.10\n\n                 Capture a high-quality, well-lit image of a person flawlessly demonstrating the <CLASS> action with\n                                                                                                              72.43   79.35   75.73\n                 impeccable visual representation to achieve unmatched.\n              <CLASS>.                                                                                   67.68   72.47   67.00\n                 Capture a <CLASS>.                                                                          67.32   71.23   69.22\n                  High-quality image of a <CLASS>.                                                             71.41   76.96   74.08\n                    Well-lit image of a <CLASS>.                                                                  69.49   77.56   73.30\n               Image of a <CLASS>.                                                                         70.17    75.5   72.74\n     IPO\n                 Person demonstrating the <CLASS>.                                                            72.60   76.31   74.41\n                 Demonstrating the <CLASS> action.                                                            71.41   74.96   73.14\n                 Impeccable visual representation of a <CLASS>.                                                 70.63   76.58   73.48\n                 Capture a person demonstrating the <CLASS>.                                                   70.53   76.47   73.38\n                  Flawlessly demonstrating the <CLASS>.                                                        70.48   78.26   74.17\n                  Visual representation of the <CLASS>.                                                          70.84   75.72   73.20\n                Achieve unmatched representation of a <CLASS>.                                                69.03   71.55   70.27\n\nTable 29: Comparison of various prompts with occlusion sensitivity analysis across different models on the\nUCF101 dataset.\n\n\n\n\n\n                                       36\n\nNeurIPS Paper Checklist\n\n      1. Claims\n        Question: Do the main claims made in the abstract and introduction accurately reflect the\n         paper’s contributions and scope?\n       Answer: [Yes]\n          Justification: The contributions and scope of this paper are claimed in the abstract. Detailed\n        information can be found in the fourth paragraph of the introduction section 1.\n         Guidelines:\n              • The answer NA means that the abstract and introduction do not include the claims\n          made in the paper.\n              • The abstract and/or introduction should clearly state the claims made, including the\n              contributions made in the paper and important assumptions and limitations. A No or\n        NA answer to this question will not be perceived well by the reviewers.\n              • The claims made should match theoretical and experimental results, and reflect how\n          much the results can be expected to generalize to other settings.\n              •  It is fine to include aspirational goals as motivation as long as it is clear that these goals\n              are not attained by the paper.\n      2. Limitations\n        Question: Does the paper discuss the limitations of the work performed by the authors?\n       Answer: [Yes]\n          Justification: We provide a \"limitation\" subsection in the conclusion section 6.\n         Guidelines:\n              • The answer NA means that the paper has no limitation while the answer No means that\n              the paper has limitations, but those are not discussed in the paper.\n              • The authors are encouraged to create a separate \"Limitations\" section in their paper.\n              • The paper should point out any strong assumptions and how robust the results are to\n              violations of these assumptions (e.g., independence assumptions, noiseless settings,\n           model well-specification, asymptotic approximations only holding locally). The authors\n            should reflect on how these assumptions might be violated in practice and what the\n              implications would be.\n              • The authors should reflect on the scope of the claims made, e.g., if the approach was\n            only tested on a few datasets or with a few runs. In general, empirical results often\n            depend on implicit assumptions, which should be articulated.\n              • The authors should reflect on the factors that influence the performance of the approach.\n            For example, a facial recognition algorithm may perform poorly when image resolution\n                is low or images are taken in low lighting. Or a speech-to-text system might not be\n            used reliably to provide closed captions for online lectures because it fails to handle\n              technical jargon.\n              • The authors should discuss the computational efficiency of the proposed algorithms\n            and how they scale with dataset size.\n              • If applicable, the authors should discuss possible limitations of their approach to\n             address problems of privacy and fairness.\n              • While the authors might fear that complete honesty about limitations might be used by\n             reviewers as grounds for rejection, a worse outcome might be that reviewers discover\n              limitations that aren’t acknowledged in the paper. The authors should use their best\n            judgment and recognize that individual actions in favor of transparency play an impor-\n               tant role in developing norms that preserve the integrity of the community. Reviewers\n              will be specifically instructed to not penalize honesty concerning limitations.\n      3. Theory Assumptions and Proofs\n        Question: For each theoretical result, does the paper provide the full set of assumptions and\n        a complete (and correct) proof?\n       Answer: [NA]\n\n\n                                       37\n\nJustification: This paper does not include theoretical results.\n\n   Guidelines:\n\n       • The answer NA means that the paper does not include theoretical results.\n       • All the theorems, formulas, and proofs in the paper should be numbered and cross-\n        referenced.\n       • All assumptions should be clearly stated or referenced in the statement of any theorems.\n       • The proofs can either appear in the main paper or the supplemental material, but if\n        they appear in the supplemental material, the authors are encouraged to provide a short\n        proof sketch to provide intuition.\n       • Inversely, any informal proof provided in the core of the paper should be complemented\n       by formal proofs provided in appendix or supplemental material.\n       • Theorems and Lemmas that the proof relies upon should be properly referenced.\n\n4. Experimental Result Reproducibility\n\n   Question: Does the paper fully disclose all the information needed to reproduce the main ex-\n   perimental results of the paper to the extent that it affects the main claims and/or conclusions\n   of the paper (regardless of whether the code and data are provided or not)?\n\n   Answer: [Yes]\n\n   Justification: We provide all experimental details in the experiment section. The detailed\n   input and output of Prompt Optimization Prompt of our method can be found in Appendix\n   and supplemental materials.\n\n   Guidelines:\n\n       • The answer NA means that the paper does not include experiments.\n       • If the paper includes experiments, a No answer to this question will not be perceived\n       well by the reviewers: Making the paper reproducible is important, regardless of\n       whether the code and data are provided or not.\n       •  If the contribution is a dataset and/or model, the authors should describe the steps taken\n        to make their results reproducible or verifiable.\n       • Depending on the contribution, reproducibility can be accomplished in various ways.\n       For example, if the contribution is a novel architecture, describing the architecture fully\n       might suffice, or if the contribution is a specific model and empirical evaluation, it may\n       be necessary to either make it possible for others to replicate the model with the same\n        dataset, or provide access to the model. In general. releasing code and data is often\n       one good way to accomplish this, but reproducibility can also be provided via detailed\n        instructions for how to replicate the results, access to a hosted model (e.g., in the case\n        of a large language model), releasing of a model checkpoint, or other means that are\n        appropriate to the research performed.\n       • While NeurIPS does not require releasing code, the conference does require all submis-\n        sions to provide some reasonable avenue for reproducibility, which may depend on the\n        nature of the contribution. For example\n        (a)  If the contribution is primarily a new algorithm, the paper should make it clear how\n            to reproduce that algorithm.\n        (b)  If the contribution is primarily a new model architecture, the paper should describe\n            the architecture clearly and fully.\n        (c)  If the contribution is a new model (e.g., a large language model), then there should\n             either be a way to access this model for reproducing the results or a way to reproduce\n           the model (e.g., with an open-source dataset or instructions for how to construct\n            the dataset).\n        (d) We recognize that reproducibility may be tricky in some cases, in which case\n            authors are welcome to describe the particular way they provide for reproducibility.\n            In the case of closed-source models, it may be that access to the model is limited in\n         some way (e.g., to registered users), but it should be possible for other researchers\n            to have some path to reproducing or verifying the results.\n\n5. Open access to data and code\n\n\n                                  38\n\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\n   tions to faithfully reproduce the main experimental results, as described in supplemental\n   material?\n   Answer: [Yes]\n   Justification: We provide all experimental details in the experiment section. The detailed\n  Prompt Optimization Prompt and Python codes of our method can be found in Appendix\n   and supplemental materials.\n   Guidelines:\n       • The answer NA means that paper does not include experiments requiring code.\n       • Please see the NeurIPS code and data submission guidelines (https://nips.cc/\n      public/guides/CodeSubmissionPolicy) for more details.\n       • While we encourage the release of code and data, we understand that this might not be\n        possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\n        including code, unless this is central to the contribution (e.g., for a new open-source\n       benchmark).\n       • The instructions should contain the exact command and environment needed to run to\n       reproduce the results. See the NeurIPS code and data submission guidelines (https:\n      //nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n       • The authors should provide instructions on data access and preparation, including how\n        to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n       • The authors should provide scripts to reproduce all experimental results for the new\n       proposed method and baselines. If only a subset of experiments are reproducible, they\n       should state which ones are omitted from the script and why.\n       • At submission time, to preserve anonymity, the authors should release anonymized\n        versions (if applicable).\n       • Providing as much information as possible in supplemental material (appended to the\n        paper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\n   Question: Does the paper specify all the training and test details (e.g., data splits, hyper-\n   parameters, how they were chosen, type of optimizer, etc.) necessary to understand the\n   results?\n   Answer: [Yes]\n   Justification: We follow the standard experimental setup in IPO, where the data splits, is set\n   as the same as previous works CoOP [8] and CoCoOP [11]. Detailed information can be\n   found in Sec. 5\n   Guidelines:\n       • The answer NA means that the paper does not include experiments.\n       • The experimental setting should be presented in the core of the paper to a level of detail\n         that is necessary to appreciate the results and make sense of them.\n       • The full details can be provided either with the code, in appendix, or as supplemental\n        material.\n7. Experiment Statistical Significance\n   Question: Does the paper report error bars suitably and correctly defined or other appropriate\n   information about the statistical significance of the experiments?\n   Answer: [Yes]\n   Justification: Following the standard experimental setup, we repeat each experiment over 3\n  random seeds and report the mean of the results.\n   Guidelines:\n       • The answer NA means that the paper does not include experiments.\n       • The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\n       dence intervals, or statistical significance tests, at least for the experiments that support\n        the main claims of the paper.\n\n\n                                  39\n\n• The factors of variability that the error bars are capturing should be clearly stated (for\n        example, train/test split, initialization, random drawing of some parameter, or overall\n        run with given experimental conditions).\n        • The method for calculating the error bars should be explained (closed form formula,\n          call to a library function, bootstrap, etc.)\n        • The assumptions made should be given (e.g., Normally distributed errors).\n        •  It should be clear whether the error bar is the standard deviation or the standard error\n         of the mean.\n        •  It is OK to report 1-sigma error bars, but one should state it. The authors should\n         preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\n         of Normality of errors is not verified.\n        • For asymmetric distributions, the authors should be careful not to show in tables or\n          figures symmetric error bars that would yield results that are out of range (e.g. negative\n          error rates).\n        •  If error bars are reported in tables or plots, The authors should explain in the text how\n         they were calculated and reference the corresponding figures or tables in the text.\n 8. Experiments Compute Resources\n    Question: For each experiment, does the paper provide sufficient information on the com-\n    puter resources (type of compute workers, memory, time of execution) needed to reproduce\n    the experiments?\n    Answer: [Yes]\n     Justification: We provide the computing resources in experiments 5.\n    Guidelines:\n        • The answer NA means that the paper does not include experiments.\n        • The paper should indicate the type of compute workers CPU or GPU, internal cluster,\n         or cloud provider, including relevant memory and storage.\n        • The paper should provide the amount of compute required for each of the individual\n         experimental runs as well as estimate the total compute.\n        • The paper should disclose whether the full research project required more compute\n         than the experiments reported in the paper (e.g., preliminary or failed experiments that\n         didn’t make it into the paper).\n 9. Code Of Ethics\n    Question: Does the research conducted in the paper conform, in every respect, with the\n    NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\n    Answer: [Yes]\n     Justification: We reviewed and followed the NeurIPS Code of Ethics.\n    Guidelines:\n        • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n        • If the authors answer No, they should explain the special circumstances that require a\n         deviation from the Code of Ethics.\n        • The authors should make sure to preserve anonymity (e.g., if there is a special consid-\n         eration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\n    Question: Does the paper discuss both potential positive societal impacts and negative\n     societal impacts of the work performed?\n    Answer: [Yes]\n     Justification: We provide the potential broader impacts in the conclusion section 6.\n    Guidelines:\n        • The answer NA means that there is no societal impact of the work performed.\n        • If the authors answer NA or No, they should explain why their work has no societal\n        impact or why the paper does not address societal impact.\n\n\n                                   40\n\n• Examples of negative societal impacts include potential malicious or unintended uses\n           (e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n           (e.g., deployment of technologies that could make decisions that unfairly impact specific\n         groups), privacy considerations, and security considerations.\n        • The conference expects that many papers will be foundational research and not tied\n          to particular applications, let alone deployments. However, if there is a direct path to\n        any negative applications, the authors should point it out. For example, it is legitimate\n          to point out that an improvement in the quality of generative models could be used to\n         generate deepfakes for disinformation. On the other hand, it is not needed to point out\n          that a generic algorithm for optimizing neural networks could enable people to train\n        models that generate Deepfakes faster.\n        • The authors should consider possible harms that could arise when the technology is\n        being used as intended and functioning correctly, harms that could arise when the\n        technology is being used as intended but gives incorrect results, and harms following\n        from (intentional or unintentional) misuse of the technology.\n        •  If there are negative societal impacts, the authors could also discuss possible mitigation\n          strategies (e.g., gated release of models, providing defenses in addition to attacks,\n        mechanisms for monitoring misuse, mechanisms to monitor how a system learns from\n        feedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\n    Question: Does the paper describe safeguards that have been put in place for responsible\n    release of data or models that have a high risk for misuse (e.g., pretrained language models,\n    image generators, or scraped datasets)?\n    Answer: [NA]\n     Justification: The data and models pose no such risks.\n    Guidelines:\n        • The answer NA means that the paper poses no such risks.\n        • Released models that have a high risk for misuse or dual-use should be released with\n         necessary safeguards to allow for controlled use of the model, for example by requiring\n          that users adhere to usage guidelines or restrictions to access the model or implementing\n         safety filters.\n        • Datasets that have been scraped from the Internet could pose safety risks. The authors\n        should describe how they avoided releasing unsafe images.\n        • We recognize that providing effective safeguards is challenging, and many papers do\n         not require this, but we encourage authors to take this into account and make a best\n          faith effort.\n12. Licenses for existing assets\n    Question: Are the creators or original owners of assets (e.g., code, data, models), used in\n    the paper, properly credited and are the license and terms of use explicitly mentioned and\n    properly respected?\n    Answer: [Yes]\n     Justification: We cite the original papers that produced the code package and datasets.\n    Guidelines:\n        • The answer NA means that the paper does not use existing assets.\n        • The authors should cite the original paper that produced the code package or dataset.\n        • The authors should state which version of the asset is used and, if possible, include a\n       URL.\n        • The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n        • For scraped data from a particular source (e.g., website), the copyright and terms of\n         service of that source should be provided.\n        • If assets are released, the license, copyright information, and terms of use in the\n        package should be provided. For popular datasets, paperswithcode.com/datasets\n        has curated licenses for some datasets. Their licensing guide can help determine the\n         license of a dataset.\n\n\n                                   41\n\n• For existing datasets that are re-packaged, both the original license and the license of\n         the derived asset (if it has changed) should be provided.\n        • If this information is not available online, the authors are encouraged to reach out to\n         the asset’s creators.\n13. New Assets\n    Question: Are new assets introduced in the paper well documented and is the documentation\n    provided alongside the assets?\n    Answer: [Yes]\n     Justification: Details of the datasets/code/model are provided in the supplemental materials.\n    Guidelines:\n        • The answer NA means that the paper does not release new assets.\n        • Researchers should communicate the details of the dataset/code/model as part of their\n        submissions via structured templates. This includes details about training, license,\n          limitations, etc.\n        • The paper should discuss whether and how consent was obtained from people whose\n          asset is used.\n        • At submission time, remember to anonymize your assets (if applicable). You can either\n         create an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\n    Question: For crowdsourcing experiments and research with human subjects, does the paper\n    include the full text of instructions given to participants and screenshots, if applicable, as\n    well as details about compensation (if any)?\n    Answer: [NA]\n     Justification: This paper does not involve crowdsourcing nor research with human subjects.\n    Guidelines:\n        • The answer NA means that the paper does not involve crowdsourcing nor research with\n       human subjects.\n        • Including this information in the supplemental material is fine, but if the main contribu-\n         tion of the paper involves human subjects, then as much detail as possible should be\n         included in the main paper.\n        • According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\n         or other labor should be paid at least the minimum wage in the country of the data\n          collector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\n    Subjects\n    Question: Does the paper describe potential risks incurred by study participants, whether\n    such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\n    approvals (or an equivalent approval/review based on the requirements of your country or\n     institution) were obtained?\n    Answer: [NA]\n     Justification: This paper does not involve crowdsourcing nor research with human subjects.\n    Guidelines:\n        • The answer NA means that the paper does not involve crowdsourcing nor research with\n       human subjects.\n        • Depending on the country in which research is conducted, IRB approval (or equivalent)\n      may be required for any human subjects research. If you obtained IRB approval, you\n        should clearly state this in the paper.\n        • We recognize that the procedures for this may vary significantly between institutions\n        and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\n         guidelines for their institution.\n        • For initial submissions, do not include any information that would break anonymity (if\n          applicable), such as the institution conducting the review.\n\n\n                                   42",
"headers": [
"arXiv:2410.15397v1  [cs.LG]  20 Oct 2024",
"IPO: Interpretable Prompt Optimization for",
"Vision-Language Models",
"Abstract",
"1",
"Introduction",
"2",
"Related work",
"3",
"Preliminaries",
"4",
"Methods",
"5",
"Experiments",
"6",
"Conclusion",
"Acknowledgment",
"References",
"A",
"Experiments on 16-Shots",
"B",
"Experiments on cross-dataset",
"C",
"Impact of LLM",
"D",
"Experiments on Segmentation Task with IPO",
"E",
"Experiments on segmentation task with IPO",
"F",
"Effect of batch size",
"G",
"Impact of prompt history length",
"H",
"Comparison with recent prompt learning methods",
"I",
"Detailed Prompt Optimization Prompt",
"J",
"Loss and accuracy curve.",
"K",
"More generated prompts",
"L",
"Image description with LMM",
"M",
"Prompt Optimization Prompt design",
"N",
"More occlusion sensitivity analysis",
"NeurIPS Paper Checklist"
],
"tables": [
"|a photo of a <CLASS>.|69.34|76.72|72.84|\n|---|---|---|---|",
"|Identify the unique visual features of the <CLASS> flower accurately.|74.17|79.65|76.81|\n|---|---|---|---|",
"|GPT-3.5-turbo|71.76|77.00|74.29|\n|---|---|---|---|",
"|MiniCPM-V-2-2.8B|71.76|77.00|74.29|\n|---|---|---|---|",
"|Caltech101|Categorize the <CLASS> shown in the image.|\n|---|---|",
"|StanfordCars|Describe the distinguishing characteristics of the <CLASS> in the image.|\n|---|---|",
"|Food101|Identify the primary ingredient in the <CLASS> and describe its texture,<br>color, and presentation.|\n|---|---|",
"|SUN397|A photo of a <CLASS>, a type of large-scale scene.|\n|---|---|",
"|EuroSAT|Analyze the <CLASS> vehicles in the satellite image with state-of-the-<br>art algorithms for precise classification and optimal efficiency.|\n|---|---|",
"|IPO|71.76|77.00|74.29|\n|---|---|---|---|",
"|IPO|74.09|69.17|71.54|\n|---|---|---|---|",
"|IPO|96.53|95.39|95.95|\n|---|---|---|---|",
"|IPO|94.48|97.93|96.43|\n|---|---|---|---|",
"|IPO|63.83|75.45|69.16|\n|---|---|---|---|",
"|IPO|74.17|79.65|76.81|\n|---|---|---|---|",
"|IPO|89.78|91.59|90.67|\n|---|---|---|---|",
"|IPO|31.43|36.32|33.70|\n|---|---|---|---|",
"|IPO|72.25|77.53|74.80|\n|---|---|---|---|",
"|IPO|55.45|62.47|58.75|\n|---|---|---|---|",
"|IPO|64.97|82.13|72.54|\n|---|---|---|---|",
"|IPO|72.43|79.35|75.73|\n|---|---|---|---|",
"|Col1|Base Novel|Col3|H|\n|---|---|---|---|\n|CLIP [30]<br>CoOP [8]<br>CoCoOp [11]<br>MaPLe [13]<br>PromptSRC [28]<br>CoPrompt [32]<br>|69.34<br>74.22<br>82.69<br>63.22<br>80.47<br>71.69<br>82.28<br>75.14<br>**84.26**<br>76.10<br>84.00<br>77.23|69.34<br>74.22<br>82.69<br>63.22<br>80.47<br>71.69<br>82.28<br>75.14<br>**84.26**<br>76.10<br>84.00<br>77.23|71.70<br>71.66<br>75.83<br>78.55<br>79.97<br>**80.48**|\n|**IPO** (1-shot)<br>|71.76|77.00<br>|74.29|\n|**IPO** (16-shot)|79.92|**80.51**|80.21|",
"|IPO|79.92|80.51|80.21|\n|---|---|---|---|",
"|IPO|77.83|72.45|75.04|\n|---|---|---|---|",
"|IPO|97.32|95.23|96.26|\n|---|---|---|---|",
"|IPO|95.21|98.23|96.70|\n|---|---|---|---|",
"|IPO|73.42|75.71|74.55|\n|---|---|---|---|",
"|IPO|96.78|78.32|86.58|\n|---|---|---|---|",
"|IPO|90.92|93.08|91.99|\n|---|---|---|---|",
"|IPO|41.21|41.42|41.31|\n|---|---|---|---|",
"|IPO|81.25|80.92|81.08|\n|---|---|---|---|",
"|IPO|82.14|66.81|73.69|\n|---|---|---|---|",
"|IPO|94.25|80.11|86.61|\n|---|---|---|---|",
"|IPO|85.32|80.92|83.06|\n|---|---|---|---|",
"|IPO|72.15|94.34|90.96|66.10|72.75|86.75|25.14|67.97|47.01|48.56|69.23|67.36|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|",
"|IPO w/ GPT-3.5|74.09|69.17|71.54|\n|---|---|---|---|\n|IPO w/ GPT-4o|76.14|72.13|74.09|",
"|Zsseg + IPO|91.2|84.7|73.2|78.6|\n|---|---|---|---|---|\n|ZegCLIP + IPO|95.3|92.7|78.7|85.1|",
"|IPO|79.92|80.51|80.21|\n|---|---|---|---|",
"|IPO|74.29|80.21|\n|---|---|---|",
"|Caltech101|Take an awe-inspiring photograph of <CLASS> that beautifully captures<br>its essence with exceptional clarity, vibrant colors, impeccable composi-<br>tion, and mesmerizing details.|\n|---|---|",
"|StanfordCars|Use a higher resolution camera to capture the vehicle photo <CLASS>..|\n|---|---|",
"|Food101|Categorize the image depicting a delicious and appetizing <CLASS><br>with remarkable visual qualities.|\n|---|---|",
"|SUN397|A photo of a <CLASS>, a type of large-scale scene.|\n|---|---|",
"|EuroSAT|Construct a state-of-the-art deep learning model on the Sentinel-2 satel-<br>lite dataset for <CLASS> leveraging cutting-edge techniques including<br>attention mechanisms, transfer learning, ensemble learning.|\n|---|---|",
"|Datasets|Images|Text description|\n|---|---|---|",
"|Col1|The cheetah’s fur has spots and stripes, its eyes are black with white pupils. It is<br>looking straight into the camera while standing against a clear blue sky background.|\n|---|---|",
"|Col1|Col2|The image prominently features a large white and red airplane with the Turkish flag<br>logo, flying against a backdrop of blue sky dotted with clouds.|\n|---|---|---|",
"|Col1|The image features a black Audi TT parked outside the dealership, with its silver<br>rims and elegant design.|\n|---|---|",
"|Col1|Col2|The car in the image is a 2016 Acura TSX, distinguished by its sleek design and<br>distinctive tail lights.|\n|---|---|---|",
"|Col1|The flowers have a combination of pink and white colors with distinct leaves, creating<br>an attractive appearance.|\n|---|---|",
"|Col1|Col2|The flowers in the image are large, have a yellow center with dark spots and petals<br>that vary from pink to red. They appear vibrant due to their color contrast against<br>natural backgrounds like leaves or rocks on trees behind them.|\n|---|---|---|",
"|Col1|The cat in the image is a light-colored, possibly an orange tabby with big yellow<br>eyes and white whiskers.|\n|---|---|",
"|Col1|Col2|The image features a small white puppy with brown spots, which is likely to be an<br>American Pit Bull Terrier breed.|\n|---|---|---|",
"|Col1|The food in the image is a delicious pastry with meat and cheese filling, covered by<br>caramelized topping.|\n|---|---|",
"|Col1|Col2|The image features a colorful and diverse salad with various ingredients such as<br>onions, corn kernels, lettuce leaves (cabbage), peanuts or pumpkin seeds.|\n|---|---|---|",
"|Col1|The aircraft in the image is a large commercial airplane, likely used for passenger<br>transportation. It has an orange and white color scheme with prominent windows<br>along its body to provide natural light inside during flights.|\n|---|---|",
"|Col1|Col2|The aircraft in the image is a Western Airlines Boeing 737, identified by its distinctive<br>tail fin with \"Western\" written on it. It has various markings including numbers and<br>letters that are part of their registration or identification system used for aviation<br>purposes.|\n|---|---|---|",
"|Col1|The image showcases a commercial airplane cabin with rows of blue and white<br>chairs, each equipped with cup holders for passengers’ convenience.|\n|---|---|",
"|Col1|Col2|The image captures the grandeur of a monument with an elaborate arch, surrounded<br>by fountains and illuminated buildings at night.|\n|---|---|---|",
"|Col1|The image showcases a close-up view of the texture on an object with distinct stripes,<br>buttons and stitching details.|\n|---|---|",
"|Col1|Col2|The leaf has a yellowish background with darker spots and irregular black marks,<br>indicating possible disease or damage.|\n|---|---|---|",
"|Col1|The fish, characterized by its light yellow body and distinct blue fins with a black<br>spot on the tail fin’s edge.|\n|---|---|",
"|Col1|Col2|The image features a white chicken with red comb and orange legs, standing on<br>wooden planks surrounded by straw or grass.|\n|---|---|---|",
"|Col1|The image features a black car with four-wheeled design, possibly indicating that it<br>is an automobile.|\n|---|---|",
"|Col1|Col2|The image captures various cars, each with distinct features such as their color and<br>shape. For instance, one car is red in color while another has a unique design that<br>stands out from the rest of them.|\n|---|---|---|",
"|Col1|A baby in a yellow shirt and blue jeans is actively crawling on the tiled floor, while<br>an adult wearing red pants stands nearby.|\n|---|---|",
"|Col1|Col2|A female gymnast is skillfully performing on a balance beam, showcasing her<br>athletic abilities as she navigates the challenging obstacle.|\n|---|---|---|",
"|a photo of a <CLASS>.|72.43|68.14|70.22|\n|---|---|---|---|",
"|Token 1, 2, 3, 4|73.20|67.43|70.20|\n|---|---|---|---|",
"|Token 1, 2, 3, 4|73.90|69.07|71.40|\n|---|---|---|---|",
"|Take a high-quality photo of a <CLASS>.|74.09|69.17|71.54|\n|---|---|---|---|",
"|a photo of a <CLASS>.|96.84|94.00|95.40|\n|---|---|---|---|",
"|Token 1, 2, 3, 4|90.63|85.20|87.83|\n|---|---|---|---|",
"|Token 1, 2, 3, 4|96.37|93.13|94.72|\n|---|---|---|---|",
"|Categorize the <CLASS> shown in the image.|96.53|95.39|95.95|\n|---|---|---|---|",
"|Col1|a photo of a <CLASS>.|89.42|96.81|92.97|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|93.73|96.23|94.96|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|93.47|96.27|94.85|\n|---|---|---|---|---|",
"|Col1|Take a well-composed photo of a <CLASS> with optimal lighting, focus,<br>and minimal distractions. Capture the pet’s unique characteristics,<br>including expression and posture, to ensure a clear and distinct image.|94.48|97.93|96.43|\n|---|---|---|---|---|",
"|Col1|a photo of a <CLASS>.|63.37|74.89|68.65|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|61.80|68.33|64.90|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|65.27|73.73|69.24|\n|---|---|---|---|---|",
"|Col1|Describe the distinguishing characteristics of the <CLASS> in the image.|63.83|75.45|69.16|\n|---|---|---|---|---|",
"|Col1|a photo of a <CLASS>.|69.34|76.72|72.84|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|71.47|72.47|71.97|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|73.67|75.50|74.57|\n|---|---|---|---|---|",
"|Col1|Identify the unique visual features of the <CLASS> flower accurately.|74.17|79.65|76.81|\n|---|---|---|---|---|",
"|Col1|a photo of a <CLASS>.|89.44|90.68|90.06|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|87.90|88.03|87.96|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|88.73|89.60|89.16|\n|---|---|---|---|---|",
"|Col1|Identify the primary ingredient in the <CLASS> and describe its texture, color, and presentation.|89.78|91.59|90.67|\n|---|---|---|---|---|",
"|Col1|a photo of a <CLASS>.|27.73|33.17|30.21|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|27.77|27.60|27.68|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|29.77|31.23|30.28|\n|---|---|---|---|---|",
"|Col1|Capture a comprehensive range of well-lit, high-resolution images of an <CLASS><br>from various angles, meticulously showcasing its specific design features with<br>perfect clarity and precision for unparalleled accuracy in aircraft.|31.43|36.32|33.70|\n|---|---|---|---|---|",
"|a photo of a <CLASS>.|69.36|75.35|72.23|\n|---|---|---|---|",
"|Token 1, 2, 3, 4|71.47|72.47|71.97|\n|---|---|---|---|",
"|Token 1, 2, 3, 4|73.67|75.50|74.57|\n|---|---|---|---|",
"|A photo of a <CLASS>, a type of large-scale scene.|72.25|77.53|74.80|\n|---|---|---|---|",
"|a photo of a <CLASS>.|54.63|59.18|56.81|\n|---|---|---|---|",
"|Token 1, 2, 3, 4|60.80|47.53|53.35|\n|---|---|---|---|",
"|Token 1, 2, 3, 4|58.70|52.70|55.54|\n|---|---|---|---|",
"|Classify the intricate <CLASS> texture.|55.45|62.47|58.75|\n|---|---|---|---|",
"|Col1|a photo of a <CLASS>.|50.26|69.90|58.47|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|69.13|50.33|58.25|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|71.13|62.87|66.75|\n|---|---|---|---|---|",
"|Col1|Analyze the <CLASS> vehicles in the satellite image with state-of-the-art algorithms<br>for precise classifciation and optimal efficiency.|64.97|82.13|72.54|\n|---|---|---|---|---|",
"|Col1|a photo of a <CLASS>.|68.15|75.07|71.44|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|72.50|63.57|67.74|\n|---|---|---|---|---|",
"|Col1|Token 1, 2, 3, 4|74.73|72.80|73.75|\n|---|---|---|---|---|",
"|Col1|Capture a high-quality, well-lit image of a person flawlessly demonstrating the <CLASS> action with<br>impeccable visual representation to achieve unmatched.|72.43|79.35|75.73|\n|---|---|---|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2410.15397v1.pdf"
}