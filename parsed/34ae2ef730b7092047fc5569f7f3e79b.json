{
"text": "APRIL: API Synthesis with Automatic Prompt\n                    Optimization and Reinforcement Learning\n\n\n                               Hua Zhong1, Shan Jiang1, and Sarfraz Khurshid1\n\n                                  The University of Texas at Austin, Austin TX 78712, USA\n                                    {hzhong,shanjiang}@utexas.edu, kuhrshid@ece.utexas.edu\n\n\n\n\n                               Abstract. APIs are central to modern software development, yet com-2025                                   posing new APIs from large libraries is difficult due to the exponential\n                                     search space; traditional component-based synthesis relies on costly ex-\n                                      ploration and hand-crafted specifications. While large language models\n                              (LLMs) can generate implementations from natural language, halluci-Aug\n                                    nations and limited access to up-to-date contextual information often\n                                        yield incorrect code. In this paper, we present APRIL, an approach that29\n                                 combines LLM-based synthesis with Automatic Prompt Optimization\n                            (APO) and Reinforcement Learning from Verifiable Rewards (RLVR):\n                      APO iteratively refines prompts for a frozen model, while RLVR fine-\n                                   tunes the policy toward functional correctness, producing an efficient\n                                      synthesis pipeline. Evaluated on 81 real-world APIs from widely used\n                                            scientific Python libraries and benchmarked against instruction-tuned[cs.SE]                           but unfine-tuned LLMs guided by expert prompts, APRIL achieves sub-\n                                         stantial improvements. These results indicate that integrating APO and\n                        RLVR provides a robust, scalable path for component-based API syn-\n                                        thesis in large libraries.\n\n                            Keywords: Component-based Synthesis · Large Language Models · Au-\n                                   tomatic Prompt Optimization · Reinforcement Learning\n\n\n                  1  Introduction\n\n                      The increasing complexity of modern software systems has made correct API\n                          usage and library integration a significant challenge for developers [20]. Even ex-\n                           perienced programmers often spend considerable time identifying relevant func-\n                             tions within a library and must frequently combine these with complex controlarXiv:2509.25196v1                      structures to achieve desired functionalities [22, 30]. This complexity complicates\n                        program synthesis, requiring both knowledge of library components and their ef-\n                               fective integration into coherent code. Correct API synthesis need to capture in-\n                         tended behavior, which is often informal and difficult to express as formal specifi-\n                              cations. While formal methods such as logical constraints [11, 32] and executable\n                              specifications [13, 16] exist, synthesizing APIs from input-output examples has\n                       become a more user-friendly and accessible alternative [30]. This approach allows\n                          developers—including non-programmers—to specify desired functionality using\n                          examples, bypassing the need for detailed technical specifications [10].\n\n2      Anon. Author et al.\n\n\n   Prior work on program synthesis has largely relied on domain-specific knowl-\nedge—e.g., string manipulation [23, 10] or data transformations [9, 6]—or on\nexpert-crafted domain-specific languages (DSLs) [26]. Domain-agnostic tech-\nniques such as loop-free synthesis [12] and oracle-guided synthesis [16] instead\nrequire formal specifications of components; specifying precise behavior for every\nlibrary primitive is costly and often infeasible, particularly when intended behav-\nior is informal or tacit. Although these methods solve various tasks, they typically\ntarget single basic blocks and scale poorly to multi-block programs with loops,\nrecursion, or complex guards, due to combinatorial growth in method sequences.\nTo mitigate this, SyPet [7], EdSynth [36], and FrAngel [30] favor black-box exe-\ncution over formal semantics, reducing dependence on DSLs and specifications.\nSyPet, however, struggles to synthesize rich control flow; FrAngel introduces an-\ngelic conditions with iterative refinement to construct more complex programs;\nand EdSynth leverages program sketches to similar effect. Live-execution ap-\nproaches such as LooPy [8] involve programmers as oracles, improving interac-\ntivity but reintroducing human dependence and limiting full automation in API\nsynthesis.\n\n   In contrast, large language models have recently demonstrated remarkable\nproficiency in natural language understanding and code synthesis tasks[24, 18].\nTrained on large datasets, including extensive code repositories and documen-\ntation, LLMs possess an implicit understanding of programming syntax and\nsemantics. These models can align user inputs with their pre-existing knowledge\nand even address tasks that are uncommon in their training data [28]. Unlike\ntraditional synthesis methods, which often rely on domain-specific knowledge[7]\nor predefined formal specifications, LLMs excel at managing complex, open-\nended programming tasks. They can synthesis APIs based on function signature\nand other part of the module, eliminating the need for explicit specifications or\nDSLs. Furthermore, LLMs handle intricate control structures such as loops and\nconditionals with ease, making them highly versatile and efficient. By directly\ninterpreting developer intent through function signatures and I/O examples,\nLLMs reduce the burden on developers, enabling rapid and adaptive solutions\nfor diverse functionalities.\n\n   However, industry experience indicates that large language models (LLMs)\ncontinue to exhibit limitations, such as hallucinations [25, 17]. Furthermore, there\nis a lack of research on systematically evaluating the ability of LLMs to synthe-\nsize code for deep learning libraries. In this paper, we evaluate the effectiveness\nof Automatic Prompt Optimization (APO) [27] and Reinforcement Learning\nfrom Verifiable Rewards (RLVR) [19] approaches for API synthesis in scientific\nPython libraries. APO refers to methods that autonomously refine prompts to\nimprove task performance in large language models. APO techniques offer key\nadvantages: they enhance model outputs without requiring access to model pa-\nrameters, systematically explore the space of possible prompts, and produce im-\nprovements that remain interpretable to humans. In addition, RLVR optimizes\nthe policy using rewards derived from objective, programmatically checkable\nsignals rather than human preference models. APO refines prompts, instruc-\n\nAPRIL: API Synthesis with APO and Reinforcement Learning      3\n\n\ntions, and in-context examples to bias a frozen LLM toward producing correct\nimplementation. RLVR prioritizes functional correctness and safe component\ncomposition by grounding feedback in unit-test outcomes and static-analysis di-\nagnostics, thereby improving pass rates and reliability. This allows the model to\ninternalize higher-level conventions and reliably apply them in synthesis tasks.\n   While both approaches have shown promise in various machine learning con-\ntexts, there has been no systematic comparison of APO and RLVR for the syn-\nthesis of new APIs in scientific Python libraries. Prior research has largely fo-\ncused on API invocation accuracy or small-scale code completion tasks, rather\nthan on the end-to-end generation of maintainable, idiomatic, and robust API\nimplementations. The reliability stakes are higher here: a poorly synthesized\nimplementation, once adopted, can ossify bad patterns across a codebase and\nbecome expensive to replace. In this paper, we present APRIL, the first con-\ntrolled, reliability-centric evaluation of APO v.s. RLVR for synthesizing new\nAPI implementations in scientific Python libraries.\n   This paper makes the following contributions:\n\n – Approach. We present a novel approach to API synthesis using LLMs by in-\n    tegrating Automatic Prompt Optimization (APO) and Reinforcement Learn-\n    ing from Verifiable Rewards (RLVR). By employing APO to iteratively refine\n   prompts and leveraging RLVR for efficient model fine-tuning, our method\n    reduces reliance on exhaustive search and manual component specification.\n – Evaluation. We conduct an experimental evaluation of our approach on 81\n   programming tasks sourced from widely used scientific Python libraries, in-\n    cluding NumPy, SciPy, and scikit-learn. We benchmark our method against\n    state-of-the-art, un-finetuned large language models (LLMs) that rely on\n   human-curated prompts. Experimental results demonstrate that our ap-\n   proach achieves a success rate exceeding 93.8% in synthesizing correct, test-\n    passing APIs, substantially outperforming baseline LLMs in terms of accu-\n    racy.\n – Test case generation. To obtain the verification oracles required for APO\n    training and RLVR fine-tuning, we employ the Gemini CLI coding agent [1]\n    to automatically synthesize test suites for scientific Python APIs in our train-\n    ing set. We additionally evaluate Gemini CLI’s test generation capability,\n    assessing the extent to which the produced suites are comprehensive and\n    semantically meaningful for validating API behavior. This analysis under-\n    scores Gemini CLI’s promise as an enabling component for automated API\n    synthesis pipelines and rigorous evaluation workflows.\n\n\n2  Problem Definition\n\nThis paper proposes a novel methodology for component-based API synthesis,\nwhich leverages large language models (LLMs) to generate an API from a set of\nbase components and input-output examples. Similar to traditional techniques,\nthe synthesis problem in our proposed methodology formally requires the fol-\nlowing inputs:\n\n4      Anon. Author et al.\n\n\n – A test oracle that, given a candidate API, returns a Boolean verdict (true/-\n    false) indicating whether the candidate satisfies the intended task specifica-\n    tion. Concretely, we instantiate the validation oracle using a comprehensive\n    test suite generated via an AI-agent-based methodology. The procedure for\n    constructing this test suite is detailed in Section 3.1.\n – A method signature that precisely specifies the API’s parameter and return\n    types, as well as its invocation modality (in Python, an instance method,\n    class method, static method, or module-level method).\n – A library of components from which the synthesizer assembles candidate\n    implementations of the target API. Specifically, we provide the library name\n   and the module path of the API’s intended location to the LLM as contextual\n   metadata, enabling it to determine the components required for synthesis.\n – A set of examples that, for any input within the API’s valid domain, returns\n    the reference output of the target API. The example set may be a subset of\n    the validation tests or constitute an independent collection of tests.\n\n\n3  API Synthesis Methodology\n\n\n\n\n\n                                                                                                              Training\n                                                                                                   data\n\n\n\n\n                                                  Manual       Initial Prompt               Optimized Prompt’\n                                                  Prompt                  Apo                           Fine-tuning\n                                                       Engineering\n\n       API     Components Sample Tests\n                                                                                         Optimized Prompt           New weights\n\n\n\n\n\n                                                               Execute                                                                                                                 Final LLM               Eval\n                                                                       Validation tests                                            data\n                                                                                 Synthesized API\n      Tests Outcome   Synthesized API\n\n\n\n\n                          Fig. 1. Workflow of APRIL\n\n\n   In this section, we illustrate the architecture of APRIL, and break it down\ninto different components. The overall workflow of APRIL is shown in Fig. 1. Our\nstudy focuses on synthesizing Scientific Python APIs. As mentioned in Section 2,\nvalidating the correctness of the synthesized API requires a test-based valida-\ntion oracle. We construct this oracle using a methodology powered by Gemini-cli,\nwhich automatically generates the requisite test suite. APRIL further requires a\ncomponent library, the target method signature, and a set of tests. These inputs\n\nAPRIL: API Synthesis with APO and Reinforcement Learning      5\n\n\nAlgorithm 1 Validation Tests Generation\n 1: procedure GenerateValidationTests(ds, ri)\n 2:    feedbackt ←∅\n 3:    feedbackc ←∅\n 4:    while true do\n 5:       Tc ←genTests(ds, ri, feedbackt, feedbackc)\n 6:          if testPass(Tc, ri) and isGoodQuality(ds, ri, Tc) then\n 7:         return Tc\n 8:        else\n 9:           feedbackt ←testPass(Tc, ri)\n10:          feedbackc ←isGoodQuality(ds, ri, Tc)\n11:      end if\n12:   end while\n13: end procedure\n\n\n\nare used to construct the API-synthesis prompt, after which we employ LLMs\nto directly generate candidate method implementations. To improve synthesis\neffectiveness, we apply Automatic Prompt Optimization (APO) and RLVR (Re-\ninforce Learning from Verifiable Rewards) fine-tuning, guiding the LLMs toward\nthe desired outputs. The details of our prompt-engineering methodology are\npresented in the following sections.\n\n\n3.1  Validation Oracle Generation\n\nFor each target API, we construct a comprehensive test suite that serves as its\nvalidation oracle. Because this test suite is only used to validate the synthesized\nAPI and is not an input to the synthesizer, we assume access to the API’s\ndocstrings ds (natural-language documentation embedded in the code) and a\ncorrect reference implementation ri. These artifacts are supplied to an AI agent,\nwhich generates the corresponding tests. Concretely, given ds and ri, we instruct\nthe agent to produce a candidate test suite Tc intended to validate the API. The\nagent then executes Tc against ri and reports the results; if any test fails, the\nfailure information is fed back to the agent, which regenerates Tc. Coinciding\nwith this, we submit ds, ri, and Tc to a second LLM acting as an evaluator\nto assess the quality of Tc with respect to comprehensiveness and test coverage\n(i.e., breadth of input scenarios and exercised behaviors). If the evaluator deems\nthe quality insufficient, its feedback is returned to the AI agent, which revises\nTc accordingly. We output the revised Tc as the final validation oracle once all\ntests pass and the quality criteria are satisfied. The algorithmic details of this\ntest-generation procedure are presented in Algorithm 1.\n\n\n3.2   Initial Prompt Construction\n\nWe define the initial prompt as the API-synthesis prompt used both as the\ninput to the baseline model and as the seed prompt for Automatic Prompt\n\n6      Anon. Author et al.\n\n\n\n\n\n 1 ### Implement a python method inside a python module.\n 2\n 3 ** Task :**\n 4\n 5 Imagine you are a Python developer specialized in machine learning and\n       scientific libraries. You will be given a python method signature , the\n       module it belongs to , the library it belongs to , and a set of test\n       cases. The method can be a instance method of a class or a module\n       method. Your task is to implement the method using the dependencies\n       from the module and the library to pass the test cases.\n 6\n 7 ** Output Format :**\n 8\n 9 Put your ouput inside the <output_api_implementations > xml element , and\n       output only the implemented method.\n10 <output_api_implementations > [your method implementation here] </\n       output_api_implementations >\n11\n12 ** Crucial Information :**\n13\n14 Carefully analyze the provided test cases. Each test case represents a\n       specific input and the expected output of the method you are\n       implementing . Your implementation *must* satisfy all given test cases.\n       Consider these test cases as concrete examples of how the method should\n        behave. Aim to re -use functions from the specified library wherever\n       possible.\n15\n16 ** Here are some examples :**\n17 Method signature: ****\n18 Module: ****\n19 Library: ****\n20 Test cases: ****\n21 Output:\n22 <output_api_implementations >\n23 ****\n24 </ output_api_implementations >\n25\n26 ****\n27\n28 Method signature: ****\n29 Module: ****\n30 Library: ****\n31 Test cases: ****\n32 Output:\n33 <output_api_implementations >\n34 ****\n35 </ output_api_implementations >\n36\n37 Method signature: { api_signature_rlvr_apo }\n38 Module: { module_rlvr_apo }\n39 Library: { library_rlvr_apo }\n40 Test cases: { tests_rlvr_apo }\n41 Output:\n\n\n                    Fig. 2. Manually engineered initial prompt\n\nAPRIL: API Synthesis with APO and Reinforcement Learning      7\n\n\nOptimization (APO). We construct this prompt manually using established\nprompt-engineering techniques, including: (i) role conditioning (“assistant cre-\nation”), which assigns an explicit persona to the LLM to situate the task context\nand constrain response style; (ii) chain-of-thought prompting, which encourages\ndecomposition of the synthesis task into smaller subproblems and the produc-\ntion of intermediate reasoning steps; and (iii) few-shot learning, which supplies\na small set of input–output exemplars to prime the LLM’s behavior through\nin-context learning. The resulting initial prompt is shown in Fig. 2.\n\n\n3.3  Automatic Prompt Optimization\n\nAutomatic Prompt Optimization (APO) is a model-agnostic procedure that op-\nerates with minimal prerequisites: a small training dataset, an initial prompt,\nand API access to an LLM. The algorithm processes mini-batches of training\nexamples to elicit “gradient”-like signals: textual critiques that diagnose limita-\ntions of the current prompt. It then uses these signals to propose and apply\nprompt edits. In practice, APO performs iterative prompt refinement analogous\nto gradient-based updates.\n\n\n\n\n\nFig. 3. Example of iterative improvement in the discriminator score (DS) between\nAPO search iterations\n\n\n   In our approach, we employ the manually engineered prompt from Section 3.3\nas the initial prompt P0 and use a subset of the benchmark dataset as the\ntraining data set A of n training tasks. Within our APO framework, text-gradient\ngeneration serves as the primary mechanism.\n\n8      Anon. Author et al.\n\n\n   To illustrate, consider a prompt candidate P and its generated solution\nsa, a ∈A at the current iteration. We provide P, sa, and T a ∈T (where T\ndenotes the validation test suite) to a discriminator prompt that instructs a\nclassification LLM llmc to execute the tests. We first request a binary verdict\n(fail/pass) from llmc and use it to compute a bad-generation penalty pa ∈0, 1\nfrom this result. If any test fails, we then request a textual critique ea from llmc\ndescribing the observed errors produced by pa. After processing all a ∈A in the\ncurrent iteration, we compute the discriminator score DS = 1 −1n Pni=1 pi and\nform the iteration’s “text gradient” by concatenating the textual critiques ei. Fig-\nure 3 illustrates the iterative improvement of the discriminator score (DS(P))\nover successive APO iterations. We then follow the standard APO procedure to\npropose edits to the current prompt and perform beam search over the resulting\ncandidates, using DS(P) to sort prompts at each level of the search tree. The\ncandidate P ⋆with the highest DS(P) in the final iteration is selected as the\noutput of our APO algorithm.\n\n\n\n3.4 RLVR Fine-tuning\n\n\nLoss function adopted by classic fine-tuning on text generation tasks is typically\nthe Negative Log-Likelihood (NLL) loss. The NLL loss quantifies the negative\nlog-probability that the model assigns to the ground-truth next token (i.e., the\ndiscrepancy between the predicted next-token distribution and the empirical\ndistribution concentrated on the observed token). Minimizing NLL promotes\nhigher probabilities for correct tokens and lower probabilities for incorrect tokens.\nIn the context of program synthesis, however, NLL primarily captures surface-\nlevel similarity between the target and LLM-generated implementations and is\ninsensitive to functional equivalence. This limitation is consequential, as distinct\nprograms can be semantically interchangeable despite syntactic divergence.\n   Given our evaluation is driven by a verification test suite, we recast fine-\ntuning from next-token prediction to RLVR, where synthesized programs receive\nrewards directly based on test outcomes (functionally correct versus incorrect).\n   To obtain training signals, we reuse the training dataset from Section 3.3\nand, for each API task x, condition the policy model πθ on the task prompt to\nsample K candidate implementations S = s1, . . . , sK. Sampling uses stochastic\ndecoding (temperature and top-p) with de-duplication of identical code strings.\nEach si  is executed against the validation tests. We assign a binary reward\nR(si) ∈0, 1, with R(si) = 1 iff all tests pass, and 0 otherwise. Candidates\nthat fail to build and execute receive R(si) = 0. We optimize πθ with Group\nRelative Policy Optimization (GRPO). For each group S, we compute a group-\nrelative advantage by centering the reward of si against the group mean, reducing\nvariance and inducing competition. Updates use a PPO-style clipped surrogate\nwith a KL penalty to a reference policy; πθold is fixed during each update and\nperiodically refreshed. Training runs over mini-batches for a few epochs with\nearly stopping when group reward stabilizes.\n\nAPRIL: API Synthesis with APO and Reinforcement Learning      9\n\n4  Evaluation\n\nIn this section, we present a comprehensive experimental evaluation to assess\nthe effectiveness of APRIL in scientific Python API synthesis tasks. The primary\nobjective is to evaluate the practicality of the generated APIs and their capability\nto accurately implement the expected functionality. Specifically, we address the\nfollowing research questions:\n\n – RQ1: Effectiveness. What proportion of APRIL generated APIs success-\n    fully build and pass the validation tests?\n – RQ2: Comparison. What is the relative success rate of APRIL, augmented\n   with APO and RLVR fine-tuning, compared with a baseline model employing\n   a manually engineered prompt?\n – RQ3: Tests Generation. How effective is the AI agent adopted in our\n    study on tests generation?\n\n\n4.1  Experimental Setup\n\nTo address the above research questions, we designed an experimental framework\nincorporating the following key components:\n\n\nLarge Language Models We employ a customized instance of Gemini 2.0—an\ninstruction-tuned large language model—as the synthesis model. The sampling\ntemperature is set to 0.7 to balance diversity and determinism in generated code,\na configuration empirically shown to be effective for code generation tasks [5].\nThe model is configured with an input context window of 32,000 tokens and\nan output context window (maximum generation length) of 8,000 tokens. For\nAPO, we use a separate customized instance of the same Gemini 2.0 model. The\nvalidation test suite is generated by the Gemini-cli AI agent [1], which internally\nleverages the Gemini 2.5 Pro model.\n\n\nBenchmarks We evaluate APRIL on three benchmark datasets comprising 81\nsynthesis tasks in total: 36 tasks from NumPy, 33 from scikit-learn, and 12 from\nSciPy. From these libraries, we additionally select 40 tasks to construct the train-\ning set used for APO and RLVR fine-tuning. The scikit-learn subset emphasizes\ncanonical machine learning scenarios (e.g., GaussianProcessRegressor), in which\nthe model is required to synthesize APIs that either transform input data or\ntrain a model on the provided data. The NumPy subset focuses on array-centric\noperations and numerically intensive routines, such as least-squares polynomial\nfitting. The SciPy subset primarily targets implementations of algebraic compu-\ntations and classical statistical procedures, like Minkowski distance calculations.\n\n\n4.2  Results Analysis\n\nRQ1 Our first evaluation of LLM-generated APIs examines effectiveness, de-\nfined by the proportion of tasks for which the synthesizer produces functionally\n\n10     Anon. Author et al.\n\n           Table 1. RQ1. Executability and test pass rate of APRIL\n\n\n               Benchmark #Tasks Executability Test Pass Rate\n            NumPy        36   36(100.0%)     35(97.2%)\n                   Scikit-learn     33    33(100%)     30(90.9%)\n                SciPy          12   12(100.0%)     11(91.7%)\n                 Total           81   81(100.0%)     76(93.8%)\n\n\n\ncorrect APIs. We decompose this measure into two components: (i) execution\nrate and (ii) the rate of passing the validation test cases. The execution rate\nassesses whether the synthesized API can be built and executed without error.\nBecause the LLM is provided with the target library and module context, and\nmay therefore depend on in-library components, we evaluate executability by in-\nserting the synthesized API into its designated module within the corresponding\nlibrary and then attempting to build and execute the API. The second com-\nponent measures whether the synthesized API passes the validation test suite\nassociated with the task. When constructing the validation test suite for a given\nAPI, we instruct Gemini-cli to generate tests within the module’s existing test\ndirectory hierarchy (i.e., the directory designated for that module’s tests). We\nthen compute the test success rate by executing the entire validation suite for\nthe target API under the library’s test harness and recording the fraction of\ntests that pass.\n   Table 1 summarizes these metrics, reporting for each benchmark the number\nof tasks alongside two outcome measures: executability and test success rate.\nEmpirically, all LLM-generated APIs built and executed successfully (100% exe-\ncutability). Among these, 93.8% passed their full validation suites. These results\nindicate high effectiveness in synthesizing functional APIs.\n\n\n\n  Table 2. RQ2. Comparison of APRIL with the baseline across three benchmarks\n\n\n        Benchmark #Tasks Success Rate: Baseline Success Rate: APRIL\n       NumPy        36            29(80.6%)           35(97.2%)\n           Scikit-learn     33            25(76.0%)           30(90.9%)\n         SciPy          12             9(75.0%)           11(91.7%)\n          Total           81            63(77.8%)           76(93.8%)\n\n\n\n\n\nRQ2 We address RQ2 by comparing the success rate of APRIL against a base-\nline that performs synthesis using the initial prompt on Gemini 2.0 model. We\nexecute all 81 tasks from the three benchmarks. The results are presented in\nTable 2. The results suggest that APRIL can outperform the state-of-the-art\nLLMs baseline across all three benchmarks. On average, it achieves improve-\n\nAPRIL: API Synthesis with APO and Reinforcement Learning     11\n\n\nments of 16.6%, 14.9%, and 16.7% over the baseline on the three benchmarks,\nrespectively.\n\n\n\n\n\n   Fig. 4. Potential iterative improvement from APO on RLVR fine-tuned Model\n\n\n   Due to time and space constraints, we do not conduct an ablation study iso-\nlating the individual contributions of APO and RLVR. Moreover, applying APO\nto the RLVR–fine-tuned model suggests additional performance gains (Fig. 4);\nhowever, a comprehensive analysis of this follow-on step is beyond the scope of\nthis paper.\n\n\n      Table 3. RQ3. Gemini-cli generated test counts and required iterations\n\n\n           Benchmark #Tasks Test count(Avg) Iterations count(Avg)\n         NumPy        36         258(7.2)               86(2.4)\n              Scikit-learn     33         282(8.5)               70(2.1)\n            SciPy          12         115(9.6)               19(1.6)\n             Total           81         655(8.1)              175(2.2)\n\n\n\n\n\nRQ3 We assess the test-generation capability of Gemini-cli along two dimen-\nsions: (i) the number of validation tests produced per API and (ii) the number\nof quality-evaluation iterations required to obtain a comprehensive test suite\nfor that API. As summarized in Table 3, Gemini-cli generates, on average, 7.2\n\n12     Anon. Author et al.\n\n\ntests per API for NumPy, 8.5 for scikit-learn, and 9.6 for SciPy. The system\nrequires an average of 2.2 iterations of the quality-evaluation loop to converge\nto a comprehensive test suite per API. These findings indicate that Gemini-cli\nis an effective tool for generating validation tests for scientific Python APIs,\nhighlighting its potential to support automated program synthesis pipelines and\nrigorous evaluation workflows.\n\n\n5  Related Work\n\n\nProgram synthesis has been a significant area of research, with various ap-\nproaches proposed to generate programs efficiently. Traditional methods, such as\nTransit [33] and Escher [2], rely on bottom-up enumerative synthesis, systemat-\nically exploring the program space to identify correct solutions. Brahma [12] in-\ntroduces an efficient SMT-based encoding for synthesizing straight-line programs\nthat involve multiple assignments to intermediate variables. Component-based\nsynthesizers, such as SyPet [7], focus on generating Java programs from exam-\nples by leveraging arbitrary libraries. However, SyPet is limited to synthesizing\nsequences of method calls and does not support control structures like loops or\nconditionals. Similarly, Python-based synthesizers such as TFCoder [29], Au-\ntoPandas [3], and Wrex [4] primarily target one-liners or sequences of method\ncalls. These tools also provide limited support for control structures, restricting\ntheir applicability for more complex program synthesis tasks. While these meth-\nods demonstrate the potential of automated program synthesis, their limitations\nin handling control structures underline the need for more versatile approaches.\n   Synthesis with control structures. EdSynth [36] support program syn-\nthesis with control flow by lazily initializing candidates during the execution\nof provided tests. The execution of partially completed candidates determines\nthe generation of future candidates, making EdSynth particularly effective for\nsynthesis tasks that involve multiple API sequences in both the conditions and\nbodies of loops or branches. FrAngel [30] is another notable tool that supports\ncomponent-based synthesis for Java programs with control structures. Unlike\nmany traditional methods, FrAngel relies on function-level specifications and,\nin principle, does not require users to have a detailed understanding of the al-\ngorithm or intermediate variables. However, in practice, to make the synthesis\nprocess feasible, users must provide a high-quality test suite, covering base and\ncorner cases. This requirement still necessitates some level of algorithmic knowl-\nedge, and FrAngel’s relatively slow performance makes it less suitable for interac-\ntive scenarios. LooPy [8], introduces the concept of an Intermediate State Graph\n(ISG), which compactly represents a vast space of code snippets composed of\nmultiple assignment statements and conditionals. By engaging human as an or-\nacle to address incomplete parts of the loop, LooPy achieves a balance between\nautomation and interactivity. Its ability to solve a wide range of synthesis tasks\nat interactive speeds makes it a practical tool for use cases requiring real-time\nfeedback and adjustments. These tools demonstrate progress in addressing the\n\nAPRIL: API Synthesis with APO and Reinforcement Learning     13\n\n\nchallenges of synthesis with control structures, but they also highlight trade-offs\nbetween usability, required user input, and performance.\n   Large Language Models (LLMs) have recently shown effectiveness in var-\nious software development tasks, including program synthesis[15, 14, 37] and test\ngeneration[35, 18]. By associating document text with code from a large train-\ning set, LLMs can generate program code from natural language prompts[34,\n31]. Reusable API[21] uses LLMs to generate APIs from code snippets collected\nfrom Stack Overflow and shows significant results in identifying API parameters\nand return types. However, they provide all components and dependencies to\nLLMs and only require LLMs to create new APIs using existing functions. Test-\ndriven program synthesis remains an under-researched topic. How well do LLMs\nperform in generating entire APIs with just a few input/output examples that\neven end users can easily prepare? In this paper, we explore the application of\nLLMs in generating API implementations and finds that LLMs are effective in\nunderstanding test cases and generating viable APIs. We also show that LLMs\nare able to generate accurate APIs even with an incomplete set of test cases,\nwhich is very convenient. To the best of our knowledge, our work is the first\nsystematic study of using LLMs with auto prompt engineering and fine-tuning\nto synthesize complex APIs.\n\n\n6  Conclusion\n\nWe propose integrating fine-tuned large language models (LLMs) with Auto-\nmatic Prompt Optimization (APO) for API synthesis. LLMs capture developer\nintent and context, enabling more effective synthesis. Empirically, our method\noutperforms a strong baseline that uses a state-of-the-art LLM with a manually\nengineered prompt. We also assess Gemini-cli’s ability to generate comprehensive\nvalidation test suites, underscoring its potential to support automated program\nsynthesis and rigorous evaluation. The approach requires minimal user inter-\naction, simplifying synthesis. Overall, the results support LLMs as a practical\nfoundation for complex API synthesis.\n\n\nReferences\n\n 1. Gemini cli, 2025. https://cloud.google.com/gemini/docs/codeassist/gemini-cli.\n 2. A. Albarghouthi, S. Gulwani, and Z. Kincaid. Recursive program synthesis.  In\n   Computer Aided Verification: 25th International Conference, CAV 2013, Saint Pe-\n    tersburg, Russia, July 13-19, 2013. Proceedings 25, pages 934–950. Springer, 2013.\n 3. R. Bavishi, C. Lemieux, R. Fox, K. Sen, and I. Stoica. Autopandas: neural-backed\n    generators for program synthesis. Proc. ACM Program. Lang., 3(OOPSLA), Oct.\n    2019.\n 4.  I. Drosos, T. Barik, P. J. Guo, R. DeLine, and S. Gulwani.  Wrex: A unified\n    programming-by-example interaction for synthesizing readable code for data sci-\n     entists. In Proceedings of the 2020 CHI Conference on Human Factors in Com-\n    puting Systems, CHI ’20, page 1–12, New York, NY, USA, 2020. Association for\n   Computing Machinery.\n\n14     Anon. Author et al.\n\n\n 5. M. Endres, S. Fakhoury, S. Chakraborty, and S. K. Lahiri. Can large language\n    models transform natural language intent into formal method postconditions? Pro-\n    ceedings of the ACM on Software Engineering, 1(FSE):1889–1912, 2024.\n 6. Y. Feng, R. Martins, J. Van Geffen, I. Dillig, and S. Chaudhuri.  Component-\n    based synthesis of table consolidation and transformation tasks from examples.\n    In Proceedings of the 38th ACM SIGPLAN Conference on Programming Language\n    Design and Implementation, PLDI 2017, page 422–436, New York, NY, USA, 2017.\n    Association for Computing Machinery.\n 7. Y. Feng, R. Martins, Y. Wang,  I. Dillig, and T. W. Reps.  Component-based\n    synthesis for complex apis. In Proceedings of the 44th ACM SIGPLAN Symposium\n   on Principles of Programming Languages, POPL ’17, page 599–612, New York,\n   NY, USA, 2017. Association for Computing Machinery.\n 8. K. Ferdowsifard, S. Barke, H. Peleg, S. Lerner, and N. Polikarpova. Loopy: in-\n    teractive program synthesis with control structures. Proc. ACM Program. Lang.,\n   5(OOPSLA), Oct. 2021.\n 9. J. K. Feser, S. Chaudhuri, and I. Dillig. Synthesizing data structure transforma-\n    tions from input-output examples. SIGPLAN Not., 50(6):229–239, June 2015.\n10. S. Gulwani. Automating string processing in spreadsheets using input-output ex-\n    amples. SIGPLAN Not., 46(1):317–330, Jan. 2011.\n11. S. Gulwani, S. Jha, A. Tiwari, and R. Venkatesan. Synthesis of loop-free programs.\n   SIGPLAN Not., 46(6):62–73, June 2011.\n12. S. Gulwani, S. Jha, A. Tiwari, and R. Venkatesan. Synthesis of loop-free programs.\n    In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language\n    Design and Implementation, PLDI ’11, page 62–73, New York, NY, USA, 2011.\n    Association for Computing Machinery.\n13. S. Heule, M. Sridharan, and S. Chandra. Mimic: computing models for opaque\n    code. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software\n    Engineering, ESEC/FSE 2015, page 710–720, New York, NY, USA, 2015. Associ-\n    ation for Computing Machinery.\n14. Y. Hong, S. Jiang, Y. Fu, and S. Khurshid. On the effectiveness of large language\n    models in writing alloy formulas. arXiv preprint arXiv:2502.15441, 2025.\n15. N. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Rajamani, and\n    R. Sharma. Jigsaw: Large language models meet program synthesis. In Proceedings\n    of the 44th International Conference on Software Engineering, pages 1219–1231,\n    2022.\n16. S. Jha, S. Gulwani, S. A. Seshia, and A. Tiwari. Oracle-guided component-based\n   program synthesis. In Proceedings of the 32nd ACM/IEEE International Confer-\n    ence on Software Engineering - Volume 1, ICSE ’10, page 215–224, New York, NY,\n   USA, 2010. Association for Computing Machinery.\n17. S. Jiang, P. Kovuri, D. Tao, and Z. Tan. Cascade: Llm-powered javascript deob-\n    fuscator at google. arXiv preprint arXiv:2507.17691, 2025.\n18. S. Jiang, C. Zhu, and S. Khurshid. Generating executable oracles to check con-\n    formance of client code to requirements of jdk javadocs using llms. arXiv preprint\n    arXiv:2411.01789, 2024.\n19. N. Lambert, J. Morrison, V. Pyatkin, S. Huang, H. Ivison, F. Brahman, L. J. V.\n    Miranda, A. Liu, N. Dziri, X. Lyu, Y. Gu, S. Malik, V. Graf, J. D. Hwang, J. Yang,\n    R. L. Bras, O. Tafjord, C. Wilhelm, L. Soldaini, N. A. Smith, Y. Wang, P. Dasigi,\n   and H. Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training.\n    In Second Conference on Language Modeling, 2025.\n20. M. Lamothe, Y.-G. Guéhéneuc, and W. Shang. A systematic review of api evolution\n     literature. ACM Comput. Surv., 54(8), Oct. 2021.\n\nAPRIL: API Synthesis with APO and Reinforcement Learning     15\n\n\n21. Y. Mai, Z. Gao, X. Hu, L. Bao, Y. Liu, and J. Sun. Are human rules necessary?\n    generating reusable apis with cot reasoning and in-context learning. Proc. ACM\n    Softw. Eng., 1(FSE), July 2024.\n22. D. Mandelin, L. Xu, R. Bodík, and D. Kimelman.  Jungloid mining: helping to\n    navigate the api jungle. In Proceedings of the 2005 ACM SIGPLAN Conference on\n   Programming Language Design and Implementation, PLDI ’05, page 48–61, New\n    York, NY, USA, 2005. Association for Computing Machinery.\n23. A. K. Menon, O. Tamuz, S. Gulwani, B. Lampson, and A. T. Kalai. A machine\n    learning framework for programming by example. In Proceedings of the 30th Inter-\n    national Conference on International Conference on Machine Learning - Volume\n    28, ICML’13, page I–187–I–195. JMLR.org, 2013.\n24. D. Nam, A. Macvean, V. Hellendoorn, B. Vasilescu, and B. Myers. Using an llm to\n    help with code understanding. In Proceedings of the IEEE/ACM 46th International\n    Conference on Software Engineering, pages 1–13, 2024.\n25. S. Nikolov, D. Codecasa, A. Sjovall, M. Tabachnyk, S. Taneja, C. Ziftci, and\n    S. Chandra.  How  is google using ai for internal code migrations?  In 2025\n   IEEE/ACM 47th  International Conference on Software Engineering (ICSE).\n   IEEE, 2025.\n26. D. Perelman, S. Gulwani, D. Grossman, and P. Provost.  Test-driven synthesis.\n    In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language\n    Design and Implementation, PLDI ’14, page 408–418, New York, NY, USA, 2014.\n    Association for Computing Machinery.\n27. R. Pryzant, D. Iter, J. Li, Y. Lee, C. Zhu, and M. Zeng.  Automatic prompt\n    optimization with “gradient descent” and beam search. In H. Bouamor, J. Pino,\n   and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in\n    Natural Language Processing, pages 7957–7968, Singapore, Dec. 2023. Association\n     for Computational Linguistics.\n28. R. Schaeffer, B. Miranda, and S. Koyejo. Are emergent abilities of large language\n    models a mirage? Advances in Neural Information Processing Systems, 36, 2024.\n29. K. Shi, D. Bieber, and R. Singh. Tf-coder: Program synthesis for tensor manipu-\n     lations. ACM Trans. Program. Lang. Syst., 44(2), May 2022.\n30. K. Shi, J. Steinhardt, and P. Liang.  Frangel: component-based synthesis with\n    control structures. Proc. ACM Program. Lang., 3(POPL), Jan. 2019.\n31. C. Spiess, D. Gros, K. S. Pai, M. Pradel, M. R.  I. Rabin, S. Jha, P. De-\n    vanbu, and T. Ahmed. Quality and trust in llm-generated code. arXiv preprint\n    arXiv:2402.02047, 2024.\n32. S. Srivastava, S. Gulwani, and J. S. Foster. From program verification to program\n    synthesis. SIGPLAN Not., 45(1):313–326, Jan. 2010.\n33. A. Udupa, A. Raghavan, J. V. Deshmukh, S. Mador-Haim, M. M. Martin, and\n    R. Alur.  Transit: specifying protocols with concolic snippets.  In Proceedings of\n    the 34th ACM SIGPLAN Conference on Programming Language Design and Im-\n    plementation, PLDI ’13, page 287–296, New York, NY, USA, 2013. Association for\n   Computing Machinery.\n34. S. Ugare, T. Suresh, H. Kang, S. Misailovic, and G. Singh. Improving llm code\n    generation with grammar augmentation. arXiv preprint arXiv:2403.01632, 2024.\n35. C. S. Xia, M. Paltenghi, J. Le Tian, M. Pradel, and L. Zhang. Fuzz4all: Univer-\n     sal fuzzing with large language models.  In Proceedings of the IEEE/ACM 46th\n    International Conference on Software Engineering, pages 1–13, 2024.\n36. Z. Yang, J. Hua, K. Wang, and S. Khurshid. Edsynth: Synthesizing api sequences\n    with conditionals and loops. In 2018 IEEE 11th International Conference on Soft-\n    ware Testing, Verification and Validation (ICST), pages 161–171, 2018.\n\n16     Anon. Author et al.\n\n\n37. H. Zhong, S. Jiang, and S. Khurshid. An approach for api synthesis using large\n    language models. arXiv preprint arXiv:2502.15246, 2025.",
"headers": [
"arXiv:2509.25196v1  [cs.SE]  29 Aug 2025",
"APRIL: API Synthesis with Automatic Prompt",
"Optimization and Reinforcement Learning",
"1",
"Introduction",
"2",
"Problem Definition",
"3",
"API Synthesis Methodology",
"4",
"Evaluation",
"5",
"Related Work",
"6",
"Conclusion",
"References"
],
"tables": [
"|Benchmark|#Tasks|Executability Test Pass Rate|\n|---|---|---|\n|NumPy<br>Scikit-learn<br>SciPy|36<br>33<br>12|36(100.0%)<br>35(97.2%)<br>33(100%)<br>30(90.9%)<br>12(100.0%)<br>11(91.7%)|\n|Total|81|81(100.0%)<br>76(93.8%)|",
"|Benchmark|#Tasks|Success Rate: Baseline|Success Rate: APRIL|\n|---|---|---|---|\n|NumPy<br>Scikit-learn<br>SciPy|36<br>33<br>12|29(80.6%)<br>25(76.0%)<br>9(75.0%)|35(97.2%)<br>30(90.9%)<br>11(91.7%)|\n|Total|81|63(77.8%)|76(93.8%)|",
"|Benchmark|#Tasks|Test count(Avg)|Iterations count(Avg)|\n|---|---|---|---|\n|NumPy<br>Scikit-learn<br>SciPy|36<br>33<br>12|258(7.2)<br>282(8.5)<br>115(9.6)|86(2.4)<br>70(2.1)<br>19(1.6)|\n|Total|81|655(8.1)|175(2.2)|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2509.25196v1.pdf"
}