{
"text": "Preprint\n\n\n          PROMPT OPTIMIZATION WITH LOGGED BANDIT DATA\n\n\n                   Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims\n                        Cornell University\n                 {hk844, dyc33, ys552, tj36 }@cornell.edu\n\n\n\n                                       ABSTRACT\n\n                    We study how to use naturally available user feedback, such as clicks, to optimize\n                                  large language model (LLM) pipelines for generating personalized sentences using\n                              prompts. Naive approaches, which estimate the policy gradient in the prompt\n                                space, suffer either from variance caused by the large action space of prompts or2025                           bias caused by inaccurate reward predictions. To circumvent these challenges, we\n                             propose a novel kernel-based off-policy gradient method , which estimates the\n                                policy gradient by leveraging similarity among generated sentences, substantiallyApr                        reducing variance while suppressing the bias. Empirical results on our newly\n                                established suite of benchmarks, demonstrate the effectiveness of the proposed\n3\n                             approach in generating personalized descriptions for movie recommendations,\n                                   particularly when the number of candidate prompts is large.\n\n\n                1  INTRODUCTION[cs.LG]             As more systems with large language model (LLM)-generated text are starting to become operational,\n                 we are naturally collecting increasing amounts of logged user feedback from their system interactions.\n                    These feedback signals provide valuable information on whether the prompt or generated sentence\n                   was effective for the user. Unlike conventional datasets used for LLM training (Stiennon et al., 2020),\n                            this feedback is available for all users at little cost, providing opportunities for personalizing sentence\n                       generation in applications like search, recommendations, and educational chatbots. Thus, it is worth\n                      developing a method to use such naturally logged user feedback to enhance the quality and outcome\n                                (i.e., reward) of language generation.\n\n                   To optimize sentence generation, we focus on learning a prompt policy (i.e., which prompt to use for\n                      a particular user or situation). As detailed in the following, learning a prompt policy is attractive for\n                       reasons of (1) safety, (2) cost, and (3) accessibility. First, for most applications it is a key requirement\n                         to not produce harmful outputs (Bai et al., 2022). By only adapting the prompts without fine-tuning\n                       the LLM itself, we do not run the risk of removing the safety properties of the underlying LLM.\n                     Second, compared to the LLM itself, a prompt policy can be a small model, which reduces the\n                       required computational resources and the amount of data needed for training (Deng et al., 2022).\n                        Additionally, compared to using a hand-engineered prompt, a prompt policy enables automatedarXiv:2504.02646v1\n                    prompt optimization and promises greater personalization. Third, a prompt policy can be trained\n                     even in situations where the LLM is closed-weights and available only through an inference API,\n                    which makes prompt-policy learning feasible even for small companies or individuals.\n\n                   While learning a prompt policy is attractive as argued above, using logged user feedback and\n                      performing Off-Policy Learning (OPL) of a new prompt policy entails several challenges due to the\n                          partial nature of the feedback. Specifically, the logged data is bandit feedback, containing the reward\n                         for only the action (prompt) chosen by the logging policy (i.e., the one used in past operations) and\n                      not for the other actions that a new policy may choose. This data-generating process is outlined in\n                      Figure 1: for each coming user, the logging policy chooses which prompt to use for generating the\n                        sentence; then, each user observes only the sentence generated by the chosen prompt and thus reveals\n                       the reward (e.g., click) for only this sentence. A naive way to deal with such counterfactuals is to\n                        regress the reward and use imputed rewards instead (Stiennon et al., 2020; Jaques et al., 2017; Snell\n                           et al., 2022b). However, imputation is often not accurate enough under covariate shift (Swaminathan\n             & Joachims, 2015) and complex relations between prompts and rewards. An alternative is importance\n                      sampling, which re-weighs reward observations w.r.t. the ratio of prompt distribution between the\n                      logging and target policies. Nonetheless, this approach suffers from severe variance when the action\n\n\n                                                           1\n\nPreprint\n\n\n\n\n\nFigure 1: Overview of the prompt-based sentence personalization with logged bandit feedback.\nFor each coming user, a policy chooses which prompt to use to generate sentences with a frozen\nLLM. Each user observes only the sentence generated by the chosen prompt and provides the reward\nfor the corresponding sentence. Logged bandit feedback is partial in that we cannot observe rewards\nfor the sentences generated by prompts not chosen by the logging policy. Examples are generated by\nChatGPT-3.5 (Brown et al., 2020).\n\n\nspace is large (Saito et al., 2024) and bias when the logging policy does not fully explore the action\nspace (Sachdeva et al., 2020). These challenges can be particularly problematic in our language\ngeneration setting, where we need to deal with a rich and diverse set of candidate prompts as actions.\n\nThe key shortcoming of the standard approaches lies in treating each prompt independently, not\ntaking the information about the generated sentence into account. In response, this paper explores\nand presents a method to leverage the similarity among generated sentences to make large-scale\nOPL for prompt-guided language generation efficient and tractable. Specifically, our Direct\nSentence Off-policy gradient (DSO) estimates the policy gradient in the sentence space (i.e., not in the\naction space of prompts) to take the generated sentence into account. We enable this by applying the\nimportance weight in the (marginalized) sentence space and by re-sampling the action conditioned on\nthe sentence when calculating the score function. Because the former effort contributes to reducing\nthe scale of the importance weight and the latter works as an implicit data augmentation about the\nprompt, we can expect variance reduction of DSO compared to typical OPL methods. Moreover, by\naggregating similar prompts and sentences using kernels, DSO also keeps the bias small.\n\nFinally, we develop and conduct experiments on a newly-developed OPL benchmark suite, called\nOfflinePrompts (Kiyohara et al., 2025), in both synthetic and full-LLM settings. The results\ndemonstrate the effectiveness of our approach, particularly when the number of prompts is large,\nshowing a 5x increase in the performance in the full-LLM experiment. We also provide our\nbenchmark suite, including the full-LLM environment for generating personalized movie descriptions\nfor recommendations based on the MovieLens (Harper & Konstan, 2015) dataset, as an open-source\nresource. Its easy-to-use API will accelerate both future research and practical applications of OPL\nof prompt-guided language generation with logged bandit feedback.\n\nOur contributions are summarized as follows.\n\n        • Problem formulation: We present a view of prompt-policy learning from naturally available\n        feedback as OPL of contextual bandits, providing a pathway for advancing OPL for prompts.\n\n        • Tailoring OPL methods for language generation: We identify how to effectively leverage\n         similarity in generated sentences by taking the gradient directly in the sentence space.\n        • Benchmarks and open-source: We conduct extensive experiments and provide a new\n       benchmark suite as open-source software for future research and applications.\n\n2  RELATED WORK\n\nThis section summarizes notable related work. Extended discussion can be found in Appendix A.\n\nPrompt Tuning.  Prompt tuning is a cost-efficient approach for optimizing language generation for\nsome specific downstream applications, including translation, summarization, and sentiment analysis.\n\n\n                                       2\n\nPreprint\n\n\n\n\n\nUnlike fine-tuning, which requires the updates of the millions of parameters of the LLM itself, prompt\ntuning reuses a “frozen” pre-trained LLM and optimizes only the choice of the special tokens added\nto the original input sentence called prompts (Brown et al., 2020). Deng et al. (2022) presented a\nreinforcement learning (RL) formulation of prompt tuning, which optimizes the prompts via policy\ngradient by treating a frozen LLM as a black-box reward generator. While this formulation is relevant\nto ours, the critical limitation of Deng et al. (2022) and similar online exploration papers (Dwaracherla\net al., 2024) is to assume that feedback (i.e., reward) is easily accessible. Unfortunately, such an\nassumption is unrealistic in many real-world applications where online interactions with users can be\ncostly, harmful, or sometimes even unethical (Matsushima et al., 2021; Gilotte et al., 2018). Instead,\nwe present a way to leverage logged user feedback naturally collected through past operations.\n\n\nReinforcement Learning for Language Generation.  RL from Human Feedback (RLHF) is a\nwidely-studied approach to align the output of LLMs using human annotation (Christiano et al., 2017;\nStiennon et al., 2020; Ouyang et al., 2022; Lin et al., 2024). Specifically, RLHF asks human annotators\nto compare two sentences and provide labels to indicate which sentence is more appropriate for a\ndownstream task (e.g., translation). Then, using the pairwise feedback, RLHF trains a model to predict\nthe task-specific score of each sentence to preserve the preference. The key challenges of RLHF are\nin two folds: (1) RLHF incurs substantial cost and ethical concerns for human annotation (Bai et al.,\n2022; Lee et al., 2023) and (2) monitoring if annotators provide sufficiently reliable labels for RLHF,\nas done in Stiennon et al. (2020); Ouyang et al. (2022), can be difficult when preferred sentences\nchange among annotators in tasks related to personalization. Our approach of learning a contextual\nprompt policy using logged bandit feedback naturally resolves the above difficulties of RLHF.\n\n\nOff-Policy Evaluation and Learning.  Off-Policy Evaluation and Learning (OPE/OPL) studies\nhow to use naturally collected user feedback to evaluate and learn new contextual bandit or RL\npolicies (Saito et al., 2021; Fu et al., 2020). Regression-based and importance sampling (IS)-based\napproaches are prevalent in OPL. First, the regression-based approach trains a reward predictor and\nthen optimizes a new policy using imputed rewards. While this approach performs well when the\nreward predictor is accurate for the entire action (i.e., prompt) space, such an accurate regression\nis often demanding due to the issues such as counterfactuals and covariate shift (Swaminathan &\nJoachims, 2015). In contrast, the IS-based approach aims to estimate the policy gradient unbiasedly\nfrom actually observed rewards by correcting the distribution shift (Precup et al., 2000). However,\nIS often suffers from high variance and deficient support, particularly when the action space is\nlarge (Saito & Joachims, 2022; Saito et al., 2023; Sachdeva et al., 2024). To overcome the limitations\nof naive approaches, Saito et al. (2024) has recently proposed a two-stage OPL framework called\nPOTEC, which first chooses which cluster among pre-defined action-clusters to use by applying\ncluster-wise IS and then chooses which action within the chosen cluster to use. However, good\nclusterings are often hard to identify, and this approach discards information about the generated\nsentences. In response, we present a way to leverage similarity among sentences by estimating the\npolicy gradient directly in the sentence space. Another related literature is Kallus & Zhou (2018),\nwhich discuss OPE of deterministic policies in a continuous action space. While we share the ideas\nof using kernels with Kallus & Zhou (2018), our idea comes from a different notion of deriving the\ngradient directly in the (marginalized) sentence space. Moreover, the theoretical analysis is entirely\ndifferent, as we apply kernels to the logging policy, while Kallus & Zhou (2018) do not. Finally, our\nnew benchmark, which simulates the personalized generation of sentences, is a unique contribution\nof ours to the OPE/OPL community.\n\n\n3  PROBLEM FORMULATION\n\nWe start by formulating prompt optimization as a new type of OPL problem, which we call contextual\nbandits with auxiliary outputs.\nLet u ∈U ⊆Rdu be a du-dimensional user feature vector (e.g., demographic profile or user id),\nsampled from an unknown distribution p(u). Let q ∈Q ⊆Rdq be a query (e.g., query to a frozen\nLLM), sampled from a conditional distribution p(q|u). Let a ∈A be a (discrete) prompt, where\neach prompt is associated with some vectorial embedding, ea ∈Rde, where de is the dimension of\nthe embeddings. The prompt is used to generate a sentence via a frozen LLM. This process can be\nformulated as a procedure of sampling sentence s ∈S as an auxiliary output from the stochastic\n\n\n                                       3\n\nPreprint\n\n\n\n\noutput distribution of the LLM: pLLM(s|q, a). A user will respond to the output sentence and provide\nsome reward r ∈R (e.g., click or purchase), where r follows p(r|u, q, s). Let π ∈Π be a prompt\npolicy where π(a|u, q) is the probability of choosing prompt a for context x := (u, q) ∈X. Our goal\nis to optimize the prompt policy to maximize the expected reward, defined as\n         V (π) := Ep(u)p(q|u) π(a|u,q) pLLM(s|q, a)p(r|u, q, s) [r] = Ep(x)π(a|x)p(r,s|x,a)[r].\n                      |=p(u,q){z }       |  =p(r,s|u,q,a){z     }\n\nWhen running a prompt policy π0 (̸= π) as part of an operational system, it works as a logging policy\nand generates logged feedback of the following form:\n                                     n\n         D := {xi, ai, si, ri}ni=1 ∼ Y p(x)π0(a|x)pLLM(s|x, a)p(r|x, s)\n                                         i=1\nwhere n is the data size and i is its index. The logged data informs us whether the prompt (ai) results\nin a high reward or not (ri) for a particular context (xi). However, a difficult aspect of using the\nlogged data is that the reward observation is partial, i.e., it is observed only for the prompt chosen by\nthe logging policy (π0) but not for all the other actions. This can be particularly challenging when\ntraining a new policy π on the logged data, as π may choose actions that are not chosen by π0. Thus,\nwe need to address such counterfactuals and distribution shift between the logging and learning\npolicies when using logged data for a reliable policy optimization (Swaminathan & Joachims, 2015).\n\nIn the rest of the paper, we parameterize the policy as πθ using some parameters θ ∈Θ (e.g., a neural\nnetwork). We also define q(x, a) := E[r|x, a] and q(x, s) := E[r|x, s]. Finally, z ∼p(z) indicates\nthat we sample a single random variable z from the probability distribution p(·), for any random\nvariable z and its corresponding probability distribution.\n\n3.1  CONVENTIONAL APPROACHES\n\nWe first review direct applications of typical OPL methods and discuss their limitations.\n\nRegression (Konda & Tsitsiklis, 1999). A typical way of using logged data is to train a reward\npredictor ˆq (Stiennon et al., 2020; Jaques et al., 2017; Snell et al., 2022b), and then use the predicted\nreward to estimate the policy gradient (PG)1.\n                              n\n             ∇θV (πθ) ≈1          X Ea∼πθ(a|xi) [∇θ log πθ(a|xi)ˆq(xi, a)] .\n                       n\n                                  i=1\nOftentimes, an accurate regression for OPL is difficult to obtain when the relation between prompts\nand reward is complex. This is because the reward observation is partial and covariate shift arises\nbetween the logging policy (π0) and the target policy (πθ).  If the learned regression model ˆq is\ninaccurate, the estimated PG can be heavily biased (Swaminathan & Joachims, 2015).\n\nImportance sampling (IS) (Swaminathan & Joachims, 2015).  Instead of using potentially inaccu-\nrate regression, IS corrects the distribution shift between π0 and πθ by reweighing the observations:\n                                  n\n                                          πθ(ai|xi)                ∇θV (πθ) ≈1           X             log πθ(ai|xi)ri.                          n    π0(ai|xi)∇θ                                      i=1\nIS is unbiased under the action support condition, i.e., ∀(x, a) ∈X × A, πθ(a|x) > 0 =⇒\nπ0(a|x) > 0. However, IS produces considerable bias due to the violation of the condition (deficient\nsupport) (Sachdeva et al., 2020) and extremely high variance due to large importance weight (Saito\net al., 2023; 2024; Sachdeva et al., 2024), which are likely when the action space is large. The key\nshortcoming here is that the typical methods treat each prompt independently and discard the rich\ninformation about the generated sentence when estimating the policy gradient.\n\n4  PROPOSAL: DIRECT SENTENCE OFF-POLICY GRADIENT (DSO)\n\nThe key idea is to make the most of the information about the generated sentence by taking the\npolicy gradient directly in the sentence space as follows.\n                ∇θV (πθ) = Ep(x)πθ(s|x)[∇θ log πθ(s|x)q(x, s)].\n   1The estimation target is the true PG defined as ∇θV (πθ) = Ep(x)πθ(a|x)p(r|x,a)[∇θ log πθ(a|x)r].\n\n\n                                       4\n\nPreprint\n\n\n\n\n\nFigure 2: Examples of the kernel weights and (soft) rejection sampling in the marginalized\nsentence space. DSO implicitly augments the data to take the observations for the neighboring\nsentences into account.  (Left) uses a smooth kernel like a Gaussian kernel, and (Right) uses a\npiecewise constant kernel like a uniform kernel.\n\n\n\nEven when we parameterize the policy in the prompt space, this is conceptually possible because we\ncan write the sentence distribution and the score function as πθ(s|x) = Pa∈A pLLM(s|x, a)πθ(a|x)\nand ∇θ log πθ(s|x) = Eπθ(a|x,s)[∇θ log πθ(a|x)], respectively (See Appendix D.1 for the derivation).\nHowever, one potential concern of this approach is that we may suffer from data sparsity when\nestimating the gradient for each sentence s, as sentences are high-dimensional. Thus, we further\nconsider taking the gradient in the marginalized sentence space to enable data-efficient OPE as\n\n            ∇θV (πθ) = Ep(x)πθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)qπθ(x, ϕ(s))],\n\nwhere ϕ(s) ∈Φ(S) is the kernel-based neighbors of sentence s.  Its probability density, policy\ndistribution, and expected reward are defined as follows.\n\n        • P(ϕ(s)|·) := R s′∈S K(s′, s; x, τ)P(s′|·)ds′, ∀P.   (marginal density)\n\n        • π(ϕ(s)|x) := Pa∈A pLLM(ϕ(s)|x, a)π(a|x), ∀π.   (policy marginal distribution)\n\n                                   K(s,s′; x,τ)π(s′|x)\n        • qπ(x, ϕ(s)) := R s′∈S      π(ϕ(s)|x)    q(x, s′)ds′, ∀π.   (expected reward)\n\nK(·) is a kernel function, which must satisfy R s′∈S K(s′, s; x, τ) = 1, and τ is a bandwidth hyperpa-\nrameter that controls the magnitude of marginalization. The intuition behind DSO is to implicitly\naugment the data by taking the observations for the neighboring sentences into account, as illustrated\nin Figure 2. Specifically, when using a smooth kernel like a Gaussian kernel, neighboring sentences\nare weighted proportional to K(s′, s; x, τ) ∝exp(−d(s, s′)), where d(s, s′) is the distance between\ntwo sentences (e.g., sentence embedding distance). In contrast, when using a piecewise constant\nkernel like a uniform kernel, all the sentences within a certain threshold is equally weighted, while all\nthe others are rejected with the weight of 0.\n\nTo estimate the policy gradient in the marginalized sentence space induced by kernels, Direct\nSentence Off-policy Gradient (DSO) applies IS as follows.\n\n\n                               n\n                                      πθ(ϕ(si)|xi)              ∇θV (πθ) ≈1          X         ∇θ log πθ(ϕ(si)|xi) ri.\n                       n     π0(ϕ(si)|xi)                                  i=1\n                                    |:=w(ϕ(si),xi){z    }\n\nBy applying IS on the marginalized sentence space (Φ(S)), DSO avoids large importance weights,\nmaking large-scale OPL more scalable regarding the number of candidate prompts, while keeping the\nbias small by leveraging the similarity among sentences. Moreover, even though we observe only a\nsingle prompt in the original logged data, DSO can further distribute the reward observation among\nmultiple prompts that generate similar sentences. This implicit data augmentation among multiple\ncounterfactual prompts also contributes to reducing variance. While the precise computation of the\nmarginal importance weight (w(ϕ(s), x)) and the score function (∇θπθ(ϕ(s)|x)) seems non-trivial,\nbelow we present how to train a model to estimate these distributions in a tractable way.\n\n\n                                       5\n\nPreprint\n\n\n\n\n\n4.1  ESTIMATION OF THE WEIGHTED SCORE FUNCTION\n\nThe key trick of DSO is to use the following expression of the weighted score function:\n                                                  K(s, s′; x, τ)∇θ log πθ(a|x)\n    w(ϕ(s), x)∇θ log πθ(ϕ(s)|x) = E(a,s′)∼πθ(a|x)pLLM(s′|x,a)                                               .\n                                                                 π0(ϕ(s)|x)\n\nWe provide the derivation in Appendix D.2. This expression indicates that DSO can be seen as\nperforming soft rejection sampling on the data (a, s′) augmented by πθ, while correcting the bias\nin the logged data by applying the inverse propensity of π0 in the marginalized sentence space.\nThe above equation also suggests that our estimation problem of the weighted score function is\nreduced to only the estimation of π0(ϕ(s)|x). This is useful, as π0(ϕ(s)|x) does not depend on\nthe parameterized policy (πθ), and it thus suffices to fit a marginal density model only once before\nrunning the policy gradient method. Because the marginal distribution is defined as π0(ϕ(s)|x) =\nEπ0(s′|x)[K(s, s′; x, τ)], we can estimate the marginal density via the monte-carlo sampling as\n\n                        m\n                     π0(ϕ(si)|xi) ≈1            X Esj∼π0(sj|x)[K(si, sj; xi, τ)],\n                  m\n                                     j=1\n\nwhere m is the number of the monte-carlo samples. Similarly, we can also estimate the marginal\ndensity with function approximation (fψ(x, s) ≈π0(ϕ(s)|x)) using the following loss:\n\n                      n\n             ℓ(fψ) ≈1       X E(s,s′)∼π0(s|xi)π0(s′|xi)[(fψ(xi, s) −K(s, s′; xi, τ))2].\n                n\n                        i=1\n\nSince the computation of this loss does not scale with the size of the action space |A|, we can easily\napply DSO even when the action (i.e., prompt) space is large.\n\n\n4.2  THEORETICAL ANALYSIS\n\nHere, we analyze the bias and variance of the DSO estimator (the proofs are in Appendix D). We first\nintroduce a new condition about support in the marginalized sentence space.\nDefinition 1. (Similar sentence support) Similar sentence support is satisfied when πθ(ϕ(s)|x) >\n0 =⇒π0(ϕ(s)|x) > 0 holds for all (x, ϕ(s)) ∈X × Φ(S).\n\nThe similar sentence support condition relaxes the action support condition of IS. That is, because\nwe have π(ϕ(s)|x) = Pa∈A pLLM(ϕ(s)|a, x)π(a|x) by definition, the similar sentence support\ncondition is always satisfied when the action support condition is satisfied. This means that deficient\nsupport under the similar sentence support is more unlikely happening compared to the action support.\nUnder this condition, we have the following degree of bias.\n\n\n   Theorem 1. (Bias of DSO) When the similar sentence support is satisfied, the bias is\n\n   \\\n         Bias( (∇θV )DSO) = Eπθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)∆q(πθ, π0; x, ϕ(s))]\n                + Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[∆(w∇θ)(ϕ(s′), ϕ(s); x)q(x, s′)]\n                + Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∆(∇θ)(ϕ(s), s′; x)q(x, s′)].\n   where  ∆q(πθ, π0; x, ϕ(s))  is  the  difference  of  ˆqπ(x, ϕ(s))  between  πθ  and  π0.\n   ∆(w∇θ)(ϕ(s′), ϕ(s); x) is the difference of weighted score function between ϕ(s′) and ϕ(s).\n   ∆(∇θ)(ϕ(s), s′; x) is the difference between the score function of ϕ(s) and s′, which is\n    equivalent to the difference of Eπθ(a|x,ϕ(s))[∇θ log πθ(a|x)] and Eπθ(a|x,s′)[∇θ log πθ(a|x)].\n\n\nTheorem 1 suggests that the bias of DSO comes from three factors. The first term is the dominant term,\nwhich arises from the within-neighbor reward shift (i.e., the difference between qπ0(x, ϕ(s)) and\nqπθ(x, ϕ(s))), as illustrated in Figure 3. This term becomes small in two cases: (i) when reward does\nnot change too much within ϕ(s), and (ii) when the within-cluster distribution shift of π(s′|x, ϕ(s))\nis small. Either case is satisfied when the radius of neighbors (i.e., kernel bandwidth hyperparameter\nτ) is small. At the same time, smooth kernels like a Gaussian kernel are also useful, as they allocates\n\n\n                                       6\n\nPreprint\n\n\n\n\n\nFigure 3: Bias-variance tradeoff of DSO and its relations to the bandwidth hyperparameter (τ)\nof a kernel function: When τ is large, the overlap between the logging policy (π0) and the current\npolicy (πθ) within ϕ(s) becomes large, thus the scale of the importance weight becomes small. This\ncontributes to reducing the variance compared to naive IS. In contrast, a small value of τ helps keep\nthe bias small, as the within-neighbor reward shift (i.e., the difference between qπ0(x, ϕ(s)) and\nqπθ(x, ϕ(s))) becomes small. The gray regions are rejected when using a uniform kernel.\n\n\nlarger weights to similar sentences depending on the distance from the pivotal sentence. In contrast,\nthe second and third terms are caused by calculating the gradient in the marginalized sentence space\n(Φ(S)) instead of the original sentence space (S). These terms also become small when the bandwidth\nhyperparameter τ is small. Thus, a small value of τ is preferable in reducing the bias.\n\nNext, we have the following degree of variance using DSO.\n\n\n   Theorem 2. (Variance of DSO) When the similar sentence support is satisfied, the conditional\n    variance is expressed as\n      nVD|x((∇θV\\)DSO) = Vπ0(s|x)(w(ϕ(s), x)∇θ log πθ(ϕ(s)|x)q(x, s))\n                + Ep(x)π0(s|x)[(w(ϕ(s), x))2(∇θ log πθ(ϕ(s)|x))2σ2(x, s)].\n\n   Compared to the naive (action) IS, the importance weight and the gradient reduce the vari-\n   ance by Eπ0(ϕ(s)|x)[Vπ0(a|x,ϕ(s))(w(a, x))] and Eπ0(ϕ(s)|x)[Vπ0(a|x,ϕ(s))(∇θ log πθ(a|x))],\n    respectively, where w(x, a) is the action importance weight.\n\nTheorem 2 suggests that DSO gains variance reduction from two sources: ∇θ log πθ(ϕ(s)|x) and\nw(ϕ(s)|x). The first variance reduction of ∇θ log πθ(ϕ(s)|x) comes from the fact that the sentence-\nbased score function is expressed as Eπθ(a|x,ϕ(s))[∇θ log πθ(a|x)], demonstrating the benefit of\napplying the implicit data augmentation and soft rejection sampling (instead of applying hard rejection\nsampling). The variance reduction becomes especially large when multiple different prompts result in\nsimilar sentences; thus, πθ(a|x, ϕ(s)) becomes adequately stochastic. Moreover, by using w(ϕ(s), x)\ninstead of w(a, x), we can expect a significant variance reduction as we avoid the variance caused\nby the within-neighbor importance weight, i.e., w(a, x; ϕ(s)) := πθ(a|x, ϕ(s))/π0(a|x, ϕ(s)). This\nmeans that a larger value of τ (i.e., the radius of neighbors) leads to a larger variance reduction.\nTogether with the analysis of bias, we can see that the value of τ plays an important role in trading\noff the bias and variance of DSO, as shown in Figure 3. Later in the experiment section, we study\nhow the performance changes with varying values of the bandwidth hyperparameter τ.\n\n5  BENCHMARKS AND OPEN-SOURCE SOFTWARE\n\nDue to the lack of existing benchmark suites for OPL of prompt policies, we implemented and will\nrelease open-source software called OfflinePrompts. This benchmark suite come with two settings:\nsynthetic and full-LLM to enable extensive and reproducible experiments. In particular, the full-LLM\nbenchmark simulates movie recommendation tasks with personalized sentence descriptions based on\nthe (sentence-augmented) MovieLens dataset (Harper & Konstan, 2015) (See Appendix B for the\ndetails). Moreover, OfflinePrompts also enables prompt tuning on users’ own logged data, facilitating\nthe practical application of OPL. Appendix A summarizes related benchmarks and the distinctive\nfeatures of our software. Appendix F also demonstrates the easy-to-use APIs of OfflinePrompts.\n\n\n                                       7\n\nPreprint\n\n\n\n\n6  SYNTHETIC EXPERIMENTS\n\nWe first evaluate the proposed DSO approach on synthetic benchmarks in OfflinePrompts, since they\nallow us to explore a wide range of conditions.2\n\n6.1  EXPERIMENT SETTING\n\nTo generate candidate actions, we first sample 5-dimensional embedding ea, from a normal distribu-\ntion. Each embedding ea is a deterministic embedding associated with an action a. Then, to generate\nlogged data, we sample 5-dimensional user and query vectors from a multivariate normal distribution.\nNext, for each query-action pair (q, a), we sample 5-dimensional sentence embeddings s as\n                s ∼N(fs(q, ea), σ2s),   fs(q, ea) = c · sine(q⊤Mq + e⊤a Me),\nwhere Mq and Me are coefficient matrices sampled from a uniform distribution. c = 5.0 is a scaling\nfactor and σs = 1.0 is the noise level of the action-output mapping. By using the sine function,\nwe simulate a situation where two different prompts (ea) can result in a similar sentence (s), while\npreserving the smoothness between the prompt and sentence embedding spaces. Then, a user responds\nto the generated sentence (s) with the following reward function:\n                 r ∼N(fr(x, s), σ2r),   fr(x, s) = (u⊤Mu + q⊤Mq)Mss⊤,\nwhere Mu, Mq, and Ms are the coefficient matrices and σr is the reward noise.\n\nWe  generate  logged  data  with  the  following  softmax  logging  policy:   π0(a|x)   :=\nexp(β0 ˆR0(x, a))/(Pa∈A exp(β0 ˆR0(x, a))). ˆR0 is the base reward model, trained on n0 = 10000\nof data points collected by the uniform random policy. β0 = 1.0 is the inverse temperature.\n\nWe compare DSO to four baselines: regression, IS, DR, and POTEC. DR (Dudík et al., 2011)\ncombines IS and regression efficiently. POTEC (Saito et al., 2024) employs a two-stage policy\nlearning, which first chooses which cluster to use via DR and then chooses which action within\nthe cluster to use via regression. All the baselines estimate the gradient in the action space. For\nthe metrics to compare the OPL methods, we use the optimality of the learned policy, defined as\n(V (π) −V (πunif))/V (πopt −V (πunif)), where πopt is the optimal policy and πunif is the uniform\nrandom policy. The definitions of DR and POTEC, and the implementation details are in Appendix C.\n\nThe experiment varies the following configurations (the bold font represents the default value):\n(1) data size: n ∈{500, 1000, 2000, 4000, 8000}, (2) number of candidate actions: |A| ∈\n{10, 50, 100, 500, 1000}, and (3) reward noises:  σr ∈{0.0, 1.0, 2.0, 3.0}.  For the ablation\nof DSO, we additionally report the results with the varying bandwidth hyperparameters of\nτ ∈{0.5, 1.0, 2.0, 4.0}, {w/ and w/o} function approximation of the marginal density, and two\ndifferent kernels, {Gaussian and uniform}. When not using function approximation, we estimate\nthe marginal density via monte-carlo sampling with m = 100 samples. Finally, to evaluate the\nrobustness of DSO to the accuracy of the distance measure in the kernel, we add noise sampled from a\nnormal distribution with std ∆s = 1.0 to the sentence embeddings. We report the mean and standard\ndeviation of the performance based on the results with 20 random seeds.\n\n6.2  RESULT\n\nFigure 4 compares the policy learning results of the OPL methods with varying data sizes (n),\nnumber of candidate actions (|A|), and reward noises (σr), respectively. The results demonstrate that\nDSO works particularly well in challenging scenarios where the baselines fall short due to variance.\nSpecifically, while we observe a sharp drop of performance for the baselines when the action space is\nlarge (|A| ≥500) and reward noise is large (σr ≥1.0), DSO maintains a favorable performance even\nunder these configurations. Moreover, comparing the performance with |A| = 1000 and σr = 1.0,\nwe observe that the performance of DSO at n = 500 outperforms that of the baselines at n = 8000.\nThis indicates that DSO is far more data-efficient than the baselines when the action space is large,\nleveraging the similarity among sentences via kernels and performing implicit data augmentation.\n\nNext, we study how the choice of kernels affects the performance of DSO, as shown in Figure 5. The\nresults tell us several interesting findings: using (1) a Gaussian kernel and (2) function approximation\n\n   2Our code will be available at a GitHub repository upon publication.\n\n\n                                       8\n\nPreprint\n\n\n\n\n\nFigure 4: Comparing the performance of the policies learned by various OPL methods with (Left)\nvarying data sizes (n), (Middle) varying number of candidate actions (|A|), and (Right) varying\nreward noises (σr). DSO uses a Gaussian kernel and function approximation of π0(ϕ(s)|x).\n\n\n\n\n\nFigure 5: Ablation results of DSO with varying bandwidth hyperparameters (τ), w/ and w/o\nfunction approximation of π0(ϕ(s)|x), and two kernels, Gaussian and uniform.\n\n\n\nimprove the robustness of DSO to the choice of bandwidth hyperparameter τ. The first observation is\nevident from the fact that a Gaussian kernel allocates larger weights to closer sentences compared to\na uniform kernel. However, when using monte-carlo estimation, we observe that even a Gaussian\nkernel needs careful tuning of τ, where a small value of τ incurs high variance and a large value\nof τ produces non-negligible bias. In contrast, by using function approximation, we can avoid a\nsmall value of ˆπ0(ϕ(s)|x), which contributes to the variance reduction3. Therefore, using function\napproximation helps improve the robustness to a small value of τ, and we do not need extensive\nhyperparameter tuning of τ. This implies that DSO is applicable to practical situations, where a\npre-trained model of ˆπ0(ϕ(s)|x) can provide substantial efficiency gains.\n\n7  FULL-LLM EXPERIMENT WITH MOVIELENS\n\nThis section compares OPL methods in a personalized generation task of movie descriptions using\nthe MovieLens-10M (Harper & Konstan, 2015) dataset. The MovieLens dataset contains 10M ratings\nbetween 71,567 users and 10,681 movies. To use this data in our personalized sentence generation\ntask, we first augment the data by generating a (general) movie description using Mistral-7B (Jiang\net al., 2023). Then, we train a sentence-based reward simulator on the augmented dataset using\nDistilBert (Sanh et al., 2019). After obtaining a reward simulator, we collect the logged data in the\nfollowing procedure. First, we randomly sample a user (u) and a movie (query) (q) as a context (x).\nNext, a logging policy (π0) chooses which prompt (a) to use in the sentence generation task. Then, a\nfrozen LLM generates sentence s, taking the prompt a and query q as the input. Finally, we generate\na reward (r) using the reward simulator. Appendix B.2 and Figure 10 describe the workflow of OPL\nand that of pre-training a reward simulator in detail.\n\nIn the full-LLM experiment, we define the logging policy by applying the softmax function on top of\nthe logits learned by the online policy, where we set the inverse temperature hyperparameter to be\n\n   3This is because, for example, when the true marginal density is 1e-5, estimating it as 1e-5 and 1e-4 does\nnot change the MSE loss too much. However, in terms of variance, 1e-4 and 1e-5 make a significant difference.\nUsing function approximation, we can avoid being too precise about small values of the marginal density.\n\n\n                                       9\n\nPreprint\n\n\n\n\n\nFigure 6: Performance comparison of OPL methods in the full-LLM experiment.: The policy\nvalue indicates how much improvement of reward we have by using a (learned) prompt policy\ncompared to the sentence generation without prompts (called no-prompt baseline). From the top, the\nhorizontal lines refer to the value of the online policy, logging policy, and no-prompt baseline.\n\n\n\nβ0 = 0.2. The candidate prompts (A) are retrieved from relatedwords.io with keywords {\"movie\",\n\"genre\", \"culture\"}, where |A| = 1000. The reward is defined as 10 × (q(x, s(a)) −q(x, s(∅))),\nwhere q(·) is the [0, 1]-score simulated by the aforementioned DistilBert sentence discriminator\nand s(∅) is the sentence generated without adding prompts. The data size is n = 50000 and the\nreward noise is σr = 0.05. For DSO, we use a Gaussian kernel with τ = 1.0 to estimate the logging\nmarginal density. The distance between two sentences are measured by the sentence embeddings\nobtained from the frozen Mistral-7B model (see Appendix C for the details). We report the results\nwith a total of 25 trials, on 5 different datasets and 5 runs on each data with different random seeds.\n\n\nResult  Figure 6 compares the performance of the OPL methods by the degree of improvement that\nthe learned policy observed over the sentences generated without prompts (which we call no-prompt\nbaseline). The results indicate that DSO often improves the effectiveness of the sentences more than\nother OPL methods, by effectively leveraging the information about similar sentences. Specifically,\nDSO is more resilient to performance corruption than IS by substantially reducing the variance\nand than regression by reducing the bias. As a result, we have 5x increase of the performance\nthan other baselines on average. It should also be worth noting that this result is observed for the\noff-the-shelf embeddings of sentences, which do not require extensive tuning of the embedding model.\nThis minimizes the difficulty in applying the proposed OPL method in practice. However, learning\n(application-specific) embeddings that further improve the performance of DSO is an interesting\ndirection for future work.\n\n\n8  CONCLUSION AND FUTURE WORK\n\n\nThis paper studied how to use naturally logged user feedback to optimize a prompt policy for language\ngeneration. We started by formulating the problem as OPL of contextual bandits with auxiliary outputs.\nThen, we pointed out the limitations of the naive approaches – (1) existing OPL methods often suffer\nfrom the large action space of prompts and (2) even though we observe generated sentences, existing\nmethods do not use the information about these sentences. To overcome these shortfalls, we proposed\nDirect Setence Off-policy gradient (DSO), which applies importance sampling taking the similarity of\nsentences into account. We also show the effectiveness of the proposed approach in both theoretical\nand empirical ways. Furthermore, our benchmarks suite called OfflinePrompts, provided as open-\nsource software, accelerates future research and practical application of prompt-guided language\ngeneration from logged bandit feedback.\n\nAs a remark, deriving a DR-style variant, which introduces a control variate for further variance\nreduction (Dudík et al., 2011), is non-trivial for DSO, as we discuss in detail in Appendix E.\nStudying a way to efficiently combine IS and regression in the DSO framework would be a promising\nfuture direction. Additionally, a good representation or distance measure of sentences that further\nimproves the performance of DSO would also be worth exploring. Finally, applying a similar idea to\nother generative AI applications, such as text-to-image diffusion models (Saharia et al., 2022), and\nextending the benchmark by publishing relevant real-world data can be interesting future works.\n\n\n                                       10\n\nPreprint\n\n\n\n\n\nACKNOWLEDGMENTS\n\nThis research was supported in part by NSF Awards IIS-2312865 and OAC-2311521. Haruka\nKiyohara and Yuta Saito are supported by Funai Overseas Scholarship.\n\nAdditionally, we thank Kiante Brantley and Aaron Tucker for the advices on the usages of LLMs\nand large computational resources, and Zhaolin Gao for the discussion on the sentence-based reward\nsimulator in the full-LLM benchmark. We also thank the anonymous reviewers for the valuable\ndiscussion to improve the manuscripts.\n\nREFERENCES\n\nShipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In\n  Proceedings of the 30th International Conference on Machine Learning, pp. 127–135, 2013.\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\n  Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\n  from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n  Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\n  Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey\n  Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\n  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\n  and Dario Amodei. Language models are few-shot learners. Advances in Neural Information\n  Processing Systems, 33:1877–1901, 2020.\n\nSayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In Proceedings of\n   the 34th International Conference on Machine Learning, pp. 844–853, 2017.\n\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\n  reinforcement learning from human preferences. Advances in Neural Information Processing\n  Systems, 30, 2017.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song,\n  Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement\n   learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\n  Processing, pp. 3369–3391, 2022.\n\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong Sun.\n  Openprompt: An open-source framework for prompt-learning. In Proceedings of the 60th Annual\n  Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 105–113,\n  2022.\n\nMiroslav Dudík, John Langford, and Lihong Li. Doubly robust policy evaluation and learning.\n  In Proceedings of the 28th International Conference on International Conference on Machine\n  Learning, pp. 1097–1104, 2011.\n\nVikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, and Benjamin Van Roy. Efficient\n  exploration for llms. In Proceedings of the 41th International Conference on Machine Learning,\n  volume 235, pp. 12215–12227, 2024.\n\nJustin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Alexander Novikov, Mengjiao Yang,\n  Michael R Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, et al. Benchmarks for deep\n   off-policy evaluation. In International Conference on Learning Representations, 2020.\n\nYunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang.  Chat-\n   rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint\n  arXiv:2303.14524, 2023.\n\nAlexandre Gilotte, Clément Calauzènes, Thomas Nedelec, Alexandre Abraham, and Simon Dollé.\n   Offline a/b testing for recommender systems.  In Proceedings of the 11th ACM International\n  Conference on Web Search and Data Mining, pp. 198–206, 2018.\n\n\n                                       11\n\nPreprint\n\n\n\n\n\nF Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm\n  transactions on interactive intelligent systems (tiis), 5(4):1–19, 2015.\n\nXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.  Neural\n  collaborative filtering. In Proceedings of the 26th International Conference on World Wide Web,\n  pp. 173–182, 2017.\n\nNatasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E Turner,\n  and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with\n   kl-control. In International Conference on Machine Learning, pp. 1645–1654. PMLR, 2017.\n\nNatasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah\n  Jones, Shixiang Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement\n   learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\n  Processing, pp. 3985–4003, 2020.\n\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\n  Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\n  Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, LavrilThibaut  , Thomas\n  Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825,\n  2023.\n\nNathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments. In\n  International Conference on Artificial Intelligence and Statistics, pp. 1243–1251, 2018.\n\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n  2014.\n\nHaruka Kiyohara, Ren Kishimoto, Kosuke Kawakami, Ken Kobayashi, Kazuhide Nakata, and Yuta\n   Saito. Scope-rl: A python library for offline reinforcement learning and off-policy evaluation.\n  arXiv preprint arXiv:2311.18206, 2023a.\n\nHaruka Kiyohara, Ren Kishimoto, Kosuke Kawakami, Ken Kobayashi, Kazuhide Nakata, and Yuta\n   Saito. Towards assessing and benchmarking risk-return tradeoff of off-policy evaluation. arXiv\n  preprint arXiv:2311.18207, 2023b.\n\nHaruka Kiyohara, Daniel Yiming Cao, Yuta Saito, and Thorsten Joachims. Offlineprompts: Bench-\n  mark suites for prompt-guided language personalization. arXiv preprint arXiv:, 2025.\n\nVijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in Neural Information Processing\n  Systems, 12, 1999.\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor\n  Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with\n   ai feedback. arXiv preprint arXiv:2309.00267, 2023.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\n  review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nLihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to\n  personalized news article recommendation. In Proceedings of the 19th international conference on\n  World wide web, pp. 661–670, 2010.\n\nXiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang\n  Low. Prompt optimization with human feedback. arXiv preprint arXiv:2405.17346, 2024.\n\nAndrzej Ma´ckiewicz and Waldemar Ratajczak. Principal components analysis (pca). Computers &\n  Geosciences, 19(3):303–342, 1993.\n\nTatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-\n   efficient reinforcement learning via model-based offline optimization. In International Conference\n  on Learning Representations, 2021.\n\n\n                                       12\n\nPreprint\n\n\n\n\n\nAllen Nie, Yi Su, Bo Chang, Jonathan N Lee, Ed H Chi, Quoc V Le, and Minmin Chen. Evolve:\n  Evaluating and optimizing llms for exploration. arXiv preprint arXiv:2410.06238, 2024.\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\n  Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\n   instructions with human feedback. Advances in Neural Information Processing Systems, 35:\n  27730–27744, 2022.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\n  Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.  Pytorch: An imperative style,\n  high-performance deep learning library. Advances in neural information processing systems, 32,\n  2019.\n\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier\n   Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,\n  Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay.\n   Scikit-learn: Machine learning in python. the Journal of Machine Learning Research, 12:2825–\n  2830, 2011.\n\nDoina Precup, Richard S. Sutton, and Satinder P. Singh.  Eligibility traces for off-policy policy\n  evaluation.  In Proceedings of the 17th International Conference on Machine Learning, pp.\n  759–766, 2000.\n\nRajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian\n  Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.  Is reinforcement learning (not) for natural\n  language processing: Benchmarks, baselines, and building blocks for natural language policy\n  optimization. In The 11th International Conference on Learning Representations, 2022.\n\nPaat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of\n  Operations Research, 35(2):395–411, 2010.\n\nNoveen Sachdeva, Yi Su, and Thorsten Joachims. Off-policy bandits with deficient support. In\n  Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data\n  Mining, pp. 965–975, 2020.\n\nNoveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, and Julian McAuley. Off-policy\n  evaluation for large action spaces via policy convolution. In Proceedings of the ACM on Web\n  Conference 2024, pp. 3576–3585, 2024.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\n  Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\n  text-to-image diffusion models with deep language understanding. Advances in neural information\n  processing systems, 35:36479–36494, 2022.\n\nYuta Saito and Thorsten Joachims. Off-policy evaluation for large action spaces via embeddings.\n  In Proceedings of the 39th International; Conference of Machine Learning, volume 162, pp.\n  19089–19122, 2022.\n\nYuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. Open bandit dataset and\n   pipeline: Towards realistic and reproducible off-policy evaluation. In 35th Conference on Neural\n  Information Processing Systems Datasets and Benchmarks Track, 2021.\n\nYuta Saito, Qingyang Ren, and Thorsten Joachims. Off-policy evaluation for large action spaces\n  via conjunct effect modeling. In Proceedings of the 40th International; Conference of Machine\n  Learning, volume 202, pp. 29734–29759, 2023.\n\nYuta Saito, Jihan Yao, and Thorsten Joachims. Potec: Off-policy learning for large action spaces via\n  two-stage policy decomposition. arXiv preprint arXiv:2402.06151, 2024.\n\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of\n   bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\n\n                                       13\n\nPreprint\n\n\n\n\n\nCharlie Snell, Sherry Yang, Justin Fu, Yi Su, and Sergey Levine. Context-aware language modeling\n   for goal-oriented dialogue systems. In Findings of the Association for Computational Linguistics,\n  pp. 2351–2366, 2022a.\n\nCharlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. Offline rl for natural\n  language generation with implicit language q learning. In The 11th International Conference on\n  Learning Representations, 2022b.\n\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\n  Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\n  Neural Information Processing Systems, 33:3008–3021, 2020.\n\nAdith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback through\n  counterfactual risk minimization. The Journal of Machine Learning Research, 16(1):1731–1755,\n  2015.\n\nMichal Valko, Nathan Korda, Rémi Munos, Ilias Flaounas, and Nello Cristianini.  Finite-time\n  analysis of kernelised contextual bandits. In Proceedings of the Twenty-Ninth29th Conference on\n  Uncertainty in Artificial Intelligence, pp. 654–663, 2013.\n\nSiddharth Verma, Justin Fu, Sherry Yang, and Sergey Levine. Chai: A chatbot ai for task-oriented\n  dialogue with offline reinforcement learning.  In Proceedings of the 2022 Conference of the\n  North American Chapter of the Association for Computational Linguistics: Human Language\n  Technologies, pp. 4471–4491, 2022.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\n  Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\n  Neural Information Processing Systems, 35:24824–24837, 2022.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\n   Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von\n   Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\n  Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art\n   natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n\nZhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu Qiao, and Zhiyong Wu.\n  Openicl: An open-source framework for in-context learning. arXiv preprint arXiv:2303.02913,\n  2023.\n\nTianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning\n  Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild.\n  arXiv preprint arXiv:2310.10634, 2023.\n\nFan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin Lu. Palr: Personal-\n   ization aware llms for recommendation. arXiv preprint arXiv:2305.07622, 2023.\n\nDongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.\n  In Proceedings of the 37th International Conference on Machine Learning, pp. 11492–11502,\n  2020.\n\n\n\n\n\n                                       14\n\nPreprint\n\n\n\n\n\n         Figure 7: Off-policy learning (OPL) workflow and OfflinePrompts modules.\n\n\nA  EXTENDED RELATED WORK\n\n(Online) linear and kernelized contextual bandits.  Linear bandits (Li et al., 2010; Agrawal &\nGoyal, 2013; Rusmevichientong & Tsitsiklis, 2010) and kernelized contextual bandits (Chowdhury &\nGopalan, 2017; Valko et al., 2013; Zhou et al., 2020) is relevant to ours in using the similarity among\nactions and rewards for improving data efficiency. Specifically, linear bandits assume that the reward\nfunction is expressed as an inner-product between (non-linear representations of) context features and\naction-specific coefficients and aims to learn the linear action features (Li et al., 2010). By assuming\nthe linear structure in the reward function, the corresponding bandit algorithm makes the exploration\nmore efficient than treating each action independently. Similarly, kernelized bandits (Valko et al.,\n2013) generalize the idea of leveraging similarity among action representations under less restrictive\nassumptions on the rewards.  Specifically, it assumes that similar features can result in similar\nrewards without assuming a linear structure and implicitly augments the reward observation using the\nReproducing kernel Hilbert space (RKHS). Our paper demonstrates that leveraging the similarity\namong auxiliary outputs (i.e., sentences) of action can improve the data efficiency in the offline\nlearning setting, not only limited to the online exploration discussed in the existing literature.\n\nOffline reinforcement learning (Offline RL) for dialog generation.  Offline RL (Levine et al.,\n2020) has emerged as a new paradigm for fine-tuning language models in dialog systems (Jaques\net al., 2020; Snell et al., 2022a;b; Verma et al., 2022). Among them, Snell et al. (2022a) and Verma\net al. (2022) focus on goal-oriented dialog system, which aims to solve some specific tasks by\ncombining RL-based planning and dialog generation. Jaques et al. (2020) and Snell et al. (2022a) aim\nto improve the quality of conversations by maximizing the users’ sentiment signals observed in text or\ninterface (e.g., thumb up). While these works are relevant to ours in using offline data, our work has\nseveral distinctions over existing works. First, while existing work focuses on RL-based fine-tuning,\nwhich requires expensive computation and is affordable only for the companies releasing pre-trained\nmodels, our work considers prompt tuning. Since prompt tuning is available for some third-party\ncompanies (e.g., advertising agencies) or even individual users that access models through APIs\n(e.g., ChatGPT), a more diverse population can customize language generation with our framework.\nMoreover, while existing work formulates the problem as an RL problem and considers only the\nregression-based approach for policy learning, ours formulates the problem as contextual bandits\nand considers applying IS in the marginalized sentence space. This makes the bias-variance tradeoff\nof policy learning more controllable than existing works. Thus, we can expect an improved policy\nperformance, as we have shown in the experiments.\n\nRelevant open-source softwares and benchmarks.  There are several open-source libraries for\nlanguage generation relevant to ours. First, RL4LMs (Ramamurthy et al., 2022) provides a framework\nfor RL-based fine-tuning of LLMs for optimizing language generation for reward maximization. In\nprompt tuning, OpenPrompt (Ding et al., 2022) works as a testbed for comparing (online) gradient-\nbased prompting strategies with various frozen LLMs. OpenICL (Wu et al., 2023) also benchmarks\n(more sophisticated) prompt conditioning strategies called in-context learning (ICL), such as chain-\nof-thoughts reasoning for solving complex mathematical problems (Wei et al., 2022). Similarly,\nOpenAgents (Xie et al., 2023) provides an interface for generating text for various real-world web\napplications using frozen LLMs, especially for the purpose of providing a platform for online RL-\nbased prompt tuning. However, these platforms are not capable of handling logged bandit feedback,\nand ours are the first to streamline OPL procedures for prompt tuning with naturally collected user\nfeedback data.\n\n\n                                       15\n\nPreprint\n\n\n\n\n\nFigure  8:  Two benchmarks  (synthetic and full-LLM)  of OfflinePrompts with four\nconfigurable  submodules.    Compared  to  the  existing OPE/OPL  frameworks  (Saito\net   al.,  2021;  Kiyohara  et   al.,  2023a;b),  our  benchmark  is  distinctive  in  providing\nAuxiliaryOutputGenerator/FrozenLLM to simulate language generation tasks as contex-\ntual bandits with auxiliary outputs.\n\n\nIn an independent field of benchmark study, OpenBanditPipeline (Saito et al., 2021) and SCOPE-\nRL (Kiyohara et al., 2023a;b) are representative open-source libraries to handle OPE and OPL\nprocedures in contextual bandits and RL. Although these libraries streamline the workflow of using\nlogged data in organized ways, they are not applicable to language generation. Thus, we release a new\nbenchmark suite for OPL of prompt tuning for language generation, putting emphasis on connecting\nOPL modules and language generation modules, while following the basic design principles of\nOpenBanditPipeline (Saito et al., 2021) and SCOPE-RL (Kiyohara et al., 2023a;b).\n\nFinally, there is also a benchmark called BanditBench (Nie et al., 2024) , which simulates the LLM-\nbased item recommendations based on the MovieLens (Harper & Konstan, 2015) dataset. While this\nbenchmark uses the same MovieLens dataset for semi-synthetic simulation, the tasks are different\nfrom each other. Specifically, BanditBench (Nie et al., 2024) aims to use LLMs as recommender\npolicies that choose items (Yang et al., 2023; Gao et al., 2023), while our work focuses on steering\nthe generation of sentence description with prompts given (already chosen) items as a query.\n\n\nB  OFFLINEPROMPTS: OPEN-SOURCE SOFTWARE OF OPL FOR LANGUAGE\n   GENERATION\n\n\nB.1  OVERVIEW AND WORKFLOW\n\nThe primal goals of OfflinePrompts are to (1) provide a standardized benchmark to compare OPL\nmethods and (2) facilitate the smooth implementation of the OPL workflow. For these purposes,\nOfflinePrompts (1) provides two standardized benchmarks and (2) streamlines the implementation\nwith three modules: dataset, OPL, and policy, as shown in Figure 7. All implementations are based\non PyTorch (Paszke et al., 2019). We elaborate on the details of each feature below.\n\n\nDataset module and benchmarks  OfflinePrompts provides two benchmarks including synthetic\nand movie description. First, the synthetic benchmark simulates (general) contextual bandits with\nauxiliary outputs with feature vectors, without involving language generation tasks. In contrast, the\nmovie description task is a full-LLM, semi-synthetic benchmark, which simulates personalized gener-\nation of movie description (i.e., actual language generation) based on the MovieLens datasets (Harper\n& Konstan, 2015). These benchmarks can be used for separate purposes. The synthetic benchmark\nis lighter and more suitable for extensive studies of how the performance of OPL methods changes\nwith various configurations than the actual full-LLM benchmark. In contrast, the movie description\nbenchmark is preferable to see the performance of OPL methods in more realistic settings than the\nsynthetic benchmark. The key remark is that the movie description benchmark is the first benchmark\nfor prompt-guided language generation from logged user feedback.\n\n\n                                       16\n\nPreprint\n\n\n\n\n\nBoth  benchmarks  provide  a  standardized  setting  and  configurable  submodules  to  con-\ntrol  the  data  generation  process.    Specifically,  as  illustrated  in  Figure  8,  each bench-\nmark consists of four submodules: ContextQueryModule, CandidateActionsModule,\nAuxiliaryOutputGenerator/FrozenLLM, and RewardSimulator. Compared to the ex-\nisting OPE/OPL benchmarks (Saito et al., 2021; Kiyohara et al., 2023a;b), our benchmark is distinctive\nin modeling AuxiliaryOutputGenerator/FrozenLLM, which enable us to simulate language\ngeneration tasks as contextual bandits with auxiliary outputs. Moreover, since our FrozenLLM\nand RewardSimulator modules are compatible with HuggingFace (Wolf et al., 2019), users can\neasily employ various language models in the full-LLM experiments. The semi-synthetic/full-LLM\ndataset module can also load custom dataset in a manner similar to the movie description benchmark.\nWe believe this feature of OfflinePrompts also facilitates practical applications of prompt tuning from\nnaturally logged feedback.\n\nOPL and policy modules  Figure 9 summarizes the implementation choices of OPL modules:\n\n\n\n\n\n            Figure 9: Implementation choices of OPL modules of OfflinePrompts.\n\nAs shown above, we implement two regression models (naive and conservative), three model-based\npolicies (softmax, epsilon-greedy, and uniform random), and four policy gradient methods (online,\nnaive, two-stage, DSO), and three gradient types (regression-based, IS-based, and hybrid). Each\ncomponent is independently configurable. Thus, we can easily try any combination of the above\npolicies and OPL methods. Moreover, by following the abstract base implementation provided in\nOfflinePrompts, researchers can test their own policies and policy gradient methods.\n\nExample codes for streamlining OPL workflow and customizing each module are available in\nAppendix F. The documentation of OfflinePrompts, which describes further details of the software, is\nalso available at: (forthcoming).\n\nB.2  TASK DESCRIPTION AND REWARD SIMULATION FOR THE MOVIE DESCRIPTION TASK\n\nWe build a semi-synthetic simulator using the MovieLens dataset (Harper & Konstan, 2015) for the\npersonalized generation task of movie descriptions. The movies consist of (partially observed) 5-star\nratings between users and items and have movie title information as the metadata. To learn a reward\nsimulator, which generates reward depending on the generated movie description, we first augmented\nthe movielens dataset with item description using a frozen LLM as follows.\n\n      1. For each movie, retrieve its title.\n      2. Then, using zero-shot inference of a frozen LLM, we generate the movie description by\n        providing instruction: \"Broadly describe in a sentence the genres of\n      the movie without including the name or any specifics of the\n      movie.  Title:  {title of the movie}, Movie description:   \".\n\nIn our standardized benchmark, we use Mistral (\"mistralai/Mistral-7B-Instruct-v0.2\") (Jiang et al.,\n2023) as the frozen LLM. Once augmenting the dataset with item description, we train a sentence\nencoder-based collaborative filtering model (as shown in Figure 10 (Top)) in the following procedures.\n\n      1. Using movielens dataset without item description, we first train a naive neural collaborative\n          filtering (CF) model (He et al., 2017), which uses user and item id embeddings.\n      2. Initialize the encoder-based CF model, which uses user id embeddings and encoded item\n         description features, with the user id embeddings learned by naive CF model.\n      3. Finetune the encoder-based CF model with the (augmented) movielens dataset with item\n         description.\n\n\n                                       17\n\nPreprint\n\n\n\n\n\nFigure 10: Procedures of reward simulation (Top) and personalized sentence generation (Bot-\ntom). The reward simulator uses a sentence encoder to get item embeddings. In the personalized\nsentence generation task, a policy aims to identify a suitable prompt (e.g., genres of the movie) for\neach user so the generated sentence aligns with the user-dependent preference.\n\n\n\n\n\nFigure 11: The reward simulation results on the MovieLens dataset (Harper & Konstan, 2015).\n(Left) Showing the original reward simulated by the fine-tuned DistilBert model (Sanh et al., 2019)\nfor 2000 samples of the validation data. \"positive\" indicates the data that originally received the\nrating of 5 by users, and \"negative\" indicates the data that received 0-3 ratings. (Right) Showing\nthe normalized reward generated for a single user and two movies with varying prompts. This\ndemonstrates how much reward difference each prompt can make compared to without prompts\n(which we call prompt effect), suggesting that effective prompts are sparse among the candidate set,\nand we have skewed distribution on the prompt effect.\n\n\nThe default reward simulator in our benchmark uses the fine-tuned DistilBert model (Sanh et al.,\n2019) as the item description encoder, and the user and item embeddings are set to be 20 dimensional.\nNote that, before training the models, we preprocess the MovieLens-10M to have binary labels – the\nrating of 5 is positive, and the ratings of 0-3 are negative. Then, we prune the dataset so that the\ndataset has balanced positive and negative labels, and each user and item has at least 10 positive and\nnegative labels. After processing the data, 36,395 users, 4,796 items, and 2,316,912 ratings remained.\nWhen using the fine-tuned model as the reward simulator in our benchmark, we use the following\nnormalized reward: 10 × (q(x, s(a)) −q(x, s(∅))), where q(·) is the original [0, 1]-score simulated\nby the model. s(a) is the sentence generated by the prompt a, and s(∅) is the sentence generated\nwithout any prompt. We report the reward simulation results in Figure 11.\n\nFinally, we simulate the data generation of the movie description task as follows.\n\n      1. Randomly sample user and item id and let the user embedding learned by the naive CF\n         as the user context u. We also let the title of the movie be query q. This is handled by\n      ContextQueryLoader. (x)\n\n      2. (A policy chooses which prompt to use, taking the user and query embeddings as inputs.)\n         (a)\n\n\n                                       18\n\nPreprint\n\n\n\n\n\nFigure 12: Example of sentences generated with varying prompts and their reward simulated\nin the full-LLM benchmark. We use the frozen Mistral-7B (Jiang et al., 2023) model to generate\ndescriptions of the movie Star Trek VI (1991) with three different prompts, { movie, scifi, tragedy\n} and highlight sentences that differ from the baseline generated without prompts. The red font\nindicates the reward simulated by the DistilBert model fine-tuned on the MovieLens dataset. While\nabstract keywords like \"movie\" do not make much difference, more specific keywords like \"scifi\" or\n\"tragedy\" can be impactful in the reward simulation.\n\n\n       3. FrozenLLM takes query and prompt as input in the following instruction: \"Broadly\n      describe in a sentence the genres of the movie without\n      including the name or any specifics of the movie.  Title:\n      {title of the movie}, Keyword:  {prompt} Movie description:\n      \" and generate movie description. (s).\n\n       4. RewardSimulator simulates reward by taking user id, item id, and the generated sen-\n         tence as inputs. User and item ids are used to retrieve user-and item-specific bias terms.\n          (r)\n\nNote that by pre-training the reward simulator with item descriptions, we expect the model to learn\nmatchings between user preferences and movie genres (e.g., user A prefers sci-fimovies). We expect\nthis differentiates the reward among varying prompts in the movie description task – e.g., for sci-fi\nlovers, we should focus on the sci-fiaspects rather than the romance aspects of a movie. The goal\nof OPL task is to identify specific features or keywords that generate suitable sentence for each\nuser from the logged data. For reference, Figure 12 shows the example of sentences generated with\nvarying prompts and their rewards simulated in our benchmark.\n\n\nB.3  DATA GENERATION PROCESS FOR THE SYNTHETIC BENCHMARK\n\nThe synthetic benchmark simulates the contextual bandits with auxiliary output using feature vectors\nwithout involving actual language generation. Specifically, the synthetic benchmark generates the\nlogged data in the following process:\n\n       1. Sample size n of context and query from ContextQueryGenerator. (x)\n\n       2. Sample embeddings (ea) for size |A| of actions to define a candidate set of actions. Then,\n          for each context, sample action from the candidates with some logging policy. (a)\n\n       3. Input both query and the chosen action to AuxiliaryOutputGenerator to generate a\n         feature vector as an auxiliary output. The auxiliary output corresponds to output sentence in\n        language generation tasks. (s)\n\n\n                                       19\n\nPreprint\n\n\n\n\n\n      4. Finally, simulate base reward R(x, s) by inputting context and auxiliary output into\n      RewardSimulator. Then, sample reward from a normal distribution N(R(x, s), σr) for\n         the auxiliary output observed by the logging policy. (r)\n\nBy running a synthetic experiment, we can easily control and study the effect of various relationship\nbetween prompt and sentence (action a, e and auxiliary output s) and that between sentence and\nreward (auxiliary output s and reward r) through varying AuxiliaryOutputGenerator and\nRewardSimulator, respectively. Therefore, we expect our synthetic benchmark to be a easy-to-\nuse testbed for checking the behaviors of OPL methods before working on a more complex, actual\nlanguage generation task.\n\nC  IMPLEMENTATION DETAILS OF EXPERIMENTS\n\nBasically, we follow the default implementation of OfflinePrompts.\n\nC.1  SYNTHETIC EXPERIMENTS\n\nThe policy is parameterized by a two-layer neural network, where the hidden dimension is 100, the\nactivation function is ReLU, and the optimizer is Adam (Kingma, 2014). All single-stage policies\n(which are used in regression-based, IS-based, and DSO) take (x, ea) as inputs and generate logit\nvalues. The probability of each prompt chosen by the policy is calculated by taking the softmax of\nthe logit values. Similarly, the first stage policy of POTEC takes (x, ec) as inputs, where ec is the\ncluster centers of prompts in the action embedding space. For all IS and DR-type methods, we apply\nthe weight clipping with a maximum of 200. To avoid the extensive tuning of learning rates, we\nuse the one that worked well for the online policy gradient in all the compared methods, which is\n5e-4. The regression model (ˆq(x, ea)), used for regression-based, DR, and POTEC, and the logging\nmarginal density model used for DSO are parameterized by a two-layer neural network with a 100\ndimensional hidden state. The regression model is trained on the logged data with the following\nMSE loss: Pni=1(ri −ˆq(xi, ai))2, while the marginal density model is trained by the loss function\ndescribed in Section 4.1. The learning rates of the regression and the marginal density models are\nboth based on the validation loss, and are set to 1e-4. Note that because ϕ(s) ranges within [−3τ, 3τ]\nwith probability more than 99% under the Gaussian kernel, we let ϕ(s) of the uniform kernel to range\n[−3τ, 3τ] to the corresponding value of τ. Finally, the action clustering used by POTEC is based on\nk-means clustering with k = 10, implemented in scikit-learn (Pedregosa et al., 2011).\n\nC.2  FULL-LLM EXPERIMENT\n\nThe implementation of the full-LLM experiment is almost the same as the synthetic experiment. The\nonly difference is that, because (q, a, s) are words or sentences, we applied some encoding to get\nvectorial embeddings of these variables. We learn the embeddings by the following steps. We first\nrandomly sample 1000 movies and sentences from the (augmented) MovieLens dataset and sample\n1000 prompts from the action set. Then, we get the last hidden states of Mistral-7B (Jiang et al.,\n2023) by providing the following instructions:\n\n        • \"Broadly describe in a sentence the genres of the movie\n      without including the name or any specifics of the movie.\n      Title:  { title of the movie }\" for each movie (q),\n        • \"Associate the word - { prompt } - in the context of movie\n      genres\" for each prompt (a),\n        • \" { sentence } \" for each sentence s.\n\nAfter obtaining (high-dimensional) embeddings from Mistral-7B, we fit PCA (Ma´ckiewicz & Rata-\njczak, 1993) to reduce the dimension of embeddings to 20. In contrast, for the user context x, we use\n20-dimensional embeddings of user ids learned by (naive) collaborative filtering, which is different\nfrom that used in the reward simulator. We use the learning rate of 8e-4 with Adagrad for policy\ngradients. The learning rate of the regression and the marginal density models are 1e-4 with Adam.\n\nIt is worth mentioning that because the full-LLM benchmark is challenging due to the sparsity of\neffective prompts as demonstrated in Figure 11, a similar learning instability to the OPL results is\n\n\n                                       20\n\nPreprint\n\n\n\n\n\nalso observed for the online policy (e.g., even the online policy sometimes fall short with a near-zero\npolicy value like 0.01, regardless the choice of the learning rates). Therefore, in the experiment,\nwe picked an online policy that performed well when defining a logging policy, so that meaningful\nreward signals should be included in the logged data.\n\n\n\nC.3  DOUBLY ROBUST (DR) ESTIMATORS\n\n\nHere, we provide the details of the DR estimators used in the experiments.\n\nDoubly Robust (DR) (Dudík et al., 2011). DR is a hybrid approach, which effectively combines the\nregression and IS to exploit the benefits of the two.\n\n\n\n                             n\n                                    πθ(ai|xi)            ∇θV (πθ) ≈1         X             log πθ(ai|xi)(ri −ˆq(xi, ai))                     n    π0(ai|xi)∇θ                                i=1\n                            n\n                         1\n               + X Ea∼πθ(a|xi)[∇θ log πθ(ai|xi)ˆq(xi, a)].\n                     n\n                                i=1\n\n\nBy using the regressed reward as a control variate, DR often reduces the variance of IS, while\nremaining unbiased under the same condition as IS. However, when the regression is inaccurate,\nthe variance reduction is limited and DR often suffers from high variance when the action space is\nlarge (Saito & Joachims, 2022).\n\nPOTEC (Saito et al., 2024). To deal with the variance issue of DR, POTEC considers the clustering\nin the action space and decomposes the policy into two stages as follows.\n\n                            πθ(a|x) = X π1stθ (c|x)π2nd(a|x, c),\n                                       c∈C\n\n\nwhere c indicates the cluster of the action a, which can be learned by applying an off-the-shelf\nclustering method to action embeddings. Using this decomposition, POTEC chooses clusters via a\nDR-style approach as follows, and chooses actions within a cluster via regression.\n\n                         n  π1stθ (c(ai)|xi)        X         ∇θV (πθ) ≈1                                                 log π1stθ (c(ai)|xi)(ri −ˆq(xi, ai))\n                   n                                       π1st0 (c(ai)|xi)∇θ                            i=1\n                         n\n                      1\n             + X Ea∼πθ(a|xi)[∇θ log π1stθ (c(ai)|xi)ˆq(xi, a)],\n                  n\n                            i=1\n\nwhere π1st0 (c(a)|x) = Pa′∈A,c(a′)=c(a) π0(a|x). The second-stage policy greedily chooses action\nas π2nd(a|x, c) = I{ˆq(x, a) = arg maxa′∈A,c(a′)=c(a) ˆq(x, a′)}. By applying IS on the clustered\naction space, POTEC reduces the variance of naive IS. POTEC is also able to convert regression\nto a pair-wise regression within a cluster. However, especially when the relation between actions\nand rewards is complex, a good clustering is often hard to identify, and POTEC cannot take the rich\ninformation about generated sentences into account.\n\n\n\nD  OMITTED PROOFS AND DERIVATIONS\n\n\nThis section provide proofs and derivations ommited in the main text.\n\n\n                                       21\n\nPreprint\n\n\n\n\n\nD.1  DERIVATION OF THE PG IN THE SENTENCE SPACE\n\nWe first derive ∇θ log πθ(s|x) = Ea∼πθ(a|x,s)[∇θ log πθ(a|x)].\n\n                                    ∇θπθ(s|x)\n                ∇θ log πθ(s|x) =\n                                        πθ(s|x)\n                Pa∈A ∇θπθ(a|x)pLLM(s|x, a)\n                     =\n                                                 πθ(s|x)\n                                       ∇θπθ(a|x)\n                     = X           πθ(a|x, s)\n                                            πθ(a|x)\n                                a∈A\n                     = Eπθ(a|x,s)[∇θ log πθ(a|x)]\n\nSimilarly,  we   also  have  ∇θ log πθ(ϕ(s)|x)  =    Eπθ(s′|x,ϕ(s))[∇θ log πθ(s′|x)]  thus\n∇θ log πθ(ϕ(s)|x) = Eπθ(s′|x,ϕ(s))[Eπθ(a|x,s′)[∇θ log πθ(a|x)]] = Eπθ(a|x,ϕ(s))[∇θ log πθ(a|x)].\n\n\nD.2  DERIVATION OF THE WEIGHTED SCORE FUNCTION\n\nWe first show w(ϕ(s), x) = Eπ0(a|x,ϕ(s))[w(a, x)].\n\n               Eπ0(a|x,ϕ(s))[w(a, x)] = X π0(a|x, ϕ(s))πθ(a|x)\n                                                    π0(a|x)\n                              a∈A\n                                                                 πθ(a|x,ϕ(s))πθ(ϕ(s)|x)\n                   = X π0(a|x, ϕ(s))     pLLM(ϕ(s)|x,a)                                                                 π0(a|x,ϕ(s))π0(ϕ(s)|x)\n                              a∈A                     pLLM(ϕ(s)|x,a)\n                                                            ϕ(s)) πθ(ϕ(s)|x)                   = X π0(a|x, ϕ(s))πθ(a|x,\n                                                      π0(a|x, ϕ(s)) π0(ϕ(s)|x)\n                              a∈A\n                   = Eπθ(a|x,ϕ(s))[w(x, ϕ(s))]\n                   = w(ϕ(s), x)\n\n\nNext, using the above expression and that derived in Appendix D.1, we have\n\n   w(ϕ(s)|x)∇θ log πθ(ϕ(s)|x)\n       πθ(ϕ(s)|x)\n   = π0(ϕ(s)|x)Eπθ(s′|x,ϕ(s))[Eπθ(a|x,s′)[∇θ log πθ(a|x)]]\n       πθ(ϕ(s)|x) Z\n   =                   πθ(s′|x, ϕ(s)) X πθ(a|x, s′)∇θ log πθ(a|x)ds′\n       π0(ϕ(s)|x)  s′∈S           a∈A\n       πθ(ϕ(s)|x) Z    pK(ϕ(s)|x, s′)πθ(s′|x)     pLLM(s′|x, a)πθ(a|x)\n   =            X               ∇θ log πθ(a|x)ds′\n       π0(ϕ(s)|x)  s′∈S      πθ(ϕ(s)|x)     a∈A       πθ(s′|x)\n          1    Z\n   =               K(s, s′; x, τ) X pLLM(s′|x, a)πθ(a|x)∇θ log πθ(a|x)ds′\n       π0(ϕ(s)|x)  s′∈S           a∈A\n               Z                            s′; x, τ)   = X πθ(a|x)      pLLM(s′|x, a)K(s,      ∇θ log πθ(a|x)ds′\n                                     π0(ϕ(s)|x)      a∈A          s′∈S\n                      K(s, s′; x, τ)\n   = Eπθ(a|x)pLLM(s′|x,a)          ∇θ log πθ(a|x)\n                          π0(ϕ(s)|x)\n\nwhere pK(ϕ(s)|x, s′) = K(s, s′; x, τ).\n\n\n                                       22\n\nPreprint\n\n\n\n\n\nD.3  DERIVATION OF THE BIAS OF DSO (PROOFS OF THEOREM 1)\n\n\nProof. To derive the bias of DSO, we first decompose the expectation of DSO as follows.\n\n\n\nED[w(ϕ(s′), x)∇θ log πθ(ϕ(s′)|x)r]\n= Eπ0(s′|x)[w(ϕ(s′), x)∇θ log πθ(ϕ(s′)|x)q(x, s′)]\n= Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[w(ϕ(s′), x)∇θ log πθ(ϕ(s′)|x)q(x, s′)]\n= Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[w(ϕ(s′), x)∇θ log πθ(ϕ(s′)|x)q(x, s′)]\n   −Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[w(ϕ(s), x)∇θ log πθ(ϕ(s)|x)q(x, s′)]\n  + Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[w(ϕ(s), x)∇θ log πθ(ϕ(s)|x)q(x, s′)]\n= Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[(w(ϕ(s′), x)∇θ log πθ(ϕ(s′)|x) −w(ϕ(s), x)∇θ log πθ(ϕ(s)|x))q(x, s′)]\n  + Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[w(ϕ(s), x)∇θ log πθ(ϕ(s)|x)q(x, s′)]\n= Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[∆(w∇θ)(ϕ(s′), ϕ(s); x)q(x, s′)]\n  + Eπ0(ϕ(s)|x)[w(ϕ(s), x)∇θ log πθ(ϕ(s)|x)qπ0(x, ϕ(s))].\n\n\n\n\nThen, for the second term, we have\n\n\n                Eπ0(ϕ(s)|x)[w(ϕ(s), x)∇θ log πθ(ϕ(s)|x)qπ0(x, ϕ(s))]\n                          πθ(ϕ(s)|x)\n                Eπ0(ϕ(s)|x)  π0(ϕ(s)|x)∇θ log πθ(ϕ(s)|x)qπ0(x, ϕ(s))\n         = X   π0(ϕ(s)|x)πθ(ϕ(s)|x)    log πθ(ϕ(s)|x)qπ0(x, ϕ(s))                                  π0(ϕ(s)|x)∇θ\n                   ϕ(s)∈Φ(S)\n         = X   πθ(ϕ(s)|x)∇θ log πθ(ϕ(s)|x)qπ0(x, ϕ(s))\n\n                   ϕ(s)∈Φ(S)\n         = Eπθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)qπ0(x, ϕ(s))].\n\n\n\n\nNext, we also transform the true gradient in the sentence space as follows:\n\n\n            Eπθ(s′|x)[∇θ log πθ(s′|x)q(x, s′)]\n       = Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∇θ log πθ(s′|x)q(x, s′)]\n       = Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∇θ log πθ(s′|x)q(x, s′)]\n                −Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∇θ log πθ(ϕ(s)|x)q(x, s′)]\n        + Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∇θ log πθ(ϕ(s)|x)q(x, s′)]\n       = Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[(∇θ log πθ(s′|x) −∇θ log πθ(ϕ(s)|x))q(x, s′)]\n        + Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∇θ log πθ(ϕ(s)|x)q(x, s′)]\n       = Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∆(∇θ)(s′, ϕ(s))q(x, s′)]\n        + Eπθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)qπθ(x, ϕ(s))].\n\n\n                                       23\n\nPreprint\n\n\n\n\n\nTherefore, the bias is\n   \\\n          Bias( (∇θV )DSO)\n      = ED[w(ϕ(s′), x)∇θ log πθ(ϕ(s′)|x)r] −Eπθ(s′|x)[∇θ log πθ(s′|x)q(x, s′)]\n      = Eπθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)qπ0(x, ϕ(s))]\n       + Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[∆(w∇θ)(ϕ(s′), ϕ(s); x)q(x, s′)]\n             −Eπθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)qπθ(x, ϕ(s))]\n              −Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∆(∇θ)(s′, ϕ(s))q(x, s′)]\n      = Eπθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)(qπ0(x, ϕ(s)) −qπθ(x, ϕ(s)))]\n       + Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[∆(w∇θ)(ϕ(s′), ϕ(s); x)q(x, s′)]\n       + Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∆(∇θ)(ϕ(s), s′)q(x, s′)]\n      = Eπθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)∆q(πθ, π0; x, ϕ(s))]\n       + Eπ0(ϕ(s)|x)π0(s′|x,ϕ(s))[∆(w∇θ)(ϕ(s′), ϕ(s); x)q(x, s′)]\n       + Eπθ(ϕ(s)|x)πθ(s′|x,ϕ(s))[∆(∇θ)(ϕ(s), s′)q(x, s′)].\nwhere we define\n   ∆q(πθ, π0; x, ϕ(s)) := qπθ(x, ϕ(s)) −qπ0(x, ϕ(s)),\n   ∆(w∇θ)(ϕ(s′), ϕ(s); x) := w(ϕ(s′), x)∇θ log πθ(ϕ(s′)|x) −w(ϕ(s), x)∇θ log πθ(ϕ(s)|x),\n   ∆(∇θ)(ϕ(s), s′) := ∇θ log πθ(s′|x) −∇θ log πθ(ϕ(s)|x).\n\n\n\nD.4  DERIVATION OF THE VARIANCE OF DSO (PROOFS OF THEOREM 2)\n\nProof. From the total law of variance, we have\n       nV((∇θV\\)DSO) = Vp(x)(ED[(∇θV\\)DSO|x])\n               + Ep(x)[Vπ0(s|x)(w(ϕ(s)|x)∇θ log πθ(ϕ(s)|x)q(x, s))]\n               + Ep(x)π0(s|x)[(w(ϕ(s)|x))2(∇θ log πθ(ϕ(s)|x))2σ2(x, s)].\nBecause  we  have  w(ϕ(s)|x)  =    Eπ0(a|x,ϕ(s))[w(x, a)]  and  ∇θ log πθ(ϕ(s)|x)  =\nEπ0(a|x,ϕ(s))[∇θ log πθ(a|x)], the following holds.\n            Vπ0(a,s|x)(w(a, x)) −Vπ0(a,s|x)(w(ϕ(s)|x)) = Eπ0(s|x)[Vπ0(a|ϕ(s)|x)(w(a, x))]\nVπ0(a,s|x)(∇θ log πθ(s|x)) −Vπ0(a,s|x)(∇θ log πθ(a|x)) = Eπ0(s|x)[Vπ0(a|ϕ(s)|x)(∇θ log πθ(a|x))]\n\n\nE  FUTURE WORK: DISCUSSION ABOUT DR VARIANTS OF DSO\n\nFrom the above theoretical analysis, the regression-based baseline required for a DR-style estimator\nlike Dudík et al. (2011); Saito et al. (2023) should be\n                           Eπθ(ϕ(s)|x)[∇θ log πθ(ϕ(s)|x)ˆqπ0(x, ϕ(s))]\nin expectation to achieve the same degree of bias as IS. However, a way of computing such baselines\nis not trivial because estimating log πθ(ϕ(s)|x) from data without applying importance sampling is\nchallenging. Specifically, while it is possible to estimate the score function as follows, as we did in\nestimating the weighted score function,\n                            π0(ϕ(s)|x)\n      ∇θ log πθ(ϕ(s)|x) = π0(ϕ(s)|x)∇θ log πθ(ϕ(s)|x)\n                                             K(s, s′; x, τ)∇θ log πθ(a|x)\n               = E(a,s′)∼π0(a|x)pLLM(s′|x,a)                                                ,\n                                                            π0(ϕ(s)|x)\nwe need additional importance sampling in the regression-based baseline term, not only in the original\nIS term. Therefore, even though DR approaches often aim for a further variance reduction, this naive\ndefinition of DSO-hybrid does not reduce the variance of DSO-IS. Figuring out an efficient way of\ncombining IS and regression would be a promising future work.\n\n\n                                       24\n\nPreprint\n\n\n\n\nF  EXAMPLE USAGES OF OFFLINEPROMPTS\n\nF.1  SEMI-SYNTHETIC BENCHMARK WITH LANGUAGE GENERATION\n\nHere, we provide example codes to streamline the OPL procedure using OfflinePrompts. While we\nfocus on the movie description (semi-synthetic) benchmark in this section, a similar workflow is also\napplicable to the synthetic benchmark. Please also refer to additional example codes including those\nwith the synthetic benchmark at: (forthcoming).\n\nF.1.1  SETTING UP A SEMI-SYNTHETIC SIMULATION\n\nTo set up the default movie description benchmark,  users can follow the codes in Code\nsnippet 1.  The default datasets, candidate prompts, and finetuned parameters are stored in\nsrc/dataset/assets/ in the OfflinePrompts repository.\n\nTo  customize  the  benchmark  setting,    it   is  also  possible  to  use  configurable  sub-\nmodules:   ContextQueryLoader,  CandidateActionsLoader,  FrozenLLM,  and\nRewardSimulator. Specifically, users can first create customized instances of these submodules\nand then pass them to SemiSyntheticDataset as exemplified in Code snippet 2-5.\n\nF.1.2  LOGGING POLICY\n\nAfter setting up the simulator, the next step is to define a logging policy to collect logged feedback.\nWe describe the procedure in Code snippets 6 and 7. Specifically, in Code snippet 6, we first fit the\ndimension reduction model to obtain low dimensional embeddings of query, prompt, and sentence.\nThese encoders are used across various models, e.g., to define the logging policy and to define a\nreward preditor, etc. Then, Code snippet 7 describes how to define a softmax logging policy. In the\nexample code, we first train a regression model used in the logging policy and then pass it to the\nsoftmax policy class.\n\nF.1.3  DATA COLLECTION AND REGRESSIONS\n\nOnce defining a logging policy, we collect logged data as shown in Code snippets 8. The outputs,\nincluding logged_feedback and meta_data, contain the following keys.\n\n        • logged_feedback:\n       {     user_id,      item_id,      context,      query,      action,\n      action_choice_probability∗, sentence, expected_reward∗, reward }\n        • meta_data∗:\n       { size, reward_type, reward_std, action_list }\n\nNote that the keys with an asterisk (∗) are optional outputs, and action is returned by index.\nreward_type indicates whether the reward is binary or continuous, and action_list con-\ntains the list of candidate prompts, corresponding to each action index.\n\nAfter obtaining the logged data, we regress the reward and train a logging marginal density model\nas described in Code snippets 8 and 9. prompt_reward_predictor is used by naive PG and\ntwo-stage PG, while sentence_reward_predictor and marginal_density_model are\nused by DSO.\n\nF.1.4  (ONLINE POLICY GRADIENT)\n\nIn OPL experiments, we often use the performance of online policy gradient as a baseline. To learn a\npolicy online, we can run online policy gradient as shown in Code snippet 10.\n\nF.1.5  SINGLE STAGE POLICY GRADIENTS\n\nCode snippet 11 shows the example codes to run naive PGs, including regression-based, IS-based,\nand hybrid ones. The procedure consists of only 3 steps: (1) define a policy, (2) then setup a learner\nclass (PolicyLearner), and (3) call one of the policy gradient methods. As seen in the example\n\n\n                                       25\n\nPreprint\n\n\n\n\n\ncode, all policy gradient methods can be called in similar formats. Researchers can also implement\ntheir own policy gradient methods in a similar way.\n\nF.1.6  DIRECT SENTENCE OFF-POLICY GRADIENT (DSO)\n\nDSO can also be run in a very similar way as the naive policy gradient.  As exempli-\nfied in Code snippet 12,  the key difference  is that DSO uses KernelPolicyLearner,\nlogging_marginal_density_model, and sentence_reward_predictor. Only the\nIS-based policy gradient is implemented for DSO.\n\n\nF.2  (ONLINE) PERFORMANCE EVALUATION\n\nFinally, after learning a policy, we test its performance through online interaction. This can be done\nin a single line of code, as shown in Code snippet 13.\n\nWe also provide additional quickstart examples at: (forthcoming)\n\n\n\n\n\n                                       26\n\nPreprint\n\n\n\n\n\n                                       27\n\nPreprint\n\n\n\n\n\n                                       28\n\nPreprint\n\n\n\n\n\n                                       29\n\nPreprint\n\n\n\n\n\n                                       30\n\nPreprint\n\n\n\n\n\n                                       31\n\nPreprint\n\n\n\n\n\n                                       32\n\nPreprint\n\n\n\n\n\n                                       33",
"headers": [
"arXiv:2504.02646v1  [cs.LG]  3 Apr 2025",
"P",
"O",
"L",
"B",
"D",
"ROMPT",
"PTIMIZATION WITH",
"OGGED",
"ANDIT",
"ATA",
"A",
"1",
"I",
"2",
"R",
"W",
"3",
"F",
"4",
": D",
"S",
"-P",
"G",
"(DSO)",
"5",
"-",
"6",
"E",
"7",
"-LLM E",
"M",
"8",
"C",
": O",
"OPL",
"DR",
"DSO"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2504.02646v1.pdf"
}