{
"text": "UniAPO: Unified Multimodal Automated Prompt Optimization\n\n                       Qipeng Zhu1,2*, Yanzhe Chen1,3‚ãÜ, Huasong Zhong1‚ãÜ‚Ä† Yan Li1,\n                            Jie Chen2, Zhixin Zhang1, Junping Zhang2, Zhenheng Yang1‚Ä°\n                                                        1ByteDance\n                                                      2Fudan University\n                                                     3National University of Singapore\n\n\n\n                               Abstract                         Song et al. 2025; Chen et al. 2025), there is a growing need\n                                                                                for a unified APO framework that can operate seamlessly\n             Prompting is fundamental to unlocking the full potential of\n                                                                          across text, image, and video inputs.               large language models. To automate and enhance this pro-2025               cess, automatic prompt optimization (APO) has been de-          Extending feedback-driven APO from  text  to  multi-\n              veloped, demonstrating effectiveness primarily in text-only       modal inputs‚Äîby naively appending image or video to-\n              input scenarios. However, extending existing APO meth-        kens to existing frameworks‚Äîmay seem straightforward but\n             ods to multimodal tasks‚Äîsuch as video-language genera-         quickly encounters two fundamental challenges (shown inAug\n             tion‚Äîintroduces two core challenges: (i) visual token infla-         Figure 1(a)). First, visual token inflation: a single high-\n                                                                             resolution image or short video generates hundreds to thou-25        tion,pacitywhereand resultlong visual-tokenin insufficientsequencesfeedback restrictsignals;context(ii) a lackca-                                                                      sands of tokens (Cao et al. 2023; Lee et al. 2024), thereby\n               of process-level supervision, as existing methods focus on\n                                                                                    restricting the number of samples that can be accommodated\n              outcome-level supervision and overlook intermediate super-\n                                                                and resulting in insufficient feedback signals. Second, a lack               vision, limiting prompt optimization. We present UniAPO:\n              Unified Multimodal Automated Prompt Optimization, the          of process-level supervision: multimodal tasks are inherently\n                   first framework  tailored  for multimodal APO. UniAPO       more complex (Zhou et al. 2025; Zhang et al. 2024c) and\n              adopts an EM-inspired optimization process that decouples       demand richer supervision signals to effectively optimize[cs.CV]       feedback modeling and prompt refinement, making the opti-        prompts. Relying solely on outcome-level supervision (cur-\n              mization more stable and goal-driven. To further address the          rent feedback) is insufficient, often leading to unstable and\n             aforementioned challenges, we introduce a short-long term        suboptimal prompt. And the problems caused by these two\n           memory mechanism: historical feedback mitigates context         challenges will also be intertwined with each other.\n                limitations, while historical prompts provide directional guid-\n                                                                   These challenges call for rethinking Multimodal APO             ance for effective prompt optimization. UniAPO achieves\n                                                                          as disentangled optimization, expanded feedback signals,               consistent gains across text, image, and video benchmarks,\n               establishing a unified framework for efficient and transferable       and dual-level supervision (shown in Figure 1(b)). (i) The\n            prompt optimization.                                            intertwined problems of insufficient feedback signals and\n                                                                       sub-optimal prompt create a vicious cycle in multimodal\n                                                                prompt optimization. To break this cycle, we propose a\n                        Introduction                                                                framework inspired by the Expectation-Maximization (EM)\n         Recent advances in automatic prompt optimization (APO)      algorithm that decouples these problems. (ii)Visual token\n         have enabled large language models to generate and refine       inflation quickly saturates limited context, necessitating a\n         prompts without human intervention (Cui et al. 2025; Li       long-short term memory mechanism to preserve histori-\n            et al. 2025; Ramnath et al. 2025). These methods‚Äîranging       cal feedback and extend the optimization horizon. (iii) In-\n         from search-based strategies (Zhou et al. 2022; Fernando       spired by reinforcement learning (Yao et al. 2023; RafailovarXiv:2508.17890v1     et al. 2024) to feedback-driven approaches (Pryzant et al.        et al. 2023), we argue that supplementing outcome-level\n         2023; Tang et  al. 2025)‚Äîhave shown promising results      supervision with process-level supervision is crucial. This\n          across various natural language tasks (Spiess et al. 2025;       dual-supervision approach stabilizes the optimization to-\n         Saleem et  al. 2025). Nevertheless, existing methods are     ward more performant and robust solutions.\n           largely restricted to unimodal text settings, limiting their                                                 We instantiate these insights in UniAPO (Unified mul-\n           applicability in real-world scenarios involving multimodal                                                                       timodal Automated Prompt Optimization), the first unified\n           inputs. As multimodal large language models become in-                                                                framework adopting an EM-inspired optimization scheme\n           creasingly capable and widely deployed (Zhang et al. 2024a;                                                                                   that explicitly decouples feedback modeling from prompt\n               *All authors marked with ‚ãÜare co-first authors.                    refinement. In the E-step, UniAPO aggregates valid and di-\n               ‚Ä†Project Leader.                                               verse feedback using both current errors and semantically\n              ‚Ä°Corresponding Author                                           relevant historical feedback, ensuring that optimization is\n           Copyright ¬© 2026, Association for the Advancement of Artificial      informed by a broader context. In the M-step, it generates\n            Intelligence (www.aaai.org). All rights reserved.                 new prompts by integrating short-term candidates with high-\n\nInitial Prompt        Feedback Generation   Prompt Optimization Optimized Prompt        Initial Prompt         Feedback Generation        Prompt Optimization Optimized Prompt\n                                                                                                                      Implement a                                              Identify sports-\n                                                                                  Extract                                                                                          concise,                                                                                                                                        systematic                                                                            Keyword                          The extracted    Keyword                                                                                                                                                                                                   related                                                                                                                                                                            keywords                       Ôºü                                                                              meaningful,                                                                                                                                                                                                                 Specificity:                               keywords                                                         ??                                                                                          about                                                                                                                 sports.               frame-by-frame   about           sports.              sport                                                                                                                                         Outcome-level                 !!\n                           need refinement.                               and specific                                         analysis elements     Supervsion                          Extract,\n                                                                                     sport                                                                              keywords,                                                                                                                    and actions.                               on                                        concise,                                                                                                                                                                                      meaningful,                                                                                                                                                                    and                                                                          +      +                 Focus                           Ôºü                                                                               exclude                                                                                                                                                          Process-level                                                                                                                                Historical                                                                                                                                                                                                               specific                                                                                                                                                                            keywords                                  specific                                     terms    Multimodal                                                                                          irrelevant                                                                                              or         Multimodal                                                                                                                                             Supervision                                                                                                          Feedback                                    that                                        directly                                                                                                                                                                                                                                          (e.g., \"soccer,\"                      Ôºü\n                                                                                 broad                                  relate                                      to the sport                                   overly                                                                                                                 Feedback                                                                                                                                Prompt                                   \"basketball,\"                                                                                       terms.                                    context.                                                                                                                                                                                \"Olympics\")                                                                                               Memory        Memory\n      (1) Visual Token Inflation  (2) Lack of Process-level Supervision\n        (Low-quality Feedback)           (Sub-optimal Prompt)                                                    E-step                             M-step\n      Init Score : 10          Interwined                               Opt Score : 30        Init Score : 10                                                     Opt Score : 70\n\n                       (a) Native Multimodal APO                                                          (b) UniAPO\n\nFigure 1: Motivation Illustration: (a) Naively extending text-based APO to multimodal inputs introduces visual token inflation\nand a lack of process-level supervision. (b) Our proposal adopts an EM-inspired optimization scheme to iteratively update\nfeedback and prompt memory to solve the above problems.\n\n\nquality historical prompts from long-term memory, effec-      2024; Keskar, Perisetla, and Greer 2025), where prompt de-\ntively anchoring the optimization. These components enable       signs are often tailored to the data modality and task struc-\nUniAPO to scale to complex multimodal tasks and achieve        ture. Despite promising results, these approaches rely heav-\nrobust, interpretable prompt optimization.                            ily on manual prompt design, which becomes increasingly\n  Our contributions are summarized as follows:                   infeasible as MLLMs are deployed across more complex, di-\n                                                                    verse, and open-ended domains. This limitation has spurred\n ‚Ä¢ We propose UniAPO, the first unified multimodal APO\n                                                        growing interest in automated prompt optimization tech-\n   framework that scales across text, image, and video tasks\n                                                             niques (Zhang et al. 2024b), aiming to scale prompt engi-\n   within a single architecture, achieving state-of-the-art\n                                                              neering in a systematic and adaptive manner.\n   performance compared to existing baselines.\n ‚Ä¢ We introduce an EM-inspired optimization scheme that     Automatic Prompt Optimization (APO)\n   decouples feedback modeling and prompt refinement,                                        APO aims to automatically discover effective prompts for\n   yielding a stable optimization process.                                           LLMs and MLLMs, reducing manual effort while enhanc-\n ‚Ä¢ We design a long-short term memory mechanism that      ing generalization across diverse tasks (Cui et al. 2025; Qu\n   alleviates visual token inflation and lack of process-level        et al. 2025; Ramnath et al. 2025; Do et al. 2025). Exist-\n   supervision via historical feedback signals and dual-level      ing approaches fall into two main paradigms: search-based\n   supervision.                                                 optimization and feedback-driven refinement. Search-based\n                                                       methods explore the prompt space by iteratively sampling\n               Related Work                       and evaluating candidates (Davari et al. 2025; Zhang, Zhou,\n                                                       and Liu 2024). APE (Zhou et al. 2022) frames prompt con-\nPrompt Engineering for MLLMs                                                                     struction as a discrete optimization task, with LLMs generat-\nPrompt engineering plays a pivotal role in enabling MLLMs      ing and scoring prompts in a closed loop. Subsequent works\nto perform both general reasoning and domain-specific      adopt evolutionary strategies (Liu et al. 2024; Fernando et al.\ntasks (Chen et al. 2023; Mohanty, Parthasarathy, and Shahid      2024) or treat LLMs as black-box optimizers (Yang et al.\n2025). A prominent line of research centers on chain-of-      2023). However, these methods often suffer from search path\nthought (CoT) prompting (Wei et al. 2022; Zhang et al.      explosion in semantically complex or open-ended settings,\n2024d; Shao et al. 2024), where prompts like ‚ÄúThink step       limiting their scalability in multimodal domains. Feedback-\nby step‚Äù are used to elicit structured reasoning, especially      driven methods improve stability by introducing an interme-\nin spatial contexts. Related works extend this to single-       diate phase: models analyze failure cases and generate tex-\nturn reasoning (Zheng et al. 2024; Wang et al. 2025b; Lin       tual feedback, which is then used to revise prompts (Agar-\net al. 2025), often prompting MLLMs to generate interme-      wal et al. 2025). APO (Pryzant et al. 2023) pioneered this\ndiate queries or reflections to enhance interpretability and      paradigm, viewing feedback as a textual ‚Äúgradient‚Äù to guide\nproblem-solving ability. Beyond reasoning, studies have ex-       optimization. Later work extends this idea with analogi-\nplored prompt formatting (He et al. 2024; Wang et al. 2025a;       cal reasoning (Tang et al. 2025), pseudo-gradient propaga-\nLamott et al. 2024) as a way to improve response consis-       tion (Yuksekgonul et al. 2024), memory-augmented reflec-\ntency, especially in scenarios requiring tool use, layout un-       tion (Yan et al. 2025), and strategic self-guidance (Wu et al.\nderstanding, or constrained output forms. To address task-      2024), achieving strong performance in text-only tasks. De-\nspecific needs, researchers have developed domain-adapted       spite success in text tasks, feedback-based APO struggles\nprompts across a wide range of applications. This includes       in multimodal contexts: visual token inflation and lack of\nopen-vocabulary grounding (Du et al. 2022a,b; Li et al.       process-level supervision. We alleviates visual token infla-\n2024b), semantic segmentation (Li et al. 2024a; Lee et al.       tion and lack of process-level supervision via historical feed-\n2025), and visual question answering (VQA) (Zhao et al.      back signals and dual-level supervision by designing a long-\n\nshort term memory mechanism.                            Overall Architecture\n                                     A core component of UniAPO is the integration of memory\n                Preliminaries                             to leverage historical information. We introduce a feedback\n                                                     memory, MtF , and a prompt memory, MtP , to store all gen-Problem Formulation and Baseline\n                                                                 erated feedback and prompts up to iteration t.\nLet the datasets be denoted as Dtrain, Ddev, and Dtest, each          Specifically, our method begins with a simple phase. We\nconsisting of sample-label pairs (x, y). We consider a sys-      use the prompt optimization model, LP , to refine a sim-\ntem of frozen MLLMs with different system prompts as al-       ple, sample-agnostic initial prompt (e.g., ‚Äúkeywords about\nternates roles: a task model LT for prediction, a feedback       sports‚Äù) to obtain a superior input prompt, P 0. This ensures\nmodel LF for generating feedback, a prompt optimization       that the optimization process starts from a more reasonable\nmodel LP , and an evolution model LE. Details of system       point in the optimization space. The optimization then pro-\nprompts are stated in the Appendix. Our primary objective      ceeds iteratively through the E-step and M-step.\nis to find the optimal prompt P ‚àóthat maximizes the expected      E-Step: At iteration t, the current prompt P t is used with\nperformance on a given dataset Dtest:                             the multimodal inputs to perform inference (assised by LT ),\n                                                                     resulting in an error set Dterror. This error set, along with the    P ‚àó= argmax                     E(x,y)‚ààDtest[Eval(LT (x; P), y)],    (1)      feedback memory MtF , is then processed by the feedback            P ‚ààP\n                                                     model LF (potentially assisted by a evolution model LE) to\nwhere P represents the space of all possible prompts and      generate new, targeted feedback F t+1. The feedback mem-\nEval(¬∑) is the evaluation metric.                               ory is subsequently updated with this new information. The\n  Then we establish a baseline method based on feedback-       entire process can be expressed as:\ndriven Automatic Prompt Optimization (APO). In a naive                                                        (F t+1, Mt+1F  ) = E-Step(Dterror, MtF ; LF , LE).   (2)\nmultimodal feedback-driven APO (Pryzant et al. 2023) loop,\n                                                     M-Step: In the subsequent M-step, the newly generatedthe optimization process is iterative. At each step t, we iden-                 t+1\n                                                                                        P are used totify an error set Dterror ‚äÜDtrain where the task model LT      feedback F    and the prompt memory Mt\nfails with the current prompt P t. Subsequently, the feed-      guide the prompt optimization model LP (alsot    assisted by\nback model LF generates feedback F t+1 based on Dterror      LE). This step refinest+1the current prompt P  to produce an                                                        improved prompt P    for the next iteration, and the promptand P t. Finally, the prompt optimization model LP opti-\n                           t                     t+1             memory is updated accordingly. This step can be formulatedmizes the prompt P  using the feedback F     to produce\n                                                                        as:an improved prompt P t+1. However, this straightforward\nfeedback-driven approach encounters two significant chal-       (P t+1, Mt+1P  ) = M-Step(F t+1, MtP , P t; LP , LE).  (3)\nlenges. Details of system prompts are stated in the Ap-                                                                  In the following subsections, we will elaborate on how the\npendix.                                                               E-step and M-step are specifically designed to address the\n                                                               challenges of Visual Token Inflation and a Lack of Process-\nCore Challenges                                                                      level Supervision, respectively.\nA naive multimodal APO framework faces two critical, in-\ntertwined challenges: visual token inflation (Cao et al. 2023;      E-step: Multimodal Feedback Generation\nLee et al. 2024) and a lack of process-level supervision (Ue-     The E-step is specifically designed to combat the Visual\nsato et  al. 2022). Visual token inflation stems from the     Token Inflation challenge during the feedback generation\nfeedback generator‚Äôs (LF ) finite context, which yields low-      phase. The essence of this problem lies in a practical con-\nquality feedback by failing to process all historical and cur-        straint: the feedback model, LF , has a finite context window.\nrent errors. Concurrently, the prompt optimizer (LP ) re-     As the generation process iterates, the cumulative set of all\nceives only this outcome-level supervision, leading to sub-      encountered errors can easily grow to exceed this capacity.\noptimal prompts. These issues create a vicious cycle of mu-      Consequently, at iteration t, it becomes infeasible to feed the\ntual degradation, making a simultaneous solution exception-       entire raw error history into LF for consideration.\nally difficult.                                            To overcome this limitation, we introduce a short- and\n                                                             long-term memory mechanism. Our key insight is that the\n              Methodology                         complete error history can be effectively represented by two\n                                                                        distinct components:\nTo tackle the two intertwined challenges of Visual Token In-\n                                                                             ‚Ä¢ Short-term Information: The current error set, Dterror,flation and a Lack of Process-level Supervision, we propose\n                                                         which captures the model‚Äôs most recent failures and isa novel framework named Unified Multimodal Automatic\n                                                             used by LP to generate the next feedback, F t+1.Prompt Optimization (UniAPO). Our approach is inspired\nby the Expectation-Maximization (EM) algorithm and em-         ‚Ä¢ Long-term Information: The feedback memory, MtF ,\nploys a divide-and-conquer strategy to decouple the prob-        which stores a cumulative history of past errors and their\nlem, as illustrated in Figure 2. UniAPO consists of two main          associated corrective feedback.\nsteps: an E-step designed to address Visual Token Inflation,     The E-step is to first extract information from these two\nand an M-step to counter a Lack of Process-level Supervi-      sources and then unify them, ensuring that a holistic view\nsion. This design effectively breaks the vicious cycle arising       of all errors can be processed within the limited context of\nfrom the interplay of these two challenges.                LF .\n\nInput                     Feedback ùêπùë°                                      Prompt ùëÉùë°           Optimized Prompt                                             E-step\n   Initial Prompt:Keywords about sports.                ùíüùëíùëüùëüùëúùëüùë°                                                                As a keyword extractor for short\n                    Cold Start                                               ùëÉùë°+1 Eval(‚ãÖ)     ùëÉùë°+1                    video ecosystem governance, your\n  Input Prompt: The keyword extractor                                                    2  1                                          role is to accurately and\n  must identify and extract concise,                                                      3  4                                     comprehensively identify sports-\n   relevant, and contextually accurate                                   Feedback Memoy      Prompt with the                                                         related keywords.\n  keywords specifically related to                         ùë°                Error Cases                        ‚Ñíùê∏(‚ãÖ)               (‚ãÖ)      1. Specificity:   sports from the provided video ‚Ä¶         ‚Ñ≥ùêπ                                                                                  TopK(‚ãÖ)                                                             ‚Ä¢  Extract concise, meaningful, and\n             Input Types                                                                                                                        Add        specific keywords (e.g., \"soccer,\"\n               Text                                                                                                                            \"basketball,\" \"Olympics\").\n   Her testimony was a pack of lies.                                                                  ùëÉùë†‚Ñéùëúùëüùë°ùë°+1          ùëÉùëôùëúùëõùëîùë°+1       ‚Ä¢  Avoid overly broad terms unless\n   She wasn‚Äôt even at the crime scene.                                                                                                                       explicitly tied to a sports context.\n   Don‚Äôt believe what the media says                     Retrieval(‚ãÖ)         ‚Ñíùêπ(‚ãÖ)                     ‚ÑíùëÉ(‚ãÖ)          TopK(‚ãÖ)     ‚Ä¢ Use detailed descriptors when\n   about why you should be skinny.\n                                                                                                                                   supported by evidence (e.g., \"100-\n    It‚Äôs all a pack of lies‚Ä¶\n                                                     (‚ãÖ)                                                                                        meter dash\" instead of \"running\").              Image\n                                                                                                                                                         2.                                                                                                                               Relevance:                                               ‚Ñíùê∏(‚ãÖ)                                               Add                                                                                                                                                                   ‚Ä¢                                                                                                                                         Extract only sports-related                                         ùêπùëôùëúùëõùëîùë°+1                                                               ùêπùë†‚Ñéùëúùëüùë°ùë°+1                     Prompt with Key    ‚Ñ≥ùëÉùë°\n                                                                                   Filter(‚ãÖ)                    Examples and FeedbackPrompt Memoy      keywords. Exclude irrelevant or\n                                                                                                                                           overly broad terms.\n              Video                                                                                                                                           ‚Ä¢  Ensure keywords reflect the core\n                                                                                                                          theme or activity depicted in the\n                                                                         ùêπùë°+1                ùíüùë°ùëüùëéùëñùëõ‚àíùíüùëíùëüùëüùëúùëüùë°                     video.\n                                               M-step                        ‚Ä¶‚Ä¶\n\nFigure 2: Illustration of our UniAPO framework for UniAPO. Starting with a simple prompt initialized by an MLLM (left),\nUniAPO iteratively refines it into a structurxed and knowledgeable prompt (right) using an Expectation-Maximization (EM)\nalgorithm. The E-step generates long- and short-term feedback from the current prompt, which is then used in the M-step to\nupdate the prompt, enabling optimization across diverse data types.\n\n\nShort-Term Feedback Generation. A  practical  chal-      Short- and Long-Term Feedback Evolving  To combine\nlenge remains: even the most recent error set, Dterror, can       the short-term (F short)t+1  and long-term (F longt+1 ) feedback, we\nbe too large to fit into the context window of LF in a sin-      devise a two-step process. First, inspired by evolutionary al-\ngle pass. To manage this, we adopt a hierarchical strategy      gorithms (B¬®ack and Schwefel 1993), an ‚ÄúEvolver‚Äù MLLM,\ninspired by techniques in multimodal Retrieval-Augmented     LE, fuses the two streams, guided by a system prompt to\nGeneration (RAG) (Yu et al. 2024). The procedure first clus-       resolve conflicts and merge salient information. Second, to\nters Dterror to group semantically similar failures, enabling      guarantee utility, the resulting candidate feedback undergoes\nLF to produce more stable feedback on common error pat-      a filtering step, Filter(¬∑), inspired by ERM (Yan et al. 2025).\nterns. Subsequently, to adhere to the model‚Äôs context limit,      This step validates the feedback by retaining only sugges-\neach resulting cluster is processed in smaller chunks. Feed-       tions that demonstrably correct errors in the original set\nback is generated for each chunk and then aggregated to rep-     Dterror. The generation of the final, validated feedback F t+1\nresent the entire cluster‚Äôs error profile, as depicted in Fig-        is formulated as:\nure 2. The entire process of generating the short-term feed-\nback, denoted as F short,t+1 can be formally expressed as:            F t+1 = Filter(LE(F short,t+1 Flongt+1 ), Dterror, P t; LT )   (6)\n       F shortt+1 = LF(Pt, Clustering(Dterror)),        (4)      where F t+1 is added into MtF to gain Mt+1F   as depicted in\n                                                           Equation (7):\nwhere Clustering(¬∑) is the DBSCAN algorithm using BGE-\nm3 (Chen et al. 2024) embeddings.                              Mt+1F = Add(MtF , F t+1).             (7)\n\nLong-Term Feedback Generation. A naive inclusion of     M-step: Multi-modal Prompt Optimization\nthe entire memory MtF is suboptimal, as obsolete feedback                                                   The M-step resolves the outcome-only supervision problemfor corrected errors can introduce semantic noise. To address\n                                                     by synergizing two distinct supervisory signals for promptthis, we shift from simple summarization to targeted re-\n                                                                 optimization.trieval. Specifically, we use the newly generated short-term\nfeedback, F short,t+1 as a dynamic query. The feedback derived         ‚Ä¢ Outcome-level Supervision: Following naitive feedback-\nfrom each error cluster acts as a separate query to retrieve          driven methods (Pryzant et al. 2023), we use the imme-\nthe most relevant entries from the memory MtF . These re-          diate feedback, F t+1, to perform a tactical update on the\ntrieved historical records are then aggregated to form a po-          current prompt, P t, yielding a short-term prompt, P short.t\ntent and contextually relevant long-term feedback, F longt+1 , as         ‚Ä¢ Process-level Supervision: Inspired by PRMs (Uesato\nillustrated in Figure 2, where Retrieval(¬∑, ¬∑) denotes the re-           et al. 2022), we introduce a novel process-level signal\ntrieval process. The entire generation process can be formu-        by distilling a long-term prompt from the entire prompt\nlated as:                                                                   history, MtP . This prompt embodies stable, historically\n         F longt+1 = Retrieval(F short,t+1 MtF ).            (5)           effective strategies.\n\nThe final prompt, P t+1, is synthesized by modulating the       following), ETHOS (Mollas et al. 2022) (hate speech detec-\nshort-term prompt with the strategic guidance from the long-        tion), and WebNLG (Gardent et al. 2017) (structured-to-text\nterm prompt. This ensures that our updates are not only re-       generation). (2) Image: Meme (Javaid 2023) (multi-image\nsponsive to immediate failures but are also grounded in a       classification requiring semantic alignment via prompt rea-\nhistory of successful optimizations, leading to superior ro-       soning). (3) Video: An in-house dataset from an interna-\nbustness and performance.                                          tional platform, covering static classification (low-motion\n                                                                     detection), occlusion classification (identifying overlays),\nShort-Term Prompt Optimization.  Our process begins\n                                      t+1                  and open-domain keyword extraction (generating keywords\nwith generating a Short-Term Prompt, Pshort, by leveraging      from multimodal metadata) across Beauty, Sport, Travel,\nan MLLM optimizer, LP , to refine the current prompt P t.                                                       and Food themes. More details are stated in Appendix.\nThis refinement is guided by the recent, coarse-grained feed-\nback F t+1. To ensure the optimizer maintains a robust un-      Evaluation  Metrics.  Tasks  are  grouped  by  domain\nderstanding of the task (Zhang, Zhou, and Liu 2023), we also      with corresponding  metrics: Text  classification (LIAR,\nprovide it with a set of positive examples, Sample(¬∑), sam-     ETHOS, BBH-navigate): binary F1 score; Text genera-\npled from Dtrain ‚àíDterror. This prevents over-fitting to recent       tion (WebNLG): ROUGE-L; Image classification (Meme):\nfailures and is formally expressed as:                              multi-class F1-micro; Video classification (Static, Occlu-\n       t+1                                                          sion): binary F1; Multimodal keyword extraction (video,\n    P short = LP (Pt, F t+1, Sample  Dtrain ‚àíDterror)    (8)                                                                four themes): F1-score More details are stated in Appendix.\nWe run the optimizer LP multiple times to generate a diverse\n                                                                Baselines.  For  all tasks, we compare UniAPO against\nset of candidate prompts, as shown in Figure 2.\n                                                               standard  prompting,  Chain-of-Thought  (CoT)  prompt-\nLong-Term Prompt Generation  To  ensure  that  our      ing (Wei et  al. 2022), and two prominent categories of\nprocess supervision signal  is derived from high-quality      automatic prompt optimization: (1) Search-based methods\nprompts, we filter the prompt history rather than using it        (e.g., EvolPrompt (Liu et al. 2024)), which iteratively mu-\nwholesale. We recognize that underperforming prompts can        tate and select prompts; (2) Feedback-based methods (e.g.,\nprovide misleading guidance. Therefore, we select only the    ERM (Yan et al. 2025)), which update prompts based on per-\ntop-k historical prompts from MtP based on their scores on      formance signals.\nthe Ddev. This selection is performed via a Top-K algorithm,\n          t+1                                           Implementation Details.  All primary experiments use\nyielding P long :                                                   GPT-4o (Achiam et  al. 2023) as the underlying MLLM\n           Plongt+1 = TopK(MtP , k)                (9)       across all stages of the UniAPO pipeline. Prompts are initial-\n                                                                ized with minimal handcrafted templates, denoted as ‚ÄúSim-\nShort- and Long-term Prompt Evolving.  To effectively       ple Prompt‚Äù to simulate a low-resource setting. In additional\nfuse the process and outcome signals, we introduce a step      experiments, we replace GPT-4o with QwenVL2.5-72B (Bai\ninspired by evolutionary crossover. We task the MLLM op-        et al. 2025) as the predictor to evaluate cross-model general-\ntimizer, LE, to act as a supervisor that intelligently synthe-       ization, while keeping the other components unchanged. We\nsizes the short-term prompt with the wisdom from the long-       also explore settings with more structured initial prompts, as\nterm prompts. This supervised crossover allows the current       detailed in relevant sections.\nprompt to adopt the proven advantages of its predecessors in\na structured way. The process is defined as:                 Comparision Study\n           P t+1 = LE(Pshort,t+1 Plongt+1 )            (10)     Comparision with different tasks.  UniAPO sets a new\n                                                                        state-of-the-art across a diverse suite of multimodal tasks\nThe generated prompt P t+1 is first evaluated on Ddev, and its       as shown in Table 1, consistently outperforming existing\nscore is recorded as it is integrated into the prompt memory,       baselines. Its superior performance and stability, particu-\nwhich is updated to Mt+1P   :                                           larly on video tasks, are driven by our unified memory\n                                                     mechanism that combats visual token inflation and a lack\n          Mt+1P = Add(MtP , P t+1).           (11)       of process-level supervision. Underscoring its robustness,\n                                              UniAPO maintains  its effectiveness when the backbone  To prevent premature convergence and expand the opti-\n                                                     model is switched from GPT-4o to Qwen2.5VL-72B, prov-mization horizon, we then employ a beam search mecha-\nnism. Specifically, we select the top-b prompts from Mt+1P        ing the generalizability of our framework.\nbased on their scores. These b prompts become parallel                                                           Generalization  of  UniAPO.  UniAPO  demonstrates\n‚Äòbeams‚Äô for the next iteration.                                                                strong generalization, which we validate through two key\n                                                              experiments: robustness to initialization and cross-model\n               Experiment                               transfer (Figure 3).\nExperimental Setting                                                 ‚Ä¢ Robustness to Initialization: UniAPO is largely insensi-\nDatasets.  We evaluate UniAPO across text, image, and           tive to the quality of the initial prompt. It consistently el-\nvideo domains on both classification and generation tasks:          evates the performance of both simple and complex start-\n(1) Text: LIAR (Wang 2017) (fake news classification),         ing prompts, as evidenced by the significant gap between\nBBH-navigate (Suzgun et al. 2023) (multi-step instruction        ‚ÄúOpt Settings‚Äù and ‚ÄúInit Settings‚Äù on ‚ÄúTest @ 4o‚Äù. This\n\nText CLS       Text GEN Image CLS      Video CLS              Video KE\n            Method\n                         LIAR BBH ETHOS WebNLG   Meme    Static Occlusion layer Beauty Sport Travel Food\n\n                                          GPT4o as Predictor\n\n                Vanilla             25.3  69.4   88.6      50.9       25.8     71.2       25.6        36.7   55.8   43.5   24.6\n    Vanilla + CoT (Wei et al. 2022)  56.9  90.7   95.0      51.1       25.6     80.1       50.0        46.9   63.9   54.1   31.5\n    EvoPrompt* (Liu et al. 2024)   58.6  92.7   96.6      50.5       26.9     82.8       33.3        47.4   56.2   44.9   24.7\n     ERM* (Yan et al. 2025)     65.2  95.4   95.6      52.1       28.6     80.1       61.5        68.3   69.3   57.4   40.3\n          UniAPO            78.7  99.4   98.1      53.2       37.6     86.3       70.3        74.7   78.3   60.9   54.3\n\n                                           QwenVL2.5-72B as Predictor\n\n                Vanilla              2.0   44.7   89.0      44.3       24.7      0.0        25.6        28.7   50.0   45.9   27.6\n    Vanilla + CoT (Wei et al. 2022)  49.4  93.2   97.6      46.3       24.6     54.5       41.9        43.9   58.6   47.1   25.3\n    EvoPrompt* (Liu et al. 2024)   50.6  94.1   98.0      46.3       25.8     78.2       30.0        44.3   52.8   46.1   27.8\n     ERM* (Yan et al. 2025)     67.4  93.3   98.2      52.3       28.2     59.8       63.2        64.0   64.1   51.2   41.4\n          UniAPO            73.1  95.8   98.9      54.4       35.7     83.1       67.9        75.2   76.8   63.7   48.6\n\nTable 1: Performance comparison using GPT-4o vs. QwenVL2.5-72B as the predictor, optimized by our UniAPO framework.\nUniAPO‚Äôs other internal components are implemented using GPT-4o. All experiments are conducted on 11 datasets including\ntext classiifaction (‚ÄúText CLS‚Äù), text generation (‚ÄúText GEN‚Äù), image classification (‚ÄúImage CLS‚Äù) and video classification\n(‚ÄúVideo CLS‚Äù) and video keyword extraction (‚ÄúVideo KE‚Äù).\n\n\n   robustness is a direct result of its EM framework, which       step refines feedback by mitigating visual inflation, and an\n    iteratively refines the solution, and its process-level su-      M-step uses dual-level supersion to optimize prompt ef-\n    pervision.                                                           fectively. This closed-loop process accelerates convergence,\n  ‚Ä¢ Cross-Model  Transferability:  Prompts  optimized by      demonstrating that UniAPO delivers state-of-the-art results\n  UniAPO transfer effectively across different architec-      with greater sample and compute efficiency.\n    tures. When prompts optimized on GPT-4o are trans-\n    ferred to different the testing predictor settings, such as                Beauty                   Sport                                                           80\n   Qwen2.5-VL-72B, they retain a substantial performance                                                                                     80\n   advantage over the original prompts (‚ÄùTest @ Qw‚Äù with        70\n   ‚ÄùOpt Settings‚Äù vs. ‚ÄùInit Settings‚Äù).                                                   Score60                                              Score70\n                                                F1                    F1                                                           50\n      Simple Prompt        Complex Prompt                                   60                   74.7                           75.2                    Test40                                    Test  75\n                         59.2          62.2  59.1       60.3         30                        50                                                            0  2  4  6  8  10       0  2  4  6  8 10 12\n  50                                                                    Training Iteration             Training IterationScore    36.7\n             28.7                                                 UniAPO      ERM*        EvolPrompt*\nF125\n\n                                                             Figure 4: Optimization efficiency and performance compar-\n   0\n   4o Qw 4o Qw     4o Qw 4o Qw           ison. This figure illustrates the Testing F1-score progression @ @ @ @   @ @ @ @              for UniAPO, ERM*, and EvolPrompt* over iterations.\nTestTest TestTest       TestTest TestTest\n                  Init Settings        Opt Settings                                                      Analysis Study\n\n                                                            Visual Token Inflation.  Here, we empirically validate theFigure 3: Evaluating the robustness and transferability of\n                                                               Visual Token Inflation (VLF) bottleneck and the efficacy ofUniAPO in beauty keyword extraction. The table compares\n                                                            our historical feedback solution (Figure 5a). We first estab-performance from ‚ÄúSimple‚Äù and ‚ÄúComplex‚Äù initial (‚ÄúInit‚Äù)\n                                                                          lish that while performance scales with the number of inputprompts against our optimized prompts (‚ÄúOpt‚Äù) based on\n                                                                         errors, it inevitably saturates as it hits the feedback gener-GPT4o. We use ‚ÄúTest @ 4o‚Äù and ‚ÄúTest @ Qw‚Äù respectively\n                                                                        ator‚Äôs context limit. This confirms the VLF problem. Criti-represent the predictor types when testing.\n                                                                           cally, introducing our historical feedback at this saturation\n                                                                 point yields further, significant performance gains. This re-\nEfficiency of UniAPO.  UniAPO is significantly more effi-        sult demonstrates that our long-term memory mechanism ef-\ncient than baselines, reaching superior performance in fewer       fectively compensates for the limited context window, en-\noptimization steps (Figure 4). This is attributed to its EM-       riching the feedback generation process with vital historical\ninspired framework, which creates a virtuous cycle: an E-       information.\n\n75  75       +3         HF       Methods                                                                                        Video CLS                                                                                                     Video                                                                            KE                                                         +2                                                         HP                                74                     ERM*  74                                                         +1                                                         HP          FG Type PO Type       +2         HF                       UniAPO                                                +2                                                 HP                                73  73       +1         HF                                                                                           Occlusion                                                                                                                 layer Beauty                                                                                                                     Sport                                                +1                                                 HP                    + HF                                72  72                                       +2 HP\n(%)71  CS = 4                          (%)71                   BS=3            ERM*  ERM*       61.5       68.3  69.3                                70                                       +1 HP  70                                                         UniAPO                                                        ERM*       65.5                                                                                                              73.1                                                                                                                    74.3                                               BS=2                                69              CS              =                     3  69                                                                    ScoreScore                                                  ERM*                                                               UniAPO                                                                                                   65.6       70.7                                                                                                                    76.7                                68                                                       BS=3       CS = 4                 CS                 =                        2  68                                      BS=1                           F1F1                                67                                                                                                   70.3                                                                                                              75.2                                                                                                                    78.3                                                         UniAPO                                                               UniAPO                         CS = 1                                                            Methods              CS              =                    3  67                                              BS=2                                66                 CS                 =                        2                                                        ERM*\n  66                            65                     UniAPO\n  65                   CS = 1      64 BS=1              + HP         Table 3: Comparison with different combainations between\n                                63  64                                   100    200    300    400         Feedback Generation methods (FG) and Prompt Optimiza-     100   200   300   400\n          Time (minutes)                    Time (minutes)              tion (PO) methods.\n(a) Effect of increasing chunk  (b) Effect of increasing beam\nsizes (‚ÄúCS‚Äù) and historical feed-  size  (‚ÄúBS‚Äù)  and   historical\nback (‚ÄúHF‚Äù).                  prompts (‚ÄúHP‚Äù).                    of process-level supervision, respectively‚Äîall hybrid con-\n                                                                     figurations underperform the complete UniAPO system.\nFigure 5: UniAPO is proven to be both practically efficient\nand highly effective to alleviate visual token inflation and a\n                                                                                         Video CLS    Video KE\nlack of process-level supervision.                              F-Mem   P-Mem\n                                                                                            Occlusion layer Beauty Sport\n\n                                                                              Short     Short        63.2       68.3  70.5\nA Lack of Process-level Supervision.  Figure 5b vali-              Short-long   Short        66.7       71.3  75.6\ndates our core hypothesis: dual-level supervision is essential                Short   Short-long      65.2       70.9  74.0\nfor robust prompt optimization. We show that a feedback-              Short-long Short-long      70.3       74.7  78.3\nonly baseline (blue line)  is insufficient. By augmenting\nthis with process-level supervision from varying numbers      Table 4: Ablation of Short-Term and Long-Short Term mem-\nof historical prompts, our method consistently boosts per-      ory mechanism in Feedback Memory (F-Mem) and Prompt\nformance across all tested beam sizes, critically, with no     Memory (P-Mem).\ncomputational overhead. This demonstrates that integrating\nprocess-level guidance with outcome-based feedback is key\nto achieving stable and superior optimization results.             Effect of each component in Memory Mechanism.  Our\n                                                                  ablation study confirms that UniAPO‚Äôs dual memory sys-\nAblation Study                                       tem is critical. The long-term memory in Feedback Gener-\n                                                                   ation (FG) is essential for mitigating visual token inflation,\n                                                            while the long-term memory in Prompt Optimization (PO)\n                        Video CLS     Video KE                provides process-level supervision. Removing either com-         E-step M-step\n                       Occlusion layer Beauty Sport              ponent cripples the system by introducing low-quality feed-\n                                                         back or sub-optimal prompt, respectively. UniAPO‚Äôs state-\n                             25.6        36.7   55.8\n                                                                       of-the-art performance is attributable to the synergy of these      ‚úì                59.3        66.3   75.1\n          ‚úì         61.2        67.8   73.0             mechanisms in solving these core multimodal challenges.\n      ‚úì   ‚úì         70.3        75.2   78.3\n                                                Case Study\n         Table 2: Ablation of E-step and M-step.         A case study on sport keyword extraction as shown in\n                                                                  the Appendix reveals how UniAPO transforms a simple\n                                                        prompt into a sophisticated, hundred-line directive. This iter-\nAblation of E-step and M-step.  As shown in Figure 2,       ative evolution is driven by specific, class-level feedback‚Äîa\nour ablation study confirms the synergistic relationship be-      product of our memory mechanism that successfully miti-\ntween UniAPO‚Äôs E-step and M-step. While both prompt op-       gates visual token inflation. The process history also con-\ntimization (M-step) and feedback generation (E-step) are       firms that the initial prompt‚Äôs structure, even when simple,\nindividually effective, yielding significant gains when used        is critical for establishing a directed optimization path, high-\nalone, the full framework that alternates between them per-       lighting the synergy of our approach.\nforms best, which validates that the complementary interac-\ntion of these two steps is critical to UniAPO‚Äôs capabilities.                     Conclusion\n                                         We present UniAPO, the first unified framework for auto-\nFeedback Generators and Prompt Optimizers.  Our ab-      mated prompt optimization (APO) that operates effectively\nlation study, which created hybrid models by swapping com-       across text, image, and video tasks. By decoupling feed-\nponents with baselines (Table 3), reveals the powerful syn-      back modeling from prompt refinement through an EM-\nergy within UniAPO. While our feedback generator (FG)       inspired scheme and introducing a long-short term memory\nand prompt optimizer (PO) each provide significant, dis-      mechanism, UniAPO overcomes key challenges in multi-\ntinct benefits‚Äîmitigating visual token inflation and a lack     modal APO. Experiments show that UniAPO consistently\n\nsurpasses existing baselines in both performance and gener-      ground-truth keywords using the BGE-m3 (Chen et al. 2024)\nalization. We believe our approach paves the way for more      model. A predicted keyword and a ground-truth keyword are\nrobust and scalable prompt optimization in future multi-      considered a match if their similarity is greater than 0.9.\nmodal language models.                           We then count the number of matched keywords. Let Àúc be\n                                                                 the number of keywords in Àúy that match at least one keyword\n               Appendix                               in y. Similarly, let c be the number of keywords in y that are\n                                                       matched by at least one keyword in Àúy. Precision and recallDetails of Experimental Setting\n                                                                 are defined as follows:\nDatasets  For all text-based datasets, we adopt the data\npartitioning scheme from ERM (Yan et al. 2025). Since                                                    Àúc              c\n                                                                              Precision =       Recall =             (12)the original Meme dataset lacks official validation and test                                             |Àúy|,              |y|\nsplits, we partition it into training, validation, and test sets                                                       where | ¬∑ | denotes the cardinality of the set.\nusing a 3:3:4 ratio. The data splits for our in-house video                                                     The F1 score for each sample is the harmonic mean of\ndatasets, designed for classification and keyword extraction                                                                 precision and recall:\ntasks, are detailed in Table 5. The splits for the classifica-\n                                                                      2 √ó Precision √ó Recalltion tasks are designed to robustly evaluate the generaliza-                                                              F1 =                                  (13)\ntion performance of our models.                                                        Precision + Recall\n                                                                Baselines.  In the multimodal domains, where no estab-\n        Task    Sub-task Train Validation Test              lished APO baselines exist, we extend EvolPrompt and\n                                     ERM to multimodal settings, denoted as EvolPrompt* and\n             BBH    38     58    144                                              ERM*, by adapting them to handle multimodal inputs. All\n       Text CLS    Ethos   798    200    200                                                       methods are evaluated alongside naive prompting and CoT-\n                     Liar    3681    461    461                                                                       style baselines for fair comparison.\n       Text GEN WebNLG  200    300    300                                                       Implementation Details.  We split each dataset into train-\n      Image CLS  Meme   207    207    698               ing, development, and test sets. In each iteration of prompt\n                                                                 optimization, candidate prompts are trained on the training\n                    Static CLS 100    100    834\n      Video CLS                                                         set, selected based on development performance, and evalu-\n                 Occlusion  91     91    204                                                                ated on the test set. We set the maximum number of training\n                  Beauty   24     25     25                iterations to 12. To prevent overfitting and reduce training\n                    Sport    44     45     45              time, we employ an early stopping strategy: training is termi-\n      Video KE\n                    Travel    22     23     23             nated if the model‚Äôs performance on the validation set does\n                 Food    22     22     22              not show improvement. For sequence generation tasks, we\n                                                            use a beam search with a beam size of 3, which is consistent\nTable 5: Data splits for our in-house video datasets. The      with the setup in ERM (Yan et al. 2025). We set the number\nnumbers represent the sample counts for each set. ‚ÄúCLS‚Äù       of historical feedback instances and historical prompts used\ndenotes the classification task, ‚ÄúGEN‚Äù denotes the genera-     by UniAPO to 3 and 2, respectively.\ntion task and ‚ÄúKE‚Äù denotes the keyword extraction task.            All primary experiments use GPT-4o (Achiam et al. 2023)\n                                                                as the underlying MLLM across all stages of the UniAPO\n                                                                     pipeline. Prompts are initialized with minimal handcrafted  For our in-house video dataset, we process each video,\n                                                               templates to simulate a low-resource setting, denoted aswhich is approximately 1 to 2 minutes in duration, by uni-\n                                                         ‚ÄúSimple Prompt‚Äù. In additional experiments, we replaceformly sampling 8 frames to represent its visual content. In\n                                                   GPT-4o with QwenVL2.5-72B (Bai et al. 2025) as the pre-addition to this visual information, we provide a rich set of\n                                                                     dictor to evaluate the generalization of UniAPO, while keep-accompanying textual modalities. This includes the video‚Äôs\n                                                               ing the other components unchanged. We also explore set-title, text extracted from stickers, text obtained via Optical\n                                                                     tings with more structured initial prompts, as detailed in rel-Character Recognition (OCR) from the video frames, and\n                                                               evant sections.the audio transcript generated by Automatic Speech Recog-\nnition (ASR).\n                                                System Prompts of UniAPO\nEvaluation Metrics.  For the text-based tasks, we use the\n                                                         System Prompt of Predictor (LT )\nF1 score as our evaluation metric, following the evaluation\nmethodology of ERM (Yan et al. 2025). For the image clas-\n                                                               1  Imagine you are a keyword extractor\nsification task (Meme), we use the F1-micro score as our                                                            for short video ecosystem\nevaluation metric. For the task of video binary classifica-               governance. I provide you the\ntion, we use the F1 score as our evaluation metric. For the               following video consisting of 8\ntask of video keyword extraction, we use the F1 score as our                frames, along with a title,\nevaluation metric. Specifically, let Àúy denote the set of key-               stickers, ocr and asr. I hope\nwords predicted by the model for a given video, and let y be               you can determine whether this\nthe corresponding set of ground-truth keywords. We com-               video meets the following\npute the cosine similarity between predicted keywords and\n\npolicy.                                          prompt is provided, improve it\n 2  The title of the video is: {title},                  only if it‚Äôs simple. For\n        the sticker texts of the video                  complex prompts, enhance\n        is: {stickers}, the ocr of the                  clarity and add missing\n        video is: {ocr}, the asr of                     elements without altering the\n       the video is: {asr}, the frames                  original structure.\n        of the video is: [VIDEO]                     24  - Clarity and Conciseness: Use\n 3  **POLICY**:                                          clear, specific language. Avoid\n 4      <policy>{policy_str}</policy>                     unnecessary instructions or\n 5  Please directly answer the                           bland statements.\n       extracted keywords list. The                 25  - Preserve User Content: If the\n       answer is wrapped with <answer>                  input task or prompt includes\n        and </answer>.                                  extensive guidelines or\n 6                                                       examples, preserve them\n 7  **Output Format:                                     entirely, or as closely as\n 8      <answer>[keyword1, keyword2,                     possible. If they are vague,\n           ...]</answer>                                consider breaking down into sub\n 9  Answer:                                              -steps. Keep any details,\n                                                         guidelines, examples, variables\n                                                         , or placeholders provided by\n                                                         the user.\nSystem Prompt of the Cold Starting                           26  - Constraints:\n                                                           27      - Confirm that the policy\n 1  # Task Overview                                          output adheres to the\n 2                                                           specified format with <\n 3  You are tasked with creating a                           policy> and </policy> tags.\n       refined policy for a zero-shot               28      - *Do not add any output format\n       keyword extraction model that                        .*\n       handles challenging examples.                29      - Do not add any input format\n 4                                                           and explanation of input\n 5  # Input Components                                       content.\n 6                                                         30      - Do not add any examples.\n 7  Original Prompt:                                    31\n 8                                                         32  **Note**: Only after thoroughly\n 9  {user_prompt}                                        verifying your internal\n10                                                       reasoning should you generate\n11  # Objective                                          the final refined output.\n12                                                         33\n13  Generate a detailed and robust                   34  # Output Format and Constraints\n       POLICY that:                                    35  - Final Output Format:\n14                                                         36      - The final refined policy must\n15  - Integrates All Details: Combines                        be wrapped within <policy>\n       every element from the original                       and </policy> tags.\n        prompt without oversimplifying             37\n        or omitting critical                         38  - Word Limit:\n       instructions.                                   39      - The entire policy must not\n16  - Generate a structured policy in                        exceed 200 words.\n       markdown format.                               40\n17                                                         41  # Final Output Template (Example):\n18  # Step-by-Step Reasoning &                        42  <think>\n       Verification Requirement                      43  ... [Your short thinking process]\n19                                                       ...\n20  Before finalizing your answer,                   44  </think>\n       please perform the following                 45  ...\n       using your internal chain-of-                46  **Detailed Verified Refined Policy\n       thought (which must not be                       for Zero-Shot Keyword\n       visible in the final output):                    Extraction in Short Video\n21                                                       Ecosystem Governance**:\n22  - Understand the Task: Grasp the                 47  <policy>\n       main objective, goals,                        48  ... [Your detailed and verified\n       requirements, constraints, and                   refined policy instructions go\n       expected output.                                 here] ...\n23  - Minimal Changes: If an existing               49  </policy>\n\n50                                                       are provided.\n51  Output:                                               35  - Ensure guidance for managing\n                                                         erroneous or missing keywords\n                                                         is explicitly addressed.\n                                                           36\nSystem Prompt of Feedback Generator (LF )                   37  2. Cross-Check for Completeness:\n                                                           38\n 1  # Task Overview                                     39  - Ensure that no critical details\n 2                                                       are missing.\n 3  You are tasked with creating a                   40  - Verify that the policy output\n       refined policy for a zero-shot                   will remain under the 4096-word\n       keyword extraction model that                     limit.\n       handles challenging examples.                41  - Confirm that the policy output\n 4                                                       adheres to the specified format\n 5  # Input Components                                    with <policy> and </policy>\n 6                                                       tags.\n 7  1. Original Prompt:                                42  - Do not add output format in the\n 8                                                       policy output.\n 9  {user_prompt}                                       43  - Do not lossing any other\n10                                                       information in the policy\n11  2. Additional Feedback (from                         output and shorten the policy\n       problematic examples):                           output.\n12                                                         44  - Do not directly add the content\n13  {feedback_str}                                       of the **INPUT** in the policy\n14                                                       output.\n15  3. Observation:                                     45  - Do not add the content of about\n16  The generated key cases based on                     **Continuous Improvement** in\n       the examples, which are                          the policy output.\n       provided under **INPUT**.                     46\n17                                                         47  **Note**: Only after thoroughly\n18  **INPUT**                                            verifying your internal\n19  [CASES_INPUT]                                        reasoning should you generate\n20                                                       the final refined output.\n21  # Objective                                          48\n22                                                         49  # Output Format and Constraints\n23  Generate a detailed and robust                   50  - Final Output Format:\n       POLICY that:                                    51      - The final refined policy must\n24                                                            be wrapped within <policy>\n25  - Integrates All Details: Combines                        and </policy> tags.\n       every element from the original             52\n        prompt and the additional                   53  - Word Limit:\n       feedback without                               54      - The entire policy must not\n       oversimplifying or omitting                          exceed 4096 words.\n       critical instructions.                        55\n26                                                         56  - Clarity and Robustness:\n27  # Step-by-Step Reasoning &                        57      - Do not produce an\n       Verification Requirement                             oversimplified version.\n28                                                           Every critical detail and\n29  Before finalizing your answer,                           instruction must be\n       please perform the following                         integrated to ensure the\n       using your internal chain-of-                        model can reliably handle\n       thought (which must not be                           difficult or noisy examples\n       visible in the final output):                        .\n30                                                         58\n31  1. Break Down Each Requirement:                  59  - Do not directly add the content\n32                                                       of the **INPUT** in the policy\n33  - Verify that the integration of                     output.\n       both the original prompt and                 60\n       the additional feedback is                   61  # Final Output Template (Example):\n       complete.                                        62  <think>\n34  - Confirm that clear instructions               63  ... [Your short thinking process]\n       on handling each data field (                    ...\n       video, predict_label, label)                 64  </think>\n\n65  ...                                                        company-wide.)\"\n66  **Detailed Verified Refined Policy              22     d. If conflicting rules can\n       for Zero-Shot Keyword                               coexist under different\n       Extraction in Short Video                           conditions, keep all and\n       Ecosystem Governance**:                             state the conditions\n67  <policy>                                                explicitly.\n68  ... [Your detailed and verified                  23\n       refined policy instructions go               24  4. **Evaluate unique rules.**\n       here] ...                                        25     a. For rules that appear in only\n69  </policy>                                                one policy, keep them **\n70                                                          only if** they are sensible,\n71  Output:                                                  broadly useful, and not\n                                                            unreasonably specific.\n                                                           26     b. If kept, rewrite for clarity\n                                                            or generality as needed.\nSystem Prompt of the Prompt Evoluter (LE)                   27     c. If removed, record the reason\n                                                             in the \"unique content\"\n 1  You must turn several policy                            subsection of the Reason\n       documents into one consistent                       block wrapped with <think>\n       policy that keeps everything                        and </think>.\n       important, removes redundancies             28\n       , and resolves contradictions.               29  5. **Quality check.**\n 2                                                         30     Ensure the resulting policy:\n 3  INPUTS                                                31     - is logically consistent;\n 4   **Policies**: Multi-line string                 32     - covers every major scenario\n        that contain **all** source                        found in the inputs;\n        policies (two or more).                      33     - contains no redundancy;\n 5     Make sure each policy is clearly             34     - satisfies {addi_restriction};\n           delimited in the input (e.g             35     - is under 4096 words (including\n          . \"### Policy 1\", \"###                            tags).\n          Policy 2\", etc.).                           36\n 6   **Restriction**: Optional extra                 37  FORMATTING RULES\n        constraints that the final                  38\n        policy must obey.                             39  - The the content of Reason block\n 7                                                       is wrapped with <think> and </\n 8  TASKS: PERFORM IN ORDER                              think>.\n 9                                                         40  - The Detailed Merged Policy: is\n10  1. **Extract rules.**                                wrapped with ‚Äò<policy>‚Äò and ‚Äò</\n11     Read every source policy and                      policy>‚Äò.\n          mentally list all of its                  41  - Keep every tag (‚Äò<think>‚Äò, ‚Äò</\n          rules (no output yet).                        think>‚Äò, ‚Äò<policy>‚Äò, ‚Äò</policy\n12                                                       >‚Äò) exactly as shown.\n13  2. **Merge identical / near-                     42  - Do NOT nest tags or add other\n       identical rules.**                               commentary outside the\n14     a. If multiple policies express                   prescribed blocks.\n          the same idea, keep the                   43  - Inside ‚Äò**Policy**‚Äò include only\n          clearest wording.                             the content of final policy\n15     b. Include the merged rule only                   string.\n          once in the final list.                   44\n16                                                         45  OUTPUT FORMAT (return nothing else)\n17  3. **Resolve conflicts.**                         46\n18     a. When two or more rules clash,             47\n           decide which is preferable               48  ### Reason\n          using general practicality                49  <think>\n          and broad applicability.                  50  similarities\n19     b. Keep the chosen rule and                   51  - ...one bullet per merged rule...\n          discard the others.                        52\n20     c. Immediately after the kept                 53  conflicts\n          rule, add a short bracketed               54  - ...one bullet per conflict\n          note explaining why it was                    handled, showing the original\n          chosen, e.g.                                  rules and the resolution\n21        \"(Preferred over Policy 3 & 2                  rationale...\n              because it applies                     55\n\n56  unique content                                       identical rules.**\n57  - ...one bullet per unique rule,                 14     a. If multiple policies express\n       marked \"KEPT\" (with rewritten                       the same idea, keep the\n       text) or \"REMOVED\" (with reason                     clearest wording.\n       )...                                              15     b. Include the merged rule only\n58  </think>                                                once in the final list.\n59                                                         16\n60  ...Detailed Merged Policy:                        17  3. **Resolve conflicts.**\n61  ### Merged Policy:                                 18     a. When two or more rules clash,\n62  <policy>[...Detailed Merged Policy                       decide which is preferable\n       ...]</policy>                                       using general practicality\n63                                                          and broad applicability.\n64                                                         19     b. Keep the chosen rule and\n65  INPUTS:                                                 discard the others.\n66  **Policies**: {policy_str}                        20     c. Immediately after the kept\n67  **Restriction**: {addi_restriction}                     rule, add a short bracketed\n68                                                          note explaining why it was\n69  Output:                                                 chosen, e.g.\n                                                           21        \"(Preferred over Policy 3 & 2\n                                                                because it applies\nPrompt of Cases for Feedback Generation                               company-wide.)\"\n                                                           22     d. If conflicting rules can\n                                                            coexist under different\n 1  The title of No.{idx} video is: {                                                            conditions, keep all and\n       title}, the sticker texts of No\n                                                            state the conditions\n       .{idx} video is: {stickers},\n                                                            explicitly.\n       the ocr of No.{idx} video is: {\n                                                           23\n       ocr}, the asr of No.{idx} video\n                                                           24  4. **Evaluate unique rules.**        is: {asr}, the label of No.{\n                                                           25     a. For rules that appear in only\n       idx} video is: {label}, the\n                                                             one policy, keep them **       prediction of No.{idx} video is\n                                                            only if** they are sensible,       : {pred}, the frames of No.{idx\n                                                             broadly useful, and not\n       } video is:\n                                                            unreasonably specific.\n                                                           26     b. If kept, rewrite for clarity\n                                                            or generality as needed.\nSystem Prompt of the Prompt Evoluter (LE)                   27     c. If removed, record the reason\n                                                             in the \"unique content\"\n 1  You must turn several policy                            subsection of the Reason\n       documents into one consistent                       block wrapped with <think>\n       policy that keeps everything                        and </think>.\n       important, removes redundancies             28\n       , and resolves contradictions.               29  5. **Quality check.**\n 2                                                         30     Ensure the resulting policy:\n 3  INPUTS                                                31     - is logically consistent;\n 4   **Policies**: Multi-line string                 32     - covers every major scenario\n        that contain **all** source                        found in the inputs;\n        policies (two or more).                      33     - contains no redundancy;\n 5     Make sure each policy is clearly             34     - satisfies {addi_restriction};\n           delimited in the input (e.g             35     - is under 4096 words (including\n          . \"### Policy 1\", \"###                            tags).\n          Policy 2\", etc.).                           36\n 6   **Restriction**: Optional extra                 37  FORMATTING RULES\n        constraints that the final                  38\n        policy must obey.                             39  - The the content of Reason block\n 7                                                       is wrapped with <think> and </\n 8  TASKS: PERFORM IN ORDER                              think>.\n 9                                                         40  - The Detailed Merged Policy: is\n10  1. **Extract rules.**                                wrapped with ‚Äò<policy>‚Äò and ‚Äò</\n11     Read every source policy and                      policy>‚Äò.\n          mentally list all of its                  41  - Keep every tag (‚Äò<think>‚Äò, ‚Äò</\n          rules (no output yet).                        think>‚Äò, ‚Äò<policy>‚Äò, ‚Äò</policy\n12                                                       >‚Äò) exactly as shown.\n13  2. **Merge identical / near-                     42  - Do NOT nest tags or add other\n\ncommentary outside the                           the examples, which are\n       prescribed blocks.                               provided under **INPUT**.\n43  - Inside ‚Äò**Policy**‚Äò include only              17\n       the content of final policy                  18  **INPUT**\n       string.                                          19  [CASES_INPUT]\n44                                                         20\n45  OUTPUT FORMAT (return nothing else)             21  # Objective\n46                                                         22\n47                                                         23  Generate a detailed and robust\n48  ### Reason                                           POLICY that:\n49  <think>                                               24\n50  similarities                                         25  - Integrates All Details: Combines\n51  - ...one bullet per merged rule...                   every element from the original\n52                                                        prompt and the additional\n53  conflicts                                            feedback without\n54  - ...one bullet per conflict                         oversimplifying or omitting\n       handled, showing the original                    critical instructions.\n       rules and the resolution                      26\n       rationale...                                    27  # Step-by-Step Reasoning &\n55                                                       Verification Requirement\n56  unique content                                      28\n57  - ...one bullet per unique rule,                 29  Before finalizing your answer,\n       marked \"KEPT\" (with rewritten                    please perform the following\n       text) or \"REMOVED\" (with reason                  using your internal chain-of-\n       )...                                             thought (which must not be\n58  </think>                                             visible in the final output):\n59                                                         30\n60  ...Detailed Merged Policy:                        31  1. Break Down Each Requirement:\n61  ### Merged Policy:                                 32\n62  <policy>[...Detailed Merged Policy              33  - Verify that the integration of\n       ...]</policy>                                    both the original prompt and\n63                                                       the additional feedback is\n64                                                       complete.\n65  INPUTS:                                               34  - Confirm that clear instructions\n66  **Policies**: {policy_str}                           on handling each data field (\n67  **Restriction**: {addi_restriction}                  video, predict_label, label)\n68                                                       are provided.\n69  Output:                                               35  - Ensure guidance for managing\n                                                         erroneous or missing keywords\n                                                         is explicitly addressed.\n                                                           36\nSystem Prompt of Feedback Generator (LF )                   37  2. Cross-Check for Completeness:\n                                                           38\n 1  # Task Overview                                     39  - Ensure that no critical details\n 2                                                       are missing.\n 3  You are tasked with creating a                   40  - Verify that the policy output\n       refined policy for a zero-shot                   will remain under the 4096-word\n       keyword extraction model that                     limit.\n       handles challenging examples.                41  - Confirm that the policy output\n 4                                                       adheres to the specified format\n 5  # Input Components                                    with <policy> and </policy>\n 6                                                       tags.\n 7  1. Original Prompt:                                42  - Do not add output format in the\n 8                                                       policy output.\n 9  {user_prompt}                                       43  - Do not lossing any other\n10                                                       information in the policy\n11  2. Additional Feedback (from                         output and shorten the policy\n       problematic examples):                           output.\n12                                                         44  - Do not directly add the content\n13  {feedback_str}                                       of the **INPUT** in the policy\n14                                                       output.\n15  3. Observation:                                     45  - Do not add the content of about\n16  The generated key cases based on                     **Continuous Improvement** in\n\nthe policy output.                               to sports from the provided\n   46                                                       video. The assessment must\n   47  **Note**: Only after thoroughly                      consider the following\n          verifying your internal                          components:\n          reasoning should you generate                 2\n          the final refined output.                      3  1. \\textbf{Title}: Analyze the\n   48                                                       title for any explicit mentions\n   49  # Output Format and Constraints                       or implications of sports-\n   50  - Final Output Format:                               related terms or activities.\n   51      - The final refined policy must              4  2. **Sticker Texts**: Evaluate text\n               be wrapped within <policy>                   present in stickers within the\n               and </policy> tags.                          video for sports-specific\n   52                                                       language or references.\n   53  - Word Limit:                                        5  3. **OCR (Optical Character\n   54      - The entire policy must not                     Recognition)**: Examine all\n              exceed 4096 words.                           visual text extracted from the\n   55                                                       video frames for keywords\n   56  - Clarity and Robustness:                            associated with sports,\n   57      - Do not produce an                              including sports names,\n              oversimplified version.                      terminologies, or related terms\n              Every critical detail and                    .\n              instruction must be                        6  4. **ASR (Automatic Speech\n              integrated to ensure the                     Recognition)**: Process the\n              model can reliably handle                    spoken content within the video\n              difficult or noisy examples                   to identify sports-related\n              .                                            mentions or themes.\n   58                                                          7  5. **Video Frames**: Contextually\n   59  - Do not directly add the content                    interpret visual elements in\n          of the **INPUT** in the policy                   the frames (e.g., players,\n          output.                                          equipment, locations) to verify\n   60                                                        relevance to sports.\n   61  # Final Output Template (Example):               8\n   62  <think>                                                9  The extracted keywords must be:\n   63  ... [Your short thinking process]               10  - **Specific**: Pertinent to sports\n          ...                                               (e.g., \"soccer,\" \"basketball\")\n   64  </think>                                             .\n   65  ...                                                   11  - **Concise**: Avoiding redundancy\n   66  **Detailed Verified Refined Policy                   or overly broad terms.\n          for Zero-Shot Keyword                         12  - **Contextually Relevant**:\n          Extraction in Short Video                        Reflecting the actual theme or\n          Ecosystem Governance**:                          content presented in the video.\n   67  <policy>                                             13\n   68  ... [Your detailed and verified                  14  Exclude generic or unrelated terms\n          refined policy instructions go                   that do not directly relate to\n          here] ...                                        sports. Perform a holistic\n   69  </policy>                                            evaluation of all input\n   70                                                       components to ensure accurate\n   71  Output:                                              keyword extraction.\n\n\nResults of UniAPO in Sport Keyword Extraction            Generated Feedback\n\n     Initial Prompt                                                               1  **Regulation 1: Contextual\n                                                           Integration of Visual and Audio\n    1  Keywords about sports.                                Cues}**\n                                                               2\n                                                               3  Develop a mechanism to integrate\n    Input Prompt                                               visual cues from the frames\n                                                           with audio cues from the ASR.\n    1  The keyword extractor must identify                  If the frames depict a specific\n           and extract concise, relevant,                   sport or activity and the ASR\n           and contextually accurate                       mentions related terms (e.g., \"\n          keywords specifically related                    pass the ball\" in a soccer\n\nscene), the system should                        recurring visual elements and\n       prioritize extracting keywords                   actions. Each frame should be\n       that combine both sources of                     evaluated independently, and\n       information. For example, in                     recurring sports-related\n       Video No.2, where soccer                         elements (e.g., players,\n       visuals and \"pass the ball\"                      equipment, settings) should be\n       audio cues were present, the                     flagged and cross-referenced\n       keywords should include \"Soccer                  across all frames. For example,\n        Pass\" or \"Soccer Interaction.\"                   if multiple frames show\n        This ensures a holistic                         individuals skateboarding in a\n       evaluation of the video content                  skatepark, keywords like \"\n       .                                                Casual Skateboarding Footage\"\n 4                                                       or \"Skatepark Activity\" should\n 5                                                       be extracted to capture the\n 6  **Regulation 2: Refine extraction                    broader context. This approach\n       from video information by                        ensures that implicit and\n       leveraging implicit contextual                   repeated sports activities are\n       clues.**                                         not overlooked.\n 7\n 8  - The extractor must analyze the\n       video information (OCR and ASR\n       data) to identify implicit                   Optimized Prompt\n       references to sports activities\n        or themes. For instance, if                  1  ### **Zero-Shot Sports-Related\n       OCR data mentions \"Naval Cadet                   Keyword Extraction Policy**\n       Entrance Exam\" and the video                  2\n       frames show individuals running              3  As a keyword extractor for short\n       , keywords like \"1,000-meter                     video ecosystem governance,\n       Run\" and \"Naval Cadet Physical                   your role is to accurately and\n       Exam\" should be extracted to                     comprehensively identify sports\n       reflect the specific context.                    -related keywords. Analyze all\n 9  - When OCR or ASR data contains                      provided video components,\n       ambiguous or fragmented text,                    including the title, stickers,\n       the extractor must cross-                        OCR, ASR, and video frames,\n       reference with video frames to                   using the detailed guidelines\n       clarify the context and extract                  below to ensure precise,\n        relevant keywords. For example                  contextually relevant, and\n       , if OCR mentions \"LED Ball\"                     specific keyword extraction.\n       and the video frames show a                   4\n       glowing basketball, the keyword              5  ---\n        \"LED Basketball\" should be                   6\n       included.                                         7  ### **Data Component Analysis**\n10  - Avoid extracting keywords that                  8\n       are unrelated to the sports                   9  1. **Title**:\n       theme, even if they appear in                10     - Extract sports-related\n       OCR or ASR data. For example,                       keywords directly or infer\n       terms like \"celebration\" or \"                       them based on context.\n       product\" should only be                       11     - Prioritize specific sports\n       included if they are directly                       disciplines, events, or\n       tied to the sports context                          activities.\n       presented in the video.                       12     - Avoid extracting unrelated or\n11  - This regulation ensures that                          generic terms unless tied\n       keywords derived from video                         explicitly to a sports\n       information are contextually                        context.\n       accurate and aligned with the                13\n       visual content.                                 14  2. **Stickers**:\n12                                                         15     - Analyze sticker texts for\n13                                                          sports themes or terminology\n14  **Regulation 3: Frame-by-Frame                          .\n       Contextual Analysis**                         16     - Extract only relevant sports-\n15                                                          related stickers, excluding\n16  Implement a systematic frame-by-                        unrelated content.\n       frame analysis to identify                   17     - Cross-reference sticker data\n\nwith visual cues to confirm               43\n          relevance.                                   44  2. **Relevance**:\n18                                                         45     - Extract only sports-related\n19  3. **OCR (Optical Character                             keywords. Exclude irrelevant\n       Recognition)**:                                      or overly broad terms.\n20     - Identify visible text in video             46     - Ensure keywords reflect the\n           frames for sports-related                       core theme or activity\n          terms, brand names, team                         depicted in the video.\n          names, locations, or events.             47\n21     - Validate OCR keywords using                 48  3. **Implied Keywords**:\n          visual evidence (e.g.,                     49     - Infer implied keywords when\n          equipment, uniforms).                            strong visual and contextual\n22     - Avoid speculative terms;                            evidence supports their\n          derive only from explicit                        inclusion.\n          textual or visual evidence.               50     - Example: If frames depict \"\n23                                                          Mbappe\" and \"Haaland\" in a\n24  4. **ASR (Automatic Speech                              competitive setting, infer \"\n       Recognition)**:                                     Mbappe vs Haaland\" or \"\n25     - Extract spoken words referring                     penalty shootout.\"\n           to sports activities,                     51     - Avoid speculative terms;\n          athlete names, teams,                            derive implied keywords from\n          tournaments, or events.                           visual and contextual cues.\n26     - Cross-check ASR data with                   52\n          visual content to ensure                  53  4. **Contextual Integration**:\n          accuracy and avoid                         54     - Cross-reference textual data (\n          overgeneralization.                              Title, Stickers, OCR, ASR)\n27     - Exclude speculative terms                          with visual cues from Video\n          unsupported by visual or                         Frames to validate or\n          textual evidence.                                enhance extracted keywords.\n28                                                         55     - Example: If frames show a\n29  5. **Video Frames**:                                    basketball court and ASR\n30     - Observe frames for sports-                         mentions \"basketball,\"\n          related actions, objects, or                     prioritize extracting \"\n           symbols to supplement                           basketball game\" or \"\n          textual data.                                    basketball match.\"\n31     - Identify specific sports                    56     - Ensure extracted keywords\n          activities, equipment,                           align with the video‚Äôs\n          uniforms, or event settings.                     overarching theme.\n32     - Use sequential frame analysis              57\n          to detect recurring patterns             58  5. **Prioritization of Visual Cues\n           or implied activities (e.g                   **:\n          ., gameplay, events).                      59     - Use visual evidence (e.g.,\n33     - Infer implied keywords (e.g.,                      equipment, player actions,\n          \"penalty shootout,\" \"boxing                      event banners) to extract\n          match\") based on visual cues                     contextually relevant\n           and frame progression.                          keywords.\n34                                                         60     - Example: Frames showing\n35  ---                                                     athletes with a basketball\n36                                                          should lead to the keyword \"\n37  ### **Key Extraction Criteria**                         basketball,\" even if textual\n38                                                           data is ambiguous.\n39  1. **Specificity**:                                61     - Leverage sequential frame\n40     - Extract concise, meaningful,                       analysis to detect recurring\n          and specific keywords (e.g.,                      patterns or implied themes\n           \"soccer,\" \"basketball,\" \"                       (e.g., continuous gameplay).\n          Olympics\").                                  62\n41     - Avoid overly broad terms                    63  6. **Avoiding Duplicates**:\n          unless explicitly tied to a               64     - Consolidate extracted keywords\n          sports context.                                   to avoid duplicates unless\n42     - Use detailed descriptors when                      contextually significant.\n          supported by evidence (e.g.,             65     - Example: Use \"soccer match\"\n           \"100-meter dash\" instead of                     rather than repeating \"\n           \"running\").                                     soccer\" and \"match\"\n\nseparately.                                      contextually relevant\n66                                                          keywords.\n67  7. **Error Handling and Missing                  90     - Example: If frames depict a\n       Data**:                                             billiards table with players\n68     - If components (e.g., Stickers,                      engaged, infer \"billiards\n           OCR, ASR) are absent or                         skills\" or \"cue sports.\"\n          uninformative, rely on                     91\n          visual analysis (e.g., video             92  4. **Handling Ambiguous or Noisy\n           frames).                                     Data**:\n69     - Example 1: If frames show a                 93     - When OCR or ASR data is\n          skatepark and individuals                        missing or uninformative,\n          performing tricks, extract \"                     rely primarily on visual\n          skateboarding,\" \"skatepark,\"                     cues.\n           or \"skateboarding tricks.\"               94     - Example: If frames depict a\n70     - Example 2: If frames depict a                      boxing ring with athletes,\n          racing track with vehicles,                      extract \"boxing match\" and \"\n          infer \"racing event\" or \"                        amateur boxing.\"\n          amateur racing.\"                            95     - Avoid overgeneralization by\n71     - Avoid speculative keywords;                        focusing on unique visual\n          derive terms from explicit                       identifiers (e.g., team\n          or strongly implied evidence                     names, player numbers).\n          .                                              96\n72                                                         97  5. **Prioritize Frame-Based\n73  ---                                                  Contextual Evidence**:\n74                                                         98     - Example: If frames depict a\n75  ### **Advanced Techniques for                           dirt track with sprint cars\n       Challenging Scenarios**                             and stickers mention \"URC\n76                                                          Sprints,\" infer \"sprint car\n77  1. **Sequential Frame Analysis**:                       racing\" and \"amateur racing\n78     - Analyze the progression of                         .\"\n          frames to identify recurring             99\n           patterns or implied                      100  6. **Avoid Overgeneralization**:\n          activities.                                 101     - Focus on unique visual\n79     - Example 1: If consecutive                          identifiers (e.g., equipment\n          frames depict players                            , player actions, event\n          passing a soccer ball, infer                     settings).\n           \"soccer passing\" or \"                    102     - Example: Extract \"goblet squat\n          amateur soccer play.\"                            \" rather than broad terms\n80     - Example 2: If frames show                          like \"glutes workout\" unless\n          repeated actions in a boxing                      explicitly supported.\n           ring, infer \"boxing match\"              103\n          or \"amateur boxing.\"                      104  ---\n81     - Use frame progression to                   105\n          validate implied keywords (e            106  ### **Output Guidelines**\n          .g., \"penalty shootout\").                107\n82                                                        108  - Submit extracted keywords as a\n83  2. **Enhanced ASR Integration**:                     comma-separated list enclosed\n84     - Cross-validate ASR data with                    within ‚Äò<answer>‚Äò and ‚Äò</answer\n          visual evidence to ensure                     >‚Äò tags.\n          accuracy.                                   109  - Ensure the keyword list is\n85     - Example: If ASR mentions \"                      directly derived from video\n          Ronaldinho\" and frames                        components by integrating\n          depict a soccer match,                        textual and visual data\n          extract \"Ronaldinho,\" \"                       seamlessly.\n          soccer,\" and \"soccer match.\"            110  - Avoid unnecessary commentary or\n86     - Avoid speculative terms                         explanations in the output.\n          unsupported by visual                     111\n          evidence.                                   112  ---\n87                                                        113\n88  3. **Contextual Integration of                  114  By systematically analyzing textual\n       Visual and Textual Cues**:                        and visual data, prioritizing\n89     - Combine visual and textual                      specificity and relevance, and\n          data to infer detailed,                       addressing challenging examples\n\nwith advanced integration                    6\n          techniques, this refined policy              7\n           ensures accurate and reliable                8  **Regulation #2: Prioritize\n          keyword extraction for short                     Contextual Relevance Derived\n          video ecosystem governance.                      from Video Frames**\n                                                               9\n                                                              10  - Enhance the extraction process by\nResults of EvolPrompt* in Sport Keyword                        explicitly requiring the\nExtraction                                                  integration of visual cues from\n                                                            video frames. For example:\n     Initial Prompt                                               11\n                                                              12  - Identify specific actions,\n    1  Keywords about sports.                               objects, or environments\n                                                           depicted in the frames (e.g., \"\n                                                           dirt track racing\" in Example\n   Optimized Prompt                                           1, \"billiard table\" in Example\n                                                           2).\n    1  Identifying, pinpointing, and                    13\n          compiling key and essential                  14  - Translate these visual elements\n          terms related to sports.                         into precise, contextually\n                                                           relevant keywords that align\n                                                           with the policy categories (e.g\nResults of ERM* in Sport Keyword Extraction                   ., \"Fan Support\" in Example 3).\n                                                              15\n     Initial Prompt                                               16  - Implement a rule that prioritizes\n                                                            specific and actionable\n    1  Keywords about sports.                               keywords over broad or generic\n                                                           terms. For instance, instead of\n                                                            extracting \"sports humor,\"\n   Generated Feedback                                         focus on extracting specific\n                                                           actions like \"pass\" or \"\n    1  *Regulation #1: Enhance Specificity                  challenge\" as seen in Example\n           in Keyword Extraction for                       4.\n          Sports Types and Content Types               17\n          **                                                18\n    2                                                         19  **Regulation #3: Enforce Contextual\n    3  - When extracting keywords related                    Keyword Combination for\n          to sports types, ensure                          Precision**\n          granularity by prioritizing                  20\n          specific terms over general                  21  - Keywords must be contextually\n          ones. For instance, instead of                   combined to capture the full\n          \"Shooting Sports Highlights,\"                    meaning of the activity or\n          extract precise terms such as \"                  event depicted in the video.\n          Air Pistol Shooting\" or \"                        This includes:\n          Turkish Athlete\" when                         22\n          identifiable in the video                     23  - Combining location and event\n          frames or ASR.                                   elements (e.g., \"Naval Cadet\n    4                                                       Entrance Exam\" with \"1,000-\n    5  - For content types, include                         meter Run\").\n          subcategories or contextual                  24      - Merging comparative or\n          elements (e.g., \"celebration,\"                       descriptive elements with\n          \"stadium\") that are visually or                      the activity (e.g., \"\n           narratively significant within                      Basketball Prototype vs\n           the video. Use visual cues                          Product\").\n          from the video frames (e.g.,                 25      - Including relevant\n          player actions, equipment) and                       descriptors such as \"\n          audio descriptions to refine                         amateur,\" \"professional,\"\n          keyword selection.\\n- Cross-                         or \"casual\" only when they\n          reference extracted keywords                         are clearly supported by\n          with the ASR and video frames                        the video content.\n          to ensure alignment with the                 26      - Avoid overgeneralization by\n          video\\‚Äôs context and avoid                           ensuring that extracted\n          overgeneralization.                                  keywords are directly tied\n\nto the specific sports type                  ., \"goal scoring,\" \"kickflip\")\n            or context shown in the                     and central objects (e.g., \"\n           video. For example, avoid                    boxing ring,\" \"billiards table\n           generic terms like \"amateur                  \") visible across multiple\n            sports\" when \"amateur                       frames.\n           skateboarding\" or \"casual                16  - **Specificity in Actions**:\n           basketball\" is more                          Replace generic terms with\n           appropriate.                                 specific descriptions. For\n                                                         example:\n                                                           17    - Replace \"sports\" with \"horse\n                                                           racing\" if horses and a\nOptimized Prompt                                             racetrack are depicted.\n                                                           18    - Replace \"fitness\" with \"front\n 1  ### **1. Objective**                                   dumbbell raises\" if the video\n 2  This policy provides a robust                           shows this specific exercise\n       framework for extracting                           .\n       precise, contextually relevant,             19  - **Temporal Progression**:\n        and comprehensive keywords                      Consider sequences of actions\n       from short video content. It                     across frames to derive\n       integrates visual and textual                    comprehensive keywords. For\n       data to accurately represent                     example, if the video shows a\n       the video‚Äôs activities, objects                  goal being scored followed by a\n       , themes, and events. The goal                    celebration, include both \"\n       is to ensure specificity, avoid                  goal scoring\" and \"celebration\n        redundancy, minimize errors,                    .\"\n       and handle challenging or                     20  - **Consistency**: Cross-reference\n       ambiguous examples effectively.                  keywords across all frames to\n 3                                                       ensure consistency in\n 4  ---                                                  representing the video‚Äôs core\n 5                                                       theme. For example, if a boxing\n 6  ### **2. Core Principles of Keyword                   ring is visible in all frames,\n        Extraction**                                     ensure \"boxing ring\" is\n 7  - Extracted keywords must                            included.\n       accurately reflect the video‚Äôs               21\n       central themes, dynamic actions             22  #### **3.2. ASR (Audio Speech\n       , objects, and events as                         Recognition)**\n       depicted in video frames and                 23  - **Complement Visual Analysis**:\n       supported by textual data (ASR,                  Use ASR data to identify spoken\n        OCR, stickers, and title).                       phrases that add context or\n 8  - Emphasize specificity by                           confirm visual findings. For\n       prioritizing detailed,                           example:\n       actionable, and contextually                 24    - If the ASR mentions \"kickflip\"\n       relevant keywords. Use generic                     during a skateboarding video,\n       terms (e.g., \"sports\") only as                      ensure \"kickflip\" is\n       a fallback when specificity is                     included as a keyword.\n       not possible.                                   25  - **Prioritize Relevance**: Extract\n 9  - Consolidate similar or                              keywords from ASR phrases that\n       overlapping keywords into                         directly describe actions,\n       precise terms unless distinct                    events, or objects relevant to\n       variations are explicitly                        the video frames. Disregard\n       emphasized in the video‚Äôs                        irrelevant or noisy ASR outputs\n       content.                                          unless supported by visual or\n10                                                       other textual data.\n11  ---                                                   26\n12                                                         27  #### **3.3. OCR (Optical Character\n13  ### **3. Data Integration and                        Recognition)**\n       Analysis**                                       28  - **Contextual Integration**:\n14  #### **3.1. Video Frames**                           Extract keywords from text\n15  - **Primary Source**: Use video                      visible in the video, such as\n       frames as the primary source                     banners, signs, or equipment\n       for identifying specific                         labels. For example:\n       actions, objects, and events.                29    - If OCR identifies \"URC SPRINTS\n       Prioritize dynamic actions (e.g                    ,\" include \"sprint cars\" or \"\n\ndirt track racing.\"                            video‚Äôs content.\n30    - If OCR reads \"Pool Tournament               51  - Derive context-specific keywords\n         2023,\" include \"pool\" and \"                    from ambiguous data based on\n         billiards.\"                                    the most likely interpretation\n31  - **Cross-Validation**: Validate                     of available information.\n       OCR findings against visual and             52\n        ASR data to ensure consistency             53  #### **4.3. Avoiding Redundancy**\n        and accuracy. Disregard                      54  - Avoid repetitive keywords unless\n       misleading or irrelevant OCR                     they describe distinct aspects\n       outputs unless corroborated by                   of the content. For example:\n       other data sources.                            55    - Include both \"goal scoring\" and\n32                                                          \"celebration\" if these are\n33  #### **3.4. Stickers and Title**                       separate actions depicted in\n34  - **Stickers**: Use sticker text to                    the video.\n        provide additional context.                 56    - Avoid duplicating \"billiards\"\n       For example:                                       and \"pool\" unless both terms\n35    - If a sticker reads \"Game On!\"                      are contextually significant.\n         and the video depicts a                     57\n         basketball court, include \"                58  ---\n         basketball\" or \"game\" as                   59\n         keywords.                                     60  ### **5. Error Handling and Edge\n36  - **Title**: Use the title as a                      Cases**\n       guide for overall context but                61  #### **5.1. Erroneous Data**\n       ensure alignment with                         62  - Exclude irrelevant or misleading\n       observable actions, objects, or                  keywords caused by noisy ASR or\n        events in the video frames and                   OCR outputs unless supported\n        textual data.                                   by other contextual data. For\n37                                                       example:\n38  ---                                                   63    - Ignore an OCR output of \"SALE\"\n39                                                         if it is unrelated to the\n40  ### **4. Specificity, Coverage, and                    video‚Äôs theme.\n        Redundancy**                                   64  - Use visual data as the primary\n41  #### **4.1. Enhancing Specificity**                  source to confirm or reject\n42  - Use detailed and actionable                        noisy textual inputs.\n       keywords rather than generic                 65\n       terms. For example:                            66  #### **5.2. Missing Data**\n43    - Replace \"sports\" with \"                       67  - If specific data fields (e.g.,\n         skateboarding\" if                              ASR, OCR) are unavailable, rely\n         skateboarding is depicted.                      more heavily on the available\n44    - Replace \"fitness\" with \"                         sources while maintaining\n         deadlift workout\" if the                       specificity and contextual\n         video shows a deadlift                         relevance.\n         exercise.                                     68  - If visual data lacks specificity,\n45  - Consolidate overlapping keywords                    use textual data (ASR, OCR,\n       unless distinct variations are                   stickers, and title) to infer\n       emphasized. For example:                         likely themes or activities.\n46    - Merge \"Amateur Arm Wrestling\"               69\n         and \"Arm Wrestling\" into \"Arm             70  #### **5.3. Fallback Strategy**\n          Wrestling\" unless the                      71  - Use broad keywords (e.g., \"sports\n         amateur nature is central to                   \") only when visual and textual\n         the theme.                                      data lack the specificity\n47                                                       needed for detailed\n48  #### **4.2. Comprehensive Coverage                   descriptions.\n       **                                                72\n49  - Analyze all input components (                 73  ---\n       video frames, ASR, OCR,                       74\n       stickers, and title) to ensure               75  ### **6. Quality Assurance**\n       no critical keywords are missed             76  #### **6.1. Iterative Testing**\n       .                                                 77  - Conduct iterative testing across\n50  - Cross-reference extracted                          diverse and challenging\n       keywords across data sources to                  examples to ensure adherence to\n        verify that they                                 the policy.\n       comprehensively represent the                78  - Update the policy regularly based\n\non observed errors and                          .\"\n        emerging edge cases to improve              115  - **Extracted Keywords**: ‚Äò<answer\n        robustness.                                      >[billiards, pool, eight-ball\n 79                                                       ]</answer>‚Äò\n 80  #### **6.2. Final Review**\n 81  - Perform a final review to ensure:\n 82    - All keywords are relevant,\n          specific, and non-redundant.\n 83    - Keywords comprehensively\n          represent the video‚Äôs content\n          .\n 84    - Justification exists for any\n          broad or fallback keywords\n          used.\n 85\n 86  ---\n 87\n 88  ### **7. Output Requirements**\n 89  - Present keywords as a concise,\n        comma-separated list.\n 90  - Example format:\n 91    - ‚Äò<answer>[keyword1, keyword2,\n          ...]</answer>‚Äò\n 92\n 93  ---\n 94\n 95  ### **8. Illustrative Examples**\n 96  #### **Example 1**:\n 97  - **Video Frames**: Show\n        individuals performing \"front\n        dumbbell raises.\"\n 98  - **ASR**: Mentions \"dumbbell\n        workout.\"\n 99  - **OCR**: Reads \"Strength Training\n        .\"\n100  - **Stickers**: Include \"Fitness\n        Goals.\"\n101  - **Extracted Keywords**: ‚Äò<answer\n        >[front dumbbell raises,\n        strength training, dumbbell\n        workout]</answer>‚Äò\n102\n103  #### **Example 2**:\n104  - **Video Frames**: Depict a\n        skateboarding activity in a\n        skatepark.\n105  - **ASR**: Mentions \"kickflip.\"\n106  - **OCR**: Displays \"Skateboarding\n        Championship.\"\n107  - **Stickers**: Say \"Extreme Sports\n        .\"\n108  - **Extracted Keywords**: ‚Äò<answer\n        >[skateboarding, kickflip,\n        skatepark]</answer>‚Äò\n109\n110  #### **Example 3**:\n111  - **Video Frames**: Show a\n        billiards table with players.\n112  - **ASR**: Includes \"Eight-ball,\n        your turn.\"\n113  - **OCR**: Reads \"Pool Tournament\n        2023.\"\n114  - **Stickers**: Include \"Game Night\n\nReferences                           Gardent,  C.;  Shimorina,  A.;  Narayan,  S.; and  Perez-\n                                                                    Beltrachini, L. 2017.   Creating training corpora for nlgAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;\n                                                              micro-planning. In ACL, 179‚Äì188.Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;\nAnadkat, S.; et al. 2023.  Gpt-4 technical report.  arXiv      He, J.; Rungta, M.; Koleczek, D.; Sekhon, A.; Wang, F. X.;\npreprint arXiv:2303.08774.                                and Hasan, S. 2024. Does prompt formatting have any im-\n                                                                pact on llm performance? arXiv preprint arXiv:2411.10541.\nAgarwal, E.; Magazine, R.; Singh, J.; Dani, V.; Ganu, T.;\n                                                                    Javaid, H. 2023. Meme Dataset. Kaggle. Kaggle dataset;and Nambi, A. 2025. PromptWizard: Optimizing Prompts\n                                                                 Twitter data collected via web scraping.via Task-Aware, Feedback-Driven Self-Evolution. In ACL,\n19974‚Äì20003.                                                Keskar, A.; Perisetla, S.; and Greer, R. 2025.  Evaluating\n                                                          multimodal vision-language model prompting strategies for\nB¬®ack, T.; and Schwefel, H.-P. 1993. An overview of evolu-\n                                                                   visual question answering in road scene understanding. In\ntionary algorithms for parameter optimization. Evolutionary\n                                                CVPR, 1027‚Äì1036.\ncomputation, 1(1): 1‚Äì23.\n                                                           Lamott, M.; Weweler, Y.-N.; Ulges, A.; Shafait, F.; Krechel,\nBai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang,                                                                D.; and Obradovic, D. 2024.  LAPDoc: Layout-Aware\nK.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl                                                         Prompting for Documents. In International Conference on\ntechnical report. arXiv preprint arXiv:2502.13923.                                                    Document Analysis and Recognition, 142‚Äì159.\nCao, S.; Yin, Y.; Huang, L.; Liu, Y.; Zhao, X.; Zhao, D.; and      Lee, M.; Cho, S.; Lee, J.; Yang, S.; Choi, H.; Kim, I.-J.;\nHuang, K. 2023. Efficient-vqgan: Towards high-resolution      and Lee, S. 2025.  Effective SAM Combination for Open-\nimage generation with efficient vision transformers.   In      Vocabulary Semantic Segmentation.   In CVPR, 26081‚Äì\nCVPR, 7368‚Äì7377.                                         26090.\nChen, B.; Zhang, Z.; Langren¬¥e, N.; and Zhu, S. 2023. Un-      Lee, S.-H.; Wang,  J.; Zhang,  Z.; Fan, D.; and  Li, X.\nleashing the potential of prompt engineering in large lan-      2024. Video token merging for long video understanding.\nguage models: a comprehensive review.   arXiv preprint      NeurIPS, 37: 13851‚Äì13871.\narXiv:2310.14735.                                                  Li, W.; Wang, X.; Li, W.; and Jin, B. 2025. A survey of\nChen,  J.; Xiao,  S.; Zhang,  P.; Luo, K.; Lian, D.; and      automatic prompt engineering: An optimization perspective.\nLiu, Z. 2024. BGE M3-Embedding: Multi-Lingual, Multi-      arXiv preprint arXiv:2502.11560.\nFunctionality, Multi-Granularity Text Embeddings Through       Li,  Y.-J.; Zhang, X.; Wan, K.; Yu,  L.; Kale, A.; and\nSelf-Knowledge Distillation. CoRR.                           Lu, X. 2024a.  Prompt-Guided Mask Proposal for Two-\n                                                            Stage Open-Vocabulary Segmentation.    arXiv  preprintChen, Y.; Zhong, H.; Li, Y.; and Yang, Z. 2025.  Uni-\n                                                            arXiv:2412.10292.Code2: Cascaded Large-scale Codebooks for Unified Mul-\ntimodal Understanding and Generation.   arXiv preprint       Li, Z.; Xu, Q.; Zhang, D.; Song, H.; Cai, Y.; Qi, Q.; Zhou,\narXiv:2506.20214.                                                R.; Pan, J.; Li, Z.; Tu, V.; et al. 2024b. GroundingGPT: Lan-\n                                                        guage Enhanced Multi-modal Grounding Model. In ACL.Cui, W.; Zhang, J.; Li, Z.; Sun, H.; Lopez, D.; Das, K.; Ma-\nlin, B. A.; and Kumar, S. 2025.  Automatic Prompt Opti-       Lin, Y.; Sun, J.; Cheng, Z.-Q.; Wang, J.; Liang, H.; Cheng,\nmization via Heuristic Search: A Survey.  arXiv preprint       Z.; Dong, Y.; He, J.-Y.; Peng, X.; and Hua, X.-S. 2025. Why\narXiv:2502.18746.                             We Feel: Breaking Boundaries in Emotional Reasoning with\n                                                        Multimodal Large Language Models. In CVPR, 5196‚Äì5206.\nDavari, M.; Garg, U.; Cai, W.; and Belilovsky, E. 2025.\n                                                                 Liu, S.; Chen, C.; Qu, X.; Tang, K.; and Ong, Y.-S. 2024.Rethinking Prompt Optimization: Reinforcement, Diversi-\n                                                          Large language models as evolutionary optimizers. In 2024fication, and Migration in Blackbox LLMs. arXiv preprint\n                                                 IEEE Congress on Evolutionary Computation (CEC), 1‚Äì8.arXiv:2507.09839.\n                                                       IEEE.\nDo, X. L.; Dinh, D.; Nguyen, N.-H.; Kawaguchi, K.; Chen,\n                                                        Mohanty, A.; Parthasarathy, V. B.; and Shahid, A. 2025. The\nN.; Joty, S.; and Kan, M.-Y. 2025.  What Makes a Good\n                                                             Future of MLLM Prompting is Adaptive: A Comprehen-\nNatural Language Prompt? In ACL, 5835‚Äì5873.\n                                                                    sive Experimental Evaluation of Prompt Engineering Meth-\nDu, Y.; Wei, F.; Zhang, Z.; Shi, M.; Gao, Y.; and Li, G.      ods for Robust Multimodal Performance.  arXiv preprint\n2022a.   Learning to prompt for open-vocabulary object      arXiv:2504.10179.\ndetection with vision-language model.  In CVPR, 14084‚Äì                                                              Mollas, I.; Chrysopoulou, Z.; Karlos, S.; and Tsoumakas, G.\n14093.                                                           2022. ETHOS: a multi-label hate speech detection dataset.\nDu, Y.; Wei, F.; Zhang, Z.; Shi, M.; Gao, Y.; and Li, G.     Complex & Intelligent Systems, 8(6): 4663‚Äì4678.\n2022b.   Learning to prompt for open-vocabulary object       Pryzant, R.; Iter, D.; Li, J.; Lee, Y. T.; Zhu, C.; and Zeng,\ndetection with vision-language model.  In CVPR, 14084‚Äì     M. 2023. Automatic Prompt Optimization with‚Äù Gradient\n14093.                                                    Descent‚Äù and Beam Search. In EMNNLP.\nFernando, C.; Banarse, D. S.; Michalewski, H.; Osindero, S.;      Qu, X.; Gou, G.; Zhuang, J.; Yu, J.; Song, K.; Wang, Q.; Li,\nand Rockt¬®aschel, T. 2024. Promptbreeder: Self-Referential       Y.; and Xiong, G. 2025.  Proapo: Progressively automatic\nSelf-Improvement via Prompt Evolution. In ICML, 13481‚Äì      prompt optimization for visual classification.   In CVPR,\n13544.                                                  25145‚Äì25155.\n\nRafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Er-      prompt optimization: the benefit of memory in exemplar-\nmon, S.; and Finn, C. 2023. Direct preference optimization:      guided reflection. In ACL.\nYour language model is secretly a reward model. NeurIPS,      Yang, C.; Wang, X.; Lu, Y.; Liu, H.; Le, Q. V.; Zhou, D.;\n36: 53728‚Äì53741.                                        and Chen, X. 2023. Large language models as optimizers.\nRamnath, K.; Zhou, K.; Guan, S.; Mishra, S. S.; Qi, X.;       In ICLR.\nShen, Z.; Wang, S.; Woo, S.; Jeoung, S.; Wang, Y.; et al.      Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\n2025. A systematic survey of automatic prompt optimiza-       Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliberate\ntion techniques. arXiv preprint arXiv:2502.16923.              problem solving with large language models, 2023. URL\nSaleem, S.; Asim, M. N.; Zulfiqar, S.; and Dengel, A.       https://arxiv. org/abs/2305.10601, 3: 1.\n2025. The Evolution of Natural Language Processing: How      Yu, S.; Tang, C.; Xu, B.; Cui, J.; Ran, J.; Yan, Y.; Liu, Z.;\nPrompt Optimization and Language Models are Shaping the     Wang, S.; Han, X.; Liu, Z.; et al. 2024. VisRAG: Vision-\nFuture. arXiv preprint arXiv:2506.17700.                      based Retrieval-augmented Generation on Multi-modality\nShao, H.; Qian, S.; Xiao, H.; Song, G.; Zong, Z.; Wang, L.;      Documents. In ICLR.\nLiu, Y.; and Li, H. 2024. Visual cot: Advancing multi-modal      Yuksekgonul, M.; Bianchi, F.; Boen, J.; Liu, S.; Huang, Z.;\nlanguage models with a comprehensive dataset and bench-       Guestrin, C.; and Zou, J. 2024. Textgrad: Automatic‚Äù differ-\nmark for chain-of-thought reasoning. NeurIPS, 37: 8612‚Äì       entiation‚Äù via text. arXiv preprint arXiv:2406.07496.\n8642.                                                         Zhang, D.; Yu, Y.; Dong, J.; Li, C.; Su, D.; Chu, C.; and Yu,\nSong, S.; Li, X.; Li, S.; Zhao, S.; Yu, J.; Ma, J.; Mao, X.;      D. 2024a. Mm-llms: Recent advances in multimodal large\nZhang, W.; and Wang, M. 2025. How to bridge the gap      language models. arXiv preprint arXiv:2401.13601.\nbetween modalities: Survey on multimodal large language                                                         Zhang, J.; Xiang, J.; Yu, Z.; Teng, F.; Chen, X.; Chen, J.;\nmodel. TKDE.                                                         Zhuge, M.; Cheng, X.; Hong, S.; Wang, J.; et al. 2024b.\nSpiess, C.; Vaziri, M.; Mandel, L.; and Hirzel, M. 2025. Au-      Aflow: Automating agentic workflow generation.  arXiv\ntopdl: Automatic prompt optimization for llm agents. arXiv       preprint arXiv:2410.10762.\npreprint arXiv:2504.04365.                                                         Zhang, Y.; Zhang, K.; Li, B.; Pu, F.; Setiadharma, C. A.;\nSuzgun, M.; Scales, N.; Sch¬®arli, N.; Gehrmann, S.; Tay, Y.;      Yang, J.; and Liu, Z. 2024c.  Worldqa: Multimodal world\nChung, H. W.; Chowdhery, A.; Le, Q. V.; Chi, E. H.; Zhou,      knowledge in videos through long-chain reasoning.  arXiv\nD.; et al. 2023. Challenging BIG-Bench Tasks and Whether       preprint arXiv:2405.03272.\nChain-of-Thought Can Solve Them. In ACL (Findings).                                                         Zhang, Y.; Zhou, K.; and Liu, Z. 2023. What makes good ex-\nTang, X.; Wang, X.; Zhao, W. X.; Lu, S.; Li, Y.; and Wen,      amples for visual in-context learning? NeurIPS, 36: 17773‚Äì\nJ.-R. 2025. Unleashing the potential of large language mod-      17794.\nels as prompt optimizers: Analogical analysis with gradient-                                                         Zhang, Y.; Zhou, K.; and Liu, Z. 2024.  Neural prompt\nbased model optimizers. In AAAI, volume 39, 25264‚Äì25272.                                                                   search. TPAMI.\nUesato, J.; Kushman, N.; Kumar, R.; Song, F.; Siegel, N.;                                                         Zhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and\nWang, L.; Creswell, A.; Irving, G.; and Higgins, I. 2022.                                                         Smola, A. 2024d. Multimodal Chain-of-Thought Reasoning\nSolving math word problems with process-and outcome-                                                                     in Language Models. TMLR, 2024.\nbased feedback. arXiv e-prints, arXiv‚Äì2211.\n                                                          Zhao, H. H.; Zhou, P.; Gao, D.; Bai, Z.; and Shou, M. Z.\nWang, C.; Luo, W.; Dong, S.; Xuan, X.; Li, Z.; Ma, L.; and                                                           2024. Lova3: Learning to visual question answering, asking\nGao, S. 2025a.  Mllm-tool: A multimodal large language                                                       and assessment. NeurIPS, 37: 115146‚Äì115175.\nmodel for tool agent learning. In WACV, 6678‚Äì6687.\n                                                         Zheng, H. S.; Mishra, S.; Chen, X.; Cheng, H.-T.; Chi, E. H.;\nWang, W. Y. 2017.   ‚Äù  liar,  liar pants on fire‚Äù: A new                                                             Le, Q. V.; and Zhou, D. 2024. Take a Step Back: Evoking\nbenchmark dataset for fake news detection. arXiv preprint                                                         Reasoning via Abstraction in Large Language Models.  In\narXiv:1705.00648.                                                       ICLR.\nWang, Z.; Chen, B.; Yue, Z.; Wang, Y.; Qiao, Y.; Wang,                                                        Zhou, P.; Peng, X.; Song, J.; Li, C.; Xu, Z.; Yang, Y.; Guo,\nL.; and Wang, Y. 2025b.  VideoChat-A1: Thinking with                                                                      Z.; Zhang, H.; Lin, Y.; He, Y.; et al. 2025. OpenING: A\nLong Videos by Chain-of-Shot Reasoning.  arXiv preprint                                                        Comprehensive Benchmark for Judging Open-ended Inter-\narXiv:2506.06097.                                                              leaved Image-Text Generation. In CVPR, 56‚Äì66.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;                                                        Zhou, Y.; Muresanu, A.  I.; Han, Z.; Paster, K.; Pitis, S.;\nChi, E.; Le, Q. V.; Zhou, D.;  et  al. 2022.   Chain-of-                                                        Chan, H.; and Ba, J. 2022.  Large language models are\nthought prompting elicits reasoning in large language mod-                                                           human-level prompt engineers. In ICLR.\nels. NeurIPS, 35: 24824‚Äì24837.\nWu, Y.; Gao, Y.; Zhu, B. B.; Zhou, Z.; Sun, X.; Yang, S.;\nLou, J.-G.; Ding, Z.; and Yang, L. 2024. StraGo: Harnessing\nStrategic Guidance for Prompt Optimization.  In EMNLP,\n10043‚Äì10061.\nYan, C.; Wang, J.; Zhang, L.; Zhao, R.; Wu, X.; Xiong, K.;\nLiu, Q.; Kang, G.; and Kang, Y. 2025. Efficient and accurate",
"headers": [
"arXiv:2508.17890v1  [cs.CV]  25 Aug 2025",
"UniAPO: Unified Multimodal Automated Prompt Optimization",
"Qipeng Zhu",
", Yanzhe Chen",
", Huasong Zhong",
"Yan Li",
",",
"Jie Chen",
", Zhixin Zhang",
", Junping Zhang",
", Zhenheng Yang",
"Introduction",
"Related Work",
"Preliminaries",
"Methodology",
"Experiment",
"Conclusion",
"Appendix",
"References",
"Prompt Engineering for MLLMs",
"Automatic Prompt Optimization (APO)",
"Problem Formulation and Baseline",
"Core Challenges",
"Overall Architecture",
"E-step: Multimodal Feedback Generation",
"M-step: Multi-modal Prompt Optimization",
"Comparision Study",
"Experimental Setting",
"Analysis Study",
"Ablation Study",
"Case Study",
"Details of Experimental Setting",
"System Prompts of UniAPO",
"Results of UniAPO in Sport Keyword Extraction",
"Results of EvolPrompt* in Sport Keyword",
"Extraction",
"Results of ERM* in Sport Keyword Extraction",
"E-step",
"M-step"
],
"tables": [
"|Feedback ùêπùë° E-step<br>ùíüùë°<br>ùëíùëüùëüùëúùëü<br>Feedback Memoy Prompt with the TopK(‚ãÖ)<br>‚Ñ≥ùë° Error Cases<br>ùêπ<br>Retrieval(‚ãÖ) ‚Ñí (‚ãÖ)<br>ùêπ<br>(‚ãÖ)<br>ùêπ ùëôùë° ùëú+ ùëõ1 ùëî ‚Ñí ùê∏(‚ãÖ) ùêπ ùë†ùë° ‚Ñé+ ùëú1 ùëüùë° Add<br>Filter(‚ãÖ)<br>ùêπùë°+1<br>M|Col2|Prompt ùëÉùë°<br>ùëÉùë°+1Eval(‚ãÖ) ùëÉùë°+1<br>2 1<br>3 4<br>‚Ñí (‚ãÖ) (‚ãÖ)<br>ùê∏<br>Add<br>ùëÉ ùë†ùë° ‚Ñé+ ùëú1 ùëüùë° ùëÉ ùëôùë° ùëú+ ùëõ1 ùëî<br>‚Ñí (‚ãÖ) TopK(‚ãÖ)<br>ùëÉ<br>Prompt with Key ‚Ñ≥ùë°<br>Examples and FeedbackPrompt MùëÉemoy<br>ùíü ‚àíùíüùë°<br>ùë°ùëüùëéùëñùëõ ùëíùëüùëüùëúùëü<br>-step|Prompt ùëÉùë°|\n|---|---|---|---|\n|_Prompt with the_<br>_Error Cases_<br>**Feedback Memoy**<br>**Retrieval(**‚ãÖ**)**<br>ùíüùëíùëüùëüùëúùëü<br>ùë°<br>**E-step**<br>**M**<br>_Feedback_ ùêπùë°<br>‚Ñíùêπ(‚ãÖ)<br>**Add** (‚ãÖ)<br>**TopK(**‚ãÖ**)**<br>ùêπùëôùëúùëõùëî<br>ùë°+1<br>ùêπùë†‚Ñéùëúùëüùë°<br>ùë°+1<br>‚Ñíùê∏(‚ãÖ)<br>**Filter(**‚ãÖ**)**<br>ùêπùë°+1<br>‚Ñ≥ùêπ<br>ùë°|**M**|**-step**|**-step**|",
"|Col1|E-step|\n|---|---|\n|ùíüùëíùëüùëüùëúùëü<br>ùë°|ùíüùëíùëüùëüùëúùëü<br>ùë°|",
"|ùëÉùë°|+1E|val(‚ãÖ)|ùëÉùë°+1|\n|---|---|---|---|\n|2 1<br>|2 1<br>|2 1<br>|2 1<br>|",
"|Col1|ùëÉùë°+1<br>ùë†‚Ñéùëúùëüùë°|\n|---|---|\n||‚ÑíùëÉ(‚ãÖ)|",
"|Col1|Add (‚ãÖ)|\n|---|---|",
"|B<br>80<br>70<br>60<br>50<br>40<br>30<br>0 2 4<br>Traini|eauty S<br>80<br>Score<br>70<br>F1<br>Test<br>60<br>50<br>6 8 10 0 2 4<br>ng Iteration Trainin|port|\n|---|---|---|\n|0<br>2<br>4<br>Traini<br>30<br>40<br>50<br>60<br>70<br>80<br> <br>|6<br>8<br>10<br>ng Iteration<br>eauy<br>0<br>2<br>4<br>Trainin<br>50<br>60<br>70<br>80<br>Test F1 Score<br>|6<br>8<br>10 1<br>g Iteration|",
"|Simple Prompt Complex Prompt<br>74.7 75.2<br>75<br>59.2 62.2 59.1 60.3<br>50<br>36.7<br>28.7<br>25<br>0<br>o w o w o w o w<br>4 Q 4 Q 4 Q 4 Q<br>@ @ @ @ @ @ @ @<br>e st T e st T e st T e st T e st T e st T e st T e st<br>Init Settings Opt Settings|Col2|x Prompt<br>75.2<br>60.3|Col4|\n|---|---|---|---|\n|est @ 4o<br>Test @ Qw<br>Test @ 4o<br>Test @ Qw<br>0<br>25<br>50<br>75<br>36.7<br>28.7<br>74.7<br>59.2<br>Simple Prompt<br>Test @ 4o<br>Test @ Qw<br>Test @ 4o<br>Test @ Qw<br>62.2<br>59.1<br>75.2<br>60.3<br>Complex Prompt<br>Init Settings<br>Opt Settings||||",
"|36.7|28.7|Col3|Col4|\n|---|---|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2508.17890v1.pdf"
}