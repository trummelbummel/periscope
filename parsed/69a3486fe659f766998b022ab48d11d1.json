{
"text": "MARS: Multi-Agent Adaptive Reasoning with Socratic Guidance\n                               for Automated Prompt Optimization\n\n                      Jian Zhang*1,2, Zhangqi Wang*1,3, Haiping Zhu1,2†, Kangda Cheng4,\n                         Kai He5, Bo Li1,3, Qika Lin5†, Jun Liu1,3, Erik Cambria6\n                              1School of Computer Science and Technology, Xi’an Jiaotong University, China\n                            2MOE KLINNS Lab, Xi’an Jiaotong University, China\n                  3Shaanxi Province Key Laboratory of Big Data Knowledge Engineering, Xi’an Jiaotong University, China\n                     4School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China\n                       5Saw Swee Hock School of Public Health, National University of Singapore, Singapore\n                           6College of Computing and Data Science, Nanyang Technological University, Singapore\n                                         zhangjian062422@stu.xjtu.edu.cn, zhuhaiping@xjtu.edu.cn2025\n\n                               Abstract\n\n             Large language models (LLMs)  typically  operate  in  aNov\n              question-answering paradigm, where the quality of the in-\n12       putOptimizationprompt critically(APO) affectsaims totheovercomeresponse.theAutomatedcognitivePromptbiases\n              of manually crafted prompts and explore a broader prompt\n              design space. However, existing APO methods often suf-\n                fer from rigid template structures and inefficient exploration\n               in the prompt space. To  this end, we propose a Multi-\n            Agent Adaptive Reasoning with Socratic guidance frame-\n            work (MARS) for APO. MARS consists of five comple-[cs.CL]      mentary agents and formulates the optimization process as\n             a Partially Observable Markov Decision Process (POMDP),         Figure 1: Three different prompts along with their corre-\n              enabling adaptive prompt refinement through explicit state                                                                    sponding responses for the word sorting task.\n             modeling and interactive feedback. Specifically, a Planner\n              agent generates flexible optimization trajectories, a Teacher-\n               Critic-Student triad engages in Socratic-style dialogue to iter-\n               atively optimize the prompt based on pseudo-gradient signals        2025a) (Shen et al. 2025) (Yan et al. 2025). Consequently,\n               in the text space, and a Target agent executes the prompt in         the quality of the prompt is of critical importance, lead-\n            downstream tasks to provide performance feedback. MARS         ing to wide interest in Automated Prompt Optimization\n               integrates reasoning, feedback, and state transition into a uni-       (APO) (Pryzant et al. 2023). As shown in Figure 1, we pro-\n                fied hidden-state evolution process, improving both the effec-         vide LLMs with three different inputs for the word sorting\n               tiveness and interpretability of optimization. Extensive exper-          task: a zero-shot prompt, a Chain of Thought (CoT) prompt,\n             iments on multiple datasets demonstrate that MARS outper-                                                                and our optimized prompt. The responses are produced in\n             forms existing APO methods in terms of optimization perfor-\n                                                                      a markedly distinct way. Specifically, the zero-shot prompt\n            mance, search efficiency, and interpretability.\n                                                                               incorrectly identifies the alterate as the more common word\n                                                                                alternate. However, the task requires faithfully preserving\n        Code — https://github.com/exoskeletonzj/MARS                                                                            the given sequence of words rather than correcting them.arXiv:2503.16874v2                                                          With the CoT prompt, the sorting remains incorrect because\n                        Introduction                            the LLM does not fully grasp the sorting task and the word\n                                                                       sequence. In contrast, our optimized prompt produces the\n         Large language models (LLMs) such as GPT-4 (Achiam                                                                              correct answer. This is because our prompt includes specific\n            et al. 2023) and Deepseek-R1 (Guo et al. 2025) provide                                                                          requirements, such as maintaining the original letter casing\n          robust support for thousands of natural language process-                                                                and specifying the sorting method.\n          ing tasks (Yuan et al. 2025). By providing a natural lan-                                                                       Thus, it is evident that APO can lead to improved per-\n         guage prompt that includes instructions and a task de-                                                                  formance in downstream tasks. As shown in Figure 2, re-\n            scription, LLMs can quickly adapt and respond (Lin et al.                                                                          cent studies (Zhou et al. 2022; Xu, Banburski-Fahey, and\n             *These authors contributed equally.                                  Jojic 2023; Wang et al. 2023) have explored prompt opti-\n              †Corresponding Author                                       mization by generating multiple candidates combined with\n           Copyright © 2026, Association for the Advancement of Artificial       diverse search strategies, while others (Yang et al. 2024a; Ye\n            Intelligence (www.aaai.org). All rights reserved.                          et al. 2023) focus on designing sophisticated meta-prompts\n\nFigure 2: Comparison of APO strategies. Generation-search and meta-prompt. Multi-Agent Adaptive Reasoning enables dy-\nnamic, collaborative reasoning. Right: With GPT-4o, MARS outperforms all baselines on three benchmarks.\n\n\nto guide optimization. Despite these advances, two key is-     POMDP, where the hidden state represents the latent rea-\nsues remain: the limited flexibility of fixed prompt tem-      soning state of the Student agent. Through multi-agent in-\nplates, and the inefficiency of prompt space exploration.           teractions and performance feedback from a Target agent,\n  The first issue is the limited flexibility of fixed tem-    MARS approximates a pseudo-gradient trajectory in the dis-\nplates. Prior works (Yang et al. 2024a; Ye et al. 2023) of-       crete prompt space, progressively refining the prompt to-\nten rely on meta-prompts, which are predefined optimization     ward an optimal solution. Our contributions are three-fold:\ntemplates that cannot be dynamically adapted to different       • This work is the first to introduce a multi-agent architec-\ntasks. Unlike domains such as event extraction (Zhang et al.       ture with POMDP modeling for APO. It proposes MARS,\n2024b) or text-to-symbol generation (Xu et al. 2024), where      which enables hidden-state reasoning and adaptive planning\nfixed templates suffice due to the task’s structural regularity,      through agent collaboration.\nAPO requires more adaptability. Rigid templates may intro-       • A Teacher-Critic-Student Socratic dialogue mechanism\nduce biases or fail to capture task-specific nuances, resulting        is designed to enable interpretable, iterative prompt refine-\nin suboptimal performance when applied to diverse or com-     ment via a gradient-inspired optimization trajectory.\nplex scenarios.                                           • We demonstrate the effectiveness and generalizability\n  The second issue is the inefficiency of prompt space       of MARS through extensive experiments on both general\nexploration. Several approaches (Zhou et  al. 2022; Xu,      and domain-specific benchmarks, and validate the inter-\nBanburski-Fahey, and Jojic 2023; Wang et al. 2023) adopt a       pretability of its optimization process.\ngeneration-search strategy, where a set of candidate prompts\nis first generated and then refined using local search tech-                  Methodology\nniques. However, this approach typically performs only lo-\ncal exploration around the initial candidates, without suffi-    MARS comprises two main modules:  (i) a  high-level\nciently covering the broader prompt space. As a result, the      Planner that generates task-specific optimization trajecto-\noptimization may converge prematurely or overlook better-        ries, and (ii) a Teacher-Critic-Student triad that performs\nperforming prompts, limiting overall effectiveness.                Socratic-style iterative refinement. The overall architecture\n                                                                                is shown in Figure 3, with the complete workflow detailed  To  this  end, we  propose  a  Multi-Agent  Adaptive\n                                                                     in Algorithm 1. This section introduces: (1) the APO taskReasoning with Socratic guidance framework (MARS) for\n                                                       and its POMDP formulation, (2) the Planner design, (3) theAPO. MARS consists of five complementary agents and\n                                                                  gradient-inspired Socratic dialogue mechanism, and (4) theformulates the optimization process as a Partially Observ-\n                                                               evaluation-feedback loop via the Target agent.able Markov Decision Process (POMDP), enabling adaptive\nprompt refinement through explicit state modeling and in-\nteractive feedback. Functionally, to address the first chal-     Task Formulation and POMDP Modeling\nlenge, MARS introduces a Planner agent that generates      Given a task-specific Target model Mtar, the goal of APO is\ntask-specific optimization trajectories, allowing prompts to       to iteratively refine an initial prompt p0 to an optimal version\nbe flexibly adapted to diverse task requirements. To tackle      p∗that maximizes performance on a downstream dataset\nthe second  challenge, MARS employs a  Socratic-style   D = {(x, y)}. A training subset Dtrain ⊂D guides the op-\nTeacher-Critic-Student dialogue mechanism, which itera-       timization, while Dtest is used for evaluation. The objective\ntively guides prompt refinement. This module enables ef-      can be formalized as:\nfective exploration of the prompt space by simulating a\ngradient-inspired optimization process, while also promot-          p∗= arg max X   f (πtar(x; p), y) ,       (1)\ning interpretability. The overall process is modeled as a                           p   (x,y)∈Dtest\n\nFigure 3: The overall architecture of the MARS model. It consists of five LLM agents. The Planner agent that autonomously\ngenerates task-specific optimization trajectories, and a Teacher-Critic-Student Socratic dialogue mechanism that iteratively\nrefines prompts, with the evaluation and iterative refinement process guided by feedback from the Target agent.\n\n\nwhere πtar(x; p) denotes the model output conditioned on x      where q(z|g, x) captures task semantics, and P(ST | z, p0)\nand prompt p, and f is a task-specific metric (e.g., accuracy,      models the trajectory conditioned on latent intent and ini-\nBLEU).                                                                         tial prompt. This hierarchical formulation induces structured\n  To capture the sequential, partially observable nature of      plans over latent space S, guiding local agent decisions un-\nthe optimization, we model APO as a Partially Observable      der global coherence and improving adaptability over static\nMarkov Decision Process (POMDP) defined by:                  templates.\n                   ⟨S, A, T , R, O⟩,\n                                                        Socratic Prompt Refinement as Joint Policy\nwhere: - S: latent state space, representing the internal rea-\n                                                    Optimizationsoning state of the Student agent; - A: action space, com-\nprising instructional signals (e.g., questions, critiques) from      Prompt refinement  in  discrete language space  presents\nTeacher and Critic; - T  : S × A →S: transition dynam-      unique challenges due to its non-differentiability, high vari-\nics, updating student states; - O  : S →P: observation       ance, and semantic ambiguity. To address these issues,\nfunction, mapping hidden states to prompts; - R(s, a) =    MARS employs a structured Socratic dialogue mechanism\nf(πtar(x; O(s)), y): reward function, based on performance      involving three collaborative agents—Teacher (πt), Critic\nof the generated prompt.                                              (πc), and Student (πs)—each fulfilling a complementary role\n  This formulation allows MARS to perform gradient-       in exploring and improving prompts through guided interac-\ninspired prompt refinement in a partially observable, dis-        tion. This framework transforms prompt optimization into\ncrete text space. Via iterative multi-agent reasoning and      an interpretable, policy-driven reasoning process grounded\nfeedback, the system progressively transitions from p0 to p∗.       in pedagogical principles.\n                                                       At each refinement step i, given a sub-goal sti ∈ST,\nOptimization Trajectory Planning                          the Teacher proposes a Socratic-style question qi to stimu-\nAs illustrated in Figure 3, MARS begins with a Planner        late reasoning, based on the prior prompt pi−1. The Critic\nagent that initiates the prompt optimization process.              then assesses its clarity, relevance, and coherence, produc-\nPlanner. Given task goal g, input x ∈Dtrain, and initial      ing feedback ci to revise or validate the proposed direction.\nprompt p0, the Planner decomposes the optimization into       Finally, the Student responds by updating its internal state\na sequence of sub-goals:                                   and generating a new prompt pi. This process is formalized\n                                                                        as:     ST = [st1, st2, . . . , stn] = πplan(g, x, p0),      (2)\n                                                                                        qi = πt(sti, pi−1),\nwhere πplan is the planning policy that adaptively generates\n                                                                                         ci = πc(qi),an optimization trajectory.                                                                                                                               (4)\n  To formalize πplan, we introduce a latent planning variable                    pi = πs((qi, ci), pi−1),\nz ∈Z, and model trajectory generation as:                                       si ∼T (si−1, (qi, ci)),   oi = pi.\n πplan(g, x, p0) = arg max Ez∼q(z|g,x) [log P(ST | z, p0)] ,                ST                                 Each agent performs a partial update to the joint optimiza-\n                                                           (3)       tion process: Teacher drives semantic direction, Critic pro-\n\nvides quality control, and Student synthesizes the final out-\nput.\n\nContext-Aware Interaction.  To improve reasoning con-\nsistency and avoid step-wise myopia, each agent conditions\nnot only on the current sub-goal and prompt, but also on the\ndialogue history H<i = {(qj, cj, pj)}i−1j=1. The full context-\naware behavior is given by:\n                 qi = πt(sti, pi−1, H<i),\n                  ci = πc(qi, H<i),                        (5)\n                pi = πs((qi, ci), pi−1, H<i).\nBy attending to prior reasoning steps, the system forms co-\nherent, memory-informed trajectories across iterations.\n\nJoint Optimization Objective.  We define the multi-agent      Figure 4: A specific illustration of a Teacher-Critic-Student\npolicy as Π = {πt, πc, πs}, and optimize it jointly to max-       Socratic guidance dialogue pattern. The case shows the fifth\nimize task performance while ensuring interpretability and       step optimization iteration.\nalignment with sub-goals:\n            \"          n              #\n max E(x,y)∼D R(Π) −λ X Lalign((qi, ci), sti)   ,  (6)      Adaptive Termination.  To ensure efficient convergence\n   Π                            i=1                            and prevent over-refinement, we adopt an adaptive early\n                                                              stopping criterion based on marginal reward improvement.\nwhere R(Π) denotes the cumulative reward from the Tar-\n                                                   The gain between two consecutive iterations is defined as:\nget agent, and Lalign penalizes semantic drift from intended\noptimization goals.                                                  ∆R(t) = R(t) −R(t−1).               (9)\n  This tri-agent structure enables interpretable, step-wise\nrefinement of prompts through structured reasoning and lo-     The refinement continues only if:\ncalized feedback, offering both flexibility and transparency\nin discrete prompt optimization.                                         ∆R(t) > δ,   t < I,               (10)\n  Proposition 1 (Socratic Policy Improvement Bound).                                                       where δ is a minimum improvement threshold, and I is the\nLet Π = {πt, πc, πs} denote the joint policy, and suppose                                              maximum number of allowed iterations.\nthe Socratic signal ai = (qi, ci) induces expected advantage                                                              This  iterative control mechanism enforces a form of\n¯Ai > 0 over the prior state si−1. Then, under bounded vari-                                                          performance-aware policy halting under the POMDP frame-\nance σ2, the cumulative improvement over n steps satisfies:                                                          work. It ensures that MARS focuses on high-impact updates\n                           n                             while avoiding excessive computation on marginally benefi-\n        E[R(pn)] −R(p0) ≥ X   ¯Ai −σ2    ,       (7)        cial revisions. As a result, the system adaptively determines\n                                 2λ                   the optimal stopping point based on observable task perfor-                              i=1\n  where λ is the local Lipschitz constant of the reward sur-      mance.\nface.                                                         Proposition 2 (Monotonic Reward Stability). Assume\n  This provides a lower bound on improvement, formally     R(p) is λ-Lipschitz and each step satisfies ∥pi −pi−1∥≤ε.\nlinking guidance signal quality to reward trajectory.           Then the reward trajectory {R(pi)} satisfies:\n  Derivation is in Appendix A.1.                                                                       |R(pi+1) −R(pi)| ≤λε.\nEvaluation and Iteration                                    In particular, if R(pi) < R(pi+1) for some i, then im-\nUpon completing the Socratic refinement trajectory, the fi-      provement is bounded and monotonic. This result guaran-\nnal prompt pℓ= pn—produced through successive dialogue       tees bounded gain/loss and motivates early stopping under\ntransitions from latent state s0 to sℓ—is evaluated by the       stable improvement.\nTarget agent πtar on the held-out test set Dtest. The evalua-         Derivation is in Appendix A.2.\ntion provides an external signal to measure the effectiveness\nof the entire optimization trajectory:                                   Experiments\n          R(t) = X   f  πtar(x; p(t)ℓ), y   ,        (8)       In this section, we present extensive experiments conducted\n                                                     on 12 general task datasets and 5 domain-specific datasets.                       (x,y)∈Dtest\n                                         We begin by introducing the datasets and hyperparameters,\nwhere f(·) is a task-specific scoring function (e.g., accuracy,      followed by the main experimental results. A detailed anal-\nBLEU, F1), and p(t)ℓ  denotes the final prompt obtained at       ysis of the efficiency of the proposed framework is also pro-\niteration t. This scalar reward serves as the global perfor-       vided.\nmance metric, closing the loop between prompt generation       More detailed descriptions of the tasks and datasets can be\nand task-level effectiveness.                                 found in Appendix B. The abbreviations used for the tasks in\n\nModels   B.E   D.QA    F.F.    G.S.   R.N.    S.U.   C.B.   C.M.   E.E.   W.H.   H.A.   M.T.   Avg.\n\n   Origin    74.70   51.41   52.20   43.37   59.84   60.24   82.52   69.77   63.89   73.73   66.22   81.55   64.95\n  CoT(ZS)   80.32   54.22   59.44   47.39   67.07   67.87   83.91   73.25   74.31   76.27   68.47   84.98   69.79\n  CoT(FS)   81.93   57.43   66.26   49.40   70.68   72.29   86.71   76.74   79.17   78.81   72.07   90.99   73.54\n   APE     83.53   61.85   61.04   51.41   77.51   74.70   88.11   75.58   69.44   82.20   75.68   87.98   74.09\n  ProTeGi   83.93   63.86   62.65   52.21   80.32   76.71   90.91   78.49   73.61   84.75   77.48   90.56   76.29\n  OPRO    86.34   66.67   63.45   53.81   83.13   82.73   93.70   83.14   77.01   86.44   79.73   92.70   79.07\n   PE2     87.95   65.46   63.86   54.62   84.34   75.90   93.01   81.40   76.39   88.14   81.08   93.56   78.81\n\n    Ours     93.17   71.89   74.70   59.44   90.36   87.95   97.90   86.05   84.03   93.22   85.59   97.00   85.11\n\nTable 1: In the performance comparison across 12 general tasks, we carefully select 6 representative subtasks from both BBH\nand MMLU, two commonly used evaluation benchmarks, to comprehensively assess MARS’s performance in diverse general-\ntask settings. The evaluation results of these subtasks indicate that MARS surpasses all existing baseline methods.\n\n\nAlgorithm 1: MARS Optimization Procedure                                      Chinese        Math  Law    Avg.\n                                                             Models\n                                                                                A.S.    U.R.P.   CL.M.  GSM.   L.A. 1: Input: Dataset D, initial prompt p0, threshold δ, max iterations\n    I                                                                  Origin    56.25   48.89    57.14    67.07   23.14   50.50\n 2: Output: Optimized prompt p∗                                                           CoT(ZS)   59.38   53.33    61.90    70.26   30.57   55.09\n 3: Planner: Generate sub-goal trajectory ST = {st1, . . . , stn}\n                                                            CoT(FS)   65.63   57.78    66.67    77.54   35.81   60.69\n 4: Initialize p(1)0  ←p0, R(0) ←0                                                 APE     65.63   62.22    71.43    74.81   29.69   60.76\n 5: for iteration t = 1 to I do\n 6:     for step i = 1 to n do                       // Generate question        ProTeGi   68.75   66.67    76.19    77.47   31.88   64.19\n 7:       Teacher generates question qi ←πt(sti, p(t)i−1)           OPRO    71.88   73.33    80.95    81.56   31.44   67.83\n 8:        repeat                                           PE2     75.00   77.78    76.19    83.46   34.50   69.39\n 9:             Critic evaluates qi & returns feedback ci ←πc(qi)                                           MARS    81.25   84.44    85.71    89.22   38.42   75.81\n10:           Teacher revises qi if ci is unsatisfactory\n11:         until Socratic quality is satisfied\n12:        Set ai ←(qi, ci)                                     Table 2: Performance comparison on three types of domain-\n13:        Student updates p(t)i  ←πs(ai, p(t)i−1)                        specific tasks: Chinese, law, and mathematics. The Chinese\n14:    end for                                          domain consists of three datasets, while the law and mathe-\n15:     Let p(t)ℓ  ←p(t)n                                          // Final prompt      matics domains each have one dataset.\n16:     Target evaluates reward\n17:    R(t) = P(x,y)∈Dtest f(πtar(x; p(t)ℓ), y)\n18:       if R(t) −R(t−1) < δ then                           Hyperparameters and Evaluation Protocol.  We adopt\n19:       break                                //Early stopping     deepseek-v2.5-1210 (Guo et al. 2025) as the primary\n20:    end if                                             backbone LLM for all APO tasks. The generation temper-\n21: end for                                                        ature is set to 0.6 to balance creativity and coherence. We\n22: return p∗←p(t)ℓ                                              configure the maximum number of optimization iterations\n                                                                as I = 10, with an early stopping threshold of δ = 0.01\n                                                           based on accuracy improvement. To enhance efficiency, each\n                                                                  assess-adjust cycle is limited to a single revision per step.Tables 1 and 2, along with their full names and dataset de-\n                                                                 Final evaluation is performed using accuracy, computed byscriptions, are provided in Appendix B. Baseline methods\n                                                        comparing the model prediction ypred with the ground truthand additional experimental details are introduced in Ap-\n                                                                     label y.pendix C.\n\nExperimental Settings                          Main Results\nDatasets.  We select a total of 17 datasets covering both    MARS enhances the average performance across diverse\ngeneral-purpose and domain-specific tasks. Specifically, we      task types.  The experimental results in Table 1 and Ta-\nuse 6 tasks from the Big-Bench Hard (BBH) suite (Suzgun       ble 2 present a comprehensive comparison between the\net al. 2022) and 6 tasks from MMLU (Wang et al. 2024b) to      prompts optimized by MARS and the baselines for the 12\nrepresent general reasoning and knowledge-intensive bench-       tasks. As shown in Table 1, on general tasks, MARS outper-\nmarks. For domain-specific evaluation, we include 3 Chi-      forms the previous SOTA by 6.04%, and exceeds the original\nnese subject-area tasks from C-Eval (Huang et al. 2024),      prompt and CoT(ZS) by 20.16% and 15.32%, respectively.\n1 legal reasoning task from LSAT-AR (Zhong et al. 2023),      This indicates that the prompts optimized by MARS enable\nand 1 arithmetic reasoning task from GSM8K (Zhang et al.    LLMs to better understand the task requirements, provid-\n2024a).                                                       ing stronger instructions for tasks across different scenar-\n\nVariation B.E. D.QA   F.F.   G.S.   R.N.    S.U.    Avg.\n\n                                          MARS    93.17  71.89  74.70  59.44  90.36   87.95   79.59\n\n                                                                      w/oPlan   86.35  65.86  68.67  54.21  82.33   79.52   72.82\n                                    ∆      (-6.82) (-6.03) (-6.03) (-5.23) (-8.03)  (-8.43)  (-6.77)\n                                                                w/oSoc    84.74  63.86  62.25  49.80  74.30   74.70   68.28\n                                    ∆      (-8.43) (-8.03) (-12.45) (-9.64) (-16.06) (-13.25) (-11.31)\n                                                                    w/oCri    89.16  68.27  72.28  56.22  86.34   83.94   76.04\n                                    ∆      (-4.01) (-3.62) (-2.42) (-3.22) (-4.02)  (-4.01)  (-3.55)\n\n                                                             Table 3: Performance under different ablation settings are\n                                                               analyzed. We performed ablation experiments on the planner\n                                                     module w/oPlan, the Teacher-Critic-Student module w/oSoc,\n                                                       and the Critic Agent w/oCri to evaluate the impact of remov-\n                                                               ing these components. w/o indicates the experiment was run\n                                                             without the specified module.\n\n\nFigure 5: Inference-time scaling law. The horizontal axis de-\n                                                             computational cost.notes the inference-time computational cost, while the verti-\ncal axis represents the average performances on all tasks.          These results highlight MARS’s strong ability to bal-\n                                                          ance performance and resource usage through its structured\n                                                               optimization strategy. By performing high-level task plan-\n                                                            ning followed by step-wise Socratic refinement, MARS en-ios. MARS surpasses existing APO methods, highlighting\nthe limitations of both the generate-search approach and the       ables more efficient resource allocation, reduces unneces-\nmeta prompts approach. These methods fail to fully grasp       sary computation, and ensures both effectiveness and robust-\n                                                              ness throughout the APO process.the deeper essence of the APO process, which constrains\ntheir optimization effectiveness. In contrast, MARS thought-\nfully considers the prompt optimization pathways for differ-              Supplementary Analysis\nent tasks and incorporates heuristic optimization strategies,\n                                                    To further validate the effectiveness of MARS, we conduct\nmaking the prompt refinement process more efficient and\n                                                                   three additional analyses in this section: an ablation study\nprecise.\n                                                                     to assess the contribution of each component, a convergence\n MARS achieves strong and consistent performance\n                                                                  analysis to examine the optimization stability over iterations,\ngains across domain-specific tasks, highlighting its ef-\n                                                       and an investigation of the sensitivity to Other Target LLMs.\nfectiveness in knowledge-intensive reasoning. Table 2\n                                                                  In addition, Appendix D reports MARS’s generalizationpresents the experimental results of MARS on domain-\n                                                          performance when applied to GPT-4o and different ini-specific tasks, covering areas like Chinese, law, and math-\n                                                                                   tial prompt p0. Appendix E analyzes the effect of sam-ematics, all of which require specialized knowledge and\n                                                                 ple size on performance. Appendix F provides the inter-reasoning. In these tasks, MARS outperforms the previous\n                                                                 nal prompts used by each agent to clarify their roles. Ap-SOTA methods to 6.42%, further demonstrating its ability to\n                                                           pendix G presents the full multi-agent interaction process onbetter guide LLMs in domain-specific knowledge discovery\n                                                            a representative APO task. Appendix H offers the optimizedand application. This not only lowers the barrier to utiliz-\n                                                         prompts for all 17 tasks to facilitate reproducibility.ing LLMs but also enhances their generalization capability.\nMoreover, compared to the original prompt and CoT(ZS),\nMARS achieves improvements of 25.31% and 20.72%, re-     Ablation Study\nspectively, underscoring its effectiveness and practicality in     The Socratic dialogue mechanism plays the most critical\nthese specialized domains.                                       role in MARS, as shown by the largest performance drop\n                                                 upon its removal. Table 3 presents the impact of remov-\nEfficiency Analysis                                                               ing three key components: the Planner agent, the Teacher-\nMARS Consistently Achieves the Highest Computa-       Critic-Student Socratic module, and the Critic agent. Re-\ntional Efficiency.  The balance between resource con-     moving the Socratic module leads to the most substantial\nsumption and performance improvement is a crucial analy-       degradation, as the system loses its iterative refinement ca-\nsis metric (Yang et al. 2024b). As shown in Figure 5, MARS       pability and sends unprocessed sub-goals directly to the Tar-\nconsistently outperforms all baseline methods in terms of       get agent, resulting in poor optimization quality. Eliminating\ncomputational efficiency, as demonstrated by its superior       the Planner also causes a notable drop, since the Socratic\ninference-time scaling behavior across multiple APO tasks.       dialogue lacks structured guidance without its sub-goal tra-\n  Notably, under the same number of output tokens, MARS       jectory. Finally, while the Critic contributes less overall, its\nachieves the highest performance across all evaluated tasks.      feedback loop with the Teacher improves prompt quality;\nConversely, to reach comparable performance levels, base-      removing it leads to a 3.55% performance loss, as shown in\nline methods require more output tokens—indicating higher      Table 3.\n\nDeepseek       GPT         Avg.         multiple candidate sequences are generated, and methods\n    Base            -V2.5  -R1    -3.5    -4    -4o                   like Monte Carlo search are used to optimize the prompt.\n                                                   The second approach (Yang et al. 2024a; Ye et al. 2023;\n   Origin   56.96  61.48  44.79  49.70  55.84  53.75                                                    Zhang et al. 2025b) is the meta prompts method, where\n  CoT(ZS)  62.72  73.82  63.45  66.94  70.38  67.46         sophisticated meta prompts are designed to optimize the\n  MARS   79.59  83.05  69.30  73.21  80.86  77.20        prompt. In contrast to these two approaches, MARS em-\n                                                              ploys a planned optimization path, iteratively generating\n                                                                 high-quality prompts. This approach alleviates the ineffi-Table 4: Performance comparison on BBH tasks under dif-\n                                                                     cient search in prompt spaces issues in the first approachferent Target model settings.\n                                                       and addresses the challenges of limited flexibility of fixed\n                                                               templates in the second approach.\nConveragence Analysis\nMARS achieves faster convergence in most tasks, im-\nproving both efficiency and optimization quality. Figure 6\npresents the convergence analysis across four BBH tasks.\nTo better monitor the APO process, we visualize the itera-\ntive optimization trajectory within a 10-iteration observation\nwindow.\n  The results show that MARS exhibits an upward reward\ntrend in the early stages. For instance, in Task ‘Ruin Names’,\nit converges to the optimal solution by iteration 5. In con-\ntrast, in the OPRO task, convergence is not reached even af-\nter 10 iterations, resulting in higher resource consumption.\nThis comparison highlights MARS’s ability to reach optimal\nprompts in fewer steps, reducing computational cost and en-\nhancing efficiency.\n\nOther Target LLMs\nMARS demonstrates strong cross-model generalization,\nmaintaining high performance  across  diverse LLM\nbackbones. We  further  evaluate MARS on  additional      Figure 6: The convergence curves across different tasks\nTarget LLMs—Deepseek-R1, GPT-3.5, GPT-4, and GPT-     show the learning progress as the number of iterations in-\n4o—using the optimized prompts from previous experi-       creases. We compare the iterative convergence process of\nments to assess robustness across model families. As shown    MARS with four different baseline methods across four\nin Table 4, prompts optimized on the Deepseek-V2.5 base       tasks to assess MARS’s advantage in convergence speed.\nmodel generalize well, preserving strong performance even\non larger or structurally different LLMs. MARS consistently\nachieves notable gains across models, validating its model-      Multi-Agent.  Based on LLMs, a combination of AI agents\nagnostic design and broad applicability.                         capable of performing specific functions forms a multi-agent\n                                                          system (Richards 2023; Yang, Yue, and He 2023; Wu et al.\n               Related Works                        2023; Zhang et al. 2025c). Given a statement of a specific\n                                                                        task, AI agents can attempt to break complex problem state-The related work is structured into two main aspects: first,\n                                                        ments into subtasks and use tools, including data retrievalan introduction to prompt optimization; and second, an ex-\n                                                       from the internet, to solve them step-by-step through au-ploration of multi-agent techniques.\n                                                              tomatic iterations. Some studies (Poldrack, Lu, and Beguˇs\nPrompt Optimization.  Early work primarily focused on      2023; Wang et al. 2024a; Xi et al. 2025; Ni and Gao 2021;\ntwo aspects: discrete optimization of hard prompts (Shin      Lin et al. 2025b) use multi-agent systems to address is-\net  al. 2020; Wen et  al. 2024; Chen et  al. 2023; Zhang      sues such as problem identification, code development and\net  al. 2022) and continuous vector optimization of soft      debugging, plotting results and analysis, and providing in-\nprompts  (Lester, Al-Rfou, and Constant 2021; Li and       teractive feedback with the human user. Ni and Buehler\nLiang 2021; Liu et al. 2024). However, these methods are      (2024) (Zhang et al. 2025a) demonstrates the potential of\nhighly task-dependent and exhibit locality. With the ad-      organizing an AI multi-agent collaborative team to auto-\nvent of LLMs, traditional methods have become outdated.       matically solve mechanical problems, showcasing an en-\nAPE (Zhou et al. 2022) pioneered the use of generative      hanced ability to understand, formulate, and validate engi-\nmethods to optimize instructions. Since APE, there have      neering problem solutions through self-correction and mu-\nbeen two major approaches. The first approach (Zhou et al.       tual correction. Inspired by their work, we leverage multi-\n2022; Xu, Banburski-Fahey, and Jojic 2023; Pryzant et al.      agent technology to autonomously plan the APO optimiza-\n2023; Wang et al. 2023) is the generate-search model, where       tion path and design a Teacher-Critic-Student collaborative\n\napproach for iterative optimization.                               Lin, Q.; Zhu, Y.; Pu, B.; Huang, L.; Luo, H.; Ma, J.; Peng,\n                                                                      Z.; Zhao, T.; Xu, F.; Zhang, J.; et al. 2025b. A Foundation\n                Conclusion                       Model for Chest X-ray Interpretation with Grounded Rea-\n                                                            soning via Online Reinforcement Learning. arXiv preprint\nWe propose MARS, a novel multi-agent framework for\n                                                            arXiv:2509.03906.\nadaptive APO  that  integrates  Socratic guidance within\n                                                                 Liu, X.; Zheng, Y.; Du, Z.; Ding, M.; Qian, Y.; Yang, Z.; anda POMDP formulation.  It includes: (1) a Planner  that\n                                                            Tang, J. 2024. GPT understands, too. AI Open, 5: 208–215.generates task-specific optimization  trajectories, and (2)\na Teacher-Critic-Student dialogue enabling  interpretable       Ni, B.; and Buehler, M. J. 2024. MechAgents: Large lan-\nprompt refinement. This simulates pseudo-gradient paths in      guage model multi-agent collaborations can solve mechan-\ndiscrete prompt space, narrowing the search scope. Modeled       ics problems, generate new data, and integrate knowledge.\nas a POMDP: the Student’s latent state is the hidden state,      Extreme Mechanics Letters, 67: 102131.\nTeacher-Critic interactions define actions, and prompt out-       Ni, B.; and Gao, H. 2021. A deep learning approach to the\nputs serve as observations. A Target agent guides iteration       inverse problem of modulus identification in elasticity. Mrs\nvia performance rewards. Experiments show MARS consis-       Bulletin, 46: 19–25.\ntently outperforms baselines while maintaining transparent      Poldrack, R.  A.; Lu,  T.; and  Beguˇs, G. 2023.   AI-\noptimization trajectories.                                           assisted coding: Experiments with GPT-4.  arXiv preprint\n                                                            arXiv:2304.13187.\n            Acknowledgments                        Pryzant, R.; Iter, D.; Li, J.; Lee, Y. T.; Zhu, C.; and Zeng, M.\nThis work was supported by the National Natural Science      2023.  Automatic prompt optimization with ”gradient de-\nFoundation of China (No. 62137002, 62277042, 62293553,       scent” and beam search. In Proceedings of the 2023 Confer-\n62450005, 62437002, 62477036, 62477037, 62176209,      ence on Empirical Methods in Natural Language Process-\n62192781, 62306229), the “LENOVO-XJTU” Intelligent       ing, (EMNLP), 7957–7968.\nIndustry Joint Laboratory Project, the Shaanxi Provincial      Richards, T. B. 2023.  Auto-GPT: An experimental open-\nSocial Science Foundation Project (No. 2024P041), the Nat-      source attempt to make GPT-4 fully autonomous.\nural Science Basic Research Program of Shaanxi (No. 2023-      Shen, T.; Mao, R.; Wang,  J.; Zhang, X.; and Cambria,\nJC-YB-593), and the Youth Innovation Team of Shaanxi      E. 2025.  Flow-guided Direct Preference Optimization for\nUniversities “Multi-modal Data Mining and Fusion”.           Knowledge Graph Reasoning with Trees.  In Proceedings\n                                                                     of the 48th International ACM SIGIR Conference on Re-\n                References                           search and Development in Information Retrieval, SIGIR\n                                                                   ’25, 1165–1175. New York, NY, USA: Association for Com-Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;\n                                                               puting Machinery. ISBN 9798400715921.Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;\nAnadkat, S.; et al. 2023.  Gpt-4 technical report.  arXiv       Shin, T.; Razeghi, Y.; Logan IV, R. L.; Wallace, E.; and\npreprint arXiv:2303.08774.                                    Singh, S. 2020. Autoprompt: Eliciting knowledge from lan-\n                                                        guage models with automatically generated prompts. arXiv\nChen, L.; Chen, J.; Goldstein, T.; Huang, H.; and Zhou,\n                                                                  preprint arXiv:2010.15980.\nT. 2023.   Instructzero: Efficient instruction optimization\nfor black-box  large language models.   arXiv  preprint      Suzgun, M.; Scales, N.; Sch¨arli, N.; Gehrmann, S.; Tay,\narXiv:2306.03082.                                                   Y.; Chung, H. W.; Chowdhery, A.; Le, Q. V.; Chi, E. H.;\n                                                        Zhou, D.; et al. 2022.  Challenging big-bench tasks and\nGuo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.;                                                          whether chain-of-thought can solve them.  arXiv preprint\nZhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1:                                                            arXiv:2210.09261.\nIncentivizing reasoning capability in llms via reinforcement\n                                                    Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang,learning. arXiv preprint arXiv:2501.12948.\n                                                                                       J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2024a. A\nHuang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,                                                             survey on large language model based autonomous agents.\nJ.; Lv, C.; Zhang, Y.; Fu, Y.; et al. 2024. C-eval: A multi-                                                                 Frontiers of Computer Science, 18(6): 186345.\nlevel multi-discipline chinese evaluation suite for foundation\n                                                    Wang, X.; Li, C.; Wang, Z.; Bai, F.; Luo, H.; Zhang, J.; Jojic,\nmodels.  Advances in Neural Information Processing Sys-\n                                                                N.; Xing, E. P.; and Hu, Z. 2023.  Promptagent: Strategic\ntems, 36.\n                                                             planning with language models enables expert-level prompt\nLester, B.; Al-Rfou, R.; and Constant, N. 2021.   The       optimization. arXiv preprint arXiv:2310.16427.\npower of scale for parameter-efficient prompt tuning. arXiv                                                    Wang, Y.; Ma, X.; Zhang, G.; Ni, Y.; Chandra, A.; Guo, S.;\npreprint arXiv:2104.08691.                                                         Ren, W.; Arulraj, A.; He, X.; Jiang, Z.; et al. 2024b. Mmlu-\nLi, X. L.; and Liang, P. 2021.   Prefix-tuning: Optimiz-       pro: A more robust and challenging multi-task language un-\ning continuous prompts for generation.   arXiv preprint      derstanding benchmark. arXiv preprint arXiv:2406.01574.\narXiv:2101.00190.                                                    Wen, Y.; Jain, N.; Kirchenbauer, J.; Goldblum, M.; Geip-\nLin, Q.; Zhao, T.; He, K.; Peng, Z.; Xu, F.; Huang, L.; Ma, J.;       ing, J.; and Goldstein, T. 2024. Hard prompts made easy:\nand Feng, M. 2025a. Self-supervised Quantized Represen-      Gradient-based discrete optimization for prompt tuning and\ntation for Seamlessly Integrating Knowledge Graphs with       discovery. Advances in Neural Information Processing Sys-\nLarge Language Models. arXiv preprint arXiv:2501.18119.       tems, 36.\n\nWu, Q.; Bansal, G.; Zhang, J.; Wu, Y.; Zhang, S.; Zhu, E.;      Zhang, J.; Wei, B.; Qi, S.; Liu, J.; Lin, Q.; et al. 2025c.\nLi, B.; Jiang, L.; Zhang, X.; and Wang, C. 2023. Autogen:    GKG-LLM: A Unified Framework for Generalized Knowl-\nEnabling next-gen llm applications via multi-agent conver-      edge Graph Construction. arXiv preprint arXiv:2503.11227.\nsation framework. arXiv preprint arXiv:2308.08155.            Zhang,  J.; Yang, C.; Zhu, H.; Lin, Q.; Xu, F.; and Liu,\nXi, Z.; Chen, W.; Guo, X.; He, W.; Ding, Y.; Hong, B.;        J. 2024b. A Semantic Mention Graph Augmented Model\nZhang, M.; Wang, J.; Jin, S.; Zhou, E.; et al. 2025.  The       for Document-Level Event Argument Extraction.  arXiv\nrise and potential of large language model based agents: A       preprint arXiv:2403.09721.\nsurvey. Science China Information Sciences, 68(2): 121101.      Zhang, T.; Wang, X.; Zhou, D.; Schuurmans, D.; and Gon-\nXu, F.; Wu, Z.; Sun, Q.; Ren, S.; Yuan, F.; Yuan, S.; Lin, Q.;       zalez, J. E. 2022. Tempera: Test-time prompting via rein-\nQiao, Y.; and Liu, J. 2024. Symbol-LLM: Towards foun-      forcement learning. arXiv preprint arXiv:2211.11890.\ndational symbol-centric interface for large language models.      Zhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang, Y.;\nIn Proceedings of the ACL, 13091–13116.                        Saied, A.; Chen, W.; and Duan, N. 2023. Agieval: A human-\nXu, W.; Banburski-Fahey, A.; and Jojic, N. 2023. Reprompt-       centric benchmark for evaluating foundation models. arXiv\ning: Automated chain-of-thought prompt inference through       preprint arXiv:2304.06364.\ngibbs sampling. arXiv preprint arXiv:2305.09993.              Zhou,  Y.; Muresanu, A.  I.; Han,  Z.;  Paster, K.;  Pitis,\nYan, H.; Xu, F.; Xu, R.; Li, Y.; Zhang, J.; Luo, H.; Wu, X.;       S.; Chan, H.; and Ba,  J. 2022.   Large language mod-\nTuan, L. A.; Zhao, H.; Lin, Q.; et al. 2025. Mur: Momen-       els are human-level prompt engineers.   arXiv preprint\ntum uncertainty guided reasoning for large language models.      arXiv:2211.01910.\narXiv preprint arXiv:2507.14958.\nYang, C.; Wang, X.; Lu, Y.; Liu, H.; Le, Q. V.; Zhou, D.;\nand Chen, X. 2024a. Large Language Models as Optimizers.\nIn The Twelfth International Conference on Learning Rep-\nresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024.\nOpenReview.net.\nYang, H.; Yue, S.; and He, Y. 2023. Auto-gpt for online de-\ncision making: Benchmarks and additional opinions. arXiv\npreprint arXiv:2306.02224.\nYang, J.; Jin, H.; Tang, R.; Han, X.; Feng, Q.; Jiang, H.;\nZhong, S.; Yin, B.; and Hu, X. 2024b. Harnessing the power\nof llms in practice: A survey on chatgpt and beyond. ACM\nTransactions on Knowledge Discovery from Data, 18(6): 1–\n32.\nYe, Q.; Axmed, M.; Pryzant, R.; and Khani,  F. 2023.\nPrompt engineering a prompt engineer.   arXiv preprint\narXiv:2311.05661.\nYuan, L.; Cai, Y.; Shen, X.; Li, Q.; Huang, Q.; Deng, Z.;\nand Wang, T. 2025.   Collaborative Multi-LoRA Experts\nwith Achievement-based Multi-Tasks Loss for Unified Mul-\ntimodal Information Extraction. In Kwok, J., ed., Proceed-\nings of the Thirty-Fourth International Joint Conference on\nArtificial Intelligence, IJCAI-25, 6940–6948. International\nJoint Conferences on Artificial Intelligence Organization.\nMain Track.\nZhang, H.; Da, J.; Lee, D.; Robinson, V.; Wu, C.; Song, W.;\nZhao, T.; Raja, P.; Slack, D.; Lyu, Q.; et al. 2024a. A careful\nexamination of large language model performance on grade\nschool arithmetic. arXiv preprint arXiv:2405.00332.\nZhang, J.; Wang, Z.; Wang, Z.; Zhang, X.; Xu, F.; Lin, Q.;\nMao, R.; Cambria, E.; and Liu, J. 2025a. MAPS: A Multi-\nAgent Framework Based on Big Seven Personality and So-\ncratic Guidance for Multimodal Scientific Problem Solving.\narXiv preprint arXiv:2503.16905.\nZhang, J.; Wang, Z.; Zhu, H.; Liu, J.; Lin, Q.; and Cambria,\nE. 2025b. MARS: A Multi-Agent Framework Incorporat-\ning Socratic Guidance for Automated Prompt Optimization.\narXiv preprint arXiv:2503.16874.\n\nProof of Proposition                  Proof of Proposition 2 (Monotonic Reward\n                                                              Stability)\nProof of Proposition 1 (Socratic Policy\n                                                            Let {pi}ni=0 denote the sequence of prompts generated byImprovement Bound)\n                                                                 the MARS refinement process, where each prompt is up-\nLet Π = {πt, πc, πs} denote the joint policy composed of      dated via:\nthe Teacher, Critic, and Student agents. Let pi denote the                                                                             pi = πs(ai, pi−1),   with  ai = (qi, ci),     (17)\nprompt state at refinement step i, and define the associated\n                                                                                           i.e., a Socratic action ai is used to refine the prompt at steplatent state as si. At each step, the action ai =  (qi, ci)\n                                                                                          i.induces a transition from si−1 to si, and yields a prompt\n                                                   Assume the reward function R : P →R is λ-Lipschitzpi = πs(ai, pi−1).\n                                                             continuous with respect to the prompt representation, i.e.,  We assume the existence of an underlying task reward\n                                                                     for all i = 1, . . . , n,function:\n                                                                    |R(pi) −R(pi−1)| ≤λ · ∥pi −pi−1∥,      (18)\n         R(p) = E(x,y)∼D [f (πtar(x; p), y)] ,       (11)                                                       where ∥·∥denotes a norm over the prompt space (e.g., token-\n                                                                      level edit distance or embedding-based distance).which is Lipschitz-continuous with constant λ >  0. Our\n                                                         Suppose further that the update step size is bounded as:goal is to lower-bound the cumulative reward improvement\nfrom p0 to pn based on Socratic signal quality.                                      ∥pi −pi−1∥≤ε.                (19)\n                                                          Then, combining (2) and (3), the reward change per step is\nStep 1: Define Local Advantage.  Let the local reward\n                                                      bounded by:\ngain be defined as:\n                                                                         |R(pi) −R(pi−1)| ≤λε.            (20)\n             Ai := R(pi) −R(pi−1).             (12)                                                              This implies that the reward improvement (or degrada-\n                                                                       tion) at each refinement step is at most linear in the prompt\nWe assume the expected advantage at each step satisfies:\n                                                        change size.\n                   E[Ai] ≥¯Ai > 0,                 (13)      Now suppose the process satisfies a minimum improve-\n                                                     ment requirement:\ni.e., the expected contribution of the composite action ai is                    R(pi) ≥R(pi−1) + δ,              (21)\nbeneficial.\n                                                                     for some δ > 0. Then combining (4) and (5), we obtain:\nStep 2: Control for Socratic Variance.  We define the                  λε ≥δ ⇒   ε ≥δ/λ.             (22)\nconditional variance of Ai given the state si−1 as:                                                             Thus, for a reward gain of at least δ, the update must in-\n                                                        duce a prompt change of at least δ/λ. Conversely, if                 Var[Ai | si−1] ≤σ2.               (14)\n                                                                                ∥pi −pi−1∥≪δ/λ,               (23)\nThis variance captures uncertainty due to imperfect ques-                                                              then the step is too small to yield meaningful improvement,\ntions or noisy feedback in the Socratic interaction.                                                       and further refinement is unlikely to be effective.\n                                                              This result provides a formal justification for the earlyStep 3: Apply Jensen-Bernstein Inequality.  Since R(p)\n                                                              stopping rule: once consecutive prompt updates fall belowis Lipschitz, and prompt updates occur in a discrete space,\n                                                            a semantic change threshold, reward improvement will nec-we use a Bernstein-style bound:\n                                                                      essarily be bounded, indicating convergence.\n           E[R(pi)] ≥R(pi−1) + ¯Ai −σ2         (15)                 Tasks and Datasets                                   2λ.\n                                                    To comprehensively evaluate the expert-level prompt opti-\nStep 4: Accumulate Across Steps.  Summing over all n      mization capabilities of our framework, we curate 17 tasks\nrefinement steps, we obtain:                                from two broad categories: General Tasks and Domain-\n                                                                  Specific Tasks.\n                           n\n        E[R(pn)] −R(p0) = X E[Ai]                    General Task Evaluation.  We select six tasks from the\n                              i=1                  BBH (Suzgun et al. 2022) and MMLU (Wang et al. 2024b)\n                           n                    (16)       datasets, respectively. BBH tasks consist of six challenging\n               ≥ X   ¯Ai −σ2    .               reasoning tasks that assess logical inference and problem-\n                                 2λ                   solving skills, including boolean expressions, disambigua-                              i=1\n                                                                     tion QA, formal fallacies, geometric shapes, ruin names,\n  This completes the proof of the lower bound. It shows that      and sports understanding. MMLU tasks cover six subject-\nas long as the Socratic actions are informative ( ¯Ai > 0) and       specific tasks designed to evaluate general knowledge across\nthe variance σ2 is controlled, the cumulative reward is guar-       diverse fields, including college biology, college medicine,\nanteed to improve linearly with the number of refinement       electrical engineering, high school world history, human ag-\nsteps.                                                                ing, and marketing.\n\nDomain-Specific Task  Evaluation.  We  include  three         Tasks                    ABBR.    Train     Test\nbenchmarks: C-Eval (Huang et al. 2024), GSM8K (Zhang                                                             Bigbench\net al. 2024a), and LSAT-AR (Zhong et al. 2023). C-Eval is a\n                                                                   Boolean Expressions           B.E.      1      249\nChinese evaluation benchmark that covers domain-specific\n                                                                      Disambiguation QA        D.QA      1      249topics such as art studies, clinical medicine, and Urban and\nRural Planner. GSM8K is a widely used mathematical rea-           Formal Fallacies                  F.F.       1      249\nsoning dataset. LSAT-AR focuses on legal reasoning, evalu-            Geometric Shapes             G.S.      1      249\nating AI performance in law-related tasks.                           Ruin Names                 R.N.      1      249\n                                                                           Sports Understanding          S.U.      1      249\nDataset Split.  In this study, we adopt a minimal train-                                     MMLU\ning paradigm by selecting only a single instance from each\n                                                                         College Biology              C.B.      1      143\ndataset for training. Despite this extremely limited supervi-\n                                                                         College Medicine           C.M.      1      172sion, our method demonstrates strong and consistent per-\nformance across a diverse range of datasets. This suggests              Electrical Engineering          E.E.      1      144\nthat our approach possesses exceptional few-shot ability, en-            HighSchool World History    W.H.      1      236\nabling effective adaptation to various tasks with minimal         Human Aging               H.A.      1      222\nprior knowledge. The detailed partition of the dataset is pre-            Marketing                  M.T.      1      233\nsented in Table 5.                                                  C-EVAL\n  One of the key highlights of this study is that the training                                                                        Art Studies                   A.S.      1      32\ndata consists of only a single sample—MARS utilizes just\n                                                                Urban And Rural Planner      U.R.P.      1      45\none data point from the current task for the entire optimiza-\n                                                                                 Clinical Medicine          CL.M.     1      21tion process. This minimal setup is enabled by the Planner\nagent’s strong capacity to identify the task definition and in-      GSM8K                  GSM.      1     1318\nterpret the provided example, allowing it to infer the under-       LSAT-AR                       L.A.      1      229\nlying task structure and semantics effectively. Since the pri-\nmary function of the Planner is to generate an optimization      Table 5: Data split of general tasks and domain-specific\ntrajectory based on the prompt-task alignment, a single rep-       tasks. One instance for training and others for testing. The\nresentative instance is sufficient to guide downstream agents.     ‘ABBR.’ column represents the abbreviations for all the\nWe further analyze the impact of different sample sizes on       tasks.\nperformance in Appendix E.\n\n     Experiment Settings and Baselines             Shot) baseline. (3) Finally, we compare MARS with some\n                                                                strong baseline methods from recent years, including Auto-\nWe select a powerful LLM, deepseek-V2.5-1210 (Guo et al.                                                            matic Prompt Engineer (APE) (Zhou et al. 2022), Prompt\n2025), as our primary agent for the APO tasks. Not only does                                                             Optimization with Textual Gradients (ProTeGi) (Pryzant\ndeepseek-V2.5-1210 exhibit strong reasoning and genera-                                                                          et al. 2023), Optimization by PROmpting (OPRO) (Yang\ntion capabilities in a variety of natural language processing                                                                          et al. 2024a), and Prompt Engineer 2 (PE2) (Ye et al. 2023).\ntasks, but it also efficiently explores multiple angles when                                          APE and ProTeGi generate multiple prompts and perform\nfacing complex prompt optimization requirements, making                                                               search optimization to find the optimal prompt, while OPRO\nit well-suited for adapting to different tasks and datasets in                                                       and PE2 optimize prompts by designing a sophisticated meta\nthe APO process. We adopt accuracy as our primary eval-      prompts.\nuation metric to comprehensively assess the performance\nof different methods across various task scenarios. Table 1       Generalization Across Different Base and\npresents our experimental results on 12 general task datasets,\n                                                               Target Modelsillustrating the performance of APO in diverse scenarios,\nwhile Table 2 summarizes its performance on five domain-       In this section, we present the optimization performance of\nspecific datasets, underscoring the model’s versatility and       the method from this study on another base model, as well as\nstability across different fields. To further validate the gen-       the optimization results of our APO on other Target LLMs.\nerality and robustness of our method, we additionally em-\nployed another high-performance LLM, GPT-4o (Achiam     Base Model of GPT-4o\net al. 2023), for extended comparative experiments, with the     To verify the generality and effectiveness of the proposed\ncorresponding findings reported in Appendix D.               method in this study, we conduct further experiments by re-\n  We compare MARS with three categories of baselines:      placing the base model with GPT-4o (Achiam et al. 2023).\noriginal prompts, CoT prompts, and some of the  latest       As shown in Table 6, in the datasets of the 17 tasks\nAPO methods. Specifically, (1) original prompts refer to the      adopted by this study, MARS achieves a new SOTA per-\nprompts used in the datasets, where each dataset often pro-      formance when using the GPT-4o base model, surpassing\nvides some initial guidance for the tasks. (2) To build the       the previous SOTA by 2.3%. This result demonstrates that\nCoT (Zero-Shot) baseline, we add the prompt Let’s think    MARS not only performs excellently on the existing base\nstep by step at the beginning of each task; based on this, we      models but also exhibits strong transferability, continuously\nfurther include a specific example to create the CoT (Few-      improving performance across different base models. This\n\nTasks   BBH  MMLU  Chinese  GSM.   L.A.   Avg.         Tasks   Train  B.E.  D.QA   F.F.   G.S.   R.N.   S.U.\n\n  Origin    60.92   83.73     58.26    72.31   20.96  59.24       APE    100   83.53  61.85  61.04  51.41  77.51  74.70\n CoT(ZS)  62.81   85.62     64.26    76.25   24.45  62.68        ProTeGi   20   83.93  63.86  62.65  52.21  80.32  76.71\n                                               OPRO    50   86.34  66.67  63.45  53.81  83.13  82.73 CoT(FS)   63.42   88.27     68.69    83.92   28.82  66.62\n                                                        PE2    100   87.95  65.46  63.86  54.62  84.34  75.90\n  APE    64.36   86.72     69.03    81.18   30.13  66.28\n ProTeGi   76.43   86.35     73.52    82.70   31.88  70.18      MARS    0    90.76  70.28  73.09  57.83  88.35  85.94\n                                           MARS    1    93.17  71.89  74.70  59.43  90.36  87.95\n  OPRO    78.73   88.25     75.79    84.74   32.75  72.05\n                                           MARS    3    93.57  72.69  74.30  60.24  89.96  88.35\n   PE2     77.59   91.89     74.67    85.43   35.81  73.08\n\n MARS   81.13   92.82     78.11    90.97   40.17  76.58       Table 8: Performance comparison of different sampling\n                                                                       strategies on the evaluation metric. Train means the train-\nTable 6: Performance comparison on difference tasks based      ing data.\non GPT-4o.\n\n                                                      Prompts for Agents\nfurther validates the versatility and robustness of the MARS                                                             Table 9 summarizes the prompts used for all agents in this\nmethod, highlighting its effectiveness on a variety of base                                                                  paper, each playing a crucial role in the overall optimization\nmodels.                                                            workflow.\n                                                     The Planner first constructs a structured plan based on the\nDifferent Initial Prompts p0                                  task requirements, defining a trajectory of sub-goals to guide\nAs shown in the Table 7, MARS consistently achieves strong       the optimization process. Beyond laying out the overall flow,\nperformance across different initial prompts p0, demonstrat-       the Planner provides semantic anchors that structure the in-\ning robust optimization capabilities. Although the choice of       teractions among downstream agents.\np0 can lead to variations in absolute performance, MARS         In the refinement phase, the Teacher generates Socratic-\nmaintains a relatively stable improvement margin across       style questions aligned with the current sub-goal, designed\ntasks. This indicates that MARS effectively adapts its op-       to elicit reasoning rather than direct edits. The Student re-\ntimization trajectory regardless of the quality of the starting      sponds by proposing refined prompts, while the Critic eval-\nprompt, highlighting its reliability and generalization ability       uates the quality and pedagogical alignment of the guidance,\nin diverse initialization scenarios.                             forming an interactive loop for iterative improvement.\n                                                                         Finally, the Target agent validates the final prompt on\n  Model    1           2           3          Avg        downstream tasks, providing external performance feedback\n                                                                       that closes the optimization loop. This validation ensures\n    p0       Let’s          Let’s         Let’s\n                                                                       that the generated prompt is not only structurally coherent              think step    work this    proceed\n            by step.       out  step    with  our                 but also effective for the intended task.\n                        by step.       tasks one\n                                   by one.                      Full-process Prompt Optimization\n MARS    79.59         78.53        75.30        77.81        Figure 7 presents a comprehensive example of full-process\n                                                       prompt optimization, using the Geometry Shapes task from\nTable 7: Performance comparison of MARS on BBH tasks       the BBH dataset. This visualization clearly illustrates the\nunder different initial prompts p0.                              end-to-end workflow of the MARS framework, highlighting\n                                               how the multi-agent system approximates a policy-guided\n                                                                      trajectory over a discrete prompt space.\n                                                     The process begins with the Planner agent, which decom-\n          Sample Size Analysis                                                            poses the input task into a series of interpretable sub-goals\nTo analyze the rationality of one-shot training, we present a       {st1, . . . , stn}, forming a high-level optimization trajectory.\ncomparison of 0-shot, 1-shot, 3-shot, and baseline methods      These steps serve as a form of global guidance for the sub-\nin Table 8.                                                   sequent reasoning path, representing the initial policy direc-\n  The results indicate that the performance difference be-       tion in the POMDP formulation.\ntween 1-shot and 3-shot is minimal, yet the 1-shot approach        Then, through the iterative interaction among the Teacher,\nis more resource-efficient while also enhancing task time       Critic, and Student agents, MARS executes a sequence of\nefficiency. This demonstrates that in resource-constrained       transitions (si−1, ai, si), where each composite action ai =\nscenarios, 1-shot training offers a better trade-off between       (qi, ci) is derived from the Socratic-style question and its\nperformance and computational cost. Other strong baseline       critique. The Student agent updates its internal latent state si\nmodels, such as PromptAgent and OPRO, use at least 20%      based on this feedback and outputs the observable prompt\nof the data for training, while our framework, using 1-shot       oi =  pi. This dialogue-driven process simulates a soft\ntraining, achieves better performance than these models.      pseudo-gradient trajectory in the POMDP landscape, gradu-\nThis clearly demonstrates the effectiveness and resource ef-       ally refining the prompt through interpretable and feedback-\nficiency of the MARS method.                                  aligned steps.\n\nMARS enables dual-level interpretability: process in-\nterpretability  arises from the  explicit optimization path\nplanned by the Planner and the Socratic dialogue struc-\nture, which makes each state transition traceable and ratio-\nnal. Result interpretability is embodied in the final prompt,\nwhich integrates task-specific constraints—such as toler-\nance thresholds or validation rules—as shown in Figure 7,\nindicating the policy-converged output under the POMDP\nframework.\n\n        Universal Optimum Solution\nThis section introduces the final optimized prompts for all\ngeneral tasks and domain-specific tasks, obtained through\nthe MARS optimization process. After multiple iterations\nfor each of the 17 sub-tasks, a prompt strategy that yields\noptimal performance was identified for each one. Tables 10\nthrough Table 26 sequentially present the best solutions for\nthese 17 sub-tasks along with their respective experimental\nresults, demonstrating the adaptability and effectiveness of\nMARS across a broad range of tasks.\n\nPlanner\n       Split the task ’Here is a topic for geometric graph generation: Given a full SVG path element containing multiple commands,\n      determine the geometric shape that would be generated if one were to execute the full path element.\n      For example: This SVG path element ¡path d=M 64.00,63.00 L 44.00,63.00 L 44.00,50.00 L 64.00,50.00 L 64.00,45.00 L\n       85.00,57.00 L 64.00,68.00 L 64.00,63.00””/¿ draws a Options: (A) circle (B) heptagon (C) hexagon (D) kite (E) line (F)\n      octagon (G) pentagon (H) rectangle (I) sector (J) triangle\n        I want to input a prompt and this topic into the big language model so that the big language model outputs the highest correctness\n        rate. Please generate the most suitable prompt according to the requirements I just mentioned.’ into detailed steps and details.\n      For example, for the clinical medicine Test, the task is planned as follows: Total steps: 4 Step 1: Analyze the input requirements,\n       focusing on the type of clinical medicine question and the format of the options. Step 2: Design a prompt that encourages the\n      model to consider the specific clinical characteristics of the condition described in the question and match the most appropriate\n       option based on medical knowledge. Step 3: Request the model to evaluate each option in the context of clinical presentation,\n      symptoms, and diagnostic characteristics of the condition to ensure it selects the most accurate answer. Step 4: Test and refine\n       the prompt to ensure the model produces the highest correctness rate for similar clinical medicine questions.\n\n      Teacher\n     You are a teacher who asks questions in the Socratic manner based on objectives and student responses. Please ask a total of\n      two questions: The first one is for the problem that appeared in the prompt given by the students in the last round. The second\n      one is an optimization solution based on the current steps of the task.\n       Please include only questions in your output and do not make answers for your students.\n\n      Student\n     You are a prompt generator, please proceed to iterate over the existing prompts as required.\n      Note that you should only output the new prompt you generated.\n\n       Critic\n     You are an evaluator responsible for judging the correctness of a given task. Your output must strictly follow these rules:\n        1. If the task is judged as correct, output only: [True] 2. If the task is judged as incorrect, output: [False] [suggestion: ¡reason\n       for the incorrect judgment¿]\n      Replace ‘¡reason for the incorrect judgment¿‘ with a clear and concise explanation of why the task is incorrect.\n     Do not include any additional text, comments, or explanations beyond the specified format.\n\n      Target\n      Prompt: Systematically analyze the given SVG path element by first breaking it down into its individual commands, such as ’M’\n      (move to), ’L’ (line to), and others. For each command, map the sequence of points it generates, ensuring you accurately trace\n       the path step by step. As you follow the path, focus on identifying key geometric properties, such as equal side lengths, parallel\n        lines, specific angles, or symmetries, that emerge between consecutive points. Use these properties to classify the shape based\n      on its defining characteristics. For example, given the path ¡path d=M 64.00,63.00 L 44.00,63.00 L 44.00,50.00 L 64.00,50.00\n     L 64.00,45.00 L 85.00,57.00 L 64.00,68.00 L 64.00,63.00/¿, calculate the distances between points to check for equal side\n       lengths, measure angles to identify parallelism or perpendicularity, and look for symmetries that align with known geometric\n       shapes. Based on these observations, determine whether the shape is a circle, heptagon, hexagon, kite, line, octagon, pentagon,\n       rectangle, sector, or triangle. Provide the correct answer by reasoning through the geometric properties derived from the path\n      commands.\n       Question: question\n       Please don’t output the process of doing the question, only the content of the answer.The answer should be a parenthesis\n       containing the capital letter of the chosen answer. please do not add any other spaces or symbols.\n\nTable 9: The table summarizes the prompts used for all Agents in this paper. The examples in the table are from the Geometry\nShapes Task of the BBH dataset.\n\n\n\n\n Boolean Expressions\n Evaluate the truth value of the following Boolean expression step by step. The expression consists of Boolean\n constants (True, False) and basic Boolean operators (and, or, not). Carefully analyze each part of the expression,\n apply the correct Boolean logic, and provide the final truth value as your answer. For example, if the input is ’not\n  ( True ) and ( True ) is’, the correct output is ’False’. Ensure your reasoning is clear and accurate.\n\n  Table 10: The table shows the final optimized prompt for the Boolean Expressions task of BBH using the MARS method.\n\nFigure 7: This figure presents a complete example of the collaborative output from all agents in a single iteration, using the\nGeometry Shapes task from the BBH dataset.\n\nDisambiguation QA\n Analyze the following sentence to determine whether the pronoun is inherently ambiguous or if it can be linked to\n a specific antecedent. Follow these streamlined steps to efficiently evaluate pronoun disambiguation while main-\n  taining accuracy, especially in complex sentence structures:\n  1. Identify the Pronoun and Its Grammatical Role:\n  2. Identify Key Contextual Cues:\n  3. List and Filter Potential Antecedents:\n  4. Evaluate Plausibility:\n  5. Determine Ambiguity or Specific Antecedent:\n  6. Align with Provided Options:\n Evaluation Metrics for Model Output:\n  1. Correctness:\n  2. Clarity:\n  3. Efficiency:\n  4. Consistency:\n Additional Considerations:\n  1. Grammatical Structure Influence:\n  2. Optimizing Contextual Cue Identification:\n By simplifying the steps and focusing on key evaluation metrics, the model can process and apply the disambigua-\n  tion process more efficiently while maintaining high accuracy and clarity in its outputs, even in complex sentence\n  structures.\n\n   Table 11: The table shows the final optimized prompt for the Disambiguation QA task of BBH using the MARS method.\n\n\n\n Formal Fallacies Syllogisms Negation\n Analyze the following argument step by step to determine its logical validity. Carefully consider the premises\n provided and assess whether the conclusion necessarily follows from them. Pay special attention to the role of\n negations in the argument. After evaluating the logical structure, decide whether the argument is deductively valid\n or invalid based on the given premises. Choose the correct option from the provided choices: valid or invalid.\n Ensure your reasoning is thorough and aligns with formal logical principles.\n\nTable 12: The table shows the final optimized prompt for the Formal Fallacies Syllogisms Negation task of BBH using the\nMARS method.\n\n\n\n Geometric Shapes\n Given an SVG path element and a list of geometric shape options, systematically analyze and interpret the sequence\n of SVG path commands to determine the number of vertices and the overall structure of the geometric shape.\n Follow this structured and optimized approach:\n  1. Dynamic Tolerance Threshold for Vertex Identification: Detailed Explanation\n  2. Optimized Vertex Counting and Connection: Detailed Explanation\n  3. Critical SVG Path Command Analysis:Detailed Explanation\n  4. Accurate Vertex Counting and Connection:Detailed Explanation\n  5. Distinguishing Between Similar Shapes:Detailed Explanation\n  6. Systematic Comparison with Provided Options:Detailed Explanation\n  7. Validation and Refinement:Detailed Explanation\n  8. Optimization for Similar Shapes:Detailed Explanation\n Key Considerations for Dynamic Tolerance and Vertex Identification:Detailed Explanation\n Optimized Comparison Process:Detailed Explanation\n By integrating these considerations into the analysis, the model can achieve a higher correctness rate in identifying\n geometric shapes from SVG paths, even when dealing with shapes that have similar properties.\n\n    Table 13: The table shows the final optimized prompt for the Geometric Shapes task of BBH using the MARS method.\n\nRuin Names\nGiven an artist, band, or movie name, create a one-character edit that changes the name in a humorous and univer-\nsally recognizable way. The edit must involve only a single-character change (adding, removing, or substituting\none letter) and should prioritize simplicity, absurdity, and surprise to evoke humor effectively. Ensure the edit\nmaintains a clear connection to the original name, making the humor immediately recognizable and universally\nunderstandable, while avoiding overly specific or niche references.\nKey Guidelines: 1. Simplicity and Surprise: 2. Cultural Universality: 3. Absurdity and Creativity:\nEvaluation Metrics:\nStrategies for Simplicity and Surprise:\nSystematic Testing Strategies:\nExamples:\nRefinement for Evaluation Metrics:\nFocus on generating edits that are simple, surprising, and universally amusing, ensuring they strictly adhere to\nthe one-character constraint and meet the evaluation criteria for humor, cultural relevance, and clarity. Test each\nedit with a diverse set of sample inputs and audiences to validate its humor consistency and cultural universality,\nensuring the edit is immediately recognizable and universally understandable.\n\n     Table 14: The table shows the final optimized prompt for the Ruin Names task of BBH using the MARS method.\n\n\n\n\n\nSports Understanding\nEvaluate the plausibility of the following sports-related sentence by considering the following key aspects:\n1. Player Abilities and Historical Performance:\n2. Event Context and Historical Significance:\n3. Terminology and Sport-Specific Knowledge:\n4. Rarity vs. Impossibility:\nGuidelines: - If the action is rare but historically documented or consistent with the player’s abilities, consider it\nplausible. - For lesser-known players or niche sports, evaluate based on typical performance levels and historical\nprecedents within that sport. - Prioritize consistency with the sport’s rules, norms, and historical records.\nExamples:\nAdditional Context for Ambiguous Cases:\nRationale Requirement:\nPotential Biases and Limitations:\nEdge Cases and Testing:\nSimplified Evaluation Process: - Focus on the core aspects of player abilities, event context, and sport-specific\nknowledge to streamline the evaluation. - Use historical examples and edge cases as supplementary references\nrather than primary determinants to avoid over-reliance and potential biases.\nOutput ’yes’ if the sentence is plausible, or ’no’ if it is not, followed by a brief rationale. Now, evaluate the\nfollowing sentence: [input sentence].\n\n Table 15: The table shows the final optimized prompt for the Sports Understanding task of BBH using the MARS method.\n\nCollege Biology\nGenerate a set of multiple-choice biology questions that explicitly test higher-order thinking skills, such as appli-\ncation, analysis, and synthesis, within the specific contexts of cellular structure, molecular biology, and ecology.\nEach question should require students to apply biological principles to novel scenarios, analyze complex biological\nsystems, or synthesize information from multiple disciplines to arrive at a solution. Ensure that the questions are\nscientifically accurate, grounded in established biological principles, and reflect current research trends in these\nareas. For each question, provide a clear, concise, and scientifically valid explanation for the correct answer, de-\ntailing how the interdisciplinary nature of biology informs the reasoning. The explanations should not only justify\nthe correct answer but also deepen understanding of the underlying biological concepts, fostering both accuracy\nand conceptual clarity. Additionally, include specific examples of how higher-order thinking skills are integrated\ninto the questions, such as requiring students to predict outcomes based on molecular interactions, analyze eco-\nlogical data to infer population dynamics, or synthesize cellular and molecular processes to explain organismal\nbehavior. To optimize the challenge level, ensure that the questions are neither too simplistic nor overly complex,\nstriking a balance that is appropriate for college-level biology students. This approach will ensure the questions are\ncomprehensive, robust, and aligned with the goal of testing advanced cognitive skills in biology while maintaining\nrelevance to the specified topics. Furthermore, refine the prompt to explicitly guide the language model to generate\nquestions that test higher-order thinking skills while maintaining scientific accuracy and relevance to college-level\nbiology. Optimize specific elements of the current prompt to better align with the goal of producing questions that\nbalance challenge and clarity, ensuring they are neither too simplistic nor overly complex. This includes empha-\nsizing the need for questions to be contextually rich, requiring students to integrate multiple biological concepts,\nand ensuring that the difficulty level is calibrated to challenge students without overwhelming them. The refined\nprompt should also encourage the generation of questions that are clear, concise, and free from ambiguity, while\nstill requiring deep biological reasoning to arrive at the correct answer.\n\n  Table 16: The table shows the final optimized prompt for the College Biology task of MMLU using the MARS method.\n\nCollege Medicine\nRefined Prompt:\nAnalyze the following scenario step by step, integrating interdisciplinary knowledge from biochemistry, sociology,\nand reasoning to identify the psychological framework that best explains unconscious bias in medical practice...\nNext, evaluate each option (Behaviorist, Psychoanalytic, Cognitive Behavioral, Humanistic) by considering how\nwell it explains the influence of unconscious bias on clinical decision-making. ...\nTo encourage deeper critical thinking, incorporate elements of Socratic questioning by asking probing questions\nsuch as...\nEnsure the prompt is structured clearly and concisely, balancing detailed theoretical explanations with clarity to\nguide the model effectively toward identifying the correct psychological framework. ...\nTo optimize the prompt for generating high-quality, contextually appropriate multiple-choice questions for a col-\nlege medicine test, incorporate the following elements: 1. Clarity and Precision: 2. Depth and Relevance: 3. Align-\nment with Learning Objectives: 4. Distractor Quality: 5. Contextual Examples: 6. Theoretical and Practical Bal-\nance:\nBy incorporating these elements, the prompt will guide the model to generate questions that are not only accurate\nand relevant but also aligned with the objectives of a college medicine test, ensuring a high correctness rate and\neducational value. ...\nAdditional Instructions for Generating High-Quality Distractors:\nEnhancements Based on New Questions:1. Inclusion of Real-World Examples: 2. Iterative Testing and Refinement:\nBy following these steps, the prompt will be continuously improved to generate questions that are both challenging\nand aligned with the learning objectives of a college medicine test, ensuring that students are effectively tested on\ntheir ability to apply interdisciplinary knowledge to real-world medical scenarios involving unconscious bias.\nSpecific Adjustments for Enhanced Critical Analysis and Practical Application:1. Interdisciplinary Integration: 2.\nScenario-Based Questions: 3. Critical Thinking Emphasis:4. Practical Mitigation Strategies:\nBy making these adjustments, the prompt will better align with the learning objectives of a college medicine test,\nensuring that students are not only tested on foundational knowledge but also challenged to critically analyze and\napply interdisciplinary concepts in real-world medical scenarios involving unconscious bias.\nFurther Refinement for Detailed Explanation and High-Quality Distractors:\nFinal Refinement for Enhanced Real-World Application and Iterative Testing:\n1. Real-World Application:\n2. Iterative Testing and Refinement:\nBy following these steps, the prompt will be continuously improved to generate questions that are both challenging\nand aligned with the learning objectives of a college medicine test, ensuring that students are effectively tested on\ntheir ability to apply interdisciplinary knowledge to real-world medical scenarios involving unconscious bias.\nSpecific Adjustments for Enhanced Real-World Application and Distractor Quality:\n1. Interdisciplinary Integration:\n2. High-Quality Distractors:\nBy making these adjustments, the prompt will guide the model to generate questions that not only accurately\nidentify the correct psychological framework but also provide a detailed explanation of how unconscious bias\nmanifests in specific medical scenarios and its impact on patient outcomes...\n\n Table 17: The table shows the final optimized prompt for the College Medicine task of MMLU using the MARS method.\n\nElectrical Engineering\n Analyze the question by focusing on the specific conditions of the Barkhausen criterion for oscillators, which are\n loop gain and phase shift. ...\n Next, provide a clear, step-by-step explanation of the Barkhausen criterion, emphasizing the two fundamental\n requirements: 1. Loop gain must be exactly unity for sustained oscillations.2. Phase shift of the feedback signal\n must be 0° or 360° relative to the input.\n To enhance understanding, include specific real-world examples, such as the design of an LC oscillator or a phase-\n locked loop, to illustrate how the Barkhausen criterion is applied in practical scenarios...\n Proceed to evaluate each option (A, B, C, D) systematically, using the following structure for clarity...\n For each option, connect the reasoning back to fundamental electrical engineering principles and provide real-\n world examples or applications where the Barkhausen criterion is critical...\n Conclude the response by reiterating the correct answer (D) and summarizing its significance in practical electrical\n engineering applications...\n To ensure the prompt’s structure and depth enhance the language model’s ability to generate accurate and relevant\n responses, consider the following adjustments: 1. Clarify the introduction 2. Focus on critical concepts 3. Use\n structured evaluation: Systematically evaluate each option with clear, logical reasoning and real-world examples\n to reinforce understanding and relevance. 4. Iterative refinement\n By structuring the response in this manner and iteratively refining the prompt...\n Additional Considerations: 1. Influence of Real-World Examples 2. Structural Adjustments\n Refinement for Multiple-Choice Evaluation: 1. Explicitly state the evaluation criteria 2. Incorporate real-world\n scenarios 3. Maintain brevity and clarity 4. Highlight key takeaways\n By refining the prompt in this manner, the language model will be better equipped to...\n Iterative Refinement Process: 1. Initial Response Generation 2. Review for Accuracy and Relevance 3. Adjust\n Prompt Accordingly 4. Repeat the Process\n This iterative approach ensures that the prompt evolves to better guide the language model, resulting in responses\n that are not only theoretically sound but also practically relevant and aligned with real-world electrical engineering\n applications.\n Optimizing the Iterative Refinement Process: 1. Incorporating Feedback Loops: 2. Enhancing Real-World Context:\n 3. Balancing Depth and Brevity: 4. Focusing on Key Concepts:\n By implementing these optimizations, the iterative refinement process...\n Explicit Guidance for Multiple-Choice Evaluation: 1. Explicitly State the Evaluation Criteria 2. Incorporate Real-\n World Scenarios 3. Maintain Brevity and Clarity 4. Highlight Key Takeaways\n Adjustments for Balancing Theoretical Depth and Practical Application: 1. Focus on Core Principles 2. Use Struc-\n tured Evaluation 3. Avoid Overloading with Details 4. Incorporate Real-World Examples\n By refining the prompt in this manner, the language model will be better equipped to generate responses...\n Specific Adjustments for Real-World Examples: 1. Demonstrate Practical Implications 2. Highlight Design Con-\n siderations 3. Provide Contextual Understanding\n Balancing Theoretical Depth and Practical Relevance: 1. Integrate Theoretical and Practical Elements 2. Maintain\n Focus on Core Principles 3. Use Clear, Concise Language\n By incorporating these adjustments, the prompt will guide the language model to...\n Influence of Real-World Examples: 1. Illustrate Practical Applications 2. Highlight Consequences of Deviations 3.\n Provide Contextual Understanding\n Optimizing the Iterative Refinement Process: 1. Incorporating Feedback Loops 2. Enhancing Real-World Context\n 3. Balancing Depth and Brevity 4. Focusing on Key Concepts\n By implementing these optimizations, the iterative refinement process will enhance the language model’s ability\n to generate responses that are both theoretically accurate and practically relevant, ensuring a high correctness rate\n and alignment with real-world electrical engineering applications.\n\nTable 18: The table shows the final optimized prompt for the Electrical Engineering task of MMLU using the MARS method.\n\nHigh School World History\n Generate a set of multiple-choice questions that test both factual knowledge and critical analysis of the intercon-\n nected historical developments of the Ottoman Empire, economic imperialism, and World War I. Each question\n should require students to analyze how these events influenced each other, leading to the outbreak of World War I,\n with a focus on cause-and-effect relationships and broader historical significance.\n  Instructions for Question Design: 1. Interconnectedness and Cause-and-Effect: 2. Accessibility and Rigor: 3. Bal-\n anced Difficulty: 4. Critical Thinking and Historical Significance: 5. Format and Contextual Accuracy:\n Example Question with Passage:: one example Additional Constraints: - Engagement and Relatability: Use engag-\n ing and relatable examples or analogies where appropriate to make the questions more accessible and interesting to\n  students. For instance, compare historical events to modern-day scenarios to help students draw parallels. - Depth\n of Analysis: Include questions that require students to analyze multiple layers of historical causation, such as how\n economic imperialism not only influenced European powers but also destabilized regions like the Balkans, con-\n  tributing to the outbreak of World War I. - Historical Contextualization: Ensure that each question provides enough\n  historical context for students to understand the significance of the events being discussed, without overwhelming\n them with unnecessary details.\n By following these guidelines, generate a set of 5-10 multiple-choice questions that effectively test students’ un-\n derstanding of the interconnectedness of the Ottoman Empire’s decline, economic imperialism, and World War I,\n while promoting critical thinking, historical analysis, and a deeper appreciation of cause-and-effect relationships\n  in history.\n\nTable 19: The table shows the final optimized prompt for the High School World History task of MMLU using the MARS\nmethod.\n\n\n\n\n Human Aging\n Refine the hierarchical elimination process to ensure the model accurately distinguishes between overlapping\n themes like cognitive decline and personality changes, especially when new terminology such as ’neuroinflam-\n mation’ is introduced, by implementing the following steps:\n  1. Test the Hierarchical Elimination Process with a Sample Question:\n  2. Optimize the Dynamic Scoring System and Contextual Weighting:\n  3. Enhance the Focus Identification Protocol with Continuous Learning:\n  4. Dynamic Evidence Integration with Contextual Weighting:\n  5. Source Reliability Scoring with Provisional Scoring for Emerging Evidence:\n  6. Evidence Strength Assessment with Contextual Weighting:\n  7. Specific Metrics for Question Evaluation:\n By refining the hierarchical elimination process with these steps and incorporating specific metrics, the model\n can more effectively navigate overlapping themes in human aging questions, ensuring the highest correctness rate\n while maintaining precision and contextual relevance.\n\n    Table 20: The table shows the final optimized prompt for the Human Aging task of MMLU using the MARS method.\n\n\n\n\n Marketing\n Analyze the following marketing-related question step by step, considering the principles of segmentation, pricing,\n market research, and other relevant marketing concepts. Carefully evaluate each of the provided options (A, B,\n C, D) and select the most suitable answer based on your analysis. Ensure your reasoning is clear and aligns\n with established marketing theories and practices. For example, if the question involves a hierarchy of effects or\n sequential model used in advertising, identify the correct model from the options provided and justify your choice.\n Proceed methodically to arrive at the most accurate answer.\n\n      Table 21: The table shows the final optimized prompt for the Marketing task of MMLU using the MARS method.\n\nGSM8K\n Think step by step to solve linguistically diverse elementary school math application problems. Break down the\n problem into 2-8 logical steps, perform the necessary calculations at each step, and provide the final result. Ensure\n accuracy by carefully following the problem’s instructions and verifying each intermediate step. For example:\n  Input: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her\n  friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How\n much in dollars does she make every day at the farmers’ market?\n Step 1: Calculate the total eggs used daily: 3 (eaten) + 4 (baked) = 7 eggs. Step 2: Subtract the used eggs from the\n  total laid: 16 - 7 = 9 eggs. Step 3: Calculate the daily earnings: 9 eggs × 2 =18. Answer: 18\n Follow this structured approach to solve similar problems.\n\n            Table 22: The table shows the final optimized prompt for the GSM8K task using the MARS method.\n\n\n\n LSAT-AR\n Carefully analyze the given scheduling problem step by step, prioritizing logical reasoning, reading comprehen-\n  sion, and analytical reasoning to ensure a thorough evaluation. Begin by explicitly listing and understanding all the\n  constraints, with a focus on the most critical ones first. Follow this structured approach to systematically eliminate\n options that violate any of the given conditions:\n  1. Prioritize the most restrictive constraints first.\n  2. Evaluate secondary constraints.\n  3. Assess the implications of Nina’s scheduling.\n Throughout this process, avoid making assumptions beyond the provided constraints. Do not infer additional rules\n or conditions that are not explicitly stated. Stick strictly to the given information and apply logical reasoning to\n  interpret and enforce the constraints.\n By adhering to this structured, methodical approach, you will systematically eliminate incorrect options and ar-\n  rive at the correct schedule with the highest accuracy. This process mirrors the analytical rigor required in legal\n reasoning and ensures that the model’s output aligns with the principles of logical and legal analysis.\n\n      Table 23: The table shows the final optimized prompt for the LSAT-AR task of AGIEval using the MARS method.\n\n\n\n Art Studies\n Please delve into the historical period represented by each option, paying particular attention to major break-\n throughs or developments in textile technology and dye processes. First, collate the cultural context and technolog-\n  ical advances of each period and analyze which period’s technological achievements are most likely to be relevant\n  to the method of blue print fabric printing. Based on this, the accuracy of the model in answering questions related\n  to these historical and technological contexts is assessed. The output of the model is evaluated by setting specific\n judgment criteria, such as accurate description of the historical context, sound reasoning about process character-\n  istics, and coherence of conclusions. Based on these criteria, the presentation of the prompts is iteratively adjusted\n and optimized to improve the model’s performance in selecting correct answers.\n\n      Table 24: The table shows the final optimized prompt for the Art Studies task of C-Eval using the MARS method.\n\n\n\n Urban And Rural Planner\n When optimizing prompts for assessing waste management plans in urban and rural planning, how can identify-\n ing aspects of solid pollutant control planning that are less emphasized (e.g., e-pollutants) help us improve our\n assessment methods? When testing prompts, what specific criteria should we consider to effectively assess their\n accuracy and relevance with respect to nuances in waste management programs? In addition, how can we ensure\n  that models can accurately understand and prioritize the treatment of different types of waste to effectively guide\n urban and rural planning decisions?\n\nTable 25: The table shows the final optimized prompt for the Urban And Rural Planner task of C-Eval using the MARS method.\n\nClinical Medicine\nIn order to improve the accuracy of choosing the most appropriate answer in a clinical medicine test question, it is\ncrucial to systematically compare the key symptoms in the question stem with each of the options on a case-by-case\nbasis. The key to this process is to 1) accurately identify diagnosticallysymptoms and features in the question stem,\n2) logically assess and eliminate these features based on their association with the options, and 3) apply clinically\ntypical presentations and relevant background knowledge to validate the plausibility of each option. Based on\nthis, the following iterative adjustments should be made: first, by continuously acquiring clinical knowledge to\nstrengthen the identification of difficult symptoms; second, by adjusting the strategy in order to be more flexible\nin matching potential answers; and finally, by utilizing reflection and evaluating the effectiveness of the model in\nresponding to similar questions over time, to identify and correct deficiencies. This fine-tuning and analysis can\nincrease the probability of choosing the correct answer.\n\n Table 26: The table shows the final optimized prompt for the Clinical Medicine task of C-Eval using the MARS method.",
"headers": [
"arXiv:2503.16874v2  [cs.CL]  12 Nov 2025",
"MARS: Multi-Agent Adaptive Reasoning with Socratic Guidance",
"for Automated Prompt Optimization",
"Jian Zhang",
", Zhangqi Wang",
", Haiping Zhu",
", Kangda Cheng",
",",
"Kai He",
", Bo Li",
", Qika Lin",
", Jun Liu",
", Erik Cambria",
"Introduction",
"Methodology",
"Experiments",
"Supplementary Analysis",
"Related Works",
"Conclusion",
"Acknowledgments",
"References",
"Proof of Proposition",
"Tasks and Datasets",
"Generalization Across Different Base and",
"Target Models",
"Experiment Settings and Baselines",
"Prompts for Agents",
"Full-process Prompt Optimization",
"Sample Size Analysis",
"Universal Optimum Solution"
],
"tables": [
"|Models B.E D.QA F.F. G.S. R.N. S.U.|C.B. C.M. E.E. W.H. H.A. M.T.|Avg.|\n|---|---|---|",
"|Origin 74.70 51.41 52.20 43.37 59.84 60.24<br>CoT(ZS) 80.32 54.22 59.44 47.39 67.07 67.87<br>CoT(FS) 81.93 57.43 66.26 49.40 70.68 72.29<br>APE 83.53 61.85 61.04 51.41 77.51 74.70<br>ProTeGi 83.93 63.86 62.65 52.21 80.32 76.71<br>OPRO 86.34 66.67 63.45 53.81 83.13 82.73<br>PE2 87.95 65.46 63.86 54.62 84.34 75.90|82.52 69.77 63.89 73.73 66.22 81.55<br>83.91 73.25 74.31 76.27 68.47 84.98<br>86.71 76.74 79.17 78.81 72.07 90.99<br>88.11 75.58 69.44 82.20 75.68 87.98<br>90.91 78.49 73.61 84.75 77.48 90.56<br>93.70 83.14 77.01 86.44 79.73 92.70<br>93.01 81.40 76.39 88.14 81.08 93.56|64.95<br>69.79<br>73.54<br>74.09<br>76.29<br>79.07<br>78.81|\n|---|---|---|",
"|Ours 93.17 71.89 74.70 59.44 90.36 87.95|97.90 86.05 84.03 93.22 85.59 97.00|85.11|\n|---|---|---|",
"|Chinese<br>Models<br>A.S. U.R.P. CL.M.|Math<br>GSM.|Law<br>L.A.|Avg.|\n|---|---|---|---|",
"|Origin 56.25 48.89 57.14<br>CoT(ZS) 59.38 53.33 61.90<br>CoT(FS) 65.63 57.78 66.67<br>APE 65.63 62.22 71.43<br>ProTeGi 68.75 66.67 76.19<br>OPRO 71.88 73.33 80.95<br>PE2 75.00 77.78 76.19|67.07<br>70.26<br>77.54<br>74.81<br>77.47<br>81.56<br>83.46|23.14<br>30.57<br>35.81<br>29.69<br>31.88<br>31.44<br>34.50|50.50<br>55.09<br>60.69<br>60.76<br>64.19<br>67.83<br>69.39|\n|---|---|---|---|",
"|MARS 81.25 84.44 85.71|89.22|38.42|75.81|\n|---|---|---|---|",
"|Variation|B.E. D.QA F.F. G.S. R.N. S.U. Avg.|\n|---|---|",
"|MARS|93.17 71.89 74.70 59.44 90.36 87.95 79.59|\n|---|---|",
"|w/o<br>Plan<br>∆<br>w/o<br>Soc<br>∆<br>w/o<br>Cri<br>∆|86.35 65.86 68.67 54.21 82.33 79.52 72.82<br>(-6.82) (-6.03) (-6.03) (-5.23) (-8.03) (-8.43) (-6.77)<br>84.74 63.86 62.25 49.80 74.30 74.70 68.28<br>(-8.43) (-8.03) (-12.45) (-9.64) (-16.06) (-13.25) (-11.31)<br>89.16 68.27 72.28 56.22 86.34 83.94 76.04<br>(-4.01) (-3.62) (-2.42) (-3.22) (-4.02) (-4.01) (-3.55)|\n|---|---|",
"|Deepseek<br>Base<br>-V2.5 -R1|GPT<br>-3.5 -4 -4o|Avg.|\n|---|---|---|",
"|Origin 56.96 61.48<br>CoT(ZS) 62.72 73.82|44.79 49.70 55.84<br>63.45 66.94 70.38|53.75<br>67.46|\n|---|---|---|",
"|MARS 79.59 83.05|69.30 73.21 80.86|77.20|\n|---|---|---|",
"|Tasks BBH MMLU Chinese GSM. L.A.|Avg.|\n|---|---|",
"|Tasks Train|B.E. D.QA F.F. G.S. R.N. S.U.|\n|---|---|",
"|Origin 60.92 83.73 58.26 72.31 20.96<br>CoT(ZS) 62.81 85.62 64.26 76.25 24.45<br>CoT(FS) 63.42 88.27 68.69 83.92 28.82<br>APE 64.36 86.72 69.03 81.18 30.13<br>ProTeGi 76.43 86.35 73.52 82.70 31.88<br>OPRO 78.73 88.25 75.79 84.74 32.75<br>PE2 77.59 91.89 74.67 85.43 35.81|59.24<br>62.68<br>66.62<br>66.28<br>70.18<br>72.05<br>73.08|\n|---|---|",
"|APE 100<br>ProTeGi 20<br>OPRO 50<br>PE2 100|83.53 61.85 61.04 51.41 77.51 74.70<br>83.93 63.86 62.65 52.21 80.32 76.71<br>86.34 66.67 63.45 53.81 83.13 82.73<br>87.95 65.46 63.86 54.62 84.34 75.90|\n|---|---|",
"|MARS 0<br>MARS 1<br>MARS 3|90.76 70.28 73.09 57.83 88.35 85.94<br>93.17 71.89 74.70 59.43 90.36 87.95<br>93.57 72.69 74.30 60.24 89.96 88.35|\n|---|---|",
"|MARS 81.13 92.82 78.11 90.97 40.17|76.58|\n|---|---|",
"|Model 1 2 3|Avg|\n|---|---|",
"|p Let’s Let’s Let’s<br>0<br>think step work this proceed<br>by step. out step with our<br>by step. tasks one<br>by one.|Col2|\n|---|---|",
"|MARS 79.59 78.53 75.30|77.81|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2503.16874v2.pdf"
}