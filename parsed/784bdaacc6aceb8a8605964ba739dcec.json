{
"text": "Preprint\n\n\n          DELVEPO: DIRECTION-GUIDED SELF-EVOLVING\n         FRAMEWORK FOR FLEXIBLE PROMPT OPTIMIZATION\n\n\n                    Tao Tao, Guanghui Zhu∗, Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang\n                          State Key Laboratory for Novel Software Technology, Nanjing University\n                         taot@smail.nju.edu.cn, zgh@nju.edu.cn\n\n\n                                       ABSTRACT\n\n                           Prompt Optimization has emerged as a crucial approach due to its capabilities in\n                                  steering Large Language Models to solve various tasks. However, current works\n                             mainly rely on the random rewriting ability of LLMs, and the optimization pro-2025                          cess generally focus on specific influencing factors, which makes it easy to fall\n                                  into local optimum.  Besides, the performance of the optimized prompt is of-\n                                 ten unstable, which limits its transferability in different tasks. To address theOct                       above challenges, we propose DelvePO (Direction-Guided Self-Evolving Frame-\n21                     workpromptsfor inFlexibleself-evolvePromptmanner.Optimization),In our framework,a task-agnosticwe decoupleframeworkpromptsto optimizeinto\n                                   different components that can be used to explore the impact that different fac-\n                                    tors may have on various tasks. On this basis, we introduce working memory,\n                              through which LLMs can alleviate the deficiencies caused by their own uncer-\n                                     tainties and further obtain key insights to guide the generation of new prompts.\n                               Extensive experiments conducted on different tasks covering various domains for\n                              both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B,[cs.CL]                              Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO\n                                  consistently outperforms previous SOTA methods under identical experimental\n                                    settings, demonstrating its effectiveness and transferability across different tasks.\n\n\n                1  INTRODUCTION\n\n                   The rapid advancement of Large Language Models (LLMs) (DeepSeek-AI, 2025; Li et al., 2025)\n                      has revolutionized various real-world applications (Shao et al., 2024; Zheng et al., 2025) . Prompt,\n                      a method that steers LLMs to produce desired results without modifying parameters, has garnered\n                          significant interest among non-AI experts from different domains (Wan et al., 2024; Guo et al., 2025;\n                     Fernando et al., 2024). Consequently, the rapid growth in users has increased demand for prompt\n                       engineering methods.\n\n                      Previous efforts primarily focused on manually designing specialized prompts (Brown et al., 2020;\n                    Kojima et al., 2022; Wei et al., 2023). However, this kind of method is time-consuming and demandsarXiv:2510.18257v1                 extensive trial and error, making it less versatile for diverse tasks and limiting their real-world effec-\n                          tiveness. To reduce the human effort required for constructing effective prompts, many researches\n                  (Shum et al., 2023; Wang et al., 2023c; Zhang et al., 2022; Feng et al., 2024; He et al., 2024) have\n                        increasingly explored methods such as curating unified demonstrations for related tasks, systemati-\n                         cally designing domain-specific templates, and identifying critical factors for prompt performance.\n                    However, these methods exhibit limited applicability across diverse scenarios.\n\n                       Subsequently, a series of research emerged that employ optimization algorithms to refine prompts.\n                   Such approaches (e.g. APE (Zhou et al., 2023b), PromptBreeder (Fernando et al., 2024), and Evo-\n                    Prompt (Guo et al., 2025)) synergistically integrate the efficiency inherent in the algorithms with the\n                      powerful text processing ability of LLMs, achieving relatively stable performance enhancement on\n                          target datasets. Although these studies analogize the mutation operation in evolutionary algorithms\n                         to the rewriting operation of LLMs, they fail to fully harness the efficiency and rapid convergence\n                        inherent to such algorithms, which ultimately limits the realization of their performance advan-\n                        tages in prompt optimization. The primary reason lies in the inherently stochastic nature of the\n\n                          ∗Corresponding Author\n\n\n                                                           1\n\nPreprint\n\n\n\n\n\nevolutionary process: the directionality of mutation operations remains uncontrollable, while their\ninterpretability is also notably limited. Furthermore, these methods neglect the potential impact of\nconstituent components within a prompt on overall performance, leading to premature convergence\nin local optima. For example, during evolutionary phase of EvoPrompt, the initial prompt inherently\ncontains the \"role\" as a critical component. However, due to the stochastic nature of the mutation\nprocess, the stochastic mutation process may accidentally remove this component. Once discarded,\nit cannot be reintegrated into subsequent evolutionary iterations. Such degradation significantly\nheightens the risk of premature convergence in local optima. A parallel limitation is observed in\nthe PromptBreeder method, which exhibits even higher stochasticity, as its implementation not only\nuses two distinct mutation prompts but also employs diverse mutation operators, amplifying ran-\ndomness throughout the optimization process. We summarize that a robust Prompt Optimization\n(PO) must have the following characteristics:\n\n• Seamlessly integrating domain expert experience: For tasks in different domains, prior experi-\n  ence from domain experts can be incorporated into the PO algorithm, thus improving the efficiency\n  of the optimization process.\n• Actively exploring factors that may affect prompt performance: The method can actively\n  explore factors affecting prompt performance to guide optimization using historical data.\n• Adaptively identifying optimal prompts for different LLMs with varying performance: The\n  algorithm self-adjusts to discover the best prompts for target tasks across differently specialized\n  models and scenarios, ensuring broad applicability in diverse professional contexts.\nIntegrating insights from existing research, we propose DelvePO 1 (Direction-Guided Self-Evolving\nFramework for Flexible Prompt Optimization) that adaptively accommodates diverse LLMs and\nself-improves through guidance from its historical optimization strategies. Inspired by the concept of\nLoci (the corresponding location of genes with important functions) and Alleles (different versions\nof the same gene) on genetics, this framework first decouples prompt instructions into functional\ncomponents (analogous to Loci). Subsequently, it iteratively evolves these components by explor-\ning the potential impacts of diverse allele variations, ultimately achieving holistic optimization of the\nentire prompt through systematic recombination. In particular, building upon the components, we\nintroduce working memory mechanism (i.e., Component Memory and Prompt Memory) to guide the\nevolutionary process. Component Memory is designed to capture evolutionary trends in individual\ncomponents and utilize these trends to guide further optimization of each element. Take the com-\nponent a step further, Prompt Memory creates interconnections between components by utilizing\ncontextual information to guide the progressive optimization of the entire prompt. The contributions\nof our work can be summarized as follows:\n\n• To the best of our knowledge, our work is the first to introduce memory mechanism to guide\n  prompt optimization, not only stabilizing the performance of the entire prompt population but\n  also greatly reducing the time required for evolutionary operations.\n• By decoupling prompt into multiple components and designing guided evolutionary mechanisms,\n  our framework integrates multiple influencing factors into a single prompt. This integration not\n  only enhances the scalability of PO methods but also improves the interpretability of the optimiza-\n  tion process, significantly lowering the difficulty to interact with the system.\n• For LLMs with varying performance levels, our framework can elicit their capabilities, striking\n  a good balance between exploring diverse components and exploiting the current derived good\n  components, ultimately obtaining optimal prompts that adapt to the target tasks and LLMs simul-\n  taneously. Extensive experimental results on multiple datasets and three widely-adopted LLMs\n  reveal that DelvePO outperforms manually crafted prompts and existing PO methods.\n\n\n2  PRELIMINARIES\n\n\nGiven task T = (D, A), D is the task-related dataset and A represents the corresponding answer\nto the dataset, prompt optimization can be briefly described as follows: Guided by the working\nmemory mechanism, the initial prompt population Pinit = {p1, p2, · · · } is continuously optimized\nto obtain the final prompt population Pfinal. The best prompt p∗can be selected as follows:\n\n\n   1DelvePO is available at https://github.com/PasaLab/DelvePO\n\n\n                                       2\n\nPreprint\n\n\n\n\n\n                  p∗←arg max feval ϕLLM(p, Ddev), A\n                                     p∈Pfinal\n\n\nwhere Ddev is the development dataset and ϕLLM(p, Ddev) means that the prompts and questions\nare combined and then fed into the LLM to produce the corresponding response. The important\nconcepts used in our proposed framework are described below.\n\nComponents Similar to the relationship between Loci and Chromosome, components are mainly\nused to identify the location of key factors that affect task performance in prompts. Different tasks\ncan introduce distinct components or reuse existing ones. Components are extensible, i.e., the type\nand number of components can be user-defined, and our method can also evolve synchronously as\nthe context length that LLMs can receive increases. In this paper, we construct a comprehensive\nand representative component pool from a broad set of related literature. Further details on how the\ncomponents are studied and predefined in our framework are provided in Appendix E.\n\nTemplates To bind components to prompts, we design a general template (corresponding to the\nChromosome functionally), whose content is mainly composed of two parts: general and unchang-\ning text; domain-specific and replaceable descriptive text (i.e., components and their correspond-\ning values). For the descriptive text, its main functions include explaining domain-specific com-\nponents, connecting different components, and providing contextual semantics about components.\nTo overcome the instability of LLMs in recognizing components, we borrow the design idea of\n\"markup\" from HyperText Markup Language (HTML) to define different domain components. Tak-\ning \"<role></role>\" as an example, the \"role\" is one of the various component types. Accordingly,\nthe value of the component will be enclosed within the markup pairs, i.e., <role>Sentence Simpli-\nfier</role>. More details can be found in Figure 5 in Appendix F.\n\n\n3  METHODOLOGY\n\n\n3.1  FRAMEWORK OF DELVEPO\n\nOur self-evolution prompt optimization framework consists of 4 necessary functional modules:\nSampling & Update module, Task-Evolution module, Solution-Evolution module and Memory-\nevolution module. We define the Task as \"discover the promising direction of evolution\", that is,\ndetermining the component (types or values) that need to evolve in the next step under the guidance\nof components memory. We define the Solution as \"make sure the process of evolutionary opera-\ntion and perform evolutionary operation\", i.e., under the guidance of prompts memory, evolutionary\noperations are applied to the component values according to the selected evolution type: for a sin-\ngle sample, only mutation is performed, while for two samples, both mutation and crossover are\nexecuted. For memory-evolution, it mainly uses the evolved prompts and component value pairs\nbefore and after evolution to update the prompts memory and components memory, respectively.\nIn the sampling and update module, when the number of iterations reaches a pre-defined value, the\npopulation is updated. Otherwise, a new sampling operation is performed within the current popu-\nlation, which in turn triggers the next round of self-evolution operations. The designs of DelvePO\nframework is shown in Figure 1. Next, we first introduce the working memory mechanism.\n\nComponents Memory stores the corresponding component values before and after evolution, which\nis selected according to the mutated component type. The value pairs will be ordered by descend,\ni.e., when injecting to the final prompt, the first value performs better than the second. Components\nMemory will guide the selection of components in the Task-Evolution stage.\n\nPrompts Memory stores the prompts after each step of evolution. The evolved prompts are stored\nin descending order according to their performance scores. There are two forms of prompts mem-\nory: discrete form and continuous form. The discrete version only stores discrete combinations of\ncomponent value in the prompt. And the continuous version stores a complete prompt formed by\ninjecting component value into the template, which means that it stores continuous text containing\ncontext. Prompts memory will be used to guide the mutation of component or the crossover of the\nprompt in the Solution-Evolution stage.\n\n\n                                       3\n\nPreprint\n\n\n\n\n\n          ① Sampling (& Update)                   ② Task-Evolution\n         Populations (old)             Sampled prompt(s): 1 or 2             Raw Task: Traw\n         p1  < p1, s1 > Sample                                             pCasepCase II       pCasepCasepCase#1#1 IIIIII   pCasepCasepCase#2#2  IIIIII                     Task description\n      ⋮   ⋮\n           pi  < pi,  si >                                                                         Mutation Only     Mutation and Crossover\n      ⋮   ⋮       Top-N       2N Populations                  CaseCase II            CaseCase IIII         Task evolve\n        pN < pN, sN >              ⋮⋮   ⋮⋮                   Sub-taskSub-task TsubTsub                                                                                                                                       11      Sub-taskSub-task TsubTsub 22\n                                         << p,p, ss >>                          Sort       pp\n         Populations (new)                             #candidates = 2N     ⋮⋮   ⋮⋮                         Evolved Task: Tevo\n       p'p' << p',p', s'>s'>                  p'p' << p',p', s'>s'>\n      ⋮⋮   ⋮⋮                 ⋮⋮   ⋮⋮\n\n\n          ④ Memory-Evolution                   ③ Solution-Evolution\n\n\n                                                                                     Raw Solution: Sraw\n\n                                                                                                            Solution description\n      Component                   Prompt                   Component                                                           Prompt                        Evolved Solution: Sevo     Memory    Memory                                  Memory    Memory\n       Mcomponents    Mprompts                       Mcomponents    Mprompts                                                           Solution evolve\n     Working Memory (old) MemoryMemory evolveevolve Working Memory (new)\n\n                      Guide\n\n\n                                                                             Prompt for Tsub 1     Prompt for Tsub 2                PromptPrompt forfor TrawTraw      s < s'     SampledSampledp   <p,promptprompts>              Evolvedp'   <p',prompts'>     EvaluateDev Dataset\n\n\nFigure 1: The Framework of DelvePO. Initialization begins with predefined components, which\nare concatenated to form individual p; multiple individuals constitute the initial population Pop-\nulations (old).  At each step, one individual (Mutation only) or two individuals (Mutation and\nCrossover) are sampled, and the Sub-task determines the evolutionary direction (i.e., the mutated\ncomponent type). Guided by Task-, Solution-, and Memory-Evolution modules, selected prompts\nare iteratively evolved, contrasting with unguided optimization. The new population Populations\n(new) is accumulated across epochs, and once the threshold is reached, the population is updated to\ninitiate the next round of self-evolution.\n\n\n\n3.2  OVERVIEW OF DELVEPO\n\n\nAs shown in Figure 1, the workflow of DelvePO contains several core stages as outlined below.\n\nInitialization & Sampling: First, we use task-agnostic component-value generation prompt (see\nFigure 4 in Appendix C) to generate candidate values for each component type. Then, we randomly\nsample from these candidates and inject the selected values into the population-construction tem-\nplate (illustrated in Figure 5 in Appendix F) to construct the initial population. Each individual in the\ninitial population is evaluated on the development dataset to obtain its performance score. Finally,\nthe sorted population is stored as the initial prompts memory. Before the population evolves, there is\nno components memory. After initialization, the sampling process begins, aiming to select prompts\nfrom the current population for evolution. Inspired by genetic principles, there are two main ways\nto generate new individuals: mutating a single individual or performing crossover between two indi-\nviduals. Notably, mutation may also occur during crossover. To account for these cases, we assume\nthat the number of individuals selected in each sampling step can be either 1 or 2.\n\nThe evolutionary process mainly includes two parts: generating new individuals based on selected\nindividuals; generating and storing the working memory. Specifically, there are 3 types of evolu-\ntion, namely Task-Evolution, Solution-Evolution, Memory-Evolution. The mechanism of Task-\nEvolution and Solution-Evolution is shown in Figure 2.\n\nTask-Evolution For task evolution, considering the components and the evolutionary operations\n(mutation and crossover), we design two kinds of evolutionary sub-tasks. The detailed information\nis shown in Figure 7 and Figure 8 (see Appendix G).\n\n\n                                       4\n\nPreprint\n\n\n\n\n\n                                                             Evolved Task: Tevo\n\n    Raw Task: Traw\n                                Task evolve        Conclude Insights from {Mcomponents}.   Conclude Insights from {Mcomponents}.\n     Based on the current prompt(s)                       Based on the Insights and current      Based on the Insights and current\n       {p,...}, find a suitable prompt for                      prompt {p}, get promising Direction    prompts {p1, p2}, get promising\n      the downstream task.                                     for mutate.                           Direction for mutate & crossover.\n\n\n                                                             Evolved Solution: Sevo\n\n    Raw Solution: Sraw\n                                      Solution evolve      Conclude Insights from {Mprompts}.    Conclude Insights from {Mprompts}.\n    Given downstream task, rewrite                     Based on the Insights and Direction,   Based on the Insights and Direction,\n     current prompt {p,...} to get a new                     mutate current prompt {p} to get       mutate & crossover current prompts\n     prompt.                                                         final prompt.                               {p1, p2} to get final prompt.\n\n\n\n\n         Raw Prompt                                     Evolved Prompt                   Evolved Prompt\n\n\nFigure 2: The mechanism of Task-Evolution and Solution-Evolution. Using the pseudo-prompt to\nexplain the details of Task- and Solution-Evolution.\n\n\n• Sub-task I: This task mainly uses mutation operations to process a single candidate prompt. First,\n  the semantic comprehension capability of the LLMs is utilized to obtain the relevant insights\n  of component evolution from the component memory Mcomponents. Then, the insights are used\n  to guide the selection of components.  Finally, the selected components will be treated as the\n  promising direction to guide the evolution of mutation-based solution.\n• Sub-task II: After performing Sub-task I on the two candidate prompts, we can get the respective\n  component types set C1 and C2 for two prompts (say p1 and p2) as the promising direction for\n  mutation. The final mutated component type is selected as ˆC = C1 ∩C2. Next, for each compo-\n  nent in ˜C = C \\ ˆC where C denotes the set of all component types, corresponding contents from\n  p1 and p2 are extracted to construct a pair. Then, based on the insights derived from Mcomponents,\n  one value from each pair is selected as the potential value to improve performance of the prompts\n  after evolution. Finally, the component types from ˆC will be treated as the promising direction\n  to guide the evolution of crossover-based solution, and the selected values from p1 or p2 whose\n  component types coming from ˜C will also be passed into the corresponding Solution-Evolution\n  phase to help construct the final prompts.\n\nSolution-Evolution The main goal of solution evolution is to utilize the insights (derived from the\nprompts memory) and direction (received from the task-evolution) to perform evolution operations\non the corresponding content in the current prompt and generate a new prompt that performs better.\nIn this phase, we propose 2 sub-solutions corresponding to 2 sub-tasks. Depending on whether the\nprompt is continuous or discrete, each sub-solution can also be further divided to eliminate the effect\nof prompt format on the final result.\n\n• Sub-solution I: Extract component contents from current prompt based on the results obtained by\n  sub-task I (i.e., the mutated components that are most likely to improve prompt performance). The\n  extracted contents are then mutated using insights obtained from the prompts memory Mprompts\n  stored in discrete or continuous forms. Those contents that have not been mutated will be retained\n  in new prompts. Finally, the mutated and unmutated component contents will be integrated as the\n  result of sub-solution I. The corresponding prompts are shown in Figure 9, 10 (see Appendix H)\n  for the prompts memory in discrete and continuous forms, respectively.\n• Sub-solution II: This mainly uses the results from sub-task II as a guide, and extracts component\n  contents from the currently selected two prompts. And the evolutionary operations would combine\n  mutation and crossover.  First, for components that do not require mutation, the corresponding\n  content is received from sub-task II. Then, for the component that need to be mutated, we extract\n   its content from the two prompts. Based on the evolutionary insights derived from the prompt\n  memory Mprompts, the mutation operations are performed on the extracted content. Next, the\n  generated two prompts will crossover on the component types that need to be mutated. Finally,\n  the results obtained from the mutation and crossover operations are integrated to generate a new\n  prompt as the result of the sub-solution II. The details are shown in Figure 11 for the prompts\n  memory in discrete form and Figure 12, 13 for continuous form (see Appendix H).\n\n\n                                       5\n\nPreprint\n\n\n\n\n\nMemory-Evolution is based on the component pairs and prompts both before and after the evolu-\ntion to update the corresponding components memory and prompts memory, which is used to guide\nthe next evolution process. In this module, the evaluation will be performed. Specifically, to clearly\ndescribe the evaluation process, we illustrate a general form of a prompt designed for LLMs that\ncan be applied across different tasks (shown in Figure 6). Evaluation refers to calculating the per-\nformance score of the generated new prompts on the development dataset based on the evaluation\nmetrics of the target task, according to which components can be sorted and memory can be updated.\n\nUpdate: Add the evolved prompts to the temporary population generated in each iteration. When\nthe iteration ends, the temporary and current populations are mixed, and Top-N is selected as the\nupdated population for the next iteration based on performance.\n\nThe details of DelvePO are outlined in Algorithm 1, which can be found in Appendix C.\n\n\n4  EXPERIMENTS\n\n4.1  EXPERIMENTAL SETTINGS\n\nBaselines In our experiments, We choose 6 commonly used methods which have been widely proven\nto be efficient in the field of prompt optimization as our baselines, which are: Crafted by human\nexperts, CoT-ZS, CoT-FS, Promptbreeder, APE, and EvoPrompt.\n\n• Human corresponds to manually crafted prompts by experts, as detailed in the relevant literature\n  Zhang et al. (2024); Sanh et al. (2022), which primarily derived from prior studies.\n• CoT has been extensively applied in various domains, represents a rationale-based approach. We\n  evaluate two representative forms of CoT: CoT-ZS (Zero-Shot CoT, Kojima et al. (2022)) and\n  CoT-FS (Few-Shot CoT, also known as Manual-CoT, Wei et al. (2023)).\n• APE (Zhou et al., 2023b) regards instructions as programs and uses Monte Carlo Search to select\n  appropriate instructions as optimized prompts under LLM guidance.\n• Promptbreeder (Fernando et al., 2024) further investigates the effect of different mutation strate-\n  gies on self-optimization based on elaborately designed evolutionary operations.\n• EvoPrompt (Guo et al., 2025) introduces evolutionary algorithms to prompt optimization for\n  the first time. Considering different scenarios, it instantiates its framework using two practical\n  evolutionary algorithms. According to its statement, compared with GA method, the DE method\n  has a wider range of use in solving complex problems. Therefore, we select EvoPrompt-DE as\n  our baseline, and denote it simply as EvoPrompt.\n\nDatasets and LLMs To demonstrate the generalizability of our method, we conducted experiments\non 11 datasets across three LLMs, covering diverse domains and representative real-world tasks.\nThe details information about datasets and LLMs are represented in Appendix B. Other experimental\ndetails (e.g., Computational Resources and Hyperparameter Details) are represented in Section 6.\n\n\n4.2  MAIN RESULTS\n\nFollowing the same settings as baselines, we tested the best prompts obtained during training. The\nmain experimental results (as shown in Table 1) on DeepSeek-R1-Distill-Llama-8B are reported as\naverages over three random seeds, with standard deviations provided.  It is worth noting that we\nobserved Promptbreeder to be significantly more time-consuming than other methods (as shown in\nFigure 3). To balance the diversity of baselines and ensure the fairness in training time, we therefore\nreport results for Promptbreeder using a single random seed.\n\nFrom Table 1, we can observe that our method achieves substantial improvements over manual ap-\nproaches. Among the automated optimization methods, our method consistently outperforms base-\nlines, demonstrating not only its effectiveness but also its adaptability to different task types. From\nthe results on classical NLP benchmarks, we observe that the baselines perform well, confirming\ntheir effectiveness on established datasets. However, on more recently introduced benchmarks that\ndemand broader capabilities, automated prompt optimization methods generally perform better, with\nour approach showing particularly substantial improvements. These results indicate that as LLMs\ncontinue to advance, prompt optimization techniques must likewise evolve, and our framework de-\nlivers consistently strong performance across diverse domains.\n\n\n                                       6\n\nPreprint\n\n\n\n\nTable 1: Main results on different downstream tasks for DeepSeek-R1-Distill-Llama-8B. Since\nexpert-written prompts are not available for all datasets, sign (\"-\") is used to indicate missing cases.\n\n\n                               Classical NLP              Question-Answering    Domain-specific  NLG\n  Method                                                                                                    Avg.\n                Subj     MR      CoLA     SQuAD   TREC     FinPB        SAMSum\n\n  Human          26.00        55.89           -                -            54.67           -                 25.68           -\n  CoT-ZS         70.00        68.00        65.45        43.91        68.00        60.00              3.23         56.74\n  CoT-FS         83.00        90.67        70.63        47.92        71.00        68.67              4.25         62.81\n\n  Promptbreeder   35.00        86.00        55.58        54.16        60.00        59.00             27.88        51.20\n APE              74.67(2.85)   83.67(1.67)   68.75(1.20)   67.57(1.62)   42.33(2.40)   70.67(2.33)          30.02(0.85)   61.25\n  EvoPrompt       82.00(2.08)   83.00(1.00)   66.75(2.73)   68.17(1.14)   67.00(1.53)   72.00(1.53)          29.18(0.47)   65.55\n\n  DelvePO         83.67(1.20)   91.00(1.00)   76.25(1.49)   68.53(2.61)   76.00(2.08)   73.33(3.06)          32.05(0.25)   70.48\n\n              Table 2: The results on different downstream tasks for GPT-4o-mini.\n\n\n                               Classical NLP       Domain-specific   Multi-domain\n         Method                                                                Avg.\n                      Subj      CoLA      FinPB          AG’s News\n\n         Human      27.33           -                -                 87.56           57.45\n          CoT-ZS      67.67        81.40        73.67             80.33           75.77\n          CoT-FS      82.00        84.93        80.67             83.00           82.65\n\n        Promptbreeder   45.00        67.72        72.00             78.00           65.68\n         APE        79.61(1.78)   81.53(1.93)   94.93(0.78)          84.60(0.93)      85.17\n         EvoPrompt     76.70(1.90)   82.72(2.11)   96.97(0.52)          86.50(1.40)      85.72\n\n         DelvePO      91.07(1.03)   83.14(1.90)   98.63(0.62)          89.40(0.81)      90.56\n\nTo further evaluate the performance of our framework on different LLMs, we conducted additional\nexperiments across different task types on the closed-source model (GPT-4o-mini, results reported\nin Table 2) and the widely used open-source model (Qwen2.5-7B-Instruct, shown in Table 5 in\nAppendix D). The experimental settings were kept identical to the main experiments. As shown\nin the results evaluated on these two LLMs, our framework consistently delivers either superior\nor competitive performance across multiple task types, demonstrating its robustness and general\neffectiveness when applied to diverse LLMs.\n\n\n4.3  COST ANALYSIS\n\nIn our experiments, the overhead primarily stems from the training time required for open-source\nLLMs and the number of tokens consumed in API requests for closed-source LLMs. Accordingly,\nfor DeepSeek-R1-Distill-Llama-8B, we randomly selected one dataset from each task type and mea-\nsured the time cost of different baselines, with results presented in Figure 3. The statistics indicate\nthat our method consistently outperforms or matches the baselines in terms of optimization speed,\nparticularly when compared with PromptBreeder. This also explains why we report its results using\na single random seed. Overall, the results demonstrate that our method can more effectively exploit\nthe rapid convergence property of evolutionary algorithms for faster optimization.\n\nMoreover, we reported token usage in terms of the actual monetary expenditure, as shown in Table 6.\nOverall, as shown in Table 2 and Table 6, although our method requires higher expenditure, it con-\nsistently delivers performance above or competitive with the baselines, indicating that our approach\noffers a favorable balance between performance and cost. We also analyzed the reasons behind the\ngenerally higher token usage. The primary factor is that the content stored in the memory module\nis included as part of the input provided to the target LLMs. In future work, we plan to integrate\nprompt compression techniques into the framework to reduce this overhead.\n\n\n4.4  ABLATION STUDY\n\nTo evaluate the impact of the memory mechanism in our framework, we conducted ablation experi-\nments on GPT-4o-mini. We selected three datasets of different types to evaluate the adaptability of\nthe memory mechanisms across multiple scenarios. Table 3 reports the performance on three types\nof datasets using a single random seed. When both memory mechanisms are included and oper-\n\n\n                                       7\n\nPreprint\n\n\n\n\n\n                         DelvePO         EvoPrompt       APE         PromptBreeder\n\n                              3.0                                                                               2.89                                    2.87\n                                                                                            2.71  2.77  2.72                                               2.73\n                              2.5                            2.48                                    hours)                                                                                    2.31                                         2.25          2.27             2.38\n\n                                                                                                                      1.97                        (GPU  2.0                                    1.88       1.87                                         2.10\n                              Epoch               1.60  1.69  1.76                1.73                                                          1.75\n                  per  1.5\n                        Time\n                              1.0\n                                                Evolving  0.5\n\n\n                              0.0\n                      MR           CoLA          TREC            FinPB        SAMSum\n\nFigure 3: Average time-consuming (GPU hours) for one epoch of optimization on DeepSeek-R1-\nDistill-Llama-8B.\n\n                         Table 3: Ablations of Memory Mechanism.\n\n\n            Memory Modules      SAMSum  SQuAD   Causal Judgement\n\n              w/o Component Memory      28.8        67.4            62.6\n              w/o Prompt Memory          29.4        67.9            61.8\n              w/o both                     28.4        64.6            61.3\n             DelvePO                    35.3        84.7            65.7\n\n\nate in coordination, the overall performance is substantially higher than in the other configurations,\ndemonstrating the effectiveness and complementary benefits of the proposed memory design.\n\nFurthermore, to investigate the impact of the\nnumber of component values for each compo-  Table 4: Sensitivity test regarding the number of\nnent type on the overall performance of initial  component values.\npopulation, we designed a sensitivity test exam-\nining how initial population performance varies                                           # Value  SAMSum  SQuAD  SST-5\nwith the number of component values at ini-\ntialization. Using GPT-4o-mini, we generated     50          29.2       67.9     57.2\ninitial populations for three different types of     40          29.2       67.3     57.4\ndatasets under a single random seed and evalu-     30          29.7       66.8     56.8\nated their performance. The results in Table 4     20          28.8       66.5     59.1\nshow that increasing the number of component     10          30.2       69.7     60.3\nvalues does not cause significant fluctuations in\nthe initial population performance.  This indicates that a relatively small number of component\nvalues is sufficient to obtain an initial population with stable and reasonable performance, and im-\nportantly, it rules out the concern that a larger number of components could lead to an overestimated\ninitial population, which might otherwise suggest that further optimization is unnecessary.\n\nMoreover, we conducted a case study to help researchers quickly understand the operational mech-\nanisms of our proposed framework. The details are presented in Appendix I.\n\n\n5  RELATED WORK\n\nPrompt Engineering Prompt engineering is a resource-efficient approach, focusing on elaborately\ndesigning expert-level prompts to steer LLMs generate desired solutions to various downstream\ntasks. In this part, we mainly focus on those works which use prompts to stimulate the internal abil-\nities of LLMs. Least-to-Most (Zhou et al., 2023a), Decomposed Prompting (Khot et al., 2023) and\nPS&PS+ (Wang et al., 2023a) use prompts to leverage the decomposition ability of LLMs, breaking\n\n\n                                       8\n\nPreprint\n\n\n\n\n\ndown complex problems into simpler ones, enabling the model to perform better when dealing with\ncomplex problems. CoT (Wei et al., 2023), PoT (Chen et al., 2023), PS & PS+ (Wang et al., 2023a),\nAutomate-CoT (Shum et al., 2023), ToT (Yao et al., 2023)and GoT (Besta et al., 2024) guide the\nmodel to utilize chain-of-thought in different ways through the design of prompts, stimulating the\nthinking ability of the model, thereby enhancing the model’s reasoning ability. Also, Complexity-\nbased Prompting (Fu et al., 2023) and DIV-SE (Naik et al., 2024) focus on the complexity and diver-\nsity of prompts, aiming to help the model think better. Rephrase and Respond (Deng et al., 2024),\nOPRO (Yang et al., 2024), and MIPRO (Opsahl-Ong et al., 2024) utilized the self-optimization\ncapabilities of LLMs through methods such as input rewriting, iterative prompt optimization and\nstructured program optimization, jointly demonstrating that LLMs can autonomously enhance the\nperformance of task execution by dynamically improving prompts. TextGrad (Yuksekgonul et al.,\n2025) and SPO (Xiang et al., 2025) combine LLMs by orchestrating Standard Operation Pipelines\n(SOPs) in advance, and uses the evaluation ability of the model itself to guide the optimization of\nprompts. These methods effectively demonstrate that LLMs can be more proactive in utilizing their\nexploration abilities under the scientific guidance of predefined SOPs. Although the above works\nhave elicited some abilities of LLMs to cope with complex problems, they cannot get rid of the\nproblem that LLMs are sensitive to inputs, which results in the inconsistency of outputs’ quality.\n\nPrompt Optimization Given a downstream task, prompt optimization aims to improve the effec-\ntiveness of prompt, which typically involves an iterative process including initialization, execution,\nevaluation and selection. This part primarily focus on those works which leverage external tech-\nnologies or exogenous intelligence sources to guide LLMs to perform prompt optimization. Using\nexternal knowledge to optimize prompt is very effective. Existing works generally referred to: 1)\nthe way humans think (Wang et al., 2023c); 2) the idea of program synthesis (Zhou et al., 2023b);\n3) external knowledge (Zhao et al., 2023) to optimize prompts which achieve good results. Format-\nting the structure of prompts can standardize the thinking process of LLMs, and to a certain extent\nimprove their reasoning capability. LangGPT (Wang et al., 2024) presents a framework for prompt\ndesign, proving that scalable structures are important for prompts migration. Prompt template (He\net al., 2024) delves into the impact of the format of the prompt template on solving problems, demon-\nstrating the effectiveness of structured prompts in eliciting LLMs’ capabilities. Furthermore, there\nare some efforts that introduce algorithms that have been widely proven to have good optimization\ncapabilities to the optimization of prompts, including K-means (Zhang et al., 2022), KNN (Shi et al.,\n2022), reinforcement learning (Pryzant et al., 2023; Wang et al., 2023b), active learning (Diao et al.,\n2024), and evolutionary algorithm (Guo et al., 2025; Fernando et al., 2024).\n\nIn summary, although existing studies have mitigated the output stochasticity of LLMs, the effi-\nciency of the optimization algorithm has still not been fully explored. These efforts generally tend\nto treat prompts as a whole unit to optimize, so the potential optimization space is very large. In\naddition, most previous researches combining optimizing algorithms (e.g., evolutionary algorithms)\nwith LLMs, do not take full advantage of the experience generated before and after optimization,\nso that the optimization process is more stochastic, which tends to fall into local optima. Inspired\nby biological Loci and Alleles, this paper proposes a flexible framework for prompt optimization,\nwhich can effectively reduce the randomness of the optimization process and significantly improves\nthe optimization speed. We hope our approach will provide possible improvements for subsequent\nPO methods, significantly lowering the learning barrier for non-AI experts to leverage LLMs.\n\n6  CONCLUSION\n\nWe introduced DelvePO, a self-evolving framework for prompt optimization that decouples prompts\ninto distinct components. With components, prompts can be modified by adding or removing con-\ntent that may affect their performance, striking a good balance between exploration and exploitation\nof factors that affect task performance. DelvePO employs a co-evolutionary mechanism to iter-\natively refine the specifics of two sub-tasks and generate corresponding solutions. The evolved\nprompt, following systematic processing, is encoded into working memory to facilitate LLMs in de-\nriving relevant insights, thereby provides directional guidance for generating task-specific prompts.\nExtensive experiments on different tasks demonstrate DelvePO consistently outperforms baselines,\nvalidating its effectiveness. As we anticipate the emergence of even more powerful LLMs that can\ndeal with longer context, we firmly believe that more professional prompts will penetrate all walks\nof life, and DelvePO will help more users complete various complex tasks.\n\n\n                                       9\n\nPreprint\n\n\n\n\nETHICS STATEMENT\n\nThis work studies prompt optimization techniques for language models (LLMs) to better elicit their\ncapabilities in solving target tasks. The primary potential risks of this research are related to the\nmisuse of LLMs, for example, generating misleading, harmful, or biased content.\n\nIn our experiments, we only use publicly available datasets and pre-trained LLMs, and no private\nor sensitive data were involved.  Specific statements on LLM usage can be found in Appendix\nA. We emphasize that our methods are intended for research and benchmarking purposes, and we\nencourage responsible use to mitigate potential societal risks.\n\nREPRODICIBILITY STATEMENT\n\nWe are committed to ensuring the reproducibility of our work. To facilitate replication, we provide\nthe following details:\n\nComputational Resources The following describes the experimental environment, including de-\ntailed information on both hardware and software configurations.\n\n• Hardware. All experiments were conducted on a computing node equipped with four NVIDIA\n  Tesla V100-SXM2 GPUs (32GB memory each), an Intel Xeon Gold 6248 CPU @ 2.50GHz with\n  20 cores, and 226 GB of RAM.\n• Software. The system runs Ubuntu 20.04.6 LTS with Linux kernel version 5.4.0. All models were\n  implemented in Python 3.10.18 using PyTorch 2.0.0 with CUDA 11.7.\n\nHyperparameter Details In order to isolate the effect of our proposed method and ensure a fair\ncomparison, we mainly followed the default configurations used in baseline methods and intention-\nally introduced no additional trainable parameters. Specifically, the detailed hyperparameter settings\nare given below.\n\n• Initial Population Size. Following the setup of EvoPrompt, which uses both human-written and\n  LLM-generated prompts, we adopted a similar strategy in spirit but tailored it to our fully auto-\n  mated framework. (1) We identify a fixed set of components through preliminary study mentioned\n  at ref . (2) For each component, we use an LLM to generate 10 candidate values based on prompt\n  templates. (3) We then randomly combine these values to create 10 initial prompts, which together\n  form the initial population for the evolutionary process.\n• Temperature. Since the stochasticity of LLM outputs is sensitive to temperature settings, we set\n  the temperature to 0.5 to strike a balance between exploration and exploitation. This choice aligns\n  with prior work such as EvoPrompt.\n• Sample Allocation. For data splits, we followed the protocols of APE and EvoPrompt. Specif-\n  ically, if the dataset has a predefined training/testing split, we used it as-is. For datasets without\n  predefined splits, we randomly selected 100 examples as the test set and used the remaining ex-\n  amples for training.\n• Randomness Control. To ensure reproducibility. Unless otherwise noted, we use 3 random seeds\n  (5, 10 and 15) in the training phrase, and reported the results on the test set.\n\nLIMITATIONS\n\nWhile our framework can adaptively design well-matched prompts for any LLM across diverse\ndownstream tasks, several limitations remain. (1) Due to substantial computational costs, we cannot\ncomprehensively evaluate all models and domains. Instead, we focused on widely used datasets to\nbalance fairness and coverage. (2) Although we report monetary cost based on actual token usage,\nvariations in token pricing across input and output types cannot be precisely captured by the API.\nAnalysis indicates that most of the cost arises from including memory content as input tokens, while\noutput token consumption remains relatively modest, particularly when \"thinking mode\" is disabled.\nFuture work will explore prompt compression to further optimize resource use. (3) We evaluated\nonly representative component values from each category due to resource constraints. Nevertheless,\neven with this limited set, our approach continues to outperforms or remains competitive with base-\nlines, demonstrating its effectiveness and suggesting that its benefits will likely increase as LLMs\nsupport longer contexts.\n\n\n                                       10\n\nPreprint\n\n\n\n\nREFERENCES\n\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gi-\n   aninazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten\n   Hoefler.  Graph of thoughts: Solving elaborate problems with large language models.  Pro-\n  ceedings of the AAAI Conference on Artificial Intelligence, 38(16):17682–17690, March 2024.\n  ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL http://dx.doi.org/10.1609/\n  aaai.v38i16.29720.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\n  wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\n  Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n   Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\n  Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\n   Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the\n  34th International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook,\n  NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\n  Disentangling computation from reasoning for numerical reasoning tasks, 2023. URL https:\n  //arxiv.org/abs/2211.12588.\n\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,\n  2025. URL https://arxiv.org/abs/2501.12948.\n\nDeepSeek Chat. Deepseek chat web interface, 2025. URL https://chat.deepseek.com/.\n  Accessed: 2025-08.\n\nYihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large\n  language models ask better questions for themselves, 2024. URL https://arxiv.org/\n  abs/2311.04205.\n\nShizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, and Tong Zhang. Active prompting\n  with chain-of-thought for large language models, 2024. URL https://arxiv.org/abs/\n  2302.12246.\n\nLongyu Feng, Mengze Hong, and Chen Jason Zhang. Auto-demo prompting: Leveraging generated\n  outputs as demonstrations for enhanced batch prompting. arXiv preprint arXiv:2410.01724, 2024.\n\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel.\n  Promptbreeder: self-referential self-improvement via prompt evolution.  In Proceedings of the\n  41st International Conference on Machine Learning, ICML’24. JMLR.org, 2024.\n\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting\n   for multi-step reasoning, 2023. URL https://arxiv.org/abs/2210.00720.\n\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and\n  Yujiu Yang. Evoprompt: Connecting llms with evolutionary algorithms yields powerful prompt\n  optimizers, 2025. URL https://arxiv.org/abs/2309.08532.\n\nJia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, and Sadid Hasan.\n  Does prompt formatting have any impact on llm performance? arXiv preprint arXiv:2411.10541,\n  2024.\n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish\n  Sabharwal. Decomposed prompting: A modular approach for solving complex tasks, 2023. URL\n  https://arxiv.org/abs/2210.02406.\n\nTakeshi Kojima,  Shixiang  (Shane) Gu,  Machel  Reid,  Yutaka Matsuo,  and Yusuke  Iwa-\n  sawa.     Large  language  models  are  zero-shot  reasoners.     In  S.  Koyejo,  S. Mo-\n  hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh  (eds.), Advances  in Neural  In-\n  formation  Processing  Systems,  volume  35,  pp. 22199–22213.  Curran  Associates,  Inc.,\n  2022.  URL https://proceedings.neurips.cc/paper_files/paper/2022/\n  file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf.\n\n\n                                       11\n\nPreprint\n\n\n\n\n\nXiaoxi Li, Guanting Dong,  Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang,\n  and Zhicheng Dou.  Search-o1:  Agentic search-enhanced large reasoning models.  CoRR,\n  abs/2501.05366, 2025.  doi: 10.48550/ARXIV.2501.05366. URL https://doi.org/10.\n  48550/arXiv.2501.05366.\n\nRanjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, and Besmira Nushi. Di-\n   versity of thought improves reasoning abilities of llms, 2024. URL https://arxiv.org/\n  abs/2310.07088.\n\nKrista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Za-\n   haria, and Omar Khattab.  Optimizing instructions and demonstrations for multi-stage lan-\n  guage model programs.  In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Pro-\n  ceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp.\n  9340–9366, Miami, Florida, USA, November 2024. Association for Computational Linguis-\n   tics. doi: 10.18653/v1/2024.emnlp-main.525. URL https://aclanthology.org/2024.\n  emnlp-main.525/.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\n  optimization with “gradient descent” and beam search. In Houda Bouamor, Juan Pino, and Ka-\n   lika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\n  Processing, pp. 7957–7968, Singapore, December 2023. Association for Computational Linguis-\n   tics. doi: 10.18653/v1/2023.emnlp-main.494. URL https://aclanthology.org/2023.\n  emnlp-main.494/.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-\n   toine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen\n  Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chh-\n   ablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo\n  Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala\n  Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\n  Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask\n  prompted training enables zero-shot task generalization. In International Conference on Learning\n  Representations (ICLR), 2022. URL https://arxiv.org/abs/2110.08207.\n\nJie-Jing Shao, Xiao-Wen Yang, Bo-Wen Zhang, Baizhi Chen, Wen-Da Wei, Guohao Cai, Zhenhua\n  Dong, Lan-Zhe Guo, and Yu feng Li. Chinatravel: A real-world benchmark for language agents\n   in chinese travel planning, 2024. URL https://arxiv.org/abs/2412.13682.\n\nWeijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer.  Nearest neighbor zero-\n  shot inference.  In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of\n  the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3254–3265,\n  Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguis-\n   tics. doi: 10.18653/v1/2022.emnlp-main.214. URL https://aclanthology.org/2022.\n  emnlp-main.214/.\n\nKashun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with\n  chain-of-thought from labeled data. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Find-\n  ings of the Association for Computational Linguistics: EMNLP 2023, pp. 12113–12139, Sin-\n  gapore, December 2023. Association for Computational Linguistics.  doi: 10.18653/v1/2023.\n  findings-emnlp.811.  URL https://aclanthology.org/2023.findings-emnlp.\n  811/.\n\nXingchen Wan, Ruoxi Sun, Hootan Nakhost, and Sercan Ö. Arı k.   Teach better or show\n  smarter? on instructions and exemplars in automatic prompt optimization.  In A. Globerson,\n  L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in\n  Neural Information Processing Systems, volume 37, pp. 58174–58244. Curran Associates, Inc.,\n  2024.  URL https://proceedings.neurips.cc/paper_files/paper/2024/\n  file/6b031defd145b02bed031093d8797bb3-Paper-Conference.pdf.\n\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng\n  Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large lan-\n  guage models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings\n\n\n                                       12\n\nPreprint\n\n\n\n\n\n   of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\n  Papers), pp. 2609–2634, Toronto, Canada, July 2023a. Association for Computational Linguis-\n   tics.  doi: 10.18653/v1/2023.acl-long.147. URL https://aclanthology.org/2023.\n  acl-long.147/.\n\nMing Wang, Yuanzhong Liu, Xiaoyu Liang, Songlian Li, Yijie Huang, Xiaoming Zhang, Sijia Shen,\n  Chaofeng Guan, Daling Wang, Shi Feng, et al. Langgpt: Rethinking structured reusable prompt\n  design framework for llms from the programming language. arXiv preprint arXiv:2402.16929,\n  2024.\n\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P.\n  Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-\n   level prompt optimization, 2023b. URL https://arxiv.org/abs/2310.16427.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\n   ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\n  2023c. URL https://arxiv.org/abs/2203.11171.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\n  Le, and Denny Zhou.  Chain-of-thought prompting elicits reasoning in large language models,\n  2023. URL https://arxiv.org/abs/2201.11903.\n\nJinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong,\n  Chenglin Wu, and Yuyu Luo.  Self-supervised prompt optimization, 2025. URL https://\n  arxiv.org/abs/2502.06855.\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun\n  Chen. Large language models as optimizers, 2024. URL https://arxiv.org/abs/2309.\n  03409.\n\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\n  Narasimhan.  Tree of thoughts: Deliberate problem solving with large language models.  In\n  A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in\n  Neural Information Processing Systems, volume 36, pp. 11809–11822. Curran Associates, Inc.,\n  2023.  URL https://proceedings.neurips.cc/paper_files/paper/2023/\n  file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf.\n\nMert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin,\n  and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature,\n  639:609–616, 2025.\n\nWenxuan Zhang, Yue Deng, Bing Liu, Sinno Pan, and Lidong Bing.  Sentiment analysis in\n  the era of large language models: A reality check.   In Kevin Duh, Helena Gomez, and\n  Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024,\n  pp. 3881–3906, Mexico City, Mexico, June 2024. Association for Computational Linguistics.\n   doi: 10.18653/v1/2024.findings-naacl.246. URL https://aclanthology.org/2024.\n  findings-naacl.246/.\n\nYue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, and Wei Bi. Multi-task instruction\n  tuning of llama for specific scenarios: A preliminary study on writing assistance, 2023. URL\n  https://arxiv.org/abs/2305.13225.\n\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in\n   large language models, 2022. URL https://arxiv.org/abs/2210.03493.\n\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing.  Verify-and-edit:\n A knowledge-enhanced chain-of-thought framework.  In Anna Rogers, Jordan Boyd-Graber,\n  and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for\n  Computational Linguistics (Volume 1: Long Papers), pp. 5823–5840, Toronto, Canada, July\n  2023. Association for Computational Linguistics.  doi: 10.18653/v1/2023.acl-long.320. URL\n  https://aclanthology.org/2023.acl-long.320/.\n\n\n                                       13\n\nPreprint\n\n\n\n\n\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei\n  Liu.  Deepresearcher: Scaling deep research via reinforcement learning in real-world environ-\n  ments, 2025. URL https://arxiv.org/abs/2504.03160.\n\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-\n  mans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables com-\n  plex reasoning in large language models, 2023a. URL https://arxiv.org/abs/2205.\n  10625.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\n  Jimmy Ba. Large language models are human-level prompt engineers, 2023b. URL https:\n  //arxiv.org/abs/2211.01910.\n\n\n\n\n\n                                       14\n\nPreprint\n\n\n\n\nA  USE OF LLMS\n\nLarge Language Models (LLMs) were used in two ways in this work. First, LLMs served as base\nmodels in our experiments on prompt optimization, where we studied how different prompts can\nelicit their capabilities to solve target tasks. Second, LLMs were employed as auxiliary tools for\nminor writing support, such as grammar checking and phrasing improvements.  Specific details\nabout the LLMs used in our experiments can be found in Appendix B. No LLMs were used to\ngenerate substantive ideas, analyses, or content of the paper.\n\n\nB  DETAILS OF DATASETS AND LLMS USED\n\nDatasets For fair comparison, we followed the datasets and evaluation metrics used in prior base-\nlines whenever possible.  Specifically, we include 4 classic NLP benchmarks (MR, Subj, CoLA,\nSST-5) and two widely used question-answering datasets (SQuAD, TREC) to validate basic capa-\nbilities; several domain-specific benchmarks to probe specialized performance, including Financial\nSentiment Evaluation dataset (FinFE), Financial PhraseBank (FinPB), reasoning related dataset\n(Casual Judgement). Besides, one multi-domain datasets (AG’s News) and one natural language\ngeneration dataset (SAMSum) are also used to assess overall robustness. To evaluate output quality\nbeyond simple accuracy, we report ROUGE-Avg on SAMSum and the Matthews correlation coef-\nficient (MCC) on CoLA. To balance computational cost while maximizing coverage, we selected\ndatasets according to a “maximize capability diversity” principle — for example, in addition to the\nmain experiments we ran Qwen2.5-7B-Instruct on Subj, AG’s News, and FinFE to cover several of\nthe categories above. Detailed results are presented in the experimental analysis section.\n\nLLMs To demonstrate the adaptability of the proposed method for LLMs, we selected DeepSeek-\nR1-Distill-Llama-8B and Qwen2.5-7B-Instruct from open-source LLMs, as well as GPT-4o-mini\nfrom closed-source LLMs, as the base models for our experiments. The experiments on DeepSeek-\nR1-Distill-Llama-8B evaluate both the performance of the DeepSeek model itself and, to some ex-\ntent, the capabilities of the underlying Llama architecture, which is primarily trained on English-\nlanguage data.  Experiments on Qwen2.5-7B-Instruct assess the framework’s performance on a\nmodel predominantly trained on Chinese-language data, demonstrating applicability to non-English\ncorpora. GPT-4o-mini was included because it is a widely used closed-source model in prior studies\nand allows cost-effective experimentation within our budget.\n\n\nC  ALGORITHM DETAILS\n\n\nAlgorithm 1 An Overview of DelvePO\nRequire: A population of prompts P, size of population N, task-related dataset D, number of\n    epochs m, number of iterations n, working memory M = {Mcomponents, Mprompts}\nEnsure: Best prompt p∗\n 1: Initialization: P = {p1, p2, · · · , pN}, Mprompts ←fsort(P), Mcomponents ←∅\n 2: for epoch = 1 to m do\n 3:     Pevo ←∅\n 4:     for step = 1 to n do\n 5:         Selection: p ←fr.w.s.(P)\n 6:        Task-Evolution: Tevo ←ϕT (p, Mcomponents | T )\n 7:        Solution-Evolution: Sevo ←ϕS(p, Mprompts | Tevo)\n 8:        Evaluation: p′ ←ϕLLM(Sevo), s′ ←feval(p′, D)\n 9:       Memory-Evolution: Mevo ←ϕM M, ⟨p, p′, s ≥s′⟩\n10:        Pevo ←{Pevo, p′}\n11:    end for\n12:    Update: P ←Top-N {P, Pevo}\n13: end for\n14: Return the best prompt p∗: p∗←arg max feval ϕLLM(p, D)\n                                    p∈P\n\n\n\n                                       15\n\nPreprint\n\n\n\n\nThe sampling function used in our framework is roulette wheel selection, denoted as fr.w.s.(·),\nwhich is commonly used in the evolution algorithm. ϕT , ϕS, ϕM refer to the Task-Evolution,\nSolution-Evolution, Memory-Evolution methods, respectively. Similarly, T , S, and M mean the\ncorresponding Task, Solution, Memory. Based on the components, we designed a task-agnostic\ntemplate described in Figure 4, through which any kind of LLMs can construct an initial content set\nof components based on a simple description of the target task input by the user.\n\n\n    ================== Task-agnostic Template for Component ==================\n\n    Hi there, I have a task to do which can be described as Downstream Task Related Information. Now I\n   want you to give me                                                                          . [OPTIONAL Example]. Please list your\n    answers in the following format: ['content 1', 'content 2',...]\n\n     ---------------------------- Downstream Task (Causal judgement) for role ----------------------------\n\n   <Query>: Hi there, I have a task to do which can be described as \"answer questions about causal\n     attribution\". Now I want you to give me    related roles who are expertise in these questions. For\n    example, 'Casusal Analysis Experts', etc. Please list your answers in the following format: [\"content 1\",\n    \"content 2\",...]\n\n   <Response>: [\"Cognitive Scientist\", \"Social Psychologist\", \"Computational Linguist\", \"AI Ethicist\",\n    \"Behavioral Economist\", \"Decision Theorist\", \"Philosophy of Mind Researcher\", \"Causal Inference\n    Data Scientist\", \"Educational Psychologist\", \"Human-Computer Interaction Specialist\"]\n\n\nFigure 4: Task-agnostic template for generating component values corresponding to the given com-\nponent types. The following part of the figure is the prompt to generate content for Component\n\"role\" using the casual judgement task as an example.\n\n\nD  ADDITIONAL EXPERIMENTS\n\n\n          Table 5: The results on different downstream tasks for Qwen2.5-7B-Instruct.\n\n\n                            Classical NLP             Question-Answering   Domain-specific   Multi-domain\n  Method                                                                                                  Avg.\n                Subj       SST-5     CoLA        TREC            FinFE        AG’s News\n\n APE          69.00(3.06)   47.00(1.10)   79.05(1.73)         43.40(1.14)            64.30(2.70)        83.43(1.90)     64.38\n  EvoPrompt   77.03(4.74)   57.67(1.19)   79.69(1.42)         67.55(2.08)            64.67(1.22)        85.73(1.29)     72.06\n\n  DelvePO     80.07(0.65)   60.00(1.69)   81.40(1.07)         70.77(1.74)            69.97(0.87)        89.27(0.97)     75.25\n\n\n      Table 6: Average monetary cost (USD) for one epoch of optimization on GPT-4o-mini.\n\n                Methods       Subj  CoLA  FinPB  AG’s News\n\n                   Promptbreeder   1.17    1.31     0.97       1.52\n              APE            0.57    0.56     0.61       0.79\n                 EvoPrompt      0.83    0.64     0.74       1.23\n                DelvePO        1.27    1.08     1.30       1.10\n\n\n\n\n\n                                       16\n\nPreprint\n\n\n\n\nE  DETAILED INFORMATION ABOUT COMPONENTS\n\nTo ensure that the types of components are as comprehensive and representative as possible, we\nfirst surveyed a broad set of related literature (Yuksekgonul et al., 2025; He et al., 2024; Feng et al.,\n2024; Opsahl-Ong et al., 2024; Diao et al., 2024; Wang et al., 2024; 2023b) and extracted a variety of\nfactors that have been shown to influence the performance of prompts, forming our component pool.\nWe then categorized all components in the pool based on the semantics implied in their original\nsources, which resulted in five categories: “Role and Expertise”, “Task Content”, “Constraints and\nNorms”, “Process and Behavior” and “Context and Examples”. From each category, we selected the\nmost representative component as our predefined component types. The complete component pool\nand its categorization are provided in Table 7.\n\nDespite this extensive literature review, we acknowledge that some important aspects may remain\nuncovered. This observation motivated our design: as more non-AI experts begin to use LLMs,\ndomain specialists should be able to adaptively define new components through our mechanism,\nthereby supporting both effective task performance and improved interpretability. It is worth noting\nthat for each component type, we can add a “null” option when generating its values, allowing\nthe presence or absence of the component to be controlled and makes the optimized prompts more\nflexible.\n\n            Table 7: The categories and types of components in the component pool\n\n\n     Categories             Related Items\n\n     Role and Expertise       Role; Role description; Scenario; Domain knowledge; Term Clarification\n     Task Content           Task description; Instruction; Goal\n     Constraints and Norms   Output format; Constraints; Principle; Style; Length; Tone; Priority &\n                           Emphasis; Exception handling; Target audience\n     Process and Behavior    Workflow; CoT; Action; Skill; Suggestions; Initialization\n     Context and Examples   Examples; Reference prompt; Attachment\n\n\n\nF  TEMPLATE FOR INJECTION & PROMPTS FOR EVALUATION ON LLMS\n\n\n\n    Template_For_Injection         =\n      <component1>{content1}</component1>. Given the             ,    <component2>{content2}\n   </component2>\n\n\n    Template_For_Injection        =\n   You are a <role>{role}</role>. Given the             , your task is to <task_description>{task_description}\n    </task_description>.\n\n\n    Template_For_Injection          =\n   You are a <role>{role}</role>. Given the                                      , your task is to <task_description>\n    {task_description}</task_description>.\n\n\nFigure 5: Template for initializing prompt populations. It is also used in the construction of Prompts\nMemory, that is, injecting discrete components into the template to obtain a continuous form prompt.\nThe above shows the general form, while the two below provide illustrative examples.\n\n\n\n\n\n                                       17\n\nPreprint\n\n\n\n\n\n   Prompt_For_LLM         =\n   <INSTRUCTION>:    {content1}. Given the             ,    {content2}\n   <      >: {input}\n   <OUTPUT FORMAT>: Output the final result starting with the tag <res> and ending with the tag\n    </res>. [OPTIONAL REQUIREMENTS]\n\n\n   Prompt_For_LLM        =\n   <INSTRUCTION>: You are a {role}. Given the             , your task is to {task_description}.\n   <      >: {input}\n   <OUTPUT FORMAT>: Output the final result starting with the tag <res> and ending with the tag\n    </res>. The final result must come from the following: [World, Sports, Business, Tech].\n\n\n   Prompt_For_LLM         =\n   <INSTRUCTION>: You are a {role}. Given the                                      , your task is to {task_description}.\n   <                  >: {input}\n   <OUTPUT FORMAT>: Output the final result starting with the tag <res> and ending with the tag\n    </res>.\n\n\n\nFigure 6: Complete prompt template for LLMs (including three parts: instruction, input, and output).\nHere we also display two practical prompts for AG’s News and Simplification Tasks.\n\n\n\n\n\n                                       18\n\nPreprint\n\n\n\n\nG  THE DETAILED PROMPTS OF TASK-EVOLUTION\n\n\n   Please follow the instructions step-by-step to get final result.\n\n    Step 1 Conclude Insights from the provided Memory Components , which consists of multiple\n   elements. Each element contains two lists: the first contains several markup pairs in the format\n   <component>content</component>. For example, in the pair <role>role_description</role>, the\n   content (\"role_description\") describes the component (\"role\"). All markup pairs follow this structure.\n   By default, the first list in each element is considered to perform better than the second.\n   Memory Components : {                 }\n\n    Step 2 Based on the Insights from Step 1 and the Current Prompt , select one or more component(s)\n   from Component Set that could potentially improve performance to form final result. Separate the\n    final result with a special token '|' and ensure that each of final result is unique and appears only\n   once. The final result must start with the tag <res> and end with the tag </res> . For example, the\n    final result must follow the format: <res>component1|...</res>.\n    Current Prompt : {    }\n   Component Set : {components}\n\n\n                             Figure 7: The prompts for sub-task I\n\n\n   Please follow the instructions step-by-step to get final result.\n\n    Step 1 Conclude Insights from the provided Memory Components , which consists of multiple\n   elements. Each element contains two lists: the first contains several markup pairs in the format\n   <component>content</component>. For example, in the pair <role>role_description</role>, the\n   content (\"role_description\") describes the component(\"role\"). All markup pairs follow this structure.\n   By default, the first list in each element is considered to perform better than the second.\n   Memory Components : {                 }\n\n    Step 2 Given a list named Old Values , where each element contains a pair of contents, use the\n   Insights from Step 1 to select one content from each pair in original order. The final result must\n    start with the tag <res> and end with the tag </res> . For example, the final results must follow the\n   format: <res>content1|...</res>.\n    Old Values : {old_values}\n\n\n                            Figure 8: The prompts for sub-task II\n\n\n\n\n\n                                       19\n\nPreprint\n\n\n\n\nH  THE DETAILED PROMPTS OF SOLUTION-EVOLUTION\n\n\n   Please follow the instructions step-by-step to get final result.\n\n    Step 1 Conclude the Insights from the Memory Prompts , which consists of multiple items. Each item\n    includes two parts: the first part contains several markup pairs in the format\n   <component>content</component>. For example, in the pair <role>role_description</role>, the\n   content (\"role_description\") describes the component (\"role\"). Other markup pairs follow this same\n    structure. The second part of each item represents its corresponding performance. The entire Memory\n   Prompts is sorted in descending order based on performance.\n   Memory Prompts : {             }\n\n    Step 2 Given a list named Old Values , use the Insights from Step 1 to generate a new mutated\n   content for each content to form a new list, i.e. final result, referring to Description, adhering to Rules\n   below.\n\n      Description:\n\n           In Old Values , each element is a markup pair like <component>content</component>\n         containing content that needs to mutate.\n\n      Rules:\n\n         1. Mutation Requirements:\n\n             For each element like <component>content</component>, generate a new one content\n               that:\n\n                           If the component is <role>, the new content must be a noun phrase describing a\n                 person.\n\n                           If the component is <task_description>, the new content must be a verb phrase\n                 describing a task.\n\n                       Is distinct from the original content.\n\n                 Preserves lexical identity (noun/verb phrase) matching the component.\n\n                           If the original content had the highest score, the new content must prioritize\n              improved performance potential (e.g., higher efficiency, enhanced properties).\n\n                 Otherwise, the new content may be derived from those contents linked to its\n                corresponding component in the Memory Prompts (optional but allowed).\n\n         2. Output Format:\n\n               Start with <res> and end with </res>.\n\n             Separate mutated contents strictly with '|' (no extra characters).\n\n            Never include original contents in the output.\n\n    Old Values : {old_values}\n\n\n          Figure 9: The prompts for Sub-solution I - Prompts Memory in discrete form\n\n\n\n\n\n                                       20\n\nPreprint\n\n\n\n\n\n   Please follow the instructions step-by-step to get final result.\n\n    Step 1 Conclude the Insights from the Memory Prompts , which contains multiple items. Each item\n   has two parts: a sentence enclosed in <prompt> and </prompt>, and its corresponding performance\n   score. The sentence includes markup pairs in the format <component>content</component>, where\n   the content describes the component. For example, <role>role_description</role> indicates that\n   \"role_description\" explains the \"role\" component. All items are sorted in descending order by\n   performance.\n   Memory Prompts : {                }\n\n    Step 2 Based on the Current Prompt and Insights from Step 1, generate a new mutated content for\n   each markup pair whose component matches those listed in Mutate Factors to form the final result,\n   referring to Description, adhering to Rules below.\n\n      Description:\n\n          In Current Prompt , markup pair like <component>content</component> contains content\n          that needs to mutate.\n\n          In Mutate Factors , each element is a component appeared in Current Prompt .\n\n      Rules:\n\n         1. Mutation Requirements:\n\n             For each markup pair like <component>content</component>, if the component in\n             Mutate Factors , generate a new one content that:\n\n                          If the component is <role>, the new content must be a noun phrase describing a\n                 person.\n\n                          If the component is <task_description>, the new content must be a verb phrase\n                 describing a task.\n\n                      Is distinct from the original content.\n\n                 Preserves lexical identity (noun/verb phrase) matching the component.\n\n                          If the original content had the highest score, prioritize generating contents with\n              improved performance potential (e.g., higher efficiency, enhanced properties).\n\n                Otherwise, the new content may derive from those contents linked to its component\n                   in the Memory Prompts (optional but allowed).\n\n         2. Output Format:\n\n              Start with <prompt> and end with </prompt>.\n\n           Only mutate contents within markup pairs specified in Mutate Factors .\n\n             Preserve all other values outside markup pairs.\n\n            Replace original contents with mutated ones directly within their components.\n\n    Current Prompt : {prompt}\n   Mutate Factors : {mutate_factors}\n\n\n        Figure 10: The prompts for Sub-solution I - Prompts Memory in continuous form\n\n\n\n\n\n                                       21\n\nPreprint\n\n\n\n\n\n   Please follow the instructions step-by-step to get final result.\n\n    Step 1 Conclude the Insights from the Memory Prompts , which consists of multiple items. Each item\n    includes two parts: the first part contains several markup pairs in the format\n   <component>content</component>. For example, in the pair <role>role_description</role>, the\n   content (\"role_description\") describes the component (\"role\"). Other markup pairs follow this same\n    structure. The second part of each item represents its corresponding performance. The entire Memory\n   Prompts is sorted in descending order based on performance.\n   Memory Prompts : {             }\n\n    Step 2 Given a list named Old Values , where each element contains a pair of contents, use the\n   Insights from Step 1 to generate a new mutated content for each pair to form a new list, i.e. final\n    result, referring to Description, adhering to Rules below.\n    Old Values : {old_values}\n\n      Description:\n\n           In Old Values, each element contains a pair of contents like [a, b].\n\n      Rules:\n\n         1. Mutation Requirements:\n\n             For each pair of contents like [a, b], generate a new one content that:\n\n                           If a and b are enclosed with <role> & </role>, the new content must be a noun\n                phrase used to describe a person.\n\n                           If a and b are enclosed with <task_description> & </task_description>, the new\n                content must be a verb phrase used to describe a task.\n\n                       Is distinct from both a and b.\n\n                 Preserve corresponding lexical identity.\n\n                           If the original pair has the highest score, prioritize generating contents with\n              improved performance potential (e.g., higher efficiency, enhanced properties).\n\n                 Otherwise, derive the new content from those contents linked to its component in the\n             Memory Prompts (optional but allowed).\n\n         2. Output Format:\n\n               Start with <res> and end with </res>.\n\n             Separate mutated contents strictly with '|' (no extra characters).\n\n            Never include original pairs in the output.\n\n\n         Figure 11: The prompts for Sub-solution II - Prompts Memory in discrete form\n\n\n\n\n\n                                       22\n\nPreprint\n\n\n\n\n\n      Please follow the instructions step-by-step to get final result.\n\n       Step 1 Conclude the Insights from the Memory Prompts , which contains multiple items. Each item\n      has two parts: a sentence enclosed in <prompt> and </prompt>, and its corresponding performance\n       score. The sentence includes markup pairs in the format <component>content</component>, where\n      the content describes the component. For example, <role>role_description</role> indicates that\n       \"role_description\" explains the \"role\" component. All items are sorted in descending order by\n      performance.\n      Memory Prompts : {                }\n\n       Step 2 Based on the Prompt 1 and Insights from Step 1, generate a new mutated content for each\n     markup pair whose component matches those listed in Mutate Factors to form the Prompt 2 , referring\n      to Description, adhering to Rules below.\n\n         Description:\n\n              In Prompt 1 , markup pair like <component>content</component> contains content that\n           needs to mutate.\n\n              In Mutate Factors , each element is a content appeared in Prompt 1 .\n\n         Rules:\n\n            1. Mutation Requirements:\n\n                For each markup pair like <component>content</component>, if the component in\n                Mutate Factors , Generate a new one content that:\n\n                               If the component is <role>, the new content must be a noun phrase describing a\n                    person.\n\n                               If the component is <task_description>, the new content must be a verb phrase\n                    describing a task.\n\n                          Is distinct from the original content.\n\n                    Preserves lexical identity (noun/verb phrase) matching the component.\n\n                               If the original content had the highest score, prioritize generating contents with\n                 improved performance potential (e.g., higher efficiency, enhanced properties).\n\n                    Otherwise, the new content may derive from those contents linked to its component\n                       in the Memory Prompts (optional but allowed).\n\n            2. Output Format:\n\n                  Start with <prompt> and end with </prompt>.\n\n              Only mutate contents within markup pairs specified in Mutate Factors .\n\n                Preserve all other values outside markup pairs.\n\n               Replace original contents with mutated ones directly within their components.\n\n      Prompt 1 : {prompt1}\n       Mutate Factors : {mutate_factors}\n\n       Step 3 Based on the Prompt 3 and Insights from Step 1, generate a new mutated content for each\n     markup pair whose component matches those listed in Mutate Factors to form the Prompt 4 , referring\n      to Description, adhering to Rules below.\n\n         Description:\n\n              In Prompt 3 , markup pair like <component>content</component> contains content that\n           needs to mutate.\n\n\n        Figure 12: The prompts for Sub-solution II - Prompts Memory in continuous form\n\n\n\n\n                                       23\n\nPreprint\n\n\n\n\n\n              In Mutate Factors , each element is a content appeared in Prompt 3 .\n\n          Rules:\n\n             1. Mutation Requirements:\n\n                For each markup pair like <component>content</component>, if the component in\n                Mutate Factors , Generate a new one content that:\n\n                               If the component is <role>, the new content must be a noun phrase describing a\n                    person.\n\n                               If the component is <task_description>, the new content must be a verb phrase\n                    describing a task.\n\n                           Is distinct from the original content.\n\n                    Preserves lexical identity (noun/verb phrase) matching the component.\n\n                               If the original content had the highest score, prioritize generating contents with\n                 improved performance potential (e.g., higher efficiency, enhanced properties).\n\n                    Otherwise, the new content may derive from those contents linked to its component\n                       in the Memory Prompts (optional but allowed).\n\n             2. Output Format:\n\n                  Start with <prompt> and end with </prompt>.\n\n              Only mutate contents within markup pairs specified in Mutate Factors .\n\n                Preserve all other values outside markup pairs.\n\n               Replace original contents with mutated ones directly within their components.\n\n      Prompt 3 : {prompt3}\n       Mutate Factors : {mutate_factors}\n\n       Step 4 Generate final result by selecting contents from pairs in Prompt 2 and Prompt 4 under\n       identical markup components, referring to Description, adhering to Rules below.\n\n         Description:\n\n              Pairs from Prompt 2 and Prompt 4 have identical components (e.g., <role>,\n             <task_description>).\n\n          Rules:\n\n             1. Selection Criteria:\n\n                For each tagged pair (e.g., <role>a</role> and <role>b</role>):\n\n                 Use Insights from Step 1 to select one content (a or b) that has higher performance\n                improvement potential (e.g., clarity, specificity, alignment with goals).\n\n                               If the component is <role>, the new content must be a noun phrase describing a\n                    person.\n\n                               If the component is <task_description>, the new content must be a verb phrase\n                    describing a task.\n\n                    Preserve the lexical identity of the component.\n\n                  Never modify text outside markup pairs.\n\n             2. Output Format:\n\n                  Start with <prompt> and end with </prompt>.\n\n                Retain the structure of Prompt 3 but replace tagged pairs with the selected contents.\n\n                          If multiple tagged pairs exist, update all while maintaining non-tagged values verbatim.\n\nFigure 13: The prompts for Sub-solution II - Prompts Memory in continuous form (extended from\nFigure 12)\n\n\n\n\n                                       24\n\nPreprint\n\n\n\n\nI  CASE STUDY DETAILS\n\nTo quickly verify the generalizability of our framework, we conducted multi-turn dialogues with\nDeepSeek Chat via the web interface provided by DeepSeek (DeepSeek Chat, 2025).\n\nThroughout the process, we take simplification task (Zhang et al., 2023) as the example, which\nallows for easy observation and interpretation of the outputs, and randomly set 4 components. The\nwhole process can be find in Appendix I. For Task-Evolution, we provide two input information (see\nFigure 14, 16) for the prompt of two sub tasks (see Figure 7, 8). And the corresponding outputs are\nshown in Figure 15, 17. From the final results, we can derive that under the guidance of direction\n(i.e., Memory Components), The LLMs could find reasonable direction for evolutionary operator.\n\nAccordingly, for Solution-Evolution, we provide four input information (see Figure 18, 20, 22, 24)\nfor four kinds of scenarios. And the corresponding responses are shown in Figure 19, 21, 23, 25,\nrespectively. Based on the observation from the responses, we also find that the procedure designed\nin this paper can accurately guide the model to deduce corresponding insights and further output\nreasonable results based on the insights.\n\nBy the way, this case study provides new users with a quick understanding of our framework. It can\nalso serve as a practical guide, allowing anyone to construct a prototype using the system prompts\nprovided in the case study without writing any code, which can help users optimize domain-specific\nprompts. We sincerely invite researchers to try it out and share their valuable feedback for further\nimprovement.\n\n\n    Step 1 Memory Components:\n\n\n     [\n\n       [\n\n         [<action>Clarify Sentence Meaning</action>, <requirements>Adapt to\n\n     Audience</requirements>],\n\n         [<action>Reduce Linguistic Complexity</action>, <requirements>Improve\n\n     Structure</requirements>]\n\n       ],\n\n       [\n\n         [<role>Readability Specialist</role>],\n\n         [<role>Sentence Simplifier</role>]\n\n       ]\n\n     ]\n\n    Step 2 Current Prompt:\n\n\n     [\n\n         <role>Sentence Simplifier</role>,\n\n         <task_description>Rewrite the provided sentences in a simpler\n\n     form</task_description>,\n\n         <action>Condense Sentences</action>,\n\n         <requirements>Adapt to Audience</requirements>\n\n     ]\n\n   Component Set:\n\n\n     [role, task_description, action, requirements]\n\n\n\n                              Figure 14: The input for sub-task I\n\n\n\n\n\n                                       25\n\nPreprint\n\n\n\n\n\n   Step 1: Conclude Insights from the Memory Components\n\n      1. In the first tuple, <action>Clarify Sentence Meaning</action> is preferred over <action>Reduce\n         Linguistic Complexity</action> , and <requirements>Adapt to Audience</requirements> is preferred\n       over <requirements>Improve Structure</requirements> .\n\n      2. In the second tuple, <role>Readability Specialist</role> is preferred over <role>Sentence\n        Simplifier</role> .\n\n    Insights:\n\n       Role: \"Readability Specialist\" outperforms \"Sentence Simplifier.\"\n\n       Action: \"Clarify Sentence Meaning\" outperforms \"Reduce Linguistic Complexity\" (and by\n        extension, other actions like \"Condense Sentences\").\n\n       Requirements: \"Adapt to Audience\" is already the preferred choice.\n\n   Step 2: Select Components to improve performance based on Current Prompt and Insights\n\n      The Current Prompt uses <role>Sentence Simplifier</role> , but the Insights suggest\n        <role>Readability Specialist</role> is better.\n\n      The Current Prompt uses <action>Condense Sentences</action> , but the Insights suggest\n         <action>Clarify Sentence Meaning</action> is better.\n\n      The Current Prompt already uses the preferred <requirements>Adapt to Audience</requirements> ,\n       so no change is needed here.\n\n    final result: <res>role|action</res>\n\n                            Figure 15: The responses for sub-task I\n\n\n\n\n\n                                       26\n\nPreprint\n\n\n\n\n\n    Step 1 Memory Components:\n\n\n     [\n\n         [\n\n             [<action>Clarify Sentence Meaning</action>, <requirements>Adapt to\n\n     Audience</requirements>],                [<action>Reduce Linguistic\n\n     Complexity</action>, <requirements>Improve Structure</requirements>]\n\n         ],\n\n         [\n\n             [<role>Readability Specialist</role>],\n\n             [<role>Sentence Simplifier</role>]\n\n         ],\n\n         [\n\n             [<action>Optimize Readability</action>, <task_description>Reduce the\n\n     complexity of the provided sentences</task_description>],\n\n             [<action>Reword for Better Clarity</action>, <task_description>Reword the\n\n     provided sentences for better clarity</task_description>]\n\n         ]\n\n     ]\n\n    Step 2 Old Values:\n\n\n     [\n\n         [<role>Readability Specialist</role>, <role>Paraphrasing Specialist</role>],\n\n         [<action>Simplify Sentence Structure</action>, <action>Break Down\n\n     Complexity</action>]\n\n     ]\n\n\n                             Figure 16: The input for sub-task II\n\n\n\n\n\n                                       27\n\nPreprint\n\n\n\n\n\n   Step 1: Conclude Insights from Memory Components\n\n   From the Memory Components:\n\n      1. First Element:\n\n          The pair <action>Clarify Sentence Meaning</action> with <requirements>Adapt to\n           Audience</requirements> is preferred over <action>Reduce Linguistic Complexity</action> with\n            <requirements>Improve Structure</requirements> .\n\n            Insight: Prioritizing audience adaptation and meaning clarification over general structural\n          improvement and complexity reduction.\n\n      2. Second Element:\n\n             <role>Readability Specialist</role> is preferred over <role>Sentence Simplifier</role> .\n\n            Insight: \"Readability Specialist\" is considered a more effective or broader role than\n           \"Sentence Simplifier\".\n\n      3. Third Element:\n\n            <action>Optimize Readability</action> is preferred over <action>Reword for Better\n             Clarity</action> .\n\n            Insight: Optimizing readability takes precedence over merely rewording for clarity.\n\n   Step 2: Apply Insights to Select from Old Values\n\n   Old Values:\n\n      1. [<role>Readability Specialist</role>, <role>Paraphrasing Specialist</role>]\n\n          Based on Insight 2, \"Readability Specialist\" is already preferred, and since \"Paraphrasing\" is\n             similar to \"Sentence Simplifier\", \"Readability Specialist\" aligns better with the insight.\n\n      2. [<action>Simplify Sentence Structure</action>, <action>Break Down Complexity</action>]\n\n          Based on Insight 1 and 3:\n\n                \"Simplify Sentence Structure\" relates more directly to optimizing readability and\n                 clarifying sentence meaning.\n\n               \"Break Down Complexity\" could be interpreted similarly to \"Reduce Linguistic\n               Complexity\", which was not preferred.\n\n            So, choose \"Simplify Sentence Structure\".\n\n    Final Output (final results):\n\n\n     <res>\n\n       <role>Readability Specialist</role>\n\n       <action>Simplify Sentence Structure</action>\n\n     </res>\n\n\n                           Figure 17: The responses for sub-task II\n\n\n\n\n\n                                       28\n\nPreprint\n\n\n\n\n\n    Step 1 Memory Promptsdiscrete:\n\n\n     [\n\n         [\n\n             [<role>Sentence Simplifier</role>, <task_description>Clarify the meaning of\n\n     the given sentences by simplifying them</task_description>, <action>Optimize\n\n     Readability</action>, <requirements>Improve Structure</requirements>],\n\n             25.0\n\n         ]\n\n         [\n\n             [<role>Readability Specialist</role>, <task_description>Reduce the complexity\n\n     of the provided sentences</task_description>, <action>Clarify Sentence\n\n     Meaning</action>, <requirements>Simplify Vocabulary</requirements>],\n\n             21.0\n\n         ]\n\n         [\n\n             [<role>Grammar & Clarity Editor</role>, <task_description>Modify the given\n\n     sentences to be more straightforward</task_description>, <action>Reduce Linguistic\n\n     Complexity</action>, <requirements>Adapt to Audience</requirements>],\n\n             16.0\n\n         ]\n\n         [\n\n             [<role>AI Sentence Simplifier</role>, <task_description>Reword the provided\n\n     sentences for better clarity</task_description>, <action>Simplify Sentence\n\n     Structure</action>, <requirements>Ensure Conciseness</requirements>],\n\n             14.0\n\n         ]\n\n     ]\n\n    Step 2 Old Values:\n\n\n     [\n\n         <role>Sentence Simplifier</role>,\n\n         <action>Reduce Linguistic Complexity</action>\n\n     ]\n\n\n           Figure 18: The input for Sub-solution I - Prompts Memory in discrete form\n\n\n\n\n\n                                       29\n\nPreprint\n\n\n\n\n\n   Based on the Memory Item List and the Old Values provided, here's how we can generate the\n   Mutated Values:\n\n       1. For <role>Sentence Simplifier</role> :\n\n            This role already has the highest performance score (25.0), so we need to create a new value\n             that's different but still relevant.\n\n            Possible mutation: \"Clarity Optimizer\" (derived from the action \"Optimize Readability\" in the\n           top-performing item)\n\n       2. For <action>Reduce Linguistic Complexity</action> :\n\n            This action comes from an item with performance score 16.0 (not the highest), so we can\n            derive a new value from higher-performing items.\n\n          The top-performing item has action \"Optimize Readability\" (score 25.0)\n\n          The second-highest has action \"Clarify Sentence Meaning\" (score 21.0)\n\n   Mutated Values:\n\n\n     <res>\n\n         <role>Clarity Optimizer</role>,\n\n         <action>Optimize Readability</action>\n\n     </res>\n\n\n         Figure 19: The responses for Sub-solution I - Prompts Memory in discrete form\n\n\n\n\n\n                                       30\n\nPreprint\n\n\n\n\n\n    Step 1 Memory Promptscontinuous:\n\n\n     [\n\n         [\n\n             <prompt>You are a <role>Sentence Simplifier</role>. Given the English\n\n     Sentence, your task is to <task_description>Clarify the meaning of the given sentences\n\n     by simplifying them</task_description>. To accomplish this, you need to\n\n     <action>Optimize Readability</action>. Please make sure to <requirements>Improve\n\n     Structure</requirements> throughout the process.</prompt>,\n\n             25.0\n\n         ]\n\n         [\n\n             <prompt>You are a <role>Readability Specialist</role>. Given the English\n\n     Sentence, your task is to <task_description>Reduce the complexity of the provided\n\n     sentences</task_description>. To accomplish this, you need to <action>Clarify Sentence\n\n     Meaning</action>. Please make sure to <requirements>Simplify Vocabulary</requirements>\n\n     throughout the process.</prompt>,\n\n             21.0\n\n         ]\n\n         [\n\n             <prompt>You are a <role>Grammar & Clarity Editor</role>. Given the English\n\n     Sentence, your task is to <task_description>Modify the given sentences to be more\n\n     straightforward</task_description>. To accomplish this, you need to <action>Reduce\n\n     Linguistic Complexity</action>. Please make sure to <requirements>Adapt to\n\n     Audience</requirements> throughout the process.</prompt>,\n\n             16.0\n\n         ]\n\n         [\n\n             <prompt>You are a <role>Technical Content Simplifier</role>. Given the English\n\n     Sentence, your task is to <task_description>Reword the provided sentences for better\n\n     clarity</task_description>. To accomplish this, you need to <action>Simplify Sentence\n\n     Structure</action>. Please make sure to <requirements>Ensure\n\n     Conciseness</requirements> throughout the process.</prompt>,\n\n             14.0\n\n         ]\n\n     ]\n\n    Step 2 Current Prompt:\n\n\n     <prompt>\n\n         You are a <role>Sentence Simplifier</role>. Given the English Sentence, your task\n\n     is to <task_description>Rewrite the provided sentences in a simpler\n\n     form</task_description>. To accomplish this, you need to <action>Condense\n\n     Sentences</action>. Please make sure to <requirements>Adapt to Audience</requirements>\n\n     throughout the process.\n\n     </prompt>\n\n    Step 2 Mutate Factors:\n\n\n     <res> role | requirements </res>\n\n\n          Figure 20: The input for Sub-solution I - Prompts Memory in continuous form\n\n\n\n\n\n                                       31\n\nPreprint\n\n\n\n\n\n   Here is the Final Prompt with mutated values for the specified tags ( role and requirements ), while\n   keeping the rest unchanged:\n\n\n     <prompt>\n\n         You are a <role>Clarity Enhancer</role>. Given the English Sentence, your task is\n\n     to <task_description>Rewrite the provided sentences in a simpler\n\n     form</task_description>. To accomplish this, you need to <action>Condense\n\n     Sentences</action>. Please make sure to <requirements>Improve Structure</requirements>\n\n     throughout the process.\n\n     </prompt>\n\n  Mutation Justification:\n\n      1. <role>\n\n            Original: \"SentenceSimplifier\"\n\n          Mutated: \"ClarityEnhancer\" (Inspired by high-performing roles like \"ReadabilitySpecialist\"\n         and \"Grammar&ClarityEditor\", but distinct.)\n\n      2. <requirements>\n\n            Original: \"AdapttoAudience\"\n\n          Mutated: \"ImproveStructure\" (Drawn from the top-performing prompt's <requirements> ,\n          ensuring relevance while avoiding repetition.)\n\n    All other tags ( task_description , action ) remain unchanged as they were not in the Mutate Factors.\n   The new values align with high-scoring patterns while being unique.\n\n\n       Figure 21: The responses for Sub-solution I - Prompts Memory in continuous form\n\n\n\n\n\n                                       32\n\nPreprint\n\n\n\n\n\n    Step 1 Memory Promptsdiscrete:\n\n\n     [\n\n         [\n\n             [<role>Sentence Simplifier</role>, <task_description>Clarify the meaning of\n\n     the given sentences by simplifying them</task_description>, <action>Optimize\n\n     Readability</action>, <requirements>Improve Structure</requirements>],\n\n             25.0\n\n         ]\n\n         [\n\n             [<role>Readability Specialist</role>, <task_description>Reduce the complexity\n\n     of the provided sentences</task_description>, <action>Clarify Sentence\n\n     Meaning</action>, <requirements>Simplify Vocabulary</requirements>],\n\n             21.0\n\n         ]\n\n         [\n\n             [<role>Grammar & Clarity Editor</role>, <task_description>Modify the given\n\n     sentences to be more straightforward</task_description>, <action>Reduce Linguistic\n\n     Complexity</action>, <requirements>Adapt to Audience</requirements>],\n\n             16.0\n\n         ]\n\n         [\n\n             [<role>AI Sentence Simplifier</role>, <task_description>Reword the provided\n\n     sentences for better clarity</task_description>, <action>Simplify Sentence\n\n     Structure</action>, <requirements>Ensure Conciseness</requirements>],\n\n             14.0\n\n         ]\n\n     ]\n\n    Step 2 Old Values:\n\n\n     [\n\n         [<role>Sentence Simplifier</role>, <role>Paraphrasing Specialist</role>],\n\n         [<action>Reduce Linguistic Complexity</action>, <action>Break Down\n\n     Complexity</action>]\n\n     ]\n\n\n           Figure 22: The input for Sub-solution II - Prompts Memory in discrete form\n\n\n\n\n\n                                       33\n\nPreprint\n\n\n\n\n\n   Step 1: Conclude the Insights from the Memory Prompts\n\n   From the Memory Prompts, we can observe the following insights:\n\n      1. Roles: The roles are centered around simplifying, clarifying, or modifying sentences to improve\n        readability and reduce complexity. Examples include \"Sentence Simplifier,\" \"Readability\n         Specialist,\" \"Grammar & Clarity Editor,\" and \"AI Sentence Simplifier.\"\n\n      2. Task Descriptions: The tasks involve simplifying, rewording, or modifying sentences for clarity\n      and straightforwardness. Examples include \"Clarify the meaning of the given sentences by\n        simplifying them\" and \"Reduce the complexity of the provided sentences.\"\n\n      3. Actions: The actions focus on optimizing readability, clarifying meaning, reducing linguistic\n        complexity, and simplifying sentence structure. Examples include \"Optimize Readability\" and\n         \"Clarify Sentence Meaning.\"\n\n      4. Requirements: The requirements emphasize improving structure, simplifying vocabulary,\n       adapting to the audience, and ensuring conciseness. Examples include \"Improve Structure\" and\n        \"Simplify Vocabulary.\"\n\n   The list is sorted in descending order of performance, with \"Sentence Simplifier\" being the highest-\n   performing role.\n\n   Step 2: Mutate a new value from each pair in Old Values\n\n   Old Value Pair 1:\n\n\n     [<role>Sentence Simplifier</role>, <role>Paraphrasing Specialist</role>]\n\n      Sentence Simplifier: From the Memory Prompts, this role involves simplifying sentences for\n          clarity.\n\n       Paraphrasing Specialist: Not explicitly in the Memory Prompts, but likely involves rewording\n       sentences while preserving meaning.\n\n       Mutation: Combine the ideas of simplifying and rewording to create a new role that emphasizes\n         clarity and adaptation. For example, \"Clarity Adaptation Editor\" or \"Simplification & Paraphrasing\n        Expert.\"\n\n         Chosen mutation: \"Clarity Adaptation Editor\"\n\n   Old Value Pair 2:\n\n\n     [<action>Reduce Linguistic Complexity</action>, <action>Break Down\n\n     Complexity</action>]\n\n      Reduce Linguistic Complexity: From the Memory Prompts, this involves making language less\n       complex.\n\n      Break Down Complexity: Similar, but more focused on decomposing complex ideas.\n\n       Mutation: Combine the ideas of reducing and breaking down complexity into a more general or\n        refined action. For example, \"Streamline Language\" or \"Simplify and Deconstruct.\"\n\n         Chosen mutation: \"Streamline Language\"\n\n    Final Mutated Values:\n\n\n     <res>\n\n         [<role>Clarity Adaptation Editor</role>, <action>Streamline Language</action>]\n\n     </res>\n\n\n         Figure 23: The responses for Sub-solution II - Prompts Memory in discrete form\n\n\n\n                                       34\n\nPreprint\n\n\n\n\n\n           Step 1 Memory Promptscontinuous:\n\n\n           [\n\n               [\n\n                   <prompt>You are a <role>Sentence Simplifier</role>. Given the English Sentence,\n\n           your task is to <task_description>Clarify the meaning of the given sentences by\n\n           simplifying them</task_description>. To accomplish this, you need to <action>Optimize\n\n           Readability</action>. Please make sure to <requirements>Improve Structure</requirements>\n\n           throughout the process.</prompt>,\n\n                   25.0\n\n               ]\n\n               [\n\n                   <prompt>You are a <role>Readability Specialist</role>. Given the English\n\n           Sentence, your task is to <task_description>Reduce the complexity of the provided\n\n           sentences</task_description>. To accomplish this, you need to <action>Clarify Sentence\n\n           Meaning</action>. Please make sure to <requirements>Simplify Vocabulary</requirements>\n\n           throughout the process.</prompt>,\n\n                   21.0\n\n               ]\n\n               [\n\n                   <prompt>You are a <role>Grammar & Clarity Editor</role>. Given the English\n\n           Sentence, your task is to <task_description>Modify the given sentences to be more\n\n           straightforward</task_description>. To accomplish this, you need to <action>Reduce\n\n           Linguistic Complexity</action>. Please make sure to <requirements>Adapt to\n\n           Audience</requirements> throughout the process.</prompt>,\n\n                   16.0\n\n               ]\n\n               [\n\n                   <prompt>You are a <role>Technical Content Simplifier</role>. Given the English\n\n           Sentence, your task is to <task_description>Reword the provided sentences for better\n\n           clarity</task_description>. To accomplish this, you need to <action>Simplify Sentence\n\n           Structure</action>. Please make sure to <requirements>Ensure Conciseness</requirements>\n\n           throughout the process.</prompt>,\n\n                   14.0\n\n               ]\n\n           ]\n\n           Step 2 Prompt 1:\n\n\n           <prompt>You are a <role>Sentence Simplifier</role>. Given the English Sentence, your task\n\n           is to <task_description>Rewrite the provided sentences in a simpler\n\n           form</task_description>. To accomplish this, you need to <action>Condense\n\n           Sentences</action>. Please make sure to <requirements>Adapt to Audience</requirements>\n\n           throughout the process.\n\n           </prompt>\n\n         Mutate Factors:\n\n\n           <res> role | requirements </res>\n\n           Step 3 Prompt 3:\n\n\n           <prompt>You are a <role>Linguistic Analyst</role>. Given the English Sentence, your task\n\n           is to <task_description>Rewrite the provided sentences in a simpler\n\n           form</task_description>. To accomplish this, you need to <action>Condense\n\n           Sentences</action>. Please make sure to <requirements>Preserve Meaning</requirements>\n\n           throughout the process.\n\n           </prompt>\n\n         Mutate Factors:\n\n\n           <res> role | requirements </res>\n\n\n         Figure 24: The input for Sub-solution II - Prompts Memory in continuous form\n\n\n\n                                       35\n\nPreprint\n\n\n\n\n\n         Step 1: Insights from the Memory Prompts\n\n              1. Highest Scoring Item (Score: 25.0):\n\n                  Role: Sentence Simplifier\n\n                Task Description: Clarify the meaning of the given sentences by simplifying them\n\n                  Action: Optimize Readability\n\n                Requirements: Improve Structure\n\n              2. Other Items:\n\n                  Role: Readability Specialist (Score: 21.0)\n\n                  Role: Grammar & Clarity Editor (Score: 16.0)\n\n                  Role: Technical Content Simplifier (Score: 14.0)\n\n         Step 2: Mutate Prompt 1\n\n         Mutate Factors: <res> role | requirements </res>\n\n              Role: \"Sentence Simplifier\" is already the highest-scoring role, so we generate a new value: \"Clarity\n              Enhancer\".\n\n             Requirements: \"Adapt to Audience\" is not the highest-scoring (highest is \"Improve Structure\"), so\n          we replace it with \"Improve Structure\".\n\n        Prompt 2:\n\n\n           <prompt>You are a <role>Clarity Enhancer</role>. Given the English Sentence, your task is\n\n           to <task_description>Rewrite the provided sentences in a simpler form</task_description>.\n\n           To accomplish this, you need to <action>Condense Sentences</action>. Please make sure to\n\n           <requirements>Improve Structure</requirements> throughout the process.</prompt>\n\n         Step 3: Mutate Prompt 3\n\n         Mutate Factors: <res> role | requirements </res>\n\n              Role: \"Linguistic Analyst\" is not in the Memory Item List, so we replace it with the highest-scoring\n                 role: \"Sentence Simplifier\".\n\n             Requirements: \"Preserve Meaning\" is not the highest-scoring, so we replace it with \"Improve\n                Structure\".\n\n        Prompt 4:\n\n\n           <prompt>You are a <role>Sentence Simplifier</role>. Given the English Sentence, your task\n\n           is to <task_description>Rewrite the provided sentences in a simpler\n\n           form</task_description>. To accomplish this, you need to <action>Condense\n\n           Sentences</action>. Please make sure to <requirements>Improve Structure</requirements>\n\n           throughout the process.</prompt>\n\n         Step 4: Combine Prompt 2 and Prompt 4\n\n        Combine the best-performing markup pairs from Prompt 2 and Prompt 4:\n\n              Role: \"Clarity Enhancer\" (from Prompt 2)\n\n             Task Description: \"Rewrite the provided sentences in a simpler form\" (common in both)\n\n              Action: \"Condense Sentences\" (common in both)\n\n             Requirements: \"Improve Structure\" (common in both)\n\n           Final Prompt:\n\n\n           <prompt>You are a <role>Clarity Enhancer</role>. Given the English Sentence, your task is\n\n           to <task_description>Rewrite the provided sentences in a simpler form</task_description>.\n\n           To accomplish this, you need to <action>Condense Sentences</action>. Please make sure to\n\n           <requirements>Improve Structure</requirements> throughout the process.</prompt>\n\n\n       Figure 25: The responses for Sub-solution II - Prompts Memory in continuous form\n\n\n\n\n                                       36",
"headers": [
"arXiv:2510.18257v1  [cs.CL]  21 Oct 2025",
"D",
"PO: D",
"-G",
"S",
"-E",
"F",
"P",
"O",
"ELVE",
"IRECTION",
"UIDED",
"ELF",
"VOLVING",
"RAMEWORK FOR",
"LEXIBLE",
"ROMPT",
"PTIMIZATION",
"A",
"1",
"I",
"2",
"3",
"M",
"4",
"E",
"5",
"R",
"W",
"6",
"C",
"L",
"U",
"LLM",
"B",
"T",
"& P",
"G",
"H",
"p",
"Mutation Justification:",
"p'",
"⋮",
"s",
"<",
"'",
"Preprint",
"Tao Tao, Guanghui Zhu",
", Lang Guo, Hongyi Chen, Chunfeng Yuan, Yihua Huang",
"State Key Laboratory for Novel Software Technology, Nanjing University",
"taot@smail.nju.edu.cn, zgh@nju.edu.cn",
"Prompt Optimization has emerged as a crucial approach due to its capabilities in",
"steering Large Language Models to solve various tasks. However, current works",
"mainly rely on the random rewriting ability of LLMs, and the optimization pro-",
"cess generally focus on specific influencing factors, which makes it easy to fall",
"into local optimum. Besides, the performance of the optimized prompt is of-",
"ten unstable, which limits its transferability in different tasks. To address the",
"above challenges, we propose",
"DelvePO",
"(",
"irection-Guid",
"e",
"d Se",
"l",
"f-E",
"v",
"olving Frame-",
"work for Fl",
"xible",
"rompt",
"ptimization), a task-agnostic framework to optimize",
"prompts in self-evolve manner. In our framework, we decouple prompts into",
"different components that can be used to explore the impact that different fac-",
"tors may have on various tasks. On this basis, we introduce working memory,",
"through which LLMs can alleviate the deficiencies caused by their own uncer-",
"tainties and further obtain key insights to guide the generation of new prompts.",
"Extensive experiments conducted on different tasks covering various domains for",
"both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B,",
"Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO",
"consistently outperforms previous SOTA methods under identical experimental",
"settings, demonstrating its effectiveness and transferability across different tasks.",
"The rapid advancement of Large Language Models (LLMs) (DeepSeek-AI, 2025; Li et al., 2025)",
"has revolutionized various real-world applications (Shao et al., 2024; Zheng et al., 2025) . Prompt,",
"a method that steers LLMs to produce desired results without modifying parameters, has garnered",
"significant interest among non-AI experts from different domains (Wan et al., 2024; Guo et al., 2025;",
"Fernando et al., 2024). Consequently, the rapid growth in users has increased demand for prompt",
"engineering methods.",
"Previous efforts primarily focused on manually designing specialized prompts (Brown et al., 2020;",
"Kojima et al., 2022; Wei et al., 2023). However, this kind of method is time-consuming and demands",
"extensive trial and error, making it less versatile for diverse tasks and limiting their real-world effec-",
"tiveness. To reduce the human effort required for constructing effective prompts, many researches",
"(Shum et al., 2023; Wang et al., 2023c; Zhang et al., 2022; Feng et al., 2024; He et al., 2024) have",
"increasingly explored methods such as curating unified demonstrations for related tasks, systemati-",
"cally designing domain-specific templates, and identifying critical factors for prompt performance.",
"However, these methods exhibit limited applicability across diverse scenarios.",
"Subsequently, a series of research emerged that employ optimization algorithms to refine prompts.",
"Such approaches (e.g. APE (Zhou et al., 2023b), PromptBreeder (Fernando et al., 2024), and Evo-",
"Prompt (Guo et al., 2025)) synergistically integrate the efficiency inherent in the algorithms with the",
"powerful text processing ability of LLMs, achieving relatively stable performance enhancement on",
"target datasets. Although these studies analogize the mutation operation in evolutionary algorithms",
"to the rewriting operation of LLMs, they fail to fully harness the efficiency and rapid convergence",
"inherent to such algorithms, which ultimately limits the realization of their performance advan-",
"tages in prompt optimization. The primary reason lies in the inherently stochastic nature of the",
"evolutionary process: the directionality of mutation operations remains uncontrollable, while their",
"interpretability is also notably limited. Furthermore, these methods neglect the potential impact of",
"constituent components within a prompt on overall performance, leading to premature convergence",
"in local optima. For example, during evolutionary phase of EvoPrompt, the initial prompt inherently",
"contains the \"role\" as a critical component. However, due to the stochastic nature of the mutation",
"process, the stochastic mutation process may accidentally remove this component. Once discarded,",
"it cannot be reintegrated into subsequent evolutionary iterations. Such degradation significantly",
"heightens the risk of premature convergence in local optima. A parallel limitation is observed in",
"the PromptBreeder method, which exhibits even higher stochasticity, as its implementation not only",
"uses two distinct mutation prompts but also employs diverse mutation operators, amplifying ran-",
"domness throughout the optimization process. We summarize that a robust Prompt Optimization",
"(PO) must have the following characteristics:",
"•",
"Seamlessly integrating domain expert experience",
": For tasks in different domains, prior experi-",
"ence from domain experts can be incorporated into the PO algorithm, thus improving the efficiency",
"of the optimization process.",
"Actively exploring factors that may affect prompt performance",
": The method can actively",
"explore factors affecting prompt performance to guide optimization using historical data.",
"Adaptively identifying optimal prompts for different LLMs with varying performance",
": The",
"algorithm self-adjusts to discover the best prompts for target tasks across differently specialized",
"models and scenarios, ensuring broad applicability in diverse professional contexts.",
"Integrating insights from existing research, we propose",
"olving",
"Framework for Fl",
"ptimization) that adaptively accommodates diverse LLMs and",
"self-improves through guidance from its historical optimization strategies. Inspired by the concept of",
"Loci (the corresponding location of genes with important functions) and Alleles (different versions",
"of the same gene) on genetics, this framework first decouples prompt instructions into functional",
"components (analogous to Loci). Subsequently, it iteratively evolves these components by explor-",
"ing the potential impacts of diverse allele variations, ultimately achieving holistic optimization of the",
"entire prompt through systematic recombination. In particular, building upon the components, we",
"introduce working memory mechanism (i.e., Component Memory and Prompt Memory) to guide the",
"evolutionary process. Component Memory is designed to capture evolutionary trends in individual",
"components and utilize these trends to guide further optimization of each element. Take the com-",
"ponent a step further, Prompt Memory creates interconnections between components by utilizing",
"contextual information to guide the progressive optimization of the entire prompt. The contributions",
"of our work can be summarized as follows:",
"• To the best of our knowledge, our work is the first to introduce memory mechanism to guide",
"prompt optimization, not only stabilizing the performance of the entire prompt population but",
"also greatly reducing the time required for evolutionary operations.",
"• By decoupling prompt into multiple components and designing guided evolutionary mechanisms,",
"our framework integrates multiple influencing factors into a single prompt. This integration not",
"only enhances the scalability of PO methods but also improves the interpretability of the optimiza-",
"tion process, significantly lowering the difficulty to interact with the system.",
"• For LLMs with varying performance levels, our framework can elicit their capabilities, striking",
"a good balance between exploring diverse components and exploiting the current derived good",
"components, ultimately obtaining optimal prompts that adapt to the target tasks and LLMs simul-",
"taneously. Extensive experimental results on multiple datasets and three widely-adopted LLMs",
"reveal that DelvePO outperforms manually crafted prompts and existing PO methods.",
"Given task",
"= (",
",",
")",
"is the task-related dataset and",
"represents the corresponding answer",
"to the dataset, prompt optimization can be briefly described as follows: Guided by the working",
"memory mechanism, the initial prompt population",
"=",
"{",
", p",
"· · · }",
"is continuously optimized",
"to obtain the final prompt population",
". The best prompt",
"can be selected as follows:",
"←",
"arg max",
"f",
"\u0000",
"ϕ",
"p,",
"\u0001",
"where",
"is the development dataset and",
"means that the prompts and questions",
"are combined and then fed into the LLM to produce the corresponding response. The important",
"concepts used in our proposed framework are described below.",
"Components",
"Similar to the relationship between Loci and Chromosome, components are mainly",
"used to identify the location of key factors that affect task performance in prompts. Different tasks",
"can introduce distinct components or reuse existing ones. Components are extensible, i.e., the type",
"and number of components can be user-defined, and our method can also evolve synchronously as",
"the context length that LLMs can receive increases. In this paper, we construct a comprehensive",
"and representative component pool from a broad set of related literature. Further details on how the",
"components are studied and predefined in our framework are provided in Appendix E.",
"Templates",
"To bind components to prompts, we design a general template (corresponding to the",
"Chromosome functionally), whose content is mainly composed of two parts: general and unchang-",
"ing text; domain-specific and replaceable descriptive text (i.e., components and their correspond-",
"ing values). For the descriptive text, its main functions include explaining domain-specific com-",
"ponents, connecting different components, and providing contextual semantics about components.",
"To overcome the instability of LLMs in recognizing components, we borrow the design idea of",
"\"markup\" from HyperText Markup Language (HTML) to define different domain components. Tak-",
"ing \"",
"<role></role>",
"\" as an example, the \"role\" is one of the various component types. Accordingly,",
"the value of the component will be enclosed within the markup pairs, i.e., <role>",
"Sentence Simpli-",
"fier",
"</role>. More details can be found in Figure 5 in Appendix F.",
"3.1",
"PO",
"Our self-evolution prompt optimization framework consists of 4 necessary functional modules:",
"Sampling & Update module, Task-Evolution module, Solution-Evolution module and Memory-",
"evolution module. We define the",
"Task",
"as \"discover the promising direction of evolution\", that is,",
"determining the component (types or values) that need to evolve in the next step under the guidance",
"of components memory. We define the",
"Solution",
"as \"make sure the process of evolutionary opera-",
"tion and perform evolutionary operation\", i.e., under the guidance of prompts memory, evolutionary",
"operations are applied to the component values according to the selected evolution type: for a sin-",
"gle sample, only mutation is performed, while for two samples, both mutation and crossover are",
"executed. For memory-evolution, it mainly uses the evolved prompts and component value pairs",
"before and after evolution to update the prompts memory and components memory, respectively.",
"In the sampling and update module, when the number of iterations reaches a pre-defined value, the",
"population is updated. Otherwise, a new sampling operation is performed within the current popu-",
"lation, which in turn triggers the next round of self-evolution operations. The designs of",
"framework is shown in Figure 1. Next, we first introduce the working memory mechanism.",
"Components Memory",
"stores the corresponding component values before and after evolution, which",
"is selected according to the mutated component type. The value pairs will be ordered by descend,",
"i.e., when injecting to the final prompt, the first value performs better than the second. Components",
"Memory will guide the selection of components in the Task-Evolution stage.",
"Prompts Memory",
"stores the prompts after each step of evolution. The evolved prompts are stored",
"in descending order according to their performance scores. There are two forms of prompts mem-",
"ory: discrete form and continuous form. The discrete version only stores discrete combinations of",
"component value in the prompt. And the continuous version stores a complete prompt formed by",
"injecting component value into the template, which means that it stores continuous text containing",
"context. Prompts memory will be used to guide the mutation of component or the crossover of the",
"prompt in the Solution-Evolution stage.",
"Figure 1:",
"The Framework of DelvePO",
". Initialization begins with predefined components, which",
"are concatenated to form individual",
"; multiple individuals constitute the initial population",
"Pop-",
"ulations (old)",
". At each step, one individual (Mutation only) or two individuals (Mutation and",
"Crossover) are sampled, and the",
"Sub-task",
"determines the evolutionary direction (i.e., the mutated",
"component type). Guided by",
"Task-",
"Solution-",
", and",
"Memory-Evolution",
"modules, selected prompts",
"are iteratively evolved, contrasting with unguided optimization. The new population",
"Populations",
"(new)",
"is accumulated across epochs, and once the threshold is reached, the population is updated to",
"initiate the next round of self-evolution.",
"3.2",
"As shown in Figure 1, the workflow of",
"contains several core stages as outlined below.",
"Initialization & Sampling",
": First, we use task-agnostic component-value generation prompt (see",
"Figure 4 in Appendix C) to generate candidate values for each component type. Then, we randomly",
"sample from these candidates and inject the selected values into the population-construction tem-",
"plate (illustrated in Figure 5 in Appendix F) to construct the initial population. Each individual in the",
"initial population is evaluated on the development dataset to obtain its performance score. Finally,",
"the sorted population is stored as the initial prompts memory. Before the population evolves, there is",
"no components memory. After initialization, the sampling process begins, aiming to select prompts",
"from the current population for evolution. Inspired by genetic principles, there are two main ways",
"to generate new individuals: mutating a single individual or performing crossover between two indi-",
"viduals. Notably, mutation may also occur during crossover. To account for these cases, we assume",
"that the number of individuals selected in each sampling step can be either 1 or 2.",
"The evolutionary process mainly includes two parts: generating new individuals based on selected",
"individuals; generating and storing the working memory. Specifically, there are 3 types of evolu-",
"tion, namely",
"Task-Evolution",
"Solution-Evolution",
". The mechanism of Task-",
"Evolution and Solution-Evolution is shown in Figure 2.",
"For task evolution, considering the components and the evolutionary operations",
"(mutation and crossover), we design two kinds of evolutionary sub-tasks. The detailed information",
"is shown in Figure 7 and Figure 8 (see Appendix G).",
"Figure 2: The mechanism of Task-Evolution and Solution-Evolution.",
"Using the pseudo-prompt to",
"explain the details of Task- and Solution-Evolution.",
"Sub-task I",
": This task mainly uses mutation operations to process a single candidate prompt. First,",
"the semantic comprehension capability of the LLMs is utilized to obtain the relevant insights",
"of component evolution from the component memory",
". Then, the insights are used",
"to guide the selection of components. Finally, the selected components will be treated as the",
"promising direction to guide the evolution of mutation-based solution.",
"Sub-task II",
": After performing Sub-task I on the two candidate prompts, we can get the respective",
"component types set",
"and",
"for two prompts (say",
") as the promising direction for",
"mutation. The final mutated component type is selected as",
"ˆ",
"∩",
". Next, for each compo-",
"nent in",
"˜",
"\\",
"denotes the set of all component types, corresponding contents from",
"are extracted to construct a pair. Then, based on the insights derived from",
"one value from each pair is selected as the potential value to improve performance of the prompts",
"after evolution. Finally, the component types from",
"will be treated as the promising direction",
"to guide the evolution of crossover-based solution, and the selected values from",
"or",
"whose",
"component types coming from",
"will also be passed into the corresponding Solution-Evolution",
"phase to help construct the final prompts.",
"The main goal of solution evolution is to utilize the insights (derived from the",
"prompts memory) and direction (received from the task-evolution) to perform evolution operations",
"on the corresponding content in the current prompt and generate a new prompt that performs better.",
"In this phase, we propose 2 sub-solutions corresponding to 2 sub-tasks. Depending on whether the",
"prompt is continuous or discrete, each sub-solution can also be further divided to eliminate the effect",
"of prompt format on the final result.",
"Sub-solution I",
": Extract component contents from current prompt based on the results obtained by",
"sub-task I (i.e., the mutated components that are most likely to improve prompt performance). The",
"extracted contents are then mutated using insights obtained from the prompts memory",
"stored in discrete or continuous forms. Those contents that have not been mutated will be retained",
"in new prompts. Finally, the mutated and unmutated component contents will be integrated as the",
"result of sub-solution I. The corresponding prompts are shown in Figure 9, 10 (see Appendix H)",
"for the prompts memory in discrete and continuous forms, respectively.",
"Sub-solution II",
": This mainly uses the results from sub-task II as a guide, and extracts component",
"contents from the currently selected two prompts. And the evolutionary operations would combine",
"mutation and crossover. First, for components that do not require mutation, the corresponding",
"content is received from sub-task II. Then, for the component that need to be mutated, we extract",
"its content from the two prompts. Based on the evolutionary insights derived from the prompt",
"memory",
", the mutation operations are performed on the extracted content. Next, the",
"generated two prompts will crossover on the component types that need to be mutated. Finally,",
"the results obtained from the mutation and crossover operations are integrated to generate a new",
"prompt as the result of the sub-solution II. The details are shown in Figure 11 for the prompts",
"memory in discrete form and Figure 12, 13 for continuous form (see Appendix H).",
"is based on the component pairs and prompts both before and after the evolu-",
"tion to update the corresponding components memory and prompts memory, which is used to guide",
"the next evolution process. In this module, the",
"evaluation",
"will be performed. Specifically, to clearly",
"describe the evaluation process, we illustrate a general form of a prompt designed for LLMs that",
"can be applied across different tasks (shown in Figure 6). Evaluation refers to calculating the per-",
"formance score of the generated new prompts on the development dataset based on the evaluation",
"metrics of the target task, according to which components can be sorted and memory can be updated.",
"Update",
": Add the evolved prompts to the temporary population generated in each iteration. When",
"the iteration ends, the temporary and current populations are mixed, and Top-N is selected as the",
"updated population for the next iteration based on performance.",
"The details of",
"are outlined in Algorithm 1, which can be found in Appendix C.",
"4.1",
"Baselines",
"In our experiments, We choose 6 commonly used methods which have been widely proven",
"to be efficient in the field of prompt optimization as our baselines, which are: Crafted by human",
"experts, CoT-ZS, CoT-FS, Promptbreeder, APE, and EvoPrompt.",
"Human",
"corresponds to manually crafted prompts by experts, as detailed in the relevant literature",
"Zhang et al. (2024); Sanh et al. (2022), which primarily derived from prior studies.",
"CoT",
"has been extensively applied in various domains, represents a rationale-based approach. We",
"evaluate two representative forms of CoT:",
"CoT-ZS",
"(Zero-Shot CoT, Kojima et al. (2022)) and",
"CoT-FS",
"(Few-Shot CoT, also known as Manual-CoT, Wei et al. (2023)).",
"APE",
"(Zhou et al., 2023b) regards instructions as programs and uses Monte Carlo Search to select",
"appropriate instructions as optimized prompts under LLM guidance.",
"Promptbreeder",
"(Fernando et al., 2024) further investigates the effect of different mutation strate-",
"gies on self-optimization based on elaborately designed evolutionary operations.",
"EvoPrompt",
"(Guo et al., 2025) introduces evolutionary algorithms to prompt optimization for",
"the first time. Considering different scenarios, it instantiates its framework using two practical",
"evolutionary algorithms. According to its statement, compared with GA method, the DE method",
"has a wider range of use in solving complex problems. Therefore, we select EvoPrompt-DE as",
"our baseline, and denote it simply as EvoPrompt.",
"Datasets and LLMs",
"To demonstrate the generalizability of our method, we conducted experiments",
"on 11 datasets across three LLMs, covering diverse domains and representative real-world tasks.",
"The details information about datasets and LLMs are represented in Appendix B. Other experimental",
"details (e.g., Computational Resources and Hyperparameter Details) are represented in Section 6.",
"4.2",
"Following the same settings as baselines, we tested the best prompts obtained during training. The",
"main experimental results (as shown in Table 1) on DeepSeek-R1-Distill-Llama-8B are reported as",
"averages over three random seeds, with standard deviations provided. It is worth noting that we",
"observed Promptbreeder to be significantly more time-consuming than other methods (as shown in",
"Figure 3). To balance the diversity of baselines and ensure the fairness in training time, we therefore",
"report results for Promptbreeder using a single random seed.",
"From Table 1, we can observe that our method achieves substantial improvements over manual ap-",
"proaches. Among the automated optimization methods, our method consistently outperforms base-",
"lines, demonstrating not only its effectiveness but also its adaptability to different task types. From",
"the results on classical NLP benchmarks, we observe that the baselines perform well, confirming",
"their effectiveness on established datasets. However, on more recently introduced benchmarks that",
"demand broader capabilities, automated prompt optimization methods generally perform better, with",
"our approach showing particularly substantial improvements. These results indicate that as LLMs",
"continue to advance, prompt optimization techniques must likewise evolve, and our framework de-",
"livers consistently strong performance across diverse domains.",
"Table 1: Main results on different downstream tasks for DeepSeek-R1-Distill-Llama-8B. Since",
"expert-written prompts are not available for all datasets, sign (\"-\") is used to indicate missing cases.",
"Table 2: The results on different downstream tasks for GPT-4o-mini.",
"To further evaluate the performance of our framework on different LLMs, we conducted additional",
"experiments across different task types on the closed-source model (GPT-4o-mini, results reported",
"in Table 2) and the widely used open-source model (Qwen2.5-7B-Instruct, shown in Table 5 in",
"Appendix D). The experimental settings were kept identical to the main experiments. As shown",
"in the results evaluated on these two LLMs, our framework consistently delivers either superior",
"or competitive performance across multiple task types, demonstrating its robustness and general",
"effectiveness when applied to diverse LLMs.",
"4.3",
"In our experiments, the overhead primarily stems from the training time required for open-source",
"LLMs and the number of tokens consumed in API requests for closed-source LLMs. Accordingly,",
"for DeepSeek-R1-Distill-Llama-8B, we randomly selected one dataset from each task type and mea-",
"sured the time cost of different baselines, with results presented in Figure 3. The statistics indicate",
"that our method consistently outperforms or matches the baselines in terms of optimization speed,",
"particularly when compared with PromptBreeder. This also explains why we report its results using",
"a single random seed. Overall, the results demonstrate that our method can more effectively exploit",
"the rapid convergence property of evolutionary algorithms for faster optimization.",
"Moreover, we reported token usage in terms of the actual monetary expenditure, as shown in Table 6.",
"Overall, as shown in Table 2 and Table 6, although our method requires higher expenditure, it con-",
"sistently delivers performance above or competitive with the baselines, indicating that our approach",
"offers a favorable balance between performance and cost. We also analyzed the reasons behind the",
"generally higher token usage. The primary factor is that the content stored in the memory module",
"is included as part of the input provided to the target LLMs. In future work, we plan to integrate",
"prompt compression techniques into the framework to reduce this overhead.",
"4.4",
"To evaluate the impact of the memory mechanism in our framework, we conducted ablation experi-",
"ments on GPT-4o-mini. We selected three datasets of different types to evaluate the adaptability of",
"the memory mechanisms across multiple scenarios. Table 3 reports the performance on three types",
"of datasets using a single random seed. When both memory mechanisms are included and oper-",
"7",
"Figure 3: Average time-consuming (GPU hours) for one epoch of optimization on DeepSeek-R1-",
"Distill-Llama-8B.",
"Table 3: Ablations of Memory Mechanism.",
"ate in coordination, the overall performance is substantially higher than in the other configurations,",
"demonstrating the effectiveness and complementary benefits of the proposed memory design.",
"Table 4: Sensitivity test regarding the number of",
"component values.",
"# Value",
"SAMSum",
"SQuAD",
"SST-5",
"50",
"29.2",
"67.9",
"57.2",
"40",
"67.3",
"57.4",
"30",
"29.7",
"66.8",
"56.8",
"20",
"28.8",
"66.5",
"59.1",
"10",
"30.2",
"69.7",
"60.3",
"Furthermore, to investigate the impact of the",
"number of component values for each compo-",
"nent type on the overall performance of initial",
"population, we designed a sensitivity test exam-",
"ining how initial population performance varies",
"with the number of component values at ini-",
"tialization. Using GPT-4o-mini, we generated",
"initial populations for three different types of",
"datasets under a single random seed and evalu-",
"ated their performance. The results in Table 4",
"show that increasing the number of component",
"values does not cause significant fluctuations in",
"the initial population performance. This indicates that a relatively small number of component",
"values is sufficient to obtain an initial population with stable and reasonable performance, and im-",
"portantly, it rules out the concern that a larger number of components could lead to an overestimated",
"initial population, which might otherwise suggest that further optimization is unnecessary.",
"Moreover, we conducted a",
"case study",
"to help researchers quickly understand the operational mech-",
"anisms of our proposed framework. The details are presented in Appendix I.",
"Prompt Engineering",
"Prompt engineering is a resource-efficient approach, focusing on elaborately",
"designing expert-level prompts to steer LLMs generate desired solutions to various downstream",
"tasks. In this part, we mainly focus on those works which use prompts to stimulate the internal abil-",
"ities of LLMs. Least-to-Most (Zhou et al., 2023a), Decomposed Prompting (Khot et al., 2023) and",
"PS&PS+ (Wang et al., 2023a) use prompts to leverage the decomposition ability of LLMs, breaking",
"8",
"down complex problems into simpler ones, enabling the model to perform better when dealing with",
"complex problems. CoT (Wei et al., 2023), PoT (Chen et al., 2023), PS & PS+ (Wang et al., 2023a),",
"Automate-CoT (Shum et al., 2023), ToT (Yao et al., 2023)and GoT (Besta et al., 2024) guide the",
"model to utilize chain-of-thought in different ways through the design of prompts, stimulating the",
"thinking ability of the model, thereby enhancing the model’s reasoning ability. Also, Complexity-",
"based Prompting (Fu et al., 2023) and DIV-SE (Naik et al., 2024) focus on the complexity and diver-",
"sity of prompts, aiming to help the model think better. Rephrase and Respond (Deng et al., 2024),",
"OPRO (Yang et al., 2024), and MIPRO (Opsahl-Ong et al., 2024) utilized the self-optimization",
"capabilities of LLMs through methods such as input rewriting, iterative prompt optimization and",
"structured program optimization, jointly demonstrating that LLMs can autonomously enhance the",
"performance of task execution by dynamically improving prompts. TextGrad (Yuksekgonul et al.,",
"2025) and SPO (Xiang et al., 2025) combine LLMs by orchestrating Standard Operation Pipelines",
"(SOPs) in advance, and uses the evaluation ability of the model itself to guide the optimization of",
"prompts. These methods effectively demonstrate that LLMs can be more proactive in utilizing their",
"exploration abilities under the scientific guidance of predefined SOPs. Although the above works",
"have elicited some abilities of LLMs to cope with complex problems, they cannot get rid of the",
"problem that LLMs are sensitive to inputs, which results in the inconsistency of outputs’ quality.",
"Prompt Optimization",
"Given a downstream task, prompt optimization aims to improve the effec-",
"tiveness of prompt, which typically involves an iterative process including initialization, execution,",
"evaluation and selection. This part primarily focus on those works which leverage external tech-",
"nologies or exogenous intelligence sources to guide LLMs to perform prompt optimization. Using",
"external knowledge to optimize prompt is very effective. Existing works generally referred to: 1)",
"the way humans think (Wang et al., 2023c); 2) the idea of program synthesis (Zhou et al., 2023b);",
"3) external knowledge (Zhao et al., 2023) to optimize prompts which achieve good results. Format-",
"ting the structure of prompts can standardize the thinking process of LLMs, and to a certain extent",
"improve their reasoning capability. LangGPT (Wang et al., 2024) presents a framework for prompt",
"design, proving that scalable structures are important for prompts migration. Prompt template (He",
"et al., 2024) delves into the impact of the format of the prompt template on solving problems, demon-",
"strating the effectiveness of structured prompts in eliciting LLMs’ capabilities. Furthermore, there",
"are some efforts that introduce algorithms that have been widely proven to have good optimization",
"capabilities to the optimization of prompts, including K-means (Zhang et al., 2022), KNN (Shi et al.,",
"2022), reinforcement learning (Pryzant et al., 2023; Wang et al., 2023b), active learning (Diao et al.,",
"2024), and evolutionary algorithm (Guo et al., 2025; Fernando et al., 2024).",
"In summary, although existing studies have mitigated the output stochasticity of LLMs, the effi-",
"ciency of the optimization algorithm has still not been fully explored. These efforts generally tend",
"to treat prompts as a whole unit to optimize, so the potential optimization space is very large. In",
"addition, most previous researches combining optimizing algorithms (e.g., evolutionary algorithms)",
"with LLMs, do not take full advantage of the experience generated before and after optimization,",
"so that the optimization process is more stochastic, which tends to fall into local optima. Inspired",
"by biological Loci and Alleles, this paper proposes a flexible framework for prompt optimization,",
"which can effectively reduce the randomness of the optimization process and significantly improves",
"the optimization speed. We hope our approach will provide possible improvements for subsequent",
"PO methods, significantly lowering the learning barrier for non-AI experts to leverage LLMs.",
"We introduced DelvePO, a self-evolving framework for prompt optimization that decouples prompts",
"into distinct components. With components, prompts can be modified by adding or removing con-",
"tent that may affect their performance, striking a good balance between exploration and exploitation",
"of factors that affect task performance. DelvePO employs a co-evolutionary mechanism to iter-",
"atively refine the specifics of two sub-tasks and generate corresponding solutions. The evolved",
"prompt, following systematic processing, is encoded into working memory to facilitate LLMs in de-",
"riving relevant insights, thereby provides directional guidance for generating task-specific prompts.",
"Extensive experiments on different tasks demonstrate DelvePO consistently outperforms baselines,",
"validating its effectiveness. As we anticipate the emergence of even more powerful LLMs that can",
"deal with longer context, we firmly believe that more professional prompts will penetrate all walks",
"of life, and DelvePO will help more users complete various complex tasks.",
"9",
"This work studies prompt optimization techniques for language models (LLMs) to better elicit their",
"capabilities in solving target tasks. The primary potential risks of this research are related to the",
"misuse of LLMs, for example, generating misleading, harmful, or biased content.",
"In our experiments, we only use publicly available datasets and pre-trained LLMs, and no private",
"or sensitive data were involved. Specific statements on LLM usage can be found in Appendix",
"A. We emphasize that our methods are intended for research and benchmarking purposes, and we",
"encourage responsible use to mitigate potential societal risks.",
"We are committed to ensuring the reproducibility of our work. To facilitate replication, we provide",
"the following details:",
"Computational Resources",
"The following describes the experimental environment, including de-",
"tailed information on both hardware and software configurations.",
"Hardware",
". All experiments were conducted on a computing node equipped with four NVIDIA",
"Tesla V100-SXM2 GPUs (32GB memory each), an Intel Xeon Gold 6248 CPU @ 2.50GHz with",
"20 cores, and 226 GB of RAM.",
"Software",
". The system runs Ubuntu 20.04.6 LTS with Linux kernel version 5.4.0. All models were",
"implemented in Python 3.10.18 using PyTorch 2.0.0 with CUDA 11.7.",
"Hyperparameter Details",
"In order to isolate the effect of our proposed method and ensure a fair",
"comparison, we mainly followed the default configurations used in baseline methods and intention-",
"ally introduced no additional trainable parameters. Specifically, the detailed hyperparameter settings",
"are given below.",
"Initial Population Size",
". Following the setup of EvoPrompt, which uses both human-written and",
"LLM-generated prompts, we adopted a similar strategy in spirit but tailored it to our fully auto-",
"mated framework. (1) We identify a fixed set of components through preliminary study mentioned",
"at ref . (2) For each component, we use an LLM to generate 10 candidate values based on prompt",
"templates. (3) We then randomly combine these values to create 10 initial prompts, which together",
"form the initial population for the evolutionary process.",
"Temperature",
". Since the stochasticity of LLM outputs is sensitive to temperature settings, we set",
"the temperature to 0.5 to strike a balance between exploration and exploitation. This choice aligns",
"with prior work such as EvoPrompt.",
"Sample Allocation",
". For data splits, we followed the protocols of APE and EvoPrompt. Specif-",
"ically, if the dataset has a predefined training/testing split, we used it as-is. For datasets without",
"predefined splits, we randomly selected 100 examples as the test set and used the remaining ex-",
"amples for training.",
"Randomness Control",
". To ensure reproducibility. Unless otherwise noted, we use 3 random seeds",
"(5, 10 and 15) in the training phrase, and reported the results on the test set.",
"While our framework can adaptively design well-matched prompts for any LLM across diverse",
"downstream tasks, several limitations remain. (1) Due to substantial computational costs, we cannot",
"comprehensively evaluate all models and domains. Instead, we focused on widely used datasets to",
"balance fairness and coverage. (2) Although we report monetary cost based on actual token usage,",
"variations in token pricing across input and output types cannot be precisely captured by the API.",
"Analysis indicates that most of the cost arises from including memory content as input tokens, while",
"output token consumption remains relatively modest, particularly when \"thinking mode\" is disabled.",
"Future work will explore prompt compression to further optimize resource use. (3) We evaluated",
"only representative component values from each category due to resource constraints. Nevertheless,",
"even with this limited set, our approach continues to outperforms or remains competitive with base-",
"lines, demonstrating its effectiveness and suggesting that its benefits will likely increase as LLMs",
"support longer contexts.",
"Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gi-",
"aninazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten",
"Hoefler.",
"Graph of thoughts: Solving elaborate problems with large language models.",
"Pro-",
"ceedings of the AAAI Conference on Artificial Intelligence",
", 38(16):17682–17690, March 2024.",
"ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL",
"http://dx.doi.org/10.1609/",
"aaai.v38i16.29720",
".",
"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-",
"wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,",
"Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.",
"Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,",
"Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,",
"Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In",
"Proceedings of the",
"34th International Conference on Neural Information Processing Systems",
", NIPS ’20, Red Hook,",
"NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.",
"Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:",
"Disentangling computation from reasoning for numerical reasoning tasks, 2023. URL",
"https:",
"//arxiv.org/abs/2211.12588",
"DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,",
"2025. URL",
"https://arxiv.org/abs/2501.12948",
"DeepSeek Chat. Deepseek chat web interface, 2025. URL",
"https://chat.deepseek.com/",
"Accessed: 2025-08.",
"Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large",
"language models ask better questions for themselves, 2024.",
"URL",
"https://arxiv.org/",
"abs/2311.04205",
"Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, and Tong Zhang. Active prompting",
"with chain-of-thought for large language models, 2024. URL",
"https://arxiv.org/abs/",
"2302.12246",
"Longyu Feng, Mengze Hong, and Chen Jason Zhang. Auto-demo prompting: Leveraging generated",
"outputs as demonstrations for enhanced batch prompting.",
"arXiv preprint arXiv:2410.01724",
", 2024.",
"Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel.",
"Promptbreeder: self-referential self-improvement via prompt evolution. In",
"41st International Conference on Machine Learning",
", ICML’24. JMLR.org, 2024.",
"Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting",
"for multi-step reasoning, 2023. URL",
"https://arxiv.org/abs/2210.00720",
"Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and",
"Yujiu Yang. Evoprompt: Connecting llms with evolutionary algorithms yields powerful prompt",
"optimizers, 2025. URL",
"https://arxiv.org/abs/2309.08532",
"Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, and Sadid Hasan.",
"Does prompt formatting have any impact on llm performance?",
"arXiv preprint arXiv:2411.10541",
"2024.",
"Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish",
"Sabharwal. Decomposed prompting: A modular approach for solving complex tasks, 2023. URL",
"https://arxiv.org/abs/2210.02406",
"Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwa-",
"sawa.",
"Large language models are zero-shot reasoners.",
"In S. Koyejo,",
"S. Mo-",
"hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.),",
"Advances in Neural In-",
"formation Processing Systems",
"volume 35,",
"pp. 22199–22213. Curran Associates,",
"Inc.,",
"2022.",
"https://proceedings.neurips.cc/paper_files/paper/2022/",
"file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf",
"11",
"Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang,",
"and Zhicheng Dou.",
"Search-o1: Agentic search-enhanced large reasoning models.",
"CoRR",
"abs/2501.05366, 2025. doi: 10.48550/ARXIV.2501.05366. URL",
"https://doi.org/10.",
"48550/arXiv.2501.05366",
"Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, and Besmira Nushi. Di-",
"versity of thought improves reasoning abilities of llms, 2024. URL",
"abs/2310.07088",
"Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Za-",
"haria, and Omar Khattab.",
"Optimizing instructions and demonstrations for multi-stage lan-",
"guage model programs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.),",
"ceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
", pp.",
"9340–9366, Miami, Florida, USA, November 2024. Association for Computational Linguis-",
"tics. doi: 10.18653/v1/2024.emnlp-main.525. URL",
"https://aclanthology.org/2024.",
"emnlp-main.525/",
"Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt",
"optimization with “gradient descent” and beam search. In Houda Bouamor, Juan Pino, and Ka-",
"lika Bali (eds.),",
"Proceedings of the 2023 Conference on Empirical Methods in Natural Language",
"Processing",
", pp. 7957–7968, Singapore, December 2023. Association for Computational Linguis-",
"tics. doi: 10.18653/v1/2023.emnlp-main.494. URL",
"https://aclanthology.org/2023.",
"emnlp-main.494/",
"Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-",
"toine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen",
"Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chh-",
"ablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo",
"Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala",
"Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan",
"Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask",
"prompted training enables zero-shot task generalization. In",
"International Conference on Learning",
"Representations (ICLR)",
", 2022. URL",
"https://arxiv.org/abs/2110.08207",
"Jie-Jing Shao, Xiao-Wen Yang, Bo-Wen Zhang, Baizhi Chen, Wen-Da Wei, Guohao Cai, Zhenhua",
"Dong, Lan-Zhe Guo, and Yu feng Li. Chinatravel: A real-world benchmark for language agents",
"in chinese travel planning, 2024. URL",
"https://arxiv.org/abs/2412.13682",
"Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. Nearest neighbor zero-",
"shot inference. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),",
"Proceedings of",
"the 2022 Conference on Empirical Methods in Natural Language Processing",
", pp. 3254–3265,",
"Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguis-",
"tics. doi: 10.18653/v1/2022.emnlp-main.214. URL",
"https://aclanthology.org/2022.",
"emnlp-main.214/",
"Kashun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with",
"chain-of-thought from labeled data. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),",
"Find-",
"ings of the Association for Computational Linguistics: EMNLP 2023",
", pp. 12113–12139, Sin-",
"gapore, December 2023. Association for Computational Linguistics.",
"doi: 10.18653/v1/2023.",
"findings-emnlp.811.",
"https://aclanthology.org/2023.findings-emnlp.",
"811/",
"Xingchen Wan, Ruoxi Sun, Hootan Nakhost, and Sercan Ö. Arı k.",
"Teach better or show",
"smarter? on instructions and exemplars in automatic prompt optimization. In A. Globerson,",
"L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.),",
"Advances in",
"Neural Information Processing Systems",
", volume 37, pp. 58174–58244. Curran Associates, Inc.,",
"https://proceedings.neurips.cc/paper_files/paper/2024/",
"file/6b031defd145b02bed031093d8797bb3-Paper-Conference.pdf",
"Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng",
"Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large lan-",
"guage models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.),",
"Proceedings",
"12",
"of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
"Papers)",
", pp. 2609–2634, Toronto, Canada, July 2023a. Association for Computational Linguis-",
"tics.",
"doi: 10.18653/v1/2023.acl-long.147.",
"acl-long.147/",
"Ming Wang, Yuanzhong Liu, Xiaoyu Liang, Songlian Li, Yijie Huang, Xiaoming Zhang, Sijia Shen,",
"Chaofeng Guan, Daling Wang, Shi Feng, et al. Langgpt: Rethinking structured reusable prompt",
"design framework for llms from the programming language.",
"arXiv preprint arXiv:2402.16929",
"Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P.",
"Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-",
"level prompt optimization, 2023b. URL",
"https://arxiv.org/abs/2310.16427",
"Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-",
"ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,",
"2023c. URL",
"https://arxiv.org/abs/2203.11171",
"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc",
"Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models,",
"2023. URL",
"https://arxiv.org/abs/2201.11903",
"Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong,",
"Chenglin Wu, and Yuyu Luo. Self-supervised prompt optimization, 2025. URL",
"https://",
"arxiv.org/abs/2502.06855",
"Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun",
"Chen. Large language models as optimizers, 2024. URL",
"https://arxiv.org/abs/2309.",
"03409",
"Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik",
"Narasimhan.",
"Tree of thoughts: Deliberate problem solving with large language models.",
"In",
"A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.),",
", volume 36, pp. 11809–11822. Curran Associates, Inc.,",
"2023.",
"https://proceedings.neurips.cc/paper_files/paper/2023/",
"file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf",
"Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin,",
"and James Zou. Optimizing generative ai by backpropagating language model feedback.",
"Nature",
"639:609–616, 2025.",
"Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Pan, and Lidong Bing.",
"Sentiment analysis in",
"the era of large language models:",
"A reality check.",
"In Kevin Duh, Helena Gomez, and",
"Steven Bethard (eds.),",
"Findings of the Association for Computational Linguistics: NAACL 2024",
"pp. 3881–3906, Mexico City, Mexico, June 2024. Association for Computational Linguistics.",
"doi: 10.18653/v1/2024.findings-naacl.246.",
"findings-naacl.246/",
"Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, and Wei Bi. Multi-task instruction",
"tuning of llama for specific scenarios: A preliminary study on writing assistance, 2023. URL",
"https://arxiv.org/abs/2305.13225",
"Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in",
"large language models, 2022. URL",
"https://arxiv.org/abs/2210.03493",
"Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing.",
"Verify-and-edit:",
"A knowledge-enhanced chain-of-thought framework.",
"In Anna Rogers, Jordan Boyd-Graber,",
"and Naoaki Okazaki (eds.),",
"Proceedings of the 61st Annual Meeting of the Association for",
"Computational Linguistics (Volume 1: Long Papers)",
", pp. 5823–5840, Toronto, Canada, July",
"2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.320. URL",
"https://aclanthology.org/2023.acl-long.320/",
"13",
"Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei",
"Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environ-",
"ments, 2025. URL",
"https://arxiv.org/abs/2504.03160",
"Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-",
"mans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables com-",
"plex reasoning in large language models, 2023a. URL",
"https://arxiv.org/abs/2205.",
"10625",
"Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and",
"Jimmy Ba. Large language models are human-level prompt engineers, 2023b. URL",
"//arxiv.org/abs/2211.01910",
"14",
"Large Language Models (LLMs) were used in two ways in this work. First, LLMs served as base",
"models in our experiments on prompt optimization, where we studied how different prompts can",
"elicit their capabilities to solve target tasks. Second, LLMs were employed as auxiliary tools for",
"minor writing support, such as grammar checking and phrasing improvements. Specific details",
"about the LLMs used in our experiments can be found in Appendix B. No LLMs were used to",
"generate substantive ideas, analyses, or content of the paper.",
"Datasets",
"For fair comparison, we followed the datasets and evaluation metrics used in prior base-",
"lines whenever possible. Specifically, we include 4 classic NLP benchmarks (",
"MR",
"Subj",
"CoLA",
") and two widely used question-answering datasets (",
"TREC",
") to validate basic capa-",
"bilities; several domain-specific benchmarks to probe specialized performance, including",
"Financial",
"Sentiment Evaluation",
"dataset (",
"FinFE",
"),",
"Financial PhraseBank",
"FinPB",
"), reasoning related dataset",
"Casual Judgement",
"). Besides, one multi-domain datasets (",
"AG’s News",
") and one natural language",
"generation dataset (",
") are also used to assess overall robustness. To evaluate output quality",
"beyond simple accuracy, we report ROUGE-Avg on",
"and the Matthews correlation coef-",
"ficient (MCC) on",
". To balance computational cost while maximizing coverage, we selected",
"datasets according to a “maximize capability diversity” principle — for example, in addition to the",
"main experiments we ran Qwen2.5-7B-Instruct on",
"to cover several of",
"the categories above. Detailed results are presented in the experimental analysis section.",
"LLMs",
"To demonstrate the adaptability of the proposed method for LLMs, we selected",
"DeepSeek-",
"R1-Distill-Llama-8B",
"Qwen2.5-7B-Instruct",
"from open-source LLMs, as well as",
"GPT-4o-mini",
"from closed-source LLMs, as the base models for our experiments. The experiments on",
"evaluate both the performance of the DeepSeek model itself and, to some ex-",
"tent, the capabilities of the underlying Llama architecture, which is primarily trained on English-",
"language data. Experiments on",
"assess the framework’s performance on a",
"model predominantly trained on Chinese-language data, demonstrating applicability to non-English",
"corpora.",
"was included because it is a widely used closed-source model in prior studies",
"and allows cost-effective experimentation within our budget.",
"Algorithm 1",
"An Overview of",
"Require:",
"A population of prompts",
", size of population",
"N",
", task-related dataset",
", number of",
"epochs",
"m",
", number of iterations",
"n",
", working memory",
"}",
"Ensure:",
"Best prompt",
"Initialization",
":",
"· · ·",
"←∅",
"for",
"epoch = 1 to",
"do",
"step = 1 to",
"Selection",
"| T",
"Evaluation",
"⟨",
"p, p",
", s",
"≥",
"⟩",
"←{",
"end for",
"Top-",
"Return",
"the best prompt",
"15",
"The sampling function used in our framework is roulette wheel selection, denoted as",
"·",
"which is commonly used in the evolution algorithm.",
"refer to the Task-Evolution,",
"Solution-Evolution, Memory-Evolution methods, respectively. Similarly,",
"mean the",
"corresponding Task, Solution, Memory. Based on the components, we designed a task-agnostic",
"template described in Figure 4, through which any kind of LLMs can construct an initial content set",
"of components based on a simple description of the target task input by the user.",
"Figure 4: Task-agnostic template for generating component values corresponding to the given com-",
"ponent types.",
"The following part of the figure is the prompt to generate content for Component",
"\"role\" using the casual judgement task as an example.",
"Table 5: The results on different downstream tasks for Qwen2.5-7B-Instruct.",
"Table 6: Average monetary cost (USD) for one epoch of optimization on GPT-4o-mini.",
"Methods",
"1.17",
"1.31",
"0.97",
"1.52",
"0.57",
"0.56",
"0.61",
"0.79",
"0.83",
"0.64",
"0.74",
"1.23",
"1.27",
"1.08",
"1.30",
"1.10",
"16",
"To ensure that the types of components are as comprehensive and representative as possible, we",
"first surveyed a broad set of related literature (Yuksekgonul et al., 2025; He et al., 2024; Feng et al.,",
"2024; Opsahl-Ong et al., 2024; Diao et al., 2024; Wang et al., 2024; 2023b) and extracted a variety of",
"factors that have been shown to influence the performance of prompts, forming our component pool.",
"We then categorized all components in the pool based on the semantics implied in their original",
"sources, which resulted in five categories: “Role and Expertise”, “Task Content”, “Constraints and",
"Norms”, “Process and Behavior” and “Context and Examples”. From each category, we selected the",
"most representative component as our predefined component types. The complete component pool",
"and its categorization are provided in Table 7.",
"Despite this extensive literature review, we acknowledge that some important aspects may remain",
"uncovered. This observation motivated our design: as more non-AI experts begin to use LLMs,",
"domain specialists should be able to adaptively define new components through our mechanism,",
"thereby supporting both effective task performance and improved interpretability. It is worth noting",
"that for each component type, we can add a “null” option when generating its values, allowing",
"the presence or absence of the component to be controlled and makes the optimized prompts more",
"flexible.",
"Table 7: The categories and types of components in the component pool",
"Figure 5: Template for initializing prompt populations.",
"It is also used in the construction of Prompts",
"Memory, that is, injecting discrete components into the template to obtain a continuous form prompt.",
"The above shows the general form, while the two below provide illustrative examples",
"17",
"Figure 6: Complete prompt template for LLMs (including three parts: instruction, input, and output).",
"Here we also display two practical prompts for AG’s News and Simplification Tasks",
"18",
"Figure 7: The prompts for sub-task I",
"Figure 8: The prompts for sub-task II",
"19",
"Figure 9: The prompts for Sub-solution I - Prompts Memory in",
"discrete",
"form",
"Figure 10: The prompts for Sub-solution I - Prompts Memory in",
"continuous",
"21",
"Figure 11: The prompts for Sub-solution II - Prompts Memory in",
"22",
"Figure 12: The prompts for Sub-solution II - Prompts Memory in",
"23",
"Figure 13: The prompts for Sub-solution II - Prompts Memory in",
"form (extended from",
"Figure 12)",
"24",
"To quickly verify the generalizability of our framework, we conducted multi-turn dialogues with",
"DeepSeek Chat via the web interface provided by DeepSeek (DeepSeek Chat, 2025).",
"Throughout the process, we take simplification task (Zhang et al., 2023) as the example, which",
"allows for easy observation and interpretation of the outputs, and randomly set 4 components. The",
"whole process can be find in Appendix I. For Task-Evolution, we provide two input information (see",
"Figure 14, 16) for the prompt of two sub tasks (see Figure 7, 8). And the corresponding outputs are",
"shown in Figure 15, 17. From the final results, we can derive that under the guidance of direction",
"(i.e., Memory Components), The LLMs could find reasonable direction for evolutionary operator.",
"Accordingly, for Solution-Evolution, we provide four input information (see Figure 18, 20, 22, 24)",
"for four kinds of scenarios. And the corresponding responses are shown in Figure 19, 21, 23, 25,",
"respectively. Based on the observation from the responses, we also find that the procedure designed",
"in this paper can accurately guide the model to deduce corresponding insights and further output",
"reasonable results based on the insights.",
"By the way, this case study provides new users with a quick understanding of our framework. It can",
"also serve as a practical guide, allowing anyone to construct a prototype using the system prompts",
"provided in the case study without writing any code, which can help users optimize domain-specific",
"prompts. We sincerely invite researchers to try it out and share their valuable feedback for further",
"improvement.",
"Figure 14: The input for sub-task I",
"25",
"Figure 15: The responses for sub-task I",
"26",
"Figure 16: The input for sub-task II",
"27",
"Figure 17: The responses for sub-task II",
"28",
"Figure 18: The input for Sub-solution I - Prompts Memory in",
"29",
"Figure 19: The responses for Sub-solution I - Prompts Memory in",
"Figure 20: The input for Sub-solution I - Prompts Memory in",
"31",
"Figure 21: The responses for Sub-solution I - Prompts Memory in",
"32",
"Figure 22: The input for Sub-solution II - Prompts Memory in",
"33",
"Figure 23: The responses for Sub-solution II - Prompts Memory in",
"34",
"Figure 24: The input for Sub-solution II - Prompts Memory in",
"35",
"Figure 25: The responses for Sub-solution II - Prompts Memory in",
"36",
"BSTRACT",
"NTRODUCTION",
"RELIMINARIES",
"ETHODOLOGY",
"XPERIMENTS",
"ELATED",
"ORK",
"ONCLUSION",
"THICS",
"TATEMENT",
"EPRODICIBILITY",
"IMITATIONS",
"EFERENCES",
"SE OF",
"ETAILS OF",
"ATASETS AND",
"SED",
"LGORITHM",
"ETAILS",
"DDITIONAL",
"ETAILED",
"NFORMATION ABOUT",
"OMPONENTS",
"EMPLATE FOR",
"NJECTION",
"ROMPTS FOR",
"VALUATION ON",
"HE",
"ROMPTS OF",
"ASK",
"VOLUTION",
"OLUTION",
"ASE",
"TUDY",
"Corresponding Author",
"DelvePO is available at https://github.com/PasaLab/DelvePO",
"Method",
"Classical NLP",
"Domain-specific",
"Multi-domain",
"Avg.",
"27.33",
"-",
"87.56",
"57.45",
"67.67",
"81.40",
"73.67",
"80.33",
"75.77",
"82.00",
"84.93",
"80.67",
"83.00",
"82.65",
"45.00",
"67.72",
"72.00",
"78.00",
"65.68",
"79.61",
"81.53",
"94.93",
"84.60",
"85.17",
"76.70",
"82.72",
"96.97",
"86.50",
"85.72",
"91.07",
"83.14",
"98.63",
"89.40",
"90.56",
"Memory Modules",
"Causal Judgement",
"w/o Component Memory",
"67.4",
"62.6",
"w/o Prompt Memory",
"29.4",
"61.8",
"w/o both",
"28.4",
"64.6",
"61.3",
"35.3",
"84.7",
"65.7",
"1:",
"2:",
"3:",
"4:",
"5:",
"6:",
"7:",
"8:",
"9:",
"10:",
"11:",
"12:",
"13:",
"14:",
"Categories",
"Related Items",
"Role and Expertise",
"Role; Role description; Scenario; Domain knowledge; Term Clarification",
"Task Content",
"Task description; Instruction; Goal",
"Constraints and Norms",
"Output format; Constraints; Principle; Style; Length; Tone; Priority &",
"Emphasis; Exception handling; Target audience",
"Process and Behavior",
"Workflow; CoT; Action; Skill; Suggestions; Initialization",
"Context and Examples",
"Examples; Reference prompt; Attachment"
],
"tables": [
"|Col1|p<br>Case I|Col3|p#1<br>Case II|\n|---|---|---|---|",
"|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||**③ Solutio**|**③ Solutio**|",
"|Col1|Col2|\n|---|---|\n|||\n|Prompt for**_T_sub**<br>**1**|Prompt for**_T_sub**<br>**1**|",
"|Col1|p <p, s>|\n|---|---|",
"|Evolved Task: T|Col2|\n|---|---|\n|**vove as:evo**|**vove as:evo**|",
"|Col1|Col2|\n|---|---|\n|**Raw Prompt**<br>**Raw Task:****_T_raw**<br>Based on the current prompt(s)<br>**{**_p,..._**}**, **find **a suitable prompt for<br>the**downstream task**.<br>**Raw Solution:****_S_raw**<br>Given**downstream task**, **rewrite**<br>current prompt**{**_p,..._**}** to get a new<br>prompt.|**Raw Prompt**<br>**Raw Task:****_T_raw**<br>Based on the current prompt(s)<br>**{**_p,..._**}**, **find **a suitable prompt for<br>the**downstream task**.<br>**Raw Solution:****_S_raw**<br>Given**downstream task**, **rewrite**<br>current prompt**{**_p,..._**}** to get a new<br>prompt.|",
"|Col1|Col2|\n|---|---|\n|**Eld Slti****_S_**|**Eld Slti****_S_**|\n|**vove ouon:evo**|**vove ouon:evo**|",
"|DelvePO|83.67(1.20)|91.00(1.00)|76.25(1.49)|68.53(2.61)|76.00(2.08)|73.33(3.06)|32.05(0.25)|70.48|\n|---|---|---|---|---|---|---|---|---|",
"|DelvePO|91.07<br>(1.03)|83.14<br>(1.90)|98.63<br>(0.62)|89.40<br>(0.81)|90.56|\n|---|---|---|---|---|---|",
"|reprint|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||||||||||\n||2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|2.5<br>3.0<br>     hours)<br>De|\n||2.5<br>3.0<br>     hours)<br>De||||2.48<br>2.31<br>2.71.|2.48<br>2.31<br>2.71.|2.48<br>2.31<br>2.71.|2.48<br>2.31<br>2.71.|2.48<br>2.31<br>2.71.|2.48<br>2.31<br>2.71.|2.48<br>2.31<br>2.71.|2.48<br>2.31<br>2.71.|2.72||227|227|227|227|2.73<br>~~2.38~~|2.73<br>~~2.38~~|\n||2.5<br>3.0<br>     hours)<br>De|1|1|1|.76|~~1.88~~<br>1.73<br>1.87|~~1.88~~<br>1.73<br>1.87|~~1.88~~<br>1.73<br>1.87|~~1.88~~<br>1.73<br>1.87||||||1.97<br>2.10<br>2.25<br>1.75|1.97<br>2.10<br>2.25<br>1.75|1.97<br>2.10<br>2.25<br>1.75|.|||\n||2.5<br>3.0<br>     hours)<br>De||1.60|1.69|||||||||||||||||\n||2.5<br>3.0<br>     hours)<br>De||||||||||||||||||||\n||2.5<br>3.0<br>     hours)<br>De||||||||||||||||||||",
"|DelvePO|80.07<br>(0.65)|60.00<br>(1.69)|81.40<br>(1.07)|70.77<br>(1.74)|69.97<br>(0.87)|89.27<br>(0.97)|75.25|\n|---|---|---|---|---|---|---|---|",
"|Col1|In Mutate Factors , each element is a content appeared in Prompt 3 .<br>Rules:<br>1. Mutation Requirements:<br>For each markup pair like <component>content</component>, if the component in<br>Mutate Factors , Generate a new one content that:<br>If the component is <role>, the new content must be a noun phrase describing a<br>person.<br>If the component is <task description>, the new content must be a verb phrase<br>_<br>describing a task.<br>Is distinct from the original content.<br>Preserves lexical identity (noun/verb phrase) matching the component.<br>If the original content had the highest score, prioritize generating contents with<br>improved performance potential (e.g., higher efficiency, enhanced properties).<br>Otherwise, the new content may derive from those contents linked to its component<br>in the Memory Prompts (optional but allowed).<br>2. Output Format:<br>Start with <prompt> and end with </prompt>.<br>Only mutate contents within markup pairs specified in Mutate Factors .<br>Preserve all other values outside markup pairs.<br>Replace original contents with mutated ones directly within their components.<br>Prompt 3 : {prompt3}<br>Mutate Factors : {mutate factors}<br>_<br>Step 4 Generate final result by selecting contents from pairs in Prompt 2 and Prompt 4 under<br>identical markup components, referring to Description, adhering to Rules below.<br>Description:<br>Pairs from Prompt 2 and Prompt 4 have identical components (e.g., <role>,<br><task description>).<br>_<br>Rules:<br>1. Selection Criteria:<br>For each tagged pair (e.g., <role>a</role> and <role>b</role>):<br>Use Insights from Step 1 to select one content (a or b) that has higher performance<br>improvement potential (e.g., clarity, specificity, alignment with goals).<br>If the component is <role>, the new content must be a noun phrase describing a<br>person.<br>If the component is <task description>, the new content must be a verb phrase<br>_<br>describing a task.<br>Preserve the lexical identity of the component.<br>Never modify text outside markup pairs.<br>2. Output Format:<br>Start with <prompt> and end with </prompt>.<br>Retain the structure of Prompt 3 but replace tagged pairs with the selected contents.<br>If multiple tagged pairs exist, update all while maintaining non-tagged values verbatim.|Col3|\n|---|---|---|",
"|Col1|Step 1: Conclude Insights from the Memory Components<br>1. In the first tuple, <action>Clarify Sentence Meaning</action> is preferred over <action>Reduce|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n||Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|\n||Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|Step 1: Conclude**Insights** from the**Memory Components**<br>1. In the first tuple,<action>Clarify Sentence Meaning</action>  is preferred over<action>Reduce|<action>Reduce||||\n||||Linguistic Complexity</action>|, and|<requirements>Adapt to Audience</requirements>|<requirements>Adapt to Audience</requirements>|<requirements>Adapt to Audience</requirements>|<requirements>Adapt to Audience</requirements>|<requirements>Adapt to Audience</requirements>|\n|||||||||||\n||||Simplifier</role>|Simplifier</role>|Simplifier</role>|Simplifier</role>|Simplifier</role>|Simplifier</role>|Simplifier</role>|",
"|<role>Sentence|Col2|Col3|\n|---|---|---|",
"|Col1|Col2|Step 1: Conclude Insights from Memory Components<br>From the Memory Components:<br>1. First Element:<br>The pair <action>Clarify Sentence Meaning</action> with <requirements>Adapt to|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n|||**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|\n|||**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|**Step 1: Conclude Insights from Memory Components**<br>From the Memory Components:<br>1.**First Element:**<br>The pair<action>Clarify Sentence Meaning</action>  with<requirements>Adapt to|<requirements>Adapt to||||\n|||||Audience</requirements>|is preferred over|<action>Reduce Linguistic Complexity</action>|<action>Reduce Linguistic Complexity</action>|<action>Reduce Linguistic Complexity</action>|<action>Reduce Linguistic Complexity</action>||\n|||||<requirements>Improve Structure</requirements>|<requirements>Improve Structure</requirements>|<requirements>Improve Structure</requirements>|<requirements>Improve Structure</requirements>|<requirements>Improve Structure</requirements>|<requirements>Improve Structure</requirements>|<requirements>Improve Structure</requirements>|\n|||||<action>Optimize Readability</action>|<action>Optimize Readability</action>|<action>Optimize Readability</action>|<action>Optimize Readability</action>|<action>Optimize Readability</action>|<action>Optimize Readability</action>|<action>Optimize Readability</action>|\n|||||Clarity</action>|Clarity</action>|Clarity</action>|Clarity</action>|Clarity</action>|Clarity</action>|Clarity</action>|\n||||||||||||",
"|<action>Reword for Better|Col2|\n|---|---|",
"|Col1|Step 1 Memory Promptscontinuous:<br>[<br>[<br><prompt>You are a <role>Sentence Simplifier</role>. Given the English Sentence,<br>your task is to <task_description>Clarify the meaning of the given sentences by<br>simplifying them</task_description>. To accomplish this, you need to <action>Optimize<br>Readability</action>. Please make sure to <requirements>Improve Structure</requirements><br>throughout the process.</prompt>,<br>25.0<br>]<br>[<br><prompt>You are a <role>Readability Specialist</role>. Given the English<br>Sentence, your task is to <task_description>Reduce the complexity of the provided<br>sentences</task_description>. To accomplish this, you need to <action>Clarify Sentence<br>Meaning</action>. Please make sure to <requirements>Simplify Vocabulary</requirements><br>throughout the process.</prompt>,<br>21.0<br>]<br>[<br><prompt>You are a <role>Grammar & Clarity Editor</role>. Given the English<br>Sentence, your task is to <task_description>Modify the given sentences to be more<br>straightforward</task_description>. To accomplish this, you need to <action>Reduce<br>Linguistic Complexity</action>. Please make sure to <requirements>Adapt to<br>Audience</requirements> throughout the process.</prompt>,<br>16.0<br>]<br>[<br><prompt>You are a <role>Technical Content Simplifier</role>. Given the English<br>Sentence, your task is to <task_description>Reword the provided sentences for better<br>clarity</task_description>. To accomplish this, you need to <action>Simplify Sentence<br>Structure</action>. Please make sure to <requirements>Ensure Conciseness</requirements><br>throughout the process.</prompt>,<br>14.0<br>]<br>]<br>Step 2 Prompt 1:<br><prompt>You are a <role>Sentence Simplifier</role>. Given the English Sentence, your task<br>is to <task_description>Rewrite the provided sentences in a simpler<br>form</task_description>. To accomplish this, you need to <action>Condense<br>Sentences</action>. Please make sure to <requirements>Adapt to Audience</requirements><br>throughout the process.<br></prompt><br>Mutate Factors:<br><res> role | requirements </res><br>Step 3 Prompt 3:<br><prompt>You are a <role>Linguistic Analyst</role>. Given the English Sentence, your task<br>is to <task_description>Rewrite the provided sentences in a simpler<br>form</task_description>. To accomplish this, you need to <action>Condense<br>Sentences</action>. Please make sure to <requirements>Preserve Meaning</requirements><br>throughout the process.<br></prompt><br>Mutate Factors:<br><res> role | requirements </res>|Col3|\n|---|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2510.18257v1.pdf"
}