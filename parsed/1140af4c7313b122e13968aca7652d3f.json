{
"text": "AutoHint: Automatic Prompt Optimization with Hint\n          Generation\n\n             Hong Sun1, Xue Li1, Yinchuan Xu1, Youkow Homma1, Qi Cao1, Min Wu1, Jian Jiao1 and\n               Denis Charles1\n\n                           1Microsoft, 1045 La Avenida, Mountain View, CA, 94043, USA\n\n\n                               Abstract\n                                    This paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large Language\n                                Models (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks,\n                                     the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework to\n                                        inherit the merits of both zero-shot learning and few-shot learning by incorporating enriched instructions derived from\n                                      input-output demonstrations to optimize original prompt. We refer to the enrichment as the Hint and propose a framework to\n                                      automatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructs\n                                    a LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints and\n                                  adds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on the\n                                 BIG-Bench Instruction Induction dataset for both zero-shot and few-shot prompts, where experiments demonstrate that our\n                               method is able to significantly boost accuracy for multiple tasks.2023                  Keywords\n                                       large language models, automatic prompt optimization, natural language processing, natural language generation\nAug           1. Introduction                                 to sample selection or even sample ordering [6, 7, 5].\n                                                                 The above analysis motivates us to seek an alternative8\n                   Large Language Models (LLM) have shown remarkable  approach by combining merits from both sides, via in-\n                        ability to achieve comparable or even surpass human an-  ducting enriched instructions from input-output demon-\n                    notation quality in various tasks [1], where high-quality   strations and then employing them to refine the original\n                  prompts are the key to activate such abilities. This has   instruction. Throughout this paper, we will refer to these\n                      entailed explorations from many directions with a large  enriched instructions as Hints. Figure 1 (right) shows\n                 body of studies on prompt engineering, including meth-  an example of the hints generated for the Epistemic Rea-[cs.CL]            ods built upon human instinct or domain knowledge [2],  soning task in BIG-Bench Instruction Induction (BBII) [3]\n                    data-driven approaches [3], or prompt optimization via   dataset. The original description for this task is to Deter-\n                     in-context learning [4, 5], etc.                        mine whether one sentence entails the next, as highlighted\n                 LLMs could be prompted by zero-shot or few-shot   in blue. In contrast, the hints generated by our method\n                      learning. In the former, the LLM will be provided general   (highlighted in green) showcase a more elaborate instruc-\n                     instructions for the task at hand, as shown in the exam-  tion, providing a breakdown of explanations for both\n                     ple in the left of Figure 1. By contrast, under a few-shot   entailment and non-entailment cases. As a result, it pro-\n                       setting, LLM is provided with a number of input-output   vides more clear information for LLM to interpret the\n                     pairs as demonstrations followed by an unseen input,  task and proceed with expected actions.\n                 and then is instructed to generate a corresponding out-    Having the enriched hints is good, but having au-\n                      put. Both of these two approaches come with their own   tomatically generated hints is better. That is why we\n                    advantages and disadvantages: the former exhibits better  propose a framework called AutoHint which aims to\n                     generalization but may lack the necessary clarity and   generate hints automatically. Our framework samples\n                         specificity, since the instructions can be vague or provide  from input-output pairs and leverages LLM for dueucing\n                   only general descriptions which is not easy for LLM to   hints accordingly. The sampling is based on wrongly\n                       interpret, whereas the latter one offers more detailed in-  answered samples, as it is fair to assume that the original\n                   formation by providing demonstrations but is sensitive  prompt has already conveyed sufficient information forarXiv:2307.07415v2\n                                                                            the correctly answered ones. Following that, we select a\n                       LLM4AIâ€™23: Workshop on Foundations and Applications in Large-scale\n                      AI Models -Pre-training, Fine-tuning, and Prompt-based Learning, co-  small subset and prompt the LLM to summarize the hints\n                           located with the 29TH ACM SIGKDD CONFERENCE ON KNOWLEDGE   that align best with that subset without including case-\n                   DISCOVERY AND DATA MINING (KDD), August 6-10, 2023, Long   specific information. These hints are incorporated into\n                         Beach, CA, USA                                           the original prompt to generate a refined version. As our\n           $ hosu@microsoft.com (H. Sun); xeli@microsoft.com (X. Li)\n                                  Â© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License method is orthogonal to zero-shot and few-shot settings,\n                                                         Attribution                                                                           4.0 International (CC                                                                 BY 4.0).\n                                                                                CEURWorkshopProceedings      http://ceur-ws.orgISSN 1613-0073                        CEUR Workshop                                                Proceedings                                                       (CEUR-WS.org)        we evaluate it under both settings on BBII dataset, where\n\nFigure 1: An example of the enriched instruction learned by our proposed method, taken from Epistemic Reasoning task in\nBBII dataset, with task description highlighted in blue, instruction in red and enriched hints in green. Left: a standard prompt\nwith very general instruction. Right: the enriched prompt after adding the learned hints.\n\n\n\nwe observe remarkable accuracy improvement.          a generation-scoring-selection workflow in [3]. In con-\n  Our main contributions are three folds:                   trast to existing works, our method enriches the general\n                                                          instructions obtained from [3, 22], via exploiting com-\n      â€¢ We propose AutoHint, an automatic prompt op-                                                  plementary hints with enhanced clarity and specificity\n        timization framework which inherits merits from                                               from input-output demonstrations. Therefore, our pro-\n      both zero-shot and few-shot learning. It adopts                                                   posed method is orthogonal to existing works and can be\n       a residual-sampling-summarize paradigm to en-                                                combined with them to obtain potentially better results.\n      hance robustness to noisy samples.\n      â€¢ We report extensive evaluation results on BBII\n       dataset to demonstrate the effectiveness of our  3. Proposed Method\n      method.\n      â€¢ Our framework explores fully leveraging GPT-  3.1. Overview\n        4â€™s capability for prompt optimization to reduce                                                 Given a task with a training dataset containing ğ‘i.i.d.\n      manual effort.                                                  samples as input-output demonstrations:  ğ’Ÿğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› =\n                                                     {(ğ‘¥1, ğ‘¦1), Â· Â· Â· , (ğ‘¥ğ‘, ğ‘¦ğ‘)}, our method first inferences\n                                                      the predictions with an initial prompt ğ‘ğ‘¡, and then\n2. Related Work                             uses another prompt ğ‘â„to deduce a reason or hint ğ‘Ÿğ‘–\n                                                           for each sample having incorrect predictions. We de-\nSince the explosive arrival of LLMs [8], many works have  note the wrongly predicted samples as ğ’Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™=\nstudied how to boost their capabilities to solve emergent,  {(ğ‘¥1, ğ‘¦1, ğ‘Ÿ1), Â· Â· Â· , (ğ‘¥ğ‘›, ğ‘¦ğ‘›, ğ‘Ÿğ‘›)}, and then sample ğ‘€\ncomplex tasks, among which in-context learning and  samples from ğ’Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™to form a  subset ğ’®  =\nChain-of-Thought[9] have proved to be effective two of  {(ğ‘¥1, ğ‘¦1, ğ‘Ÿ1), Â· Â· Â· , (ğ‘¥ğ‘€, ğ‘¦ğ‘€, ğ‘Ÿğ‘€)}. After that, we lever-\nthe most effective methods. Along this line of research,  age a prompt ğ‘ğ‘ to summarize all the individual ğ‘Ÿğ‘–in ğ’®\ndifferent directions have been explored [10, 2, 11, 12, 13,  into a final hint ğ‘Ÿâ€². Finally, a new prompt is generated\n14, 15]. Nevertheless, in most tasks the most effective  by merging learned ğ‘Ÿâ€² into ğ‘ğ‘¡to form prompt ğ‘ğ‘¡+1. The\nprompts are still human-crafted, which greatly limits the   overall procedure is summarized in Algorithm 1.\napplication of LLM prompting.\n  To fill this gap, there has been a growing interest in\n                                                    3.2. Inference and Get Residual Dataautomatic prompt engineering, with most prior attempts\nrequiring access to the internal variables of LLMs on   Figure 2(a) shows the template used to inference predic-\ndifferent levels, either through the differentiable tuning   tion ğ‘¦Ë†ğ‘–for each sampled input data from ğ’Ÿğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›:\nof soft prompts [16, 17] or training auxiliary components\nor models for prompt optimization[18, 19, 12, 4, 20, 21].                 ğ‘ğ¿ğ¿ğ‘€(ğ‘¦Ë†ğ‘–|(ğ‘¥ğ‘–), ğ‘ğ‘¡)                      (1)\nHowever, with the growing trend of limited accessibility\n                                              where ğ‘ğ‘¡is the initial prompt constructed by concate-\nto LLMs, such methods are becoming less feasible for\n                                                       nating a very general task description with the sampled\ngeneral practitioners.\n                                                            input.\n  For that reason, recent works are witnessing a notice-\n                                            Upon obtaining the inference results, we will only\nable shift towards approaches that solely rely on feed-\n                                                          retain incorrectly predicted samples i.e., ğ’Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™, for\nback from LLMs [3, 22, 5, 7]. More specifically, [22] in-\n                                                         further processing. The reasons for this are twofold.\nstructs LLMs to deduce the task based on input-output\n                                                                    First, proceeding with all training data would result in\ndemonstrations, and this idea is further extended into\n                                                            significant costs. Second, we believe that the current\n\nFigure 2: (a): Prompt template for inferencing output as in Section 3.2. (b): Prompt template for hints generation as in\nSection 3.3.\n\n\nAlgorithm 1 AutoHint for Prompt Optimization\nRequire: Training set ğ’Ÿğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›= (ğ‘¥ğ‘–, ğ‘¦ğ‘–) with ğ‘–âˆˆ[1, ğ‘], validation set ğ’Ÿğ‘£ğ‘ğ‘™, test set ğ’Ÿğ‘¡ğ‘’ğ‘ ğ‘¡, ğ‘â„, ğ‘ğ‘ , an initial\n   prompt ğ‘.\n  1: Initialize: ğ‘0 = ğ‘, ğ’Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™= âˆ…\n  2: for ğ‘¡= 0, Â· Â· Â· , ğ‘‡do\n  3:    ğ‘¦Ë†ğ‘–= ğ‘ğ¿ğ¿ğ‘€(ğ‘¥ğ‘–, ğ‘ğ‘¡) for ğ‘–âˆˆ[1, ğ‘]\n  4:    ğ’Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™= {(ğ‘¥ğ‘–, ğ‘¦ğ‘–, ğ‘¦Ë†ğ‘–)} ğ‘“ğ‘œğ‘Ÿğ‘¦Ë†ğ‘–Ì¸= ğ‘¦ğ‘–\n  5:    ğ‘Ÿğ‘–â‡ğ‘ğ¿ğ¿ğ‘€(ğ‘Ÿğ‘–|(ğ‘¥ğ‘–, ğ‘¦ğ‘–), ğ‘â„) for (ğ‘¥ğ‘–, ğ‘¦ğ‘–) âˆˆğ’Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™Section 3.3\n  6:    ğ‘†â‡sampling(ğ’Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™) Section 3.4\n  7:     ğ‘Ÿâ€² â‡ğ‘ğ¿ğ¿ğ‘€((ğ‘¥ğ‘–, ğ‘¦ğ‘–, ğ‘Ÿğ‘–), ğ‘ğ‘ ) Section 3.5\n  8:    Generate ğ‘ğ‘¡+1 based on ğ‘ğ‘¡and ğ‘Ÿâ€²\n  9: end for\n\n\n\nprompt already provides sufficient information for the    Our framework makes no assumptions on specific clus-\nLLM to make accurate decisions for correctly predicted   tering methods for Clustering strategy, and we employ\ndata. Therefore, we only send the error cases to subse-  K-Means in our implementation to obtain the cluster\nquent steps, aiming to generate new information that can  index ğ‘ğ‘–for sample ğ‘–based on its feature vector ğ‘£ğ‘–for\nassist in rectifying the current decision.            âˆ€ğ‘–âˆˆğ‘†, due to its simplicity:\n\n                                         ğ‘£ğ‘–= ğœ†ğ‘¥ğ‘“(ğ‘¥ğ‘–) + ğœ†ğ‘¦ğ‘“(ğ‘¦ğ‘–) + ğœ†ğ‘Ÿğ‘“(ğ‘Ÿğ‘–),           (3)\n3.3. Hints Generation\n                                               where ğ‘“(Â·) denotes the encoder that transforms the tex-After having the residual dataset, another prompt is used\n                                                            tual input ğ‘¥ğ‘–, ğ‘¦ğ‘–and ğ‘Ÿğ‘–into dense vectors, with ğœ†ğ‘¥, ğœ†ğ‘¦to generate ğ‘Ÿğ‘–for each input-output pair:\n                                               and ğœ†ğ‘Ÿdenoting the combination weights. We use the\n        ğ‘Ÿğ‘–â‡ğ‘ğ¿ğ¿ğ‘€(ğ‘Ÿğ‘–|(ğ‘¥ğ‘–, ğ‘¦ğ‘–), ğ‘â„)                (2)  BERT-base model as our encoder ğ‘“(Â·), but other encoders\n                                                     could be easily integrated into our framework.\nwhere ğ‘â„is a template prompt constructed by concatenat-\n                                              As will be discussed in Section 4, we use a validation\ning the general task description and the corresponding\n                                                             set to examine different sampling strategies and select\ninput-output pair, as shown in Figure 2(b).\n                                                      the best performing one, and then report its evaluation\n                                                              results on the holdout test set.\n3.4. Sampling Strategies\n\nWe explore various sampling strategies to generate ğ‘†,  3.5. Hints Summarization\nincluding\n                                                 Given the hints generated from Section 3.3, we leverage\n      â€¢ Random:  randomly draw ğ¾samples from  another prompt ğ‘ğ‘ shown in Figure 3 to generate a sum-\n       ğ’Ÿğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™.                                    marized hint to add to the initial prompt ğ‘ğ‘¡, resulting in\n      â€¢ Random-balanced: randomly draw ğ¾samples  an enriched prompt ğ‘ğ‘¡+1. An example of the enriched\n      from each category (applies to classification tasks  prompt with the summarized hints is shown in Figure 1\n        only).                                                    (right).\n      â€¢  Clustering:    first  cluster samples based on    The idea of summarizing individual hints here is con-\n        ğ‘¥ğ‘–, ğ‘¦ğ‘–, ğ‘Ÿğ‘–, and then randomly draw ğ¾samples   ceptually analogous to the calculation involved in de-\n      from each cluster.                                riving a mini-batch stochastic gradient descent (SGD),\n\nmetrics. Balanced Accuracy is averaged per-category\n                                                         accuracy.\n                                               Implementation details We use the Azure OpenAI\n                                                API service (GPT-4) to evaluate both the baseline and our\n                                                  method. For the inference and summarization prompts,\n                                       we set temperature as 0 and topP as 1. For hint genera-\n                                                                 tion, we set temperature as 0.1 and topP as 0.95 to allow\n                                            some exploration. We conduct hyper-parameter tuning\n                                                         to examine different sampling strategies. For each task,\nFigure 3: Prompt template for summarizing hints.        we test 20 sets of hyper-parameters on the validation set\n                                                and select the best one to conduct evaluation on the test\n                                                                    set. To save costs, we randomly sample a subset from\n                                                      the validation set for comparison if it is too large, then\nwhere individual gradients within the mini-batch are ag-\n                                                keep the top three settings to run a full validation set\ngregated into a single gradient. However, compared to\n                                                       evaluation and select the final prompt for the test set\nits numerical counterpart, summarizing hints is much\n                                                          evaluation.\nmore difficult, since the former operates in a continuous\nspace with well-established numerical operations, which\nis clearly not the case for hints summarization. In this  4.2. Experiment Results and Analysis\npaper, we propose to transform this non-trivial task into                                            Our main results are summarized in Table 1, where we\na tractable problem by leveraging the ability of LLMs to                                                    observe significant improvement on both accuracy and\nsolve emergent tasks. To our knowledge, this is the first                                                      balanced accuracy on 5 out of the 6 tasks after adding the\nattempt to leverage LLMs to summarize hints or prompts                                                         hints mined from the proposed method, demonstrating\nin automatic prompt generation to make the result more                                                        the effectiveness of our proposed framework under zero-\nstable and comprehensive.                                                       shot settings.\n                                       We also look into the generated hints and observe that\n4. Experiments                                   for most tasks, our method is able to mine useful defini-\n                                                          tions and explanations with more detailed information\n                                                                after the very first iteration. For the example in Figure 1,4.1. Experimental Setup\n                                                             practical instructions are added for both the entailment\nDataset We examine our method on 6 tasks from the BIG-  and non-entailment segments, assisting annotators in-\nBench Instruction Induction (BBII)1 dataset introduced   cluding LLMs to better understand the task at hand.\nin [3], including Epistemic Reasoning, Logical Fallacy De-    Regarding sampling strategies, Clustering achieves the\ntection, Implicatures, Hyperbaton, Causal Judgment and   best result in 3 out of 6 tasks, Random-balanced leads\nWinowhy. We specifically select these tasks based on   to the best result on the remaining tasks, while plain\ntheir relatively large data volume and easy-to-parse an-  Random sampling shows the worst results. This high-\nswer format, which allows us to obtain higher accuracy.   lights the importance of adopting proper sampling strate-\nFor each task, we randomly split the dataset into train-  gies for generating a good summary. Meanwhile, the\ning (60%), validation (20%) and test sets (20%), where the  remarkable performance of Random-balanced sampling\ntraining set is used for inducing the hints as mentioned   also echos our finding that our method is able to induce\nin Section 3, while the validation set is used for hyper-   explicit per-category explanations as shown in Figure 1.\nparameter tuning and prompt selection. After choosing   In addition, the best results are achieved when no more\nthe best enriched prompt based on the validation set  than 3 samples are used per label. This may be because\nperformance, we report the final evaluations on the test  more samples confuse the model when generating the\nset.                                             summary.\n  Baselines and Metrics Since our proposed frame-    Meanwhile, as our method is orthogonal to few-short\nwork aims to exploit useful information on top of given   learning, we also evaluate AutoHint in this setting, and\nprompts, we employ the initial prompt ğ‘ğ‘¡as our base-  the results are summarized in Table 2, where our method\nline, which is the prompt retrieved from BBII dataset   boosts accuracy significantly in 5 out of 6 tasks. After\nin our implementation. Given all the tasks are binary-  adding demonstrations randomly sampled from the train-\nclassification tasks, we report Overall Accuracy (Acc.)  ing set, we observe a decrease in accuracy on 4 out of 6\nand Balanced Accuracy (Bal Acc.) as our main evaluation   tasks, along with notable performance fluctuation across\n                                                             different sampling seeds. This observation aligns with\n                                                     previous studies [6, 7] highlighting the importance of1https://github.com/keirp/automatic_prompt_engineer/tree/main/\n data/bigbench-ii                                            carefully selecting effective demonstrations to mitigate\n\nTable 1\nZero-Shot Prompt Evaluation Results\n\n   Task                                    Testset                  Baseline                   Treatment\n                                   Count         Acc.           Bal Acc.        Acc.           Bal Acc.\n\n   Epistemic Reasoning                  499             82.5            82.9           90.5          90.15\n    Logical Fallacy Detection              700            77.46           76.18          84.03         83.66\n   Implicatures                         100            88.89           88.66          91.93         91.92\n   Hyperbaton                         11k            66.81           66.81          82.41         82.41\n   Causal Judgment                     40              60.53           60.53          65.79         65.79\n   Winowhy                           570            74.17           73.24           73.12           71.78\n\n\nTable 2\nFew-Shot Prompt Evaluation Results\n\n     Task                                                        Baseline                   Treatment\n                                                         Acc.           Bal Acc.        Acc.           Bal Acc.\n\n     Epistemic Reasoning                                 76.5            80.87          83.75         85.96\n     Logical Fallacy Detection                            86.76           86.64           84.64           84.28\n     Implicatures                                        88.89           88.73          92.93         92.89\n    Hyperbaton                                         61.88           61.88          80.41         80.41\n     Causal Judgment                                   55.26           55.26          73.68         73.68\n    Winowhy                                           71.55           70.43          74.69         74.04\n\n\n\nthe sensitivity towards samples.                      and hypothesis are identical or include additional details.\n                                                While it is beneficial to infuse new, correct information,\n                                                                                it places higher demands on the ability of LLM to com-4.3. More Iterations\n                                                 prehend these diverse hints. Our current approach to\nIn addition, we also conduct experiments by running ad-  incorporating hints from later iterations is simplistic,\nditional iterations to generate more informative prompts.  and as a result, it hinders the language model from effec-\nNew hints are appended to existing ones by a prefix And   tively assimilating the more informative instructions. As\nadditional hint might be useful is. Overall, we observe a  a result, it requires a more effective strategy for merging\nsignificant boost on the average accuracy over prompt   information from different iterations. Therefore, we plan\ncandidates on the validation set, this indicates our im-  to address this as part of our future work.\nprovement on the original prompt is solid. However, after\nselecting the best prompt, only 3 tasks (Causal Judge-\n                                                    4.4. Cost Analysis\nment, Logical Fallacy Detection and Hyperbaton) show\nfurther improvement while the rest show accuracy de-  There are 4 calls to LLMs in each iteration: 1) inference,\ncay. When manually checking the new hints, we find   2) hints generation, 3) summarization and 4) prompt se-\nthat they indeed present complementary or more specific   lection. Among these, the first call is not unique to our\ninformation to existing ones. For instance, new hints  method therefore it does not incur additional cost. Given\nfor the Epistemic Reasoning task focus on if the premise   the cost for the third call is negligible, we only need to\n\n\n\nTable 3\nAccuracy comparison after second iteration (zero-shot)\n\n     Task                                                              First iteration              Second iteration\n                                                         Acc.           Bal Acc.        Acc.           Bal Acc.\n\n     Epistemic Reasoning                              90.5          90.15           88.2            88.2\n     Logical Fallacy Detection                            84.03           83.66          87.68        87\n     Implicatures                                        91.93           91.92          92.92         91.91\n    Hyperbaton                                         82.41           82.41          90.56         90.56\n     Causal Judgment                                   65.79           65.79          71.05         71.05\n    Winowhy                                           73.12           71.78           73.1            71.8\n\noptimize costs for steps 2 and 4. For step 2, we can sample    [7] W. Xu, A. Banburski-Fahey, N. Jojic, Reprompt-\na subset first as the final data points needed to obtain the         ing: Automated chain-of-thought prompt infer-\nsummary is small. For the last step, we can evaluate on a       ence through gibbs sampling,   arXiv preprint\nsubset if the validation set is large, or similar to APE [3],        arXiv:2305.09993 (2023).\niteratively prune the prompts.                               [8]  L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wain-\n                                                               wright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\n                                                        A. Ray, et al., Training language models to follow\n5. Conclusion                                         instructions with human feedback, Advances in\n                                                       Neural Information Processing Systems 35 (2022)\nWe propose a novel framework for automatic prompt\n                                                          27730â€“27744.\noptimization by combining merits from zero-shot and\n                                                              [9]  J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia,\nfew-shot learning. We evaluate our method on the BBII\n                                                                     E. H. Chi, Q. V. Le, D. Zhou, et al., Chain-of-thought\ndataset under both settings, and demonstrate the effec-\n                                                       prompting elicits reasoning in large language mod-\ntiveness of the proposed method. Our proposed method\n                                                                                 els, in: Advances in Neural Information Processing\ncould also be combined with existing methods to achieve\n                                                           Systems, 2022.\npotentially better results. While we didnâ€™t deploy this\n                                                          [10] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen,\nmethod in our production system yet, we are able to\n                                                      H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,\nleverage this method for reducing some manual analysis\n                                                                R. Nakano, et al., Training verifiers to solve math\neffort in prompt optimization. There are more discus-\n                                                word problems, arXiv preprint arXiv:2110.14168\nsions along this work like more-representative sample\n                                                                    (2021).\nselection, merging information from different iterations\n                                                          [11]  T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, Y. Iwasawa,\nto generate better prompt and cost saving, which we\n                                                         Large language models are zero-shot reasoners, in:\nleave them for future work.\n                                                    Advances in Neural Information Processing Sys-\n                                                            tems, 2022.\nReferences                                        [12]  Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith,\n                                                          D. Khashabi, H. Hajishirzi, Self-instruct: Aligning\n [1]  F. Gilardi, M. Alizadeh, M. Kubli,  Chatgpt out-       language model with self generated instructions,\n     performs crowd-workers for text-annotation tasks,       arXiv preprint arXiv:2212.10560 (2022).\n     arXiv preprint arXiv:2303.15056 (2023).              [13]  E. Zelikman, Y. Wu, J. Mu, N. Goodman, Star: Boot-\n [2]  T. Wu, M. Terry, C. J. Cai, Ai chains: Transparent        strapping reasoning with reasoning, in: Advances\n     and controllable human-ai interaction by chaining        in Neural Information Processing Systems, ????\n      large language model prompts,  in: Proceedings   [14] D. Zhou, N. SchÃ¤rli, L. Hou,  J. Wei, N. Scales,\n      of the 2022 CHI Conference on Human Factors in        X. Wang, D. Schuurmans, O. Bousquet, Q. Le, E. Chi,\n    Computing Systems, 2022, pp. 1â€“22.                     Least-to-most prompting enables complex reason-\n [3]  Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis,       ing in large language models,   arXiv preprint\n     H. Chan, J. Ba, Large language models are human-       arXiv:2205.10625 (2022).\n      level prompt engineers,  in: The Eleventh Inter-  [15]  P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu,\n     national Conference on Learning Representations,        T. Rajpurohit, P. Clark, A. Kalyan,  Dynamic\n      2023.                                          prompt learning via policy gradient for semi-\n [4]  T. Zhang, X. Wang, D. Zhou, D. Schuurmans, J. E.        structured mathematical reasoning, arXiv preprint\n     Gonzalez, Tempera: Test-time prompt editing via        arXiv:2209.14610 (2022).\n     reinforcement learning,  in: The Eleventh Inter-  [16] G. Qin, J. Eisner, Learning how to ask: Querying\n     national Conference on Learning Representations,       lms with mixtures of soft prompts,  in: Proceed-\n      2023.                                                    ings of the 2021 Conference of the North American\n [5] R. Pryzant, D.  Iter,  J.  Li,  Y. T. Lee, C. Zhu,       Chapter of the Association for Computational Lin-\n    M. Zeng, Automatic prompt optimization with\"         guistics: Human Language Technologies, 2021, pp.\n     gradient descent\" and beam search, arXiv preprint       5203â€“5212.\n     arXiv:2305.03495 (2023).                             [17] B. Lester, R. Al-Rfou, N. Constant, The power of\n [6]  Y. Lu, M. Bartolo, A. Moore, S. Riedel, P. Stenetorp,        scale for parameter-efficient prompt tuning,  in:\n      Fantastically ordered prompts and where to find       Proceedings of the 2021 Conference on Empirical\n     them: Overcoming few-shot prompt order sensitiv-      Methods in Natural Language Processing, 2021, pp.\n       ity,  in: Proceedings of the 60th Annual Meeting       3045â€“3059.\n      of the Association for Computational Linguistics   [18] M. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo,\n     (Volume 1: Long Papers), 2022.                             T. Shu, M. Song, E. P. Xing, Z. Hu, Rlprompt: Op-\n                                                              timizing discrete text prompts with reinforcement\n\nlearning, arXiv preprint arXiv:2205.12548 (2022).\n[19]  Y. Hao, Z. Chi, L. Dong, F. Wei, Optimizing prompts\n      for text-to-image generation,   arXiv preprint\n     arXiv:2212.09611 (2022).\n[20]  T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace,\n      S. Singh, Autoprompt: Eliciting knowledge from\n     language models with automatically generated\n     prompts, arXiv preprint arXiv:2010.15980 (2020).\n[21]  J. Long,  Large language model guided tree-of-\n     thought, arXiv preprint arXiv:2305.08291 (2023).\n[22] O. Honovich, U. Shaham, S. R. Bowman, O. Levy,\n     Instruction induction: From few examples to nat-\n     ural language task descriptions,  arXiv preprint\n     arXiv:2205.10782 (2022).",
"headers": [
"arXiv:2307.07415v2  [cs.CL]  8 Aug 2023",
"AutoHint: Automatic Prompt Optimization with Hint",
"Generation",
"1. Introduction",
"3. Proposed Method",
"2. Related Work",
"4. Experiments",
"5. Conclusion",
"References",
"Hong",
"Sun",
",",
"Xue",
"Li",
"Yinchuan",
"Xu",
"Youkow",
"Homma",
"Qi",
"Cao",
"Min",
"Wu",
"Jian",
"Jiao",
"and",
"Denis",
"Charles",
"3.1. Overview",
"3.2. Inference and Get Residual Data",
"3.3. Hints Generation",
"3.4. Sampling Strategies",
"3.5. Hints Summarization",
"4.2. Experiment Results and Analysis",
"4.1. Experimental Setup",
"4.3. More Iterations",
"4.4. Cost Analysis"
],
"tables": [
"|Col1|Col2|\n|---|---|\n|CEUR<br>Workshop<br>Proceedings<br>http://ceur-ws.org<br>ISSN 1613-0073|CEUR<br>Workshop<br>Proceedings<br>http://ceur-ws.org<br>ISSN 1613-0073|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2307.07415v2.pdf"
}