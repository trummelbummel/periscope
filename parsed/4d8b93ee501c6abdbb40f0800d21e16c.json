{
"text": "COURSEGPT-ZH: AN EDUCATIONAL LARGE LANGUAGE MODEL\n         BASED ON KNOWLEDGE DISTILLATION INCORPORATING\n                    PROMPT OPTIMIZATION ∗\n\n\n\n\n                                Zheyan Qu, Lu Yin, Zitong Yu, Wenbo Wang, Xing zhang∗\n                Wireless Signal Processing and Network Laboratory, Beijing University of Posts and Telecommunications, Beijing\n                                    Department of Computer Science, University of Aberdeen, UK\n                                                        zhangx@ieee.org2024\nMay                                     ABSTRACT\n8\n                     Large language models (LLMs) have demonstrated astonishing capabilities in natural language\n                      processing (NLP) tasks, sparking interest in their application to professional domains with higher\n                        specialized requirements. However, restricted access to closed-source LLMs via APIs and the diffi-\n                         culty in collecting massive high-quality datasets pose obstacles to the development of large language\n                    models in education fields of various courses. Given these challenges, we propose CourseGPT-zh,\n                     a course-oriented education LLM that supports customization and low-cost deployment. To ad-[cs.CL]                 dress the comprehensiveness and diversity requirements of course-specific corpora, we design a\n                        high-quality question-answering corpus distillation framework incorporating prompt optimization,\n                    which effectively mines textbook knowledge and enhances its diversity. Moreover, considering the\n                      alignment of LLM responses with user needs, a novel method for discrete prompt optimization based\n                   on LLM-as-Judge is introduced. During optimization, this framework leverages the LLM’s ability\n                         to reflect on and exploit error feedback and patterns, allowing for prompts that meet user needs and\n                        preferences while saving response length. Lastly, we obtain CourseGPT-zh based on the open-source\n              LLM using parameter-efficient fine-tuning. Experimental results show that our discrete prompt\n                       optimization framework effectively improves the response quality of ChatGPT, and CourseGPT-zh\n                         exhibits strong professional capabilities in specialized knowledge question-answering, significantly\n                      outperforming comparable open-source models.\n\n\n          1  Introduction\n\n              Large language models, such as ChatGPT [1], GPT4 [2], LLaMA [3], and ChatGLM [4], have demonstrated remarkablearXiv:2405.04781v1       performance and generalization capabilities across various NLP tasks, significantly expanding the boundaries of\n              language applications. With the increase in model parameters and pretraining corpus size, capabilities such as logical\n               reasoning, instruction following, and In-Context Learning [5],[6],[7] have emerged. Based on these breakthroughs, the\n                 latest LLMs have shown profound understanding and professionalism in various fields, such as virtual assistants, text\n               generation, and code annotation. Utilizing LLMs to disrupt industries has become an inevitable trend, including the\n                 field of education[8],[9].\n\n               Recently, there has been a desire to leverage the extensive knowledge of large language models to construct domain-\n                specific LLMs in various vertical fields, which require greater expertise and accuracy. To address the issue that\n              general-purpose LLMs cannot meet specific domain requirements, a variety of methods have been proposed. For\n               instance, steering foundation models through role-playing or prompt engineering have been used to tap into the\n            knowledge learned during the pre-training phase, which can unleash their deep-seated expert capabilities [10],[11].\n             Other approaches involve pretraining or continual pre-training with domain-specific corpus to incorporate domain-\n                specific knowledge into large language models [8],[12],[13],[14]. In addition, to reduce the hallucination during the\n              response generation, retrieval augmentation has also been applied to provide reliable references [8],[15]. Based on these\n\n                ∗Xing zhang is the corresponding author.\n\napproaches, successful implementations such as MedAgents [10], ChatLaw [15], EduChat [8], and FinGPT [16] have\ndemonstrated the potential of LLMs to provide professional responses and insights in various vertical fields, including\nhealthcare, law, finance, and education.\n\nHowever, constructing domain-specific large language models is still labor-consuming and expensive. To begin with, for\nclosed-source large language models like ChatGPT, the high costs of text generation and fine-tuning services are often\nprohibitive. As for open-source LLMs, there is a significant gap in parameter size and pre-training corpus compared\nto closed-source LLMs, resulting in significantly weaker general capabilities such as reasoning, and domain-specific\nknowledge extraction [9],[17],[18],[19]. Faced with complex professional terminology, open-source large language\nmodels often fail to meet user requirements for domain knowledge. In this context, it often requires a large amount of\nin-domain pre-training corpus or expertise datasets to enhance professionalism in vertical fields.\n\nAlthough various existing works have developed specialized datasets and evaluation criteria for various fields such\nas philosophy, medicine, and law, as well as for scenarios including network operation and geospatial semantics\n[17],[18],[19],[20],[21], there is still a considerable demand for manual effort in constructing datasets for courses or\nprivatized scenarios that are not covered by these datasets. This challenge is particularly pronounced when accessible\ncorpora in the field are scarce, making it extremely difficult to construct tens of thousands of specialized instruction data.\nFurthermore, the majority of models are primarily pre-trained on English corpora, which may lead to a degradation in\ntheir performance in other languages [22],[23].\n\nIn addition to the challenges of constructing specialized corpora, the high cost of inference incurred by open-source\nlarge language models cannot be overlooked. Compared to the concise responses provided by humans, the responses\ngenerated by large language models, while more comprehensive, also include a significant amount of redundant\ninformation, resulting in unnecessary inference overhead. Typically, to further align the responses of large language\nmodels with specific preferences, methods such as RLHF (Reinforcement Learning from Human Feedback)[24]\nare introduced for fine-tuning models. However, this approach still requires a substantial amount of human-labeled\npreference data. Consequently, promoting alignment between the responses and human preferences, as well as reducing\ninference costs, is also a key factor in fostering the widespread adoption of open-source large models in specialized\nvertical domains.\n\nTargeted at these issues, we propose CourseGPT-zh, an open-source education large language model, and design a\npipeline for constructing high-quality question-answer pairs through mining textbook knowledge. By utilizing the\nconstructed diverse question-answer pairs, we perform parameter-efficient fine-tuning on the open-source model to\nmitigate the resource constraints required for deployment. In addition, in the data construction process, we incorporate\nLLM-as-Judge and utilize discrete prompt optimization to generate optimal prompts, steering ChatGPT to produce\nhigh-quality training data aligned with human preferences. Through this method, we ensure high-quality responses\nwhile reducing the deployment costs associated with response length.\n\nOur main contributions can be summarized as:\n\n\n        • In this paper, we propose CourseGPT-zh, an open-source education large language model, with a pipeline\n         for constructing high-quality and diverse question-answer pairs. Based on textbooks, we guide the model to\n        conduct thorough exploration and questioning of textbooks, extracting knowledge from both closed-source\n         large language models and specialized texts. Additionally, we employ a method inspired by self-instruct to\n        guide the large language models in generating related questions, further enhancing the diversity.\n\n\n        • Considering that although large language models can generate comprehensive answers, some content may be\n        redundant or incorrect. Therefore, we employ prompt engineering to guide ChatGPT in generating responses\n          that align with human preferences. To obtain the optimal prompts, we have designed an iterative discrete\n       prompt optimization framework, which incorporates LLM-as-Judge to facilitate automatic evaluation of the\n         quality of responses guided by prompts. Furthermore, the optimized prompt allows the large language model\n         to achieve a balance between the quality of responses and their length, achieving information compression in\n         responses.\n\n\n        • A parameter-efficient fine-tuning method of the ChatGLM3 model is conducted based on constructed high-\n         quality question-answering data, resulting in the CourseGPT-zh. Experimental evidence has shown that\n       CourseGPT-zh exhibits improved alignment with human responses, and delivers more concise answers\n        while maintaining a high level of response quality. On various NLP task evaluation metrics, CourseGPT-zh\n          significantly outperforms other open-source large models.\n\n\n                                               2\n\n2  Related-work\n\nWith fierce competition and rapid development, large language models ranging from billions to trillions of parameters\nhave achieved remarkable performance across various NLP tasks after being pre-trained on massive amounts of text.\nRepresented by LLMs such as ChatGPT, GPT4, and GPT4-Turbo, the OpenAI model family has successively reset\nthe benchmarks for NLP tasks, being regarded as one of the greatest inventions in history. Concurrently, a multitude\nof open-source large language models, including llama-2-13b, ChatGLM3-6b, and Mistral-8x7B-MoE[25], have also\nshown astonishing improvements, even surpassing the level of ChatGPT on some dimensions. More importantly, they\ncan be deployed on a single to several GPUs and can be flexibly customized through fine-tuning.\n\n\n2.1  Domain-specific LLMs\n\nAlthough general-purpose large language models have achieved exceptional performance on generic NLP tasks, they\noften fall short in vertical domains that necessitate extensive specialized knowledge and high accuracy requirements.\nThe performance of zero-shot large language models in these domains is typically inadequate, thereby granting domain-\nspecific LLMs significant attention. Closed-source large language models, while exhibiting superior performance across\nvarious capabilities, present challenges for continual pre-training and fine-tuning with private corpora. Therefore, the\nconstruction of domain-specific models based on closed-source LLMs frequently leverages role-playing or collaboration\nabilities to extract knowledge in the specialized field during the pre-training phase. In contrast, open-source LLMs can\nbe further pre-trained or fine-tuned with extensive high-quality domain-specific data, and they have achieved multiple\nsuccessful applications in fields such as medicine, law, education, finance, etc.\n\nHuatuoGPT [26] employs a mixed dataset comprising distilled data from ChatGPT and real-world data provided\nby physicians’ medical advice to fine-tune an open-source model. Furthermore, it aligns the model’s response with\nhuman preferences through RLAIF (Reinforcement Learning from Artificial Intelligence Feedback). By learning\nfrom the response styles of real-world doctor-patient interactions, the fine-tuned model can engage with users in a\nhuman-like manner and significantly surpasses other models at a similar level across various metrics. MedChatZH\n[12] has developed a dialogue model specifically designed for Traditional Chinese Medicine, incorporating extensive\nChinese medical literature for continual pre-training. After fine-tuning millions of question-answer data from the\nInternet and various Chinese hospitals, the model achieves state-of-the-art performance in the field of Chinese medicine.\nChatLaw [15], targeting the legal domain, not only provides professional responses concerning legal knowledge but also\nacquires problem-solving abilities through training on multiple-choice question data. Furthermore, it employs a method\ncombining vector database retrieval with keyword search, effectively reducing the hallucination in responses. EduChat\n[8] offers a range of functionalities, including open-ended question answering, paper assessment, and Socratic teaching,\nenhancing various skills through fine-tuning and the integration of tools. The model gains interdisciplinary knowledge\nthrough continual pre-training and strengthens its question-answering and instruction-following capabilities with\nlarge-scale instruction and open-domain dialogue datasets. FinGPT [16] adopts a data-centric approach, focusing on\nautomated data management pipelines and lightweight adaptive technologies, establishing a comprehensive framework\nfrom data processing to feature engineering and application, while also enhancing the transparency of the overall\nframework. One of its strengths lies in its ability to integrate seamlessly with both open-source and closed-source large\nlanguage models without the need for further training.\n\n\n2.2  Discrete prompt engineering\n\nPrompt engineering aims to guide large language models to fully leverage their potential through the meticulous\ndesign of prompts. Extensive research has demonstrated that well-crafted prompts can significantly enhance the ability\nof large language models to improve their performance across various NLP tasks [27],[28]. Prompt engineering\nencompasses continuous prompt learning and discrete prompt optimization. Continuous prompt learning aims to adapt\nlarge language models to various tasks by incorporating learnable parameters within the prompts [29], [30]. However,\ncontinuous prompt learning typically requires access to the gradient vectors of the LLMs, which restricts its application\nin closed-source models that are accessed only through APIs. For discrete prompts, traditional methods often rely on\nmeticulous manual design, which not only demands considerable human effort but also may not necessarily maximize\nthe model’s performance. Consequently, numerous methods for automatically generating optimal discrete prompts have\nbeen explored, leveraging the large model itself as an optimizer to autonomously enhance its performance in NLP tasks.\n\nRecently, several leading automated discrete prompt optimization frameworks have been proposed. EVOPROMPT[31]\ndraws on the principles of evolutionary algorithms (EAs) to iteratively guide LLMs to generate new prompts through\nevolutionary operators. It does not require any gradient information from LLMs and can achieve a balance between\nexploration and exploitation. Experiments on nine datasets have shown that optimized prompts can significantly\nimprove task performance. APE[32], inspired by program synthesis, represents discrete prompting optimization as\n\n\n                                               3\n\nLLM-as-Judge\n\n                                   Factual           User\n                                                                                          Reflection                               Accuracy        Satisfaction\n\n                                     Clarity                                              Condensa-                      Resample            Role-playing                                                                  bility\n                                             Model                                                                          Prompts                                  Questions                   Questions                                                                                                                                Model\n                               Sampled                  Generated       Optimized Prompts              Question-\n                          By ChatGPT             By ChatGPT                                 Answering Pairs              Chat                                                                                                    Pre-trained\n                                                                                                             Filtering\n\n                                  Questions                    Questions            References                Training Data                              Sampled                   Generated                                                                                                                                                                                                                                                                                                                                                                                              Course-oriented                                                                                                    Open-source\n                           By GLM4                By GLM4\n\n                                                            LLM Model\n\n                                Question         Question         Paragraphs\n                              Generation        Sample\n                                                                          Textbook\n\n                                     Figure 1: CourseGPT-zh Framework\n\n\na black-box optimization problem. It treats instructions as \"programs\" and optimizes them by searching through the\ncandidate instruction pool proposed by LLMs. Furthermore, it employs an iterative Monte Carlo search to further\nenhance prompt performance. OPRO[33] utilizes LLMs to generate new candidate prompts from previously generated\nresults and their scores and then evaluates the new candidate prompts for the next iteration. PROMPTAGENT[34]\napproaches it as a strategic planning problem, using Monte Carlo tree search to achieve a balance between exploration\nand exploitation. Unlike other discrete prompt optimization frameworks, it also leverages learning capabilities based\non error summarization of large language models, introducing expert-level domain knowledge and guidance based\non reflection. The optimal prompts obtained from these prompting optimization frameworks have achieved results\nsignificantly better than manually crafted prompts on various NLP tasks, including GSM8K and Big-Bench Hard tasks.\n\n3  Data Construction\n\nLarge language models often require at least tens of thousands of high-quality instruction-tuning data to demonstrate\nsatisfactory performance; however, the collection and processing of such data can be prohibitively labor-intensive.\nUnlike fields such as medicine, which benefit from a wealth of open-source question-answering datasets gathered from\nthe internet and hospital medical databases, amassing large volumes of high-quality question-answering datasets poses\nsignificant challenges. In light of these barriers to data construction and the demand for low-cost model development, we\npropose a pipeline based on knowledge distillation from ChatGPT and GLM-4, which ensures the comprehensiveness\nand diversity of questions, as well as the professionalism and alignment of the distilled responses. The entire process\nconsists of two components: question construction and response generation.\n\n3.1  Question Generation\n\nTo ensure that the fine-tuned model can provide professional answers to users’ questions about various knowledge both\ninside and outside the textbooks, the comprehensiveness and diversity of the questions are of great importance. Merely\nrelying on human-generated questions as a seed pool and using methods like self-instruct [35] to generate question data\nmay not cover all knowledge points comprehensively, especially for the understanding of various professional terms in\nspecialized fields. In addition, the diversity of questioning methods for the same set of knowledge points also needs\nto be addressed. Diverse questioning and answering of the same knowledge can guide the model to learn the internal\nrelationships of knowledge more effectively.\n\nIn response to these challenges, we have developed a diversified question generation pipeline, as depicted in the orange\nsection of Figure 2. Initially, knowledge extraction and questioning are based on textbook paragraphs to ensure the\ncomprehensiveness of the questions. The textbook is divided into paragraphs, which are sequentially input into a large\nlanguage model to guide the generation of a list of questions targeting specific knowledge points. Concurrently, during\nthe process, 6 questions from a seed pool comprising 50 carefully chosen human-written questions, along with two\ngenerated questions are randomly selected as in-context question examples. This approach steers the model towards\ngenerating diverse questions for a group of knowledge points, such as interpretive questions, pros and cons comparison\nquestions, and comparative questions. Lastly, in terms of question quantity, we force the large language model to\n\n\n                                               4\n\nQuestion\n                                  Seed pool\n                                                     pool\n\n\n                                                                LLM +\n                    Textbook\n                                          Few-shots   LLM                       Question       Role-playingprompts\n                                                                 Questions                                     Answers\n                    Paragraphs          Paragraph                             Paragraph\n\n\n\n\n\n                                                                 Repeat   LLM +\n                                                                                 Role-playing                       Seed pool          Few-shots\n                                    LLM                 prompts\n                                                                 Questions                   Answers\n                        Question\n                                           Questions\n                           pool\n\n\n                                    Figure 2: Data Construction Framework\n\n\n\n\n\ngenerate an excess number of questions for the limited length of paragraphs. This serves to distill the knowledge from\nthe large model and enhance the diversity of the questions.\n\nHowever, the question lists constructed based on textbook paragraphs are still limited in form and content by the\ntextbook. To further enhance the diversity, inspired by self-instruct, we employ an iterative approach to distill new\nquestions from a large model. Specifically, a set of questions generated in the previous round is selected as content\nexamples, guiding the large language model to sample and generate new content-related questions. At the same time,\n3 questions are randomly selected from the seed pool as style examples to enrich the diversity of question forms.\nAfter generating a new set of questions, it is then used as content examples for the next iteration. In this way, we can\nreduce the influence of referencing textbook paragraphs, and a large number of new questions can be obtained through\ndistillation.\n\nFinally, we find that different large language models exhibit significant variation in the style and distribution of the\nquestions they generate. Consequently, we employ both ChatGPT and GLM-4 for concurrent question generation and\nsampling, and subsequently deduplicated the final question list. As CourseGPT-zh is an adaptable course-specific large\nlanguage model, we selected Communication Principles as the focus of our experiment, which encompasses specialized\ndomains such as signal modulation, quantization, and coding, requiring the model to provide professional and precise\nresponses. Acknowledging the limitations of open-source models in mathematical derivation capabilities such as\nintegration, our focus is directed towards the learning of conceptual knowledge in this field. Utilizing the textbook\non Communication Principles, we separately generate approximately 10k questions using ChatGPT and GLM-4 and\nconduct two rounds of iterative sampling based on these questions, resulting in about 20k sampled questions.\n\n\n\n\n3.2  Answer Generation\n\n\nAfter obtaining a diverse list of questions, the next challenge is to utilize large language models to generate professional\nanswers that meet users’ preferences and needs. Generally, LLMs tend to produce comprehensive and well-structured\nanswers that are reader-friendly. However, these lengthy responses not only significantly differ from the style of human\nreplies but also substantially increase response latency and reasoning. Therefore, it is necessary to align the response\nstyle of LLMs with that of humans and enhance the accuracy and professionalism of the responses. As shown in the\nblue part of the Figure 2, we leverage role-playing prompts to guide the large language model in generating responses\nthat meet the requirements. Taking advantage of role-playing capabilities, the large language model can focus on\nprofessional fields and generate more reliable responses. For the same group of tasks, different prompts can lead to\nsignificant differences in model performance, which is also the case in the question-answering field. It is worth noting\nthat for questions derived from textbook paragraphs, we refer to the original text to improve the accuracy of the answers.\nThe optimization of prompts will be introduced in the next section.\n\n\n                                               5\n\n4  Discrete Prompt Optimization\n\nResearch indicates that the design of prompts significantly affects the performance of large language models on NLP\ntasks [27],[28].  Prior studies have emphasized the design of prompts at various stages, yet the prompts deemed\noptimal by humans may not necessarily elicit the desired performance. Therefore, optimizing the prompts is essential,\nparticularly during the process of knowledge distillation in specialized domains, which can prompt the large language\nmodels to generate more professional responses. In our discrete prompt optimization framework, we further incorporate\nLLM-as-Judge to evaluate the alignment between the responses guided by prompts and human responses. By training\non the distillation data guided by optimal prompts, the fine-tuned large language model obviates the need for further\noptimization using Reinforcement Learning from Human Feedback (RLHF).\n\n\n4.1  LLM-as-Judge\n\nAlignment is a crucial step that ensures the model’s performance aligns with user intentions and human preferences.\nGenerally, Reinforced Learning from AI Feedback (RLAIF) is extensively applied in alignment tasks. However, reward\nmodels can be challenging to train and are susceptible to the influence of human errors in the training data. Moreover, in\ncontrast to the approach of first training with mixed data and then aligning, we aim to maintain alignment between LLM\nresponses and human responses starting from the construction of the training data. This method avoids potential issues\nsuch as convergence difficulties and reward hacking in reinforcement learning models [36],[37]. Furthermore, recent\nresearch has started to introduce large models, such as ChatGPT, as judges for alignment evaluation. By comparing\nLLM-generated responses with human responses across multiple dimensions, LLMs have demonstrated a high degree\nof consistency with human judgments [38],[39]. More importantly, it requires only a subset of validation samples\nto effectively reflect the overall situation and can quickly adjust the evaluation dimensions according to user needs,\nsignificantly reducing costs.\n\nIn this section, we adopt the prompt design of the judge in AlignBench [39], taking into account Factual Accuracy,\nUser Satisfaction, Clarity, and Condensability. We separately score each evaluation dimension using CoT to obtain\nthe final score, thereby enhancing the transparency and credibility of the evaluation process and improving alignment\nperformance. Factual Accuracy considers the accuracy of information in the LLM response and whether it aligns\nwith the facts presented in human responses. User Satisfaction evaluates whether the LLM response adequately and\nappropriately meets the user’s query needs in comparison to human responses. Clarity aims to assess whether the LLM\nresponse is concise and clear in comparison to human responses, enhancing user readability. Lastly, Condensability\nevaluates whether the LLM response is succinct and refined, considering the potential redundancy in the LLM response.\nFurthermore, by collecting evaluation results, we can understand the strengths and weaknesses of the current prompts\nacross various dimensions, providing instructive feedback for further prompt improvement. Based on this feedback, we\ncan further refine the prompts using reflection [40].\n\n\n4.2  Discrete Prompt Optimization Framework\n\nThe application of knowledge distillation using large language models such as ChatGPT has been demonstrated to\neffectively train student models, thereby enhancing their performance across various NLP tasks [26], [41]. However, the\nquality of responses generated by ChatGPT heavily depends on the design of the prompts, which has been overlooked in\nprevious work within specialized domains. Different prompts significantly affect the style, accuracy, and professionalism\nof the responses. Considering the refined language and precise factual responses typical of human replies, we aim to\nfine-tune the model using distillation data guided by optimal prompts. This will enable the fine-tuned model to enhance\nthe information density of its responses while meeting user requirements, thereby avoiding unnecessary output and\nimproving generation speed.\n\nIn this case, we have developed a discrete prompt optimization framework based on reflection [40] and LLM-as-Judge.\nIn this framework, we task LLM-as-Judge with scoring candidate prompts on a randomly sampled set of 50 human-\ndrafted question-answer pairs to assess the quality. The evaluation feedback provided by LLM-as-Judge offers a clear\ndirection for the improvement of the candidate prompts. Consequently, we integrate the feedback with Reflection to\nguide the learning process of the LLM, thereby enhancing the quality of the prompts. It is noteworthy that LLM-as-Judge\ninitially provides separate evaluations for each dimension and concludes with a comprehensive evaluation and scoring.\nTo reduce the overhead associated with the reflection operation, we randomly collect 5 comprehensive evaluation results\nas feedback for each candidate prompt, which serve as the basis for subsequent improvements.\n\nHowever, in this process, we found that although utilizing evaluation results to make improvements could enhance\nthe performance of the prompts and make them more aligned with the evaluator’s criteria, it simultaneously tends to\ngenerate overly lengthy prompts and encourages ChatGPT to produce longer responses to achieve higher scores. This\n\n\n                                               6\n\nLLM-as-Judge                                                              Top 5 Prompts        Resample&\n                                                                                                            Reflection\n                                                     Factual Accuracy\n                                                                                              Prompts-\n                                 Candidate       User Satisfaction             Scores              resample                   Seed pool\n                                 prompts\n                                                            Clarity\n                                                                                              Prompts-\n                                                                       Feedback\n                                                                                                                   reflection                                                      Condensability\n\n\n\n                                                        Validation set\n\n\n                              Figure 3: Discrete Prompt Optimization Framework\n\n\n\n\nmight lead to deviations from human-like response styles and increase the cost of inference. To address this issue, we\ntook the following measures: Firstly, in addition to the LLM scores, we introduced a length penalty factor by referencing\nthe length of human responses to more reasonably balance the comprehensiveness and length of the responses. Secondly,\nduring the prompt optimization process, besides improving the prompts with feedback, we incorporated a Resample\nmodule. Specifically, after calculating the scores of candidate prompts using LLM-as-Judge, we select the top five\nhighest-scoring prompts sorted in descending order. These sorted prompt-score pairs are then used as input to guide the\nlarge language model to automatically discover the key information within the prompts and the patterns related to the\nscores, thereby resampling an equivalent number of prompts. This approach allows for the retention of key information\nwithin the prompts while also achieving their conciseness.\n\nAs illustrated in Figure 3, the proposed optimization framework for discrete prompts is presented. Initially, a set of\nten manually crafted prompts are employed as the preliminary candidates, and their respective scores and evaluation\noutcomes are obtained using LLM-as-Judge. The significance of these initial prompts lies in their capacity to inject the\npreliminary requirements into the optimization framework for further refinement. It is noteworthy that role-playing is\nincorporated into the initial prompts to guide the large model to concentrate on professional domains, thereby generating\nmore specialized responses.\nIn terms of scoring, the calculation method is as described by Equation 1, where sLLMi     represents the comprehensive\nscore given by LLM-as-Judge, lres denotes the length of the response generated by the LLM, and lref indicates the\nlength of the reference human response. If the LLM-generated response is shorter than the reference length, no penalty\nis imposed, and the evaluation is solely based on the LLM-as-Judge’s assessment across various dimensions. Conversely,\nif the LLM-generated response exceeds the reference length, a penalty is applied to the additional length, with the\ndegree of penalty determined by the parameter α.\n\n\n\n                               sLLMi                        lres ≤lref\n                                       si =                    lres                                                        (1)\n                               sLLMi   −α( lref −1)   lres > lref\n\n\n\n\nAfter obtaining the scores and feedback for each candidate prompt, we iteratively generated the next generation of\ncandidate prompts by combining reflection and resampling modules. Initially, we selected the top 5 prompts with\nthe highest scores for subsequent operations. The reflection method effectively utilizes the evaluation results from\nLLM-as-Judge to refine the prompts, ensuring that the generated prompts better meet the requirements of multiple\ndimensions. Furthermore, by incorporating the resampling module, the LLM can identify patterns and key components,\nsampling a new generation of prompts. During optimization, the top 5 prompts might include candidates generated\nfrom the two aforementioned modules in the previous iteration. In such cases, the resampling module can leverage the\nkey information from the prompts optimized by both modules.\n\nIn the experiment, we set the parameter α to 0.5 and conducted three iterations, and we selected the prompt term with\nthe highest score on the validation set as the optimal prompts.\n\n\n                                               7\n\n5  Experiment and Analysis\n\n5.1  Model Training\n\nWe conducted fine-tuning based on ChatGLM3-6B as the foundational architecture. ChatGLM3-6B is a pre-trained\nlarge language model tailored for Chinese, which possesses excellent features such as conversational fluency and low\ndeployment threshold. In the instruction fine-tuning, we employed the LoRA strategy [42], with the rank textbfk set to\n64 and textbfalpha set to 128. Additionally, the learning rate, batch size, and maximum context length were set to 1e-4,\n32, and 2048, respectively. Regarding training facilities, we distributed the model across 8 A40 GPUs using Pytorch to\naccelerate the training process.\n\n5.2  Benchmarks\n\nWe constructed a test dataset consisting of 200 QA pairs derived from examination papers and web pages in the field\nof communication principles, encompassing various chapters within this domain to evaluate the model’s proficiency\nin various knowledge. In our tests, we selected several open-source and closed-source models that support Chinese,\nincluding Qianfan-llama2-7b-chinese, Qianfan-llama2-13b-chinese, chatglm3-6B, ERNIE-Bot-turbo [43], and ChatGPT,\nfor comparative purposes.\n\nAmong them, Qianfan-llama2-7b-chinese and Qianfan-llama2-13b-chinese are versions based on Llama-2 that have\nundergone enhanced pre-training with large-scale Chinese-English corpora and fine-tuning for instruction following,\nwhich has improved their performance in Chinese-English question-answering. ChatGLM3-6B is a bilingual dialogue\nlanguage model released by Zhipu AI and the KEG Laboratory of Tsinghua University, demonstrating optimal\nperformance among models below 10B parameters in semantics, mathematics, reasoning, and coding. ERNIE-Bot-\nturbo, developed by Baidu, encompasses a vast amount of Chinese data, exhibiting impressive capabilities in Chinese\nquestion-answering and Chinese content generation. Lastly, ChatGPT is recognized as one of the most advanced models.\nTo verify the effectiveness of prompt optimization, we also conducted a comparative analysis of the response quality of\nGPT-3.5-turbo under the guidance of optimal prompts.\n\n5.3  Evaluation Metrics\n\nWe employed the BLEU[44], GLEU[45], and ROUGE[46] metrics to evaluate the similarity between the responses\ngenerated by LMM and the reference human responses. BLEU is an accuracy-based evaluation method that assesses\nthe similarity between LLM responses and reference responses through the overlap precision of n-grams. GLEU further\ntakes into account factors such as lexical overlap and order, reflecting the fluency and naturalness of sentences. Unlike\nBLEU, ROUGE primarily focuses on the recall of n-grams, based on the comprehensiveness and coverage of the LLM\nresponses. Finally, ROUGE-L is based on the calculation of the longest common subsequence of matches.\n\nHowever, these metrics are solely based on n-grams matching and cannot evaluate the alignment of semantics between\nLLM responses and reference human responses. Therefore, we also incorporated LLM-as-Judge to assess the quality\nof responses in terms of factual accuracy, user engagement, clarity, and conciseness. Additionally, we recorded the\nLLM-as-Judge scores, length penalty scores, and comprehensive scores to accurately and comprehensively reflect the\nquality of the responses.\n\n5.4  CourseGPT-zh\n\nAll models were scored on traditional Natural Language Processing (NLP) metrics as detailed in Table 1. For each\ncomparative model, we utilized the officially provided APIs or checkpoint models to obtain responses on the test set. As\ndepicted in the table, closed-source models such as ERNIE-Bot-turbo and ChatGPT exhibit significant advantages over\nopen-source models due to their larger parameter counts and the benefits of pre-trained corpora. Additionally, compared\nto the performance of ChatGPT without the use of additional prompts, the application of optimal prompts led to effective\nimprovements across all metrics for ChatGPT, particularly enhancing the fluency of responses by approximately 17%.\nThis validates the efficacy of the discrete prompt optimization framework we proposed. Surprisingly, the fine-tuned\nChatGLM3 surpassed ChatGPT on all metrics, especially achieving a higher recall rate in the ROUGE metric. This\nmay be attributed to its training on the data distilled under the guidance of optimal prompts.\n\nHowever, these traditional metrics have clear limitations as they only focus on the calculation of n-gram-related\naccuracy or recall rates, lacking the ability to evaluate complex semantic information. Therefore, we continued to use\nLLM-as-Judge to evaluate the response quality of each model, as shown in Table 2. Firstly, by comparing the results of\nthe open-source models, it can be observed that in the field of domain-specific question-answering, simply increasing the\nnumber of model parameters does not result in significant progress. Instead, training with specialized and high-quality\n\n\n                                               8\n\nModel                  BLEU-1  BLEU-2  BLEU-3  BLEU-4  GLEU  ROUGE-1  ROUGE-2  ROUGE-L\n  Qianfan-llama2-7b-chinese    0.157      0.073      0.037      0.018      0.056    0.254       0.059       0.193\n  Qianfan-llama2-13b-chinese   0.176      0.081      0.040      0.021      0.060    0.258       0.062       0.190\n ChatGLM3-6B               0.166      0.075      0.039      0.020      0.056    0.245       0.053       0.180\n ERNIE-Bot-turbo            0.180      0.083      0.041      0.021      0.060    0.247       0.057       0.178\n ChatGPT                    0.229      0.108      0.055      0.028      0.077    0.274       0.068       0.202\n ChatGPT-prompt             0.254      0.117      0.059      0.030      0.090    0.300       0.072       0.222\n CourseGPT-zh               0.253      0.120      0.063      0.033      0.088    0.297       0.076       0.218\n                                Table 1: Benchmark on QA dataset.(zero-shot)\n\n\n\ncorpora is necessary. The superior performance of ChatGLM3-6b among open-source models may be related to its\nbroader Chinese pretraining corpus. Furthermore, by comparing the results of open-source and closed-source models, it\ncan be seen that although both tend to generate long responses to meet user needs, there are significant differences in\nresponse quality. The model with the highest comprehensive score is ChatGPT, while ERNIE-Bot-turbo has the highest\nresponse quality, albeit with excessively long responses.\n\nIn comparison to the responses generated by ChatGPT under the guidance of optimal prompts, the reduction in length\npenalty is more than threefold, while achieving nearly identical response quality. This indicates that under the guidance\nof optimal prompts, the density of effective information in its responses has significantly increased, taking into account\nmultiple dimensions such as accuracy and responsiveness to user needs. Furthermore, the response quality of the\nfine-tuned ChatGLM3 model significantly surpasses that of various open-source models, and due to its refined responses,\nit has obtained the second-highest overall score. Compared to the ChatGLM3-6B model without fine-tuning, it achieves\nbetter response quality while reducing the length penalty by 63%. This demonstrates that fine-tuning with specialized\ndistillation corpora in a specific style can significantly enhance the response quality of open-source models.\n\nIn comparison to the vanilla ChatGPT, the ChatGPT guided by optimal prompts exhibited a reduction in length penalty\nby more than threefold, while maintaining nearly equivalent response quality. This demonstrates that, under the\nguidance of optimal prompts, there is a significant enhancement in the density of effective information, satisfying\nmultidimensional requirements such as accuracy and responsiveness to user needs.  Furthermore, the fine-tuned\nChatGLM3-6B model significantly outperformed various open-source models in terms of response quality, and it\nachieved the second-highest comprehensive score due to its refined answers. This indicates that fine-tuning with\nspecialized distillation corpora of a specific style can markedly improve the response quality of open-source models.\n\nIt is worth noting that the LLM-as-Judge is tested based on ChatGPT. With the evolvement of ChatGPT, the evaluation\nscores might change in the future, so it is necessary to pay attention to the relative scores.\n\n           Model                     Comprehensive Score  LLM-as-Judge   Length Penalty\n             Qianfan-llama2-7b-chinese    4.64                   5.54            0.90\n             Qianfan-llama2-13b-chinese   4.92                   5.84            0.92\n          ChatGLM3-6B               5.28                   6.21            0.93\n            ERNIE-Bot-turbo            5.72                   6.75            1.03\n          ChatGPT                    6.14                   6.69            0.55\n           ChatGPT-prompt             6.48                   6.65            0.17\n           CourseGPT-zh               6.21                   6.55            0.34\n             Table 2: The model scores on 200 single-turn questions, using LLM-as-Judge (ChatGPT)\n\n\n\n5.5  Discrete prompt optimization framework\n\nAs shown in Table 3, three types of prompts were collected during the optimization process. The first prompt with\nthe highest comprehensive score was selected as the optimal prompts for subsequent data construction. This prompt\nutilized role-playing and extracted the most critical semantic components from the optimization experience, achieving a\nbalance between answer quality and length. The second prompt was generated through the reflection module, which\neffectively integrated the experience of feedback. However, this prompt was overly comprehensive, resulting in the best\nanswer quality but at the cost of excessively long responses. Compared to the non-optimized prompt, its length penalty\nincreased by 0.3. The last prompt, although capable of generating brief responses, failed to guide the generation of\nhigh-quality content. This demonstrates that under nearly the same response length, the design of the prompt has a\nsignificant impact on the quality of the response. Additionally, during the prompt optimization process, the quality of\nthe prompts did not consistently improve but experienced certain fluctuations.\n\n\n                                               9\n\nPrompts                                        Comprehensive   LLM-as-Judge    Length\n                                                         score                               Penalty\n 你是一位通信工程领域的专家，你以简洁明了的方   6.48               6.65               0.17\n 式提供准确无误的回答。你的回答简洁明了、准确无\n 误，避免冗长和繁琐。\n As an expert in the field of telecommunications engineering,\n you provide concise and unambiguous responses that are\n  error-free. Your answers are succinct and precise, devoid of\n  superfluity and complexity.\n 你是一位通信工程领域的教授，专注于通信原理领   6.37               7.23               0.85\n 域。以深厚的知识和清晰的表达著称。你的回答结构\n 化、简明扼要、易于理解、准确无误、全面清晰。你\n 提供准确信息，确保回答满足用户需求。\n As a professor in the field of telecommunications engi-\n  neering, specializing in communication principles, you are\n renowned for your profound knowledge and clear articula-\n  tion. Your responses are structured, concise, and easy to\n  understand, while also being accurate, comprehensive, and\n  clear. You provide accurate information to ensure that your\n answers meet the user’s needs.\n 请用简洁明了的语言回答以下问题，确保回答准确无   5.98               6.17               0.19\n 误、全面清晰。请核实事实，满足用户需求，并尽量\n 使用通俗易懂的语言，简化句子结构，提高回答的凝\n 炼性。\n Please answer the following questions with concise and\n  clear language, ensuring that the answers are accurate, com-\n  prehensive, and lucid. Verify the facts, meet the user’s needs,\n and strive to use easily understandable language. Simplify\n  the sentence structure to enhance the conciseness of the\n  responses.\n\n                          Table 3: Sample prompts from discrete prompt optimization.\n\n\n\n5.6  Case study\n\nQuestion: What are the impacts of inter-symbol interference?\n\nAs shown in Table 4, we present case studies of responses from different models to the same question. Firstly, ChatGPT\nguided by optimal prompts provided a refined response and was able to correctly answer the question, covering aspects\nsuch as bit error rate and communication quality. However, ChatGLM3-6B explained the concept of inter-symbol\ninterference but failed to correctly respond to the question. Finally, CourseGPT-zh, with a similarly refined style,\nprovided explanations regarding bit error rate, system capacity, and modulation performance, correctly addressing the\nuser’s inquiry.\n\n\n6  Conclusion and Future work\n\n\nIn this paper, we introduce CourseGPT-zh, an open-source education large language model for professional courses, and\nthe corresponding question-answering data construction framework. Based on this framework, we have implemented\na communication principles specialized chatbot. Unlike previous work, we have focused on the comprehensiveness\nand diversity of questions, as well as the alignment of question responses with human needs. Furthermore, we\nhave integrated discrete prompt optimization to enhance response quality, with LLM-as-judge for automatic multi-\ndimensional evaluation. Finally, we have trained CourseGPT-zh using the parameter-efficient fine-tuning method\nLoRA. Experimental results show that CourseGPT-zh exhibits impressive performance in question-answering for the\ncorresponding course topics.\n\n\n                                              10\n\nModel/References   Response\nReferences      1.错误解码:码间串扰可能导致接收端错误地解码接收到的信号...... 导致数据传输错误或降低解\n         调的准确性。\n           2.误码率增加:码间串扰增加了误码率. . . . . .导致信号质量下降。\n           3.降低系统容量:码间串扰眼制了数字通信系统的容量...... 系统无法传输高速数据或提供更高的\n         数据传输速率。\n          4.降低信号质量:码间串扰引入了额外的噪声和失真. . . . . .影响通信的可靠性和性能。\n                        1. Error Decoding: Inter-symbol interference may lead to the receiver incorrectly decoding the received\n                          signal... resulting in data transmission errors or reduced demodulation accuracy.\n                         2. Increased Bit Error Rate: Inter-symbol interference increases the bit error rate... leading to a decrease in\n                       signal quality.\n                         3. Reduced System Capacity: Inter-symbol interference constrains the capacity of digital communication\n                       systems... the system is unable to transmit high-speed data or provide higher data transmission rates.\n                         4. Degraded Signal Quality: Inter-symbol interference introduces additional noise and distortion... affecting\n                      the reliability and performance of communication.\nChatGPT-prompt  码间串扰是指不同码片序列之间相互干扰的现象，在通信系统中会产生以下影响:\n          1.码误差:码间串扰会号致接收端无法正确解码接收到的信号. . . . . .降低通信系统的性能。\n           2.信号衰减:码间串扰会引起信号的衰减. . . . . .导致通信质量下降。\n           3.互调干扰:码间串扰还可能引起互调干扰. . . . . .影响系统的动态范围和信号的有效传输。\n           4.报文错误率增加:码间串扰会增加报文的错误率...... 码间串扰对通信系统的性能和质量都会产\n         生不利的影响。\n                     Inter-symbol interference refers to the phenomenon of interference between different chip sequences,\n                   which has the following effects in communication systems:\n                         1. Code Error: Inter-symbol interference can cause the receiver to be unable to decode the received signal\n                       correctly ... thus degrading the performance of the communication system.\n                        2. Signal Attenuation: Inter-symbol interference can cause signal attenuation ... leading to a decrease in\n                   communication quality.\n                         3. Intermodulation Interference: Inter-symbol interference may also cause intermodulation interference ...\n                       affecting the dynamic range of the system and the effective transmission of signals.\n                        4. Increased Message Error Rate: Inter-symbol interference can increase the error rate of messages ...\n                      inter-symbol interference has an adverse effect on the performance and quality of communication systems.\nChatGLM3-6B   码间串扰是指两个或多个信号在同一信道中传输时,由于信号之间的相互作用而产生的相互干\n                    扰...... 码间串扰的影响因素包括:\n             1.信道条件: ......当信道条件较差时, 码间串扰会更严重。\n             2.信号特征: ......信号的幅度和相位特征可能会导致码间串扰的产生。\n                3.信道编码:...... 也能够通过解码恢复出原始信号。\n                4.信号干扰:...... 可以采用一些技术, 例如信道编码、自适应滤波器、多址技术等。\n                     Inter-symbol interference refers to the mutual interference that occurs between two or more signals trans-\n                      mitted over the same channel due to their interactions...The factors influencing inter-symbol interference\n                       include:\n                        1. Channel conditions: ... inter-symbol interference is more severe when channel conditions are poor.\n                         2. Signal characteristics: ... the amplitude and phase characteristics of the signal may lead to the generation\n                      of inter-symbol interference.\n                        3. Channel coding: ... and can also recover the original signal through decoding.\n                        4. Signal interference:... some techniques, such as channel coding, adaptive filters, and multiple access\n                      technologies, can be employed to address this issue.\nCourseGPT-zh   码间串扰会产生以下影响:\n          1.增加误码率:码间串扰会引入额外的噪声. . . . . .从而增加误码率。\n          2.降低系统容量:码间串扰会限制不同信号之间的距离. . . . . .从而降低系统的容量。\n          3.影响调制性能:码间串扰会改交信号的频谐特性. . . . . . 影响系统的传输质量。\n          4.增加信号功率:为了抵消码间串扰引起的误码率增加. . . . . .增加了功耗和成本。\n          因此......需要采取合适的调制方案、均衡技术和干扰抑制方法来提高系统的性能。\n                     Inter-symbol interference (ISI) results in the following effects:\n                        1. Increased bit error rate: ISI introduces additional noise ..., thereby increasing the bit error rate.\n                        2. Reduced system capacity: ISI constrains the distance between different signals ..., thus decreasing the\n                    system capacity.\n                        3. Impacted modulation performance: ISI alters the frequency characteristics of the signals ..., affecting\n                      the transmission quality of the system.\n                         4. Increased signal power: In order to counteract the increased bit error rate caused by ISI ..., signal power\n                   and cost are increased.\n                    Therefore ..., appropriate modulation schemes, equalization techniques, and interference suppression\n                   methods are required to enhance system performance.\n\n                               Table 4: Sample responses from three models.\n\n                                             11\n\nHowever, due to the limitations of the model’s parameter size, particularly its pre-trained knowledge and reasoning\ncapabilities, further work is needed to expand and enhance the performance of CourseGPT-zh. Future directions for\ndevelopment are outlined below.\n\nReducing Hallucinations   For specialized courses, the accuracy of responses is crucial for the user experience.\nHowever, even the most advanced models, such as GPT4, struggle with the issue of hallucinations, which is more\npronounced in 6B parameter models. Furthermore, the performance of the 6B model in answering questions about\nspecific professional knowledge is constrained by the limitations of its pre-trained corpus. To address these issues,\nwe plan to construct a knowledge base of specialized knowledge based on textbooks and encyclopedias in the future\nand utilize Retrieval Augmented Generation (RAG) technology to provide references for response generation. This\napproach can reduce the model’s hallucination problems and increase the accuracy of responses. Moreover, to address\nthe potential changes in response distribution after incorporating references, we can use prompt optimization and joint\nadjustment with RAG to achieve the desired response style.\n\nHigh-quality Professional Knowledge Base   CourseGPT-zh conducts knowledge extraction and question-answer\npair construction based on a high-quality professional knowledge base. The richness and comprehensiveness of the\nprofessional knowledge base have a significant impact on the quality of the constructed data. Moreover, the most\nup-to-date knowledge base can prompt large models to generate the latest professional content. However, in the current\nwork, data construction is only based on the main reference books of the corresponding courses. In the future, more\nfield-related professional knowledge bases will be introduced to further improve the quality and quantity of the data and\nto further tap the potential of large language models.\n\nGeneral Tasks and Extended Capabilities   Currently, CourseGPT-zh is optimized for single-turn question-answering.\nHowever, multi-turn question-answering is also of great importance in future work. To improve the quality of multi-turn\nquestion-answering structured data in vertical domains, it is necessary to design a dedicated framework to ensure\nthat large models can accurately understand contextual needs and refer to professional knowledge during the data\nconstruction process. Furthermore, other extended capabilities, such as Socratic teaching and problem-solving reasoning,\nare also very important for large language models in the field of education. These capabilities need to be further\nexpanded in future work.\n\nFinally, in light of the potential social risks posed by CourseGPT-zh, it is imperative to further enhance its security\nmeasures to prevent malicious utilization.\n\nReferences\n\n [1] OpenAI ChatGPT. optimizing language models for dialogue. openai. 2022, 2023.\n [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\n     Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\n     arXiv:2303.08774, 2023.\n [3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n     Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\n     models. arXiv preprint arXiv:2302.13971, 2023.\n [4] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\n     Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\n [5] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\n    Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.  arXiv preprint\n     arXiv:2206.07682, 2022.\n [6] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang,\n    Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846,\n     2023.\n [7] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-\n     thought. arXiv preprint arXiv:2210.01240, 2022.\n [8] Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tie, Yougen Zhou, Yilei\n    Wang, et al. Educhat: A large-scale language model-based chatbot system for intelligent education. arXiv preprint\n     arXiv:2308.02773, 2023.\n [9] Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li,\n     Qianyu He, Rui Xu, et al. Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. arXiv\n     preprint arXiv:2306.05783, 2023.\n\n\n                                              12\n\n[10] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Ger-\n      stein. Medagents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint\n     arXiv:2311.10537, 2023.\n\n[11] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan\n     Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning?\n     case study in medicine. arXiv preprint arXiv:2311.16452, 2023.\n\n[12] Yang Tan, Mingchen Li, Zijie Huang, Huiqun Yu, and Guisheng Fan. Medchatzh: a better medical adviser learns\n    from better instructions. arXiv preprint arXiv:2309.01114, 2023.\n\n[13] Xuanyu Zhang and Qing Yang. Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions pa-\n     rameters. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,\n     pages 4435–4439, 2023.\n\n[14] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagné,\n     Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al.  Bloom: A 176b-parameter open-access\n     multilingual language model. 2022.\n\n[15] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language model\n     with integrated external knowledge bases. arXiv preprint arXiv:2306.16092, 2023.\n\n[16] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large language models.\n     arXiv preprint arXiv:2306.06031, 2023.\n\n[17] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,\n     Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.\n    Advances in Neural Information Processing Systems, 36, 2024.\n\n[18] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,\n    and Nan Duan.  Agieval: A human-centric benchmark for evaluating foundation models.  arXiv preprint\n     arXiv:2304.06364, 2023.\n\n[19] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin.\n    Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023.\n\n[20] Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao\n     Chen, Dongfeng Zhang, et al. Owl: A large language model for it operations. arXiv preprint arXiv:2309.09298,\n     2023.\n\n[21] Dongyang Li, Ruixue Ding, Qiang Zhang, Zheng Li, Boli Chen, Pengjun Xie, Yao Xu, Xin Li, Ning Guo,\n     Fei Huang, et al.  Geoglue: A geographic language understanding evaluation benchmark.  arXiv preprint\n     arXiv:2305.06545, 2023.\n\n[22] Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. Not all\n     languages are created equal in llms: Improving multilingual capability by cross-lingual-thought prompting. arXiv\n     preprint arXiv:2305.07004, 2023.\n\n[23] Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun\n    Chen, and Lei Li. Extrapolating large language models to non-english by aligning languages. arXiv preprint\n     arXiv:2308.04948, 2023.\n\n[24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n     Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human\n     feedback. Advances in neural information processing systems, 35:27730–27744, 2022.\n\n[25] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Deven-\n     dra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv\n     preprint arXiv:2401.04088, 2024.\n\n[26] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu,\n     Zhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be a doctor. arXiv preprint\n     arXiv:2305.15075, 2023.\n\n[27] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too.\n    AI Open, 2023.\n\n[28] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\n     Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing\n     systems, 35:24824–24837, 2022.\n\n\n                                              13\n\n[29] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint\n     arXiv:2101.00190, 2021.\n[30] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt\n     tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602,\n     2021.\n[31] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang.\n     Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint\n     arXiv:2309.08532, 2023.\n[32] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.\n     Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.\n[33] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large\n     language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.\n[34] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, and\n     Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level prompt optimization.\n     arXiv preprint arXiv:2310.16427, 2023.\n[35] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Ha-\n       jishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560,\n     2022.\n[36] Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and\n     Johan Ferret. Warm: On the benefits of weight averaged reward models. arXiv preprint arXiv:2401.12187, 2024.\n[37] Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Moham-\n    mad Shoeybi, and Bryan Catanzaro.  Odin: Disentangled reward mitigates hacking in rlhf.  arXiv preprint\n     arXiv:2402.07319, 2024.\n[38] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\n      Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\n     Information Processing Systems, 36, 2024.\n[39] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu,\n    Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint\n     arXiv:2311.18743, 2023.\n[40] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\n    Language agents with verbal reinforcement learning.(2023). arXiv preprint cs.AI/2303.11366, 2023.\n[41] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay\n     Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with\n      less training data and smaller model sizes. arXiv preprint arXiv:2305.02301, 2023.\n[42] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\n     Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n[43] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin\n     Zhao, Yuxiang Lu, et al. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and\n     generation. arXiv preprint arXiv:2107.02137, 2021.\n[44] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of\n     machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,\n     pages 311–318, 2002.\n[45] Andrew Mutton, Mark Dras, Stephen Wan, and Robert Dale. Gleu: Automatic evaluation of sentence-level fluency.\n     In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344–351, 2007.\n[46] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out,\n     pages 74–81, 2004.\n\n\n\n\n\n                                              14",
"headers": [
"arXiv:2405.04781v1  [cs.CL]  8 May 2024",
"B",
"K",
"D",
"I",
"P",
"O",
"C",
"GPT-",
":",
"E",
"L",
"M",
"ASED ON",
"NOWLEDGE",
"ISTILLATION",
"NCORPORATING",
"ROMPT",
"PTIMIZATION",
"OURSE",
"ZH",
"AN",
"DUCATIONAL",
"ARGE",
"ANGUAGE",
"ODEL",
"∗",
"A",
"1",
"Introduction",
"2",
"Related-work",
"3",
"Data Construction",
"4",
"Discrete Prompt Optimization",
"5",
"Experiment and Analysis",
"6",
"Conclusion and Future work",
"References"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/test/2405.04781v1.pdf"
}