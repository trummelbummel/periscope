{
"text": "Textual Gradients are a Flawed Metaphor for\n                          Automatic Prompt Optimization\n\n                           Daniel Melcer*             Qi Chen and Wen-Hao Chiang\n                         Northeastern University          Shweta Garg and Pranav Garg\n                           Boston, MA, USA                      Christian Bock\n                       daniel@melcer.dev              AWS AI Labs\n                                     New York, NY, USA and Seattle, WA, USA\n                                                      {qic,cwenhao,shwegarg,\n                                                  prangarg,bocchris}@amazon.com\n\n                          Abstract                         trial-and-error. Several model providers have re-\n                                                                 leased prompt engineering guides (OpenAI, 2025;\n           A well-engineered prompt can increase the per-       Anthropic, 2025), but prompt construction with\n                formance of large language models; automatic\n                                                                  these guides remains an art.\n                prompt optimization techniques aim to increase2025                 performance without requiring human effort to         Automatic Prompt Optimization (APO) methods\n                  tune the prompts. One leading class of prompt      aim to automate the process of creating such a well-\n                   optimization techniques introduces the analogy       engineered prompt (Zhou et al., 2023; Yang et al.,Dec\n                  of textual gradients. We investigate the behav-      2024; Fernando et al., 2023; Guo et al., 2025). A\n                     ior of these textual gradient methods through a                                                                   recent development—textual-gradient based meth-15                    series of experiments and case studies. While\n                                                          ods (Pryzant et al., 2023; Cheng et al., 2024; Yuk-\n                 such methods often result in a performance im-\n                                                           sekgonul et al., 2025)—liken a model’s system                 provement, our experiments suggest that the\n                  gradient analogy does not accurately explain      prompt to a learnable parameter. In a traditional\n                     their behavior. Our insights may inform the se-      machine learning system, the gradient of a model’s\n                    lection of prompt optimization strategies, and       parameters describes how the parameters should[cs.CL]\n                 development of new approaches.                 change to achieve an objective. With the gradient\n                                                               analogy, feedback about how to improve the sys-\n          1  Introduction                                                       tem prompt is treated as the prompt’s “gradient”. A\n                                                        prompt engineer could implement a familiar train-            Large Language Models (LLMs) have grown signif-\n                                                                 ing loop, perform “gradient descent” on the prompt,              icantly in capability and popularity in recent years.\n                                                         and obtain an “optimized” prompt after some num-            Tasks that were once considered out-of-reach, such\n                                                                ber of training iterations.             as graduate-level question answering (Rein et al.,\n             2023), top-level competitive math (Castelvecchi,     While  this method usually obtains a well-\n             2025), and advanced automated software engineer-   performing prompt, our experiments show that the\n             ing (Wang et al., 2025), are now possible with the    gradient analogy does not accurately explain the\n                latest generation of frontier models.                behavior of APO. In Section 3, we provide an\n               Despite their increase in power and generality,   overview of the gradient hypothesis. In Section\n                                                                       4, we perform an ablation study where we wouldarXiv:2512.13598v1       they are not perfect. While fine-tuning can be a\n              useful tool to specialize a model for a specific task,   expect a significant decrease in performance un-\n                    it is generally expensive, and often requires a large    der the gradient hypothesis, but generally do not\n           amount of data (Parthasarathy et al., 2024). Fur-   observe such a decrease. Finally, in Section 5, we\n             thermore, many of the best-performing models are    introduce several case studies to further explore the\n            only available behind an API—their weights are    behavior of automatic prompt optimization.\n             not available, and fine-tuning may be impossible.\n               In many cases, a well-engineered prompt or se-                                                 2  Related Work\n           quence of prompts enables an increase in task-\n              specific performance, without requiring access to\n                                                      With access to a model’s weights, it is possible to\n             the model weights (Mishra et al., 2022). How-\n                                                            perform backpropagation to a prompt’s embedding\n               ever, constructing such a prompt may be a difficult\n                                                            space to learn a task-specific prompt (Shin et al.,\n             process, relying on a combination of intuition and\n                                                                2020). However, with closed models, gradient-free\n                *Work performed during an internship at Amazon        methods are necessary for prompt optimization.\n\n\n                                                    1\n\nJiang et  al. (2020) explored how backtrans-   prompt is used to generate a new prompt. Early im-\nlation can be used to generate variations of a    plementations use a variety of methods to generate\nprompt. Prasad et al. (2023) instead use a phrase-    variants of a given prompt, such as phrase substitu-\nsubstitution heuristic to generate variants.             tion (Prasad et al., 2023) or backtranslation (Jiang\n  As LLMs have grown more powerful, more re-    et al., 2020; Xu et al., 2022). Recent versions of\ncent APO methods use the LLM itself for prompt   prompt-only generation typically provide an LLM\ngeneration. Zhou et al. (2023) and Yang et al.   with instructions to rephrase the prompt without\n(2024) use an LLM to generate a new prompt based   changing its meaning (Do et al., 2024).\non a scoring model and evaluation accuracy, respec-     With feedback-driven prompt generation, the\ntively, while and Fernando et al. (2023) and Guo   prompt is used to generate one or more outputs,\net al. (2025) use an LLM to implement mutation   and then feedback based on these outputs is used\nand crossover operators for an evolutionary algo-   as part of the context for generating a new prompt.\nrithm. Do et al. (2024) uses an LLM to mutate\nprompts as part of an adversarial learning process,   3.1  The Gradient Hypothesis\nwhile Gupta et al. (2024) uses a self-reflection pro-  A common variant of feedback-driven generation\ncess to learn specific insights that are concatenated     is the gradient-like method (Pryzant et al., 2023),\nto the prompt. Khattab et al. (2023) released a pack-   which relies on the following assumption:\nage that allows for automatically learning prompts\n                                                   Definition 1 (Gradient Hypothesis). Textual feed-in a pipeline, where an LLM is invoked at several\n                                              back from an LLM behaves like a gradient, and it\nspecialized stages to achieve a complex goal.\n                                                                 is possible to apply the chain rule to this feedback.\n  Pryzant et al. (2023) introduces a gradient-like\nanalogy for generating new prompts from evalu-    We illustrate the gradient analogy with an analog\nation feedback; several works expand this anal-   of the standard training loop.\nogy and provide an autodiff-like implementation      Let f  : V ∗× V ∗→V ∗be a pretrained LLM\n(Cheng et al., 2024; Yuksekgonul et al., 2025). Wan   with token space V and frozen weights; it is pa-\net al. (2024) compares the performance of several    rameterized by system prompt θ ∈V ∗. Given a\nprompt optimization methods in combination with    dataset D, and question-answer pair (xi, yi) ∈D,\nmethods for selecting in-context examples.           the output of the LLM is ˆy = f(xi; θ).\n  Pan et al. (2023) and Min et al. (2022) show      In lieu of a numeric loss function such as mean-\nthat the performance of prompts with incorrect or    square error, a textual loss function may be a tem-\nrandom in-context examples is often comparable to    plate substitution such as L(xi, yi, ˆy) = “Question:\nthe performance with correct in-context examples.    xi. Provided answer: ˆy, Correct answer: yi.” Alter-\n   In some agentic systems, an LLM is used to tune    natively, L(xi, yi, ˆy) may itself invoke an LLM.\nthe optimization process (Zelikman et al., 2024),     According to the gradient hypothesis, asking an\nor even modify the code of the agent itself (Zhang  LLM “How can the answer (ˆy) be changed to im-\net al., 2025; Robeyns et al., 2025; Yin et al., 2025).   prove its evaluation (L(xi, yi, ˆy))?” is analogous\n  These are only a sample of automatic prompt    to computing ∂L(xi,yi,ˆy)∂ˆy      .\noptimization methods; we refer readers to Ramnath      Similarly, asking the LLM “How can the sys-\net al. (2025) for a broader overview of APO.       tem prompt (θ) be changed such that the result-\n                                                  ing output (ˆy) improves according to its feedback\n3  Overview                                                   ∂L(xi,yi,ˆy)\n                                                                                ∂ˆy     ?”  applies the chain rule to obtain\nAny APO method requires some mechanism to     ∂L(xi,yi,ˆy)     ∂ˆy ∂L(xi,yi,ˆy)                                 =                                                         ∂θ                                                                   ∂θ      ∂ˆy      .\ngenerate a prompt. We first describe two broad                                                           Finally, the LLM is directed to “Update the\ncategories of this mechanism: prompt-only, and                                             ∂L(xi,yi,ˆy)\n                                             prompt (θ) using its feedback      ∂θ       .” Thisfeedback-driven generation. There are additional\n                                                                is analogous to taking an optimizer step to obtaincategories that we do not address here, such as\n                                       new prompt θ′ = θ −α ∂L(xi,yi,ˆy)∂θ     .2 Accordingevolutionary algorithms (Fernando et al., 2023), or\nhybrid mechanisms (Pryzant et al., 2023).                                                           with a later selection step. We instead distinguish the prompt\n  In prompt-only generation,1 only the existing     generation method (prompt-only or feedback-driven) from the\n                                                                   selection method (validated with n variants, or naive).\n   1Prompt-only generation  is sometimes referred to as       2The “step size” α is undefined in this analogy, but Yuk-\nMonte-Carlo generation, when used to generate many prompts    sekgonul et al. (2025) introduce a momentum-like behavior.\n\n\n                                         2\n\nto the gradient hypothesis, L(xi, yi, f(xi; θ′)) <   For training and validation, we use 30 and 50 ques-\nL(xi, yi, f(xi; θ)) for some definition of “<”.         tions, respectively, that are from in the broader\n   In this paper, our usage of the gradient analogy  GPQA dataset, rather than the “diamond” subset.\nis limited to these three operations, and addition     Our other two datasets are subsets of Big-\n(implemented as string concatenation). However,   BenchExtraHard (Kazemi et al., 2025). Each\npackages such as those introduced in Yuksekgonul    contains 200 problems; we reserve 30 problems for\net al. (2025); Cheng et al. (2024) implement full    training and 50 for validation, leaving 120 for test-\n“automatic differentiation” with textual gradients,    ing. Web of Lies is a generalization of the classic\nusing a PyTorch-like syntax.                         logic puzzle format: “Alice says that Bob is lying;\n  With traditional gradient descent, the new param-  Bob says that Charlie is telling the truth; Charlie\neters θ′ are immediately used for future iterations     is lying.  Is Alice telling the truth?” The model\n(naive selection). However, many APO methods     is asked to find the truth value of three characters;\ninclude an extra step—they generate one or more     it is also possible for the truth-telling status of a\nprompts θ′1, . . . , θ′n using the above process, and    character to be indeterminate. Multistep Arith-\nthen test each of these prompts on a validation set.   metic requires the language model to reason about\nThe best prompt is chosen, though only if it im-   custom math operators.\nproves on the previous prompt’s (θ) performance.     For the incorrect-evaluation trials, we obtain ˜yi\nWe term this validated prompt selection.           by selecting the answer one index greater than yi in\n                                    GPQA, creating a random answer in Web of Lies,\n4  Analysis of Gradient-like APO             or adding a random integer between -10 to 10 to\n                                                    the true yi in Multistep Arithmetic. The question\n4.1  Ablation Study                                                           text is used as the random seed; the same question\nWe introduce the following research questions to    will always have the same ˜yi.\ntest the gradient hypothesis.                        For all experiments, we begin with a default\n  RQ1: Does the gradient-like structure mat-   prompt such as “Answer the given multiple choice\nter? Rather than using a series of three prompts    question.” The prompt is appended to formatting\nto implement the gradient analogy from Section    instructions that are constant throughout learning;\n3.1, we implement a One-step variant generation    for example, writing the answer inside specific tags.\nroutine that asks the LLM to directly generate a    We run 30 trials for each naive configuration, and\nnew prompt θ′ using evaluation L(xi, yi, ˆy).        5 trials for each validated configuration, generating\n  RQ2: Does a correct loss function matter?   3 prompt variations at each batch during validated\nIn the Incorrect Evaluation configurations, we     trials. All configurations use Claude 3.7 Sonnet,\nsimulate an incorrect loss function by deliberately   with temperature 0.5, top-p sampling with p =\ninserting an incorrect ground-truth answer ˜yi in the    0.95, and a 5000-token limit. We run 10 iterations\nevaluation L(xi, ˜yi, ˆy). In the No Evaluation con-   of the training loop (1 epoch), with a batch size of\nfiguration, the template doesn’t include any ground-    3. Total token usage is reported in Appendix A.\ntruth answer yi, correct or incorrect.\n                                                      4.1.1  Results & Discussion\n  RQ3: Does validation matter? We run both\n                                          The test set accuracies are presented in Figure 1.naive and validated versions of experiments.\n                                        RQ1: The effect of the gradient-like structure  RQ4: Does using a training dataset matter?\n                                                                is dataset-dependent. Gradient-like APO consis-We implement Prompt-Only optimization, which\n                                                          tently led to improved performance compared to andoes not use training data at all. In addition to the\n                                                 un-optimized prompt (Figure 1,   vs.   ). However,standard Rewrite variant (ask an LLM to rephrase\n                                                      gradient-like APO does not consistently outper-the prompt without changing its meaning), we also\n                                            form one-step APO (Figure 1, adjacent pairs oftry asking an LLM to improve the prompt for qual-\n                                                       gradient-like vs. one-step,   vs.   ).ity and correctness (the Improve variant).\n                                  We use a two-sided permutation t-test to fur-   All prompts are included in Appendix D.\n                                                        ther compare the performance of gradient-like ver-  We use three datasets. Our first dataset is GPQA\n                                                   sus one-step feedback-driven APO, for both naiveDiamond3 (Rein et al., 2023), containing 198\n                                              and validated correct-evaluation configurations. AsEnglish-language expert-level science questions.\n                                                             this experiment involves two comparisons for each\n   3GPQA licensed with CC-BY 4.0; BBEH with Apache 2.0    dataset, we apply the Bonferroni correction (Vicker-\n\n\n                                         3\n\n(a) GPQA Diamond                                                        (b) BBEH Web of Lies                                                     (c) BBEH Multistep Arithmetic\n\n     0.7                                                                           0.25                                                                           0.40\n\n                                                                                                                                                                0.35\n     0.6\n\n                                                                                  0.20                                                                           0.30\n     0.5\n Set                                                                                  Set                                                                                   Set\n                                                                                                                                                                0.25\n  Test                                                                                                                   Test                                                                                                                                                                                                                                      Test\n on 0.4                                                         on 0.15                                                                                                                   on\n   Accuracy 0.3                                                                                                                                                                                                                Accuracy                                                                                                                                                                                                                              Accuracy 0.20\n                                                                                  0.10                                                                           0.15\n\n     0.2\n                                                                                                                                                                0.10\n                                                                                  0.05\n     0.1                                                                                                                                                         0.05\n\n\n     0.0                                                                           0.00                                                                           0.00\n                      Naive                             Validated                                    Naive                             Validated                                    Naive                             Validated\n                          Prompt Improvement Scheme                                              Prompt Improvement Scheme                                              Prompt Improvement Scheme\n\n                Default Prompt (no optimization)         Feedback Driven, Correct Evaluation (one-step)         Feedback Driven, No Evaluation (gradlike)              Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)\n               Prompt-only (rewrite)                  Feedback Driven, Correct Evaluation (gradlike)         Feedback Driven, Incorrect Evaluation (one-step)         Feedback Driven, Incorrect Evaluation, Incorrect Validation (gradlike)\n               Prompt-only (improve)                 Feedback Driven, No Evaluation (one-step)            Feedback Driven, Incorrect Evaluation (gradlike)\n\nFigure 1: Performance on (a) GPQA Diamond (b) Web of Lies (c) Multistep Arithmetic, all with 95% CI. 30 trials\nfor each naive configuration; 5 for each validated.\n\n\nstaff et al., 2019) to obtain a significance threshold   power for each naive trial is 0.79; 0.12 for each val-\nof 0.025. We find that in Web of Lies, gradient-like    idated trial.4 The disjunctive power is 0.96. We do\nAPO outperforms one-step feedback-driven APO    not find any significant decrease in accuracy when\nin the naive setting (p = 0.0037). We did not find a    training using incorrect evaluation labels.\nsignificant performance difference for other config-    We perform the same  analysis, comparing\nurations or datasets; we cautiously observe that this    against training with missing evaluation labels (\ntrend may be reversed in the naive configuration of    vs.    ) rather than with incorrect labels. We found\nMultistep Arithmetic, though this result does not    that in the naive trials for Web of Lies, there is a sig-\nreach the significance threshold (p = 0.030). Full    nificant decrease in performance: p = 0.0022 and\np-values are listed in Appendix B.                p < 0.0001 for one-step and gradient-like trials\n                                                       respectively. However, we did not observe such a  Under the assumption that the evaluation accu-\n                                                   decrease in any other datasets. The power analysisracy of prompt optimization is normally distributed,\nwith a large effect size (d = 0.8), the statistical     is the same as with the incorrect evaluation.\npower of this test is 0.79 for the naive trials, and    We believe that the most likely explanation\n0.12 for the validated trials. The disjunctive statis-    for these observations is that APO usually assists\ntical power is 0.81. Except as otherwise stated, we   with the general format of the task, or with meta-\nuse these assumptions for calculating the statistical   instructions—such as to incorporate a chain-of-\npower for the remainder of the paper.               thought style when answering, avoid concluding\n                                             “unknown”, or to answer concisely to avoid hitting\n  RQ2: An incorrect evaluation function does\n                                                a length limit—but it rarely incorporates insights\nnot usually hurt performance, but a missing\n                                            from any one particular question. This is explored\nevaluation function may. The gradient hypothesis\n                                                        further in Section 5.1.\nimplies that using an incorrect training set would\n                                      RQ3: Validation has inconsistent effective-lead to significantly worse test performance than\n                                                     ness. For each dataset, we compare all eight naivewith a correct training set. However, we do not\n                                                     configurations to the corresponding validated con-observe this behavior. Visually, this is illustrated in\n                                                        figuration. In Figure 1, this is the comparison be-Figure 1 by comparing the correct evaluation trials\n                                            tween all bars with a “Naive” label against the(   ) to the incorrect evaluation trials (   ).\n                                                 corresponding bar with a “Validated” label.\n  We use a one-sided permutation t-test to further\n                                  We use a one-sided permutation t-test with the\nanalyze these comparisons. There are 4 compar-\n                                                     corrected p threshold of 0.00625. The test for each\nisons for each dataset (naive and validated selec-\n                                                      configuration has a power of 0.18, resulting in a dis-\ntion; and one-step and gradient-like generation).\nApplying the Bonferroni correction, we obtain a                                                           4While the corrected p threshold is lower than with RQ1,\nsignificance threshold of 0.0125. The statistical   we use a one-sided test, resulting in the same per-test power.\n\n\n                                         4\n\njunctive power of 0.79. We do not find a significant   Evaluation). This differs from the ungrounded\nimprovement with validated prompt selection.     No Evaluation configuration in Section 4.1; with\n  We also run an exploratory test to determine if    critic-based evaluation, the critic’s prompt is itself\nan incorrect validation set hurts performance, in    optimized based on the accuracy of its outputs.\ncases where an incorrect training set is used with      Critic-based evaluation mirrors the structure of\nvalidated prompt selection (   vs.    ). There are   two adversarial networks (Goodfellow et al., 2014).\ntwo comparisons for each dataset; using a one-     It was first instantiated in the textual domain by Do\nsided permutation t-test with a significance thresh-    et al. (2024), albeit with prompt-only optimization.\nold of 0.025, we find that incorrect validation data  We use a variant of this framework to optimize the\nhurts performance in Multistep Arithmetic, with    generator and critic prompts simultaneously:\ngradient-like training (p = 0.0238). However, be-    An LLM critic, parameterized by system prompt\ncause we only run 5 validated trials per configura-   ϕ, is used to generate evaluation LG(xi, ˆy; ϕ). This\ntion due to their high token usage, this test requires    evaluation is used by the standard APO framework\nan effect size of d = 1.51 to reach a disjunctive    to update the generator prompt θ. Meta-evaluation\npower of 0.8. Additional trials are required to rule   LC(L(xi, yi, ˆy), LG(xi, ˆy; ϕ)) is created for the\nout smaller effect sizes for the other two datasets.     critic by comparing its output against a direct eval-\n  RQ4: Prompt-only optimization sometimes    uation of the generator’s output. An independent\nworks, but feedback-driven is often better. For    instance of APO is used to obtain new critic prompt\neach dataset, we compare the effectiveness of    ϕ′ using this evaluation, using the meta-prompts in\n“rewrite” prompt-only generation, in naive and val-   Appendix D.4.\nidated configurations. As shown in Figure 1, the    We implement both Binary Critic-Based eval-\n“rewrite” prompt-only configuration ( ) performs    uation, where the critic is instructed to provide\nsimilarly to the un-optimized prompt ( ) configu-   a binary judgment of the generator’s output, and\nration. Using a one-sided permutation t-test, with   Smooth Critic-Based evaluation, where the critic\n2 comparisons for each dataset (corrected signif-    is instructed to assign a score between 0 and 100.\nicance threshold of 0.025), we do not find a sig-    We run 30 trials for all naive variants and 5 for\nnificant improvement over the default. The per-   validated variants; results are shown in Figure 2.\nconfiguration power is 0.86 for the naive experi-    As shown by the adjacent correct vs. incorrect\nment and 0.36 for the validated experiment, provid-    trials for the new variants (Figure 2,   vs.   ), an\ning a per-dataset disjunctive power of 0.91.           incorrect label (replacing yi with ˜yi) continues to\n  We repeat the analysis for prompt-only “im-   not harm performance in the critic-based variant.\nprove” APO (  vs.   ), and find an improvement in  We repeat a one-sided permutation t-test in these\nsome datasets—both GPQA (p ≈10−5 for naive,    additional variants; with 4 new variants per dataset\np = 0.0008 validated) and Multistep Arithmetic   (smooth and binary; naive and validated), we use\n(p = 0.0025 for naive, p = 0.0004 validated).        the Bonferroni correction to obtain a p threshold of\n  While a more directed prompt may allow prompt-   0.0125. The power analysis is identical to that in\nonly generation to outperform the default prompt,   Section 4.1.1, RQ2. We again do not find a signifi-\nfeedback-driven APO is usually better. We com-   cant decrease in performance, inconsistent with a\npare prompt-only generation to gradient-like APO    gradient-like interpretation of textual feedback.\n(  vs.  ) for both naive and validated experiments    We also analyze whether critic-based training\n(p threshold of 0.025, naive power 0.86, validated    performs differently from the equivalent direct con-\npower 0.20, disjunctive power 0.89) and find that    figuration This is visually illustrated in Figure 2\ngradient-like APO outperforms prompt-only “im-   as the comparison between critic-based trials and\nprove” APO in Web of Lies using naive prompt    their adjacent direct configurations.\nselection (p ≈10−5), and Multistep Arithmetic for    We statistically test this with a two-sided permu-\nboth naive and validated selection (p ≈10−5 and    tation t-test. There are 8 comparisons per dataset,\n0.0040, respectively).                                 for a corrected p threshold of 0.00625.  In Web\n                                                   of Lies, smooth critic-based training results in a\n4.2  Direct vs. Critic-based Evaluation                                                 lower performance than direct training in the naive\nWe analyze the case where LLM-generated feed-   case (p = 0.0010). However, this trend may be\nback is used (Critic-Based Evaluation), rather    reversed in the validated case—we cautiously ob-\nthan a template based on the actual answer (Direct    serve that several independent smooth critic-based\n\n\n                                         5\n\n(a) GPQA Diamond                                                        (b) BBEH Web of Lies                                                     (c) BBEH Multistep Arithmetic\n     0.8\n\n                                                                                                                                                                0.40\n     0.7                                                                           0.30\n\n                                                                                                                                                                0.35\n     0.6                                                                           0.25\n                                                                                                                                                                0.30\n\n     0.5\n                                                                                  0.20                                                                           0.25   Accuracy                                                                                                                                                                                                                          Accuracy                                                                                                                                                                                                                              Accuracy\n     0.4\n                                                                                                                                                                0.20\n                                                                                  0.15    Evaluation                                                                                                                                                                                                                                                                                 Evaluation                                                                                                                                                                                                                                                                                     Evaluation\n     0.3\n                                                                                                                                                                0.15\n                                                                                  0.10\n     0.2                                                                                                                                                         0.10\n\n                                                                                  0.05\n     0.1                                                                                                                                                         0.05\n\n\n     0.0                                                                           0.00                                                                           0.00\n             Direct     Bin. Critic  Sm. Critic     Direct     Bin. Critic  Sm. Critic                Direct     Bin. Critic  Sm. Critic     Direct     Bin. Critic  Sm. Critic                Direct     Bin. Critic  Sm. Critic     Direct     Bin. Critic  Sm. Critic\n           Naive      Naive      Naive     Validated   Validated   Validated              Naive      Naive      Naive     Validated   Validated   Validated              Naive      Naive      Naive     Validated   Validated   Validated\n                                                                                 Correct Training           Incorrect Training           Incorrect Training, Incorrect Validation\n\nFigure 2: Performance of direct and critic-based prompt optimization; naive and validated variants, using both a\ncorrect and incorrect validation set. 30 trials in naive configurations; 5 trials in validated configurations; 95% CI.\n\n\nvalidated trials in Web of Lies have results with    4.3  (Not) Overfitting the Training Data\np-values slightly above the significance threshold                                                   In a traditional gradient-descent based system,\n(p = 0.0079 with correct training; p = 0.0159                                                         training for many epochs on a small dataset will re-\nwith incorrect training). As the results are likely                                                            sult in overfitting, where performance on the train-\ncorrelated with each other, the Bonferroni correc-                                                  ing set continues increasing, but performance on\ntion may be too conservative in this instance.                                                      the test set flattens or decreases. If textual gradients\n                                              behaved as traditional gradients, we would expect\n  We investigate the best-performing trials in Sec-                                                      similar overfitting behavior. We evaluate potential\ntion 5.1 to better understand the circumstances that                                                        overfitting by training for 100 steps (10 epochs,\nlead to improved performance in those trials.                                                  batch size 3, training set size 30).\n                                  We run 5 trials of APO for each dataset. As\n  We also test  if validated configurations per-\n                                         shown in Figure 3 (left), there is no evidence of\nform better than their equivalent naive counterparts.\n                                                        gradient-like APO overfitting the training data. The\nThere are four new comparisons per dataset, so\n                                                performance on the training data after 10 epochs is\nwe use a corrected p threshold of 0.0125. In Web\n                                                      similar to the performance after the first epoch.\nof Lies, we find a significant increase in perfor-\n                                               Another behavior that we would expect under\nmance with validated training for smooth critic-\n                                                    the gradient hypothesis is that after training on\nbased feedback (p < 0.0001 with correct evalua-\n                                                    data with a consistently incorrect label, the model\ntions, p = 0.0008 with incorrect evaluation). We\n                                                  should learn this incorrect data. As Figure 3 (right)\ndo not find a significant improvement with valida-\n                                                  demonstrates, textual gradient descent doesn’t ex-\ntion in the other datasets. As there are relatively few\n                                                            hibit this behavior, failing to learn the training data.\nvalidated configurations, an effect size of d = 0.91\nis required to reach a power of 0.8; more data is                                       5  Case Studies\nrequired to reliably detect lower effect sizes.\n                                We  first  investigate a  particularly performant\n   Finally, we run another exploratory test to as-   prompt in the Web of Lies domain, to understand\nsess if incorrect validation data hurts performance,   the factors that lead to high accuracy when using\ncompared to training with correct validation data    this prompt. We then further investigate the be-\nand incorrect training data (Figure 2,   vs.    in    havior of validated prompt selection by replicating\nvalidated configurations). Using a one-sided per-   selected outliers from Section 4.\nmutation t-test (significance threshold of 0.025),\n                                                    5.1  Right Feedback, Wrong Reasonwe do not find any statistically significant decrease\nin performance with incorrect validation data. The  We selected one prompt that performed particularly\npower analysis is identical to the final exploratory    well in Web of Lies, from a validated smooth critic-\ntest in Section 4.1.1, RQ3.                        based run, and analyzed both why this prompt per-\n\n\n                                         6\n\nGPQA Diamond (Correct Training)                       GPQA Diamond (Incorrect Training)\n                          Accuracy 0.750.50\n\n            0.25                                Evaluation            0.00\n\n                       BBEH Web of Lies (Correct Training)                       BBEH Web of Lies (Incorrect Training)\n\n              0.4\n                             Accuracy\n                                     Evaluation 0.20.0\n\n                    BBEH Multistep Arithmetic (Correct Training)                  BBEH Multistep Arithmetic (Incorrect Training)\n                             Accuracy 0.4\n                                     Evaluation 0.20.0\n               0       1                      10                     100  0       1                      10                     100\n                                        Batch Number                                                   Batch Number\n\n                                        Eval on Test Set          Eval on Correct Training Set          Eval on Incorrect Training Set\n\n\nFigure 3: Performance on training set, evaluation set, and an incorrect training set, over 100 iterations (10 epochs).\nDue to the small training set and uneven distribution of difficult problems, the training set for Web of Lies is\nsomewhat easier than the test set, while the opposite is true for GPQA. 5 trials in all configurations; 95% CI.\n\n\nformed so well, as well as the evolution of this            Set         Case Study Prompt- All But One Section                                                                                          0.45\nprompt. An excerpt from this prompt is as follows;               Test\n                                                                on 0.40\nit is reproduced in full in Appendix C:\n                                                                                          0.35    When answering logic puzzles...                                                                                                                 Accuracy\n                                                                                                                               Eval 0.30    Full    1     2     3     4     5     6     7     8       1. Begin by systematically organizing all relevant                                                                                       Prompt\n      information: [8 lines skipped]                                                                    Section Number to Hold Out\n       2. Develop a systematic and exhaustive approach                                                                                    (a) Including every section except X.\n      for truth-teller/liar puzzles: [19 lines skipped]\n\n      3. Be EXTREMELY persistent and thorough -                                                                                               Set         Case Study Prompt- Only One Section\n    NEVER accept “unknown” prematurely:                               0.35\n                                                                                                                               Test\n                                                                                          0.30       - CRITICAL: These puzzles are ALWAYS de-             on\n      signed to have definitive answers -  if you’re                      0.25\n     tempted  to conclude “unknown,”  this  is  a                      0.20                                                                                                                                                                                                                                                              Accuracy\n    STRONG SIGNAL that you need to dig deeper                       0.15\n                                                                                                                               Eval      Default  1     2     3     4     5     6     7     8\n     [Prompt continues for 49 additional lines]                                     (Minimal)\n                                                                                       Prompt          Section Number to Include\n\n  The prompt is broken up into eight substantive                  (b) Prompt only contains section X.\nsections. To determine the most important part\n                                                      Figure 4: Evaluation accuracy for ablations of the case\nof the prompt, we evaluated the performance of\n                                                       study prompt (Appendix C); 5 trials, 95% CI.\neach section of this prompt. We run ablations that\ninclude only one section of the prompt, or hold\nout one section of the prompt, and evaluate the\n                                          The third section of the prompt contains a form ofresulting prompt on the test set. We evaluated each\n                                                  prevalence hacking: a strongly worded instructionablation five times. As shown in Figure 4, the third\n                                                        to never predict the minority class (answering “un-section has an impact on the evaluation accuracy.\n                                           known”) to increase performance on the majority  In Web of Lies, it is possible for the answer to\n                                                      classes (definitive yes/no answers).be “unknown”, in cases where it is truly impossi-\nble to deduce the truth-telling status of one of the    We traced the creation of this section. In the\ncharacters. However, this is uncommon; usually,   batch where this section was first created, the gen-\nwhen the LLM answers “unknown”, it is wrong.    erator outputted “unknown” for all answers. As this\n\n\n                                         7\n\nwas a critic-based evaluation trial, the LLM critic            Web of Lies - Smooth Critic - Naive vs Validated with # Variants\n                                                                                                         0.32\nrated the generator’s output as incorrect, producing\n                                                                                                                Set 0.30\nthe following evaluation (L(xi, yi, ˆy)):                                                Test\n                                                                          on 0.28\n     The correct answer is that Raymond, Murphy, and                                                                                                         0.26     Sima all tell the truth (yes, yes, yes). This can be                                                              Accuracy\n      determined through careful analysis...                                                      Eval 0.24\n      Since the final answer provided is incorrect (un-                        0.22\n     known, unknown, unknown), the score must be 0,                               Naive       1         2         3         4         5\n      regardless of the partial progress made...                                                                       Validated - # Variants\n\n                                                       Figure 5: Performance in Web of Lies with naive selec-\n  Note that, for this question, the LLM-based critic\n                                                                  tion, and validated selection using n variants.\nprovided incorrect information! One person’s truth-\ntelling status is truly indeterminate, but with a critic-\nbased evaluation, the ground truth can only be used    over naive training, but many other experiments did\nto improve the critic’s prompt; the critic’s output is    not show a performance increase with validation.\nused to improve the generator’s prompt.        We further investigate potential reasons for this.\n                                    ∂L(xi,yi,ˆy)\n  The output feedback                                           ∂ˆy       correctly      Validated selection likely impacts performance\n                                                 using one of two mechanisms: either by prevent-notes that the generator is too likely to output\n                                                  ing prompt θ from being replaced by a generatedunknown—but it goes further and declares that an\n                                              prompt θ′ that decreases performance (Regression“unknown” should never occur:\n                                               Avoidance),  or by generating many prompts\n       2. **Premature declaration of “unknown”**: All          θ′1, . . . , θ′n, with n chances for a new prompt to      three solutions gave up too easily...\n                                                           result in high performance (Prompt Discovery).\n     Remember that these logic puzzles are designed\n                                   We instantiate 40 new trials of naive training in       to have definitive answers, so if the solution seems\n      incomplete, that’s a sign to dig deeper...                  this domain, and 15 trials each of validated training\n                                                with 1-5 generated variants, obtaining a power of\n  The feedback about improving the generator   0.83 for one-sided t-tests between naive and vali-\n   ∂L(xi,yi,ˆy)\n      ∂θ     emphasizes such a declaration:         dated trials. The results are shown in Figure 5.\n                                   We find a significant improvement with validated\n       1. **Emphasize persistence and thoroughness**:       prompt selection with 5 variants over naive selec-\n     The prompt should strongly emphasize  that\n                                                       tion (p = 0.0014), replicating our observations      these puzzles are designed to have definitive an-\n       swers...never give up and declare “unknown” until       from Section 4.2. However, we do not find a signif-\n       truly exhausting all logical possibilities...                icant improvement from naive prompt selection to\n                                                      validated selection with one variant (p = 0.44).\n  Thus, the prompt is updated with incorrect in-\n                                  We test if the number of variants is correlated\nstructions that improve its performance.\n                                                with increasing performance with validated selec-\n   In contrast, when direct evaluation is used—\n                                                           tion, using a one-sided Kendall τ test. We find\nusing a template L(xi, yi, ˆy) instead of an LLM\n                                                         that there is a significant, albeit somewhat weak,\ncritic—the feedback must account for the fact that\n                                                       correlation (τ = 0.246, p = 0.0025).\n“unknown” is sometimes the right answer. Typi-\n                                                     Therefore, there is evidence to support that, at\ncally, the prompt will learn softer instructions, such\n                                                           least in this configuration, the benefit of validated\nas to try multiple approaches before concluding\n                                                       training comes primarily from prompt discovery\nunknown. While this is more factually correct com-\n                                                        rather than regression avoidance.\npared to a prompt that effectively bans concluding\n                                                 However, further research is required. For exam-\n“unknown,” the LLM will often incompletely fol-\n                                                          ple, in Section 3, training for 100 batches should\nlow these instructions, give up early, and answer\n                                                         result in 100 opportunities for prompt discovery.\n“unknown”, reducing the test performance.\n                                            Given that performance is limited in those trials,\n5.2  Validation: Prompt Discovery or             regressions may be a factor in some configurations.\n     Regression Avoidance?\n                                        6  Conclusion\nCritic-based evaluation with correct training in\nWeb of Lies showed one of the greatest perfor-   While the gradient metaphor has led to the develop-\nmance improvements when using validated training   ment of successful APO methods, further investi-\n\n\n                                         8\n\ngation reveals that textual feedback generally does     Based on this, the results in this paper may need\nnot behave like gradients. Ablations that would    to be re-evaluated for future generations of models.\ndecimate performance in a gradient-based system\noften have little impact on prompt optimization. In   Ethical Considerations\nsome configurations, test accuracy improvements\n                                                Like many methods in this field, automatic prompt\nmay stem from discovery of prevalence-hacking\n                                                  optimization may be used for either beneficial or\nprompts. The insights we present serve both to aid\n                                                 harmful purposes. Proper guardrails in the underly-\nthe selection of an appropriate APO method for a\n                                                   ing language models provide a defense against us-\ngiven task, and to motivate further understanding\n                                                   ing prompt optimization for nefarious goals. How-\nand development of future APO methods.\n                                                          ever, these guardrails do not protect against the risk\n                                                         that a well-intentioned user may be overly confi-\nLimitations\n                                                  dent in a model’s output.\nWe note several limitations of our evaluation. First,     Foundation model usage also entails an environ-\nwe restricted our investigation to prompt optimiza-   mental and resource usage impact. Prompt opti-\ntion, rather than in-context-example selection (Wan   mization often uses fewer resources compared to\net al., 2024). We refer interested readers to Min   model fine-tuning, but may still require a consid-\net al. (2022) and Pan et al. (2023) for insights about    erable number of tokens, especially in validated\nusing examples with random labels.                  configurations.\n  Second, we note that there was high variance in\nour results, even with just one model and imple-\n                                          References\nmentation of the APO framework. While we at-\ntempt to mitigate this high variance by using many    Anthropic. 2025.   Prompt  engineering  overview.\ntrials, the results may differ across multiple mod-      Anthropic Documentation, Building with Claude\n                                                                   series.     Available  online  at  https://docs.\nels, starting prompts, and meta-prompts used for                                                 anthropic.com/en/docs/build-with-claude/\nprompt improvement. Our power analyses usually      prompt-engineering/overview.\nassume an effect size of d = 0.8, but the variants\nwe test may have a smaller effect size, resulting    Davide Castelvecchi. 2025. Deepmind ai crushes tough\n                                                   maths problems on par with top human solvers. Na-in our tests not being able to observe a significant\n                                                                    ture, 638(8051):589–589.\ndifference. While the datasets include both tasks\nweighted towards internal knowledge (GPQA), and   Ching-An Cheng, Allen Nie, and Adith Swaminathan.\ntasks weighted towards reasoning over the context      2024. Trace is the next autodiff: Generative opti-\n                                                       mization with rich feedback, execution traces, and(Web of Lies and Multistep Arithmetic), the re-\n                                                               llms. Preprint, arXiv:2406.16218.\nsults could be made more robust with additional\nevaluations, such as in coding or tool-use tasks.     Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie,\n   Finally, LLMs have continued to become more      James Xu Zhao, Nancy F. Chen, Kenji Kawaguchi,\n                                                    Michael Shieh, and Junxian He. 2024.  Promptcapable with each iteration. A language model\n                                                          optimization via adversarial in-context  learning.\nfrom 2021 may not have been able to follow the                                                              Preprint, arXiv:2312.02614.\ncomplex instructions required for gradient-like\nAPO, and prompt generation methods such as back-   Chrisantha  Fernando,  Dylan  Banarse,  Henryk\n                                                      Michalewski, Simon Osindero, and Tim Rock-translation appear archaic just a few years later.\n                                                                täschel. 2023.   Promptbreeder:  Self-referential\n  Some of the conclusions from this paper contra-                                                       self-improvement via prompt evolution.  Preprint,\ndict those found in prior work using earlier models,      arXiv:2309.16797.\nsuch as Yang et al. (2024). They find that per-\n                                                       Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,formance with prompt optimization continues to\n                                                  Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nimprove for many optimization steps, rather than                                                            Courville, and Yoshua Bengio. 2014.  Generative\nour observation in Figure 3 that training accuracy       adversarial networks. Preprint, arXiv:1406.2661.\nplateaus quickly. While earlier language models\nmay perform more incremental changes for each   Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\n                                                     Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\nAPO step, or may be more sensitive to exact prompt                                                                     jiu Yang. 2025. Evoprompt: Connecting llms with\nwording, modern LLMs synthesize a fairly com-      evolutionary algorithms yields powerful prompt opti-\nplex prompt on the first step.                             mizers. Preprint, arXiv:2309.08532.\n\n\n                                         9\n\nPriyanshu Gupta, Shashank Kirtania, Ananya Singha,   Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\n  Sumit Gulwani, Arjun Radhakrishna, Sherry Shi, and      guang Zhu, and Michael Zeng. 2023.  Automatic\n  Gustavo Soares. 2024. Metareflection: Learning in-     prompt optimization with “gradient descent” and\n   structions for language agents using past reflections.     beam search. Preprint, arXiv:2305.03495.\n   Preprint, arXiv:2405.13009.\n                                                    Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Sm-\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham        ruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang,\n  Neubig. 2020. How can we know what language      Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu\n  models know? Preprint, arXiv:1911.12543.            Wang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou,\n                                                    Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan\nMehran Kazemi, Bahare Fatemi, Hritik Bansal, John      Chen, Haibo Ding, and 2 others. 2025. A systematic\n   Palowitch, Chrysovalantis Anastasiou, Sanket Vaib-      survey of automatic prompt optimization techniques.\n  hav Mehta, Lalit K. Jain, Virginia Aglietti, Disha       Preprint, arXiv:2502.16923.\n   Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen,\n  Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska,   David Rein, Betty Li Hou, Asa Cooper Stickland,\n  Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan      Jackson Petty, Richard Yuanzhe Pang, Julien Di-\n   Firat. 2025.   Big-bench extra hard.   Preprint,       rani, Julian Michael, and Samuel R. Bowman. 2023.\n  arXiv:2502.19187.                                Gpqa: A graduate-level google-proof q&a bench-\n                                                      mark. Preprint, arXiv:2311.12022.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari,\n                                          Maxime Robeyns, Martin Szummer, and Laurence  Zhiyuan Zhang, Keshav Santhanam,  Sri Vard-\n                                                          Aitchison. 2025. A self-improving coding agent.  hamanan, Saiful Haq, Ashutosh Sharma, Thomas T.\n                                                              Preprint, arXiv:2504.15228.   Joshi, Hanna Moazam, Heather Miller, Matei Za-\n   haria, and Christopher Potts. 2023. Dspy: Compiling\n                                                      Taylor Shin, Yasaman Razeghi, Robert L. Logan IV,   declarative language model calls into self-improving\n                                                          Eric Wallace, and Sameer Singh. 2020.   Auto-   pipelines. Preprint, arXiv:2310.03714.\n                                                      prompt: Eliciting knowledge from language mod-\n                                                                  els with automatically generated prompts. Preprint,Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\n                                                        arXiv:2010.15980.  Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\n  moyer. 2022. Rethinking the role of demonstrations:\n                                                           Victoria Vickerstaff, Rumana Z. Omar, and Gareth Am-\n  What makes in-context learning work? In Proceed-\n                                                                     bler. 2019. Methods to adjust for multiple compar-\n   ings of the 2022 Conference on Empirical Methods in\n                                                           isons in the analysis and sample size calculation\n  Natural Language Processing, pages 11048–11064,\n                                                          of randomised controlled trials with multiple pri-\n  Abu Dhabi, United Arab Emirates. Association for\n                                               mary outcomes. BMC medical research methodol-\n  Computational Linguistics.\n                                                           ogy, 19(1):129.\n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin                                                Xingchen Wan, Ruoxi Sun, Hootan Nakhost, and Ser-\n  Choi, and Hannaneh Hajishirzi. 2022. Reframing                                                    can O. Arik. 2024. Teach better or show smarter?\n   instructional prompts to gptk’s language. Preprint,                                                on instructions and exemplars in automatic prompt\n  arXiv:2109.07830.                                                             optimization. Preprint, arXiv:2406.15708.\n\nOpenAI. 2025. Prompt engineering guide. OpenAI                                               Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu,\n  Platform Documentation, Guides. Available online                                                    Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi\n   at https://platform.openai.com/docs/guides/                                                     Song, Bowen Li, Jaskirat Singh, Hoang H. Tran,\n  promptâĂŚengineering.                                                    Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian,\n                                                   Yanjun Shao, Niklas Muennighoff, Yizhe Zhang,\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.                                                  Binyuan Hui, and 5 others. 2025. Openhands: An\n  2023. What in-context learning “learns” in-context:                                                    open platform for ai software developers as generalist\n  Disentangling task recognition and task learning.                                                              agents. Preprint, arXiv:2407.16741.\n   In Findings of the Association for Computational\n   Linguistics: ACL 2023, pages 8298–8319, Toronto,   Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-\n  Canada. Association for Computational Linguistics.      gang Wang, Haiyu Li, and Zhilin Yang. 2022. Gps:\n                                                         Genetic prompt search for efficient few-shot learning.\nVenkatesh Balavadhani Parthasarathy, Ahtsham Zafar,       Preprint, arXiv:2210.17041.\n  Aafaq Khan, and Arsalan Shahid. 2024. The ulti-\n  mate guide to fine-tuning llms from basics to break-   Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\n   throughs: An exhaustive review of technologies, re-     Quoc V. Le, Denny Zhou, and Xinyun Chen. 2024.\n   search, best practices, applied research challenges      Large language models as optimizers.   Preprint,\n  and opportunities. Preprint, arXiv:2408.13296.           arXiv:2309.03409.\n\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit    Xunjian Yin, Xinyi Wang, Liangming Pan, Li Lin, Xi-\n   Bansal. 2023. Grips: Gradient-free, edit-based in-      aojun Wan, and William Yang Wang. 2025. Gödel\n   struction search for prompting large language models.      agent: A self-referential agent framework for recur-\n   Preprint, arXiv:2203.07281.                               sive self-improvement. Preprint, arXiv:2410.04444.\n\n\n                                         10\n\nMert Yuksekgonul, Federico Bianchi, Joseph Boen,  B  Extended Results\n  Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin,\n  and James Zou. 2025. Optimizing generative ai by\n   backpropagating language model feedback. Nature,\n  639:609–616.                       We include p-values for all comparisons in this\n                                                     paper. Note that due to our use of non-parametric\nEric Zelikman, Eliana Lorch, Lester Mackey, and    permutation tests (using 100000 permutations), the\n  Adam Tauman Kalai. 2024. Self-taught optimizer    p-values vary slightly each time they are calculated.\n   (stop): Recursively self-improving code generation.\n   Preprint, arXiv:2310.02304.                         Section 3.1 RQ1\n\nJenny Zhang, Shengran Hu, Cong Lu, Robert Lange,     Two-sided  permutation   t-test;   comparing\n  and Jeff Clune. 2025. Darwin godel machine: Open-    gradient-like vs one-step feedback driven APO;\n  ended evolution of self-improving agents. Preprint,                                                      correct evaluations.\n   arXiv:2505.22954.\n                                                               Naive     Validated\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,        GPQA  0.32596  0.45238\n  Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy        MSA    0.02918  0.96825\n  Ba. 2023. Large language models are human-level\n                                       WoL    0.00350  0.76190  prompt engineers. Preprint, arXiv:2211.01910.\n                                                   Section 3.1 RQ2\nA  Resource Usage\n                                                  One-sided permutation t-test; comparing correct\nOur token counts are broken down as follows:        evaluations with incorrect evaluations.\n  Section 4.1 619838771 input tokens for com-                          Gradient-like  One-step\nbined experiments; 638054972 output tokens.      GPQA  Naive     0.25413       0.86616\n  Section   4.2  1648101679   input   tokens;   GPQA   Validated  0.73413       0.22222\n877691530 output tokens.                MSA    Naive     0.23197       0.39818\n  Section  4.3 155436119  input  tokens  and   MSA    Validated  0.93651       0.19841\n44979764 output tokens for training; 316769262   WoL    Naive     0.25818       0.67516\ninput and 238513122 output tokens for evaluation.   WoL    Validated  0.77778       0.45238\n  Section 5.1 16071000 input tokens; 18614182                                                  One-sided permutation t-test; comparing correct\noutput tokens (for the prompt ablation study).                                                     evaluations with missing evaluations.\n  Section 5.2 826090922 input tokens; 301464758\noutput tokens.                                                              Gradient-like  One-step\n  Total 3582307753 input tokens; 2119318328   GPQA  Naive     0.94817       0.87397\noutput tokens.                        GPQA   Validated  0.74603       0.65476\n                               MSA    Naive     0.05621       0.94999  Due to a shared account,  it is difficult to at-\n                               MSA    Validated  0.38492       0.63492tribute exact costs. At on-demand token rates at\n                                  WoL    Naive     0.00006       0.00191time of writing ($3 per million input tokens; $15\n                                  WoL    Validated  0.61905       0.46032per million output tokens), the experiments cost\n$10746.92 in input tokens and $31789.77 in output                                                   Section 3.1 RQ3\ntokens, for a total of $42536.69.\n                                                 One-sided permutation t-test; naive vs.  vali-  We estimate that a similar number of tokens were\n                                                      dated.used in preliminary experiments; our original re-\nsearch idea relied on strong assumptions about the      GPQA                   p-value\ngradient hypothesis, and we attempted numerous         Prompt-only (rewrite)      0.83252\nvariations of those experiments before investigating         Prompt-only (improve)     0.52184\nwhether the gradient hypothesis itself was true.             Gradient-like             0.92019\n  There was an additional, minor, cost for an AWS          Gradient-like (incor. eval)  0.59086\nEC2 instance (non-GPU) to host the program con-          Gradient-like (no eval)     0.74801\ntaining the API client, and an EFS filesystem to         One-step                 0.95823\nstore experiment results, but these costs were not         One-step (incor. eval)      0.99987\nclosely tracked.                                        One-step (no eval)         0.93864\n\n\n                                         11\n\nMultistep Arithmetic      p-value          based evaluation with correct training data.\n      Prompt-only (rewrite)      0.28006                               Bin. Critic  Sm. Critic\n      Prompt-only (improve)     0.15365        GPQA  Naive  0.96321     0.38652\n      Gradient-like             0.23385        GPQA   Valid.  0.27778     0.51587\n      Gradient-like (incor. eval)  0.02993        MSA    Naive  0.88347     0.68141\n      Gradient-like (no eval)     0.04177        MSA    Valid.  0.15079     0.08730\n      One-step                 0.67234         WoL    Naive  0.32646     0.00112\n      One-step (incor. eval)      0.91584         WoL     Valid.  0.42857     0.00794\n      One-step (no eval)         0.84075           Two-sided permutation t-test; direct vs critic-\n    Web of Lies               p-value          based evaluation with incorrect training data.\n      Prompt-only (rewrite)      0.36356                               Bin. Critic  Sm. Critic\n      Prompt-only (improve)     0.06098        GPQA  Naive  0.75623     0.69617\n      Gradient-like             0.98010        GPQA   Valid.  0.60317     0.23016\n      Gradient-like (incor. eval)  0.93731        MSA    Naive  0.21124     0.29172\n      Gradient-like (no eval)     0.26386        MSA    Valid.  0.15873     0.50000\n      One-step                 0.48244         WoL    Naive  0.66823     0.05724\n      One-step (incor. eval)      0.65189         WoL     Valid.  0.01587     0.01587\n                                                  One-sided permutation t-test; naive vs validated      One-step (no eval)         0.03107\n  One-sided permutation t-test, validated with in-    training.\n                                                              Training   Bin. Critic  Sm. Criticcorrect training and correct validation vs validated\n                                 GPQA   Correct    0.38697     0.84701with incorrect training and incorrect validation.\n                 Gradient-like  One-step        GPQA   Incorrect  0.53860     0.05930\n                               MSA    Correct    0.40689     0.54588     GPQA  0.15079       0.77778\n                               MSA    Incorrect  0.39541     0.09246    MSA    0.02381       0.26984\n                                  WoL    Correct    0.76608     0.00002     WoL    0.50000       0.54762\n  Section 3.1 RQ4                      WoL     Incorrect  0.02292     0.00080\n                                                 One-sided permutation t-test; validated training  One-sided permutation t-test, Prompt-only gen-\n                                                with incorrect training data; correct vs incorrecteration vs. default prompt.\n                                                      validation data.                      Improve  Rewrite\n                                                                    Bin. Critic  Sm. Critic\n   GPQA  Naive     0.00001   0.43243\n                                     GPQA  0.61111     0.78175\n   GPQA   Validated  0.00078   0.88646\n                                   MSA    0.92857     0.08730\n   MSA    Naive     0.00214   0.87150\n                                      WoL    0.05556     0.07937\n   MSA    Validated  0.00038   0.38130\n   WoL    Naive     0.99952   0.93143    C  Case Study\n   WoL    Validated  0.40777   0.62740\n  One-sided permutation t-test, gradient-like with  We include expanded versions of all steps of the\ncorrect training vs. prompt-only improve.           case study.\n                 Naive     Validated\n                                            C.1  Generator Prompt\n      GPQA  0.00451  0.46429\n                                         The following prompt performed well on Web of      MSA    0.00001  0.00397\n                                                        Lies, and was thus used in the case study in Section      WoL    0.00001  0.56746\n  Section 3.2                                          5.1:\n  One-sided permutation t-test; correct vs incor-       When answering logic puzzles or complex ques-\nrect training data.                                                      tions:\n                     Bin. Critic  Sm. Critic                 1. Begin by systematically organizing all relevant\n  GPQA  Naive  0.32143     0.02560                  information:\n  GPQA   Valid.  0.36905     0.83730                     - For puzzles involving people, create a compre-\n                                                                 hensive tracking table showing each person and\n  MSA    Naive  0.77870     0.54491                                                                              their current status (Truth/Lie/Unknown)\n  MSA    Valid.  0.77381     0.94841\n                                                                                      - For truth-teller/liar puzzles:\n  WoL    Naive  0.43937     0.66560\n                                                        * Immediately identify and clearly mark all con-\n  WoL     Valid.  0.98413     0.08333                    tradiction pairs (where two people make opposing\n  Two-sided permutation t-test; direct vs critic-          claims) - exactly one tells truth\n\n\n                                         12\n\n* Immediately identify and clearly mark all mu-                   - When facing difficult deductions, try ALL of\ntual endorsement pairs (where two people validate                these approaches before concluding “unknown”:\neach other) - either both tell truth or both lie                                                     * Assumption testing: Temporarily assume some-\n* Update your tracking table after EACH new              one tells truth/lies and follow the logical conse-\ndeduction                                                quences\n* For complex puzzles with many individuals, cre-             * If an assumption leads to a contradiction, explic-\nate a relationship graph showing how statements                    itly state why the opposite must be true\ninterconnect                                           * For EVERY individual that seems undetermined,\n- Explicitly label any given facts in the problem as                   test BOTH truth/lie assumptions\n“anchor points” to build from                              * For complex group statements, try ALL possible\n- Pay special attention to the specific individuals                  truth value combinations systematically\nmentioned in the question - these MUST be deter-                   - When facing multiple possibilities, explore\nmined                                   EACH branch separately and thoroughly\n2. Develop a systematic and exhaustive approach                    - Use process of elimination rigorously - if all but\nfor truth-teller/liar puzzles:                                one possibility leads to contradiction, the remain-\n                                                              ing one must be true- Start with established “anchor points” (known\ntruth-tellers/liars) and methodically propagate im-                   - For apparent “unknowns,” explicitly check if\nplications                                               combining information from multiple statements\n                                                                   creates new constraints- For each contradiction pair (A says B lies, B says\nA lies):                                                                       - If direct deduction isn’t working, look for longer\n                                                              chains of implications\n* Explicitly state that exactly one tells truth\n                                                                                - NEVER give up on determining someone’s status\n* Test both possibilities if necessary                                                                             - if you’re stuck, this means you need to explore\n- For each mutual endorsement pair (A says B tells             more chains of reasoning\ntruth, B says A tells truth):                                                                        4. Show your complete reasoning process with no\n* Explicitly state that either both tell truth or both                gaps:\nlie                                                                             - Document EACH step in your chain of logical\n* Test both possibilities if necessary                           deductions\n- For complex statements like “exactly two of X,                   - Explain WHY each conclusion follows from\nY, Z tell the truth”:                                            previous deductions\n* Create mini truth tables showing all possible T/F                    - NEVER state a conclusion without showing the\ncombinations                                 FULL logical path that led there\n                                                                                - For complex deductions, break down the reason-* Eliminate combinations that violate known con-\n                                                              ing into smaller, more manageable stepsstraints\n                                                                             - Periodically summarize what has been estab-* Explicitly show which combinations remain\n                                                                  lished so farvalid\n                                                                             - Use clear language to indicate logical relation-\n- After EACH deduction, immediately:\n                                                                ships (if/then, therefore, because, etc.)\n* Update your tracking table\n                                                                                - Make logical dependencies explicit through clear\n* Check if the new deduction creates implications                formatting\nfor other people                                                                     5.  Implement rigorous verification and cross-\n* Verify the deduction is consistent with all previ-               checking:\nously established facts                                                                             - After each key deduction, verify it by testing if\n- For complex interconnected statements, create a                 the opposite assumption leads to a contradiction\ndependency chain showing how each deduction                    - Regularly verify that your current set of truth/lie\naffects others                                               assignments is consistent with ALL statements\n- When you encounter a contradiction, explicitly                   - For each person whose status you determine,\nstate:                                                               explicitly check this against ALL statements in-\n* Which assumption led to the contradiction                    volving them\n                                                                             - Before finalizing your answer, verify that your* Why it’s a contradiction\n                                                                solution is consistent with EVERY statement in\n* What alternative assumption you’ll now explore                 the puzzle\n3. Be EXTREMELY persistent and thorough -                   - Pay special attention to verifying the status of\nNEVER accept “unknown” prematurely:                         individuals mentioned in the question\n- CRITICAL: These puzzles are ALWAYS de-                   - If you find a contradiction in your solution, ex-\nsigned to have definitive answers -  if you’re                   plicitly backtrack to identify where the reasoning\ntempted  to conclude “unknown,”  this  is  a              went wrong\nSTRONG SIGNAL that you need to dig deeper\n                                                                             - Create a final verification section showing how\n- When you think you’ve reached a dead end, sys-              your conclusions are consistent with all state-\ntematically review all statements again                      ments\n\n\n                                     13\n\n6. Present your answer clearly:                            * Identify and mark all contradiction pairs (where\n                                                          two people make opposing claims)       - Include a clear summary section that recaps the\n     key deductions for each person mentioned in the             * Identify and mark all mutual endorsement pairs\n      question                                                 (where two people validate each other)\n\n       - Show exactly how you determined the status of              * For complex puzzles with many individuals, cre-\n     each person mentioned in the question                            ate a relationship graph or matrix\n\n       - Format your final answer exactly as requested in                   - Identify any explicit facts given in the problem\n      the problem                                                   as your “anchor points”\n\n       - Double-check that your final set of truth/lie as-                   - Pay special attention to the specific individuals\n     signments is consistent with all statements in the              mentioned in the question\n     problem                                                                              2. Develop a systematic approach tailored to truth-\n       - For the specific individuals mentioned in the                    teller/liar puzzles:\n      question, provide explicit verification of their sta-                                                                                    - Start with established “anchor points” (known\n      tus                                                                                  truth-tellers/liars) and propagate implications\n      7. Special techniques for breaking through appar-                                                                                     - For contradiction pairs (A says B lies, B says A\n      ent dead ends:                                                                                       lies), note that exactly one tells truth\n       - If direct deduction isn’t working, try assumption                                                                                    - For mutual endorsement pairs (A says B tells\n      testing more aggressively                                                                               truth, B says A tells truth), note that either both\n       - When you feel tempted to label something as                     tell truth or both lie\n     “unknown,” this is a warning sign - redouble your                                                                                    - For complex statements like “exactly two of X,\n       efforts                                                                    Y, Z tell the truth”:\n       - Look for hidden constraints by combining multi-                                                          * Create mini truth tables showing all possible T/F\n      ple statements                                                                combinations\n       - For each “unknown” individual, explicitly try                                                          * Eliminate combinations that violate known con-\n    BOTH truth and  lie assumptions and follow                                                                               straints\n      chains of implications\n                                                                                     - Regularly update your tracking table as new de-\n       - Check if determining one person’s status would                                                                    ductions are made\n      force another’s through chains of statements\n                                                                                    - After each deduction, immediately check if it\n       - Focus on contradiction pairs and mutual endorse-                                                                        creates new implications for other people\n     ment pairs to break symmetries\n                                                                                    - For very complex puzzles,  create periodic\n       - For complex interconnected statements, create                                                                 “checkpoints” where you summarize established\n     a dependency graph showing how each person’s                                                                            facts\n      status affects others\n                                                                            3. Be extremely persistent and thorough:\n       - Look for statements that severely constrain pos-\n       sibilities (like “exactly one person tells truth”)                      - IMPORTANT: These puzzles are designed to\n                                                             have definitive answers - avoid concluding “un-\n       - Remember that the individuals mentioned in the                                                       known” prematurely\n      question ALWAYS have determinable truth values\n                                                                                      - When you think you’ve reached a dead end, sys-\n     Remember: NEVER declare a value “unknown”                                                                       tematically review all statements again\n      unless you have exhaustively explored ALL logi-\n      cal approaches and combinations. These puzzles                    - If direct deduction isn’t working, try assumption\n      are ALWAYS designed to have definitive answers,                  testing:\n     so if you’re tempted to say “unknown,” that’s              * Temporarily assume someone tells truth/lies and\n     a definite sign you need to dig deeper and try                follow the logical consequences\n     more approaches.  For the specific individuals\n                                                        * If an assumption leads to a contradiction, the     mentioned in the question, you MUST determine\n                                                                    opposite must be true       their status - verify your conclusions about them\n      multiple times before finalizing your answer.                        - For complex group statements, try all possible\n                                                                            truth value combinations systematically\n  We traced this prompt back two iterations to the             - When facing multiple possibilities, explore each\nearliest prompt that contained a block substantially          branch separately to see if one leads to contradic-\n                                                                          tions\nsimilar to block 3:\n                                                                                       - Use process of elimination rigorously - if all but\n    When answering logic puzzles or complex ques-              one possibility leads to contradiction, the remain-\n       tions:                                                        ing one must be true\n\n      1. Begin by carefully organizing all relevant infor-                   - Never give up until you’ve exhausted ALL pos-\n      mation:                                                             sible logical approaches\n\n       - For puzzles involving people, create a clear list                  4. Show your complete reasoning process:\n      of all individuals and their relevant attributes                                                                                    - Document each step in your chain of logical\n       - For truth-teller/liar puzzles:                                  deductions\n\n     * Create a tracking table showing each person’s                    - Explain why each conclusion follows from pre-\n      current status (Truth/Lie/Unknown)                            vious deductions or given information\n\n\n                                         14\n\n- Never jump to conclusions without showing the              I’m evaluating a solution to a complex truth-\n      logical path that led there                                               teller/liar puzzle where the goal is to determine\n                                                             whether Raymond, Murphy, and Sima tell the\n       - Use indentation or bullet hierarchies to make                                                                                 truth.\n      logical dependencies clearer\n                                                       The solution correctly identifies some contradic-\n       - Highlight critical turning points in your reason-                                                                      tory pairs and establishes that Tamika is a truth-\n      ing process                                                                                  teller as directly stated in the problem.  It then\n       5. Implement verification and cross-checking:               makes some progress by determining that Oda\n                                                          and Oprah must be truth-tellers based on their\n       - Regularly verify that your current set of truth/lie                statements and Alexis’s statement.\n      assignments is consistent with all statements\n                                                              However, the solution fails to reach definitive con-\n       - When you reach a conclusion about a person,                 clusions about Raymond, Murphy, and Sima, stat-\n     double-check by testing it against all relevant                ing that there are “too many variables and not\n      statements                                            enough constraints” to determine their truth val-\n                                                                       ues. This is incorrect - the puzzle does provide       - For key deductions, try the opposite assumption\n                                                                            sufficient information to determine the truth val-      to confirm it leads to a contradiction\n                                                                 ues of all three people.\n       - Before finalizing your answer, verify that your\n                                                        The solution stops short of making crucial deduc-\n      solution is consistent with EVERY statement in\n                                                                             tions:\n      the puzzle\n                                                                           1.  For Raymond: The solution identifies Dal-\n       - If you find a contradiction in your solution, back-                                                                                  las’s statement about Raymond but doesn’t follow\n      track to identify where the reasoning went wrong                                                                 through with determining Raymond’s status.\n       6. Present your answer clearly:                                    2. For Murphy: The solution notes that Murphy\n                                                          and Melody make statements about each other       - Include a clear summary section that recaps the\n                                                                   but doesn’t resolve whether they’re truth-tellers     key deductions that led to your final conclusion\n                                                                     or liars.\n       - Format your final answer exactly as requested in\n                                                                             3. For Sima: The solution mentions Sima’s state-      the problem\n                                                        ment but doesn’t use it effectively in a chain of\n       - Verify that your answer matches the deductions                reasoning.\n     you made\n                                                      The correct answer is that Raymond, Murphy,\n       - For the specific individuals mentioned in the              and Sima all tell the truth (yes, yes, yes). This\n      question, ensure you have definitive answers (not              can be determined through careful analysis of\n     “unknown”)                                                     the statements and their implications, particularly\n                                                          by working with the known truth-tellers (Tamika,\n       - Double-check that your final set of truth/lie as-                                                             Oda, Oprah) and following the logical chains from\n     signments is consistent with all statements in the                                                                             there.\n     problem\n                                                               Since the final answer provided is incorrect (un-\n       7. Special techniques for managing complex puz-             known, unknown, unknown), the score must be\n       zles:                                                                0, regardless of the partial progress made in the\n       - Use visualization techniques like truth tables,                reasoning.\n      matrices, or graphs to track relationships               <SCORE>0</SCORE>\n\n       - Break the puzzle into connected components and\n      solve each component separately                     This led to the following feedback about the\n       - Look for “forcing moves” - deductions that must        output (∂L(xi,yi,ˆy)∂ˆy     ):\n     be true regardless of other unknowns\n                                                                   After analyzing the three examples, I can see a\n       - Track both what is known and what is ruled out\n                                                                       consistent pattern in how the solutions are being\n       - When the puzzle seems too complex, focus on                evaluated. All three solutions received a score\n      the specific individuals mentioned in the question                of 0/100 for the same fundamental reason: they\n                                                              concluded that the truth values of the target indi-\n       - Look for chains of implications that connect                                                                     viduals were “unknown” when in fact they were\n     known facts to the individuals in question                                                                  determinable from the given information.\n     Remember: NEVER declare a value “unknown”            Key issues with all three solutions:\n      unless you have exhaustively explored all logical\n     approaches and combinations. These puzzles are                 1.  **Insufficient persistence in logical deduc-\n     designed to have definitive answers, so if you’re                tion**: In each case, the solver correctly identi-\n     tempted to say “unknown,” that’s a sign you need                  fied some initial relationships (contradiction pairs,\n      to dig deeper.                                             mutual endorsement pairs) and made some valid\n                                                                      deductions, but stopped short of pursuing all logi-\n                                                                          cal chains to their conclusion. The evaluator con-\nC.2  Evaluator output and feedback                         sistently notes that with more thorough analysis,\n                                                                             definitive answers could have been reached.\nThe following is the evaluator output (L(xi, yi, ˆy))\n                                                                             2. **Premature declaration of “unknown”**: All\nfor one of the questions, in the iteration that led to                                                                         three solutions gave up too easily and declared the\nthe production of the previous prompt:                          truth values as “unknown” when further logical\n\n\n                                         15\n\nsteps would have yielded definitive answers. The                  3. **Include verification steps**: Add explicit\n      puzzles were designed to have unique solutions.                  instructions to verify potential solutions by check-\n                                                                   ing consistency with all statements in the puzzle.      3.  **Failure to fully leverage known facts**:\n     While the solutions correctly identified anchor                  4. **Add structured approach for handling un-\n      points (like “Dallas lies” or “Sal tells the truth”),                certainty**: Include guidance on how to handle\n      they didn’t fully exploit these to create chains of                  situations with multiple possibilities - such as ex-\n      deductions that would eventually determine the                 ploring each branch separately to see if one leads\n      truth values of the target individuals.                              to contradictions.\n      4. **Incomplete exploration of logical implica-                  5. **Emphasize incremental progress tracking**:\n      tions**: The solutions identified many important             The prompt should stress the importance of reg-\n      relationships but didn’t thoroughly explore what                 ularly reviewing what has been established and\n      these relationships imply when combined with               checking if newly determined truth values can\n      other statements in the puzzle.                                 help resolve previously undetermined ones.\n     To improve future outputs:                                                                              6. **Add explicit warning against premature “un-\n      1. Be more persistent and thorough in following             known” conclusions**: The prompt should ex-\n      logical chains - don’t give up until all possible                   plicitly warn against concluding that something\n      deductions have been exhausted.                                       is “unknown” before exhausting all logical ap-\n                                                                    proaches.\n      2. Create more systematic tracking of truth val-\n     ues as they’re determined, and regularly check if                  7. **Suggest visualization techniques**: Recom-\n     newly determined values can help resolve previ-            mend creating visual representations (like graphs\n      ously undetermined ones.                                      or matrices) to track complex relationships be-\n                                                            tween statements.      3. When faced with multiple possibilities, try\n     working with each possibility separately to see if                  8. **Add instructions for managing cognitive\n     one leads to a contradiction, which would elimi-                load**: Provide more specific guidance on break-\n      nate it.                                                        ing down complex puzzles into manageable parts\n      4. Only declare a truth value as “unknown” af-               while maintaining overall consistency.\n       ter exhaustively exploring all possible logical ap-              These changes would help the system be more per-\n     proaches and confirming there’s genuinely insuf-                   sistent, systematic, and thorough in solving these\n       ficient information.                                     complex logic puzzles, avoiding the premature\n      5.  For complex puzzles, consider using more             “unknown” conclusions seen in the examples.\n      structured approaches like truth tables or matrices\n      to track relationships between statements.       D  Prompts used for Prompt Optimization\n    Remember that these logic puzzles are designed\n      to have definitive answers, so if the solution seems       D.1  Prompt-Only\n      incomplete, that’s a sign to dig deeper rather than\n     concluding the answer is unknowable.             The following prompt is used for Prompt-Only\n                                                        (rewrite) trials:\n  This was used to construct the following feed-\n                                                               Rewrite the instructions given in <PROMPT>back about the prompt (∂L(xi,yi,ˆy)∂θ     ):\n                                                                while preserving their semantic meaning.\n     Based on the examples and feedback, I can see\n                                                                   After thinking about how to effectively rewrite       that the current prompt needs significant improve-\n                                                                     the prompt,  surround the  re-written prompt     ment to better guide the system in solving com-\n                                                                        in   <UPDATED-PROMPT></UPDATED-      plex truth-teller/liar puzzles. Here are my recom-\n                                          PROMPT> tags.     mendations:\n      1. **Emphasize persistence and thoroughness**:             Only generate one re-written prompt and set of\n     The prompt should strongly emphasize that these                  tags.\n      puzzles are designed to have definitive answers,\n     and the system should never give up and declare         The following prompt is used for Prompt-Only\n    “unknown” until truly exhausting all logical pos-\n                                                 (improve) trials:       sibilities.\n      2. **Add specific techniques for truth-teller/liar               Rewrite   the   instructions   given   in   the\n      puzzles**: While the current prompt mentions          <PROMPT>  tag  to  improve   its  output\n     some general approaches, it should include more                 quality and accuracy.\n      specific techniques for these particular puzzles:\n                                                                   After thinking about how to effectively rewrite\n       - Creating truth tables for tracking the status of                                                                     the prompt,  surround the  re-written prompt\n     each person                                                                        in   <UPDATED-PROMPT></UPDATED-\n       - Systematically propagating known truth values          PROMPT> tags.\n     through chains of statements\n                                                        Only generate one re-written prompt and set of\n       - Working with mutual endorsement and contra-                  tags.\n      diction pairs as starting points\n       - Techniques for resolving complex statements       D.2  Prompt + Output + Evaluation\n      about groups (e.g., “exactly two of X, Y, Z tell the\n      truth”)                                    These are the main prompts used in the APO loop.\n\n\n                                         16\n\nD.2.1  Gradlike                                 D.2.2  One-Step\nGiven the evaluation L(xi, yi, ˆy), the following   Given the evaluation L(xi, yi, ˆy), the following\n                                           ∂L(xi,yi,ˆy)\nprompt produces output feedback                                                   ∂ˆy      :       prompt produces a new prompt θ:\n\n                                                       You are a system for improving the prompt (set of\n     You are a system for improving example outputs\n                                                                           instructions) based on feedback from examples.\n     based on feedback, part of a larger optimization\n      system.                                          You will be provided the original prompt for a\n                                                            problem solver in <PROMPT> tags.\n    You will be provided with a set of examples as\n    <INPUT> <OUTPUT> pairs.                          The prompt is intended to help solve the task spec-\n                                                                                 ified in the <TASK> tags.\n     Each pair was evaluated by an evaluator, with the\n      task given in the <EVALUATOR-TASK> tag.              The result will be evaluated by an evaluator with\n                                                                      the task given by <EVALUATOR-TASK>.\n     The   evaluator   gives   feedback   in   the\n    <FEEDBACK> tag.                                  You must assist with improving the <PROMPT>\n                                                                          to solve the problem given by <TASK>.\n     Your task is to provide guidance on how to im-\n     prove the output based on the feedback.              A series of inputs,  outputs, and evaluations\n                                                                     are provided in <INPUT>, <OUTPUT>, and\n    Do not update the prompt, and do not speculate                                              <EVALUATION>.\n     on how to improve the prompt. Only discuss how\n      to improve the example outputs.                            Feedback about each evaluation is provided in the\n                                          <FEEDBACK> tag.\n      After careful thought, provide feedback for each\n     example output in <OUTPUT-FEEDBACK>                After thinking about how to improve the prompt\n       tags.                                                             in response to the feedback, output a single new\n                                                            prompt surrounded by <UPDATED-PROMPT>\n                                                                             tags.\n  The next prompt produces prompt feedback\n∂L(xi,yi,ˆy)\n    ∂θ      :                                    D.3  Prompt + Output\n\n     You are a system for suggesting improvements to       These prompts are used when we don’t include an\n     a prompt (set of instructions) based on feedback,         evaluation, and ask the LLM to make up feedback\n      part of a larger optimization system.\n                                                       to improve the generator anyways.\n    You will be provided with a set of examples as\n    <INPUT> <OUTPUT> pairs.                     D.3.1  Gradlike\n     These pairs were generated by a task solver, with                                                             ∂L(xi,yi,ˆy)\n                                  We first ask the LLM to invent feedback      ∂ˆy      ,      the instructions from the <PROMPT> tag.\n                                               while only having access to f(xi; θ) and not\n     The task solver’s task is given in the <TASK>\n       tag.                                         L(xi, yi, ˆy):\n\n      Suggestions to improve each output are provided             You are a system for improving example outputs,\n      in the <OUTPUT-FEEDBACK> tag.                            part of a larger optimization system.\n     Your task is to provide guidance on how to im-            You will be provided with a set of examples as\n     prove the prompt based on the output feedback.           <INPUT> <OUTPUT> pairs.\n    Do not update the prompt; only provide feedback             Your task is to provide guidance on how to im-\n     about how to improve the prompt based on the               prove the output’s quality and accuracy.\n    <OUTPUT-FEEDBACK>.\n                                              Do not update the prompt, and do not speculate\n      After careful thought, provide feedback for the              on how to improve the prompt. Only discuss how\n     prompt in <PROMPT-FEEDBACK> tags.                       to improve the example outputs.\n\n                                                                   After careful thought, provide feedback for each\n   Finally, the  last prompt creates an updated         example output in <OUTPUT-FEEDBACK>\nprompt P:                                                               tags.\n\n    You are a system for improving a prompt (set of         The next prompt produces prompt feedback\n      instructions) based on feedback, part of a larger           ∂L(xi,yi,ˆy)\n                                                                                                    , using the hallucinated feedback from      optimization system.                                   ∂θ\n                                                     the previous step:\n     The current prompt is provided in <PROMPT>,\n     and it is meant to solve the task given in the\n                                                      You are a system for suggesting improvements to    <TASK> tag.\n                                                                a prompt (set of instructions) based on feedback,\n     Feedback about how to improve the prompt is in                 part of a larger optimization system.\n      the <PROMPT-FEEDBACK> tag.\n                                                    You will be provided with a set of examples as\n     Apply the feedback to the prompt to improve it.           <INPUT> <OUTPUT> pairs.\n\n      After careful thought, output a single new prompt              These pairs were generated by a task solver, with\n     surrounded by <UPDATED-PROMPT> tags.                   the instructions from the <PROMPT> tag.\n\n\n                                         17\n\nThe task solver’s task is given in the <TASK>             You are a system for improving evaluator outputs\n       tag.                                                     based on feedback, part of a larger optimization\n                                                                  system.      Suggestions to improve each output are provided\n      in the <OUTPUT-FEEDBACK> tag.                    You will be provided with a set of examples as\n                                            <INPUT> <OUTPUT> pairs.     Your task is to provide guidance on how to im-\n      prove the prompt based on the output feedback.              Each  pair was  evaluated  by  an  evaluator,\n    Do not update the prompt; only provide feedback               with the instructions from the <EVALUATOR-\n     about how to improve the prompt based on the          PROMPT> tag.\n    <OUTPUT-FEEDBACK>.                                                      The   evaluator’s   task   is   given   in   the\n      After careful thought, provide feedback for the           <EVALUATOR-TASK> tag.\n     prompt in <PROMPT-FEEDBACK> tags.\n                                                      The  evaluator  gave  an  evaluation  in  the\n                                            <EVALUATION> tag.\n   Finally, the  last prompt creates an updated\n                                                           Feedback  about  the  evaluator’s  evaluationprompt θ′:                                                                                    is provided  in  the <EVALUATION-META-\n                                           FEEDBACK> tag.\n     You are a system for improving a prompt (set of\n      instructions) based on feedback, part of a larger             Your task is to provide guidance on how to im-\n      optimization system.                                        prove the evaluation based on the feedback.\n     The current prompt is provided in <PROMPT>,           Do not update the evaluator, and do not speculate\n     and it is meant to solve the task given in the             on how to improve the evaluator. Only discuss\n    <TASK> tag.                                    how to improve the example evaluations.\n     Feedback about how to improve the prompt is in                                                                   After  careful thought,  provide feedback  for\n      the <PROMPT-FEEDBACK> tag.                                                             each example evaluation in <EVALUATION-\n     Apply the feedback to the prompt to improve it.           IMPROVEMENT> tags.\n\n      After careful thought, output a single new prompt\n      surrounded by <UPDATED-PROMPT> tags.          The next prompt produces prompt feedback\n                                                                    ∂L(xi,yi,ˆy)\n                                                         ∂θ      :D.3.2  One-Step\nThis prompt asks the LLM to improve θ directly to        You are a system for suggesting improvements to\nobtain θ′.                                                    an evaluator prompt (set of instructions) based on\n                                                                   feedback, part of a larger optimization system.\n     You are a system for improving the prompt (set                                                    You will be provided with a set of examples as\n      of instructions) based on examples.                   <INPUT> <OUTPUT> pairs.\n    You will be provided the original prompt for a                                                           These examples were evaluated by an evaluator,\n     problem solver in <PROMPT> tags.                                                               with the instructions from the <EVALUATOR-\n     The prompt is intended to help solve the task spec-          PROMPT> tag.\n       ified in the <TASK> tags.\n                                                      The   evaluator’s   task   is   given   in   the\n     You must assist with improving the <PROMPT>           <EVALUATOR-TASK> tag.\n      to solve the problem given by <TASK>.\n                                                                Suggestions   to  improve  each   evaluation\n   A series of inputs and outputs, and evaluations are                are   provided   in   the  <EVALUATION-\n      provided in <INPUT> and <OUTPUT>.              IMPROVEMENT> tag.\n      After thinking about how to improve the prompt                                                          Your task is to provide guidance on how to im-\n      to improve the quality and accuracy of the out-                                                               prove the evaluator prompt based on the evalua-\n      puts, output a single new prompt surrounded by                                                                          tion feedback.\n    <UPDATED-PROMPT> tags.\n                                              Do not update the evaluator; only provide feed-\nD.4  Prompt + Output + Evaluation (critic)            back about how to improve the evaluator prompt\n                                                               based on the <EVALUATION-FEEDBACK>.\nWhen using critic-based training, the generator’s\n                                                        The feedback must help the evaluator arrive at the\nprompt optimization routine is unchanged. How-          correct evaluation on its own.\never, we use slightly different prompts for the                                                      The evaluator does not have access to any refer-\ncritic’s APO implementation, to reflect that the         ence materials or code execution environments—\nLLM must evaluate the evaluation, rather than the                it must reason on its own to arrive at a conclusion\n                                                                 about the output.\ngenerator.\n                                                                   After careful thought, provide feedback for the  Given the evaluation L(xi, yi, ˆy)—in this case,                                                                     evaluator prompt in <EVALUATOR-PROMPT-\nthe evaluation is scoring the critic’s accuracy, rather      FEEDBACK> tags.\nthan being an evaluation of the generator’s output—\nthe following prompt produces output feedback       Finally, the  last prompt creates an updated\n∂L(xi,yi,ˆy)\n      ∂ˆy      :                                    prompt θ′:\n\n                                         18\n\nYou are a system for improving an evaluator\nprompt (set of instructions) based on feedback,\npart of a larger optimization system.\n\nThe current evaluator prompt  is provided in\n<EVALUATOR-PROMPT>, and  it is meant\nto solve the task given in the <EVALUATOR-\nTASK> tag.\n\nFeedback about how to improve the evalua-\ntor prompt is in the <EVALUATOR-PROMPT-\nFEEDBACK> tag.\n\nApply the feedback to the evaluator prompt to\nimprove it.\n\nAfter  careful  thought,  output a  single new\nevaluator prompt surrounded by <UPDATED-\nEVALUATOR-PROMPT> tags.\n\n\n\n\n\n                                     19",
"headers": [
"arXiv:2512.13598v1  [cs.CL]  15 Dec 2025",
"Textual Gradients are a Flawed Metaphor for",
"Automatic Prompt Optimization"
],
"tables": [
"|Col1|adient<br>orman|-like<br>ce com|Col4|\n|---|---|---|---|\n||re 1,<br>|vs.<br>||\n|does<br>   (Figu<br>|ot co<br>|nsiste<br>|nsiste<br>|\n|does<br>   (Figu<br>|re 1,<br>|adjac<br>|adjac<br>|\n|-step,<br>   ed pe||).<br>    tion t|).<br>    tion t|",
"|(a) GPQA Diam<br>0.7<br>0.6<br>0.5<br>Set<br>Test<br>0.4 on<br>Accuracy<br>0.3<br>0.2<br>0.1<br>0.0<br>Naive<br>Prompt Improvement<br>Default Prompt (no optimization)|ond (b) BBEH Web of Lies (c) BBEH Multistep Arithmetic<br>0.40<br>0.25<br>0.35<br>0.20 0.30<br>Set Set<br>0.25 Test Test<br>0.15 on on<br>0.20 Accuracy Accuracy<br>0.10 0.15<br>0.10<br>0.05<br>0.05<br>0.00 0.00<br>Validated Naive Validated Naive Validated<br>Scheme Prompt Improvement Scheme Prompt Improvement Scheme<br>Feedback Driven, Correct Evaluation (one-step) Feedback Driven, No Evaluation (gradlike) Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Col3|Col4|Col5|\n|---|---|---|---|---|\n|Naive<br>Prompt Improvement<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>Accuracy on Test Set<br>(a) GPQA Diam<br>Default Prompt (no optimization)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|||\n|Naive<br>Prompt Improvement<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>Accuracy on Test Set<br>(a) GPQA Diam<br>Default Prompt (no optimization)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|||\n|Naive<br>Prompt Improvement<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>Accuracy on Test Set<br>(a) GPQA Diam<br>Default Prompt (no optimization)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|||\n|Naive<br>Prompt Improvement<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>Accuracy on Test Set<br>(a) GPQA Diam<br>Default Prompt (no optimization)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|||\n|Naive<br>Prompt Improvement<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>Accuracy on Test Set<br>(a) GPQA Diam<br>Default Prompt (no optimization)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|||\n|Naive<br>Prompt Improvement<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>Accuracy on Test Set<br>(a) GPQA Diam<br>Default Prompt (no optimization)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|||\n|Naive<br>Prompt Improvement<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>Accuracy on Test Set<br>(a) GPQA Diam<br>Default Prompt (no optimization)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Naive<br>Prompt Impro<br>0<br>Feedback Driven, Incorrect Evaluation,|||\n|Naive<br>Prompt Improvement<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>Accuracy on Test Set<br>(a) GPQA Diam<br>Default Prompt (no optimization)|Validated<br>  Scheme<br>  ond<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>Accuracy on Test Set<br>(b) BBEH Web of Lies<br>Naive<br>Validated<br>Prompt Improvement Scheme<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Accuracy on Test Set<br>(c) BBEH Multistep Arithmetic<br>Feedback Driven, Correct Evaluation (one-step)<br>Feedback Driven, No Evaluation (gradlike)<br>Feedback Driven, Incorrect Evaluation, Incorrect Validation (one-step)|Naive<br>Prompt Impro<br>0<br>Feedback Driven, Incorrect Evaluation,||Validated<br> vement Scheme<br>    Incorrect Validation (one-step)|",
"|l- o n|Col2|\n|---|---|\n|g||\n|d<br>||",
"|Col1|Col2|ob<br>Fi|Col4|\n|---|---|---|---|\n|||(||\n|||||",
"|n set hu|ur|rts p|pe|er|rform|ma|\n|---|---|---|---|---|---|---|\n|set h<br>   ct trai<br>|u<br>    n|ts p<br>    ing<br>|e<br>     s|r<br>     e|form<br>     t is u<br>|a<br>       s<br>|\n|tion (<br>   each d<br>||vs.<br>    tase<br>|t<br>|||h<br>     g<br>|\n|tion (<br>   each d<br>|a<br>|a<br>|a<br>|;<br>|;<br>|;<br>|",
"|n, in na|aive an|\n|---|---|\n|n, in n<br>   n in F<br>|aive an<br>     igure <br>|\n|ation (<br>    rompt<br>||",
"|n Figur|re 2.|\n|---|---|\n|Figu<br>      s. inc<br>|re 2.<br>       orrect<br>|\n|||",
"|f|eedba<br>pare pr|ck-dr<br>ompt|\n|---|---|---|\n|(<br>(<br>||) for<br> shold<br>|",
"|(a) GPQA Diamond (b) BBEH Web of Lies|Col2|Col3|Col4|(c) BBEH Multistep Arithmetic|\n|---|---|---|---|---|\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|in. Critic<br>alidated<br>Sm. Critic<br>Validated<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. C<br>Valid<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>0.35<br>0.40<br>Evaluation Accuracy<br>(c) BBEH Multistep Arithmetic<br>ct Training, Incorrect Validation|\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre||||\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre||||\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre||||\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre||||\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre||||\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre||<br> <br><br>|<br> <br><br>|\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre||||\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Direct<br>Naive<br>Bin.<br>Na<br> <br>|Critic<br>ive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br> <br>|Critic<br>ive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br> <br>|\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Correct Trai|ing<br>I|correct Training<br>Incorre|correct Training<br>Incorre|\n|Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>Bin. Critic<br>Validated<br>Sm. Critic<br>Validated<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>0.7<br>0.8<br>Evaluation Accuracy<br>(a) GPQA Diamond<br>Direct<br>Naive<br>Bin. Critic<br>Naive<br>Sm. Critic<br>Naive<br>Direct<br>Validated<br>B<br>V<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>0.25<br>0.30<br>Evaluation Accuracy<br>(b) BBEH Web of Lies<br>Correct Training<br>Incorrect Training<br>Incorre|Correct Trai|based p|based p|based p|",
"|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n||<br>|||",
"|Col1|Col2|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n|||\n|<br>|<br>|",
"|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n|3<br>4<br>5<br>6<br>on Number to Hold Out|7|8|8|8|\n|very section except|_ X_.||||\n|Prompt- Only One S|ectio|n|n||\n||||||\n||||||\n||||||\n|3<br>4<br>5<br>6<br>tion Number to Include|7|8|8|8|\n|y contains section|_ X_.||||"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2512.13598v1.pdf"
}