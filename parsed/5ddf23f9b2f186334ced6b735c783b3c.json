{
"text": "Boosting Private Domain Understanding of Efficient MLLMs: A\n             Tuning-free, Adaptive, Universal Prompt Optimization Framework\n\n                 Jiang Liu1, †, Bolin Li2, †, Haoyuan Li2, †, Tianwei Lin1, Wenqiao Zhang1, *, Tao Zhong2, Zhelun Yu2, Jinghao Wei2,\n            Hao Cheng2, Wanggui He2, Fangxun Shu2, Hao Jiang2, *, Zheqi Lv1, *, Juncheng Li1, Siliang Tang1, Yueting Zhuang1\n                                                               1Zhejiang University\n                                                                 2Alibaba Group\n\n                                         {jiang.liu, lintw, wenqiaozhang, zheqilv, junchengli, siliang, yzhuang}@zju.edu.cn;\n\n             {libolin.lbl, lihaoyuan.lhy, zt395565, yuzhelun.yzl, weijinghao.wjh, chenghao.x, wanggui.hwg, shufangxun.sfx, aoshu.jh}@taobao.com\n\n\n                          Abstract                        costs compared to MLLMs, making them ideal for\n                                                        deployment on resource-constrained devices for\n                    Efficient multimodal large language models\n                                                        enhanced processing efficiency (Jin et al., 2024).\n              (EMLLMs), in contrast to multimodal large\n                                                          However, the scenarios encountered by different de-                 language models (MLLMs), reduce model size\n                                                                     vices, including various private business contexts,                and computational costs and are often deployed\n               on resource-constrained devices.  However,      can vary significantly. Moreover, ethical consid-2025\n                due to data privacy concerns, existing open-       erations and the necessity to protect trade secrets\n                  source EMLLMs rarely have access to private       prevent the use of device data for pre-training by\n                  domain-specific data during the pre-training       open-source, general-purpose EMLLMs. Conse-Feb\n                   process, making them difficult to directly apply                                                                   quently, a discrepancy between the distribution of\n                    in device-specific domains, such as certain busi-17                                                               pre-training data and device-specific data emerges,\n                  ness scenarios. To address this weakness, this\n                                                               leading to reduced performance or even failure                  paper focuses on the efficient adaptation of EM-\n            LLMs to private domains, specifically in two      when EMLLMs process this unique and highly pri-\n                    areas: 1) how to reduce data requirements, and        vate domain data.\n                  2) how to avoid parameter fine-tuning. Specif-     A viable solution involves utilizing EMLLMs,[cs.AI]              ically, we propose a tunIng-free, aDaptivE,         e.g., InternVL2-2B (Chen et al., 2023b, 2024) and\n                universAL Prompt Optimization Framework,                                                Qwen2-VL-2B (Wang et al., 2024), to perform\n                  abbreviated as IDEALPrompt which consists\n                                                                      either full-parameter (Chung et al., 2024; Zhang\n                  of two stages: 1) Predefined Prompt, based on\n                                                                         et al., 2024b) or efficient fine-tuning (Hu et al.,                   the reinforcement searching strategy, generate a\n                                                          2021; Lin et al., 2024a) on private device data                prompt optimization strategy tree to acquire op-\n                   timization priors; 2) Prompt Reflection initial-        for domain-specific adaptation (Chang et al., 2019;\n                   izes the prompt based on optimization priors,      Zhang et al., 2022, 2024c; Li et al., 2023a). How-\n                  followed by self-reflection to further search and        ever, this approach incurs large training costs, ne-\n                     refine the prompt. By doing so, IDEALPrompt        cessitates large-memory GPUs, and entails long\n                   elegantly generates the “ideal prompts” for pro-                                                                    training times. The elevated training costs signifi-\n                   cessing private domain-specific data. Note that\n                                                                 cantly increase the application threshold, thereby\n                 our method requires no parameter fine-tuning\n                                                                        restricting the availability and generalizability of                and only a small amount of data to quickly\n                 adapt to the data distribution of private data.      such methods on private domain data. Therefore,arXiv:2412.19684v2                 Extensive experiments across multiple tasks       our goal is to minimize the adaptation cost for EM-\n                 demonstrate that our proposed IDEALPrompt     LLMs when deployed on devices utilizing private\n                    significantly improves both efficiency and per-       domain-specific data.\n                formance compared to baselines.                                                                  In response to the aforementioned challenges,\n                                             we propose a tunIng-free, aDaptivE, universAL\n          1  Introduction\n                                                  Prompt optimization framework, referred to as\n             In recent years, multimodal large language models   IDEALPrompt. Our bootstrapping philosophy\n          (MLLMs) have achieved significant advancements,   aims to strengthen EMLLMs’ capability in private\n               particularly in cross-modal information understand-   domain tasks by progressively and adaptively opti-\n             ing and integration (Huang et al., 2023; Gong et al.,   mizing the prompt. IDEALPrompt consists of two\n            2023; Ye et al., 2023; Li et al., 2023b). Efficient    progressive optimization steps: i) Reinforcement\n            multimodal large language models (EMLLMs) sig-  Warm-up Strategy (RWS), which employs a re-\n              nificantly reduce model size and computational   inforcement learning tree search strategy for the\n\n\n                                                    1\n\nFigure 1: (a) shows the gap between public domain data and private domain data. (b) describes the simplified\nversion of our IDEALPrompt. (c) illustrates that compared to baseline methods, our method achieves superior\nperformance on Taobao-PDA benchmark.\n\ngeneral prompt optimization, encouraging combi-   minimizing computational costs. Our contributions\nnations of diverse prompt strategies that are most    are as follows:\neffective given the context of the current task. This     • To the best of our knowledge, this is the first\nstep enables the framework to gather prior knowl-     study that focuses on the efficient adaptation of\nedge that can be adapted across tasks and models,      on-device EMLLMs to private data distributions\nsignificantly reducing iteration cycles while main-       that differ significantly from the pre-trained data\ntaining performance. It effectively functions as a       distribution.\n“Pre-Training” phase for task-specific prompt engi-     • We meticulously devise a pre-made prompt\nneering, similar to the pre-training in the MLLMs’      dataset for multimodal business scenarios, called\ntraining paradigm, thereby encouraging the frame-    Taobao-PDA, which serves as fundamental\nwork’s adaptability to varied task requirements. ii)      data support for promoting private domain un-\nEmpirical Self-reflective Optimization (ESO),      derstanding.\nfurther refines the prompts by selecting critical bad     • We propose IDEALPrompt, a tuning-free, adap-\ncases from the error distribution of inference re-       tive, universal prompt optimization framework,\nsults and incorporating self-reflection. By focusing     which leverages a two-stage design to facilitate\non these critical bad cases, IDEALPrompt learns to    EMLLMs’ content understanding of private do-\noptimize prompts by identifying and correcting am-     main data.\nbiguous task descriptions and labels. This step al-     • Extensive and comprehensive experiments are\nlows the framework to continuously improve its un-     conducted and followed by  detailed analy-\nderstanding of prompt definitions in private domain        sis, which demonstrates the superiority of our\ntasks, functioning as a “Supervised Fine-Tuning”     method compared to the baselines.  Notably,\nphase for prompt refining, similar to the super-     our method even outperforms the fine-tuning\nvised fine-tuning in the MLLMs’ training paradigm,     method on proposed Taobao-PDA.\nthereby boosting the framework’s performance.\n  To evaluate our method in private domain adap-   2  Related Work\ntation thoroughly, we introduce the Taobao Pri-\nvate Domain Adaptation (Taobao-PDA), a new   Prompt Optimization. Prompt optimization can\nChinese benchmark comprising multiple complex   enhance the performance of MLLMs without pa-\nmultimodal tasks, which is able to evaluate the    rameter fine-tuning, and several methods have been\nprivate domain understanding capability of EM-   proposed to address the challenges of automated\nLLMs. The results of the experiment demonstrate   prompt optimization. Recent works focus on dis-\nthat IDEALPrompt exhibits strong generalization    crete optimization methods, such as GRIPS (Prasad\nand adaptability across multiple tasks, achieving    et al., 2022) and APO (Pryzant et al., 2023), which\nrobust performance in content understanding while    utilize edit-based operations to generate various\n\n\n                                         2\n\ncandidate prompts for subsequent optimization.   3  Methodology\nOther approaches leverage advanced algorithms\n                                                  This section describes the details of IDEALPrompt\nfor prompt optimization, including EvoPrompt\n                                                  (Figure 2). We will present each module and its\n(Guo et al., 2023), PromptBreeder (Fernando et al.,\n                                                   optimization procedure.\n2023), AELP (Hsieh et al., 2023), and PhaseEvo\n(Cui et al., 2024), which leverage evolutionary algo-                                                    3.1  Problem Formulation and Notations\nrithms to produce different prompts; RLPROMPT\n                                              Data. The input query of the on-device private\n(Deng et al., 2022) and PRewrite (Kong et al., 2024)\n                                                   data is represented by X (e.g., image for vision-\napply reinforcement learning techniques; Promp-\n                                               language models). High-value critical bad cases\ntAgent (Wang et al., 2023) and Agent-Pro (Zhang\n                                             sampled by IDEALPrompt from X is represented\net al., 2024a) utilize agent-based methods. APE\n                                          by X sub ⊆X. Correspondingly, we use Y, and\n(Zhou et al., 2022) generates candidate prompts us-\n                                                 Ysub to represent their ground truths. For a sample,\ning MLLMs, followed by a resampling of those\n                                      x and y represent a sample in X and Y respectively.\nprompts. OPRO (Yang et al., 2023a) leverage\n                                            Given a multimodal understanding task-t that is\nMLLMs as prompt optimizers, iteratively gener-\n                                                    characterized by private domain data distribution\nating superior prompts by considering previously\n                                                Dt. Prompt set is represented by V, and a promptgenerated solutions with their values. Additionally,\n                                                        in the V is represented by v.\nsome research reformulates prompt optimization\n                                            Model. The EMLLM adapted to Dt is representedas a continuous optimization problem, as seen in\n                                          by GE, with parameters Θ. The general MLLMInstructZero (Chen et al., 2023a), INSTINCT (Lin\n                                                        that assists the adaptation process of GE is repre-et al., 2024c), and ZOPO (Hu et al., 2024). APOHF\n                                                    sented by GM. fE(·), fM(·) represents the forward(Lin et al., 2024b) prompt optimization based on\n                                                   propagation process of GE, GM respectively, for ex-human preference feedback rather than specific nu-\n                                                 ample, fE(v, x; Θ) is the process of the predictionmerical scores. IPC (Levi et al., 2024) aims to\n                                       made by GE based on the prompt v and the inputoptimize prompt engineering for MLLMs by utiliz-\n                                               query x with parameters Θ.ing synthetic case samples.\n                                            Formula. Our proposed IDEALPrompt avoids pa-\n                                                 rameter fine-tuning; therefore, the comparison ofMultimodal Chain-of-Thought Prompting. Re-\n                                                formulas between parameter fine-tuning and IDE-cent works indicate that multimodal Chain-of-\n                                        ALPrompt is as follows:Thought reasoning significantly enhances the un-\n                                             Parameter Fine-tuning: Optimize the parametersderstanding capabilities of MLLMs. MM-CoT\n                            Θ based on the training data X.(Zhang et al., 2023) develops a two-stage frame-\nwork where the model first learns to produce ra-\n                                       max E([v,X],Y)∼Dth(fE(v, X; Θ), Y)    (1)tionales using ground-truth annotations and then        Θ\nemploys all available information to generate the fi-\n                                           IDEALPrompt: Find the optimal v and X sub, with-nal answer. DDCoT (Zheng et al., 2023) integrates\n                                                   out parameter fine-tuning and without optimize onmultimodality into reasoning by initially separating\n                                                     the whole dataset.the responsibilities of MLLMs into reasoning and\nrecognition, then incorporating the visual recog-\n                                     max    E([v,X sub],Ysub)∼Dth(fE(v, X sub; Θ), Ysub),nition capabilities of visual models into the com-   v∈V,X sub⊆X\nbined reasoning process. The work of Yang et al.                                                    (2)\n(2023b) introduces Question-Driven Visual Explo-   where h(·)  is score function applied to mea-\nration (QVix), which leverages MLLMs to gener-   sure the alignment between the EMLLM’s output\nate input-exploratory questions, guiding MLLMs   fE(v, x; Θ) and the ground truth y.\nto explore visual content more comprehensively\n                                                    3.2  IDEALPromptand uncover subtle or peripheral details. CCoT\n(Mitra et al., 2024) first generates a scene graph  We propose IDEALPrompt, a tuning-free, adap-\nusing the MLLMs and then uses that scene graph     tive, prompt optimization framework for EMLLMs,\nin the prompt to produce a response. These multi-   which consists of two progressive optimization\nmodal Chain-of-Thought methods rely on guiding    stages. Specifically, at the outset, several prompt\nMLLMs through multiple steps reasoning to en-   optimization strategies are designed using human\nhance performance.                                    priors, referred to as the Strategy Pool S, which\n\n\n                                         3\n\ntask t, model m                        Human-aligned Strategy Pool\n     Memory of\n    task t、model m      ε prob.           1-ε prob.       Reasoning          Role-Prompting      Caption          Strategies\n                              Exploration        Exploita          Reinterpretation    Decomposition      Rephrasing\n                          prompt             tion            Simplification        Self-Criticism\n\n\n      no          strategy                                            . . .                                                         coarse                          select         selected\n                      1-ε prob.            ε prob.      optimized                              Bad             Strategy\n                                                         prompt                               Cases         Reward\n                               Exploitation       Explora                        Error Dist.\n                          prompt             tion                       infer\n      \"Reasoning\"                                                                              Error\n         selected                                                              Accuracy         Causes\n                                                                         . . .                                               analyze                      GPT-4o\n                                                                          iterate\n                                                                                                 summarize\n\n                                                                        . . .\n    \"Reinterpretation\"                                                        generate         selected                                                          refined                Task Optimization\n                                                                                 Label Definitions         EMLLM       ...                                              prompt\n                      prompt+   +   +                                        ...\n\n       (a) Reinforcement Warm-up Strategy                 (b) Empirical Self-reflective Optimization        Symbols\n\nFigure 2: The architecture of IDEALPrompt. It includes Strategy Pool, Reinforcement Warm-up Strategy and\nEmpirical Self-reflective Optimization, avoiding parameter fine-tuning and requiring only a small amount of data\nfor efficient adaptation on the device.\n\ndirectly guides the prompts’ optimization direction.   provided in Appendix A.2.\nBuilding on this, in the first stage, Reinforcement     Note that the strategies in strategy pool can be\nWarm-up Strategy (RWS), a strategy tree search    freely modified, removed or expanded to flexibly\nbased on reinforcement learning is conducted to    adapt various task requirements. In this work, we\nacquire prior knowledge of optimization strategies.   showcase the aforementioned strategies adapting\nIn the second stage, Empirical Self-reflective Opti-    to private domain data as our strategy pool.\nmization (ESO), the error distribution derived from\n                                                      3.2.2  Reinforcement Warm-up Strategythe validation inference results is analyzed to iden-\ntify critical bad cases, and self-reflection is subse-   Based on these human-aligned strategies within\nquently applied to refine the prompts.                the strategy pool, Reinforcement Warm-up Strat-\n  During the optimization process, a highly capa-   egy (RWS) conducts a reinforcement learning\nble general MLLM is utilized as the prompt opti-    exploration-exploitation strategy tree search across\nmizer, while EMLLMs are employed as the prompt    tasks and models to optimize prompts, as illustrated\ninference model.                                      in Figure 2 (a).\n                                                             Specifically, the strategy nodes sk ∈S constitute\n3.2.1  Human-aligned Strategy Pool                                                    the action space of reinforcement learning, while\nWhile some works have used MLLMs for text    the evaluation results of private domain task-t on\ngradient-based prompt optimization, we instead  EMLLM-GE are considered action rewards. The\nleverage human-designed prompt optimization   reward distribution of actions is maintained in the\nstrategies constructed a priori that effectively opti-  memory module M. Note that we maintain a dis-\nmize prompts.                                            tinct distribution for each task type within each\n  The work of (Schulhoff et al., 2024) indicates a  EMLLM, i.e., M = {Mt0,GE0, . . . , Mtn,GEm}.\nwide range of general prompt optimization strate-      In the initial state of RWS, the framework has\ngies and has demonstrated the effectiveness of these    not encountered any private domain tasks nor per-\nstrategies across various domains. We carefully   formed any strategy search on any EMLLMs, i.e.,\nselect several representative strategies that are ben-   the memory module M is empty.  When the\neficial, as identified by human experts, to form  EMLLM-GE0 encounters the first private domain\nour Strategy Pool S = {s1, s2, . . . , sk}, where    task-t0, the framework performs a complete strat-\nsk denotes each individual strategy. Specifically,   egy tree search based on the human-aligned strate-\nwe identify the following optimization strategies:    gies within the strategy pool, simultaneously updat-\nReasoning, Reinterpretation, Simplification, Role-   ing the action-reward distribution Mt0,GE0:\nPrompting, Decomposition, Self-Criticism, Caption\nand Rephrasing. The details of the strategies are             Mt0,GE0 = ψ(S, t0, GE0),         (3)\n\n\n                                         4\n\nwhere ψ denotes the tree search operation.            private domain tasks and EMLLMs. Building on\n  For a new private domain task-tn on a new    this foundation, Empirical Self-reflective Optimiza-\nEMLLM-GEm, the framework utilizes prior infor-    tion (ESO) further refines the prompts by select-\nmation stored in the memory module M and ap-   ing critical bad cases from the error distribution of\nplies an ε-greedy strategy to balance exploration    evaluation results and leveraging EMLLMs’ self-\nand exploitation. Specifically, the general MLLM    reflection to better adapt to private domain data.\nGM first selects the distribution with the highest       Specifically, the process of ESO is as follows:\ntask-model similarity to task-tn and EMLLM-GEm     First, the coarse optimized prompts obtained from\nfrom the existing memory module M as a refer-  RWS are used as the initial prompts for ESO.\nence:\n                                                                               v′0 = v0 + S∗                (7)\n  Mref = arg  max   ρ((t, GE), (tn, GEm)),\n             Mt,GE ∈M                                                     Building upon this foundation, ESO begins itera-\n                                                 (4)\n                                                            tive optimization with the EMLLM first performing\nwhere ρ denotes the task-model similarity.\n                                                   evaluation on the validation set. During the opti-\n  The distribution is then smoothed and homog-\n                                                mization process, historical evaluation results of\nenized based on task-model similarity to reduce\n                                                    the validation set are considered, including previ-\nbias:\n                                                ous prompts, accuracy rates, and error distributions.\n  Mtn,GEm = σ(Mref, ρ((t, GE), (tn, GEm))),      Optimization by the general MLLM GM is carried\n                                                 (5)    out based on two key aspects:\nwhere σ denotes the smooth operation.                  • i) Global Error Distribution Learning: the gen-\n  This distribution is used as the action-reward       eral MLLM GM analyzes error evaluation results\ndistribution Mtn,GEm for the private domain task-      to identify and assess patterns in error distribu-\ntn and EMLLM-GEm. An ε-greedy search is then       tion of private domain, uncovering task descrip-\nperformed based on this distribution, offering a 1-ε      tions and labels that are commonly misunder-\nprobability of selecting one of top-k historically      stood or not well comprehended by EMLLMs.\neffective strategies for exploitation, while with a      This macro-level analysis provides insights into\nε probability, the framework randomly searches      the EMLLM’s weaknesses within specific tasks,\nother strategies for exploration:                       guiding further optimization.\n                                                              •  ii) Local Case-based Learning:  the general\n    (        κ(Mtn,GEm)         prob. 1 −ε,      MLLM GM selects critical bad cases accord- S∗=                                         (6)\n        ξ(S \\ κ(Mtn,GEm))  prob. ε,              ing to error distribution based on the principles\n                                                      of typicality and diversity to identify confusing\nwhere κ denotes the selection one of top-k opera-                                                     or distinctive private domain bad cases. These\ntion, ξ denotes the random selection operation. s∗                                                      cases are analyzed alongside ground truth labels\nrepresents the selected strategies.                                                           to diagnose errors, refining private domain tasks\n RWS fully utilizes prior knowledge of prompt                                               and label definitions as needed.\nengineering strategies. Compared to other search                                           The optimization process can be formalized as\nmethods, it is more transparent and explicit, enhanc-                                                     follows:\ning the interpretability of the prompt optimization\nprocess. We expect RWS to exhibit a degree of   R(v′0) = E([v′0,X sub],Ysub)∼Dth(fE(v′0, X sub; Θ), Ysub)\ntransferability across different private domain tasks          vi+1 = vi + ∇viR(vi) for i = 0, 1, . . .\nand models, similar to pre-training in the MLLMs’                                                    (8)\ntraining paradigm. This stage allows for the ac-     This process iterates until it reaches the specified\nquisition of a more generalizable combination of   number of iterations.\nprompt strategies for private domain-specific tasks,\nshowcasing significant advantages in multimodal,       T (vi) = True if max_iterations ≤i    (9)\nmulti-task understanding in real-world business\n                                                 Thus, ESO can utilize the priors obtained fromscenarios.\n                              RWS to develop fine-tuned prompts for unknown\n3.2.3  Empirical Self-reflective Optimization      private domain tasks with minimal search steps,\nAfter the RWS, we obtained the coarse optimized    similar to supervised fine-tuning in the MLLMs’\nprompts as well as optimized priors for various    training paradigm.\n\n\n                                         5\n\nImage                        Commodity                  Outfit                    w/o\n  Method        Parameter   Graphic  Commodity  Commodity  Commodity  Commodity  Concatenation    Outfit   Average\n                  Tuning     Layout    Location    Information  Background    Border        Situation       Style\n\n  Fine-Tuning                     38.5        26.0          69.8          61.5          58.3           88.5          47.1       55.7\n MM-CoT      Í         21.9        27.1          24.0          42.7          25.0           38.5          17.6       28.4\n  Zero-Shot      Í         16.7         5.2           14.6          42.7          54.2             5.2          14.7       22.5\n APE         Í         34.4        26.0           6.3           38.5          18.8           15.6           8.8       21.2\n OPRO        Í         20.8        27.1          39.6          68.8          54.2           32.3          26.5       38.5\n\n IDEALPrompt   Í         47.9        51.0          45.8          69.7          59.4           89.6          38.2       57.4\n\n\nTable 1: Performance comparison of IDEALPrompt and other baseline methods on the Taobao-PDA benchmark.\nAll methods leverage the efficient MLLM, InternVL2-2B, for inference. We color each row as the best and\n second best .\n\n\n                                                                                                             train\n           10%                                                                                set                                 Stages\n                  15%              Graphic Layout                           16%                   Method                                Average Performance\n                                     Commodity Location                                                                  Stage 1  Stage 2\n       15%\n                                     Commodity Information\n                      15%       Commodity Background                          16%                   Zero-Shot                                           22.5\n                                     Commodity                                                 Border       15%                                                          Í                                                                            w/o                                                                                                         Self-reflection                                                                                                                                           43.5                                                                                                                                         val.                                          Concatenation                                                           Situation                                                      68%                    15%                                                                                                        set                                                               Í                                                                            w/o                                                                           Warm-up Strategy                                                                                                                                           41.1                                         Outfit Style                                                                    test            15%\n                                                                   set\n                                                                 IDEALPrompt      Í    Í              56.3\n          (a) Category list in Taobao-PDA                (b) Partition of Taobao-PDA\n\n   Figure 3: Data characteristics of Taobao-PDA       Table 2: Ablation results of involving different stages.\n\n4  Experiment                                    scores, and error examples to guide models gen-\n                                                       erate prompts with higher scores.  In addition,\n4.1  Experimental Setting                                    we compare our method with (4) Zero-Shot in-\nDatasets and Tasks. As our primary objective is    ference and (5) Fine-Tuning method learning on\nto address tasks focused on multimodal content    the train set of Taobao-PDA. Among them, Fine-\nunderstanding of the private domain, we collect ex-   Tuning involves parameter tuning, MM-CoT em-\nternal data from Taobao’s private domain involved    ploys multi-step inference, while other prompt opti-\nmultimodal content understanding and propose a    mization methods (i.e., APE and OPRO) generally\nbenchmark, namely Taobao Private Domain Adap-    utilize single-step inference without parameter tun-\ntation (Taobao-PDA) benchmark, encompassing    ing but prompt optimization.\n7 multimodal content understanding tasks across 3   Implementation Details. In the experiments, we\ncategories, where the ground truth are labeled by    leverage GPT-4o as the general MLLM for prompt\nGPT-4o and further refined by human annotation.   optimization and InternVL2-2B as the EMLLM\nThe detailed tasks are as follows:                     for inference. We use a simple 0-1 loss as the\n • Image: Graphic Layout, Commodity Location.   score function h(·), i.e., h(fE(v, x; Θ), y) = 1 if\n • Commodity: Commodity Information, Com-   fE(v, x; Θ) = y, otherwise h(fE(v, x; Θ), y) = 0.\n  modity Background, Commodity Border, Con-  We set the general MLLM’s sampling temperature\n   catenation Situation.                                to 1 and EMLLM’s sampling temperature to 0 dur-\n • Outfit: Outfit Style.                          ing the whole optimization process.  Details of\n  Figure 3 shows the detailed characteristics of  RWS and ESO are provided in Appendix A.2 and\nTaobao-PDA, with additional details provided in   Appendix A.3 respectively.\nAppendix A.1 1.\nBaselines. We compare IDEALPrompt with sev-   4.2  Main Results\neral prompt engineering baseline methods:  (1)  We evaluate the performance of IDEALPrompt\nMM-CoT (Zhang et al., 2023), which generates   on the Taobao-PDA benchmark compared to the\nrationales before generating the final answer; (2)    baseline methods, as shown in Table 1. Our ob-\nAPE (Zhou et al., 2022), which generates instruc-   servations are summarized as follows: (i) IDEAL-\ntions using powerful models; (3) OPRO (Yang   Prompt achieve the highest average performance\net al., 2023a), which leverages historical prompts,   of 57.3 among all baseline methods.  This re-\n                                                           sult significantly surpasses that of other meth-   1Note that Taobao-PDA’s original language is Chinese,\nwhich is translated to English for presentation in this paper.     ods, showcasing IDEALPrompt’s superior per-\n\n\n                                         6\n\n(a)                              (b)                                   (c)                              (d)\n\n\n\n\n\nFigure 4: (a) Performance comparison between the single strategy and the exploration-exploitation strategy. (b)\nPerformance and search steps comparison between the brute-force search and the exploration-exploitation strategy\ntree search. (c) and (d) Performance comparison among the absence of various components in Empirical Self-\nreflective Optimization.\n\n                                   Average     Inference  GPU Memory      ing the complete two-stage framework with ver-   Method          Model\n                                 Performance    Time       Usage\n                                                    sions that omit one of the stages. Table 2 shows\n    Zero-Shot       InternVL2-8B        52.2        ~1.2×      ~16GB\n   IDEALPrompt  InternVL2-2B        56.3       ~1×       ~4GB         that both stages of IDEALPrompt are effective,\n                                              and combining both stages outperforms using each\nTable 3: Comparison of average performance, inference    stage individually. The effectiveness of both stages\ntime, and GPU memory usage between Zero-Shot on    avoids bias caused by introducing either stage.\nInternVL2-8B and IDEALPrompt on InternVL2-2B.\n                                                    4.4  In-Depth Analysis\nformance across various tasks.  Whether deal-\n                                               Analysis of the Strategy Optimization in RWS.\ning with Image, Commodity or Outfit cate-\n                                 We first compare the prompt optimized with a sin-\ngories, IDEALPrompt demonstrates consistent and\n                                                     gle strategy against the resulting coarse optimized\nremarkable effectiveness.  (ii) IDEALPrompt ex-\n                                            prompt on the training set, as illustrated in Fig-\ncel in several key tasks, achieving the best perfor-\n                                                    ure 4-(a). The ensemble strategy tree achieves a per-\nmance across 5 out of 7 tasks, i.e., “Graphic Lay-\n                                             formance of 44.8, outperforming other individual\nout” (47.9), “Commodity Location” (51.0), “Com-\n                                                          strategies. Furthermore, we compare the exhaus-\nmodity Background” (69.7), “Commodity Border”\n                                                            tive search of the strategy tree with the exploration-\n(59.4) and “Merging Situation” (89.6).  Particu-\n                                                      exploitation-based search, as illustrated in Figure 4-\nlarly compared with the fine-tuning method, IDE-\n                                                            (b). For new tasks, exhaustive search through the\nALPrompt performs worse on only two tasks, yet\n                                                        strategy tree requires over 90 search steps, whereas\nslightly outperforms it in terms of overall average\n                                                our method needs only about 10 steps, achieving\nperformance. Considering the higher costs and de-\n                                              performance comparable to that of the exhaustive\nployment of the fine-tuning method, this outcome is\n                                                      search.\ndeemed acceptable.  (iii) Our method consistently\n                                                  Effectiveness of Each Component in ESO. We\nachieves improvements across various tasks. In\n                                               conduct a performance comparison to evaluate\ncontrast, other prompt optimization methods often\n                                                    the impact of the absence of various components\nexperience either a lack of performance enhance-\n                                                  within ESO. Specifically, we considered the fol-\nment or even a decline. This may be attributed to\n                                              lowing components during the generation of new\nthe model’s inability to accurately infer a reason-\n                                                  prompts: (i) without selection of bad cases; (ii) ran-\nable optimization direction, thereby limiting their\n                                     dom selection of samples as bad cases; (iii) without\noptimization capabilities.\n                                                  using error distribution; and (iv) without using his-\n  We conduct experiments on various MLLMs\n                                                           torical inference results. Figure 4-(c) sequentially\nfor prompt optimization, experiments on public\n                                            shows the performance improvement derived from\ndomain benchmark, human evaluation and multi-\n                                                    the analysis of selected bad cases, while Figure 4-\nlingual experiments on Taobao-PDA’s English\n                                                      (d) sequentially shows the performance improve-\nversion as well, see Appendix A.4 for details.\n                                            ment derived from analyzing conclusions based on\n                                                          historical evaluation results.\n4.3  Ablation Study\n                                              Additional Baseline Comparision. We conduct\nTo investigate the effectiveness of the two-stage    the comparison of the average performance, in-\nframework, we conduct an ablation study compar-   ference time, and GPU memory usage between\n\n\n                                         7\n\ninitial prompt                                                                                                                final prompt\n\n     Based on the input image, determine the value of the \"overall graphic layout label of the overall image information category\". The   You are a professional image\n     meaning of this label is: the overall graphic layout structure of the image. This is a single-choice question, options include:           analyst, ... .\n       uncertain/left-right structure-left text right image/left-right structure-left image right text/left-right structure-left and right image    Based on the input image,\n      splicing/up-down structure-up image down text/up-down structure-up text and down image/up-down structure-up and down         determine the value of the\n     image splicing. Please output the answer directly, do not include the analysis process.                                                    \"overall graphic layout label of the\n                                                                                                                                                      overall image information\n                                      Warm-up Strategy                                                              category\". The meaning of this\n        coarse strategy prompt                                                                                                              labelPleaseis: answer... .    the following\n     prompt + Role-Prompting + Decomposition                                                                         questions step by step\n       1. Role-Prompting: You are a professional image analyst, adept at extracting and judging graphic layout structures      according to the input image:\n      from images.                                                                                                                                                                          ... .\n       2. Decomposition: Please answer the following questions step by step according to the input image: i. Does the image           Label definition:\n     contain text and images? (Yes/no) ii. If yes, what is the relative position of the text and image? (left and right/up and       1. left-right structure - left\n     down) iii. If the structure is left or right, is the text on the left or right? (Left/right) iv. If the structure is up or down,      text right image: ...\n       is the text on top or bottom? (up/down)                                                                                      2. left-right structure - left\n                                                                                                        image right text: ...\n                                              Self-reflective Optimization                                                      3. left-right structure - left\n         fine reflective prompt                                                                            and4. up-downright imagestructuresplicing:- up...\n      Label definition:                                                                                      image down text: ...\n      1. left-right structure - left text right image: The left side is mainly text content, the right side is image.                        5. up-down structure - up text\n      2. left-right structure - left image right text: The left side is the image, the right side is mainly the text content.            down image: ...\n      3. left-right structure - left and right image splicing: The left and right sides are images, and there may be a small amount      6. up-down structure - up and\n       of text or signs in the middle.                                                                     down image splicing: ...\n      4. up-down structure - up image down text: The up is the image, and the down is mainly the text content.                      7. Unclear: ...\n      5. up-down structure - up text down image: The up is mainly text content, and the down is an image.                           Please output the answer directly,\n      6. up-down structure - up and down image splicing: The up and down parts are images, and there may be a small amount     do not include the analysis\n       of text or signs in the middle.                                                                                                      process.\n      7. Unclear: The graphic layout is scattered and there is no obvious left, right or up and down structure.\n\n\n                      Figure 5: A case of prompt optimization using the IDEALPrompt.\n\n\n                    Average      Iteration                    Graphic  Commodity  Commodity  Commodity  Commodity  Concatenation   Outfit\n  Method                                 Method                                                                                            Average\n                  Performance     Steps                      Layout    Location    Information  Background    Border        Situation      Style\n\n OPRO                38.5        50 ± 3      Zero-Shot          33.3        52.1          56.3          38.5          67.7           76.0         41.2     52.2\n\n IDEALPrompt        56.3        15 ± 3     IDEALPrompt     71.9        57.3          61.5          42.7          69.8           84.4         76.5     66.3\n\n\nTable 4: Comparison between Table 5: Performance comparison between Zero-Shot and IDEALPrompt on\nOPRO and IDEALPrompt.     InternVL2-8B.\n\n\n                                             Performance\n   Type   EMLLM      Method                                                                          Steps     We conduct failure modes analysis, computa-\n                               Image  Commodity  Average           tional efficiency evaluation in Appendix A.5 and\n                          w/o adapt     49.0        49.7         49.5     93\n    Tasks    InternVL2-2B                  w adapt      49.5        66.1         60.6     13       additional discussion in Appendix A.6.\n\n                          w/o adapt     16.1        62.8         47.2     93\n   Models  Qwen2-VL-2B\n                  w adapt      17.7        58.6         45.0     13       4.5  Case Study\n\n                                                   Figure 5 shows a case of prompt optimization using\nTable 6: Comparison of performance and steps between\n                                                    the IDEALPrompt framework.  After RWS, theadapt from tasks/model and without adapt.\n                                                 optimal strategy combination, “Role-Prompting +\nZero-Shot on InternVL2-8B and IDEALPrompt on   Decomposition”, is identified. Subsequently, ESO\nInternVL2-2B, as shown in Table 3. IDEALPrompt    refines the label by providing detailed definitions,\non InternVL2-2B achieved higher average perfor-   ultimately producing the final optimized prompt.\nmance than Zero-Shot on InternVL2-8B while\nmaintaining lower inference time and GPU mem-   5  Conclusion\nory usage. In addition, we conduct the comparison\n                                                    In this paper, we propose IDEALPrompt, a tuning-\nof performance and average iteration steps between\n                                                           free,  adaptive,  universal prompt optimization\nOPRO and IDEALPrompt on InternVL2-2B, as\n                                            framework consisting of two stages, which fo-\nshown in Table 4. IDEALPrompt achieved higher\n                                                 cuses on boosting EMLLMs’ content understand-\naverage performance than OPRO while maintain-\n                                                  ing of private domain data.  Specifically, IDE-\ning lower iteration steps. Table 5 shows the perfor-\n                                        ALPrompt incorporates human experts’ prior op-\nmance comparison between Zero-Shot and IDEAL-\n                                                    timization strategies along with a reinforcement\nPrompt on InternVL2-8B, which demonstrates the\n                                                    learning-based strategy tree search and utilizes the\ngeneral effectiveness of our method across different\n                                               model’s self-reflection to refine prompts. In ad-\nmodels.\n                                                         dition, we propose Taobao-PDA benchmark to\nAdaptability. Notably, our method demonstrates\n                                                 study the private domain adaptation of IDEAL-\nadaptability across both tasks and models, table 6\n                                              Prompt.  Experimental results demonstrate that\npresents the adaptability of our method in task and\n                                       IDEALPrompt outperforms existing prompt opti-\nmodel dimensions. Regardless of adapting to tasks\n                                                  mization approaches while significantly improving\nor models, IDEALPrompt achieved competitive\n                                                   optimization efficiency.\nresults with fewer iteration steps.\n\n\n                                         8\n\nLimitations                                Hyung Won Chung, Le Hou, Shayne Longpre, Barret\n                                                   Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nOur limitations and potential risks are as follows:      Wang, Mostafa Dehghani, Siddhartha Brahma, et al.\n                                                        2024. Scaling instruction-finetuned language models.\n • Large Consumption of API Calls. Our method      Journal of Machine Learning Research, 25(70):1–53.\n  involves extensive searching and still requires a\n                                             Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun,\n   large number of API calls (although this is much                                              Damien Lopez, Kamalika Das, Bradley Malin, and\n   less than existing methods), which is a common       Sricharan Kumar. 2024. Phaseevo: Towards unified\n   issue in automatic prompt optimization.               in-context prompt optimization for large language\n                                                       models. arXiv preprint arXiv:2402.11347.\n • Burden of human-aligned strategy definition.\n                                                  Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan   Strategies defined by human experts are intended\n                                               Wang, Han Guo, Tianmin Shu, Meng Song, Eric P\n   to provide models with human-derived optimiza-                                                      Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing\n   tion priors. This is because humans can usu-       discrete text prompts with reinforcement learning.\n   ally intuitively perceive better descriptions, but      arXiv preprint arXiv:2205.12548.\n  optimizing them directly is difficult. However,\n                                                      Chrisantha  Fernando,  Dylan  Banarse,  Henryk\n   this increases the burden of manual definitions,                                                      Michalewski, Simon Osindero, and Tim Rock-\n  thereby automatic construction of external strate-       täschel. 2023.   Promptbreeder:  Self-referential\n   gies will be a key area for our future research.        self-improvement via prompt evolution.   arXiv\n                                                             preprint arXiv:2309.16797.\nEthics Statement\n                                                 Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,\n                                                Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,In the process of collecting Taobao-PDA, we ob-\n                                                     Ping Luo, and Kai Chen. 2023. Multimodal-gpt: A\ntained TaoBao’s permission and carried out desen-                                                              vision and language model for dialogue with humans.\nsitization processing. In addition, racial discrimi-      arXiv preprint arXiv:2305.04790.\nnation and gender discrimination were eliminated.\nEspecially for the data of the outfit category   Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\n                                                     Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\ninvolving people, we only consider some clothes                                                                     jiu Yang. 2023. Connecting large language models\nwith distinct characteristics, regardless of gender      with evolutionary algorithms yields powerful prompt\nor race.                                                     optimizers. arXiv preprint arXiv:2309.08532.\n\n                                                     Cho-Jui Hsieh, Si Si, Felix X Yu, and Inderjit S Dhillon.\n                                                        2023. Automatic engineering of long prompts. arXiv\nReferences                                                             preprint arXiv:2311.10117.\nWoong-Gi Chang, Tackgeun You, Seonguk Seo, Suha\n  Kwak, and Bohyung Han. 2019. Domain-specific   Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\n   batch normalization for unsupervised domain adap-      Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n   tation. In Proceedings of the IEEE/CVF conference      and Weizhu Chen. 2021.  Lora: Low-rank adap-\n  on Computer Vision and Pattern Recognition, pages       tation of large language models.  arXiv preprint\n  7354–7362.                                           arXiv:2106.09685.\n\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng   Wenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu,\n  Huang, and Tianyi Zhou. 2023a. Instructzero: Ef-     Xiangqiang Lin, Zhongxiang Dai, See-Kiong Ng,\n   ficient instruction optimization for black-box large      and Bryan Kian Hsiang Low. 2024.   Localized\n   language models. arXiv preprint arXiv:2306.03082.       zeroth-order prompt optimization.  arXiv preprint\n                                                        arXiv:2403.02993.\nZhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye,\n  Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi   Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\n  Hu, Jiapeng Luo, Zheng Ma, et al. 2024. How far     Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\n   are we to gpt-4v?  closing the gap to commercial      Cui, Owais Khan Mohammed, Qiang Liu, et al.\n  multimodal models with open-source suites. arXiv      2023.  Language is not all you need:  Aligning\n   preprint arXiv:2404.16821.                             perception with language models.  arXiv preprint\n                                                        arXiv:2302.14045.\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\n  Chen, Sen Xing, Muyan Zhong, Qinglong Zhang,   Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai\n  Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong     Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin\n  Lu, Yu Qiao, and Jifeng Dai. 2023b.   Internvl:      Tan, Zhenye Gan, Yabiao Wang, Chengjie Wang,\n   Scaling up vision foundation models and aligning      and Lizhuang Ma. 2024.   Efficient multimodal\n   for generic visual-linguistic tasks.  arXiv preprint       large language models: A  survey.    Preprint,\n   arXiv:2312.14238.                                    arXiv:2405.10739.\n\n\n                                         9\n\nWeize Kong, Spurthi Amba Hombaiah, Mingyang        atic survey of prompting techniques. arXiv preprint\n  Zhang, Qiaozhu Mei, and Michael Bendersky. 2024.      arXiv:2406.06608.\n   Prewrite: Prompt rewriting with reinforcement learn-\n   ing. arXiv preprint arXiv:2401.08189.              Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-\n                                                   hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nElad Levi, Eli Brosh, and Matan Friedmann. 2024.     Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei\n   Intent-based prompt calibration: Enhancing prompt      Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang\n   optimization with synthetic boundary cases. arXiv      Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-\n   preprint arXiv:2402.03099.                                   vl: Enhancing vision-language model’s perception\n                                                          of the world at any resolution.   arXiv preprint\nHaoyuan Li, Hao Jiang, Tao Jin, Mengyan Li, Yan Chen,                                                        arXiv:2409.12191.\n   Zhijie Lin, Yang Zhao, and Zhou Zhao. 2023a. Date:\n  Domain adaptive product seeker for e-commerce. In                                               Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai,\n  Proceedings of the IEEE/CVF Conference on Com-                                                      Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P\n   puter Vision and Pattern Recognition (CVPR), pages                                                      Xing,  and  Zhiting Hu.  2023.    Promptagent:\n  19315–19324.                                                              Strategic planning with language models enables\n                                                              expert-level prompt optimization.  arXiv preprintHaoyuan Li, Hao Jiang, Tianke Zhang, Zhelun Yu,\n                                                        arXiv:2310.16427.  Aoxiong Yin, Hao Cheng, Siming Fu, Yuhao\n  Zhang, and Wanggui He. 2023b.   Traineragent:\n                                               Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao  Customizable and efficient model training through\n                                                           Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.  llm-powered multi-agent system.  arXiv preprint\n                                                        2023a. Large language models as optimizers. ArXiv,  arXiv:2311.06622.\n                                                        abs/2309.03409.\nTianwei Lin, Jiang Liu, Wenqiao Zhang, Zhaocheng\n   Li, Yang Dai, Haoyuan Li, Zhelun Yu, Wang-   Kaiwen Yang, Tao Shen, Xinmei Tian, Xiubo Geng,\n   gui He, Juncheng Li, Hao Jiang,  et  al. 2024a.     Chongyang Tao, Dacheng Tao, and Tianyi Zhou.\n  Teamlora: Boosting low-rank adaptation with ex-      2023b. Good questions help zero-shot image rea-\n   pert collaboration and competition. arXiv preprint       soning. arXiv preprint arXiv:2312.01598.\n  arXiv:2408.09856.\n                                               Qinghao Ye, Haiyang Xu, Guohai Xu,  Jiabo Ye,\nXiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-     Ming Yan, Yiyang Zhou, Junyang Wang, An-\n  Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang     wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.\n  Low. 2024b. Prompt optimization with human feed-      mplug-owl:  Modularization empowers large lan-\n   back. arXiv preprint arXiv:2405.17346.                guage models with multimodality. arXiv preprint\n                                                        arXiv:2304.14178.\nXiaoqiang  Lin,  Zhaoxuan Wu,  Zhongxiang  Dai,\n  Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jail-   Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang,\n   let, and Bryan Kian Hsiang Low. 2024c. Use your      Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li,\n  INSTINCT: INSTruction optimization for LLMs us-      Yueting Zhuang, and Weiming Lu. 2024a. Agent-\n  Ing neural bandits coupled with transformers.  In       pro: Learning to evolve via policy-level reflection\n  Proceedings of the 41st International Conference on      and optimization. arXiv preprint arXiv:2402.17574.\n  Machine Learning, volume 235 of Proceedings of\n  Machine Learning Research, pages 30317–30345.   Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun\n  PMLR.                                             Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao\n                                                   Zhou, Zheqi Lv, Hao Jiang, et al. 2024b.  Hyper-\nChancharik Mitra, Brandon Huang, Trevor Darrell, and                                                                      llava: Dynamic visual and language expert tuning for\n  Roei Herzig. 2024. Compositional chain-of-thought                                                     multimodal large language models. arXiv preprint\n  prompting for large multimodal models. In Proceed-                                                        arXiv:2403.13447.\n   ings of the IEEE/CVF Conference on Computer Vi-\n   sion and Pattern Recognition, pages 14420–14431.                                              Wenqiao Zhang, Zheqi Lv, Hao Zhou, Jia-Wei Liu,\n                                                     Juncheng Li, Mengze Li, Yunfei Li, Dongping Zhang,Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit\n                                                         Yueting Zhuang, and Siliang Tang. 2024c. Revisiting   Bansal. 2022. Grips: Gradient-free, edit-based in-\n                                                           the domain shift and sample uncertainty in multi-   struction search for prompting large language models.\n                                                        source active domain transfer.  In Proceedings of  arXiv preprint arXiv:2203.07281.\n                                                            the IEEE/CVF Conference on Computer Vision and\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-      Pattern Recognition, pages 16751–16761.\n  guang Zhu, and Michael Zeng. 2023.  Automatic\n  prompt optimization with\" gradient descent\" and   Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu\n  beam search. arXiv preprint arXiv:2305.03495.         Zhang, Andrew Makmur,  Qingpeng  Cai,  and\n                                              Beng Chin Ooi. 2022. Boostmis: Boosting medical\nSander Schulhoff, Michael Ilie, Nishant Balepur, Kon-      image semi-supervised learning with adaptive pseudo\n   stantine Kahadze, Amanda Liu, Chenglei Si, Yin-      labeling and informative active annotation. In Pro-\n  heng Li, Aayush Gupta, HyoJung Han, Sevien Schul-      ceedings of the IEEE/CVF conference on computer\n   hoff, et al. 2024.  The prompt report: A system-       vision and pattern recognition, pages 20666–20676.\n\n\n                                         10\n\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\n  George Karypis, and Alex Smola. 2023.  Multi-\n  modal chain-of-thought reasoning in language mod-\n   els. arXiv preprint arXiv:2302.00923.\n\nGe Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and\n   Sibei Yang. 2023.  Ddcot: Duty-distinct chain-of-\n  thought prompting for multimodal reasoning in lan-\n  guage models. Advances in Neural Information Pro-\n   cessing Systems, 36:5168–5191.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\n  Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  Ba. 2022. Large language models are human-level\n  prompt engineers. arXiv preprint arXiv:2211.01910.\n\n\n\n\n\n                                         11\n\nA  Appendix                                       ysis) and various subfields (such as apparel, fast-\n                                          moving consumer goods, electronics, and health-\nA.1  Taobao-PDA Data Detail\n                                                        care). Therefore, we have collected data from ad-\nA.1.1  Private Domain Data                        ditional modalities (such as video) and different\nPrivate domain data refers to data characterized by   e-commerce subfields (such as apparel and elec-\nhighly specialized domain knowledge, which often    tronics).\nresults in models trained on public domain data\n                                                 A.1.2  Task Definition\nperforming poorly on private domain data. For\ninstance, in our work, we focus on commodity   Figure 6 illustrates the detailed definition of tasks.\nimages within the e-commerce context. The data   Each task can be viewed as a visual question an-\ndistribution in public data typically centers on im-   swering task in a distinct private domain of under-\nage captions/descriptions, visual question answer-   standing.\ning (VQA), grounding, and similar tasks. In con-\n                                                 A.1.3   Initial Prompt\ntrast, the data distribution in e-commerce domain\ndata tends to focus on the distribution and types of   Our zero-shot initial prompts of various tasks are\ncommoditys in the image, commodity-background   shown in Table 7.\nboundaries, and other specific features. These dif-\n                                            A.2  Strategy Detailferences in data distribution result in the inadequate\nperformance of models trained on public domain    In RWS, part of strategies use the {initial prompt\ndata when applied to private domain data, as we  + strategy prompt} as the strategy-based prompt,\ndemonstrate in Tbale 1, zero-shot performance.      while other utilize GPT-4 to generate the strategy-\n  In addition, while we are currently focused on   based prompt. The top-k value is set to 3. ε is set to\nthe e-commerce domain, e-commerce itself encom-    0.3, meaning there is 0.7 probability of exploitation\npasses a wide array of technical fields (such as im-   and 0.3 probability of exploration. Task-model\nage recognition, video understanding, and text anal-    similarity is assessed by GPT-4.\n\n\n\n {\n     \"overall image information\": {\n         \"overall graphic layout\": {\n             \"type\": \"single-choice question\",\n             \"description\": \"the overall graphic layout structure of the image\",\n             \"option\": [\"uncertain\", \"left-right structure-left text right image\", \"left-right structure-left image right text\",\n                        \"left-right structure-left and right image splicing\", \"up-down structure-up image down text\",\n                        \"up-down structure-up text and down image\",\n                        \"up-down structure-up and down image splicing\"]\n         },\n         \"commodity image location\": {\n             \"type\": \"single-choice question\",\n             \"description\": \"the location of the commodity image in the overall image\",\n             \"option\": [\"no commodity\", \"uncertain\", \"full screen\", \"center of screen\", \"left and right halves\",\n                        \"left half\", \"right half\", \"up and down halves\", \"top half\", \"bottom half\"]\n         }\n     },\n     \"commodity\": {\n         \"commodity information\": {\n             \"type\": \"single-choice question\",\n             \"description\": \"commodity information\",\n             \"option\": [\"no commodity\", \"single commodity single style\", \"single commodity multiple styles\", \"multiple commodities\"]\n         },\n         \"commodity image background\": {\n             \"type\": \"single-choice question\",\n             \"description\": \"the background color of the commodity image\",\n             \"option\": [\"white\", \"other\"]\n         },\n         \"commodity image border\": {\n             \"type\": \"single-choice question\",\n             \"description\": \"the shape of the commodity image's border\",\n             \"option\": [\"no obvious border\", \"square border\", \"other shaped border\"]\n         },\n         \"commodity image concatenation situation\": {\n             \"type\": \"single-choice question\",\n             \"description\": \"the concatenation situation of the commodity image\",\n             \"option\": [\"single image without concatenation\", \"2 images concatenation\", \"3 images concatenation\",\n                        \"4 images concatenation\", \"5 or more images concatenation\"]\n         }\n     },\n     \"outfit\":{\n         \"outfit style\":{\n             \"type\": \"single-choice question\",\n             \"description\": \"the outfit style of the person in the image\",\n             \"option\": [\"Anklei\", \"Girlcore\", \"Sporty Chic\", \"Y3K\", \"Mori\", \"Dopamine\", \"Bohemian\", \"Intellectual\"]\n         }\n     }\n }\n\n\n                                        Figure 6: Tasks definition.\n\n                                         12\n\nTask                                                               Initial Prompt\n\n              Based on the input image, determine the value of the \"overall graphic layout label of the overall image information\n                 category\". The meaning of this label is: the overall graphic layout structure of the image. This is a single-choice\n   Graphic      question, options include: uncertain/left-right structure-left text right image/left-right structure-left image right text/left-\n   Layout       right structure-left and right image splicing/up-down structure-up image down text/up-down structure-up text and down\n              image/up-down structure-up and down image splicing. Please output the answer directly, do not include the analysis\n                 process.\n\n              Based on the input image, determine the value of the \"commodity image location label of the overall image information\n                 category\". The meaning of this label is: the location of the commodity image in the overall image.  This is a\n Commodity\n                 single-choice question, options include: no commodity/uncertain/full screen/center of screen/left and right halves/left\n   Location\n                   half/right half/up and down halves/top half/bottom half. Please output the answer directly, do not include the analysis\n                 process.\n\n              Based on the input image, determine the value of the \"commodity information label of the commodity category\". The\n Commodity   meaning of this label is: commodity information. This is a single-choice question, options include: no commodity/single\n Information   commodity single style/single commodity multiple styles/multiple commodities. Please output the answer directly, do\n                not include the analysis process.\n\n              Based on the input image, determine the value of the \"commodity image background label of the commodity category\".\n Commodity\n             The meaning of this label is: the background color of the commodity image. This is a single-choice question, options\n Background\n                 include: white/other. Please output the answer directly, do not include the analysis process.\n\n              Based on the input image, determine the value of the \"commodity image border label of the commodity category\". The\n Commodity   meaning of this label is: the shape of the commodity image’s border. This is a single-choice question, options include:\n   Border     no obvious border/square border/other shaped border. Please output the answer directly, do not include the analysis\n                 process.\n\n              Based on the input image, determine the value of the \"commodity image concatenation situation label of the commodity\nConcatenation   category\". The meaning of this label is: the concatenation situation of the commodity image. This is a single-choice\n   Situation      question, options include: single image without concatenation/2 images concatenation/3 images concatenation/4 images\n                concatenation/5 or more images concatenation. Please output the answer directly, do not include the analysis process.\n\n              Based on the input image, determine the value of the \"outfit style label of the outfit category\". The meaning of this label\n    Outfit         is: the outfit style of the person in the image. This is a single-choice question, options include: Anklei/Girlcore/Sporty\n     Style       Chic/Y3K/Mori/Dopamine/Bohemian/Intellectual. Please output the answer directly, do not include the analysis\n                 process.\n\n\n                                   Table 7: Initial prompts of tasks.\n\n   Strategy                                             Prompt\n\n  Reasoning    {prompt} + Please carefully understand the question before answering and provide your thought and analysis process.\n\n             {prompt} + Please do not rush to answer; before providing a response, reread and carefully understand my question\nReinterpretation\n                and requirements.\n\n              You are now an expert prompt engineer, tasked with optimizing prompts to help smaller models accurately reason\n                 through complex problems. Focus on simplifying the expression of the problem and its requirements.\n Simplification\n           My prompt is: {prompt}.\n                  Please output your optimized prompt directly without providing the analysis process.\n\n              You are now an expert prompt engineer, tasked with optimizing prompts to help smaller models accurately reason\n                 through complex problems. Focus on incorporating role-playing (e.g., “You are a xxx...”) as a method of optimization.\nRole-Prompting\n           My prompt is: {prompt}.\n                  Please output the optimized prompt directly without providing the analysis process.\n\n              You are now an expert prompt engineer, tasked with optimizing prompts to help smaller models accurately reason\n                 through complex problems. Focus on decomposing the problem description into a combination of multiple simple and\nDecomposition   understandable questions to enhance performance through multi-step reasoning.\n           My prompt is: {prompt}.\n                  Please output the optimized prompt directly without providing the analysis process.\n\n             {prompt} + Please do not rush to answer, before giving the answer, carefully reflect on your answer is correct, confirm\n Self-Criticism\n                   the answer is correct before output the answer.\n\n              You are now an expert prompt engineer, tasked with optimizing prompts to help smaller models accurately reason\n                 through complex problems. Focus on having the model first provide a detailed description of the image content, and\n    Caption       then formulate an answer based on this description.\n           My prompt is: {prompt}.\n                  Please output the optimized prompt directly without providing the analysis process.\n\n             {prompt} + Did you understand the task above? Please summarize the tasks you need to do and show how you will\n  Rephrasing\n                  execute the detailed plan for the task\n\n\n                                    Table 8: Prompts of strategies.\n\n                                       13\n\nYou are currently an accomplished prompt engineer. Your task is to analyze potential causes of errors based on the\n  model inference prompt and typical erroneous samples. These causes will be used to optimize the model inference\n  prompt.\n The following is the prompt for model inference:\n {prompt}\n The strategies obtained in Reinforcement Warm-up Strategy are:\n {strategies}\n The current error distribution of the model is as follows:\n {error_distribution}\n  Here are some representative error samples:\n {error_cases}\n  Please analyze the possible causes of errors from the following perspectives:\n  1. Clarity of Task Definition:\n  (1) Typically, models with a 2B parameter size have limited instruction-following capabilities. Is the task description in\n  the prompt overly complex, insufficiently concise, or unclear, leading to the model’s inability to comprehend the task\n  (2) Are the descriptions of options unclear, causing the model to misunderstand the meaning of the options?\n  (3) Are the boundaries between the options indistinct, causing the model to easily confuse certain options?\n  2. Model Capability:\n  (1) Although the task and options are clearly described, the model’s capacity may be insufficient to solve the task. It\n  might be necessary to attempt task decomposition or other methods to reduce task complexity.\n  Based on the error cause analysis, please propose improvement methods for this prompt. Note that in the improvement\n  suggestions regarding options, the names of the options must not be changed. Instead, identify the boundaries between\n  option definitions to help the model clearly understand the specific meaning of each option and prevent confusion.\n  Please follow this format to structure your output: {\"Error Causes\": \"\", \"Improvement Methods\": \"\"}\n\n\n                                      Table 9: Error analysis prompt.\n\n You are currently an accomplished prompt engineer. Your task is to optimize the prompt used for inference based on\n  historical records of prompt optimization, the current inference prompt, error distribution, and error cause analysis,\n  with the aim of enhancing the inference performance of smaller models.\n The following are the prompts, accuracy rates, and error distribution information from previous inference rounds:\n {historical_results}\n  For the current inference, my prompt is:\n {prompt}\n The current analysis of error causes and directions for optimization are as follows:\n {error_analysis_results}\n  Please provide the revised prompt directly, omitting the process of analysis.\n\n\n                                    Table 10: Error summary prompt.\n\nA.2.1  Strategy Definition                           signs a specific role to MLLMs, guiding their\n                                                approach to generating responses and framing\nDetailed definition of strategies are followed:\n                                                       the context accordingly.\n • Reasoning represents that the prompt requires                                                              • Decomposition represents that the prompt re-\n MLLMs to explicitly articulate their logical pro-                                                         quires MLLMs to decompose complex questions\n  cess and rationale within the output, enabling a                                                         into simpler sub-instructions, addressing each\n  deeper understanding of the question’s essence                                                    sub-question individually for clarity and preci-\n  and ensuring the response’s validity.                                                           sion.\n • Reinterpretation represents that the prompt re-                                                              • Self-Criticism represents that the prompt re-\n  quires MLLMs to re-read and reinterpret the                                                      quires MLLMs to critically reflect on their re-\n  question before answering, ensuring comprehen-                                                     sponses, identifying and addressing potential\n  sion is both accurate and contextually appropri-                                                weaknesses or errors.\n   ate.                                                              • Caption represents that the prompt requires\n • Simplification represents that the prompt re-                                  MLLMs to provide a detailed description or cap-\n   quires MLLMs to remove irrelevant information,                                                           tion of an image before generating an answer, en-\n  ensuring that the content is concise and directly                                                   hancing contextual understanding and relevance.\n  focused on the question.                                                              • Rephrasing represents that the prompt requires\n • Role-Prompting represents that the prompt as-\n\n\n                                         14\n\nImage                        Commodity\n  Models                   Graphic  Commodity  Commodity  Commodity  Commodity  Concatenation  Average\n                           Layout    Location    Information  Background    Border        Situation\n\n  InternVL2-2B (Zero-Shot)     16.7         5.2           14.6          42.7          54.2             5.2          23.1\n\n  Qwen2-VL-72B-Instruct       31.3        41.7          35.4          52.1          55.2           64.6          46.7\n\n Qwen-VL-Max                36.5        45.8          35.4          66.7          55.2           67.7          51.2\n\n  Gemini-2.0-Flash              36.5        41.7          38.5          61.5          57.3           88.5          54.0\n\n  GPT-4o                        47.9        51.0          45.8          69.7          59.4           89.6          60.6\n\n\n  Table 11: Performance comparison of various closed-source and open-source models for prompt optimization.\n\n MLLMs to thoroughly understand the task,                                                                                    causal_judgement    disambiguation_qa\n  rephrase it based on their comprehension, and     Method                                                                                                        (train / test / overall)   (train / test / overall)\n   outline a detailed plan for its completion.\n  The above  strategies  are  validated  general      Zero-Shot         54.1 / 49.3 / 50.3      62.0 / 63.0 / 62.8\nprompt optimization strategies across various do-    OPRO             78.4 / 64.0 / 66.8      78.0 / 75.0 / 75.6\nmains, which enhances the generalizability of our     IDEALPrompt     83.8 / 64.7 / 68.4      82.0 / 77.0 / 78.0\nmethod and prevents potential overfitting to spe-\ncific domains or tasks.                                     Table 12: Accuracies on part of BBH tasks.                                                 10\n                                                  9                                          8.75A.2.2  Strategy Prompt\n                                                  8                             7.67\nOur specific prompts to add strategies by general       7\nMLLMs are shown in Table 8.                        6    5.5         5.83\n                                                  5A.3   Self-reflective Detail                              Score 4\n                                                  3The number of critical bad cases is set to 5.\n                                                  2\nA.3.1   Self-reflective Prompt                       1\n                                                  0\nTable 9 and Table 10 shows the error analysis         Zero-Shot    APE     OPRO  IDEALPrompt\nprompt and error summary prompt in Empirical                                                        Figure 7: Comparison of human evaluation scores.\nSelf-reflective Optimization, respectively.\n                                           Experiments on Public Domain Benchmark.\nA.4  Additional Experimental Results          To evaluate the effectiveness and universality of\n                                          IDEALPrompt, we conducted experiments on pub-\nExperiments on Various MLLMs for Optimiza-\n                                                                lic domain benchmark, comparing with zero-shot\ntion.  In Table 1, we uses GPT-4o for prompt op-\n                                            and OPRO. BBH is a public domain benchmark\ntimization. We supplemented closed-source mod-\n                                            which is often used for verifying prompt optimiza-\nels (Qwen-VL-Max, Gemini-2.0-Flash) with much\n                                                          tion; therefore, we selected part of BBH tasks for\nlower prices than GPT-4o and a powerful open-\n                                                      evaluation on InternVL2-2B. Following OPRO, we\nsource model (Qwen2-VL-72B-Instruct) to guide\n                                                        select 20% of the data for each task as the train-\nprompt optimization for experiments, as illustrated\n                                                  ing set, with the remaining 80% used as test set.\nin Table 11.\n                                                  Consequently, we present accuracies in the format\n  The cost of Qwen-VL-Max is approximately\n                                                   of “training / test / overall (training + test)”, as\none-sixth of that of GPT-4o, and the cost of\n                                                          illustrated in Table 12. The results indicate that\nGemini-2.0-Flash is about half that of GPT-4o,\n                                       IDEALPrompt still achieves better performance\nyet both achieve high performance. Qwen2-VL-\n                                          on public domain benchmark, demonstrating its\n72B-Instruct is a free and open-source model, with\n                                                     effectiveness not only on private domain-specific\nperformance slightly lower than that of Qwen-VL-\n                                                          tasks.\nMax and Gemini-2.0-Flash.\n  Our method allows for the flexible replacement  Human Evaluation. We further conduct a hu-\nof the model that guides prompt optimization, en-  man evaluation on Taobao-PDA. Specifically, we\nabling users to customize their selection.              recruit 6 well-educated people to rank the randomly\n\n\n                                         15\n\nA.5.2  Computational Efficiency Evaluation    Benchmark             Average Performance\n                                         The average cost of GPT-4o API calls required\n   Taobao-PDA (Chinese)           57.4\n                                                      for a single task is approximately 0.8 RMB. The\n   Taobao-PDA (English)           56.3\n                                                  average computation time required for a single task\n                                                                is approximately 15 minutes. We utilized a single\nTable 13:  Comparison of average performance on\n                                       A100 GPU throughout the optimization process.\nTaobao-PDA (Chinese) and Taobao-PDA (English).\n\n                                            A.6  Additional Discussion\nshuffled prompts optimized by APE, OPRO and\n                                                 A.6.1  Two-stage Effectiveness DiscussionIDEALPrompt. The scores range from 1 to 10 (10\nmeans best) and are allowed to be equal for com-   The two-stage optimization process we propose\nparable instances. As shown in Figure 7. IDEAL-   can be viewed as an optimization from coarse to\nPrompt demonstrates best performance in human    fine. In the first stage, RWS, a coarse optimization\nevaluation as well.                                based on strategies is performed. The second stage,\n                                        ESO, conducts a fine optimization based on self-\nMultilingual Experiments.  In addition, we con-    reflective mechanisms.\nducted multilingual experiments to demonstrate     Our ablation study have validated the indepen-\nthe effectiveness of our method across multiple lan-   dent effectiveness of both stages. We posit that the\nguages.  Specifically, we constructed a batch of    two-stage optimization process not only enhances\nEnglish prompt data and conducted relevant experi-    prior knowledge integration in the first-stage opti-\nments based on the current Taobao-PDA dataset,   mization strategies but also expedites the conver-\nas illustrated in Table 13. The results indicate that   gence of second-stage self-reflection mechanisms.\nour method remains effective in English as well.\n                                                 A.6.2  Novelty Discussion\n\n                                              Novelty of RWS.  Although search strategies\nA.5  Additional In-Depth Analysis\n                                              based on reinforcement learning are widely used,\nA.5.1  Failure Modes Analysis                   our implementation in IDEALPrompt is unique in\n                                                    the RWS stage. Strategy Pool: We have meticu-\nTaking the Commodity Background task as an ex-                                                    lously designed a strategy pool that includes vari-\nample, this task requires determining the back-                                               ous optimization strategies, which are selected and\nground color in a commodity image, with op-                                                  defined by human experts based on the character-\ntions being “white” and “others.” Typical failure                                                              istics of the private domain. This design not only\nmodes include: unclear task definitions, ambiguous                                                enhances the flexibility of optimization but also pro-\nboundaries between options, and incorrect identi-                                                   vides richer optimization directions for the model.\nfication of the background and its color. During                                    By defining such a strategy pool, our search effi-\nself-reflection, the model’s reasoning process is as                                                 ciency has significantly improved, and we can in-\nfollows: “Based on the error analysis, unclear task                                                        tegrate more state-of-the-art prompt strategies into\ndefinitions and option descriptions are the main                                                    the pool in the future. Exploration-Exploitation\nreasons why the model fails to accurately under-                                                 Strategy: We adopt an ε-greedy strategy in RWS\nstand the task and option meanings. Therefore, the                                                        to balance exploration and exploitation. This strat-\ntask description in the prompt needs further simpli-                                              egy enables the model to quickly identify effective\nfication, and the specific meanings of the options                                                  optimization strategy combinations on new tasks\nneed to be clarified. Additionally, including exam-                                            and models, significantly reducing the number of\nple images can help the model better understand                                                   search steps.\nthe task requirements. The current prompt has par-\ntially addressed these issues, but it can be further   Novelty of the Whole Framework.  Although\noptimized to enhance accuracy. Specifically, the   you pointed out that some individual components\nprompt can more explicitly explain the distinction   (such as RL-based search and self-reflective op-\nbetween ‘white’ and other colors, and provide con-   timization) are not entirely novel from a techni-\ncrete examples to aid the model’s understanding.”   cal perspective, we believe that the overall frame-\nThus, it can be intuitively observed that the model   work of IDEALPrompt is distinctly innovative. By\ncan correctly analyze the failure modes of historical    integrating the Reinforcement Warm-up Strategy\nresults.                                  (RWS) and Empirical Self-reflective Optimization\n\n\n                                         16\n\n(ESO), IDEALPrompt efficiently adapts to private\ndomain data. This two-stage optimization strategy\nhas not yet been implemented similarly in existing\nliterature, especially in terms of achieving rapid\nadaptation to private data distributions without pa-\nrameter fine-tuning and with only a small amount\nof data. Our ablation study demonstrate that both\nstages are highly effectiv\n\n\n\n\n\n                                         17",
"headers": [
"arXiv:2412.19684v2  [cs.AI]  17 Feb 2025",
"Boosting Private Domain Understanding of Efficient MLLMs: A",
"Tuning-free, Adaptive, Universal Prompt Optimization Framework"
],
"tables": [
"|Image Commodity Outfit<br>w/o<br>Method Parameter Graphic Commodity Commodity Commodity Commodity Concatenation Outfti Average<br>Tuning Layout Location Information Background Border Situation Style|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|**Fine-Tuning**<br>**MM-CoT**<br>Í<br>**Zero-Shot**<br>Í<br>**APE**<br>Í<br>**OPRO**<br>Í|**Fine-Tuning**<br>**MM-CoT**<br>Í<br>**Zero-Shot**<br>Í<br>**APE**<br>Í<br>**OPRO**<br>Í|**38.5**|26.0|**69.8**|**69.8**|**58.3**|**88.5**|**47.1**|**55.7**|\n|**Fine-Tuning**<br>**MM-CoT**<br>Í<br>**Zero-Shot**<br>Í<br>**APE**<br>Í<br>**OPRO**<br>Í|**Fine-Tuning**<br>**MM-CoT**<br>Í<br>**Zero-Shot**<br>Í<br>**APE**<br>Í<br>**OPRO**<br>Í|21.9<br>**27.1**<br>16.7<br>5.2<br>34.4<br>26.0<br>20.8<br>**27.1**|**27.1**|**27.1**|**27.1**|**27.1**|**27.1**|17.6<br>14.7<br>8.8<br>26.5|28.4<br>22.5<br>21.2<br>38.5|\n|**Fine-Tuning**<br>**MM-CoT**<br>Í<br>**Zero-Shot**<br>Í<br>**APE**<br>Í<br>**OPRO**<br>Í|**Fine-Tuning**<br>**MM-CoT**<br>Í<br>**Zero-Shot**<br>Í<br>**APE**<br>Í<br>**OPRO**<br>Í|21.9<br>**27.1**<br>16.7<br>5.2<br>34.4<br>26.0<br>20.8<br>**27.1**|**27.1**|**27.1**|**68.8**|**68.8**|**68.8**|**68.8**|**68.8**|\n|**IDEALPrompt**|Í|**47.9**|**51.0**|**45.8**|**69.7**|**59.4**|**89.6**|**38.2**|**57.4**|",
"|Stages<br>Method Average Performance<br>Stage 1 Stage 2|Col2|Col3|Col4|\n|---|---|---|---|\n|**Method**<br>Stages<br>**Average Performance**<br>**Stage 1**<br>**Stage 2**|**Stage 1**<br>**Stage 2**|**Stage 1**<br>**Stage 2**|**Stage 1**<br>**Stage 2**|\n|Zero-Shot<br>w/o Self-reflection<br>w/o Warm-up Strategy|Í<br>Í|Í<br>Í|22.5<br>43.5<br>41.1|\n|<br>**IDEALPrompt**|Í|Í|**56.3**|",
"|Method|Model|Average<br>Performance|Inference<br>Time|GPU Memory<br>Usage|\n|---|---|---|---|---|\n|**Zero-Shot**|InternVL2-8B|52.2|~1.2_×_|~16GB|\n|**IDEALPrompt**|InternVL2-2B|56.3|~1_×_|~4GB|",
"|ii|. If yes, what is the relative position of the text and image?|\n|---|---|",
"|iii|. If the structure is left or right, is the text on the left or right?|\n|---|---|",
"|iv|. If the structure is up or down|\n|---|---|",
"|Method|Average<br>Performance|Iteration<br>Steps|\n|---|---|---|\n|**OPRO**|38.5|50_ ±_ 3|\n|**IDEALPrompt**|56.3|15_ ±_ 3|",
"|Method|Graphic Commodity<br>Layout Location|Col3|Commodity Commodity Commodity Concatenation<br>Information Background Border Situation|Col5|Col6|Col7|Outfti<br>A<br>Style|verage|\n|---|---|---|---|---|---|---|---|---|\n|**Zero-Shot**|33.3<br>52.1|33.3<br>52.1|56.3<br>38.5<br>67.7<br>76.0|56.3<br>38.5<br>67.7<br>76.0|56.3<br>38.5<br>67.7<br>76.0|56.3<br>38.5<br>67.7<br>76.0|41.2|52.2|\n|**IDEALPrompt**|**71.9**|**57.3**|**61.5**|**42.7**|**69.8**|**84.4**|**76.5**|**66.3**|",
"|Type|EMLLM|Method|Performance|Col5|Col6|Steps|\n|---|---|---|---|---|---|---|\n|**Type**|**EMLLM**|**Method**|Image<br>Commodity<br>**Average**|Image<br>Commodity<br>**Average**|Image<br>Commodity<br>**Average**|Image<br>Commodity<br>**Average**|\n|**Tasks**|InternVL2-2B|w/o adapt|49.0<br>49.7<br>49.5|49.0<br>49.7<br>49.5|49.0<br>49.7<br>49.5|93|\n|**Tasks**|InternVL2-2B|w adapt|49.5|66.1|60.6|13|\n|**Models**|Qwen2-VL-2B|w/o adapt|16.1<br>62.8<br>47.2|16.1<br>62.8<br>47.2|16.1<br>62.8<br>47.2|93|\n|**Models**|Qwen2-VL-2B|w adapt|17.7|58.6|45.0|13|",
"|Task Initial Prompt|Col2|\n|---|---|\n|Graphic<br>Layout<br>Based on the input image, determine the value of the \"overall graphic layout label of the overall image information<br>category\". The meaning of this label is: the overall graphic layout structure of the image. This is a single-choice<br>question, options include: uncertain/left-right structure-left text right image/left-right structure-left image right text/left-<br>right structure-left and right image splicing/up-down structure-up image down text/up-down structure-up text and down<br>image/up-down structure-up and down image splicing. Please output the answer directly, do not include the analysis<br>process.|Graphic<br>Layout<br>Based on the input image, determine the value of the \"overall graphic layout label of the overall image information<br>category\". The meaning of this label is: the overall graphic layout structure of the image. This is a single-choice<br>question, options include: uncertain/left-right structure-left text right image/left-right structure-left image right text/left-<br>right structure-left and right image splicing/up-down structure-up image down text/up-down structure-up text and down<br>image/up-down structure-up and down image splicing. Please output the answer directly, do not include the analysis<br>process.|\n|Commodity<br>Location|Based on the input image, determine the value of the \"commodity image location label of the overall image information<br>category\". The meaning of this label is: the location of the commodity image in the overall image. This is a<br>single-choice question, options include: no commodity/uncertain/full screen/center of screen/left and right halves/left<br>half/right half/up and down halves/top half/bottom half. Please output the answer directly, do not include the analysis<br>process.|\n|Commodity<br>Information|Based on the input image, determine the value of the \"commodity information label of the commodity category\". The<br>meaning of this label is: commodity information. This is a single-choice question, options include: no commodity/single<br>commodity single style/single commodity multiple styles/multiple commodities. Please output the answer directly, do<br>not include the analysisprocess.|\n|Commodity<br>Background|Based on the input image, determine the value of the \"commodity image background label of the commodity category\".<br>The meaning of this label is: the background color of the commodity image. This is a single-choice question, options<br>include: white/other. Please output the answer directly, do not include the analysisprocess.|\n|Commodity<br>Border|Based on the input image, determine the value of the \"commodity image border label of the commodity category\". The<br>meaning of this label is: the shape of the commodity image’s border. This is a single-choice question, options include:<br>no obvious border/square border/other shaped border. Please output the answer directly, do not include the analysis<br>process.|\n|Concatenation<br>Situation|Based on the input image, determine the value of the \"commodity image concatenation situation label of the commodity<br>category\". The meaning of this label is: the concatenation situation of the commodity image. This is a single-choice<br>question, options include: single image without concatenation/2 images concatenation/3 images concatenation/4 images<br>concatenation/5 or more images concatenation. Please output the answer directly, do not include the analysisprocess.|\n|Outfit<br>Style|Based on the input image, determine the value of the \"outfit style label of the outfit category\". The meaning of this label<br>is: the outfit style of the person in the image. This is a single-choice question, options include: Anklei/Girlcore/Sporty<br>Chic/Y3K/Mori/Dopamine/Bohemian/Intellectual. Please output the answer directly, do not include the analysis<br>process.|",
"|Strategy Prompt|Col2|\n|---|---|\n|Reasoning<br>{prompt} + Please carefully understand the question before answering and provide your thought and analysis process.|Reasoning<br>{prompt} + Please carefully understand the question before answering and provide your thought and analysis process.|\n|Reinterpretation|{prompt} + Please do not rush to answer; before providing a response, reread and carefully understand my question<br>and requirements.|\n|Simplification|You are now an expert prompt engineer, tasked with optimizing prompts to help smaller models accurately reason<br>through complex problems. Focus on simplifying the expression of the problem and its requirements.<br>My prompt is: {prompt}.<br>Please outputyour optimizedprompt directly withoutproviding the analysisprocess.|\n|Role-Prompting|You are now an expert prompt engineer, tasked with optimizing prompts to help smaller models accurately reason<br>through complex problems. Focus on incorporating role-playing (e.g., “You are a xxx...”) as a method of optimization.<br>My prompt is: {prompt}.<br>Please output the optimizedprompt directly withoutproviding the analysisprocess.|\n|Decomposition|You are now an expert prompt engineer, tasked with optimizing prompts to help smaller models accurately reason<br>through complex problems. Focus on decomposing the problem description into a combination of multiple simple and<br>understandable questions to enhance performance through multi-step reasoning.<br>My prompt is: {prompt}.<br>Please output the optimizedprompt directly withoutproviding the analysisprocess.|\n|Self-Criticism|{prompt} + Please do not rush to answer, before giving the answer, carefully reflect on your answer is correct, confirm<br>the answer is correct before output the answer.|\n|Caption|You are now an expert prompt engineer, tasked with optimizing prompts to help smaller models accurately reason<br>through complex problems. Focus on having the model first provide a detailed description of the image content, and<br>then formulate an answer based on this description.<br>My prompt is: {prompt}.<br>Please output the optimizedprompt directly withoutproviding the analysisprocess.|\n|Rephrasing|{prompt} + Did you understand the task above? Please summarize the tasks you need to do and show how you will<br>execute the detailed plan for the task|",
"|Image Commodity<br>Models Graphic Commodity Commodity Commodity Commodity Concatenation Average<br>Layout Location Information Background Border Situation|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|**InternVL2-2B (Zero-Shot)**<br>16.7<br>5.2<br>14.6<br>42.7<br>54.2<br>5.2<br>23.1<br>**Qwen2-VL-72B-Instruct**<br>31.3<br>41.7<br>35.4<br>52.1<br>55.2<br>64.6<br>46.7<br>**Qwen-VL-Max**<br>36.5<br>45.8<br>35.4<br>66.7<br>55.2<br>67.7<br>51.2<br>**Gemini-2.0-Flash**<br>36.5<br>41.7<br>38.5<br>61.5<br>57.3<br>88.5<br>54.0|**InternVL2-2B (Zero-Shot)**<br>16.7<br>5.2<br>14.6<br>42.7<br>54.2<br>5.2<br>23.1<br>**Qwen2-VL-72B-Instruct**<br>31.3<br>41.7<br>35.4<br>52.1<br>55.2<br>64.6<br>46.7<br>**Qwen-VL-Max**<br>36.5<br>45.8<br>35.4<br>66.7<br>55.2<br>67.7<br>51.2<br>**Gemini-2.0-Flash**<br>36.5<br>41.7<br>38.5<br>61.5<br>57.3<br>88.5<br>54.0|**InternVL2-2B (Zero-Shot)**<br>16.7<br>5.2<br>14.6<br>42.7<br>54.2<br>5.2<br>23.1<br>**Qwen2-VL-72B-Instruct**<br>31.3<br>41.7<br>35.4<br>52.1<br>55.2<br>64.6<br>46.7<br>**Qwen-VL-Max**<br>36.5<br>45.8<br>35.4<br>66.7<br>55.2<br>67.7<br>51.2<br>**Gemini-2.0-Flash**<br>36.5<br>41.7<br>38.5<br>61.5<br>57.3<br>88.5<br>54.0|**InternVL2-2B (Zero-Shot)**<br>16.7<br>5.2<br>14.6<br>42.7<br>54.2<br>5.2<br>23.1<br>**Qwen2-VL-72B-Instruct**<br>31.3<br>41.7<br>35.4<br>52.1<br>55.2<br>64.6<br>46.7<br>**Qwen-VL-Max**<br>36.5<br>45.8<br>35.4<br>66.7<br>55.2<br>67.7<br>51.2<br>**Gemini-2.0-Flash**<br>36.5<br>41.7<br>38.5<br>61.5<br>57.3<br>88.5<br>54.0|**InternVL2-2B (Zero-Shot)**<br>16.7<br>5.2<br>14.6<br>42.7<br>54.2<br>5.2<br>23.1<br>**Qwen2-VL-72B-Instruct**<br>31.3<br>41.7<br>35.4<br>52.1<br>55.2<br>64.6<br>46.7<br>**Qwen-VL-Max**<br>36.5<br>45.8<br>35.4<br>66.7<br>55.2<br>67.7<br>51.2<br>**Gemini-2.0-Flash**<br>36.5<br>41.7<br>38.5<br>61.5<br>57.3<br>88.5<br>54.0|**InternVL2-2B (Zero-Shot)**<br>16.7<br>5.2<br>14.6<br>42.7<br>54.2<br>5.2<br>23.1<br>**Qwen2-VL-72B-Instruct**<br>31.3<br>41.7<br>35.4<br>52.1<br>55.2<br>64.6<br>46.7<br>**Qwen-VL-Max**<br>36.5<br>45.8<br>35.4<br>66.7<br>55.2<br>67.7<br>51.2<br>**Gemini-2.0-Flash**<br>36.5<br>41.7<br>38.5<br>61.5<br>57.3<br>88.5<br>54.0|**InternVL2-2B (Zero-Shot)**<br>16.7<br>5.2<br>14.6<br>42.7<br>54.2<br>5.2<br>23.1<br>**Qwen2-VL-72B-Instruct**<br>31.3<br>41.7<br>35.4<br>52.1<br>55.2<br>64.6<br>46.7<br>**Qwen-VL-Max**<br>36.5<br>45.8<br>35.4<br>66.7<br>55.2<br>67.7<br>51.2<br>**Gemini-2.0-Flash**<br>36.5<br>41.7<br>38.5<br>61.5<br>57.3<br>88.5<br>54.0|**InternVL2-2B (Zero-Shot)**<br>16.7<br>5.2<br>14.6<br>42.7<br>54.2<br>5.2<br>23.1<br>**Qwen2-VL-72B-Instruct**<br>31.3<br>41.7<br>35.4<br>52.1<br>55.2<br>64.6<br>46.7<br>**Qwen-VL-Max**<br>36.5<br>45.8<br>35.4<br>66.7<br>55.2<br>67.7<br>51.2<br>**Gemini-2.0-Flash**<br>36.5<br>41.7<br>38.5<br>61.5<br>57.3<br>88.5<br>54.0|\n|**GPT-4o**|**47.9**|**51.0**|**45.8**|**69.7**|**59.4**|**89.6**|**60.6**|",
"|Graphic Commodity<br>Layout Location|Commodity Commodity Commodity Concatenation<br>Information Background Border Situation|\n|---|---|",
"|causal judgement disambiguation qa<br>Method _ _<br>(train / test / overall) (train / test / overall)|Col2|Col3|\n|---|---|---|\n|**Zero-Shot**<br>54.1 / 49.3 / 50.3<br>62.0 / 63.0 / 62.8<br>**OPRO**<br>78.4 / 64.0 / 66.8<br>78.0 / 75.0 / 75.6|**Zero-Shot**<br>54.1 / 49.3 / 50.3<br>62.0 / 63.0 / 62.8<br>**OPRO**<br>78.4 / 64.0 / 66.8<br>78.0 / 75.0 / 75.6|**Zero-Shot**<br>54.1 / 49.3 / 50.3<br>62.0 / 63.0 / 62.8<br>**OPRO**<br>78.4 / 64.0 / 66.8<br>78.0 / 75.0 / 75.6|\n|**IDEALPrompt**|**83.8** /** 64.7** /** 68.4**|**82.0** /** 77.0** /** 78.0**|",
"|T 0|Table 12: Accuracies on part of BBH tasks.|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|8<br>9<br>||||||||||\n|8<br>9<br>|~~8.75~~|~~8.75~~|~~8.75~~|~~8.75~~|~~8.75~~|~~8.75~~|~~8.75~~|~~8.75~~|~~8.75~~|\n|8<br>9<br>|~~7.67~~|~~7.67~~|~~7.67~~|~~7.67~~|~~7.67~~|~~7.67~~|~~7.67~~|~~7.67~~||\n|8<br>9<br>|~~7.67~~|~~7.67~~|~~7.67~~|~~7.67~~|~~7.67~~|||||\n|2<br>3<br>4<br>5<br>6<br>7|~~583~~|~~583~~|~~583~~|~~583~~|~~583~~|||||\n|2<br>3<br>4<br>5<br>6<br>7|~~5.5~~<br>|~~5.5~~<br>|~~5.5~~<br>|~~.~~||||||\n|2<br>3<br>4<br>5<br>6<br>7||||||||||\n|2<br>3<br>4<br>5<br>6<br>7||||||||||\n|2<br>3<br>4<br>5<br>6<br>7||||||||||\n|1<br>||||||||||\n|h<br><br>0<br>||||||||||\n|h<br><br>0<br>||||||||||"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2412.19684v2.pdf"
}