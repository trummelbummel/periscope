{
"text": "Emotion-Conditioned Text Generation through\n                          Automatic Prompt Optimization\n\n\n                             Yarik Menchaca Resendiz and Roman Klinger\n                              Institut für Maschinelle Sprachverarbeitung, University of Stuttgart\n                  {yarik.menchaca-resendiz,roman.klinger}@ims.uni-stuttgart.de\n\n\n\n\n                          Abstract                                         I.  Input prompt               Generated text\n\n                                                                   0  Text with disgust       Disgust     is     a\n                  Conditional natural language generation meth-                                                                                              character in Inside\n                ods often require either expensive fine-tuning                                    Out\n                   or training a large language model from scratch.\n                                                                   1  Text expressing disgust  Disgusting\n                Both are unlikely to lead to good results with-\n                  out a substantial amount of data and compu-        2  Write a text to express  A  look  of  disgust2023                                                                disgust                 came over his face.                    tational resources. Prompt learning without\n                 changing the parameters of a large language\n                                                                  Table 1: Hypothetical example for a prompt optimiza-               model presents a promising alternative.  It is\n                                                                              tion process. The seed prompt is given in Iteration (I.) 0Aug           a cost-effective approach, while still achiev-\n                                                             and misinterpreted to mention the character “Disgust”.                  ing competitive results. While this procedure is9                                                                   This issue is fixed through iterative optimization.              now established for zero- and few-shot text clas-\n                     sification and structured prediction, it has re-\n                  ceived limited attention in conditional text gen-\n                    eration. We present the first automatic prompt                                                                         et al., 2017; Song et al., 2019; Zhou et al., 2018;\n                   optimization approach for emotion-conditioned\n                                                     Menchaca Resendiz and Klinger, 2023).[cs.CL]              text generation with instruction-fine-tuned mod-                                                                    In areas like text classification or structured pre-                       els. Our method uses an iterative optimization\n                 procedure that changes the prompt by adding,        diction, prompt optimization has established itself\n                 removing, or replacing tokens. As objective       as a zero- or few-shot learning paradigm (Ding\n                   function, we only require a text classifier that         et al., 2022; Zhang et al., 2022; Wang et al., 2022),\n                 measures the realization of the conditional vari-       also in emotion analysis (Plaza-del Arco et al.,\n                  able in the generated text. We evaluate the       2022; Zheng et al., 2022; Yin et al., 2019). Here,\n               method on emotion-conditioned text genera-                                                            only parameters that are concatenated to the in-\n                    tion with a focus on event reports and compare\n                                                              put are optimized and the large language model’s\n                           it to manually designed prompts that also act\n                                                              parameters are frozen. Such models, therefore, ex-                   as the seed for the optimization procedure. The\n                  optimized prompts achieve 0.75 macro-average        ploit encoded knowledge in models such as Flan\n                F1 to fulfill the emotion condition in contrast       (Tay et al., 2023), GPT-3 (Brown et al., 2020) and\n                   to manually designed seed prompts with only       Alpaca (Taori et al., 2023) more explicitly than fine-\n                  0.22 macro-average F1.                           tuning them for the task at hand. The optimizationarXiv:2308.04857v1                                                     method learns “how to use” a model, not “how to\n          1  Introduction\n                                                          change” it.\n           Emotions are fundamental  in communication,      In recent instruction-based models, the prompt\n           where they play an important role in transferring     is an instruction to elicit a desired response. The\n           meaning and intent (Ekman, 1992).  Emotion-    instruction serves as a starting point for generating\n             conditioned natural language generation models    text that aligns with the intended task. Prompting in\n           aim at improving human–computer interaction, by    text classification (Hu et al., 2022; Gu et al., 2022)\n             generating text that is not limited to conveying    usually includes the instruction (e.g., “classify the\n             propositional information. However, state-of-the-    text...”) and the label representation (e.g., “posi-\n               art conditional generation models require a large    tive”, “negative”). Summarization has been repre-\n           amount of data and computational power to achieve    sented as an instruction by appending “TL;DR” or\n           models that allow for a fine-grained control over   “summarize” (Radford et al., 2019; Narayan et al.,\n             the generated texts (Pascual et al., 2021; Ghosh    2021). For text generation that translates tables\n\nto text, Li and Liang (2021) proposed to tune a       Original Prompt     Oper.  Modified Prompt\nprefix prompt to accomplish the task. In machine       Text that expresses   Add.   Text string that expresses\ntranslation, prompts typically mention the source       Text that expresses   Repl.   Text a expresses\nand target language, such as “translate English to       Text that expresses  Rem.   Text expresses\nGerman” (Raffel et al., 2020).\n                                                     Table 2: The prompt operations (Oper.) are performed\n  The task of prompt optimization can be formu-                                              on the same prompt.  The Addition (Add.)  adds\nlated in various directions. The goal is to find the   RoBERTa’s special mask token (<mask>) between Text\noptimal sequence of tokens to represent the prompt   and that.  The Replacement (Repl.)  masks the tar-\nfor a specific model (e.g., Flan) and task (e.g.,    get word (that). The unmasked/predicted tokens by\nsummarization), while keeping the model weights   RoBERTa are underlined, and the replaced or removed\nunchanged. AutoPrompt (Shin et al., 2020) de-   tokens from the original are in bold. Removal (Rem.)\n                                                            deletes one token from the prompt.fines the prompt optimization as “fill-in-the-blanks”\nbased on a gradient-guided search. OpenPrompt\n(Ding et al., 2022) provides a toolkit for training   one “parent” prompt, we create λ > 1 “children”.\nprompts using a template dataset, along with cor-      Addition adds the most probable token at any po-\nresponding verbalizers for different classes. Deng    sition within the prompt, including both the begin-\net al. (2022) use reinforcement learning to infer a    ning and end of the prompt. We use the pre-trained\nsuccessful prompt variation strategy. A different   RoBERTa model (Liu et al., 2019) to retrieve prob-\napproach for optimization is fine-tuning the model    able tokens for each of these positions. Removal\nto improve its performance with a specific prompt,    deletes a token from the prompt. The Replacement\nwhile keeping the prompt unchanged (Jian et al.,   operation exchanges a token by the most probable\n2022; Gu et al., 2022).                               token, again as predicted by RoBERTa.\n   In contrast to most previous work, we use models     The Addition and Replacement operations use\nthat have been fine-tuned to solve instruction-based    the <mask> special token to predict the word. We\ntasks; in our case to generate emotion-conditioned   exemplify these operations in Table 2.\ntexts. This comes with distinct challenges because\n                                              Text generation. We then use each of the λthe loss function cannot be determined by a sin-\n                                            prompt variations to create text using a large pre-gle expected label (e.g., positive or negative). In\n                                                     trained language model (e.g., Flan). To do so, weour work, we use a classifier that measures the ful-\n                                                           instantiate it with the emotion category. We refer tofillment of the condition as a source to calculate\n                                                             this instantiation as the Conditional-Prompts. Eachthe value of an objective function. The optimiza-\n                                                   of them consists of the modified prompt and thetion procedure that we propose is an evolutionary\n                                                        specified emotion (e.g., “Text that expresses ⟨em⟩”).optimization method (Simon, 2013). Next to the\n                                                  Here, ⟨em⟩is replaced by each of the emotion cate-objective function, an important component are ac-\n                                                     gories under consideration.tions that allow changes to a prompt to explore the\nsearch space.                                               Evaluation.  Each prompt  is  then  evaluated\n                                                 through the texts that are generated with its instan-\n2  Methods\n                                                          tiated Conditional-Prompts. In the evaluation, we\nWe propose a method (summarized in pseudocode   do not further consider texts that are a paraphrase\nin Algorithm 1) for text generation conditioned on    of the Conditional-Prompt. We calculate the BLEU\nemotions using prompt optimization.  It involves    score (Papineni et al., 2002) and filter all texts with\nan iterative optimization procedure with three mod-   a score greater than 0.2. For example, a language\nules, namely prompt modification, text generation,   model could generate “The text expresses joy.” for\nand prompt evaluation. We describe the modules    a Conditional-Prompt “Text that expresses joy”.\nin Section 2.1 and the iterative optimization in Sec-     The actual evaluation is performed by comparing\ntion 2.2.                                            the emotion condition to the judgment of an emo-\n                                                      tion classifier, applied to the generated texts. We\n2.1  Modules                                                use the F1 measure both as an objective function\nPrompt modification.  In each optimization iter-   during optimization and for final evaluation. Note\nation, we apply the three operations, one at a time,    that these two scores are based on two separate\nto all the tokens in the prompt. Therefore, based on    classifiers, trained on independent data.\n\n2.2  Iterative Optimization                     Algorithm 1: Automatic Prompt Optimiza-\nAlgorithm 1 shows the iterative prompt optimiza-     tion. Eval involves an emotion classifier\ntion for a given seed prompt P (e.g., “Text that    and the BLEU score.\nexpresses”). The optimization is based on a (µ, λ)      Input  :Seed Prompt P,\n                                                    Maximum Iterations N\nevolutionary algorithm (Eiben and Smith, 2015),      Output :Optimized Prompt Popt\nmore concretely (1, λ), because we keep only the      Popt ←P;\none best-performing prompt for the next optimiza-        i ←0;\ntion iteration. In contrast to a (µ+λ), the respective      Pcands ←{};\n                                                            while i < N do\nparent is not further considered in the next iteration.                                              Pmod ←{};\nThis makes the algorithm less likely to get stuck in            for token ∈Popt do\na local optimum.                                    Pmod += Add(Popt, token);\n                                                 Pmod += Replace(Popt, token);   Initially, Popt (the optimized prompt) is initial-\n                                                 Pmod += Remove(Popt, token);ized with the seed prompt P. Next, each token in\n                                                        Topt ←{};\nPopt is modified using the Addition, Replacement,            for promptmod ∈Pmod do\nand Removal. Each operation is performed one at           Tmod ←Generate(promptmod);\na time, and the results are stored in Pmod (Sec-                      if Eval(Tmod) > Eval(Topt) then\ntion 2.1). The Generate method produces a text                  Popt ←promptmod;\n                                                               Topt ←Tmod;for each Conditional-Prompt-combination of the\ninput prompt and the emotion class (e.g., “Text that        Pcands+= Popt\nexpresses joy”, “Text that expresses anger”; Sec-              i ←i + 1;\ntion 2.1). We compare the generated text from      Popt ←select-one-best(Pcands);\n                                                            return Popt;Popt (namely Topt) against the generated text\nfrom each modified prompt (Pmod), denoted as\nTmod.  If the F1 of Tmod is higher than that of                                                      the same subset of emotions as the ISEAR dataset.1\nTopt, the prompt promptmod is assigned as the                                             Both classifiers are built on top of RoBERTa using\nnew optimized prompt (Popt) and added to the\n                                                       default parameters for 10 epochs.2\nbest-performing candidates (Pcands). Finally, this                                              These data sets are independent of each other,\nprocess is repeated for a total of N times and Popt\n                                              and therefore the objective signal is independent ofis updated with the best-performing prompt from\n                                                    the final evaluation. Both sets, however, are com-\nPcands.                                                     parable: they contain texts in which people were\n3  Experiments                              asked to report on an emotion-triggering event,\n                                                  given a predefined emotion. In the original ISEAR\nSection 3.1 explains the experimental settings used                                                   corpus, these texts were acquired in an in-lab set-\nto optimize an initial prompt that we assume to                                                         ting in the 1990s, while the crowd-enVENT corpus\nbe provided by a user.  Section 3.3 validates                                                has recently been collected in 2022 in a crowd-\nthe proposed method by showing that emotion-                                                 sourcing setup. An example from the ISEAR cor-\nconditioned text generation improves when using                                              pus is “When I was involved in a traffic accident.”\nthe optimized prompt compared to the seed prompt.                                          – an example from crowd-enVENT is “When my\n                                               son was poorly with covid”.3.1  Experimental Settings\nTo validate the  feasibility of our method for   Prompt Modification.  We selected a straightfor-\nemotion-conditioned text generation, and its cost-   ward seed prompt—“Write a text that expresses\neffectiveness in terms of data and computational   ⟨em⟩”—for ten iterations and all operations.\nresources, we utilized available pre-trained models\n                                               Text Generation.  For each Conditional-Prompt,\nand datasets. Specifically, we used Flan (Tay et al.,\n                                     we generate the three most probable sentences us-\n2023), an open-source model trained on instruction-\n                                                  ing a beam search with a beam size of 30, a next-\nbased datasets, as a generative model. We trained\n                                                token temperature of 0.7, and a top-p (nucleus)\ntwo classifiers using (1) the ISEAR dataset (Scherer\nand Wallbott, 1994) for prompt optimization in       1The emotion labels are: Anger, Disgust, Fear, Guilt, Joy,\n                                                             Sadness, and Shame.\neach iteration, and (2) the crowd-enVent dataset\n                                                         2The crowd-enVent and ISEAR-based classifiers have\n(Troiano et al., 2023) for final evaluation, utilizing    macro-F1 of .78 and .77, respectively.\n\nI. Ope. Optimized Prompt (Popt)                   F1     to prompt optimization in other NLP tasks, where\n 0 —  Write a text that expresses ⟨em⟩                .28    the resulting prompts often become less human-\n 1 Repl. Write a text to expresses ⟨em⟩                 .80    readable. For example, in the fact retrieval task\n 2 Add. Write in a text to expresses ⟨em⟩               .91\n                                                “[X] limestone depositedati boroughDepending [Y]” 3 Add. Write in a text string to expresses ⟨em⟩         .88\n 4 Add. Write in a long text string to expresses ⟨em⟩    .94    performs better than “[X] is the capital of [Y]”\n 5 Rem. Write in long text string to expresses ⟨em⟩      .94    (Ding et al., 2022).\n 6 Repl. Write in long text strings to expresses ⟨em⟩     .91\n                                                   Table 4 showcases examples of generated texts\n                                            from various prompt candidates. The prompt can-Table 3: Prompt optimization at different iterations (I.),\nwith Iteration 0 representing the seed prompt. The ⟨em⟩    didates at the same iteration are a few examples of\ntoken represents any of the seven emotions in the ISEAR    the resulting prompt modifications as described in\ndataset. The macro F1 score is calculated using the    Section 2. The provided F1 scores refer to the per-\nISEAR classifier, across all the emotions.              formance of the prompt across the 7 emotions, not\n                                                    the performance of the specific examples shown.\n                                            Comparing the generated text from the seed prompt\nsample of 0.7. We ensure that our output excludes\n                                        (Row 1) and the first optimization (Row 2), we ob-\nsentences with repeated instances of the same bi-\n                                                     serve a better fulfillment of the emotion disgust for\ngram.\n                                                     the optimized prompt—the uncertainty expressed\nPrompt Evaluation. We filter out all prompts    in Row 1 indicates fear. Prompt modifications at\nwhere the average BLEU score is higher than 0.2    the same iteration have different performances. For\nacross all the conditional prompts. Next, we select    example, in Iteration 2 (Rows 4/5), there is a differ-\nthe prompt with the best F1 score using the ISEAR   ence of 33 pp in F1. It is important to note that the\nclassifier.                                             best F1 score does not always indicate an improve-\n                                           ment in fulfilling the condition of the generated\n3.2  State-of-the-art Baseline                                                             text. Sometimes, the best-scoring text can be a\nWe compare our method against the plug-and-play   paraphrase of the prompt, which may be falsely\nmethod proposed by Pascual et al. (2021)—a state-    classified as correct due to the presence of the emo-\nof-the-art model for affective text generation. To    tion class name (e.g., Row 6/Iteration 5, Row 3/It-\ndo so, we train the emotion discriminators that    eration 2).\nare part of that method on top of GPT-2 with the       Finally, Table 5 shows an independent evalua-\nISEAR dataset. The comparison is not straight-    tion of the method along with the results achieved\nforward since this method uses the prompt as a   with the method by Pascual et al. (2021). We report\nstarting point to generate the sentence, whereas   F1 scores for the ISEAR-based classifier used dur-\nour approach treats the prompt as an instruction.   ing the optimization process and the independent\nTherefore, we select the most frequent n-grams   crowd-enVENT-based classifier. The latter num-\nfrom the ISEAR dataset as prompts: “When I was”,    bers therefore constitute an independent evaluation\n“When a”, and “When someone”. For each prompt–    result. We observe that the numbers of both classi-\ndiscriminator combination, we generate the 5 most     fiers are comparable to each other. The comparison\nprobable sentences.                                   to the baseline shows that our seed prompt per-\n                                              forms on par with Pascual’s method (.18, .12, and\n3.3  Results\n                                                   .17 vs. .22, respectively). Our optimized prompt,\nWe begin the discussion of the results with Table 3,   however, shows a higher performance (.75 F1).\nwhich shows the prompt optimization and perfor-\nmance across iterations.  It reveals two notable   4  Conclusion and Future Work\nfindings: First, already the first iteration, compared\n                                                   In this study, we introduced the first automatic\nto the seed prompt in Iteration 0, shows an increase\n                                            prompt optimization method for text generation\nby 52 pp in F1. This is a change only by replacing                                                  conditioned on emotions. Our approach involved\n“that” with “to”. Given our selection criteria which\n                                                      three token operations: addition, replacement, and\ndoes not include the parent prompt in the selection,\n                                                 removal. We utilized a BLEU score and an au-\nthere can be a decrease in performance, which can\n                                                  tomatic classifier to filter and rank the modified\nbe observed in Iteration 2. Second, all prompts\n                                                prompts. We demonstrated that the optimized\nin Table 3—the best-performing prompts at each\n                                              prompts led to a higher fulfillment of the intended\niteration—are human-readable. This is in contrast\n\nIter.  Prompt                                    Generated text                        F1\n\n          0     Write a text that expresses disgust                     i don’t know what to do about it.         .28\n          1     Write a text to expresses disgust              i’m disgusted.                           .80\n          1     a text that expresses fear                      a text that expresses fear                 .90\n          2     Write in a text to expresses shame                     i don’t know how to express my shame.   .91\n          2     Write a text that to expresses shame           i’m sorry to hear that.                    .58\n          5     Write in a long enough string to expresses joy   a long enough string to express joy.       1.0\n          5     Write a long text string to expresses joy             i love you so much                       .94\n\nTable 4: Generated text examples from the same seed prompt (1st row) at different optimization steps. The macro\nF1 score is computed for the prompt across all the emotions using the ISEAR classifier.\n\n\n                                              crowd-   5  Ethical Considerations & Limitations\nMethod Prompt                   ISEAR enVent\n                                         The proposed method aims at optimizing prompts      When I was                          .18     .18\n Pascual      When a                               .43     .12    for conditional text generation, particularly when\n (2021)\n      When someone                       .21     .17    conditioned on emotions. The generated affective\n        Write a text that expresses ⟨em⟩        .28     .22     texts do not only serve as a source to study the Popt\n        Write in long text string to ex-     .94     .75     capabilities of large language models from a com-\n         presses ⟨em⟩\n                                                     putational perspective. We believe that they can\n                                                     also be of value to better understand the represen-Table 5: Comparison between our method (Popt) and\nPascual (2021) Rows 1–3 are the most frequent n-grams    tation of psychological concepts in automatically\nfor the ISEAR dataset. The 4th row corresponds to the    generated text. However, there are some risks as-\nseed prompt, and the 5th row represents the optimized    sociated with the method if not used with care,\nprompt. The macro-average F1-score for both ISEAR    primarily inherited from the underlying language\nand crowd-enVent datasets is computed across all emo-                                              model. Optimized prompts could potentially re-\ntions.\n                                                             sult in generating text that reinforces stereotypes or\n                                                   marginalize certain groups. When dealing with the\nemotions compared to the seed prompt, with a    expression of emotions, it is essential to exercise\n53 pp improvement in the F1 score.  It is a cost-   caution when employing these models due to their\neffective method in terms of both data and resource    potential impact on individuals.\nrequirements, while still achieving good results.     A limitation in our evaluation and method is that\n  This leads to important future work. While our   we rely heavily on the seed prompts. This can lead\napproach improves emotion-conditioned text gen-    to fast convergence—if the seed prompt is adequate\neration, there are several areas that need to be ex-    for the task, the optimization process is more likely\nplored further. First, we need to explore different    to be successful. The optimization is based on a\nsearch techniques for prompt optimization (e.g.,   (µ, λ) approach, which can be seen as a brute-force\nBeam search). Second, it is essential to compare    search. However, alternative search algorithms may\nthe performance of the optimized prompts across    provide a more efficient optimization of the prompt\ndifferent domains to assess the generalizability of    in terms of iterations.\nour method. Our evaluation is arguably compara-      Overall, the method has proven to be useful for\nbly narrow, with only one seed prompt and one    text generation conditioned on emotions. We invite\ndomain in which emotions are expressed. Finally,   people to keep the above limitations in mind when\nit is crucial to analyze our approach by comparing    considering the capabilities and applications of our\nit against a fine-tuned or trained model from scratch   method.\nto evaluate its effectiveness and efficiency.\n                                      Acknowledgements  Another interesting direction of research would\nbe to study in more detail how the expected domain                                                This work has been supported by a CONACYT\nof the generated texts (here: emotion self-reports)                                                   scholarship (2020-000009-01EXTF-00195) and\nmight be in conflict with the emotion condition and                                          by the German Research Council (DFG), project\nhow that can be encoded in either the optimization                                                “Computational Event Analysis based on Appraisal\nprocess, the seed prompt selection or the objective                                                Theories for Emotion Analysis” (CEAT, project\nfunctions, or in combinations of these parameters.                                          number KL 2869/1-2).\n\nReferences                                    Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\n                                                     Optimizing continuous prompts for generation. In\nTom Brown, Benjamin Mann, Nick Ryder, Melanie                                                       Proceedings of the 59th Annual Meeting of the Asso-\n   Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind                                                             ciation for Computational Linguistics and the 11th\n   Neelakantan, Pranav Shyam, Girish Sastry, Amanda                                                              International Joint Conference on Natural Language\n   Askell, et al. 2020. Language models are few-shot                                                       Processing (Volume 1: Long Papers), pages 4582–\n   learners. Advances in neural information processing                                                      4597, Online. Association for Computational Lin-\n   systems, 33:1877–1901.                                                                   guistics.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yi-                                                 Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\n  han Wang, Han Guo, Tianmin Shu, Meng Song, Eric                                                         dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n  Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing                                               Luke Zettlemoyer, and Veselin Stoyanov. 2019.\n   discrete text prompts with reinforcement learning.                                                        Roberta: A robustly optimized bert pretraining ap-\n   In Proceedings of the 2022 Conference on Empiri-                                                          proach.\n   cal Methods in Natural Language Processing, pages\n  3369–3391, Abu Dhabi, United Arab Emirates. As-   Yarik Menchaca Resendiz and Roman Klinger. 2023.\n   sociation for Computational Linguistics.                  Affective natural language generation of event de-\n                                                               scriptions through fine-grained appraisal conditions.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\n                                                            In Proceedings of the 16th International Conference\n  Zhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022.\n                                                  on Natural Language Generation, Prague, Czech Re-\n  OpenPrompt: An open-source framework for prompt-\n                                                               public. Association for Computational Linguistics.\n   learning. In Proceedings of the 60th Annual Meet-\n   ing of the Association for Computational Linguistics:                                                    Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo\n  System Demonstrations, pages 105–113, Dublin, Ire-                                                       Simões, Vitaly Nikolaev, and Ryan McDonald. 2021.\n   land. Association for Computational Linguistics.                                                        Planning with learned entity prompts for abstractive\n                                                       summarization. Transactions of the Association for\nAgoston E. Eiben and James E. Smith. 2015. Introduc-\n                                                     Computational Linguistics, 9:1475–1492.\n   tion to evolutionary computing. Springer.\n                                                    Kishore Papineni, Salim Roukos, Todd Ward, and Wei-\nPaul Ekman. 1992. An argument for basic emotions.\n                                                            Jing Zhu. 2002. Bleu: a method for automatic evalu-\n  Cognition & emotion, 6(3-4):169–200.\n                                                             ation of machine translation. In Proceedings of the\nSayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-      40th Annual Meeting of the Association for Compu-\n   Philippe Morency, and Stefan Scherer. 2017. Affect-       tational Linguistics, pages 311–318, Philadelphia,\n  LM: A neural language model for customizable af-      Pennsylvania, USA. Association for Computational\n   fective text generation. In Proceedings of the 55th       Linguistics.\n  Annual Meeting of the Association for Computational\n                                            Damian Pascual, Beni Egressy, Clara Meister, Ryan   Linguistics (Volume 1: Long Papers), pages 634–642,\n                                                                   Cotterell, and Roger Wattenhofer. 2021. A plug-and-  Vancouver, Canada. Association for Computational\n                                                         play method for controlled text generation. In Find-   Linguistics.\n                                                          ings of the Association for Computational Linguis-\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.       tics: EMNLP 2021, pages 3973–3997, Punta Cana,\n  2022. PPT: Pre-trained prompt tuning for few-shot      Dominican Republic. Association for Computational\n   learning. In Proceedings of the 60th Annual Meet-       Linguistics.\n   ing of the Association for Computational Linguistics\n                                                        Flor Miriam Plaza-del Arco, María-Teresa Martín-  (Volume 1: Long Papers), pages 8410–8423, Dublin,\n                                                             Valdivia, and Roman Klinger. 2022.  Natural lan-   Ireland. Association for Computational Linguistics.\n                                                    guage inference prompts for zero-shot emotion clas-\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan       sification in text across corpora. In Proceedings of\n   Liu, Jingang Wang, Juanzi Li, Wei Wu, and Maosong       the 29th International Conference on Computational\n  Sun. 2022. Knowledgeable prompt-tuning: Incor-       Linguistics, pages 6805–6817, Gyeongju, Republic\n   porating knowledge into prompt verbalizer for text       of Korea. International Committee on Computational\n   classification.  In Proceedings of the 60th Annual       Linguistics.\n  Meeting of the Association for Computational Lin-\n                                                 Alec Radford, Jeffrey Wu, Rewon Child, David Luan,   guistics (Volume 1: Long Papers), pages 2225–2240,\n                                                       Dario Amodei, Ilya Sutskever, et al. 2019. Language   Dublin, Ireland. Association for Computational Lin-\n                                                    models are unsupervised multitask learners. OpenAI   guistics.\n                                                             blog, 1(8):9.\nYiren Jian, Chongyang Gao, and Soroush Vosoughi.\n                                                    Colin Raffel, Noam Shazeer, Adam Roberts, Kather-  2022.  Contrastive learning for prompt-based few-\n                                                           ine Lee, Sharan Narang, Michael Matena, Yanqi   shot language learners. In Proceedings of the 2022\n                                                    Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the  Conference of the North American Chapter of the\n                                                                limits of transfer learning with a unified text-to-text  Association for Computational Linguistics: Human\n                                                             transformer. Journal of Machine Learning Research,  Language Technologies, pages 5577–5587, Seattle,\n                                                        21(140):1–67.  United States. Association for Computational Lin-\n   guistics.\n\nKlaus R. Scherer and Harald G. Wallbott. 1994. Evi-   Xiaopeng Zheng, Zhiyue Liu, Zizhen Zhang, Zhaoyang\n  dence for universality and cultural variation of dif-     Wang, and Jiahai Wang. 2022. UECA-prompt: Uni-\n   ferential emotion response patterning.  Journal of       versal prompt for emotion cause analysis. In Proceed-\n   personality and social psychology, 66(2):310.             ings of the 29th International Conference on Com-\n                                                            putational Linguistics, pages 7031–7041, Gyeongju,\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV,      Republic of Korea. International Committee on Com-\n   Eric Wallace, and Sameer Singh. 2020. AutoPrompt:       putational Linguistics.\n   Eliciting knowledge from language models with auto-\n   matically generated prompts. In Empirical Methods   Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan\n   in Natural Language Processing (EMNLP).             Zhu, and Bing Liu. 2018. Emotional chatting ma-\n                                                           chine: Emotional conversation generation with in-\nDan Simon. 2013.  Evolutionary Optimization Algo-       ternal and external memory. In Proceedings of the\n   rithms. John Wiley & Sons, Hoboken, New Jersey,      Thirty-Second AAAI Conference on Artificial Intelli-\n  USA.                                               gence and Thirtieth Innovative Applications of Artifi-\n                                                                 cial Intelligence Conference and Eighth AAAI Sym-\nZhenqiao Song, Xiaoqing Zheng, Lu Liu, Mu Xu, and                                                    posium on Educational Advances in Artificial Intelli-\n  Xuanjing Huang. 2019. Generating responses with                                                          gence, AAAI’18/IAAI’18/EAAI’18. AAAI Press.\n  a specific emotion in dialog. In Proceedings of the\n  57th Annual Meeting of the Association for Computa-\n   tional Linguistics, pages 3685–3695, Florence, Italy.\n  Association for Computational Linguistics.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n  Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\n  and Tatsunori B. Hashimoto. 2023. Stanford alpaca:\n  An instruction-following llama model.  https://\n  github.com/tatsu-lab/stanford_alpaca.\n\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,\n  Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara\n   Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil\n  Houlsby, and Donald Metzler. 2023. UL2: Unifying\n  language learning paradigms. In The Eleventh Inter-\n   national Conference on Learning Representations.\n\nEnrica Troiano, Laura Oberländer, and Roman Klinger.\n  2023.  Dimensional modeling of emotions in text\n  with appraisal theories: Corpus creation, annotation\n   reliability, and prediction. Computational Linguis-\n   tics, 49(1).\n\nJianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan,\n  Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang,\n  and Ming Gao. 2022. Towards unified prompt tuning\n   for few-shot text classification. In Findings of the\n  Association for Computational Linguistics: EMNLP\n  2022, pages 524–536, Abu Dhabi, United Arab Emi-\n   rates. Association for Computational Linguistics.\n\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Bench-\n  marking zero-shot text classification: Datasets, eval-\n   uation and entailment approach. In Proceedings of\n   the 2019 Conference on Empirical Methods in Natu-\n   ral Language Processing and the 9th International\n   Joint Conference on Natural Language Processing\n  (EMNLP-IJCNLP), pages 3914–3923, Hong Kong,\n  China. Association for Computational Linguistics.\n\nHaoxing Zhang, Xiaofeng Zhang, Haibo Huang, and Lei\n  Yu. 2022. Prompt-based meta-learning for few-shot\n   text classification. In Proceedings of the 2022 Con-\n   ference on Empirical Methods in Natural Language\n  Processing, pages 1342–1357, Abu Dhabi, United\n  Arab Emirates. Association for Computational Lin-\n   guistics.",
"headers": [
"arXiv:2308.04857v1  [cs.CL]  9 Aug 2023",
"Emotion-Conditioned Text Generation through",
"Automatic Prompt Optimization",
"Yarik Menchaca Resendiz",
"and",
"Roman Klinger",
"Institut für Maschinelle Sprachverarbeitung, University of Stuttgart",
"{yarik.menchaca-resendiz,roman.klinger}@ims.uni-stuttgart.de",
"Abstract",
"1",
"Introduction",
"2",
"Methods",
"3",
"Experiments",
"4",
"Conclusion and Future Work",
"5",
"Ethical Considerations & Limitations",
"Acknowledgements",
"References"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/test/2308.04857v1.pdf"
}