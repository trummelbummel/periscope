{
"text": "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    1\n\n    The Prompt Alchemist: Automated LLM-Tailored\n      Prompt Optimization for Test Case Generation\n\n                  Shuzheng Gao1, Chaozheng Wang1, Cuiyun Gao2∗, Xiaoqian Jiao2, Chun Yong Chong3,\n                                          Shan Gao4, Michael R. Lyu1\n                                     1 The Chinese University of Hong Kong, Hong Kong, China\n                                           2 Harbin Institute of Technology, Shenzhen, China\n                             3 School of Information Technology, Monash University Malaysia, Malaysia\n                                                    4 Independent Researcher, China\n             szgao23@cse.cuhk.edu.hk, adf111178@gmail.com, gaocuiyun@hit.edu.cn, 210110210@stu.hit.edu.cn,\n                      chong.chunyong@monash.edu, gaoshan cs@outlook.com, lyu@cse.cuhk.edu.hk\n2025\n\n           Abstract—Test cases are essential for validating the reliabil-   Generation, Large Language Models.\n           ity and quality of software applications. Recent studies haveJan\n         demonstrated the capability of Large Language Models (LLMs)\n2   to generate useful test cases for given source code. However, the                               I. INTRODUCTION\n          existing work primarily relies on human-written plain prompts,\n        which often leads to suboptimal results since the performance     Test cases play a crucial role in validating the reliability\n          of LLMs can be highly influenced by the prompts. Moreover,  and quality of software applications  [1],  [2]. By allowing\n          these approaches use the same prompt for all LLMs, overlooking   developers to identify and rectify bugs and defects at the early\n          the fact that different LLMs might be best suited to different   development stage, it remarkably enhances the overall stability\n         prompts. Given the wide variety of possible prompt formulations,\n                                                                         of the software [3]. However, manually writing test cases is a[cs.SE]   automatically discovering the optimal prompt for each LLM\n          presents a significant challenge. Although there are methods   challenging and time-consuming task. Consequently, the task\n        on automated prompt optimization  in the natural language   of test case generation, which aims at creating high-quality\n         processing  field, they are hard to produce  effective prompts   test cases automatically, has attracted both developers’ and\n          for the test case generation task. First, the methods iteratively   researchers’ attention in recent years [4]–[6].\n         optimize prompts by simply combining and mutating existing\n                                                                              Traditional  test case  generation methods such  as Evo-         ones without proper guidance, resulting in prompts that lack\n          diversity and tend to repeat the same errors in the generated   suite [7] and Randoop [4] mainly employ search-based and\n           test cases. Second, the prompts are generally lack of domain   constraint-based techniques to craft  test suites. Recent ad-\n          contextual knowledge, limiting LLMs’ performance in the task.   vancements in deep learning have introduced many learning-\n           In this paper, we introduce MAPS, an LLM-tAilored Prompt   based test generation approaches. For instance, AthenaTest [5]\n         generation  method  for  teSt  case  generation. MAPS  com-\n                                                                             fine-tunes BART [8] on a dataset designed for test generation.\n          prises three main modules: Diversity-guided Prompt Generation,\n          Failure-driven Rule Induction, and Domain Contextual Knowl-   A3Test [9] further incorporates assertion knowledge and a\n         edge  Extraction.  Specifically,  in  the  Diversity-Guided Prompt   test  signature  verification mechanism  for achieving  better\n         Generation module, MAPS creates varied prompts by exploring   results. These models aim  at leveraging general program-\n          diverse modification paths during the optimization process. It  ming knowledge acquired from extensive developer-written\n         prevents  the  optimization  process from  converging  to  localarXiv:2501.01329v1                                                          code corpora to generate more comprehensive and meaningful\n         optima. The Failure-driven Rule Induction module aims at iden-\n           tifying promising optimization direction by reflecting common    tests. Recently, Large Language Models (LLMs), such as\n           failures in generated test cases, in which the reflection outputs  ChatGPT [10], have gained widespread adoption in various\n         are softly integrated into prompts based on a rule transformation   Software Engineering (SE) tasks, including test case genera-\n         method. The Domain Contextual Knowledge Extraction module   tion, and show promising results. Due to their powerful zero-\n        aims at enriching the prompts with related domain knowledge by\n                                                                         shot capabilities, LLMs can be directly deployed for down-\n         incorporating both in-file and cross-file context information. To\n          evaluate the effectiveness of MAPS, we compare it with four state-   stream tasks through prompt engineering without requiring\n          of-the-art prompt optimization methods across three popular   fine-tuning [11]. For example, ChatUniTest [12] harnesses the\n       LLMs. The experimental results demonstrate that our method   capabilities of LLMs and employs a generation-validation-\n         outperforms baseline methods by a large margin, achieving a   repair mechanism to rectify errors in generated  test cases.\n        6.19% higher line coverage rate and a 5.03% higher branch\n                                                           Yuan et al. [13] evaluate ChatGPT’s performance in test case\n         coverage rate on average. Moreover, experiments on different\n      LLMs show that our method can effectively find the most suitable   generation and enhance it through an iterative test refinement\n        prompt for each LLM.                                           process.\n                                                                  However, the existing LLM-based work primarily relies on           Index Terms—Software  testing and  debugging,  Test Case\n                                                                   human-written plain prompts, which often leads to suboptimal\n            ∗Corresponding author. The author is also affiliated with Peng Cheng   results since the performance of LLMs can be highly influ-\n           Laboratory.                                                 enced by the prompts [14], [15]. Additionally, different LLMs\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    2\n\n\nTABLE  I: Comparison of test case generation prompts and                                                                      different LLMs through three key modules: diversity-guided\ntheir  line coverage  rates across  different LLMs using the                                                       prompt generation, failure-driven rule induction, and domain\nDefects4J [16] benchmark.                                                                contextual knowledge extraction. The diversity-guided prompt\n   Prompt                                ChatGPT   Llama-3.1     generation module creates varied prompts by exploring di-\n    Write unit tests for the provided Java classes to                                               21.92%     26.45%      verse modification paths during prompt optimization. This\n     test the methods and functionalities of each class.\n    Write unit tests for the given Java classes to                          approach prevents premature convergence to local optima, en-\n                                             24.46%     24.07%\n    ensure proper functionality of the methods.                              suring a more comprehensive exploration of the prompt space.\n    Write test cases for the given Java class to\n                                               22.90%     25.80%     The failure-driven rule induction module aims at identifying    ensure the correct behavior of its methods.\n                                                           promising optimization direction by reflecting common errors\n                                                                    in generated test cases and guide the optimization process\nmay be best suited to different prompts. For instance, as shown  by transforming the reflection results into rules. These rules\nin Table I, our preliminary experiments of three prompts on a   are then incorporated into the prompt to prevent recurring er-\nsubset of Defects4J [16] reveals varying performance across   rors. Furthermore, the domain contextual knowledge extraction\ndifferent LLMs. Specifically, the best prompt on ChatGPT  module provides LLMs with both in-file and cross-file context\nachieves a 24.46% line coverage rate, while the worst one   information, such as inheritance relationship information, to\nachieves only 21.92%,  indicating  that prompt choice can   help them generate accurate test cases. The optimized prompt,\ngreatly influence the performance of LLMs for test case gen-   induced  rules, and extracted context information are then\neration and plain prompts may not yield satisfactory results.   integrated together to form the  final prompt for  test case\nFurthermore, our analysis reveals that the prompt performing   generation. To evaluate the effectiveness of MAPS, we conduct\nbest with ChatGPT actually performs worst when applied   experiments on a popular benchmark Defects4J  [16]. We\nto Llama-3.1 [17]. Therefore, given the considerable time   apply MAPS to three popular LLMs including ChatGPT [10],\nrequired for manual prompt design, the automated generation   Llama-3.1 [17], and Qwen2 [23] and compare  it with four\nof tailored prompts for different LLMs is worth studying but   state-of-the-art prompt optimization approaches. The experi-\nhas not received adequate attention.                          mental results demonstrate that MAPS outperforms baseline\n  To achieve LLM-tailored prompts, one potential approach  methods by a large margin, achieving a 6.19% higher line\nis to leverage prompt optimization methods from the Natural   coverage rate and a 5.03% higher branch coverage rate on\nLanguage Processing (NLP) field [18], [19]. These methods   average. Besides, experiments on different LLMs reveal that\ntypically use LLMs and evolutionary algorithm [20], [21]  MAPS can effectively generate the most suitable prompt for\nto iteratively search the discrete natural language space for   each LLMs, surpassing manually designed prompts.\neffective prompts through a generate-and-validate approach.     Contributions. In summary, the main contributions of this\nHowever, when applied to test case generation, these methods  work are as follows:\nfall short of achieving promising results due to three main limi-    1) To the best of our knowledge, this paper presents the first\ntations: (1) Low diversity in generated prompts. These methods       study on automatically producing LLM-tailored prompt\noptimize prompts by simply combining and mutating existing        for test case generation.\nones using LLMs, while ignoring the diversity in generated    2) We propose a novel method MAPS that effectively im-\nprompts, which potentially leads to insufficient exploration       proves the prompt optimization process by integrating\nof the vast natural language search space. Consequently, the        diversity-guided prompt generation, failure-driven rule\noptimization process may converge prematurely to local op-        induction and domain contextual knowledge extraction.\ntima, hindering the discovery of the most suitable prompt. (2)    3) Extensive experiments on three popular LLMs demon-\nLack of proper guidance on avoiding common errors. Existing         strate that our method substantially outperforms baseline\nmethods generate new prompts based solely on existing ones       approaches and effectively generate tailored prompts for\nwithout considering the recurring  errors. As a  result,  test        different LLMs.\ncases produced by optimized prompts often exhibit the same                                                            Organization. The rest of this paper is organized as follows.\nissues as those generated by unoptimized prompts. Therefore,                                                              Section II describes the background ans shows our motivating\nit is important to effectively guide the optimization process                                                           examples. Section  III  details the three components in the\nwith directed improvement and prevent LLMs from making                                                          proposed MAPS, including the domain contextual knowledge\nrecurring errors. (3) Absence of domain contextual knowledge.                                                                     extraction, diversity-guided prompt generation and  failure-\nExisting LLM-based  test case generation approaches [13],                                                               driven rule induction. Section IV describes the evaluation\n[22] typically utilize only the focal method or limited in-file                                                          methods, including the research questions, datasets, baselines,\ncontext information. They lack necessary domain contextual                                                       and implementation details. Section V presents the experimen-\nknowledge such as subclass inheritance and class invocation                                                                               tal results. Section VI discusses some cases and threats to\ninformation, which is crucial for generating accurate test cases.                                                                          validity. Section VIII concludes the paper.\nGiven the complex inheritance and invocation relationships\nbetween classes and functions in real-world projects,  it  is\n                                                                                     II. BACKGROUND AND MOTIVATING EXAMPLE\ndifficult for LLMs to infer such information.\n                                                             A. Background  In this paper, we propose MAPS, the first LLM-tAilored\nPrompt generation method for teSt case generation. MAPS     In  this work, we concentrate on black-box LLM-based\neffectively and automatically generates suitable prompts for   Automatic Prompt Optimization (APO) [18], [24], given the\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    3\n\n\nTABLE II: Examples of prompts generated by OPRO [25] in the optimization process. The underlined part represents the\nsimilar pattern among the prompts.\n\n                Prompt\n                       1. Create unit tests to verify the correctness of method implementations in the provided Java classes.\n                       2. Create unit tests to validate the functionality of specific methods within the provided Java classes.\n                       3. Create unit tests to ensure that the methods in the supplied Java classes behave as expected.\n                       4. Create unit tests to confirm that the methods behave as expected and produce the correct results.\n\n\nwidespread adoption and powerful capabilities of black-box   // Focal method:\nLLMs. APO utilizes LLMs to optimize prompts by iteratively   public class TimeSeries extends Series implements\n                                                           Cloneable,Serializable{\nsearching for the most effective ones within the discrete space     public TimeSeries createCopy(int start, int end)\nof natural language. Formally, for a task, we work with a       throws CloneNotSupportedException {\nblack-box model M, a small development set Ddev, a test set       if(start < 0){throw new IllegalArgumentException\n                                                           (\"Requires start >= 0.\");}\nDtest, and a scoring function s(·). APO aims at discovering an       if(end < start){throw new\nadvanced prompt p based on Ddev from the natural language       IllegalArgumentException(\"Requires start <= end.\nspace that maximizes the performance of M on the test set       \");}\n                                                           ...\nDtest. The prompt p is expected to guide the model directly  // Test case generated by seed prompt:\ngenerate high-quality responses instead of time-consuming   public void testCreateCopy_empty() {\nmulti-iteration generation during  test time. A typical APO     TimeSeries timeSeries = new TimeSeries(\"Test\");\nframework operates as follows.  First,  it begins with a set     TimeSeries copy =  timeSeries.createCopy(0,\nof seed prompts which can be obtained either manually or       timeSeries.getItemCount()-1);\n                                                         ...\nthrough automatic techniques. Then the seed prompts are used                                                       // Test case generated by optimized prompt:\nto generate responses for Ddev via M and the responses are  public void testCreateCopy_empty() {\nevaluated using the scoring function s(·), such as the line    TimeSeries timeSeries = new TimeSeries(\"EmptyTest\"\n                                                           );\ncoverage rate in test case generation. Prompts that perform\n                                                         TimeSeries copy =  timeSeries.createCopy(0,\nwell are  retained, while those  that do not are discarded.\n                                                            timeSeries.getItemCount()-1);\nUsing the retained prompts, the APO methods query M to\n                                                         ...\ngenerate new prompts. For example, a representative method\n                                                                 Listing 1: One example showing recurring errors made byOPRO [25] generates new prompts by prompting LLMs with\n                                                                 the seed prompt and optimized prompt. The error lines arethe prompt “Generate an instruction that is different from all\n                                                                highlighted in red.the instructions and has a higher score than all the instructions\nabove”. The newly generated prompts will be integrated with\n                                                         Observation 2 [Recurring Common Errors Across It-the retained prompts for next iteration optimization. After\n                                                                erations]: Additionally, by analyzing the generated test casesseveral iterations, the best prompt on Ddev will be used as\n                                                     on Ddev in different iterations, we find that the test casesthe final optimized prompt for Dtest.\n                                                             generated by optimized prompts tend to exhibit the same\n                                                                    errors as the unoptimized ones. For example, as shown in\nB. Motivating Examples\n                                                                 Listing 1, both the test cases generated by the seed prompt and\n  We first conduct a preliminary study by applying existing   the optimized prompt lack exception handling statements and\nAPO methods to real-world test case generation on Defects4J   encounter the same runtime errors. Existing prompt optimiza-\nand find that it struggles to produce well-performing prompts.   tion methods rely solely on current prompts without proper\nBy analyzing its optimized prompts and generated test cases,   guidance, making it difficult to achieve directed improvements\nwe identify three main problems of current APO methods.     and address the errors made by current prompts. To tackle\n  Observation 1 [Low Diversity of Prompts Generated   these challenges, we propose to leverage failed  test cases\nduring Optimization Process]: First, upon inspection of the   to identify shortcomings in current prompts. Specifically, we\nprompts generated during the optimization process, we find  make LLMs reflect common errors in generated test cases\nthat they tend to exhibit similar phrases and lack diversity.  and softly incorporate the reflection outputs into prompts as\nTable  II presents some examples of prompts generated by   concise rules to help LLMs avoid making recurring errors.\nOPRO [25] which contain similar phrases such as “Create unit\n                                                       // Focal method:\ntests to” and “the provided Java classes”. The low diversity                                                       public abstract class AbstractCategoryItemRenderer\nconstrains the optimization process to a small portion of the       extends AbstractRenderer implements\ndiscrete natural language search space, limiting exploration       CategoryItemRenderer,\n                                                           Cloneable, PublicCloneable, Serializable {\nof potentially more effective alternative phrases. This makes                                                           public CategoryItemLabelGenerator\nthe search process susceptible to convergence at local optima       getItemLabelGenerator\nand yielding suboptimal performance. Therefore, to deal with           ...\n                                                       // Test case:\nthis problem, the first key idea of our method is to improve                                                       public void testFindRangeBoundsValidDataset() {\nthe diversity of generated prompts by enforcing them to use       AbstractCategoryItemRenderer renderer =\ndifferent modification methods in the optimization process.         new AbstractCategoryItemRenderer();\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    4\n\n\n   Seed  ① Domain Contextual Knowledge Extraction   ② Diversity-guided Prompt Generation\n  Prompts\n                                                                           Prompts    Cov rate           Modification methods               Search Space                                                                      Class D\n                                                                                   Class                                                                   B                                             ①                                                                                                    Write                                                                                                                         unit                                                                                                                                                                                      ...   25.69%                        +              +                                                                                                                                                    1.                                                                                                                                                               identify                                                                                                                                                exceptional                                                                                                                                                       scenarios                                                                                                                                                                                                                                                                                 ...       new                                                                      Class A                                                                                                                                                                                          prompts                                             ②                                                                                                    Write                                                                                                                         unit ...                                                                                               27.25%                                                                                   Class                                                             C                                                                                                                                                    2.                                                                                                                     expose                                                                                                                                                              potential bugs                                                                                                                                                                                                                                                                     ...\n               Focal Method    Local Context    Global Context       ③ Write test ...   21.41%           3. cover both typical and atypical ...                      selected\n                                             ④ Write unit ...   26.05%                 ...                                                      prompts\n                        Domain                                        Prompts Selection       Modification Generation    New Prompts Generation\n      Initialization          Knowledge\n                          Augmentation\n                              ③ Failure-driven Rule Induction\n         Current Prompts                           Test Cases &                                                      Reflection:\n                                              Feedback                          2×0.88                             This error is induced by\n    Write diverse unit tests for a                                                                                                                                  [xxx]. To avoid this                Temporal  Temporal  Temporal\n    Java class by  ...                                                                                                3×0.91                   problem we can [xxx]             prompt 1   prompt 2  prompt 3\n                        xN                                                                                                                       Transformation:     Specifically,               please                         follow the                                                                                                               weighted                                                                                                                                                                      [rule                                                                                                                                                                                     1]:    following rules                                 ...                                  Generation &                                             4×0.15                                                                                                               sampling                                                                                                                                                                                                                                        ...\n    public void                         Evaluation                                                                                     2   1    3\n    setTickLabelInsets  ...                                                              Failure Information Selection         Error Reflection            Rule Validation\n        Update\n\n                                            Fig. 1: Overview of MAPS ’s workflow.\n\n\n    ...                                               Algorithm 1 Algorithm of MAPS\nListing 2: One example illustrating the issue of lacking domain   Input: Iteration number I, Seed prompt P, Domain contex-\ncontext information. The error lines are highlighted in red.          tual knowledge C, LLM M\n                                                      Output: Final prompt\n  Observation 3 [Lack  of Domain Contextual Knowl-      1: R ←∅, H ←∅       ▷Initialize the set of rules R and\nedge]:  Finally, we  thoroughly  analyzed  the  focal  meth-      handled failures H in previous iteration\nods  where   all  prompts  and LLMs  failed  to  generate      2: for each i in I do\ncorrect  test  cases.  The  primary  issue  identified  is  the      3:     Evaluate FORMAT(P , R, C) on the sampled develop-\nlack  of domain  contextual  knowledge. As  illustrated  in      ment set\nListing  2,  the  given  focal method  is from  an  abstract      4:    P, NR, NH = PROMPTIMPROVEMENT(P, R, H, M,\nclass “AbstractCategoryItemRenderer”. The gener-     C)\nated test case directly initialize with an abstract class which      5:   R ←R ∪NR, H ←H ∪NH\nleads to the error: “AbstractCategoryItemRenderer      6: p ←SELECTTOP(P, 1) ▷Select the best prompt from P\nis abstract; cannot be instantiated”. Without knowledge of      7: return FORMAT(p, R, C)  ▷Formalize the final prompt\nits subclasses, LLMs cannot generate test cases that correctly\ninitialize this class and invoke the method. Therefore, another\nkey idea of MAPS is to extract the relevant domain contextual   Finally, the best-optimized instruction from diversity-guided\ninformation and provide it to the LLMs for capturing contex-   prompt generation, induced rules, and extracted context infor-\ntual knowledge.                                           mation are integrated to construct the final prompt for test case\n                                                                 generation. Fig. 2 illustrates the format of the final prompt. In\n                    III. PROPOSED APPROACH                    the following sections, we will introduce these three modules\n                                                                    in details.A. Overview\n\n  We provide an overview of MAPS’s workflow in Fig. 1.\n                                                             B. Domain Contextual Knowledge ExtractionMAPS  starts with a  set  of seed prompts and augments\nthe focal methods with both  in-file and  cross-file context    The domain contextual knowledge extraction module aims\ninformation through  the   1 domain contextual knowledge   to provide LLMs with  related  project-level context  infor-\nextraction module. In each iteration, MAPS  first evaluates   mation, enabling them to generate accurate  test cases. As\nthe performance of the current prompts on the small de-   illustrated  in Fig.  2, the contextual knowledge  is divided\nvelopment  set. The   2 diversity-guided prompt generation   into two categories: in-file contextual knowledge and cross-\nmodule then selects the top-performing prompts and infers    file contextual knowledge.\ndiverse modification methods, which are used to help generate      • In-file Contextual Knowledge contains the class sig-\ncreates varied prompts. In the  3  failure-driven rule induction        nature,  focal method, and  the  signatures  of member\nmodule, MAPS aggregates and selects representative failure       methods. The class signature includes the type and name\ninformation from failed test cases and induces concise rules        of the class containing the focal method, which could\nto avoid such failures using a reflection-validation method.       help LLMs avoid direct initialization of abstract or private\nAs shown in Algorithm 1, this iterative optimization process        classes. The focal method is the specific method to gen-\ncontinues until reaching the maximum iteration number I.        erate test cases. Following previous research [5], [9], we\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    5\n\n\n             Optimized      Write diverse unit tests for a Java class by covering a wide range of scenarios, including normal cases,\n                Instruction     edge cases, failure cases, and boundary cases to ensure high test coverage ...\n                                        Specifically, please follow the following rules when generating test cases:\n              Induced         1. Ensure that all necessary classes are imported at the beginning of the test file to prevent errors  ...\n               Rules          2. Ensure that classes have constructors that match the required parameter types and consider  ...\n                                                         ...\n                                   public abstract class Axis implements Cloneable, Serializable {               Class signature\n                                       public void setTickLabelInsets(RectangleInsets insets) {                   Focal method\n                                           if(insets==null) {\n                                               throw new IllegalArgumentException(\"Null 'insets' argument.\");\n                                           }                                                                                 Focal\n                                           if (!this.tickLabelInsets.equals(insets))                                                                                     {                                         method                                               this.tickLabelInsets = insets;\n                                               notifyListeners(new AxisChangeEvent(this));                                              with In-file\n                                           }                                                                                Context\n                                       }\n           Focal Method          RectangleInsets getTickLabelInsets();                         Member methods\n           With Domain           void...  setTickLabelInsets(RectangleInsets insets);\n            Contextual\n          Knowledge        // Avaible SubClasses:                                                         Inheritance\n                                   // public class CategoryAxis extends Axis implements Cloneable, Serializable {\n\n                                   // Class definition of input parameters:                                         Invocation                                   // public                                             class                                                   RectangleInsets implements                                                                              Serializable                                                                                           {\n                                        public                                               RectangleInsets(double                                                                      top, double                                                                                  left, double                                                                                               bottom, double right) {                       Cross-file\n                                          this(UnitType.ABSOLUTE, top, left, bottom, right);                                         Context\n                                        }\n                                        public RectangleInsets(UnitType unitType, double top, double left, double bottom, double right)\n                                        ...\n\n                    Fig. 2: An illustration of the format of final prompt and extracted context information.\n\n\n     also incorporate the function signatures of other member   constant prompt number following previous work [18]. These\n    methods within the class, as the focal method may invoke   modification methods serve as diverse exploration directions\n    them, and these signatures can guide the correct usage of   within the  discrete  natural language search space. MAPS\n     these functions.                                          then leverages LLM M to generate new prompts based on\n   • Cross-file Contextual Knowledge  refers to the con-   each modification method sequentially (Lines 5-7). Finally,\n     text information from other files within the project. We   the selected prompts and the newly generated prompts are\n    propose to extract two types of cross-file information  combined to serve as the new prompts for the next iteration\n     that are critical for test case generation but ignored in   of optimization.\n     previous work, namely class inheritance information and\n     class invocation information. For focal methods from\n                                                      D. Failure-driven Rule Induction\n     abstract or private classes, we scan the entire project to\n     locate their subclasses and extract the class signatures.    The failure-driven rule induction module aims at identifying\n    This subclass information enables LLMs  to properly   promising optimization direction by avoiding LLMs to make\n     instantiate the class within the test case. Furthermore, for   recurring errors. It leverages common failures in generated test\n     the class invocation information, we identify the types   cases to identify the parts where existing prompts most need\n     of arguments in the focal method, trace the definitions  improvement and induces rules to optimize the prompt using\n     of user-defined types, and extract their signatures and   a reflection-validation method. As shown in Fig. 1 3  , this\n     constructors. This invocation information aids LLMs in   process contains three phases: failure information selection,\n     using correct input arguments for the focal method.        error reflection, and rule validation. The details are illustrated\n                                                                    in Algorithm 2.\n                                                                  1) Failure Information Selection: To identify shortcomings\nC. Diversity-guided Prompt Generation                                                                    in current prompts, we propose to delve into the failed test\n  The diversity-guided prompt generation module aims  at   cases generated by current prompts and select their common\nproducing diverse prompts to foster a more comprehensive   errors. Specifically, MAPS first collects the failed test cases\nexploration of the prompt space by enforcing them to use   generated by the selected prompts SP  associated with the\ndifferent modification methods. As illustrated in Fig. 1 2    corresponding focal method and error messages. Then, MAPS\nand Algorithm 2, after evaluating the performance of current   aggregates those failure information F based on the typical\nprompts on  the  evaluation  set, MAPS  selects  the top-K  DBSCAN [26] clustering algorithm (Lines 8). To determine\nprompts with the highest average line coverage and branch  which  failures to address in each  iteration, we employ a\ncoverage. Using these selected samples, MAPS first leverages   weighted sampling method. The weight of each cluster  is\nthe LLM M to generate N distinct modification methods for   based on two factors: its size and the similarity of its failure\nthe current prompts based on a modification prompt template   information to handled failures H in previous iterations. A\nshown in Fig. 3 (a) (Lines 4), where N = SIZE(P) −K and   larger cluster size indicates a higher probability of the failure\nSIZE(P) indicates the number of seed prompts, to maintain a   type, so we assign a larger weight to it. As for the similarity\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    6\n\n\nAlgorithm 2 PROMPTIMPROVEMENT\n Input: Prompts P, Existing rules ER, Handled failures H, LLM M, Domain contextual knowledge C, New prompts number\n  N\n Output: Optimized prompts OP, New induced rules NR, New handled failures NH\n  1: OP ←∅, NR ←∅\n      // Diversity-guided Prompt Generation\n  2: SP ←SELECTTOP(P, SIZE(P) −N)                                         ▷Select the top K prompts from P\n  3: D ←generate N different modification methods using M\n  4: for each d in D do\n  5:    p ←generate new prompt using M based on SP and d\n  6:     OP.insert(p)\n  7: OP ←OP ∪SP\n      // Failure-driven Rule Induction\n  8: F ←CLUSTERFAILUREINFO(SP)                                                      ▷Clustering by DBSCAN\n  9: Fi ←SAMPLEREPRESENTATIVECLUSTER(H, F)                                       ▷Sample by Eq. 2\n 10: (E, S) ←REFLECTION(Fi)                                    ▷Prompt M to get explanations and solutions\n 11: R ←SUMMARIZE(E, S)                                         ▷Prompt M to transform them into rules\n 12: for each r in R do\n 13:       if FORMAT(SELECTTOP(P, 1), ER ∪r, C) > FORMAT(SELECTTOP(P, 1), ER, C) then\n 14:        NR.insert(r)\n 15: NR ←SELECTTOP(NR, 1), NH ←Fi                         ▷Select the best rule from NR if NR is not empty\n 16: return OP, NR, NH\n\n\n\nwith H, to prevent the model from getting stuck on the     3) Rule Validation: To maintain the quality of the induced\nsame difficult-to-solve issues, MAPS measures the similarity   rules, this part aims at retaining only the most effective ones by\nbetween the failures in each cluster and those in H, and assigns   validating each newly generated rule and incorporating the best\na lower weight to clusters with higher similarity. Specifically,  one into the prompt. To this end, as shown in Lines 12 to 14\nthe weight is calculated as follows:                             of Algorithm 2, MAPS first constructs temporary prompts for\n                                                          each newly generated rule. The optimized instruction part of\n                           ED(fi, h)\n         simi = 1 −maxh∈H( max(len(fi), len(h)))        (1)  andthe temporarythe inducedpromptsrules partis fromof thethetemporarybest-performingpromptsoneincludesin P,\n                            size(fi) · simi                   both the existing rules ER and each newly generated rule\n            weighti =                                    (2)\n             Pnj=1 size(fj) · simj                   r. MAPS then evaluates the performance of the temporary\n                                                        prompts on the sampled development set and incorporates the\nwhere ED(·) denotes edit distance, and size(·) denotes the                                                                    rule corresponding to the temporary prompt that achieves the\ncorresponding cluster size. fi and T denote the failure infor-                                                                highest performance into the final prompt (Lines 15).\nmation of the ith cluster’s center sample and handled failures\nH in previous iterations, respectively.\n  2) Error Reflection: With the selected failure information                 IV. EXPERIMENTAL SETUP\nFi, this part aims to enhance prompts by incorporating ef-   A. Research Questions\nfective mitigation strategies to prevent LLMs from repeating\n                                                                  In the evaluation, we focus on the following four research\nthe same errors. First, MAPS chooses a few test cases whose\n                                                                 questions:\nfailure information exhibits the lowest Euclidean Distance to\nthe cluster center of Fi to construct the reflection prompt as   RQ1: How  effective  is MAPS compared with  existing\ndepicted in Fig. 3 (b). The reflection prompt  is then used          prompt optimization methods?\nto instruct the LLM M to provide detailed explanations and   RQ2: Is MAPS able to generate tailored prompts for dif-\nsolutions for the failure information (Lines 10). Additionally,            ferent LLMs?\nto ensure that the solutions can be applied to more examples   RQ3: What is the impact of each module on the perfor-\nand not just the given ones, the reflection prompt also requires         mance of MAPS?\nthe model to remove example-specific information and make   RQ4: How does MAPS’s performance vary under different\nthe solutions applicable to other similar cases. To avoid poten-           experimental settings?\ntial performance degradation brought by lengthy prompts [27],    To study RQ1, we conduct a comprehensive evaluation\n[28], we propose a soft incorporation of reflection outputs by   of MAPS by comparing  it with four representative prompt\nconverting them into concise rules. Specifically, MAPS tasks   optimization methods on three popular LLMs. For RQ2, we\nLLM M with transforming the explanations and solutions   assess MAPS’s ability to generate LLM-tailored prompts for\ninto structured rules R based on the transformation prompt   different LLMs by evaluating the performance of optimized\ntemplate as shown in Fig. 3 (c) (Lines 11).                  prompts produced by MAPS and manually designed prompts\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    7\n\n\n   (a) Modification Prompt Template            (b) Reflection Prompt Template           (c) Transformation Prompt Template\n  System Prompt                                System Prompt                               System Prompt\n  You are a tutor and now you will help to write              You are a software engineer and now you will help to                                                                                                          You are a tutor and now you will help to write rules.\n  suggestions. Please wrap each suggestion with               analyze the program and give suggestions.                                                                                                                                     Directly give the content of the rules.\n  <START> and </END>.                              User Prompt                                                                                             User Prompt\n  User Prompt                                         Here are some examples of buggy unit tests along           Here are some examples of common mistakes\n  Here are some prompts for Java unit test generation.         with their error messages. Please identify the causes          students make when writing unit tests and their\n  Please write {N} different modification suggestions            of these errors and provide a strategy to avoid such            solutions. Based on these examples, please select\n  for those prompts to make the modified prompt can           errors in the future. Ensure that your                    one most effective rule and rewrite it into one\n  better help students understand this task and write         recommendation is broadly applicable to similar              precise sentence with the format \"Ensure that ...\"\n  diverse unit tests with a high coverage rate.                  types of errors.                                             to help these students avoid these mistakes in\n  {selected prompt 1}                                               {test case 1} {error information 1}                            future unit tests.\n   ...                                                                                                  ...                                                           {explanation and solution}\n\n          Fig. 3: The prompt templates of MAPS. The complete ones can be found in our replication package [29].\n\n\n    TABLE III: Statistics of the Defects4J benchmark.                                                          C. Baselines\n\n   Project           Abbr.   Bug number   Focal class   Focal method     To provide a comprehensive evaluation, we experiment on\n  Commons-Cli      Cli             29          13           645    three popular LLMs and compare MAPS with four represen-\n  Commons-Csv    Csv             15           5           373    tative prompt optimization methods, with details as below.\n  Gson           Gson           17          15           220      For LLMs, we select the following powerful LLMs in code-\n   Jfreechart         Chart           26          24           1,318\n  Commons-Lang   Lang            60          28           2,712    related tasks for evaluation:\n   All                            147          85           5,278       • ChatGPT [10] is a popular model known for its versatile\n                                                                           capabilities across various fields such as code generation.\n                                                                                             It is a closed-source model developed by OpenAI and\non different LLMs. For RQ3, we remove different modules of     we use the latest version gpt-3.5-turbo-0125 in our ex-\nMAPS to evaluate their individual contributions. For RQ4, we        periments.\ninvestigate MAPS’s performance under different experimental      • Llama-3.1 [17] is a family of state-of-the-art open-source\nsettings, including the number of seed prompts, the number of     LLMs that have different sizes including 7B, 70B, and\ngenerated prompts per iteration N, and the maximum iteration       405B. In this paper, we use the instruction-tuned Llama-\nnumber I.                                                          3.1-70B-Instruct for experiments.\n                                                                   • Qwen2 [23] is an open-source large language model that\nB. Datasets and Metrics                                           achieves promising in a variety of code intelligence tasks.\n  We evaluate MAPS on the widely-used Defects4J [16]          It has a 128k context length to deal with project-level long\ndataset. Following previous  studies  [5],  [9], we use  five       code. Specifically, we choose Qwen2-72B-Instruct in this\ncommonly used Java  projects from  this  dataset including        paper.\nApache Commons CLI, Apache Commons CSV, Google Gson,    As for prompt optimization methods, we compare MAPS\nJFreeChart, and Apache Commons Lang. For each project, we   with the basic prompt and four state-of-the-art prompt opti-\nuse the fixed versions used by existing work [9] for evaluation.   mization methods:\nThese projects span diverse domains, including command-      • Basic denotes the performance of the best seed prompt. It\nline interface, data processing, serialization, visualization, and          is used to measure how much improvements could prompt\nutility  libraries, respectively. Table  III presents the overall        optimization methods to achieve.\ninformation on the dataset and the detailed information such      • APE [19] is a typical prompt optimization method that di-\nas  specific versions and commit hashes can be found  in        rectly asks LLMs to generate variants for current prompts\nour replication package [29]. As for evaluation metrics, we         that can keep their semantic meanings in each iteration.\nfollow previous work [22], [30] and adopt two most popular      • OPRO [25] further incorporates the performance infor-\nmetrics to evaluate the performance of MAPS and the baseline       mation and lets the LLM generate new prompts that can\napproaches:                                                  enhance the test accuracy based on existing prompts and\n   • Line coverage (%) measures the percentage of code         their performance.\n     lines executed during testing. It checks whether each line      • EVOPROMPT [18]  is the state-of-the-art prompt opti-\n     of the source code is executed at least once,  i.e., Line       mization method that generates new prompts based on\n    Coverage(%) = NumberTotal numberof executedof lineslines × 100. Only the lines        evolutionary operators. It has two versions: EVOPROMPT\n    covered by passed test cases are used for calculation.        (GA) and EVOPROMPT (DE), which use the Genetic\n   • Branch  coverage (%)  measures  the  percentage  of       Algorithm, and Differential Evolution, respectively.\n     branches executed during testing. It checks whether each\n    branch  in control  structures  is executed,  i.e., Branch\n                        Number of executed branches                  D. Implementation\n    Coverage(%) =    Total number of branches × 100. Only the\n     branches covered by passed  test  cases  are used  for     In our experiments, the number of seed prompts, the number\n     calculation.                                               of generated prompts per  iteration N, and the maximum\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    8\n\n\nTABLE IV: Comparison with prompt optimization methods on ChatGPT. The number in “()” denotes the standard deviation.\n\n                           Projects            Chart            Cli          Csv         Gson         Lang        Average\n\n                                                                          Line Coverage\n\n                          Basic             47.41         36.76         37.49         22.42         57.58          45.56\n                   APE             43.34         39.43         39.32         23.42         54.92          44.58\n                 OPRO            44.52         42.49         38.96         25.71         53.70          45.13\n               EVOPROMPT (GA)       48.47         45.71         38.59         21.82         56.34          46.63\n               EVOPROMPT (DE)       49.60         42.88         39.87         24.17         53.39          45.89\n\n                 MAPS         51.56 (0.66)   58.88 (1.14)   50.05 (1.02)   25.17 (0.60)   61.35 (0.42)   53.80 (0.04)\n\n                                                                      Branch Coverage\n\n                          Basic             33.07         23.27         32.39         14.21         46.19          34.24\n                   APE             33.72         24.30         34.32         15.53         44.59          34.25\n                 OPRO            33.86         28.26         32.78         16.94         42.46          34.28\n               EVOPROMPT (GA)       34.36         31.64         34.18         14.53         45.07          35.88\n               EVOPROMPT (DE)       35.70         29.32         34.24         16.04         43.27          35.11\n\n                 MAPS         38.68 (0.25)   41.54 (1.58)   39.53 (1.75)   16.20 (0.40)   51.11 (0.40)   41.84 (0.26)\n\n\n\niteration number I  are  set  to  5,  2, and  5,  respectively.   simply combining and mutating existing prompts is difficult\nThe impact of different experimental settings  is discussed   to produce effective prompts for test case generation.\nin Section V-D. We repeat MAPS three times and report\nits average results and variance to eliminate the influence   MAPS achieves substantial improvement over baseline\nof sampling and fluctuations in LLM. During the prompt  methods. As can be seen in Table IV-VI, MAPS consid-\noptimization stage, we randomly sample ten bugs from the   erably outperforms the baseline methods across  all LLMs.\nDefects4J benchmark as our development set Ddev and use all   For example, compared with the strongest baseline method,\nbugs as test set Dtest. We present the sampled development  EVOPROMPT (GA), MAPS achieves an average improvement\nset Ddev  in our replication package [29]. To save manual   of 6.19% and 5.03% in line coverage and branch coverage,\nefforts, we obtain the seed prompts automatically by ChatGPT   respectively. These results demonstrate the effectiveness of\nand the existing Automatic Prompt Engineer method [19].  MAPS in finding effective prompts within the vast search\nTo ensure a fair comparison, we use the same development   space.\nset and seed prompts for our tool and all baseline methods.\nThe seed prompts, and all prompt templates used in our work                                                 The performance of different LLMs on different projects\ncan be found in our replication package [29]. We conduct all                                                                  varies. By comparing the performance on different projects\nexperiments on an Ubuntu 20.04 server with a 112-core Intel                                                               across different LLMs, we further observe that different LLMs\nXeon Platinum CPU.                                                             tend  to perform well on  different  projects. For  instance,\n                                                               as shown in Table IV-VI, although the overall performance\n              V. EXPERIMENTAL RESULTS                  of ChatGPT and Llama-3.1 with basic prompts are similar,\n                                                                        their performance on individual projects exhibits large differ-\nA. RQ1: Performance Evaluation                                                                ences. Specifically, on the Lang project, ChatGPT outperforms\n  To evaluate the effectiveness of MAPS in test case gen-   Llama-3.1 and Qwen2 by 4.69% and 15.47% in terms of\neration, we compare  it with four representative prompt op-   line coverage, respectively; while on the Csv  project, the\ntimization methods across three popular LLMs. Tables IV-   performance of ChatGPT is much worse than Llama-3.1 and\nVI present the performance of MAPS along with baseline  Qwen2, with a decrease of 5.70% and 3.24% in terms of\nmethods on Defects4J. For each method, we provide the   line coverage, respectively. This indicates that different LLMs\naverage performance across  all bugs, as well as  detailed   tend to excel in different domains and also demonstrates the\naverage results for each project. Based on these results, we   importance of building tailored prompts for different LLMs.\nderive the following findings.\n  Existing prompt optimization methods struggle to pro-   MAPS  could  achieve  higher  improvements on  the\nduce effective prompts for test case generation. By com-   projects that the seed prompts do not perform well. At\nparing the performance of the basic prompt and four baseline    last, as shown in Table IV-VI, we find that the improvements\nmethods, we can observe that existing methods struggle to pro-   achieved by MAPS on different projects also vary across\nduce effective prompts for test case generation. Specifically, as   different LLMs. For instance, in the Lang project, the relative\nshown in Table IV, the best-performing baseline, EVOPROMPT   improvement on ChatGPT and Qwen2 are 6.55% and 14.98%,\n(GA), can only achieve 1.07% and 1.64% improvements over   respectively; whereas in the Csv project, the improvement on\nthe basic prompt in line coverage and branch coverage, respec-  ChatGPT and Qwen2 are 33.50% and 22.27%, respectively.\ntively. Moreover, methods like APE and OPRO even perform   These results demonstrate that MAPS can achieve a higher\nworse than the basic prompt in terms of line coverage, with   increase on projects where LLMs do not excel, and  it can\ndecreases of 0.98% and 0.43%, respectively. This suggests that   provide directed improvements tailored to different LLMs.\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                    9\n\n\nTABLE V: Comparison with prompt optimization methods on Llama-3.1. The number in “()” denotes the standard deviation.\n\n                           Projects            Chart            Cli          Csv         Gson         Lang         Average\n\n                                                                          Line Coverage\n\n                          Basic             46.52         45.99         43.19         22.81         52.89          45.93\n                  APE             45.05         46.26         42.17         21.56         50.98          44.70\n                 OPRO            45.45         44.39         42.81         23.39         54.09          45.95\n              EVOPROMPT (GA)       45.95         45.82         42.70         24.70         54.24          46.52\n              EVOPROMPT (DE)       45.70         44.96         43.24         22.77         52.29          45.34\n\n                MAPS         49.68 (0.21)   51.83 (1.54)   44.05 (0.19)   26.38 (0.96)   58.56 (1.49)    50.59 (0.56)\n\n                                                                      Branch Coverage\n\n                          Basic             35.46         28.55         36.92         16.83         42.73          35.06\n                  APE             34.54         28.49         35.52         16.37         41.58          34.22\n                 OPRO            34.85         26.61         37.25         16.86         43.20          34.80\n              EVOPROMPT (GA)       34.71         29.28         35.51         16.74         43.01          35.03\n              EVOPROMPT (DE)       34.82         29.28         36.95         17.00         42.62          35.07\n\n                MAPS         37.73 (0.68)   35.06 (0.27)   39.02 (1.14)   19.36 (0.65)   48.24 (1.69)    39.50 (0.68)\n\n TABLE VI: Comparison with prompt optimization methods on Qwen2. The number in “()” denotes the standard deviation.\n\n                           Projects            Chart            Cli          Csv         Gson         Lang        Average\n\n                                                                          Line Coverage\n\n                          Basic             39.75         36.80          40.73          26.51         42.11          38.70\n                  APE             39.52         34.76          45.73          23.83         44.44          39.41\n                 OPRO            40.84         38.51          38.94          26.41         33.41          35.49\n              EVOPROMPT (GA)       39.08         36.49          37.50          21.23         43.56          38.17\n              EVOPROMPT (DE)       39.08         36.49          37.50          21.23         43.56          38.17\n\n                MAPS         44.37 (3.56)   47.56 (0.40)    49.80 (3.48)   29.75 (1.29)   48.42 (2.16)   45.51 (1.28)\n\n                                                                      Branch Coverage\n\n                          Basic             30.88         23.79          32.55          18.18         30.55          28.05\n                  APE             31.07         21.35          36.22          16.79         33.27          28.92\n                 OPRO            32.47         26.76          29.58          19.92         25.26          26.66\n              EVOPROMPT (GA)       30.81         22.16          29.71          13.84         33.14          27.98\n              EVOPROMPT (DE)       30.81         22.16          29.71          13.84         33.14          27.98\n\n                MAPS         30.58 (3.26)   31.73 (1.44)    36.30 (2.83)    18.98 (2.01)   37.11 (1.51)   32.71 (1.43)\n\n             TABLE VII: Evaluation of MAPS in generating tailored prompts for different LLMs.\n\n                              Approach         ChatGPT   Llama-3.1   Qwen2   ChatGPT   Llama-3.1   Qwen2\n\n                                                             Line Coverage                  Branch Coverage\n\n                        ChatGPT’s final prompt      53.80       41.92      35.98      41.84       32.31      26.87\n                           Llama-3.1’s final prompt      51.35       50.59      44.94      40.05       39.50      34.43\n                        Qwen2’s final prompt       51.14       43.98      45.51      38.39       32.97      32.71\n                         Manually-designed prompt     48.55       48.46      42.85      37.55       37.60      31.88\n\n\n Answer to RQ1: MAPS effectively enhances prompts for   package [29]. Based on the results, we have the following\n test case generation. It consistently outperforms all baseline    observations:\n methods across various LLMs, achieving a 6.19% higher                                                 The performance of different prompts varies a lot. By\n line coverage rate and a 5.03% higher branch coverage rate                                                        comparing the performance of each final prompt on different\n compared to the strongest baseline.                                                           models. We can find that the performance of different prompts\n                                                     on the same LLM varies a lot. Specifically, the line coverage\nB. RQ2: LLM-Tailored Prompt Generation Evaluation           rate and branch coverage rate of Llama-3.1 on different final\n                                                        prompts range from 41.92%-50.59% and 32.31%-39.50%,  In this RQ, we study whether MAPS could generate tailored\n                                                                     respectively, which further demonstrates the importance ofprompts for different LLMs. To achieve  this, we evaluate\n                                                         automated generating tailored prompts for different LLMs.the performance of the three final prompts obtained by three\nmodels on each model. Additionally, we also compare the   MAPS effectively produces tailored prompts for  dif-\nprompt used in [13] to validate whether the prompt  built   ferent LLMs. As in Table VII, we can observe that each\nby MAPS could outperform the manually-designed prompt.  model tends to achieve the best performance on their own\nThe experimental  results  are depicted  in Table  VII. The   final prompt. For example, the performance of ChatGPT’s final\ndetailed results on each project can be found in our replication  prompt outperforms the final prompt obtained by Llama-3.1\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                   10\n\n\n                            TABLE VIII: Ablation Study of MAPS.\n\n                                             Approach                          Line Coverage   Branch Coverage\n\n                                      MAPS                          53.80             41.84\n                                   w/o Domain contextual knowledge extraction        44.16             33.31\n                      ChatGPT      w/o Diversity-guided prompt generation          45.59             34.04\n                                         w/o Failure-driven rule induction              46.86             37.08\n                                   Only domain contextual knowledge extraction        48.03             35.76\n\n                                      MAPS                          50.59             39.50\n                                   w/o Domain contextual knowledge extraction        45.37             34.73\n                          Llama-3.1      w/o Diversity-guided prompt generation          49.68             38.35\n                                         w/o Failure-driven rule induction              49.73             38.88\n                                   Only domain contextual knowledge extraction        49.48             37.95\n\n                                      MAPS                          45.51             32.71\n                                   w/o Domain contextual knowledge extraction        42.12             29.73\n                      Qwen2       w/o Diversity-guided prompt generation          43.81             32.10\n                                         w/o Failure-driven rule induction              43.92             31.58\n                                   Only domain contextual knowledge extraction        40.56             30.21\n\n\n\nand Qwen2 by 2.45% and 2.66% in terms of line coverage on  which demonstrates the importance of prompt diversity in the\nChatGPT. This indicates that MAPS could effectively produce   search space exploration process.\ntailored prompts for each LLM.                                 Failure-driven rule induction. We conduct this experiment\n  Prompts optimized by MAPS outperform manually-  by removing the induced rules in the  final prompt. From\ndesigned prompts.  Additionally,  the prompt obtained by   Table VIII, we can observe that without failure-driven rule\nMAPS  also  outperforms  the  line  coverage  of  manually-   induction, the performance of MAPS drops a lot across all\ndesigned prompt by 5.25%, 2.13%, and 2.66% on ChatGPT,  LLMs. Specifically, in ChatGPT, the line coverage decreases\nLlama-3.1, and Qwen2, respectively. These results demon-  by 6.94% and branch coverage by 4.76%, respectively. This\nstrate MAPS’s efficacy  in automatically  crafting  effective,   indicates the benefits of using LLM-induced rules to guide the\nLLM-tailored prompts.                                        optimization process and avoid LLMs making recurring errors.\n                                        We further show some cases in Section VI-A for illustration.\n Answer to RQ2: The performance of different prompts     Only domain contextual knowledge extraction. As the\n varies a lot and MAPS could effectively produce tailored   domain contextual knowledge extraction module provides the\n prompts for different LLMs.                              most significant performance gains, we further evaluate how\n                                                much could this module only bring to the basic prompt to\n                                                             ensure fairness in comparison. We conduct this experiment by\n                                                       removing both the diversity-guided prompt generation module\nC. RQ3: Ablation Study\n                                                       and the failure-driven rule induction module. As shown in\n  We conduct ablation studies to validate the effectiveness of   Table VIII, removing both these two parts lead to substantial\neach module in our method, i.e. domain contextual knowledge   performance to MAPS. Specifically, solely involving the do-\nextraction, diversity-guided prompt generation, and failure-  main contextual knowledge extraction can only bring limited\ndriven rule induction. The average results for each method are  improvement over the basic prompt, i.e., improving the line\npresented in Table VIII, with detailed results for each project   coverage and branch coverage  for Qwen2 by 1.86% and\navailable in our replication package [29].                    2.16%, respectively. Its performance still falls behind MAPS\n  Domain contextual knowledge extraction. We conduct  by a large margin, which indicates that simply combining\nthis experiment by removing the cross-file context information   basic prompt and the context information without  further\nin the final prompt. As can be seen in Table VIII, excluding   optimization can not achieve satisfactory performance.\nthe cross-file context information dramatically degrades per-\n                                                 Answer to RQ3: All modules in MAPS contribute to theformance across all LLMs. Specifically, the branch coverage\n                                                             performance. Removing the domain contextual knowledgerate drops by 8.53%, 4.77%, and 2.98% on ChatGPT, Llama-\n                                                                   extraction part leads to the largest performance decreases.3.1, and Qwen2, respectively. These results demonstrate the\neffectiveness of integrating project context information to help\nLLMs generate accurate test cases.\n                                                      D. RQ4: Parameter Analysis  Diversity-guided prompt generation. To validate the effec-\ntiveness of diversity-guided prompt generation, we experiment      In this section, we study how different experimental set-\nby replacing the optimized instruction part of the final prompt   tings affect the performance of MAPS and baseline methods,\nwith the one produced by the best baseline method. As shown   including the number of seed prompts, the number of gener-\nin Table VIII, removing the diversity-guided prompt genera-   ated prompts N, and the maximum iteration number I. As\ntion leads to a consistent drop in all tasks and metrics. For   these parameters primarily influence the prompt optimization\nexample, the line coverage rate decreases by 8.21%, 1.01%,   process, we report their performance on the development set in\nand 1.70% on ChatGPT, Llama-3.1, and Qwen2, respectively,   this section. In each study, we vary only the parameter under\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                   11\n\n\n                     42                                               42\n\n\n\n\n                     32                                                                      32                                            Coverage 37                                               37                                                                                                                                                                            Coverage\n                      Line 27                                                         Line 27\n                                                  EVOPROMPT(GA)   MAPS                                        EVOPROMPT(GA)   MAPS\n                     22                                               22\n                            3        4        5        6                        1          2          3\n\n                               (a) Number of seed prompts.                         (b) Number of generated prompts.\n\n                  Fig. 4: Parameter analysis of number of seed prompts and generated prompts on ChatGPT.\n\n    42                                          40                                         40\n             APE                                                 APE                                                 APE\n           OPRO                                           OPRO                                           OPRO\n    37      EVOPROMPT(GA)                          36      EVOPROMPT(GA)                          36      EVOPROMPT(GA)\n             EVOPROMPT(DE)                                        EVOPROMPT(DE)                                        EVOPROMPT(DE)\n            MAPS                                             MAPS                                             MAPS\n                                                                                           32                                               32                                                                                                  CoverageCoverage 32                                                                                                  Coverage\nLine 27                                                 Line 28                                                 Line 28\n\n    22                                          24                                         24\n           0        1        2        3                 0        1        2        3                 0        1        2        3\n                     Iteration                                     Iteration                                     Iteration\n                  (a) ChatGPT.                                    (b) Llama-3.1.                                      (c) Qwen2.\n\n                                        Fig. 5: Parameter analysis of the Iteration number.\n\n\n analysis and keep others constant. For the analysis on number   the performance of the basic prompt without optimization\n of seed prompts and generated prompts, we only present the  by MAPS. As shown in Fig. 5, MAPS achieves the best\n results on ChatGPT; the complete results are available in our   performance in most cases. Specifically, MAPS outperforms\n replicate package [29].                                          baseline methods by at least 7.94% in line coverage when the\n  Number  of seed prompt. We conduct experiments  to  maximum iteration number is set to three. Additionally, due\n evaluate how MAPS and baseline methods perform under dif-   to low prompt diversity, baseline methods tend to converge\n ferent numbers of seed prompts. Specifically, we use the best-   to  local optima  in  the  first  iteration and  fail  to achieve\n performing baseline EVOPROMPT (GA) and set the number of   further improvement. In contrast, MAPS continually enhances\n seed prompts to 3, 4, 5, and 6 respectively. From Fig. 4 (a),   performance with each iteration.\nwe can observe that MAPS consistently achieves better perfor-\n                                                 Answer  to RQ4: MAPS consistently achieves the bestmance across different numbers of seed prompts. Additionally,\n                                                           performance across different parameter settings. Our hyper-by comparing the performance under different numbers of seed\n                                                             parameter settings, with the number of seed prompts set to prompts, we can find that MAPS and EVOPROMPT (GA) tend\n                                                                        5, N to 2, and I to 5, achieve effective results. to achieve better performance with a larger number of seed\n prompts, and the improvements over five seed prompts are\n                                                                            VI. DISCUSSION\n marginal. Therefore, we set the number of seed prompts to\n                                                              A. Case study five in this paper.\n  Number of generated prompt. We also study the effect    To better understand how MAPS improves test case gen-\n of a number of generated prompts by varying it from 1 to 3.   eration, we present two examples of the final prompts cre-\nAs shown in Fig. 4 (b), MAPS consistently achieves better   ated by MAPS and  the  resulting  test  cases from  these\n performance across different numbers of generated prompts.   final  prompts.  First,  Fig 6  (a) shows  the  final prompt\nWhile a larger number of generated prompts can lead to better   for Llama-3.1, along with  the generated  test case based\n performance,  it also increases costs. Therefore, we set the  on  the  focal method  in  Listing  1. We can  find  that by\nnumber of generated prompts to two in this experiment.        following  the  second  induced  rule,  Llama-3.1  correctly\n  Maximum iteration number. In this study, we vary the   generates a  test case  that uses the “try {...} catch\nnumber of maximum iteration number from 1 to 3 and inves-  (IllegalArgumentException e)” in the test method.\n tigate the performance of prompts in each iteration. We present   Second, Fig 6 (b)  illustrates another example using Chat-\n the results of the best prompt generated by MAPS and each  GPT, where the focal method is taken from Listing 2. Com-\n baseline method on the development set. Iteration 0 represents   pared to the incorrect  test case generated by the baseline\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                   12\n\n\n         Final prompt of Llama-3.1                              Final prompt of ChatGPT\n       Write a comprehensive set of unit tests for the given           Write diverse unit tests for a Java class by covering a wide range of\n      Java class, covering both happy paths and edge               scenarios, including normal cases, edge cases, failure cases, and\n       cases, to ensure that all methods behave correctly           boundary cases to ensure high test coverage and validation of the\n      under various input scenarios, aiming for a minimum          methods' correctness.\n      code coverage of 90%.                                             Specifically, please follow the following rules when writing test cases:\n        Specifically, please follow the following rules when              1. Ensure that all necessary classes are imported at the beginning of\n        writing test cases:                                            the test file to prevent errors  ...\n        1. Ensure that all necessary classes, methods, and             2. Ensure that classes have constructors that match the required\n        fields are properly imported and correctly spelled and         parameter types and consider using factory methods or the builder\n       capitalized ...                                                   pattern for flexibility in object creation ...\n        2. Ensure that when calling a method that throws a                    // Focal method:\n      checked exception ... either wrap the call in a try-              public abstract class AbstractCategoryItemRenderer {\n       catch block to handle the exception or declare the                 public Range findRangeBounds(CategoryDataset dataset) {\n       exception to be thrown by the test method using the                           ...\n       `throws` keyword.                                                                         // Avaible SubClasses:\n           // Focal method ...                                                                         // public class AreaRenderer extends AbstractCategoryItemRenderer\n                                                              implements Cloneable, PublicCloneable, Serializable\n                                                                                                                  // ...\n       Test case generated by final prompt\n      public void                                            Test case generated by final prompt\n      testCreateCopyEmptyItemTimeSeries()\n      throws CloneNotSupportedException{                 public void testAreaRendererFindRangeBoundsEmptyDataset()\n        TimeSeries series = new TimeSeries();            {\n        int start = 0;                                     AreaRenderer areaRenderer = new AreaRenderer();\n        int end = timeSeries.getItemCount()-1;             CategoryDataset dataset = new DefaultCategoryDataset();\n                                                            Range range = areaRenderer.findRangeBounds(dataset);        try {series.createCopy(start, end);}\n                                                            assertNull(range);        catch (IllegalArgumentException e) {\n                                                         }        ...\n\n                    (a) Case study on Llama-3.1.                                         (b) Case study on ChatGPT.\n\nFig. 6: Two cases showing the difference of optimized prompts from different models and how the optimized prompt help\ngenerate correct test case.\n\n\n                                          TABLE IX: Comparison of Randoop, A3Test, ChatTESTER,prompt in Listing 2, the cross-file contextual knowledge in\n                                                       and MAPS.the optimized prompt allows  the model  to  correctly  ini-\ntialize the “AbstractCategoryItemRenderer”  class.                                                                                                 Projects        Line Coverage   Branch Coverage\nAdditionally, by comparing two final prompts obtained by\n                                                                           Randoop           49.51             34.45\nMAPS, we observe that the induced rules for Llama-3.1 and                A3Test            34.11             15.72\nChatGPT are different. The first rules of these two methods\n                                                                          ChatGPT+Basic        45.56             34.24\nare similar, but Llama-3.1’s second rule focuses on exception          ChatGPT+MAPS        53.80             41.84\nhandling, whereas ChatGPT’s concerns method parameters.\nThis indicates that these models tend to make different types\nof errors, and MAPS can effectively introduce tailored rules   unit into noteworthy states. A3Test [9] is a state-of-the-art non-\nfor different LLMs.                                   LLM-based deep learning model that fine-tunes PLBART [31]\n  Moreover, by  calculating  the  average  edit  distance  of   for test case generation. For Randoop, we reproduce it based\nprompts obtained in each optimization iteration by MAPS,  on the “gen_tests.pl” script provided in Defects4J. For\nwe find that  it generates more diverse prompts during the   A3Test, we reproduce the results based on A3Test’s replicate\noptimization process. Specifically, the average edit distance   package [32].\nof prompts from MAPS is 27.0, remarkably larger than that     Table IX presents the experimental results in terms of line\nof OPRO, which averages only 9.3 edits, as shown in Table II.   coverage and branch coverage. Compared to three baseline\nThis further demonstrates MAPS’s effectiveness in generating   methods, ChatGPT+MAPS achieves the highest line coverage\ndiverse prompts during the optimization process.                     (i.e., 53.80%) and branch coverage (i.e., 41.84%), outperform-\n                                                              ing traditional methods by  at least 4.29% and 7.39%, re-\nB. Comparison with Other Methods                                                                     spectively. This demonstrates MAPS’s effectiveness in helping\n  To comprehensively study the advantages and limitations  LLMs generate test cases with high coverage. Besides, when\nof LLMs-based test case generation methods compared with   comparing the performance of Randoop, ChatGPT+basic, and\ntraditional approaches and previous deep learning-based ap-  ChatGPT+MAPS, we can find that the performance of Chat-\nproaches, we compare ChatGPT+MAPS with two baseline  GPT+basic is lower than traditional method Randoop, meaning\nmethods including Randoop [4] and A3Test [9]. Randoop [4]   that simply prompting LLMs can not achieve  satisfactory\nis a widely used automated software testing tool that employs   results, while MAPS can produce suitable prompts for LLMs\nrandom fuzzing on unit APIs to construct prefixes that lead the  and make LLMs outperform traditional methods.\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                   13\n\n\n  Although these are also other LLM-based methods such as  models to convert target methods into their corresponding test\nChatUniTest [13], we do not compare with them because our   cases or assertions. A series of recent studies [5], [36] have\nresearch is orthogonal to them. MAPS focuses on optimizing  employed deep learning techniques for test case generation\ntailored prompts for test case generation.  It can be further  by formulating the test case generation as a neural machine\ncombined with existing methods such as incorporating static   translation task and train models to convert target methods\ninformation [22] or multi-turn refinement method [13] and   into their corresponding test cases or assertions. For example,\nachieve better performance.                                 AthenaTest [5] fine-tunes BART [8] on a dataset designed\n                                                                    for test generation. A3Test [9] further incorporates assertion\n                                                      knowledge and a test signature verification mechanism forC. Threats to Validity\n                                                              achieving better results. Recently, leveraging advancements in\n  We identify two main threats to the validity of our study:                                                LLMs, test case generation approaches based on LLMs have\n  Limited LLMs. Given the rapid development of  large                                                                  also been proposed and shown promising results. For example,\nlanguage models, some models are not covered in this paper.                                          CodaMOSA [37] leverages LLMs to provide example  test\nTo mitigate this issue, we select the three most representative                                                               cases for under-covered functions when search-based testing\nand popular LLMs  that contain both open-source models                                                                         hits a coverage stall. ChatTESTER [13] incorporates ChatGPT\nand closed-source models.  Additionally, MAPS  is model-                                                           along with an iterative test refiner to generate tests. Different\nagnostic and does not require access to the model’s parameters.                                                      from those works, our method serves as  the  first LLM-\nTherefore, we believe MAPS can also achieve improvements                                                                     tailored prompt generation method for test case generation and\non other LLMs.                                                         can be further combined with existing methods to enhance\n  Limited Programming Languages. In this paper, we con-                                                                        their performance. Besides, our method aims at to directly\nduct experiments using the Defects4J benchmark, which only                                                            avoid generating low-quality  test cases with an optimized\ncontains Java projects. This benchmark is popular and widely                                                       prompt instead of time-consuming multi-iteration generation\nused in previous work. Furthermore, our method is language-                                                       and fixing during test time.\nagnostic and can be easily adapted to other programming\nlanguages. In the future, we plan to conduct experiments on\n                                                          C. LLMs for Software Engineeringmore datasets including those with languages such as Python.\n                                                           Large Language Models have recently been widely adopted\n                  VII. RELATED WORK                        for various software engineering tasks due to their impressive\n                                                         performance in both code generation and understanding [38]–\nA. Automatic Prompt Optimization\n                                                                     [41]. For example, Yuan et al. [13] evaluate the performance\n  Automatically discovering optimal prompts has emerged as   of ChatGPT for test case generation and improve it by iterative\nan important challenge in the era of LLMs [19], [33]. Most ex-   test refiner. Gao et al. [14] investigate how to set the in-context\nisting methods follow an iterative prompt optimization process.   demonstration for ChatGPT for code summarization and code\nThey start with a set of seed prompts and iteratively synthesize   generation tasks. CHATRepair [42] iteratively evaluates pro-\nnew prompt candidates, evaluating their performance to select  grams on test cases and feeds the error messages to LLMs\nthe top ones for the next iteration. For example, APE [19] is a   for further patch generation. Self-edit [43] utilizes compiler\ntypical prompt optimization method that directly asks LLMs   error messages to enhance the correctness of code generation.\nto generate variants of current prompts while maintaining   Li et al. [44] investigates the feasibility of slicing commer-\ntheir semantic meanings in each iteration. OPRO [25] further   cial black-box LLMs using medium-sized backbone models.\nincorporates the performance information and lets the LLM  SBLLM [45] combines search-based methods and LLMs to\ngenerate new prompts that can enhance the test accuracy based   iteratively improve code efficiency. DeepSeek-Coder [46] is an\non existing prompts and their performance. EVOPROMPT [18]   open-source Mixture-of-Experts (MoE) code language model\nis the state-of-the-art prompt optimization method that gen-   that achieves state-of-the-art performance across various code\nerates new prompts based on evolutionary operators.  It has   intelligence  tasks. StarCoder 2 [47]  is an advanced LLM\ntwo versions: EVOPROMPT (GA) and EVOPROMPT (DE),   trained in 600+ programming languages. It is trained on the\nwhich use the Genetic Algorithm, and Differential Evolution,   Stack2 [47] dataset and natural language text from Wikipedia,\nrespectively. Different from those works, this paper focuses on   Arxiv, and GitHub issues. Magicoder [48] is a recent model\nLLM-tailored prompt optimization for test case generation and   trained on synthetic instruction data enhanced with open-\ninvestigates improving the exploration of the search process.    source code  snippets.  It proposes OSS-INSTRUCT which\n                                                           produces diverse and realistic instruction tuning data from\nB. Test Case Generation                                     open-source code snippets to address the biases typically found\n                                                                    in synthetic data generated by LLMs.  Traditional  methods  like Randoop  [4]  utilize random\nfuzzing on  unit APIs  to  construct  prefixes  that  lead  the\n                                                                                VIII. CONCLUSIONunit into noteworthy states. Evosuite [7]  is a search-based\ntest generation strategy that employs evolutionary algorithms     In this paper, we introduced a novel automatic LLM-tailored\nto autonomously craft test suites for Java classes aimed at  prompt generation method MAPS for test case generation.\nimproving coverage rate. A series of recent studies [5], [34],   During the optimization process, MAPS generates diverse\n[35] have employed deep learning techniques by  training   candidate prompts to facilitate the exploration of the prompt\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                   14\n\n\nsearch space and induces rules from failure cases to avoid    [17] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nrecurring errors. Additionally, MAPS integrates various do-        A. Mathur, A. Schelten, A. Yang, A. Fan, A. Goyal, A. Hartshorn,\n                                                                         A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao,\nmain contextual knowledge for generating correct test cases in        A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Rozi`ere, B. Biron,\npractical projects. Extensive experiments on Defects4J show        B. Tang, B. Chern, C. Caucheteux, C. Nayak, C.  Bi, C. Marra,\nthat MAPS outperforms existing prompt optimization meth-        C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer,\n                                                                             C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Esiobu,\nods. The replicate package of this work is publicly available        D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes,\nat https://zenodo.org/records/14287744.                                  E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith,\n                                                                                                 F. Radenovic, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Nail,\n                                                                         G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu,\n                 REFERENCES                                 H. Touvron, I. Zarov, I. A. Ibarra, I. M. Kloumann, I. Misra, I. Evtimov,\n                                                                                                              J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah,\n [1] D. M. Rafi, K. R. K. Moses, K. Petersen, and M. M¨antyl¨a, “Benefits and            J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang,\n      limitations of automated software testing: Systematic literature review            J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun,\n     and practitioner survey,” in 7th International Workshop on Automation            J. Saxe, J. Jia, K. V. Alwala, K. Upasani, K. Plawiak, K. Li, K. Heafield,\n      of Software Test, AST 2012, Zurich, Switzerland, June 2-3, 2012. IEEE        and K. Stone, “The llama 3 herd of models,” CoRR, vol. abs/2407.21783,\n     Computer Society, 2012, pp. 36–42.                                        2024.\n [2] M. M. Almasi, H. Hemmati, G. Fraser, A. Arcuri, and J. Benefelds,    [18] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, and\n    “An industrial evaluation of unit test generation: Finding real faults in a         Y. Yang, “Connecting large language models with evolutionary algo-\n      financial application,” in 39th IEEE/ACM International Conference on         rithms yields powerful prompt optimizers,” in The Twelfth International\n     Software Engineering: Software Engineering in Practice Track, ICSE-        Conference on Learning Representations, ICLR 2024, Vienna, Austria,\n    SEIP 2017, Buenos Aires, Argentina, May 20-28, 2017. IEEE Computer      May 7-11, 2024.  OpenReview.net, 2024.\n      Society, 2017, pp. 263–272.                                              [19] Y. Zhou, A.  I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\n [3]  P. Runeson, “A survey of unit testing practices,” IEEE Softw., vol. 23,            J. Ba, “Large language models are human-level prompt engineers,” in\n     no. 4, pp. 22–29, 2006.                                               The Eleventh International Conference on Learning Representations,\n [4] C. Pacheco and M. D. Ernst, “Randoop: feedback-directed random       ICLR 2023, Kigali, Rwanda, May 1-5, 2023.  OpenReview.net, 2023.\n      testing for java,” in Companion to the 22nd Annual ACM SIGPLAN                                                                               [20]  J. H. Holland, Adaptation in natural and artificial systems: an intro-\n     Conference on Object-Oriented Programming, Systems, Languages, and                                                                                 ductory analysis with applications to biology, control, and  artificial\n      Applications, OOPSLA 2007, October 21-25, 2007, Montreal, Quebec,                                                                                              intelligence.  MIT press, 1992.\n     Canada.  ACM, 2007, pp. 815–816.                                                                               [21] R. Storn and K. Price, “Differential evolution–a simple and efficient\n [5] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and N. Sundaresan,                                                                                            heuristic for global optimization over continuous spaces,” Journal of\n     “Unit test case generation with transformers and focal context,” arXiv                                                                                   global optimization, vol. 11, pp. 341–359, 1997.\n      preprint arXiv:2009.05617, 2020.\n                                                                               [22] G. Ryan, S. Jain, M. Shang, S. Wang, X. Ma, M. K. Ramanathan,\n [6]  J. Wang, Y. Huang, C. Chen, Z. Liu, S. Wang, and Q. Wang, “Software\n                                                                         and B. Ray, “Code-aware prompting: A study of coverage-guided test\n      testing with large language models: Survey, landscape, and vision,” IEEE\n                                                                                    generation in regression setting using LLM,” Proc. ACM Softw. Eng.,\n     Trans. Software Eng., vol. 50, no. 4, pp. 911–936, 2024.\n                                                                                              vol. 1, no. FSE, pp. 951–971, 2024.\n [7] G. Fraser and A. Arcuri, “Evosuite: automatic  test suite generation\n                                                                               [23] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu,\n      for object-oriented software,” in SIGSOFT/FSE’11 19th ACM SIGSOFT\n                                                                                                 F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu,     Symposium on the Foundations of Software Engineering (FSE-19) and\n                                                                                                              J. Zhang, J. Ma, J. Yang, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang,     ESEC’11: 13th European Software Engineering Conference (ESEC-13),\n                                                                         K. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang,\n     Szeged, Hungary, September 5-9, 2011.  ACM, 2011, pp. 416–419.\n                                                                             R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai, S. Tan, T. Zhu,\n [8] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\n                                                                                          T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei,\n     V.  Stoyanov, and  L.  Zettlemoyer, “BART:  denoising  sequence-to-\n                                                                         X. Ren, X. Liu, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu,\n     sequence pre-training for natural language generation, translation, and\n                                                                                  Z. Cui, Z. Zhang, Z. Guo, and Z. Fan, “Qwen2 technical report,” CoRR,\n     comprehension,” in Proceedings of the 58th Annual Meeting of the\n                                                                                              vol. abs/2407.10671, 2024.\n     Association for Computational Linguistics, ACL 2020, Online, July 5-10,\n                                                                               [24] R. Ma, X. Wang, X. Zhou,  J. Li, N. Du, T. Gui, Q. Zhang, and     2020.  Association for Computational Linguistics, 2020, pp. 7871–7880.\n                                                                         X. Huang, “Are large language models good prompt optimizers?” CoRR, [9]  S. Alagarsamy, C. Tantithamthavorn, and A. Aleti, “A3test: Assertion-\n                                                                                              vol. abs/2402.02101, 2024.     augmented automated test case generation,” CoRR, vol. abs/2302.10352,\n                                                                               [25] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen,     2023.\n                                                                             “Large language models as optimizers,” in The Twelfth International[10] ChatGPT, “Chatgpt,” https://chat.openai.com/, 2024.\n                                                                             Conference on Learning Representations, ICLR 2024, Vienna, Austria,[11]  T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large\n                                                            May 7-11, 2024.  OpenReview.net, 2024.     language models  are  zero-shot  reasoners,”  in Advances  in Neural\n     Information Processing Systems 35: Annual Conference on Neural    [26] M. Ester, H. Kriegel,  J. Sander, and X. Xu, “A density-based algo-\n     Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,         rithm for discovering clusters in large spatial databases with noise,”\n    USA, November 28 - December 9, 2022, 2022.                                   in Proceedings of the Second International Conference on Knowledge\n[12] Z. Xie, Y. Chen, C. Zhi, S. Deng, and J. Yin, “Chatunitest: a chatgpt-        Discovery and Data Mining (KDD-96), Portland, Oregon, USA. AAAI\n     based automated unit test generation tool,” CoRR, vol. abs/2305.04764,          Press, 1996, pp. 226–231.\n     2023.                                                                    [27] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\n[13] Z. Yuan, M. Liu, S. Ding, K. Wang, Y. Chen, X. Peng, and Y. Lou,           P. Liang, “Lost in the middle: How language models use long contexts,”\n     “Evaluating and improving chatgpt for unit test generation,” Proc. ACM         Trans. Assoc. Comput. Linguistics, vol. 12, pp. 157–173, 2024.\n      Softw. Eng., vol. 1, no. FSE, pp. 1703–1726, 2024.                       [28] G. Kamradt, “Needle in a haystack  - pressure testing llms,”  https:\n[14]  S. Gao, X. Wen, C. Gao, W. Wang, H. Zhang, and M. R. Lyu,        //github.com/gkamradt/LLMTest NeedleInAHaystack, 2024.\n    “What makes good in-context demonstrations for code intelligence tasks    [29] MAPS,  “Replication  package,”  https://zenodo.org/records/14287744,\n     with llms?” in 38th IEEE/ACM International Conference on Automated        2024.\n     Software Engineering, ASE 2023, Luxembourg, September 11-15, 2023.    [30] M. Ivankovic, G. Petrovic, R. Just, and G. Fraser, “Code coverage at\n     IEEE, 2023, pp. 761–773.                                                     google,” in Proceedings of the ACM Joint Meeting on European Software\n[15] H. Gonen, S.  Iyer,  T. Blevins, N. A. Smith, and L. Zettlemoyer,        Engineering Conference and Symposium on the Foundations of Software\n     “Demystifying prompts in language models via perplexity estimation,”         Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30,\n      in Findings of the Association for Computational Linguistics: EMNLP        2019.  ACM, 2019, pp. 955–963.\n     2023, Singapore, December 6-10, 2023.  Association for Computational    [31] W. U. Ahmad, S. Chakraborty, B. Ray, and K. Chang, “Unified pre-\n      Linguistics, 2023, pp. 10 136–10 148.                                             training for program understanding and generation,” in Proceedings of\n[16] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database of existing         the 2021 Conference of the North American Chapter of the Association\n      faults to enable controlled testing studies for java programs,” in Inter-          for Computational Linguistics: Human Language Technologies, NAACL-\n     national Symposium on Software Testing and Analysis, ISSTA ’14, San      HLT 2021, Online, June 6-11, 2021.   Association for Computational\n      Jose, CA, USA - July 21 - 26, 2014.  ACM, 2014, pp. 437–440.                Linguistics, 2021, pp. 2655–2668.\n\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020                                                                                   15\n\n\n\n[32] A3Test, “A3test replication package,” https://github.com/awsm-research/\n     A3Test ShowCase, 2023.\n[33] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng, “Automatic\n     prompt optimization with ”gradient descent” and beam search,”  in\n     Proceedings of the 2023 Conference on Empirical Methods in Natural\n     Language Processing, EMNLP 2023, Singapore, December 6-10, 2023.\n     Association for Computational Linguistics, 2023, pp. 7957–7968.\n[34] H. Yu, Y. Lou, K. Sun, D. Ran, T. Xie, D. Hao, Y. Li, G. Li, and\n     Q. Wang, “Automated assertion generation via information retrieval and\n       its integration with deep learning,” in 44th IEEE/ACM 44th International\n     Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA,\n    May 25-27, 2022.  ACM, 2022, pp. 163–174.\n[35] W. Sun, H. Li, M. Yan, Y. Lei, and H. Zhang, “Revisiting and improving\n     retrieval-augmented deep assertion generation,” in 38th IEEE/ACM In-\n      ternational Conference on Automated Software Engineering, ASE 2023,\n     Luxembourg, September 11-15, 2023.  IEEE, 2023, pp. 1123–1135.\n[36]  S. Gao, W. Mao, C. Gao, L. Li, X. Hu, X. Xia, and M. R. Lyu,\n     “Learning in the wild: Towards leveraging unlabeled data for effectively\n     tuning pre-trained code models,” in Proceedings of the 46th IEEE/ACM\n      International Conference on Software Engineering, ICSE 2024, Lisbon,\n      Portugal, April 14-20, 2024.  ACM, 2024, pp. 80:1–80:13.\n[37] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, “Codamosa: Escaping\n     coverage plateaus in  test generation with pre-trained large language\n     models,” in 45th IEEE/ACM International Conference on Software En-\n      gineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023.  IEEE,\n     2023, pp. 919–931.\n[38]  S. Gao, H. Zhang, C. Gao, and C. Wang, “Keeping pace with ever-\n      increasing data: Towards continual learning of code intelligence models,”\n      in 45th IEEE/ACM International Conference on Software Engineering,\n    ICSE 2023, Melbourne, Australia, May 14-20, 2023.  IEEE, 2023, pp.\n     30–42.\n[39] Z. Yang, F. Liu, Z. Yu, J. W. Keung, J. Li, S. Liu, Y. Hong, X. Ma,\n     Z. Jin, and G. Li, “Exploring and unleashing the power of large language\n     models in automated code translation,” Proc. ACM Softw. Eng., vol. 1,\n     no. FSE, pp. 1585–1608, 2024.\n[40]  S. Gao, C. Gao, Y. He,  J. Zeng, L. Nie, X. Xia, and M. R. Lyu,\n    “Code structure-guided transformer for source code summarization,”\n   ACM Trans. Softw. Eng. Methodol., vol. 32, no. 1, pp. 23:1–23:32, 2023.\n[41] Z. Li, C. Wang, S. Wang, and C. Gao, “Protecting intellectual property\n      of large language model-based code generation apis via watermarks,” in\n     Proceedings of the 2023 ACM SIGSAC Conference on Computer and\n     Communications Security, CCS 2023, Copenhagen, Denmark, November\n     26-30, 2023.  ACM, 2023, pp. 2336–2350.\n[42] C. S. Xia and L. Zhang, “Keep the conversation going: Fixing 162 out\n      of 337 bugs for $0.42 each using chatgpt,” CoRR, vol. abs/2304.00385,\n     2023.\n[43] K. Zhang, Z. Li, J. Li, G. Li, and Z. Jin, “Self-edit: Fault-aware code\n      editor for code generation,” in Proceedings of the 61st Annual Meeting\n      of the Association  for Computational  Linguistics (Volume 1: Long\n      Papers), ACL 2023, Toronto, Canada, July 9-14, 2023.   Association\n      for Computational Linguistics, 2023, pp. 769–787.\n[44] Z. Li, C. Wang, P. Ma, C. Liu, S. Wang, D. Wu, C. Gao, and Y. Liu,\n    “On extracting specialized code abilities from large language models: A\n      feasibility study,” in Proceedings of the 46th IEEE/ACM International\n     Conference on Software Engineering, ICSE 2024, Lisbon, Portugal,\n      April 14-20, 2024.  ACM, 2024, pp. 74:1–74:13.\n[45]  S. Gao, C. Gao, W. Gu, and M. Lyu, “Search-based llms for code\n      optimization,” in 2025 IEEE/ACM 47th International Conference on\n     Software Engineering (ICSE). IEEE Computer Society, 2024, pp. 254–\n     266.\n[46] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,\n     Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder:\n    When the large language model meets programming - the rise of code\n      intelligence,” CoRR, vol. abs/2401.14196, 2024.\n[47] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi,\n     A. Tang, D. Pykhtar, J. Liu, Y. Wei, T. Liu, M. Tian, D. Kocetkov,\n     A. Zucker, Y. Belkada, Z. Wang, Q. Liu, D. Abulkhanov, I. Paul, Z. Li,\n    W. Li, M. Risdal, J. Li, J. Zhu, T. Y. Zhuo, E. Zheltonozhskii, N. O. O.\n     Dade, W. Yu, L. Krauß, N. Jain, Y. Su, X. He, M. Dey, E. Abati, Y. Chai,\n     N. Muennighoff, X. Tang, M. Oblokulov, C. Akiki, M. Marone, C. Mou,\n    M. Mishra, A. Gu, B. Hui, T. Dao, A. Zebaze, O. Dehaene, N. Patry,\n     C. Xu, J. J. McAuley, H. Hu, T. Scholak, S. Paquet, J. Robinson, C. J.\n     Anderson, N. Chapados, and et al., “Starcoder 2 and the stack v2: The\n     next generation,” CoRR, vol. abs/2402.19173, 2024.\n[48] Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang, “Magicoder: Source\n     code is all you need,” CoRR, vol. abs/2312.02120, 2023.",
"headers": [
"The Prompt Alchemist: Automated LLM-Tailored",
"Prompt Optimization for Test Case Generation",
"arXiv:2501.01329v1  [cs.SE]  2 Jan 2025",
"Shuzheng Gao",
", Chaozheng Wang",
", Cuiyun Gao",
", Xiaoqian Jiao",
", Chun Yong Chong",
",",
"Shan Gao",
", Michael R. Lyu",
"The Chinese University of Hong Kong, Hong Kong, China",
"Harbin Institute of Technology, Shenzhen, China",
"School of Information Technology, Monash University Malaysia, Malaysia",
"Independent Researcher, China",
"szgao23@cse.cuhk.edu.hk, adf111178@gmail.com, gaocuiyun@hit.edu.cn, 210110210@stu.hit.edu.cn,",
"chong.chunyong@monash.edu, gaoshan cs@outlook.com, lyu@cse.cuhk.edu.hk"
],
"tables": [
"|Prompt|ChatGPT|Llama-3.1|\n|---|---|---|\n|Write unit tests for the provided Java classes to<br>test the methods and functionalities of each class.|21.92%|**26.45%**|\n|Write unit tests for the given Java classes to<br>ensureproper functionality of the methods.|**24.46%**|24.07%|\n|Write test cases for the given Java class to<br>ensure the correct behavior of its methods.|22.90%|25.80%|",
"|meSeries copy =|timeSeries.createCopy(0,|Col3|\n|---|---|---|\n|timeSeries.getItemCount()-1);<br>|timeSeries.getItemCount()-1);<br>||",
"|meSeries copy =|timeSeries.createCopy(0,|Col3|\n|---|---|---|\n|timeSeries.getItemCount()-1);<br>|timeSeries.getItemCount()-1);<br>||",
"|① Write unit ...<br>② Write unit ...<br>③ Write test ...<br>④ Write unit ...|25.69%<br>27.25%<br>21.41%<br>26.05%|\n|---|---|",
"|Projects|Chart Cli Csv Gson Lang|Average|\n|---|---|---|",
"|Col1|Line Coverage|\n|---|---|",
"|Basic<br>APE<br>OPRO<br>EVOPROMPT (GA)<br>EVOPROMPT (DE)|47.41 36.76 37.49 22.42 57.58<br>43.34 39.43 39.32 23.42 54.92<br>44.52 42.49 38.96 25.71 53.70<br>48.47 45.71 38.59 21.82 56.34<br>49.60 42.88 39.87 24.17 53.39|45.56<br>44.58<br>45.13<br>46.63<br>45.89|\n|---|---|---|",
"|MAPS|51.56 (0.66) 58.88 (1.14) 50.05 (1.02) 25.17 (0.60) 61.35 (0.42)|53.80 (0.04)|\n|---|---|---|",
"|Col1|Branch Coverage|\n|---|---|",
"|Basic<br>APE<br>OPRO<br>EVOPROMPT (GA)<br>EVOPROMPT (DE)|33.07 23.27 32.39 14.21 46.19<br>33.72 24.30 34.32 15.53 44.59<br>33.86 28.26 32.78 16.94 42.46<br>34.36 31.64 34.18 14.53 45.07<br>35.70 29.32 34.24 16.04 43.27|34.24<br>34.25<br>34.28<br>35.88<br>35.11|\n|---|---|---|",
"|MAPS|38.68 (0.25) 41.54 (1.58) 39.53 (1.75) 16.20 (0.40) 51.11 (0.40)|41.84 (0.26)|\n|---|---|---|",
"|Projects|Chart Cli Csv Gson Lang|Average|\n|---|---|---|",
"|Col1|Line Coverage|\n|---|---|",
"|Basic<br>APE<br>OPRO<br>EVOPROMPT (GA)<br>EVOPROMPT (DE)|46.52 45.99 43.19 22.81 52.89<br>45.05 46.26 42.17 21.56 50.98<br>45.45 44.39 42.81 23.39 54.09<br>45.95 45.82 42.70 24.70 54.24<br>45.70 44.96 43.24 22.77 52.29|45.93<br>44.70<br>45.95<br>46.52<br>45.34|\n|---|---|---|",
"|MAPS|49.68 (0.21) 51.83 (1.54) 44.05 (0.19) 26.38 (0.96) 58.56 (1.49)|50.59 (0.56)|\n|---|---|---|",
"|Col1|Branch Coverage|\n|---|---|",
"|Basic<br>APE<br>OPRO<br>EVOPROMPT (GA)<br>EVOPROMPT (DE)|35.46 28.55 36.92 16.83 42.73<br>34.54 28.49 35.52 16.37 41.58<br>34.85 26.61 37.25 16.86 43.20<br>34.71 29.28 35.51 16.74 43.01<br>34.82 29.28 36.95 17.00 42.62|35.06<br>34.22<br>34.80<br>35.03<br>35.07|\n|---|---|---|",
"|MAPS|37.73 (0.68) 35.06 (0.27) 39.02 (1.14) 19.36 (0.65) 48.24 (1.69)|39.50 (0.68)|\n|---|---|---|",
"|Projects|Chart Cli Csv Gson Lang|Average|\n|---|---|---|",
"|Col1|Line Coverage|\n|---|---|",
"|Basic<br>APE<br>OPRO<br>EVOPROMPT (GA)<br>EVOPROMPT (DE)|39.75 36.80 40.73 26.51 42.11<br>39.52 34.76 45.73 23.83 44.44<br>40.84 38.51 38.94 26.41 33.41<br>39.08 36.49 37.50 21.23 43.56<br>39.08 36.49 37.50 21.23 43.56|38.70<br>39.41<br>35.49<br>38.17<br>38.17|\n|---|---|---|",
"|MAPS|44.37 (3.56) 47.56 (0.40) 49.80 (3.48) 29.75 (1.29) 48.42 (2.16)|45.51 (1.28)|\n|---|---|---|",
"|Col1|Branch Coverage|\n|---|---|",
"|Basic<br>APE<br>OPRO<br>EVOPROMPT (GA)<br>EVOPROMPT (DE)|30.88 23.79 32.55 18.18 30.55<br>31.07 21.35 36.22 16.79 33.27<br>32.47 26.76 29.58 19.92 25.26<br>30.81 22.16 29.71 13.84 33.14<br>30.81 22.16 29.71 13.84 33.14|28.05<br>28.92<br>26.66<br>27.98<br>27.98|\n|---|---|---|",
"|MAPS|30.58 (3.26) 31.73 (1.44) 36.30 (2.83) 18.98 (2.01) 37.11 (1.51)|32.71 (1.43)|\n|---|---|---|",
"|Approach|ChatGPT Llama-3.1 Qwen2|ChatGPT Llama-3.1 Qwen2|\n|---|---|---|",
"|Col1|Line Coverage|Branch Coverage|\n|---|---|---|",
"|ChatGPT’s final prompt<br>Llama-3.1’s final prompt<br>Qwen2’s final prompt<br>Manually-designed prompt|53.80 41.92 35.98<br>51.35 50.59 44.94<br>51.14 43.98 45.51<br>48.55 48.46 42.85|41.84 32.31 26.87<br>40.05 39.50 34.43<br>38.39 32.97 32.71<br>37.55 37.60 31.88|\n|---|---|---|",
"|Approach|Line Coverage Branch Coverage|\n|---|---|",
"|ChatGPT|MAPS<br>w/o Domain contextual knowledge extraction<br>w/o Diversity-guided prompt generation<br>w/o Failure-driven rule induction<br>Only domain contextual knowledge extraction|53.80 41.84<br>44.16 33.31<br>45.59 34.04<br>46.86 37.08<br>48.03 35.76|\n|---|---|---|",
"|Llama-3.1|MAPS<br>w/o Domain contextual knowledge extraction<br>w/o Diversity-guided prompt generation<br>w/o Failure-driven rule induction<br>Only domain contextual knowledge extraction|50.59 39.50<br>45.37 34.73<br>49.68 38.35<br>49.73 38.88<br>49.48 37.95|\n|---|---|---|",
"|Qwen2|MAPS<br>w/o Domain contextual knowledge extraction<br>w/o Diversity-guided prompt generation<br>w/o Failure-driven rule induction<br>Only domain contextual knowledge extraction|45.51 32.71<br>42.12 29.73<br>43.81 32.10<br>43.92 31.58<br>40.56 30.21|\n|---|---|---|",
"|Col1|Col2|Col3|EVOPR|OMPT(GA) MAPS|\n|---|---|---|---|---|\n|**2**<br>Fig.<br>**42**<br>APE<br>|**2**<br>Fig.<br>**42**<br>APE<br>|**2**<br>**3**<br>**4**<br><br><br>(a) Number of seed pr<br> 4: Parameter analysis o|||\n|**2**<br>Fig.<br>**42**<br>APE<br>|APE<br>||||\n|**2**<br>Fig.<br>**42**<br>APE<br>|OPRO<br>EVOPROMPT(GA)<br>EVOPROMPT(DE)<br>MAPS||||",
"|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||**22**<br>**1**<br>**2**<br>**3**<br>()<br>(b) Number of generated prompts.<br>        rompts and generated prompts on Chat<br>**40**<br>APE<br>OPRO|()|||\n|APE<br>|||||\n|OPRO<br>EVOPROMPT(GA)<br>EVOPROMPT(DE)<br>MAPS|||GA)<br>DE)|GA)<br>DE)|",
"|Final prompt of L|lama-3.1|\n|---|---|",
"|Col1|Col2|\n|---|---|\n|**  g**|**   y  pp**|",
"|Projects|Line Coverage Branch Coverage|\n|---|---|",
"|Randoop<br>A3Test|49.51 34.45<br>34.11 15.72|\n|---|---|",
"|ChatGPT+Basic<br>ChatGPT+MAPS|45.56 34.24<br>53.80 41.84|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2501.01329v1.pdf"
}