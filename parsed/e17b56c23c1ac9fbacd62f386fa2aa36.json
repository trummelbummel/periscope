{
"text": "A Survey of Automatic Prompt Optimization with Instruction-focused\n                            Heuristic-based Search Algorithm\n\n\n               Wendi Cui1*, Zhuohang Li3, Hao Sun 4, Damien Lopez1, Kamalika Das1,2\n                         Bradley Malin3,5, Sricharan Kumar1,2, Jiaxin Zhang1,2∗\n                   1Intuit   2Intuit AI Research   3Vanderbilt University    4 University of Cambridge\n                                        5Vanderbilt University Medical Center\n\n\n\n                          Abstract                         In contrast, automatic prompt optimization\n                                                          aims to systematically discover and refine prompts,\n                Recent advances in Large Language Models       minimizing human efforts and potentially uncov-\n              (LLMs) have led to remarkable achievements,       ering highly effective solutions that manual ex-\n               making prompt engineering increasingly cen-                                                              perimentation might overlook (Zhou et al., 2023).2025                       tral to guiding model outputs.  While man-\n                                                       These techniques treat prompt design as a search\n                   ual methods (e.g., “chain-of-thought,” “step-by-\n                                                            problem, wherein a heuristic-based search al-Jul            step” prompts) can be effective, they typically\n                    rely on intuition and do not automatically refine       gorithmic process iteratively evaluates candidate\n12           prompts. In contrast, automatic prompt op-      prompts and adapts them based on performance\n                 timization employing heuristic-based search       feedback or other criteria (Pryzant et al., 2023;\n                  algorithms can systematically explore and im-      Chen et al., 2024a; Yang et al., 2023a). Other lines\n                 prove prompts with minimal human oversight.       of work applies reinforcement learning (Zhang\n                 This survey proposes a comprehensive taxon-                                                                         et al., 2022; Deng et al., 2022; Sun et al., 2023a)\n            omy of these methods, categorizing them by\n                                                                 or ensemble methods (Hou et al., 2023; Pitis et al.,[cs.CL]          where optimization occurs, what is optimized,\n                                                         2023) to optimize prompts dynamically or adap-                what criteria drive the optimization, which op-\n                    erators generate new prompts, and which itera-        tively.  This survey focuses on heuristic-based\n                     tive search algorithms are applied. We further       search methods due to their ability to unify a broad\n                   highlight specialized datasets and tools that       range of strategies under an interpretable, mod-\n                  support and accelerate automated prompt re-       ular framework that is compatible with both dis-\n                   finement. We conclude by discussing key open                                                                    crete and soft prompt spaces. We further concen-\n                 challenges for future opportunities for more\n                                                                           trate on instruction-focused approaches, empha-\n                   robust and versatile LLM applications.\n                                                                   sizing the clarity and structure of the instruction\n                                                              while optionally incorporating in-context examples.\n          1  Introduction\n                                                                 Instruction-based prompting continues to be the\n          The rapid evolution of Large Language Models   dominant paradigm, making it a practical and im-\n          (LLMs) has catalyzed significant progress in di-   pactful focus. By narrowing our scope to this in-\n                                                                         tersection, we aim to provide a coherent taxonomyarXiv:2502.18746v2       verse tasks (Bubeck et al., 2023; Yang et al., 2023b).\n          As these models become more capable, the de-   and detailed analysis of methods that are both theo-\n             sign of the prompt used to interface with them    retically grounded and widely applicable.\n            has emerged as a crucial factor in terms of both    We begin by examining the fundamental dimen-\n            prompt content and format (Zhu et al., 2023). Man-   sions of where optimization happens (optimization\n             ual approaches such as “chain-of-thought” (Wei    space) and what is optimized (optimization target).\n               et al., 2023) prompting or instructing the model to  We then review what criteria to optimize (optimiza-\n            “Let’s think step by step” (Kojima et al., 2023) can    tion criteria), recognizing that many practitioners\n             yield enhanced performance in certain scenarios,   now consider objectives beyond task performance.\n             but these methods remain fundamentally reliant on   Then, we dive into how optimization happens by\n          human intuition and repeated trial-and-error.         categorizing which operators are used to create new\n                                                       prompt candidates and which iterative algorithm\n                 *The corresponding GitHub repository for this paper will    guides the refinement loop. With a growing body\n              be updated at https://github.com/jxzhangjhu/\n                                                                  of literature addressing these topics, we also review           Awesome-LLM-Prompt-Optimization.  Correspon-\n              dence to wendi_cui@intuit.com, jxzhangai@gmail.com.      benchmarking datasets covering a wide range of\n\n\n                                                    1\n\ndomains. Moreover, we survey a range of tools   3  Where does Optimization Happen?\n that can automate or streamline prompt optimiza-\n                                            Prompt optimization in LLMs can be broadly cat- tion workflows, enabling rapid iteration with less\n                                                  egorized into soft prompt space optimization and manual effort. We conclude by identifying open\n                                                        discrete prompt space optimization. problems. Addressing these challenges will unlock\n reliable, adaptable, and ethical LLM applications.\n                                                    3.1  Soft Prompt Space Optimization\n 2  Preliminary\n                                                   Soft prompt optimization operates in a continu-\n 2.1  Prompt Composition                      ous space, allowing for smooth adaptations with\n                                                  techniques such as gradient-based optimization. Prompts generally contain two main components:\n                                                     Different approaches vary in whether they utilize\n1. Instruction:  The  instruction  is a human-                                                      gradients and how they apply them.\n   readable statement describing the task or objec-\n   tive. An instruction establishes the intent and                                            Gradient for Embeddings  Using gradients for\n   context for the model, guiding it toward the de-                                                 optimizing prompt embeddings is a widely used\n   sired behavior for the particular task.                                               approach (Li and Liang, 2021; Zhang et al., 2021;\n2. Examples: In-context examples show the model   Sun et al., 2022b,a, 2024). Li et al. (2024a) uses\n  how to map specific inputs to outputs. These ex-   gradient descent to optimize prompts leveraging a\n   amples clarify the nature of the task and can im-    loss function tailored to improve prompts’ general-\n   prove model performance (Brown et al., 2020).     ization capability across diverse domains.\n                                             To improve efficiency, some research chooses to Prompts may include several examples or none.\n                                                    estimate gradients, rather than directly computing For this survey, we cover the instruction-focused\n                                                them. ZOPO (Hu et al., 2024) employs Zeroth- automatic prompt optimization, excluding research\n                                            Order Optimization to refine prompts without ex- on example-focused optimization.\n                                                                plicit gradient computation. It enables gradient es-\n 2.2  Heuristic-based Search Algorithms           timation by using the Neural Tangent Kernel (Jacot\n                                                              et al., 2018) in a derived Gaussian process, approx- Heuristic-based search algorithms provide a prac-\n                                                   imating optimization dynamics in neural networks. tical framework for optimization problems such\n as automatic prompt optimization (Pryzant et al.,\n                                             Gradient-Based Target Selection  Another line 2023; Zhou et al., 2023; Fernando et al., 2024).\n                                                   of research utilizes gradients to identify target to- Unlike brute-force methods that evaluate every pos-\n                                               kens to replace within a prompt candidate (Zhou sible variation, heuristic methods apply problem-\n                                                           et al., 2024; Zou et al., 2023; Zhao et al., 2024). specific knowledge or strategies to navigate the\n                                            Greedy Coordinate Gradient (GCG) (Zou et al., search space more efficiently (Blum and Roli, 2003;\n                                              2023) leverages gradient information to detect to- Talbi, 2009). Key steps in a heuristic-based ap-\n                                                kens that can minimize objective loss. It computes proach typically involve:\n                                                       the gradient of the loss function with respect to the\n1. Initialization:  Generating one or more can-\n                                                      vector of each token, selecting the top-k tokens with\n   didates (e.g., randomly, using partial domain\n                                                      the highest gradient values for modification. Zhao\n   knowledge, or by perturbing a known baseline).\n                                                           et al. (2024) enhance GCG with Probe Sampling,\n2. Evaluation: Measuring the performance of each   which accelerates prompt optimization by using\n   candidate with respect to a chosen metric (e.g.,   two models: a smaller, faster \"draft model\" that as-\n   accuracy on a validation set, or a scoring function    sesses potential token replacement candidates and\n   provided by a user).                                      filters out unpromising ones; and a larger, more\n                                                powerful \"target model\" which takes the filtered3. Selection and Update: Applying operators or\n                                                   candidates for a full evaluation. Probe Sampling   transitions to the current set of candidates to cre-\n                                                dynamically adjusts how many candidates are fil-   ate new candidates with improved performance,\n                                                        tered by measuring the agreement between the draft   diversity, or both.\n                                             and target models’ rankings of a small \"probe set\"\n4. Termination: Determining when to stop (e.g.,                                                      of candidates. This adaptive filtering minimizes un-\n   after a fixed number of iterations or once perfor-                                                  necessary computation for the large target model,\n  mance converges).                                                    leading to substantial speed gains.\n\n\n                                          2\n\nGradient for Embedding\n\n\n                                                                                                                            Gradient for Targets\n                                                                                               Soft Prompt (§3.1)\n                                        Where does Optimization Hap-\n                                              pen (§3)                                                                Gradient for Vocabulary\n                                                                                             Discrete Prompt (§3.2)\n                                                                                                                   Non-Gradient Approach\n\n                                                                                                Instruction-only (§4.1)           Example to Instruction\n\n                                           What is Optimized (§4)           Instruction & Example (§4.2)           Instruction to Example\n\n                                                                                                Instruction & Optional Example     Concurrent Instruction and Ex-\n                                                                                            (§4.3)                          ample\n\n\n                                                                                     Task Performance\n\n\n                                                                                                   Generalizability\n                                         What Criteria to Optimize (§5)\n                                                                                        Safety and Ethical Constraint\n\n\n                                                                                              Multi-Objective\n                                                                                                                     Lamarckian\n                                                                                           Zero-Parent (§6.1)\n                                                                                                                 Model-Based\n                    Optimization Methods                                                                                                                                           Partial Application\n                                                                                                                         Semantic\n                                                                                                                               Whole Prompt Application\n\n\n                                                                                                                                      LLM-Feedback\n                                                                                              Single-Parent (§6.2)\n                                         Which Operators are Used (§6)\n                                                                                                                     Feedback                   Human-Feedback\n\n\n                                                                                                           Add/ Subtract/ Replace              Gradient-Feedback\n\n\n                                                                                     EDA\n                       Search                                                                                 Multi-Parent (§6.3)                    Crossover                                  Optimization\n                 Prompt                                                                              Bandit Algorithm (§7.1)                   Difference\n                                                                   Beam Search (§7.2)                                                          Heuristic-based\n                                                                                             Heuristic Sampling (§7.3)                         Automatic with\n                                                                                                        Monte Carlo Search\n                                        Which  Iterative Algorithm  is      Monte Carlo Search (§7.4)\n                                           Used (§7)                                                    Monte Carlo Tree Search\n                                                                                                                                                            Genetic Algorithm\n                                     BBH,  Instruction  Induction,                                           Evolutionary Algorithm\n                                                             Ethos,                                                                SST-2,                                                                      Hot-                       Database (§8)         GSM8K,\n                                            potQA, Iris,                                               SVAMP,                                                                         Subj,                                                              CR,                                                                                        Differential Evolution\n                                    MR, TREC, Liar ...                 Metaheuristic Algorithm (§7.5)         General Metaheuristic\n\n                                                  PromptPerfect,    PromptIM,        Iterative Refinement (§7.6)             Phased Algorithm\n                                               Dspy, OpenPrompt, VertexAI,\n                          Tools (§9)             PromptBench, AWS Bedrock,\n                                                 Anthropic\n\nFigure 1: Taxonomy of Heuristic-based Search Algorithm in Automatic Prompt Optimization. Additional mapping\nof methods to taxonomy can be seen in Appendix Section A.\n\nGradient for Vocabulary  In DPO (Wang et al.,   3.2  Discrete Prompt Space Optimization\n2024b), gradients are estimated by a Shortcut Text                                                   Discrete prompt optimization treats prompts as\nGradient approach that continuously relaxes the                                                     fixed textual structures and refines them directly\ncategorical word choices to a learnable smooth dis-                                                (Diao et al., 2022; Prasad et al., 2023). Unlike soft\ntribution over the vocabulary using Gumbel Soft-                                              prompt methods, which adjust prompts in a contin-\nmax trick. This allows for the computation of gra-                                              uous space, discrete methods optimize prompts in\ndients through the non-differentiable embedding                                                 a non-differentiable space.\nlookup table. By ultimately learning a distribution                                             While soft prompt methods leverage gradient-\nover the vocabulary, DPO iteratively improves the                                               based optimization, discrete approaches have de-\nquality of the generated prompts.                                               veloped gradient-like strategies suited for non-\n                                                       differentiable settings.  ProTeGi (Pryzant et al.,\nNon-Gradient Approach  Other works optimize   2023) employs an LLM-based feedback system to\nin soft prompt space but do not employ a gradient-   generate pseudo-gradients and utilizes beam search\nbased approach. InstructZero (Chen et al., 2024a)    to iteratively refine prompts, effectively mimicking\nemploys Bayesian Optimization to adjust prompt    the refinement process in gradient-based methods.\nrepresentations and propose new soft prompts with-     Beyond gradient-like approaches, other methods\nout calculating gradients.                          explore alternative optimization strategies. Evo-\n\n\n                                         3\n\nPrompt (Guo et al., 2024) integrates evolutionary    prioritizes selecting the best combination of in-\nalgorithms to iteratively refine prompts, employing    struction and examples from a pool of pre-defined\nsemantic modification, crossover, and difference    candidates, using bandit algorithms to identify the\nmechanisms for optimization. Cheng et al. (2024)   most effective prompt structure. Whereas Adv-ICL\ntrains a prompt-optimize model to rewrite prompts.   (Long et al., 2024), dynamically generates new in-\n   Overall, while soft prompt optimization excels    structions and examples, refining both components\nin flexible and differentiable adjustments, discrete    iteratively with three models. By optimizing both\nprompt optimization remains crucial for structured   elements in tandem, these methods ensure that in-\nmodifications where interpretability and explicit    structions and examples are mutually reinforcing.\ntextual refinement are necessary.\n                                                    4.3  Instruction & Optional Example\n4  What is Optimized?                                                 Unlike prior approaches that strictly generate exam-\n                                                       ples with instructions, PhaseEvo (Cui et al., 2024b)4.1  Instruction-only Optimization\n                                                     introduces a flexible framework capable of generat-\nEarly approaches primarily focused on refining                                                    ing both few-shot and zero-shot prompts depending\nthe instruction itself through techniques such as                                           on what works best for the task. This adaptability\nrephrasing, adding constraints, or incorporating ad-                                                  allows the model to optimize performance dynam-\nditional context (Yang et al., 2023a; Hsieh et al.,                                                              ically, selecting whether examples are necessary\n2024; Pan et al., 2024).  After instruction opti-                                               based on empirical effectiveness.\nmization, some approaches introduce examples ran-\ndomly to enhance task performance (Zhou et al.,   5  What Criteria to Optimize\n2023; Pryzant et al., 2023). However, this choice\noften overlooks the interaction between added ex-   In heuristic-based automatic prompt optimization,\namples and instructions, resulting in suboptimal    the objectives or criteria for refinement vary widely\nsolutions (Wang et al., 2024a; Wan et al., 2024).     depending on the application domain. While early\n                                                   research predominantly focused on task perfor-\n4.2  Instruction & Example Optimization        mance, growing interest in real-world deployments\n                                                 has led to broader optimization goals:Recent research has increasingly focused on opti-\nmizing both instructions and examples. Existing   1. Task Performance Most approaches prioritize\napproaches can be classified into three paradigms:      task-specific metrics and optimize prompts to\n                                               enhance performance on the given task. Some\nExample to Instruction  This approach begins                                                 approaches use a validation set to evaluate can-\nby selecting and preprocessing examples, which                                                       didates (Guo et al., 2024), while others use a\nare then used to generate an appropriate instruction.                                                    surrogate model of the objective function for\nMoP (Wang et al., 2024a) adopts this strategy by                                                     candidate evaluation and selection (Opsahl-Ong\nclustering examples into Expert Subregions and                                                              et al., 2024; Chen et al., 2024b).\nderiving specialized instructions for each cluster.\nThis ensures that the instructions are well-aligned   2. Generalizability  Some  methodologies  ex-\nwith the examples, improving task adaptation and      tend beyond single-task performance, seeking\noverall effectiveness.                               prompts that generalize across multiple domains.\n                                                    Li et al. (2024a) introduce a Concentrate-focused\nInstruction to Example  In this approach, an ini-     framework to improve the domain generalizabil-\ntial instruction is used to generate examples that       ity of prompts by leveraging internal information\nalign with the intended task. MIPRO (Opsahl-Ong     from deep model layers. Their findings indicate\net al., 2024) follows this methodology by employ-      that prompts receiving higher attention from deep\ning a default instruction to create successful input-      layers tend to generalize better and that prompts\noutput pairs as examples. This ensures that the      with stable attention distributions enhance gen-\nexamples complement the instruction and reinforce       eralization. Their approach optimizes for gener-\nthe model’s understanding of the task.                    alizability in both soft and discrete spaces.\n\nConcurrent Instruction and Example  This cat-   3. Safety and Ethical Constraints Ensuring safety\negory focuses on optimizing both instructions and       is a critical aspect of prompt optimization for\nexamples simultaneously. EASE (Wu et al., 2024)      large language models.  Studies such as RPO\n\n\n                                         4\n\n(Zhou et al., 2024) emphasize the importance of               I gave a friend an instruction and some input.\n   designing prompt suffixes that resist adversarial          The friend read the instruction and wrote an\n                                                                      output for every one of the inputs.\n   manipulations and mitigate unintended behav-          Here are the input-output pairs:\n   iors. These safeguards are essential for defend-\n   ing against jailbreaking attempts.                        ## Example ##\n                                                                      {input output pairs}\n4. Multi-Objective Optimization Multi-objective\n                                                         The instruction was:\n   optimization plays a crucial role in balancing dif-\n   ferent priorities such as accuracy, efficiency, and       Table 1: Lamarckian Operator Prompt Example\n   safety. SOS (Sinha et al., 2024) employs an inter-\n   leaved multi-objective evolutionary algorithms    of prompt performance using Bayesian Optimiza-\n   to optimize both task performance and safety    tion. MIPRO (Opsahl-Ong et al., 2024) uses a\n  where one objective is optimized first followed    Tree-structured Parzen estimator to build a surro-\n  by another. In contrast, other approaches adopt    gate model to select the instruction and example\n   a parallel optimization strategy of all objectives,    pair. INSTINCT (Lin et al., 2024b) uses a trained\n   followed by Pareto Optimization to derive the    neural network for score prediction. Although such\n   most effective prompts across multiple objectives   models do not take any parent prompts directly,\n  (Menchaca Resendiz and Klinger, 2025; Yang    they do tap into learning from previous prompts to\n   and Li, 2023; Baumann and Kramer, 2024).       enhance their performance.\n\n 6  Which Operators are Used                  6.2  Single-Parent Operators\n                                           Semantic  These operators generate candidates\n For iterative optimization, generating new candi-\n                                                         that share semantic meaning with their parent, ei-\n date prompts is essential. These generation meth-\n                                                       ther through the use of LLMs or alternative meth-\n ods, referred to as operators, are categorized based\n                                                     ods. Semantic operators can be categorized into:\n on the number of parent prompts needed. Parent\n                                                               • Partial Application: Semantic operators selec- prompts are existing prompts used to derive new\n candidates.                                                 tively modify specific sections of a prompt while\n                                                    maintaining the overall structure. This targeted\n 6.1  Zero-Parent Operators                       approach enables precise adjustments, allowing\n                                                         for fine-tuning of particular aspects without al-\n Lamarckian  The Lamarckian operator  is an\n                                                          tering the entire prompt. In AELP (Hsieh et al.,\n LLM-based operator that emulates the Lamarckian\n                                                  2024) and SCULPT (Kumar et al., 2024), partial\n adaptation process, which refers to the idea of feed-\n                                                         application of LLM-based semantic operators is\n ing back learned improvements (i.e., successful\n                                                              utilized to systematically adjust key components\n outputs or reasoning traces—phenotypes) into the\n                                                       of extensive prompts.\n prompt itself (genotype) to inform future genera-\n tions. This mimics the Lamarckian notion of inher-    • Whole Prompt Application: Other semantic\n iting acquired traits (Fernando et al., 2024). By an-     operators apply transformations to the entire\n alyzing concrete inputs that yield correct outputs, it     prompt. PhaseEvo (Cui et al., 2024b) uses an\n attempts to reverse-engineer the instruction prompt.    LLM-based semantic operator to perform the\n This operator is widely adopted in research, espe-      last-mile optimization in a phased optimization\n cially during initialization (Zhou et al., 2023; Hu      process. FIPO (Lu et al., 2025) finetunes a model\n et al., 2024; Wu et al., 2024; Fernando et al., 2024;      to perform such rewriting. Xu et al. (2022) trans-\nWang et al., 2024a). MIPRO (Opsahl-Ong et al.,       lates a candidate to another language and back as\n 2024) extends this concept further by incorporating      a way to create new candidates. For soft prompts,\n additional contextual information, such as identi-     Shen et al. (2023) uses a perturbation kernel to\n fying patterns within raw datasets, which enables      perturb the sampled candidate embeddings from\nLLMs to better comprehend task intentions. Table      the previous iteration to create new prompts.\n1 presents an example of a Lamarckian operator.\n                                           Feedback  These methods utilize feedback from\n Model-Based  This approach uses models to gen-   various sources to optimize prompts. They usually\n erate the next candidate. Chen et al. (2024a) and    involve two steps: feedback generation and feed-\n Sabbatella et al. (2023) build probabilistic models   back application. Such feedback in a soft prompt\n\n\n                                          5\n\nspace can be considered as the gradient. Based on        You are a mutator. Given a series of prompts, your\nthe feedback generation process, such operators           task is to generate another prompt with the same\n                                                                 semantic meaning and intentions.\ncan be categorized as:\n• LLM-Feedback:  These operators leverage         ## Existing Prompts ##\n                                                                        {existing prompt}\n LLMs to evaluate and refine prompts. Such an\n  approach taps into LLMs’ self-reflection (Shinn        The newly mutated prompt is:\n   et al., 2023) ability to pinpoint deficiencies and\n                                                                Table 2: EDA Prompt Example  suggest refinements, facilitating the automated\n  creation of more robust and effective prompts\n                                     Guo et al., 2024). The idea is to mix traits of both\n  through continuous self-improvement (Cheng\n                                                    parents to compose a diverse candidate.\n   et al., 2024; Dong et al., 2024; Ye et al., 2024).\n                                                 Difference  These LLM operators analyze differ-\n• Human-Feedback: Human feedback plays a\n                                                  ences between prompts to identify patterns for gen-\n  crucial role in prompt optimization, providing nu-\n                                                        erating new candidates. EvoPrompt-DE (Guo et al.,\n  anced, context-aware evaluations that automated\n                                             2024) uses the Differential Evolution algorithm\n  systems may miss. PROMPST (Chen et al.,\n                                                 with such operators.\n  2024b) integrates human-designed rules to offer\n  corrective feedback when errors arise, ensuring                                       7  Which Iterative Algorithm is Used\n  more precise refinements. APOHF (Lin et al.,\n  2024a) leverages human preferences as an indi-    Iterative algorithms are crucial in automatic prompt\n  cator for selecting the most effective prompts.      optimization. They guide the selection and applica-\n                                                        tion of operators to refine prompts effectively.\n• Gradient-Feedback:  Gradient-feedback  in-\n  volves using optimization techniques that adjust                                                    7.1  Bandit Algorithms\n  prompts based on gradient-based signals derived\n                                                Bandit algorithms are a class of decision-making\n  from the model’s performance metrics. This ap-\n                                                        strategies designed to balance the exploration-\n  proach is particularly effective for soft-prompt\n                                                       exploitation trade-off. Wu et al. (2024) formulates\n  optimization and allows for precise and efficient\n                                                automatic prompt selection as a bandit problem,\n  adjustments (Wang et al., 2024b; Sordoni et al.,\n                                              employing a neural bandit algorithm to predict the\n  2023; Zhou et al., 2024).\n                                                     effectiveness of different sets of exemplars based\nAdd/Subtract/Replace  These operators refine   on their embeddings. Similarly, Shi et al. (2024) in-\nprompts by inserting, removing, or substituting el-   troduces the BAI-FB framework, which efficiently\nements (Prasad et al., 2023; Juneja et al., 2024;   explores and identifies the optimal prompt from\nZhang et al., 2024d). DPO (Wang et al., 2024b)   a candidate pool while operating under a con-\nmodels each word as a genotype and applies these    strained budget. These approaches demonstrate\noperators within an evolutionary algorithm frame-   the effectiveness of bandit-based methods in refin-\nwork for end-to-end prompt optimization.           ing prompt selection and improving overall model\n                                                 performance.\n6.3  Multiple-Parent Operators\n                                                    7.2  Beam Search\nEDA  These operators generate new candidates\n                                         The Beam Search method systematically expandsfrom multiple parent prompts. OPRO (Yang et al.,\n                                                a set of promising prompt candidates and prunes2023a) adds both parent prompts and their perfor-\n                                                        less effective ones, allowing efficient explorationmance on the validation set as additional informa-\n                                                   of large search spaces. ERM (Yan et al., 2024),tion. IPO (Du et al., 2024) applies a similar strategy\n                                            ProTeGi (Pryzant et al., 2023) use beam search tobut on multimodal tasks. LCP (Li et al., 2024b)\n                                                           iteratively select candidates for optimization.introduces contrastive examples to boost perfor-\nmance. Table 2 is an example of an EDA operator.                                                    7.3  Heuristic Sampling\n\nCrossover  This operator follows genetic algo-   Heuristic sampling is a method that utilizes rule-\nrithms and combines components from two-parent   based strategies to efficiently select representatives\nprompts to create new prompts (Baumann and   from large sets of candidates minimizing computa-\nKramer, 2024; Yang and Li, 2023; Jin et al., 2024b;    tional resources while maintaining high accuracy.\n\n\n                                         6\n\nChen et al. (2024b) employs heuristic-based sam-   pares both across identical tasks, demonstrating\npling to prioritize the most promising prompts from    that each algorithm excels in different scenarios.\nan extensive pool, guided by human feedback to\n                                           General Metaheuristic  Additional metaheuris-ensure their relevance and effectiveness.\n                                                                tic algorithms such as Hill Climbing, Simulated\n7.4  Monte Carlo Search                         Annealing, Tabu Search, Harmony Search, and oth-\n                                                      ers following metaheuristic principles are widely\nMonte Carlo Search  Monte Carlo search is a\n                                                adopted for automatic prompt optimization as well\nprobabilistic algorithm that uses random sampling\n                                             (Zhang et al., 2024a; Sun et al., 2023b; Yang et al.,\nto explore and evaluate possible actions or deci-\n                                                2024; Jin et al., 2024a; Lin et al., 2024b; Gao et al.,\nsions within a given problem space, enabling the\n                                               2025; Tang et al., 2025). Pan et al. (2024) specifi-\nestimation of optimal strategies through repeated\n                                                        cally conducted a comparison among them.\nsimulations. APE (Zhou et al., 2023) leverages the\nMonte Carlo search to enhance prompt engineering                                          Phased Algorithms  Cui et al. (2024b) proposes\nby systematically exploring a vast array of potential                                                  a phased algorithm adopting a metaheuristic frame-\nprompts and assessing their effectiveness.                                          work to increase the efficiency of the optimization\n                                                     process. By creating four phases balancing explo-\nMonte Carlo Tree Seach  Monte Carlo Tree\n                                                       ration and exploitation, they achieve several mag-\nSearch (MCTS) is a search algorithm that builds a\n                                                   nitudes of efficiency improvements compared to\nsearch tree incrementally through a series of selec-\n                                                     other algorithms in terms of LLM calls.\ntion, expansion, simulation, and backpropagation\nsteps. PromptAgent (Wang et al., 2024c) constructs                                                    7.6  Iterative Refinement\na search tree where each node represents a potential\nprompt. By keeping a state-action value function    Iterative Refinement refers to the other algorithms\nthat calculates the potential rewards from following    that repeatedly use different operators to refine\nthe path, the system iteratively refines prompts to   prompts.  Gradient descent is a widely-adopted\nenhance performance.                           example (Hu et al., 2024; Wang et al., 2024b; Zhou\n                                                            et al., 2024).\n7.5  Metaheuristic Algorithms\n                                       8  Common Datasets Used\nMetaheuristic algorithms are high-level, problem-\nindependent optimization strategies designed to ef-                                                Considering the broad applicability of prompt op-\nficiently explore large and complex search spaces                                                      timization, a variety of databases across different\nwhere exact methods are infeasible. Inspired by                                            domains were used, as shown in Table 4 in the\nnatural processes such as evolution, physical sys-                                              Appendix. The two most common ones are:\ntems, or social behavior, these algorithms guide the\n                                                              • BBH (Big-Bench Hard) (Aarohi and bench au-\nsearch toward optimal or near-optimal solutions\n                                                             thors, 2023): A subset of the broader Big-Bench\nthrough iterative improvement, balancing explo-\n                                                          project, BBH is designed to probe the limits of\nration and exploitation (Talbi, 2009).\n                                                  language models with especially challenging or\nEvolutionary Algorithms  Evolutionary algo-     nuanced tasks.\nrithms are widely utilized in prompt optimization,                                                              • Instruction Induction (Honovich et al., 2022):\ndrawing inspiration from natural selection to it-                                                  This dataset explicitly focuses on inferring new\neratively refine prompts (Li and Wu, 2023; Jin                                                         task instructions from examples, making it partic-\net al., 2024b). Two algorithms in this category                                                         ularly relevant for evaluating instruction-based\nare Genetic Algorithm (GA) and Differential Evolu-                                               prompt optimization approaches.\ntion(DE): GA applies evolutionary principles, in-\ncluding mutation, selection, and crossover, to it-                                       9  Common Tools\neratively enhance prompts, ensuring gradual im-\nprovement across successive generations. DE gen-   Automatic prompt optimization tools aim to ac-\nerates new prompt candidates by utilizing the dif-    celerate and streamline the optimization process.\nferences between existing solutions, promoting di-   Below, we provide an overview of notable tools and\nversity while converging toward optimal solutions.    their key characteristics. Table 3 gives a high-level\nEvoPrompt (Guo et al., 2024) systematically com-   overview of these tools.\n\n\n                                         7\n\nTool                          Optimization Space             Key Features             Open Source\n\n   PromptPerfect                          Discrete           Web-based, optimized for user queries        No\n\n  PromptIM                              Discrete              Iterative refine with human in the loop         Yes\n\n  Dspy (Khattab et al., 2024)              Discrete         Task decomposition and example bootstrap       Yes\n\n  OpenPrompt (Ding et al., 2021)       Soft/Discrete        Predefined templates for prompt learning        Yes\n\n   Vertex AI (Wan et al., 2024)             Discrete            Google Cloud-based optimization         No\n\n  PromptBench (Zhu et al., 2023)         Discrete                    Test prompt robustness                Yes\n\n  AWS Bedrock                          Discrete         Playground with evaluation and A/B testing      No\n\n   Anthropic Claude                       Discrete              Interactive editor with live feedback        No\n\n                        Table 3: Comparison of automatic prompt optimization tools.\n\nPromptPerfect  PromptPerfect is a commercial    templates for different prompting methods, such\nplatform that offers automated prompt optimiza-   as prefix-tuning and P-tuning.  Its combinability\ntion services.  It allows users to input a prompt    across different Pretrained LMs, task formats, and\nand target a specific LLM. The platform then uses    optimization methods makes it a valuable tool.\nits internal algorithms to refine and improve the\n                                              Vertex AI Prompt Optimizer  Google Cloud’s\nprompt. It provides a user-friendly interface and\n                                                 Vertex AI platform offers a Prompt Optimizer as\nis designed to be accessible even to users without\n                                                       part of its suite of tools. This service allows users\ndeep technical expertise.\n                                                      to experiment with and optimize prompts follow-\nPromptIM  PromptIM is an experimental open-   ing learning from Wan et al. (2024). Integrated\nsource library. Given an initial prompt, a dataset,   within the Vertex AI ecosystem, the Prompt Opti-\nand evaluators, PromptIM iteratively refines the   mizer benefits from Google’s cloud infrastructure\nprompt using a meta prompt to suggest improve-   and provides a scalable solution for prompt opti-\nments based on evaluation scores. It integrates with    mization tasks.\nLangSmith for data and prompt management and                                       PromptBench  PromptBench (Zhu et al., 2023)\nsupports optional human feedback. Contrary to                                                                is an open-source benchmark designed to evalu-\nother tools, PromptIM prioritizes keeping humans                                                      ate the robustness of prompts under adversarial\nin the loop throughout the optimization process.                                                      perturbations. Rather than introducing new task\n                                                        datasets, PromptBench applies systematic trans-DSPy  DSPy is a framework developed by Stan-\n                                            formations—such as instruction negation or para-ford researchers that takes a more declarative ap-\n                                             phrasing—to existing prompts across a variety ofproach to building complex LLM applications. In-\n                                                   standard NLP datasets. It helps researchers assessstead of directly writing prompts, users define\n                                     how well prompt optimization methods preservethe desired \"program\" as a series of declarative\n                                           model performance under distributional shift.steps. DSPy implements MIPRO (Opsahl-Ong\net al., 2024) and uses LLMs to generate and opti-  AWS Bedrock AWS Bedrock includes a Prompt\nmize the underlying prompts to fulfill the program’s    Engineering Playground within its cloud platform,\ngoals. This approach allows for more structured    allowing users to prototype and evaluate prompts\nand modular development of LLM applications    across multiple foundation models. The interface\nand facilitates prompt optimization as part of the    supports A/B testing, real-time inference, and eval-\nprogram execution. DSPy emphasizes the decom-   uation metrics. While not open-source, it provides\nposition of complex tasks into simpler sub-tasks    a practical environment for optimizing and compar-\nand is widely used in research (Soylu et al., 2024).   ing prompt variants in production-ready workflows.\n\nOpenPrompt  OpenPrompt (Ding et al., 2021)   Anthropic Claude Prompt Tools  Anthropic’s\nis an open-source toolkit specifically designed   Claude platform offers interactive tools for prompt\nfor prompt-learning.  It provides a standardized    optimization directly through its web interface.\nframework for implementing and experimenting   These tools provide live feedback, suggest rewrites,\nwith various templating, verbalizing, and optimiza-   and support prompt experimentation tailored specif-\ntion strategies.  OpenPrompt offers pre-defined    ically to Claude models. While proprietary, they\n\n\n                                         8\n\nare useful for developers seeking to iteratively   DLN-2 optimizes the Evidence Lower Bound to\nrefine instructions with guidance grounded in    refine both prompts. MIPRO (Opsahl-Ong et al.,\nClaude’s internal safety and helpfulness principles.   2024) extends this concept to multi-stage optimiza-\n                                                           tion, treating each stage as a module and using\n10  Open Challenges                         Bayesian Search to identify the best prompt combi-\n                                                      nations. These methods represent a shift towards\nSoft to Discrete Projection  Soft prompt spaces\n                                                  concurrent prompt optimization, reducing human\nenable continuous optimization but often lack in-\n                                                          effort while improving adaptability for complex\nteroperability provided by discrete prompts. To\n                                                      task scenarios.\naddress this, some methods project soft embed-\ndings back into discrete space. Hu et al. (2024),   Additional Challenges  Beyond the challenges\nWen et al. (2023) and Zhao et al. (2024) adopt a    discussed earlier, several open issues remain criti-\npre-generated finite set of unique candidates, where    cal. Multi-objective optimization continues to be a\ngradient-updated embeddings are mapped back to   complex area, requiring methods that can balance\nthe closest entry in this set. However, this approach    performance, safety, generalizability, and efficiency\nheavily depends on the quality of the pre-generated    simultaneously (Sinha et al., 2024; Menchaca Re-\nset. Another approach leverages an open-source    sendiz and Klinger, 2025; Yang and Li, 2023). Re-\nLLM as a converter to translate soft prompts into    cent work explores Pareto-front approximations,\ndiscrete instructions (Chen et al., 2024a; Lin et al.,   but reliable aggregation of heterogeneous metrics\n2024b), offering a more flexible and adaptive solu-   remains unsolved. In addition, Scalability across\ntion. Further research in this area could enhance   Domains and Tasks is limited by overfitting to spe-\nboth optimization effectiveness and interpretability.    cialized datasets. General-purpose optimizers must\n                                                     learn transferable representations or search strate-\nDynamic N-shot Selection  While Instruction &                                                     gies applicable in diverse settings. Another area\nExample paradigms have shown significant im-                                                                 is On-line fashion optimization. Existing methods\nprovements by jointly optimizing examples and                                                   take thousands of API calls (Cui et al., 2024b) or\ninstructions (Wan et al., 2024; Menchaca Resendiz                                                     require specific training (Hu et al., 2024), making\nand Klinger, 2025), recent findings indicate that                                                                              it impractical for online optimization. Incremental\nfew-shot prompting does not always enhance per-                                                update rules and memory-efficient surrogate mod-\nformance and can \"consistently degrades its perfor-                                                          els could empower near real-time regimes.\nmance\" (DeepSeek-AI et al., 2025). This highlights\nthe necessity of Instruction & Optional Example   11  Conclusion\nparadigm which dynamically selects between few-\n                                                This survey has explored heuristic-based searchshot and zero-shot prompting based on empirical\n                                                  algorithms  for  Automatic  Instruction-Focusedeffectiveness rather than a fixed strategy.  Initial\n                                           Prompt Optimization, organizing methods alongsteps in this direction have been explored by Cui\n                                                          five key dimensions: the optimization space, theet al. (2024b), and future optimization approaches\n                                                  optimization target, the optimization criteria, theshould emphasize adaptability, tailoring prompt\n                                                     operators, and the iterative algorithms. The goalstructures to specific tasks for optimal performance.\n                                         was to allow mixing and matching components\nConcurrent Optimization  For complex tasks    like a toolkit, enabling the design of new prompt\nusing LLMs, multiple agents might be involved    optimization pipelines by combining different op-\n(Zhang et al., 2024c; Cui et al., 2024a; Zhang et al.,    erators with various search or learning algorithms.\n2024b). Traditionally, humans will define the scope   To make this more concrete, one might think of\nof each agent and optimize them separately. Re-   our framework like a recipe book: operators are\ncent research has introduced automated concurrent    the ingredients (e.g., \"add\", \"replace\", \"rephrase\"),\noptimization, which optimizes multiple prompts   and the iterative algorithms are the cooking meth-\nconcurrently. DLN-2 (Sordoni et al., 2023) allows   ods (e.g., \"bake\" with genetic algorithms, \"slow\nconcurrent optimization of two prompts by consid-   simmer\" with beam search). Different combina-\nering both LLMs as probabilistic layers in a net-    tions yield different flavors—and innovations. We\nwork. It treats the first prompt’s output as a latent   hope this survey could jump-start researchers in\nvariable. Using variational inference to approxi-   understanding the existing landscape and inspire\nmate the latent variable with a simpler distribution,   new research on the practical application of LLMs.\n\n\n                                         9\n\n12  Limitations                                        optimization in multi-step tasks (PROMST): Integrat-\n                                                         ing human feedback and heuristic-based sampling.\nThe work does not cover In Context Learning opti-      In Proceedings of the 2024 Conference on Empiri-\nmization or methods using reinforcement learning.      cal Methods in Natural Language Processing, pages\n                                                   3859–3920, Miami, Florida, USA. Association forWe also focus on works after 2023 so previous work\n                                                      Computational Linguistics.\nis not fully covered. Such space can be expanded\nfor future works.                                             Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning\n                                               Wang, Yuxiao Dong, Jie Tang, and Minlie Huang.\nAcknowledgments                                 2024.  Black-box prompt optimization: Aligning\n                                                             large language models without model training. In\n                                                      Proceedings of the 62nd Annual Meeting of the As-This work includes contributions from Vanderbilt\n                                                            sociation for Computational Linguistics (Volume 1:\nUniversity researchers, supported by funding from                                                 Long Papers), pages 3201–3219, Bangkok, Thailand.\nIntuit.                                                   Association for Computational Linguistics.\n\n                                                    Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nReferences                                   Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\n                                                             Plappert, Jerry Tworek, Jacob Hilton, Reiichiro\nSrivastava Aarohi and BIG bench authors. 2023. Be-     Nakano, Christopher Hesse, and John Schulman.\n  yond the imitation game: Quantifying and extrapolat-      2021. Training verifiers to solve math word prob-\n   ing the capabilities of language models. Transactions       lems. arXiv preprint arXiv:2110.14168.\n  on Machine Learning Research.\n                                             Wendi Cui, Zhuohang Li, Damien Lopez, Kamalika\nJill Baumann and Oliver Kramer. 2024. Evolutionary      Das, Bradley A. Malin, Sricharan Kumar, and Jiaxin\n   multi-objective optimization of large language model      Zhang. 2024a.  Divide-conquer-reasoning for con-\n  prompts for balancing sentiments.  arXiv preprint       sistency evaluation and automatic improvement of\n   arXiv:2401.09862.                                        large language models. In Proceedings of the 2024\n                                                     Conference on Empirical Methods in Natural Lan-\nChristian Blum and Andrea Roli. 2003. Metaheuristics      guage Processing: Industry Track, pages 334–361,\n   in combinatorial optimization: Overview and concep-     Miami, Florida, US. Association for Computational\n   tual comparison. ACM Computing Surveys (CSUR),       Linguistics.\n   35(3):268–308.\n                                             Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun,\nTom Brown, Benjamin Mann, Nick Ryder, Melanie     Damien Lopez, Kamalika Das, Bradley Malin, and\n   Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind       Sricharan Kumar. 2024b. Phaseevo: Towards unified\n   Neelakantan, Pranav Shyam, Girish Sastry, Amanda       in-context prompt optimization for large language\n   Askell,  Sandhini Agarwal,  Ariel  Herbert-Voss,      models. arXiv preprint arXiv:2402.11347.\n  Gretchen Krueger, Tom Henighan, Rewon Child,\n                                                  DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,  Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\n                                                       Junxiao Song, Ruoyu Zhang, Runxin Xu,  Qi-   Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\n                                                  hao Zhu, Shirong Ma, Peiyi Wang, et al. 2025.   teusz Litwin, Scott Gray, Benjamin Chess, Jack\n                                                       Deepseek-r1: Incentivizing reasoning capability in   Clark, Christopher Berner, Sam McCandlish, Alec\n                                                        llms via reinforcement learning.  arXiv preprint   Radford, Ilya Sutskever, and Dario Amodei. 2020.\n                                                        arXiv:2501.12948.  “language models are few-shot learners\".  In Ad-\n  vances in Neural Information Processing Systems                                                  Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\n  33 (NeurIPS 2020).                                               Wang, Han Guo, Tianmin Shu, Meng Song, Eric P\n                                                       Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-                                                              discrete text prompts with reinforcement learning.\n   dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,                                                        arXiv preprint arXiv:2205.12548.\n   Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\n   berg, et al. 2023. Sparks of artificial general intelli-   Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li,\n   gence: Early experiments with gpt-4. arXiv preprint     Yong Lin, Xiao Zhou, and Tong Zhang. 2022. Black-\n   arXiv:2303.12712.                                 box prompt learning for pre-trained language models.\n                                                        arXiv preprint arXiv:2201.08531.\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng\n  Huang, and Tianyi Zhou. 2024a. InstructZero: Ef-   Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\n   ficient instruction optimization for black-box large      Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun.\n  language models. In Proceedings of the 41st Inter-      2021. Openprompt: An open-source framework for\n   national Conference on Machine Learning, volume       prompt-learning. arXiv preprint arXiv:2111.01998.\n  235 of Proceedings of Machine Learning Research,\n   pages 6503–6518. PMLR.                         Yihong Dong, Kangcheng Luo, Xue Jiang, Zhi Jin, and\n                                          Ge Li. 2024. PACE: Improving prompt with actor-\nYongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang,        critic editing for large language model. In Findings of\n   Nicholas Roy, and Chuchu Fan. 2024b. PRompt       the Association for Computational Linguistics: ACL\n\n\n                                         10\n\n2024, pages 7304–7323, Bangkok, Thailand. Associ-   Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang,\n   ation for Computational Linguistics.                  Wujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong,\n                                                      Sanguthevar Rajasekaran, and Dimitris N. Metaxas.\nYingjun Du, Wenfang Sun, and Cees Snoek. 2024.      2024a. Apeer: Automatic prompt engineering en-\n   Ipo: Interpretable prompt optimization for vision-      hances large language model reranking.   arXiv\n  language models. In Advances in Neural Informa-       preprint arXiv:2406.14449.\n   tion Processing Systems, volume 37, pages 126725–\n                                                   Feihu Jin, Yifan Liu, and Ying Tan. 2024b. Zero-shot  126766. Curran Associates, Inc.\n                                                          chain-of-thought reasoning guided by evolutionary\n                                                          algorithms in large language models. arXiv preprintChrisantha  Fernando,  Dylan  Banarse,  Henryk\n                                                        arXiv:2402.05376.  Michalewski, Simon Osindero, and Tim Rock-\n   täschel. 2024.   Promptbreeder:   self-referential                                                  Gurusha Juneja, Nagarajan Natarajan, Hua Li, Jian Jiao,\n  self-improvement via prompt evolution.  In Pro-                                                  and Amit Sharma. 2024.  Task facet learning: A\n  ceedings of the 41st International Conference on                                                             structured approach to prompt optimization. arXiv\n  Machine Learning, ICML’24. JMLR.org.                                                             preprint arXiv:2406.10504.\n\nShuzheng Gao, Chaozheng Wang, Cuiyun Gao, Xiao-   Omar Khattab, Arnav Singhvi, Paridhi Maheshwari,\n   qian Jiao, Chun Yong Chong, Shan Gao, and Michael      Zhiyuan Zhang, Keshav Santhanam,  Sri Vard-\n  Lyu. 2025. The prompt alchemist: Automated llm-     hamanan, Saiful Haq, Ashutosh Sharma, Thomas T.\n   tailored prompt optimization for test case generation.       Joshi, Hanna Moazam, Heather Miller, Matei Zaharia,\n  arXiv preprint arXiv:2501.01329.                     and Christopher Potts. 2024.  DSPy: Compiling\n                                                               declarative language model calls into self-improving\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao       pipelines. In Proceedings of the Twelfth International\n  Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu      Conference on Learning Representations.\n  Yang. 2024. Connecting large language models with\n   evolutionary algorithms yields powerful prompt op-    Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\n   timizers. In International Conference on Learning       taka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\n  Representations (ICLR).                              guage models are zero-shot reasoners. arXiv preprint\n                                                        arXiv:2205.11916.\nOr Honovich, Uri Shaham, Samuel R Bowman, and\n                                                Shanu Kumar, Akhila Yesantarao Venkata, Shubhanshu  Omer Levy. 2022. Instruction induction: From few\n                                                       Khandelwal, Bishal Santra, Parag Agrawal, and Man-  examples to natural language task descriptions. arXiv\n                                                              ish Gupta. 2024. Sculpt: Systematic tuning of long   preprint arXiv:2205.10782.\n                                                        prompts. arXiv preprint arXiv:2410.20788.\n\nBairu Hou, Joe O’connor, Jacob Andreas, Shiyu Chang,                                              Chengzhengxu Li, Xiaoming Liu, Zhaohan Zhang,\n  and Yang Zhang. 2023. Promptboosting: Black-box                                                   Yichen Wang, Chen Liu, Yu Lan, and Chao Shen.\n   text classification with ten forward passes. In Inter-                                                      2024a.  Concentrate attention: Towards domain-\n   national Conference on Machine Learning, pages                                                             generalizable prompt optimization for language mod-\n  13309–13324. PMLR.                                                                       els. In Advances in Neural Information Processing\n                                                           Systems, volume 37, pages 3391–3420. Curran Asso-\nCho-Jui Hsieh, Si Si, Felix Yu, and Inderjit Dhillon.       ciates, Inc.\n  2024. Automatic engineering of long prompts. In\n  Findings of the Association for Computational Lin-   Mingqi Li, Karan Aggarwal, Yong Xie, Aitzaz Ahmad,\n   guistics: ACL 2024, pages 10672–10685, Bangkok,      and Stephen Lau. 2024b. Learning from contrastive\n   Thailand. Association for Computational Linguistics.      prompts: Automated optimization and adaptation.\n                                                        arXiv preprint arXiv:2409.15199.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\n   rizing customer reviews. In Proceedings of the 2004   Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\n ACM SIGKDD International Conference on Knowl-      Optimizing continuous prompts for generation. arXiv\n  edge Discovery and Data Mining.                         preprint arXiv:2101.00190.\n\n                                                Xiaoyu Li, Wei Zhang, Jiahui Chen, and Hongwei Liu.\nWenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu,\n                                                      2024c.   Iris: Benchmarking large language mod-\n  Xiaoqiang Lin, Zhongxiang Dai, See-Kiong Ng, and                                                                 els for information retrieval tasks.  arXiv preprint\n  Bryan Kian Hsiang Low. 2024. Localized zeroth-                                                        arXiv:2401.12345.\n   order prompt optimization. In Advances in Neural\n  Information Processing Systems, volume 37, pages                                                       Yujian Betterest Li and Kai Wu. 2023. Spell: Semantic\n  86309–86345. Curran Associates, Inc.                                                  prompt evolution based on a llm.  arXiv preprint\n                                                        arXiv:2310.01260.\nArthur Jacot, Franck Gabriel, and Clément Hongler.\n  2018. Neural tangent kernel: Convergence and gen-   Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-\n   eralization in neural networks. In Advances in Neural     Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang\n  Information Processing Systems, volume 31, pages      Low. 2024a. Prompt optimization with human feed-\n  8571–8580.                                            back. arXiv preprint arXiv:2405.17346.\n\n\n                                         11\n\nXiaoqiang  Lin,  Zhaoxuan Wu,  Zhongxiang  Dai,   Rakesh Patel, Shyam Upadhyay, Debanjan Mahata, Ya-\n  Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet,     man Kumar, and Rakesh Gosangi. 2021. Svamp:\n  and Bryan Kian Hsiang Low. 2024b. Use your in-      Evaluating mathematical reasoning of transformers\n   stinct: Instruction optimization for llms using neural      on realistic problems. In Findings of the Association\n   bandits coupled with transformers. arXiv preprint       for Computational Linguistics: EMNLP 2021.\n  arXiv:2310.02905.\n                                                          Silviu  Pitis,  Michael R  Zhang,  Andrew Wang,\nXuan Do Long, Yiran Zhao, Hannah Brown, Yuxi Xie,     and Jimmy Ba. 2023.   Boosted prompt ensem-\n  James Xu Zhao, Nancy F. Chen, Kenji Kawaguchi,       bles for large language models.  arXiv preprint\n  Michael Shieh, and Junxian He. 2024. Prompt op-      arXiv:2304.05970.\n   timization via adversarial in-context learning.  In\n                                                     Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit  Proceedings of the 62nd Annual Meeting of the As-\n                                                         Bansal. 2023. GrIPS: Gradient-free, edit-based in-   sociation for Computational Linguistics (Volume 1:\n                                                                 struction search for prompting large language models.  Long Papers), pages 7308–7327, Bangkok, Thailand.\n                                                          In Proceedings of the 17th Conference of the Euro-  Association for Computational Linguistics.\n                                                  pean Chapter of the Association for Computational\nJunru Lu, Siyu An, Min Zhang, Yulan He, Di Yin,       Linguistics, pages 3845–3864, Dubrovnik, Croatia.\n  and Xing Sun. 2025. FIPO: Free-form instruction-      Association for Computational Linguistics.\n   oriented prompt optimization with preference dataset\n                                                 Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang  and modular fine-tuning schema. In Proceedings of\n                                                      Zhu, and Michael Zeng. 2023. Automatic prompt op-   the 31st International Conference on Computational\n                                                             timization with “gradient descent” and beam search.   Linguistics, pages 11029–11047, Abu Dhabi, UAE.\n                                                          In Proceedings of the 2023 Conference on Empiri-  Association for Computational Linguistics.\n                                                             cal Methods in Natural Language Processing, pages\n                                                   7957–7968, Singapore. Association for Computa-Yarik Menchaca Resendiz and Roman Klinger. 2025.\n                                                                 tional Linguistics.  MOPO: Multi-objective prompt optimization for af-\n   fective text generation. In Proceedings of the 31st\n                                                 Antonio  Sabbatella,  Andrea  Ponti,  Antonio Can-\n   International Conference on Computational Linguis-\n                                                                     delieri,  Ilaria Giordani, and Francesco Archetti.\n   tics, pages 5588–5606, Abu Dhabi, UAE. Associa-\n                                                      2023. A bayesian approach for prompt optimiza-\n   tion for Computational Linguistics.\n                                                               tion in pre-trained language models. arXiv preprint\n                                                        arXiv:2312.00471.Ioannis Mollas, Zafeiria Chrysopoulou, Spiros Karlos,\n  and Grigorios Tsoumakas. 2022. Ethos: an online                                          Maohao Shen, Soumya Ghosh, Prasanna Sattigeri,\n   hate speech detection dataset. In Proceedings of the                                                    Subhro Das, Yuheng Bu, and Gregory Wornell. 2023.\n  5th Workshop on Online Abuse and Harms (WOAH                                                             Reliable gradient-free and likelihood-free prompt tun-\n  2022).                                                               ing.  In Findings of the Association for Computa-\n                                                              tional Linguistics: EACL 2023, pages 2416–2429,\nKrista Opsahl-Ong, Michael J Ryan, Josh Purtell, David\n                                                      Dubrovnik, Croatia. Association for Computational\n  Broman, Christopher Potts, Matei Zaharia, and Omar\n                                                               Linguistics.\n   Khattab. 2024. Optimizing instructions and demon-\n   strations for multi-stage language model programs.                                                 Chengshuai Shi, Kun Yang, Zihan Chen, Jundong Li,\n   In Proceedings of the 2024 Conference on Empiri-                                                          Jing Yang, and Cong Shen. 2024. Efficient prompt\n   cal Methods in Natural Language Processing, pages                                                          optimization through the lens of best arm identifica-\n  9340–9366, Miami, Florida, USA. Association for                                                                    tion. In Advances in Neural Information Processing\n  Computational Linguistics.                                                           Systems, volume 37, pages 99646–99685. Curran As-\n                                                                 sociates, Inc.\nRui Pan, Shuo Xing, Shizhe Diao, Wenhe Sun, Xiang\n   Liu, KaShun Shum, Jipeng Zhang, Renjie Pi, and   Noah Shinn, Federico Cassano, Edward Berman, Ash-\n  Tong Zhang. 2024. Plum: Prompt learning using      win Gopinath, Karthik Narasimhan, and Shunyu Yao.\n   metaheuristics. In Findings of the Association for      2023. Reflexion: Language agents with verbal rein-\n  Computational Linguistics: ACL 2024, pages 2177–      forcement learning. In Advances in Neural Informa-\n  2197, Bangkok, Thailand. Association for Computa-       tion Processing Systems, volume 36.\n   tional Linguistics.\n                                                   Ankita Sinha, Wendi Cui, Kamalika Das, and Jiaxin\nBo Pang and Lillian Lee. 2004. A sentimental education:      Zhang. 2024.  Survival of the safest: Towards se-\n  Sentiment analysis using subjectivity summarization      cure prompt optimization through interleaved multi-\n  based on minimum cuts. In Proceedings of the 42nd       objective evolution. In Proceedings of the 2024 Con-\n  Annual Meeting of the Association for Computational       ference on Empirical Methods in Natural Language\n   Linguistics.                                           Processing: Industry Track, pages 1016–1027, Mi-\n                                                          ami, Florida, US. Association for Computational Lin-\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting        guistics.\n   class relationships for sentiment categorization with\n   respect to rating scales. In Proceedings of the 43rd    Richard Socher, Alex Perelygin, Jean Wu, Jason\n  Annual Meeting of the Association for Computational      Chuang, Christopher D Manning, Andrew Y Ng, and\n   Linguistics.                                            Christopher Potts. 2013. Recursive deep models for\n\n\n                                         12\n\nsemantic compositionality over a sentiment treebank.   Ruochen Wang, Sohyun An, Minhao Cheng, Tianyi\n   In Proceedings of the 2013 Conference on Empirical      Zhou, Sung Ju Hwang, and Cho-Jui Hsieh. 2024a.\n  Methods in Natural Language Processing.            One prompt is not enough: automated construction of\n                                                       a mixture-of-expert prompts. In Proceedings of the\nAlessandro Sordoni, Eric Yuan, Marc-Alexandre Côté,      41st International Conference on Machine Learning,\n  Matheus Pereira, Adam Trischler, Ziang Xiao, Arian      ICML’24. JMLR.org.\n   Hosseini, Friederike Niedtner, and Nicolas Le Roux.\n  2023.  Joint prompt optimization of stacked llms   Ruochen Wang, Ting Liu, Cho-Jui Hsieh, and Boqing\n  using variational inference. In Advances in Neural      Gong. 2024b. On discrete prompt optimization for\n  Information Processing Systems, volume 36, pages       diffusion models. In Proceedings of the 41st Inter-\n  58128–58151. Curran Associates, Inc.                    national Conference on Machine Learning, volume\n                                                 235 of Proceedings of Machine Learning Research,\nDilara Soylu, Christopher Potts, and Omar Khattab.      pages 50992–51011. PMLR.\n  2024. Fine-tuning and prompt optimization: Two\n                                                  William Yang Wang. 2017.  “liar, liar pants on fire”:   great steps that work better together. In Proceedings\n                                  A new benchmark dataset for fake news detection.   of the 2024 Conference on Empirical Methods in\n                                                        arXiv preprint arXiv:1705.00648.  Natural Language Processing, pages 10696–10710,\n  Miami, Florida, USA. Association for Computational                                               Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Hao-\n   Linguistics.                                                                 tian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing,\n                                                  and Zhiting Hu. 2024c.  Promptagent:  StrategicHao Sun, Alihan Hüyük, and Mihaela van der Schaar.\n                                                        planning with language models enables expert-level  2023a. Query-dependent prompt evaluation and opti-\n                                                    prompt optimization. In International Conference on   mization with offline inverse rl. arXiv e-prints, pages\n                                                      Learning Representations (ICLR).  arXiv–2309.\n                                                    Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nHong Sun, Xue Li, Yinchuan Xu, Youkow Homma,                                                 Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\n  Qi Cao, Min Wu, Jian Jiao, and Denis Charles. 2023b.                                              Denny Zhou. 2023.  Chain-of-thought prompting\n  Autohint: Automatic prompt optimization with hint                                                                       elicits reasoning in large language models.  arXiv\n   generation. arXiv preprint arXiv:2307.07415.                                                             preprint arXiv:2201.11903.\nTianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou,   Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Gold-\n  Xuan-Jing Huang, and Xipeng Qiu. 2022a. Bbtv2:      blum, Jonas Geiping, and Tom Goldstein. 2023. Hard\n  towards a gradient-free future with large language      prompts made easy: Gradient-based discrete opti-\n  models. In Proceedings of the 2022 Conference on      mization for prompt tuning and discovery.  In Ad-\n  Empirical Methods in Natural Language Processing,      vances in Neural Information Processing Systems,\n  pages 3916–3930.                                 volume 36, pages 51008–51025. Curran Associates,\n                                                                 Inc.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\n  Huang, and Xipeng Qiu. 2022b. Black-box tuning for   Zhaoxuan Wu,  Xiaoqiang  Lin,  Zhongxiang  Dai,\n   language-model-as-a-service. In International Con-     Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet,\n   ference on Machine Learning, pages 20841–20855.     and Bryan Kian Hsiang Low. 2024. Prompt opti-\n  PMLR.                                              mization with ease? efficient ordering-aware auto-\n                                                    mated selection of exemplars. In Advances in Neural\nYifan Sun, Jean-Baptiste Tien, and Karthik Lakshmanan.      Information Processing Systems, volume 37, pages\n  2024.  Retrieval augmented prompt optimization.     122706–122740. Curran Associates, Inc.\n   In Proceedings of the International Conference on\n  Learning Representations (ICLR).                 Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang\n                                                   Yanggang, Haiyu Li, and Zhilin Yang. 2022. GPS:\nEl-Ghazali Talbi. 2009. Metaheuristics: From Design      Genetic prompt search for efficient few-shot learning.\n   to Implementation. John Wiley & Sons, Hoboken,      In Proceedings of the 2022 Conference on Empiri-\n  NJ.                                                       cal Methods in Natural Language Processing, pages\n                                                   8162–8171, Abu Dhabi, United Arab Emirates. As-\nXinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan                                                              sociation for Computational Linguistics.\n  Lu, Yaliang Li, and Ji-Rong Wen. 2025. Unleashing\n   the potential of large language models as prompt    Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao,\n   optimizers: Analogical analysis with gradient-based      Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang\n  model optimizers. arXiv preprint arXiv:2402.17564.     Kang, and Yangyang Kang. 2024.  Efficient and\n                                                          accurate prompt optimization: the benefit of mem-\nEllen M Voorhees and Dawn M Tice. 2000. The trec-8      ory in exemplar-guided reflection.  arXiv preprint\n   question answering track report. In Proceedings of      arXiv:2411.07446.\n   the Eighth Text REtrieval Conference (TREC-8).\n                                               Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao\nXingchen Wan, Ruoxi Sun, Hootan Nakhost, and Ser-      Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.\n  can O. Arik. 2024. Teach better or show smarter?      2023a. “challenging big-bench tasks and whether\n  on instructions and exemplars in automatic prompt       chain-of-thought can solve them\".  arXiv preprint\n   optimization. arXiv preprint arXiv:2406.15708.          arXiv:2309.03409.\n\n\n                                         13\n\nHeng Yang and Ke Li. 2023. InstOptima: Evolution-      language models better few-shot learners.  arXiv\n   ary multi-objective instruction optimization via large       preprint arXiv:2108.13161.\n  language model-based instruction operators. In Find-\n   ings of the Association for Computational Linguis-   Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale\n   tics: EMNLP 2023, pages 13593–13602, Singapore.     Schuurmans, and Joseph E Gonzalez. 2022. Tem-\n  Association for Computational Linguistics.                pera: Test-time prompting via reinforcement learning.\n                                                        arXiv preprint arXiv:2211.11890.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\n  Han, Qizhang Feng, Haoming Jiang, Bing Yin, and    Yiran Zhao, Wenyue Zheng, Tianle Cai, Do Xuan Long,\n  Xia Hu. 2023b.  Harnessing the power of llms in      Kenji   Kawaguchi,   Anirudh   Goyal,   and\n   practice: A survey on chatgpt and beyond. arXiv      Michael  Qizhe  Shieh.  2024.     Accelerating\n   preprint arXiv:2304.13712.                           greedy coordinate gradient and general prompt\n                                                         optimization via probe sampling.  In Advances in\nSheng Yang, Yurong Wu, Yan Gao, Zineng Zhou,      Neural Information Processing Systems, volume 37,\n  Bin Benjamin Zhu, Xiaodi Sun, Jian-Guang Lou,      pages 53710–53731. Curran Associates, Inc.\n  Zhiming Ding, Anbang Hu, Yuan Fang, Yunsong Li,\n  Junyan Chen, and Linjun Yang. 2024. AMPO: Auto-   Andy Zhou, Bo Li, and Haohan Wang. 2024. Robust\n  matic multi-branched prompt optimization. In Pro-     prompt optimization for defending language models\n   ceedings of the 2024 Conference on Empirical Meth-      against jailbreaking attacks. In Advances in Neural\n  ods in Natural Language Processing, pages 20267–      Information Processing Systems, volume 37, pages\n  20279, Miami, Florida, USA. Association for Com-     40184–40211. Curran Associates, Inc.\n   putational Linguistics.\n                                              Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,      Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\n  William Cohen, Ruslan Salakhutdinov, and Christo-      Ba. 2023. Large language models are human-level\n  pher D Manning. 2018.  Hotpotqa: A dataset for      prompt engineers. arXiv preprint arXiv:2211.01910.\n   diverse, explainable multi-hop question answering.\n   In Proceedings of the 2018 Conference on Empiri-    Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen\n   cal Methods in Natural Language Processing, pages     Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei\n  2369–2380.                                          Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.\n                                                    Promptbench: Towards evaluating the robustness of\nQinyuan Ye, Maxamed Axmed, Reid Pryzant, and       large language models on adversarial prompts. arXiv\n   Fereshte Khani. 2024. Prompt engineering a prompt       preprint arXiv:2306.04528.\n   engineer. arXiv preprint arXiv:2311.05661.\n                                          Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr,\nChenrui Zhang, Lin Liu, Chuyuan Wang, Xiao Sun,        J. Zico Kolter, and Matt Fredrikson. 2023. Univer-\n  Hongyu Wang, Jinpeng Wang, and Mingchen Cai.       sal and transferable adversarial attacks on aligned\n  2024a.   Prefer:  prompt ensemble learning via      language models. arXiv preprint arXiv:2307.15043.\n   feedback-reflect-refine. In Proceedings of the Thirty-\n  Eighth AAAI Conference on Artificial Intelligence\n  and Thirty-Sixth Conference on Innovative Applica-\n   tions of Artificial Intelligence and Fourteenth Sym-\n  posium on Educational Advances in Artificial Intelli-\n   gence, AAAI’24/IAAI’24/EAAI’24. AAAI Press.\n\nJiaxin Zhang, Wendi Cui, Yiran Huang, Kamalika Das,\n  and Sricharan Kumar. 2024b. Synthetic knowledge\n   ingestion: Towards knowledge refinement and in-\n   jection for enhancing large language models. arXiv\n   preprint arXiv:2410.09629.\n\nJiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A.\n  Malin, and Sricharan Kumar. 2024c. Sac3: Reliable\n   hallucination detection in black-box language models\n   via semantic-aware cross-check consistency. arXiv\n   preprint arXiv:2311.01740.\n\nLechen Zhang, Tolga Ergen, Lajanugen Logeswaran,\n  Moontae Lee, and David Jurgens. 2024d.  Sprig:\n  Improving large language model performance by\n  system prompt  optimization.    arXiv  preprint\n  arXiv:2410.14826.\n\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng,\n  Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun\n  Chen. 2021. Differentiable prompt makes pre-trained\n\n\n                                         14\n\nA  Methods Categorization based on Taxonomy\n\nBelow are the categorizations for methods surveyed in this paper based on the taxonomy.\n\n\n\n\n\n                                                                             Wen et al. (2023), Shen et al.\n                                                                             Gradient for Embedding         (2023), ZOPO (Hu et al., 2024),\n                                                                                                          Li et al. (2024a)\n\n\n                                                                  GCG (Zou et al., 2023), RPO\n                                                                               Gradient for Targets         (Zhou et al., 2024),Probe Sam-\n                                                                                                                   pling (Zhao et al., 2024)\n                                                Soft Prompt (§3.1)\n\n                                                                             Gradient for Vocabulary      DPO (Wang et al., 2024b)\n\n\n                                                                                                                InstructZero  (Chen   et   al.,\n                                                                                                           2024a), INSTINCT (Lin et al.,\n                                                                         Non-Gradient Approach\n                                                                                                            2024b), Sabbatella et al. (2023),\n                                                                     DLN (Sordoni et al., 2023)\n    Where does Optimization Hap-\n     pen (§3)                                           GPS (Xu et al., 2022), APE (Zhou et al., 2023), GrIPS (Prasad et al.,\n                                                                         2023), ProTeGi (Pryzant et al., 2023), AutoHint (Sun et al., 2023b),\n                                                     PREFER (Zhang et al., 2024a), OPRO (Yang et al., 2023a), EvoPrompt\n                                                              (Guo et al., 2024), PromptBreeder (Fernando et al., 2024), SPELL (Li\n                                                                  and Wu, 2023), PromptAgent (Wang et al., 2024c), InstOptima (Yang\n                                                                    and Li, 2023), BPO (Cheng et al., 2024), PE2 (Ye et al., 2024), Plum (Pan\n                                                                                        et al., 2024), AELP (Hsieh et al., 2024), Adv-ICL (Long et al., 2024),\n                                            EMO (Baumann and Kramer, 2024), EOT (Jin et al., 2024b), PROMPST\n                                              Discrete Prompt (§3.2)        (Chen et al., 2024b), EPO (Shi et al., 2024), PhaseEvo (Cui et al., 2024b),\n                                                          FIPO (Lu et al., 2025), GPO (Tang et al., 2025), PACE (Dong et al.,\n                                                                           2024), EASE (Wu et al., 2024), APOHF (Lin et al., 2024a), UniPrompt\n                                                                             (Juneja et al., 2024), MIPRO (Opsahl-Ong et al., 2024), APEER (Jin\n                                                                                        et al., 2024a), DLN (Sordoni et al., 2023), MoP (Wang et al., 2024a),\n                                                                    Soylu et al. (2024), LCP (Li et al., 2024b), AMPO (Yang et al., 2024),\n                                                           SoS (Sinha et al., 2024), SPRIG (Zhang et al., 2024d), IPO (Du et al.,\n                                                                          2024), SCULPT (Kumar et al., 2024), ERM (Yan et al., 2024), MOPO\n                                                                 (Menchaca Resendiz and Klinger, 2025), MAPS (Gao et al., 2025)\n\n                         Figure 2: \"Where does optimization happen\" Categorization\n\n\n\n\n\n                                         15\n\nGPS (Xu et al., 2022), APE (Zhou et al., 2023), Wen et al. (2023), GrIPS\n                                                                     (Prasad et al., 2023), Shen et al. (2023), ProTeGi (Pryzant et al., 2023),\n                                                                          InstructZero (Chen et al., 2024a), AutoHint (Sun et al., 2023b), PREFER\n                                                             (Zhang et al., 2024a), OPRO (Yang et al., 2023a), EvoPrompt (Guo\n                                                                                    et al., 2024), PromptBreeder (Fernando et al., 2024), SPELL (Li and Wu,\n                                                                       2023), INSTINCT (Lin et al., 2024b), PromptAgent (Wang et al., 2024c),\n                                               BPO (Cheng et al., 2024), PE2 (Ye et al., 2024), Plum (Pan et al., 2024),\n                                               AELP (Hsieh et al., 2024), Sabbatella et al. (2023), EMO (Baumann\n                                                             and Kramer, 2024), EOT (Jin et al., 2024b), PROMPST (Chen et al.,\n                                         Instruction-only (§4.1)\n                                                                     2024b), EPO (Shi et al., 2024), FIPO (Lu et al., 2025), GPO (Tang et al.,\n                                                                    2025), PACE (Dong et al., 2024), Zhao et al. (2024), ZOPO (Hu et al.,\n                                                                    2024), APOHF (Lin et al., 2024a), UniPrompt (Juneja et al., 2024), Li\n                                                                                 et al. (2024a), APEER (Jin et al., 2024a), DLN (Sordoni et al., 2023),\n                                            DPO (Wang et al., 2024b), Soylu et al. (2024), LCP (Li et al., 2024b),\n                                          AMPO (Yang et al., 2024), SoS (Sinha et al., 2024), SPRIG (Zhang et al.,\n                                                                     2024d), IPO (Du et al., 2024), SCULPT (Kumar et al., 2024), ERM (Yan\n                                                                                 et al., 2024), MOPO (Menchaca Resendiz and Klinger, 2025), MAPS\n                                                            (Gao et al., 2025)\n\n   What is Optimized (§4)\n                                                              Example to Instruction      MoP (Wang et al., 2024a)\n\n\n                                                                      MIPRO  (Opsahl-Ong  et  al.,\n                                                                                Instruction to Example\n                                       Instruction & Example (§4.2)                                      2024)\n\n\n                                                                                                      InstOptima (Yang and Li, 2023),\n                                                                 Concurrent Instruction and Ex-\n                                                                      EASE (Wu et al., 2024), Adv-\n                                                             ample\n                                                                                   ICL (Long et al., 2024)\n\n                                      Instruction & Optional Example\n                                                             PhaseEvo (Cui et al., 2024b)\n                                     (§4.3)\n\n                            Figure 3: \"What is optimized\" Categorization\n\n\n\n\n\n                                                  GPS (Xu et al., 2022), APE (Zhou et al., 2023), Wen et al. (2023), GrIPS\n                                                                     (Prasad et al., 2023), Shen et al. (2023), ProTeGi (Pryzant et al., 2023),\n                                                                          InstructZero (Chen et al., 2024a), AutoHint (Sun et al., 2023b), PREFER\n                                                             (Zhang et al., 2024a), OPRO (Yang et al., 2023a), EvoPrompt (Guo\n                                                                                 et al., 2024), PromptBreeder (Fernando et al., 2024), SPELL (Li and\n                                                     Wu, 2023), INSTINCT (Lin et al., 2024b), PromptAgent (Wang et al.,\n                                                                      2024c), BPO (Cheng et al., 2024), PE2 (Ye et al., 2024), Plum (Pan et al.,\n                                                                    2024), AELP (Hsieh et al., 2024), Sabbatella et al. (2023), EOT (Jin\n                                      Task Performance\n                                                                                 et al., 2024b), PROMPST (Chen et al., 2024b), EPO (Shi et al., 2024),\n                                                      FIPO (Lu et al., 2025), GPO (Tang et al., 2025), PACE (Dong et al.,\n                                                                    2024), Zhao et al. (2024), ZOPO (Hu et al., 2024), APOHF (Lin et al.,\n                                                                    2024a), UniPrompt (Juneja et al., 2024), Li et al. (2024a), APEER (Jin\n                                                                                 et al., 2024a), DLN (Sordoni et al., 2023), DPO (Wang et al., 2024b),\n                                                               Soylu et al. (2024), LCP (Li et al., 2024b), AMPO (Yang et al., 2024),\n                                                    SPRIG (Zhang et al., 2024d), IPO (Du et al., 2024), SCULPT (Kumar\n                                                                                   et al., 2024), ERM (Yan et al., 2024), MAPS (Gao et al., 2025)\nWhat Criteria to Optimize (§5)\n                                             Generalizability            Li et al. (2024a)\n\n\n                                          GCG (Zou et al., 2023), RPO\n                                   Safety and Ethical Constraints\n                                                             (Zhou et al., 2024)\n\n\n                                                                    InstOptima (Yang and Li, 2023),\n                                       EMO (Baumann and Kramer,\n                                          Multi-Objective            2024), MOPO (Menchaca Re-\n                                                                      sendiz and Klinger, 2025), SOS\n                                                                    (Sinha et al., 2024)\n\n                         Figure 4: \"What criteria to optimize\" Categorization\n\n\n\n\n\n                                      16\n\nGPS (Xu et  al., 2022), APE\n                                                                                          (Zhou  et  al., 2023), Prompt-\n                                                                                                    Breeder (Fernando et al., 2024),\n                                                                       Lamarckian            PhaseEvo (Cui et al., 2024b),\n                                                                        EASE (Wu et al., 2024), ZOPO\n                                                                                     (Hu et al., 2024), MoP (Wang\n                                                                                                                            et al., 2024a)\n                                         Zero-Parent (§6.1)\n\n                                                                                                           InstructZero  (Chen   et   al.,\n                                                                                                      2024a), INSTINCT (Lin et al.,\n                                                                                                    2024b), BPO (Cheng  et  al.,\n                                                                     Model-Based\n                                                                                                      2024), Sabbatella et al. (2023),\n                                                                                  FIPO (Lu et al., 2025), MIPRO\n                                                                                               (Opsahl-Ong et al., 2024)\n\n                                                                                              AELP  (Hsieh  et  al.,  2024),\n                                                                                                                                Partial Application\n                                                                                                 SCULPT (Kumar et al., 2024)\n\n\n                                                                                                GPS (Xu et al., 2022), GrIPS\n                                                                                                                                          (Prasad et al., 2023), Shen et al.\n                                                                          Semantic                                                     (2023), InstructZero (Chen et al.,\n                                                                                                                                        2024a), EvoPrompt (Guo et al.,\n                                                                                                                                        2024),  PromptBreeder  (Fer-\n                                                                                                                         nando et al., 2024), InstOptima\n                                                                                      Whole Prompt Application       (Yang and Li, 2023), EMO (Bau-\n                                                                                                          mann and Kramer, 2024), EOT\n                                                                                                                                                             (Jin et  al., 2024b), PhaseEvo\n                                                                                                                                    (Cui et al., 2024b), SoS (Sinha\n                                                                                                                                                                   et  al., 2024), MOPO (Men-\n                                                                                                                                 chaca Resendiz and Klinger,\n                                                                                                                                2025)\n\n\n                                                                                                                          ProTeGi (Pryzant et al., 2023),\n                                                                                                                            AutoHint (Sun et  al., 2023b),\n                                                                                                PREFER (Zhang et al., 2024a),\n                                                                                                                        PromptAgent  (Wang  et   al.,\n                                                                                                                                        2024c), BPO (Cheng  et  al.,\n                                                                                                                                        2024), PE2 (Ye et al., 2024),\n                                                                                                                Adv-ICL (Long et  al., 2024),\n                                                                                               PROMPST (Chen et al., 2024b),\n                                         Single-Parent (§6.2)                                           LLM-Feedback           PhaseEvo (Cui et al., 2024b),\n                                                                                                 PACE (Dong  et  al.,  2024),\nWhich Operators are Used (§6)                                                                                                 UniPrompt (Juneja et al., 2024),\n                                                                                   AMPO (Yang et al., 2024), SoS\n                                                                                                                                       (Sinha et al., 2024), SCULPT\n                                                                                                                      (Kumar et al., 2024), ERM (Yan\n                                                                                                                                                                     et al., 2024), MAPS (Gao et al.,\n                                                                        Feedback                                              2025)\n\n\n                                                                                               PROMPST (Chen et al., 2024b),\n                                                                                              Human-Feedback\n                                                                                           APOHF (Lin et al., 2024a)\n\n\n                                                                                                 Wen et al. (2023), Shen et al.\n                                                                                                                                                   (2023), RPO (Zhou et al., 2024),\n                                                                                                         Gradient-Feedback         Zhao et al. (2024), ZOPO (Hu\n                                                                                                                                                                   et al., 2024), Li et al. (2024a),\n                                                                                        GrIPS (Prasad  et  al., 2023),   DPO (Wang et al., 2024b)\n                                                                                   Plum (Pan et al., 2024), RPO\n                                                                                          (Zhou et al., 2024), UniPrompt\n                                                               Add/ Subtract/ Replace         (Juneja  et  al.,  2024), DPO\n                                                                                       (Wang et al., 2024b), UniPrompt\n                                                                                                           (Juneja et  al., 2024), SPRIG\n                                                                                             (Zhang et al., 2024d)\n\n\n                                                                        PREFER (Zhang et al., 2024a),\n                                                                   OPRO (Yang  et  al., 2023a),\n                                                                                                 PromptBreeder (Fernando et al.,\n                                                                                                      2024), SPELL  (Li and Wu,\n                                                     EDA                 2023), PhaseEvo (Cui  et  al.,\n                                                                                                        2024b), GPO (Tang et al., 2025),\n                                                                          LCP (Li et al., 2024b), IPO (Du\n                                                                                                                          et al., 2024), ERM (Yan et al.,\n                                                                                                2024)\n\n\n                                                                                       EvoPrompt (Guo et al., 2024),\n                                                                                                 PromptBreeder (Fernando et al.,\n                                         Multi-Parent (§6.3)                                             2024), InstOptima (Yang and\n                                                                                                                     Li, 2023), EMO (Baumann and\n                                                                             Crossover              Kramer, 2024), EOT (Jin et al.,\n                                                                                                    2024b), PhaseEvo (Cui et al.,\n                                                                                                        2024b), SoS (Sinha et al., 2024),\n                                                             MOPO (Menchaca  Resendiz\n                                                                                             and Klinger, 2025)\n\n\n                                                                                Difference             EvoPrompt (Guo et al., 2024)\n\n                        Figure 5: \"Which operators are used\" Categorization\n\n\n\n\n\n                                      17\n\nEPO (Shi et al., 2024), EASE\n                                     Bandit Algorithm (§7.1)      (Wu et al., 2024), APOHF (Lin\n                                                                                   et al., 2024a)\n\n\n                                                           GrIPS (Prasad  et  al., 2023),\n                                                             ProTeGi (Pryzant et al., 2023),\n                              Beam Search (§7.2)       AELP  (Hsieh  et  al.,  2024),\n                                               SCULPT (Kumar et al., 2024),\n                                         ERM (Yan et al., 2024)\n\n\n                                                    INSTINCT (Lin et al., 2024b),\n                                        Heuristic Sampling (§7.3)     PROMPST (Chen et al., 2024b),\n                                              MoP (Wang et al., 2024a)\n\n                                                              Monte Carlo Search       APE (Zhou et al., 2023)         GPS (Xu et  al., 2022), Shen\n                                                                                                                                                                   et al. (2023), EvoPrompt (Guo\n                               Monte Carlo Search (§7.4)                                                                                          et  al., 2024), PromptBreeder\n                                                                                          PromptAgent  (Wang  et   al.,     (Fernando et al., 2024), SPELL                                                            Monte Carlo Tree Search\n                                                                                                 2024c)                             (Li and Wu, 2023), INSTINCT\n                                                                                                                                        (Lin et al., 2024b), InstOptima\n                                                                                                           Genetic Algorithm          (Yang and Li, 2023), EMO (Bau-\n                                                                                                          mann and Kramer, 2024), EOT\n                                                                                                                                                                 (Jin et al., 2024b), PACE (Dong\n                                                                                                                                                                    et al., 2024), DPO (Wang et al.,\n                                                                         Evolutionary Algorithm                                          2024b), SPRIG (Zhang et al.,\n                                                                                                                                      2024d), MOPO (Menchaca Re-\n                                                                                                                                             sendiz and Klinger, 2025)\nWhich  Iterative Algorithm  is\nUsed (§7)                                                                                                               Differential Evolution        EvoPrompt (Guo et al., 2024)\n\n\n                                                                                                           InstructZero  (Chen   et   al.,\n                                                                                                      2024a), AutoHint (Sun et al.,\n                                                                                                    2023b),  PREDER  (Zhang\n                                                                                                                          et  al., 2024a), OPRO (Yang\n                                                                                                                          et al., 2023a), PE2 (Ye et al.,\n                                                                                                      2024), Plum (Pan et al., 2024),\n                                    Metaheuristic Algorithm (§7.5)                          RPO (Zhou et al., 2024), GPO\n                                                                                               (Tang et al., 2025), Zhao et al.\n                                                                      General Metaheuristic          (2024), Uniprompt (Juneja et al.,\n                                                                                                      2024), MIPRO (Opsahl-Ong\n                                                                                                                           et al., 2024), APEER (Jin et al.,\n                                                                                                      2024a), DLN (Sordoni et  al.,\n                                                                                                      2023), LCP (Li et al., 2024b),\n                                                              AMPO (Yang et al., 2024), SoS\n                                                                                                     (Sinha et al., 2024), IPO (Du\n                                                                                                                            et al., 2024), MAPS (Gao et al.,\n                                                                                                2025)\n\n\n                                                                     Phased Algorithm          PhaseEvo (Cui et al., 2024b)\n\n\n                                                 Wen et al. (2023), Shen et al.\n                                                                      (2023), BPO (Cheng  et  al.,\n                                                                    2024), Sabbatella et al. (2023),\n                                                        Adv-ICL (Long et  al., 2024),\n                                            Iterative Refinement (§7.6)\n                                                      FIPO (Lu et al., 2025), ZOPO\n                                                         (Hu  et  al., 2024),  Li  et  al.\n                                                                      (2024a), DPO (Wang  et  al.,\n                                                               2024b)\n\n                     Figure 6: \"Which iterative algorithm is used\" Categorization\n\n\n\n\n\n                                      18\n\nB  Datasets and Tools\n\n\n                      Dataset Name                             Dataset Category\n\n               BBH (Aarohi and bench authors, 2023)      NLP Benchmark\n                          Instruction Induction (Honovich et al., 2022)  NLP Benchmark\n               GSM8K (Cobbe et al., 2021)                 Mathematical Reasoning\n                      Ethos (Mollas et al., 2022)                   Bias and Ethics Evaluation\n                     SST-2 (Socher et al., 2013)                   Sentiment Analysis\n                   HotpotQA (Yang et al., 2018)                Question Answering\n                               Iris (Li et al., 2024c)                             Scientific Classification\n               SVAMP (Patel et al., 2021)                  Mathematical Reasoning\n                      Subj (Pang and Lee, 2004)                      Subjectivity Detection\n               CR (Hu and Liu, 2004)                      Sentiment Analysis\n             MR (Pang and Lee, 2005)                    Sentiment Analysis\n                TREC (Voorhees and Tice, 2000)             Question Answering\n                         Liar (Wang, 2017)                           Misinformation Detection\n\n                                    Table 4: Commonly used datasets\n\n\n\n\n\n                                         19",
"headers": [
"arXiv:2502.18746v2  [cs.CL]  12 Jul 2025",
"A Survey of Automatic Prompt Optimization with Instruction-focused",
"Heuristic-based Search Algorithm",
"Wendi Cui",
", Zhuohang Li",
", Hao Sun",
", Damien Lopez",
", Kamalika Das",
"Bradley Malin",
", Sricharan Kumar",
", Jiaxin Zhang",
"Intuit",
"Intuit AI Research",
"Vanderbilt University",
"University of Cambridge",
"Vanderbilt University Medical Center",
"Abstract",
"1",
"Introduction",
"3",
"Where does Optimization Happen?",
"2",
"Preliminary",
"4",
"What is Optimized?",
"5",
"What Criteria to Optimize",
"6",
"Which Operators are Used",
"7",
"Which Iterative Algorithm is Used",
"8",
"Common Datasets Used",
"9",
"Common Tools",
"10",
"Open Challenges",
"11",
"Conclusion",
"12",
"Limitations",
"Acknowledgments",
"References",
"A",
"Methods Categorization based on Taxonomy",
"B",
"Datasets and Tools"
],
"tables": [
"|Tool|Optimization Space|Key Features|Open Source|\n|---|---|---|---|",
"|PromptPerfect|Discrete|Web-based, optimized for user queries|No|\n|---|---|---|---|",
"|PromptIM|Discrete|Iterative refine with human in the loop|Yes|\n|---|---|---|---|",
"|Dspy (Khattab et al., 2024)|Discrete|Task decomposition and example bootstrap|Yes|\n|---|---|---|---|",
"|OpenPrompt (Ding et al., 2021)|Soft/Discrete|Predefined templates for prompt learning|Yes|\n|---|---|---|---|",
"|Vertex AI (Wan et al., 2024)|Discrete|Google Cloud-based optimization|No|\n|---|---|---|---|",
"|PromptBench (Zhu et al., 2023)|Discrete|Test prompt robustness|Yes|\n|---|---|---|---|",
"|AWS Bedrock|Discrete|Playground with evaluation and A/B testing|No|\n|---|---|---|---|",
"|Anthropic Claude|Discrete|Interactive editor with live feedback|No|\n|---|---|---|---|",
"|Difference|Col2|EvoPrompt (Guo et al., 2024)|\n|---|---|---|\n|Difference|||",
"|Dataset Name|Dataset Category|\n|---|---|",
"|BBH (Aarohi and bench authors, 2023)<br>Instruction Induction (Honovich et al., 2022)<br>GSM8K (Cobbe et al., 2021)<br>Ethos (Mollas et al., 2022)<br>SST-2 (Socher et al., 2013)<br>HotpotQA (Yang et al., 2018)<br>Iris (Li et al., 2024c)<br>SVAMP (Patel et al., 2021)<br>Subj (Pang and Lee, 2004)<br>CR (Hu and Liu, 2004)<br>MR (Pang and Lee, 2005)<br>TREC (Voorhees and Tice, 2000)<br>Liar (Wang, 2017)|NLP Benchmark<br>NLP Benchmark<br>Mathematical Reasoning<br>Bias and Ethics Evaluation<br>Sentiment Analysis<br>Question Answering<br>Scientific Classification<br>Mathematical Reasoning<br>Subjectivity Detection<br>Sentiment Analysis<br>Sentiment Analysis<br>Question Answering<br>Misinformation Detection|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2502.18746v2.pdf"
}