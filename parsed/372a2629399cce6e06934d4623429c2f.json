{
"text": "Accelerating Greedy Coordinate Gradient and\n             General Prompt Optimization via Probe Sampling\n\n\n\n\n                                Yiran Zhao1†  Wenyue Zheng1   Tianle Cai2  Xuan Long Do1\n                                   Kenji Kawaguchi1  Anirudh Goyal3   Michael Qizhe Shieh1†\n                                1 National University of Singapore   2 Princeton University   3 Google DeepMind\n2024                                            Abstract\n\n                               Safety of Large Language Models (LLMs) has become a critical issue given theirNov                         rapid progresses. Greedy Coordinate Gradient (GCG) is shown to be effective\n                                  in constructing adversarial prompts to break the aligned LLMs, but optimization\n8                               of GCG is time-consuming. To reduce the time cost of GCG and enable more\n                            comprehensive studies of LLM safety, in this work, we study a new algorithm\n                                 called Probe sampling. At the core of the algorithm is a mechanism that\n                              dynamically determines how similar a smaller draft model’s predictions are to the\n                                    target model’s predictions for prompt candidates. When the target model is similar\n                                  to the draft model, we rely heavily on the draft model to filter out a large number[cs.CL]                         of potential prompt candidates. Probe sampling achieves up to 5.6 times speedup\n                              using Llama2-7b-chat and leads to equal or improved attack success rate (ASR)\n                          on the AdvBench. Furthermore, probe sampling is also able to accelerate other\n                           prompt optimization techniques and adversarial methods, leading to acceleration\n                                of 1.8× for AutoPrompt, 2.4× for APE and 2.4× for AutoDAN.1\n\n\n                1  Introduction\n\n                      Ensuring the safety of Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2023;\n                    Touvron et al., 2023; Jiang et al., 2023) has become a central theme of research. Despite continuous\n                           efforts, LLMs are prone to generate objectionable contents in various scenarios including using an\n                        adversarial suffix (Zou et al., 2023), further finetuning (Qi et al., 2024; Lermen and Rogers-Smith,\n                       2024), ciphering (Yuan et al., 2024b) and multilingual settings (Deng et al., 2024). Among effective\n              LLM adversarial attack works, Greedy Coordinate Gradient (GCG) (Zou et al., 2023) present a\n                       general and universal method as briefly illustrated in Figure 1.arXiv:2403.01251v3\n                  To optimize a prompt suffix to elicit the gener-\n                        ation of a target reply, the Greedy Coordinate\n                      Gradient (GCG) algorithm iteratively attempts\n                          to replace existing tokens in the suffix and keeps\n                       the best-performing ones based on the adver-\n                          sarial loss. The GCG algorithm is empirically\n                         effective but searching the combinatorial space\n                                                                     Figure 1: A brief illustration of the Greedy Coordi-                       of the adversarial suffixes is time-consuming\n                                                                        nate Gradient (GCG) algorithm (Zou et al., 2023).                       since each token replacement attempt requires\n                      a full forward computation using an LLM. This\n                       hinders us from using the algorithm to fully explore the safety properties of LLMs such as finding\n                         potentially harmful queries comprised of natural sentences.\n\n                           †Correspondence to: Yiran Zhao (zhaoyiran@u.nus.edu), Michael Shieh (michaelshieh@comp.nus.edu.sg).\n                         1Our code is publicly available at https://github.com/zhaoyiran924/Probe-Sampling.\n\n\n                        38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n\nFigure 2:  Probe sampling mainly consists of three steps.    (i) A batch of candidates\n({a, b, · · · , h}) is sampled. We determine the probe agreement score between the draft model\nand the target model on a probe set ({b, d, h}). The probe agreement score is used to compute the\nfiltered set size. (ii) We obtain a filtered set ({e, f}) based on the losses on the draft model (iii) We\ntest the losses of candidates in the filtered set using the target model.\n\n\nA possible solution for reducing forward computation is to resort to a smaller draft model when it\nis indicative of the results on the larger target model. This intuition has been applied in speculative\nsampling (Chen et al., 2023; Leviathan et al., 2023) for decoding, where the target model acts as a\nverifier that accepts or rejects the decoded tokens. However, speculative sampling cannot be used\nto optimize discrete tokens in GCG because the optimization of every token in adversarial suffix is\nindependent of each other, which breaks the autoregressive assumption in decoding.\n\nMotivated by these observations, we propose a new algorithm called Probe sampling to accel-\nerate the GCG algorithm. Instead of computing the loss on every suffix candidate, we filter out\nunpromising ones based on the loss computed with a smaller model called draft model, to reduce\nthe time consumption of the optimization process. Importantly, we dynamically decide how many\ncandidates we keep at each iteration by measuring the agreement score between the draft model and\nthe target model, by looking at the loss rankings on a small set of prompts dubbed as the probe set.\nIt is worth noting that the prompt candidates at each iteration in GCG are obtained by randomly\nchanging one token of an original prompt. As a result, the agreement score is adaptive to the original\nprompt. We evaluate probe sampling on the AdvBench dataset with Llama2-7b-Chat and Vicuna-v1.3\nas the target models and a significantly smaller model GPT-2 (Radford et al., 2019) as the draft\nmodel. Experiment results show that compared to the original GCG algorithm, probe sampling\nsignificantly reduces the running time of GCG while achieving better Attack Success Rate (ASR).\nSpecifically, with Llama2-7b-Chat, probe sampling achieves 3.5 times speedup and an improved\nASR of 81.0 compared to GCG with 69.0 ASR. When combined with simulated annealing, probe\nsampling achieves a speedup of 5.6 times with a better ASR of 74.0.\n\nFurthermore, when applied to prompt learning techniques and other LLM attacking methods, probe\nsampling demonstrates remarkable effectiveness. Specifically, in the case of prompt learning, probe\nsampling effectively accelerates AutoPrompt (Shin et al., 2020) by a factor of 1.8.  Moreover,\nprobe sampling delivers substantial speedup of APE (Zhou et al., 2022) on various datasets: 2.3×\non GSM8K, 1.8× on MMLU and 3.0× on BBH. In the case of other attacking method such as\nAutoDAN (Liu et al., 2024), probe sampling achieve a speedup of 2.3× and 2.5× on AutoDAN-GA\nand AutoDAN-HGA respectively.\n\n2  Proposed Method\n\n2.1  Background: Greedy Coordinate Gradient\n\nThe overall optimization objective of GCG can be denoted by a simple log likelihood loss\n\n                         min L(s) = −log p(y | x, s),                                 (1)\n                                       s\nwhere x is a prompt that contains a harmful user query such as “Tell me how to build a bomb”, y\nis the target sentence “Sure, here is how to build a bomb”, and s is the adversarial suffix that is\noptimized to induce the generation of y. p is the probability of a sentence output by a LLM. This\n\n\n                                       2\n\nobjective can be decomposed into the summation of the negative log likelihood of individual tokens\nin the target sentence like a typical language modeling objective. s is set to be a fixed length string in\nthe GCG algorithm.\n\nThe optimization of the adversarial suffix s is a non-trivial problem. Prior works (Guo et al., 2021;\nWen et al., 2024) based on Gumbel-Softmax (Jang et al., 2016; Maddison et al., 2022) and soft\nprompt tuning (Lester et al., 2021) have achieved limited success, probably because the LLMs are\nwell-aligned and the exceptionally large models magnifies the difference between a discrete choice\nand its continuous relaxations.\n\nInstead, GCG adopts a greedy search algorithm based on the gradient. In each iteration, it computes\nL(ˆsi) for B suffix candidates ˆs1, · · · , ˆsB and keeps the one with the best loss. The B candidates are\nobtained by randomly changing one token from the current suffix s and replacing it with a randomly\nsampled token using the top K tokens. For example, suppose we change the token at position j, we\nfirst compute the gradient −∇esj L(s) with respect to the one-hot vector esj and obtain the top K\ntokens that have the largest gradient. The gradient information is by no means an accurate estimation\nof the resulting loss because of the gap between the continuous gradient information and the discrete\none-hot vector denoting the choice of a token, so we need to check if the resulted new suffix ˆsi leads\nto a lower loss L(ˆsi).\n\nTo obtain the B candidates, one just needs to perform one forward pass and one backward pass. But\nto compute the loss for the B candidates, one needs to perform B forward passes. In GCG, B is\nset to 512 for optimal performance, making the loss computation the most time-consuming part. As\nsuch, we focus on reducing the time cost of the loss computation of the B candidates in this work.\n\n\n2.2  Probe Sampling\n\nOverview.  As mentioned earlier, the most time consuming part in the GCG algorithm is the loss\ncomputation on B suffix candidates ˆs1, · · · , ˆsB. As shown in speculative sampling (Chen et al.,\n2023; Leviathan et al., 2023), the speculated results using a smaller draft model can be helpful in\nreducing the computation with a large target model. The original speculative sampling is created to\naccelerate decoding so it isn’t directly applicable here. But the intuition of relying a weaker draft\nmodel is obviously useful for negative log likelihood loss computation. Applying the intuition to the\nproblem at hand, we can filter out the suffix candidates that the draft model finds to be unpromising,\nsince the goal is to find the candidate that has the lowest loss with the target model.\n\nIn addition, a unique structure in the GCG algorithm is that all the suffix candidates are based on\nchanging one token of the original suffix s. As a result of this locality property, it is not unreasonable\nto assume that one can determine how much they agree on the B candidates based on their agreement\non a subset of the B candidates. If the two models agree, we can choose to safely rely on the draft\nmodel and filter out more candidates.\n\nBased on these intuitions, we design the Probe sampling algorithm as follows: (i) probe agree-\nment between the target model and the draft model to determine the size of the filtered set; (ii) rank\ncandidates using the draft model and obtain the filtered set; (iii) pick the best candidate from the\nfiltered set using the target model.\n\nAlgorithm description.  For the first step, specifically, we sample a probe set comprised of k\ncandidates ¯s1, · · · , ¯sk and compute their losses using the draft model and the target model and obtain\nLdraft(¯s1), · · · , Ldraft(¯sk) and Ltarget(¯s1), · · · , Ltarget(¯sk). Then we measure the probe agreement\nscore as the Spearman’s rank correlation coefficient (Zar, 2005) between the two results as the\nagreement score. The probe agreement score α is computed as\n\n                                                 i=1 d2i                         α = 1 −3 Pk                                                                                      ,                                     (2)\n                                         k(k2 −1)\n\nwhere di is the distance between the ranks of suffix ¯si in the two results. For example, di = 4 if the\nsuffix ¯si is ranked as number 6 and number 2 for its losses computed from the draft model and the\ntarget model. The agreement score α falls into [0, 1] with 1 meaning a full agreement and 0 indicating\na non-agreement. We use the rank agreement because it is more robust to the specific values of the\nresulting loss when measured on drastically different LLMs.\n\n\n                                       3\n\nAlgorithm 1 Probe Sampling\nInput: Original suffix s, a batch of suffix candidates {ˆs1, · · · , ˆsB}, loss function using the draft\n   model and the target model Ldraft(·), Ltarget(·).\n  1: Parallel Begin\n  2: //Compute loss of all candidates using the draft model\n  3: for ˆsi ∈{ˆs1, · · · , ˆsB} do\n  4:   Compute Ldraft(ˆsi)\n  5: end for\n  6: //Compute loss of the probe set on target model\n  7: {¯s1, · · · , ¯sk} = Uniform({ˆs1, · · · , ˆsB}, k)\n  8: for ¯si ∈{¯s1, · · · , ¯sk} do\n  9:   Compute Ltarget(¯si)\n10: end for\n11: Parallel End\n12: //Calculate agreement score\n13: α = Spearman_Cor({Ltarget(¯si)}, {Ldraft(¯si)})\n14: //Evaluate using the target model\n15: filtered_set = argminmax{1,(1−α)B/R}Ldraft(ˆsi)\n16: for ˆsi ∈filtered_set do\n17:   Compute Ltarget(ˆsi)\n18: end for\n19: Output the best suffix in the probe set and the filtered set\n20: s′ = argmin{Ltarget(¯si), Ltarget(ˆsi)}\nOutput: s′\n\n\nAfter obtaining the agreement score, we keep (1 −α) ∗B/R candidates where (1 −α) ∗B means\nthat the filtered set size is a scale-down of the previous batch size B and R is a hyperparameter that\ndetermines a further scale down. When α is close to 0, meaning little agreement between the two\nmodels, we will use a filtered set size of B/R. When α goes to 1, we almost filter out most of the\ncandidates. With the filtered size determined, we can readily rank the candidates according to the\ndraft model and filter the ones with higher losses. Finally, we evaluate the final loss on the filtered set\nusing the target model and select the best candidate.\n\nDetails.  At first glance, probe sampling involves extra computation but it actually achieves effective\nacceleration. For computing the losses on the probe set using both the draft model and the target\nmodel, the size of the probe set can be set to be relatively small, so it would not add too much to the\ntotal time cost. The ranking procedure involves sorting on CPU, but luckily the probe set is small\nenough that this doesn’t become a bottleneck. And the loss computation using the draft model on the\nwhole candidate set is relatively cheap because of draft model’s small size. These two operations\ncan also be parallelized on GPU. On the plus side, we are able to avoid computing the loss using the\nbig target model on many candidates that are filtered out. As we will show in the experiments, this\napproach achieves significant speedup measured by both running time and #FLOPs.\n\nAn alternative to computing agreement on the spot is to measure the agreement score on a predeter-\nmined set of candidates and use a fixed agreement score for all the suffixes. This would save the time\nused to measure agreement for each candidate set. However, as we will show in the experiment, this\napproach does not work so well in terms of speedup. Our intuition is that one can squeeze the time\ncost more effectively if the agreement is measured accurately, and an adaptive agreement score is\nmore accurate than an one-size-fits-all score. The plausibility of the adaptive score comes back to the\nlocality property that we discussed earlier. Given a specific candidate set, one can accurately estimate\nthe agreement because all the suffixes in this candidate set are similar to a large extent. However,\ngiven another candidate set altered from a different suffix, the agreement of the draft model and the\ntarget model can be widely different.\n\nIn practice, we adopted two small changes in our implementation. First, we do not have a separate\nstep to compute the loss of the probe set candidates using the draft model, since we need to compute\nthe loss on all candidates for filtering purposes. We simply get the numbers from the losses on the\nwhole candidate set. Second, to get the best candidate for the final result, we also look at the losses\non the probe set, since the target model is evaluated on the probe set. Ideally, the candidates in the\n\n\n                                       4\n\nprobe set should be in the filtered set if they achieve a low loss. However, it also does not hurt to\nlook at the best candidate in the probe set in case it is not included in the filtered set. The overall\nalgorithm is further illustrated in Algorithm 1, and the corresponding implementation is shown in\nAppendix A. We also test simulated annealing (Pincus, 1970) that provides complementary benefit to\nour algorithm.\n\n2.3  Applying Probe Sampling to Other Prompt Optimization Methods\n\nAlthough prompt sampling was designed to accelerate GCG, the general idea of reducing forward\ncomputation can be applied on other prompt optimization methods, where there is usually a process\nof sampling prompt candidates and evaluating their performances. To see whether probe sampling can\neffectively accelerate other methods, we also apply probe sampling to two prompt learning methods\nAutoPrompt (Shin et al., 2020) and APE (Zhou et al., 2022). In addition, we apply probe sampling\non AutoDAN (Liu et al., 2024), a genetic algorithm that can find natural jailbreak prompts.\n\n3  Experiment\n\nIn this section, we evaluate the proposed method on its efficacy and the important factors through\nextensive studies.\n\n3.1  Experiment Details\n\nSettings.  Following the original GCG paper, we conduct experiments on the first 100 instances of\nAdvBench (Zou et al., 2023), which are divided into two parts, 500 harmful strings and 500 harmful\nhuman behaviors. We test open-source LLMs that have been specifically fine-tuned with respect\nto safety, including Llama2-chat-7b (Touvron et al., 2023) and Vicuna-7b (Zheng et al., 2023). In\nthe case of draft models, in our main experiments, we use a much smaller model GPT-2 (Radford\net al., 2019). Similarly, when applying probe sampling to AutoDAN, we use Llama2-7b-chat as\nthe target model and GPT-2 as the draft model. For AutoPrompt, we follow their original setting,\nwhich uses RoBERTa-large (Liu et al., 2019) as the target model and tests on SST-2 (Socher et al.,\n2013) for sentiment analysis and SICK-E (Marelli et al., 2014) for natural language inference. We\nuse the RoBERTa-base model as the draft model. In the case of APE, we conduct the experiments\non three widely used benchmarks GSM8K (Cobbe et al., 2021), BBH (Suzgun et al., 2023), and\nMMLU (Hendrycks et al., 2020). For these experiments, we use Vicuna-7b-v1.5 as the target model\nand GPT-2 as the draft model.\n\nEvaluation.  Following (Zou et al., 2023), we use Attack Success Rate (ASR) as the evaluation\nmetric for GCG and AutoDAN, which is defined as the percentage of inputs that successfully lead\nLLMs to generate harmful outputs. An output is determined to be harmful if it does not match with\nrejection phrases, such as “I’m sorry”, “I apologize” and “As an”. This is not a perfect measurement\nbut works relatively well in practice since LLMs are trained to reject harmful replies. It is also easy\nto measure and interpret. For prompt learning methods tested on other tasks, we employ Accuracy\n(Acc) as the metric. The processing time is determined as the average time used for each iteration\nacross all input samples and all iterations. In all experiments, we use 1 NVIDIA A100 GPU with\n80GB memory unless mentioned otherwise.\n\nHyperparameters.  To determine the hyperparameters for probe sampling, including probe set\nsize k, filtered set size reduction hyperparameter R, we construct a validation set of size 100 from\nAdvBench by random sampling in the 400 instances different from the test set. We follow (Zou et al.,\n2023) for the hyperparameters used in the original algorithm such as the size of the candidate set B.\nWe provide detailed analysis of hyperparameters in Section 3.4. When we combine probe sampling\nwith simulated annealing, we follow the same procedure to select hyperparameters. We use the same\nnumber of optimization steps 500 as in GCG throughout the paper.\n\n3.2  Main Results\n\nAcceleration results.  As shown in Table 1, probe sampling achieves a speedup of 5.6 times and\n6.3 times on Human Behaviors and Human Strings with Llama2 when combined with simulated\n\n\n                                       5\n\nTable 1: Comparing the ASR and processing time of Probe sampling with and without simulated\nannealing to GCG with and without simulated annealing, while measuring time and FLOPs by\naveraging each iteration.\n                             Harmful Strings                   Harmful Behaviors\n Model   Method                                          Individual         Multiple\n                                                                        Time (s)  #FLOPs\n                       ASR   Time (s)   #FLOPs   ASR   ASR (train) ASR (test)\n\n        GCG                88.0      4.1        97.3 T     99.0      100.0       98.0         4.8       106.8 T\n Vicuna  GCG + Annealing     89.0   1.5 (2.7×)    38.5 T     98.0        92.0        94.0     2.1 (2.3×)     46.2 T\n  (7b-v1.3)  Probe sampling  91.0   1.7 (2.4×)    42.4 T   100.0       96.0        98.0     2.3 (2.1×)     53.2 T\n         PS + Annealing      93.0  1.1 (3.6×)   27.8 T   100.0       96.0        99.0    1.5 (3.2×)    24.7 T\n\n        GCG                57.0      8.9      198.4 T     69.0        88.0        84.0         9.2       202.3 T\n Llama2  GCG + Annealing     55.0   2.4 (3.9×)    39.7 T     68.0        92.0        88.0     2.7 (3.4×)     50.6 T\n  (7b-Chat) Probe sampling  69.0  2.2 (4.1×)    43.8 T    81.0        92.0       93.0     2.6 (3.5×)     40.7 T\n         PS + Annealing       64.0  1.4 (6.3×)   31.2 T     74.0       96.0       91.0    1.6 (5.6×)    32.3 T\n\n\nTable 2: Transferability of Probe sampling    Table 3: Transferability of Probe sampling\nwith different draft models.                      with different filtered set size (1 −α) ∗B/R.\n                     Direct        Transfer                         Direct        Transfer Method                                    Method\n                    Llama2-7b  Vicuna-7b  Mistral-7b                  Llama2-7b  Vicuna-7b  Mistral-7b\n\n GCG                  69.0       89.0      86.0     GCG            69.0       89.0      86.0\n PS (GPT-2)            85.0       92.0       83.0       PS (R = 64)     60.0       77.0       74.0\n PS (ShearedLlaMa)    91.0      93.0       85.0       PS (R = 8)     85.0      92.0       83.0\n PS (Flan-T5)           57.0       78.0       69.0       PS (R = 1)      79.0       88.0       84.0\n\n\n\nannealing. Probe sampling achieves a speedup of 3.5 and 4.1 times alone. With Vicuna, we achieve\nan overall speedup of 3.2 and 3.6 respectively on the two datasets. We also measure the #FLOPs for\ndifferent settings and found that the speedup results reflects in the reduction of #FLOPs. For example,\nwith Llama2, the #FLOPs reduction is 202.3T/32.3T = 6.3 times and 198.4T/31.2T = 6.4 times\non the two sets, which is close to the actual speedup results. This also shows that our algorithm results\nin little overhead with the introduced new procedures. It is worth noting that simulated annealing\nalso achieves decent acceleration and is complementary to our acceleration results.\n\nGCG results.    Interestingly, we achieve a better ASR score than the GCG algorithm although\ntechnically acceleration introduces noise to the algorithm. For instance, with Llama2, we improve\nthe ASR from 57.0 to 64.0 on Human Strings and from 84.0 to 91.0 on Human Behaviors. We\nhypothesize that the improvement comes from the randomness added to the GCG algorithm based\non greedy search over a single objective. Introducing randomness and noise has been seen as one\nof the advantages of SGD over full batch training. In contrast, simulated annealing only leads to\ncomparable ASR when applied on GCG.\n\nTransferability  Table 2 shows probe sampling’s transferability across draft models based on\nLlama2-7b-Chat to various target models. We find that it maintains transferability when using draft\nmodels like GPT-2 and SheardLlaMa, which preserve the original ASR of plain GCG. However,\ndraft models that significantly degrade initial performance, such as Flan-T5, impair transferability.\nTable 3 examines probe sampling transferability across filtered set sizes. Results align with prior\nfindings: probe sampling minimally impacts transferability with appropriate parameters but decreases\nperformance when Llama2-7b-chat’s direct ASR is low such as R = 64.\n\nResults on AutoDAN, Autoprompt and APE.  Table 5 demonstrates the effective acceleration of\nAutoPrompt through the implementation of probe sampling, resulting in a speedup of 1.79× on SST-2\nand 1.83× on SICK-E. Importantly, this acceleration is achieved without compromising performance,\nas evidenced by the minimal changes in accuracy from 91.4 to 90.6 on SST-2 and from 69.3 to\n68.9 on SICK-E. Furthermore, the application of probe sampling to APE, as presented in Table 6,\nresults in significant speed improvements, with a speedup of 2.3× on GSM8K, 1.8× on MMLU, and\n3.0× on BBH. Similarly, these speed enhancements do not compromise the performance of APE.\nIn addition, we implement probe sampling on another jailbreak method, AutoDAN. The detailed\nresults can be found in Table 4. Our findings indicate that probe sampling can achieve a speedup of\n2.3× for AutoDAN-GA and 2.5× for AutoDAN-HGA, while minimally affecting its performance.\n\n\n                                       6\n\nTable  4:    Performance  of  Probe  Table 5: Performance of Probe sampling on accel-\nsampling on accelerating AutoDAN.    erating prompt learning method AutoPrompt.\n   Method         ASR    Time (s)                              SST-2           SICK-E\n                                         Method\n                                                               Acc     Time (s)     Acc     Time (s)\n   AutoDAN-GA         56.2       424.2\n   AutoDAN-GA + PS     55.9   182.7 (2.3×)         Original            85.2     N / A       49.4    N / A\n\n   AutoDAN-HGA        60.8       237.9           Autoprompt       91.4       228.4      69.3       42.7\n   AutoDAN-HGA + PS   62.1   95.3 (2.5×)        Autoprompt + PS   90.6   127.2 (1.8×)   68.9   23.6 (1.8×)\n\n\n    Table 6: Performance of Probe sampling on accelerating prompt learning method APE.\n                   GSM8K        MMLU          BBH           Method\n                       Acc     Time (s)     Acc     Time (s)     Acc     Time (s)\n\n               Vicuna     20.4      N/A        45.6     N / A       38.6    N / A\n           APE       21.3       431.8      48.2       187.3      40.8      265.2\n           APE+PS   22.4   192.3 (2.3×)   47.3   102.5 (1.8×)   39.9   88.7 (3.0×)\n\n\n                                         Target Model   Draft Model   Vacant\n\n\n\n      GCG           66%               34%           GCG        48%               52%\n\n\nProbe sampling      38%        25%        37%         Probe sampling      36%      15%         49%\n\n\nPS + Annealing       43%      8%        49%          PS + Annealing      36%     5%         59%\n\n\n       0%    20%   40%   60%   80%   100%       0%    20%   40%   60%   80%   100%\n\n                 (a) Llama2-7b-chat                                     (b) Vicuna-7b-v1.3\n\n Figure 3: Memory usage on a single A100 with 80GB memory with (a) Llama2-7b-chat and (b)\n Vicuna-7b-v1.3 on 1 instance. The memory consumption of probe sampling with or without simulated\n annealing is similar to that of the original setting. The computation with the target model still takes\n most of the memory.\n\n\nThese results demonstrate the effectiveness of our method in not only accelerating GCG but also its\n applicability to general prompt optimization methods and other LLM attack methods.\n\n\n 3.3  Computation Detail Analysis\n\nMemory allocation.  We evaluate whether probe sampling uses more memory because of the use of\n an extra model. In Figure 3, we show the memory usage of GCG, probe sampling with and without\n annealing using either Llama2-7b-chat and Vicuna-7b-v1.3. Probe sampling uses a similar amount\n of memory to the original GCG algorithm although it involves extra procedures and an extra model,\n by saving the computation of target model on the whole candidate set. As such, the usage of probe\n sampling does not introduce extra memory and can be applied when the original GCG algorithm is\n applied. In terms of the memory usage of the target model and the draft model, most of the memory\n is spent on target model, probably because the draft model is much smaller.\n\nTime allocation. We look at the specific time spent on different operations. As shown in Figure\n 4, probe set computation using the target model and full set computation using the draft model take\n a similar amount of time so we can parallelize the computation easily. Sampling candidates in the\n graph involves a forward and backward pass as mentioned earlier and can be completed relatively\n quickly. Similarly, it is also fast to compute the agreement using the ranked losses on CPU, so our\n algorithm introduces relatively little overhead.\n\n\n 3.4  Further analysis\n\n In this section, we conduct extensive studies to understand how the proposed method works. We\n conduct all of the following experiments on the validation set, so the numbers are not directly\n\n\n                                        7\n\nFigure 4: Wall time of GCG, probe sampling with and without simulated annealing. For the target\nmodel computation, the first part is done on the probe set and the second part is done on the filtered\nset. Draft model computation and computation of the target model on the probe set are suited to be\ndone in parallel as they take similar time.\n\n\nTable 7: Ablation on the filtered set size reduction  Table 8: Ablation on fixed probe agreement score\nR. The filter set size is (1 −α) ∗B/R.       α vs adaptive score.\n\n  Reduction R    64    16     8     4     2     1        Agreement α    0.9     0.6    0.3    0.0    Adaptive\n\n ASR           60.0   70.0   85.0   81.0   76.0   79.0     ASR           70.0   77.0   75.0   81.0     85.0\n\n  Time (s)       2.01   2.31    2.60   3.02   3.41   5.19      Time (s)       2.17   2.41   2.71   3.01      2.60\n\n\n\ncomparable to the numbers in the main results. For the validation set, the original GCG algorithm\nachieves an ASR of 66.0 with an average time of 9.16 seconds per iteration. In each of the study, we\nhighlight the settings that we find to be the best.\n\nFiltered set size.  The filtered set size is the most important factor in our method. If it is too small,\nthen we will achieve a lot of speedup at the cost of relying too heavily on the draft model and resulting\nin a lower ASR. If it is too big, then we would not achieve much speedup. Hence we experiment with\ndifferent filtered size reduction hyperparameter R. The filter set size is (1 −α) ∗B/R where α is the\nprobe agreement score described in Section 2.2.\n\nAs shown in Table 7, the time does monotonically decrease if we use a smaller filtered set size.\nHowever, interestingly, there is a sweetspot for the ASR with R set to 8. We believe that this can\nresonates with the hypothesis of introducing randomness as the source of ASR boosts. Both too much\nor too little randomness hurt performance. As such, we use R = 8 for probe sampling. We further\nshow several convergence processes with varying values of R in Appendix B.\n\nAdaptive vs fixed filtered set size.  As mentioned in Section 2.2, an alternative to use an adaptive\nfiltered set size is to use a fixed size. Here we investigate whether it matters to use an adaptive filtered\nset size that is determined by how much the draft model and the target model agree on each candidate\nset. To use a fixed size, we simply fix the probe agreement score α to be 0.9, 0.6, 0.3, and 0.0 and\ncompare with the adaptive case. As shown in Table 8, fixed probe agreement scores always lead to\nworse ASR. Furthermore, when adopting GPT-2 as the draft model, the average agreement score is\n0.45 with a standard deviation of 0.11. This shows that the agreement score between the two models\nvaries significantly for different candidate sets. We also provide the statistics of α for other draft\nmodels in Table 11.\n\nProbe agreement measurement.  We also experiment alternatives to measure the probe agreement\nscore, including the Pearson correlation coefficient (Pearson, 1900), Kendall’s Tau correlation\ncoefficient (Kendall, 1938), and Goodman and Kruskal’s gamma (Goodman et al., 1979) where the\nPearson correlation coefficient directly uses the loss values to compute the agreement and the others\nuse the ranking information. As shown in Table 9, all methods have similar time cost, and Spearman’s\nrank correlation coefficient achieves the best ASR. The Pearson correlation coefficient performs\nworse than other ranking-based agreement measurement.\n\nProbe set size.  The size of the probe set also determines whether the probe agreement score is\nmeasured accurately. As such, we experiment with different probe set size and report the performance\nin Table 10. We find that using a small probe set such as B/64 or B/32 can result in inaccurate\n\n\n                                       8\n\nTable 9: Ablation on probe agreement measure-   Table 10: Ablation on the probe set size k. Using\nments.  All methods achieve similar speedup   B/16 leads to accurate probe agreement mea-\nwhile Spearman’s rank correlation coefficient   surement while achieving significant accelera-\nachieves the best ASR.                                tion.\n\n   Cor       Spearman   Pearson   Kendall   Kruskal          Probe    B/64  B/32   B/16  B/4  B/2   B\n\n  ASR        85.0       70.0      74.0      79.0         ASR       64.0    72.0     85.0    86.0   85.0   87.0\n\n   Time (s)      2.60       2.47      2.53     2.43          Time (s)   2.10    2.57     2.60    3.41   5.61   9.58\n\n\nTable 11: Experiments with different draft models. Models with over 1B parameters, like TinyLlama,\nPhi, and ShearedLlMa, need two GPUs for parallel computation. ShearedLlMa achieves the highest\nASR probably because it is a pruned version of Llama2. Both GPT-2 and GPT-Neo achieve a good\nbalance of ASR and speedup.\n\n                                 1 GPU                                          2 GPUs\n              GPT-2      GPT-Neo      Flan-T5     BART       TinyLlama         Phi        ShearedLlaMa\n  Model\n             (124M)      (125M)      (248M)      (406M)         (1.1B)         (1.3B)           (1.3B)\n\n α          0.45 ± 0.10   0.51 ± 0.11   0.61 ± 0.13   0.46 ± 0.09    0.52 ± 0.13    0.52 ± 0.11      0.35 ± 0.12\n\n ASR          85.0          81.0          57.0          76.0           72.0           82.0           91.0\n\n  Time (s)      2.60          2.82          3.89          2.93           3.38           4.83            3.93\n\n\nagreement score, which put a put a significant toll on the attack success rate. It also does not lead to\ntoo much time reduction since the draft model computation done in parallel takes more time and the\nreduced computation is not the bottleneck. Using a larger probe set size such as B/4 and B/2 will\nlead to more accurate agreement score but does not increase the ASR significantly. As such, using a\nprobe set of size B/16 is good enough to accurately measure the agreement and achieves maximum\ntime reduction.\n\nDraft model study.  Here we also experiment with bigger draft models, some of which is of similar\nsize to Llama2. We experiment with GPT-Neo (Gao et al., 2020), Flan-T5-base (Chung et al., 2024),\nBART (Lewis et al., 2019), Phi-1.5 (Li et al., 2023), TinyLlama (Zhang et al., 2024) and Sheared-\nLLaMA (Xia et al., 2023). Among them, Sheared-LLaMA might be the closest to Llama2 since it is\na pruned version of Llama2. For TinyLlama, Phi and Sheared-LLaMA, we use 2 A100s with 80GB\nmemory to fit the whole computation.\n\nAs shown in Table 11, Sheared-LlaMa achieves the best ASR although the time reduction is not as\ngood as smaller models such as GPT-2 and there would be a higher time cost if we manage to fit all\ncomputation in one GPU. On contrast, Flan-T5, BART, TinyLlama and Mistral all achieve lower\nASRs probably because of being very different than Llama2. However, the results are still better than\nthe baseline ASR 66.0. GPT-2 and GPT-Neo achieve a good balance of performance and speedup.\n\n4  Related Work\n\nAlignment of LLMs.  To build safe LLMs, alignments has also been a widely studied topic in\nthe community (Stiennon et al., 2020; Ouyang et al., 2022). Efforts have been put into improving\nhelpfulness (Bai et al., 2022a; Cheng et al., 2023), honesty (Kaddour et al., 2023; Liu et al., 2023;\nXu et al., 2023), and harmlessness (Hartvigsen et al., 2022). Among these works, there has been a\ngrowing interest in using feedback from a LLM to perform alignment (Bai et al., 2022b; Gulcehre\net al., 2023; Burns et al., 2024; Yuan et al., 2024a). Despite all the efforts, there has not been a\ndefinitive answer for LLM safety alignments, which also motivates our research in LLM safety.\n\nDiscrete Prompt Optimization.  Attacking LLMs via adversarial prompt can be formulated as\na discrete prompt optimization problem (Zou et al., 2023). In this context, attacking algorithms\nstrive to discover superior prompts that effectively steer aligned LLMs toward generating adversarial\nanswers. Some approaches leverage LLMs themselves to iteratively refine prompts (Xu et al., 2022;\nPryzant et al., 2023). However, aligned LLMs may resist refining adversarial prompts, rendering\nthese methods ineffective. Other strategies employ RL-based prompt optimization techniques such\nas those in (Mingkai and Jianyu, 2022; Lu et al., 2023), necessitating additional MLP training with\n\n\n                                       9\n\nextensive adversarial data and specific reward design. Moreover, other models introduced in (Cho\net al., 2023; Long et al., 2024) to help with prompt optimization must remain unaligned, particularly\nin jailbreak scenarios (Chao et al.). However, their performance tends to be limited, especially when\ndealing with strongly fine-tuned models like Llama2-Chat.\n\nLLM Jailbreaks. LLM Jailbreaks have received considerable interests recently since due to the\nimplications of applying LLMs widely in human society. Although there is a continuous effort to\nbuild safe and reliable LLMs, bypassing the safety mechanism of LLMs is not uncommon. For\nexample, fine-tuning a safe LLM on a few data instances can easily breaks its safety guarantees (Qi\net al., 2024; Lermen and Rogers-Smith, 2024). Treating the jailbreak as a prompt optimization\nproblem has also led to a certain level of success (Zou et al., 2023; Mökander et al., 2023; Liu et al.,\n2024; Chao et al.; Geisler et al., 2024). In addition, conversing in a ciphered language (Yuan et al.,\n2024b), planting a backdoor during RLHF (Rando and Tramèr, 2023), using a less well-aligned\nlanguage (Deng et al., 2024) and multi-modality (Shayegani et al., 2024) can also lead to successful\njailbreaks. Researchers also construct large dataset of manual jailbreak prompts (Toyer et al., 2023).\n\nAmong these jailbreak methods, the prompt optimization method GCG (Zou et al., 2023) provides\nthe more general and universal solution for us to study the jailbreaking problem. As such, in this\nwork, we mainly focus on the acceleration of GCG, but the idea of delegating computation to a draft\nmodel can also be applied in other situations such as the multi-modality case and finetuning case. We\nleave the extension of this work for future work.\n\nAcceleration.  In the field of acceleration, speculative sampling (Chen et al., 2023; Leviathan et al.,\n2023) is the most relevant to our method. They also use a draft model but its design cannot be directly\napplied to accelerate the GCG algorithm. REST (He et al., 2024) adopts the concept of speculative\nsampling but uses a retrieval approach based on a Trie to construct the candidate. The attention\nmodule has also been a focus of acceleration because of its quadratic nature (Dao et al., 2022; Cai\net al., 2024). There have also been continuous interests in more efficient versions of Transformers (So\net al., 2019; Dai et al., 2021; Liu et al., 2021; Gu et al., 2020, 2021). These architectural changes are\ncomplementary to our algorithm design and we leave it to future work.\n\n5  Conclusion\n\nIn this paper, we propose an algorithm probe sampling that can effectively accelerate the GCG\nalgorithm. We achieve an acceleration ranging from 2.1× to 6.3× in different scenarios on AdvBench.\nWe illustrate the intuition and how the algorithm works through extensive experiments. Furthermore,\nthis approach is also applied to general prompt optimization methods and other jailbreak techniques,\nincluding AutoPrompt, APE, and AutoDAN. We believe the idea of using the probe agreement score\nto perform adaptive computation can be applied to cases other than GCG. For example, it could\npotentially be used to perform conditional computation for attention. Another direction is to extend\nthe framework to the multi-modality case which can be interesting given the vast amount of video\ndata. It would also be interesting to run a small draft model on the scale of web data to detect the\nexistence of natural adversarial prompts.\n\nLimitation and Impact Statements\n\nProbe sampling has two main limitations. Firstly, it exhibits relatively slow performance when tested\non large-sized test sets, which hampers its efficiency. Secondly, it is limited to supporting only\nopen-source models, thereby excluding proprietary or closed-source models from benefiting from\nthe proposed acceleration techniques. These limitations indicate the need for further improvements\nto enhance the speed and broaden the model support in order to make the jailbreak acceleration\napproach more robust and applicable across a wider range of language models.\n\nProbe sampling can be applied to accelerate GCG algorithm. Having a faster algorithm to explore\nadversarial cases of alignments enable us to study how to make LLMs safer. As far as we know, as\nof now, there is not a LLM that can use this algorithm to achieve malicious behavior in real-world\nthat would not be possible without the algorithm. The goal of this research is to present a general\nalgorithm which may inspire new research, and also contribute to the gradual progress of building\nsafe and aligned AIs.\n\n\n                                       10\n\nAcknowledgements\n\nThis research is partially supported by the National Research Foundation Singapore under the AI\nSingapore Programme (AISG Award No: AISG2-TC-2023-010-SGIL) and the Singapore Ministry\nof Education Academic Research Fund Tier 1 (Award No: T1 251RES2207). Xuan Long Do is\nsupported by the A*STAR Computing and Information Science (ACIS) scholarship. We thank Liwei\nKang for insightful discussion, Liying Cheng for helping with plotting figures.\n\nReferences\n\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\n  Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless\n   assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\n  Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai:\n  Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n  Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models\n   are few-shot learners. Advances in neural information processing systems, 33:1877–1901.\n\nCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner,\n  Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. 2024. Weak-to-strong generalization:\n   Eliciting strong capabilities with weak supervision. In Forty-first International Conference on\n  Machine Learning.\n\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao.\n  2024. Medusa: Simple llm inference acceleration framework with multiple decoding heads. In\n   Forty-first International Conference on Machine Learning.\n\nPatrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong.\n  Jailbreaking black box large language models in twenty queries. In R0-FoMo: Robustness of\n  Few-shot and Zero-shot Learning in Large Foundation Models.\n\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John\n  Jumper. 2023. Accelerating large language model decoding with speculative sampling. arXiv\n   preprint arXiv:2302.01318.\n\nPengyu Cheng, Yifan Yang, Jian Li, Yong Dai, and Nan Du. 2023. Adversarial preference optimiza-\n   tion. arXiv preprint arXiv:2311.08045.\n\nSukmin Cho, Soyeong Jeong, Jeong yeon Seo, and Jong Park. 2023. Discrete prompt optimization\n   via constrained generation for zero-shot re-ranker. In The 61st Annual Meeting Of The Association\n  For Computational Linguistics.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\n   Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm:\n  Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113.\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li,\n  Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned\n  language models. Journal of Machine Learning Research, 25(70):1–53.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n  Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\n  Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\n\nZihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. 2021. Coatnet: Marrying convolution and\n   attention for all data sizes. Advances in neural information processing systems, 34:3965–3977.\n\n\n                                       11\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and\n  memory-efficient exact attention with io-awareness. Advances in Neural Information Processing\n  Systems, 35:16344–16359.\n\nYue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. 2024.  Multilingual jailbreak\n  challenges in large language models.  In The Twelfth International Conference on Learning\n  Representations.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\n  Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text\n   for language modeling. arXiv preprint arXiv:2101.00027.\n\nSimon Geisler, Tom Wollschläger, MHI Abdalla, Johannes Gasteiger, and Stephan Günnemann. 2024.\n  Attacking large language models with projected gradient descent. arXiv preprint arXiv:2402.09154.\n\nLeo A Goodman, William H Kruskal, Leo A Goodman, and William H Kruskal. 1979. Measures of\n  association for cross classifications. Springer.\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. 2020. Hippo: Recurrent\n  memory with optimal polynomial projections. Advances in neural information processing systems,\n  33:1474–1487.\n\nAlbert Gu, Karan Goel, and Christopher Ré. 2021.  Efficiently modeling long sequences with\n   structured state spaces. arXiv preprint arXiv:2111.00396.\n\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\n  Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced\n   self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998.\n\nChuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. 2021. Gradient-based adversar-\n   ial attacks against text transformers. In Proceedings of the 2021 Conference on Empirical Methods\n   in Natural Language Processing, pages 5747–5757.\n\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar.\n  2022. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech\n   detection.  In Proceedings of the 60th Annual Meeting of the Association for Computational\n  Linguistics (Volume 1: Long Papers), pages 3309–3326.\n\nHorace He. 2023. GPT-Fast. [Online]. Available: https://github.com/pytorch-labs/\n  gpt-fast/tree/main?tab=readme-ov-file. Accessed: Feb. 2, 2024.\n\nZhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and Di He. 2024. Rest: Retrieval-based speculative\n  decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association\n   for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\n  1582–1595.\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-\n  cob Steinhardt. 2020. Measuring massive multitask language understanding. In International\n  Conference on Learning Representations.\n\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization with gumbel-softmax.\n  In International Conference on Learning Representations.\n\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\n  Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\n  2023. Mistral 7b. arXiv preprint arXiv:2310.06825.\n\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert\n  McHardy. 2023.  Challenges and applications of large language models.  arXiv preprint\n  arXiv:2307.10169.\n\nMaurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81–93.\n\nSimon Lermen and Charlie Rogers-Smith. 2024. Lora fine-tuning efficiently undoes safety training\n   in llama 2-chat 70b. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models.\n\n\n                                       12\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient\n  prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\n  Processing, pages 3045–3059.\n\nYaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via\n  speculative decoding. In International Conference on Machine Learning, pages 19274–19286.\n  PMLR.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\n  Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for\n   natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.\n\nYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.\n  2023. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463.\n\nHanxiao Liu, Zihang Dai, David So, and Quoc V Le. 2021. Pay attention to mlps. Advances in\n  Neural Information Processing Systems, 34:9204–9215.\n\nXiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2024. Autodan: Generating stealthy\n   jailbreak prompts on aligned large language models. In The Twelfth International Conference on\n  Learning Representations.\n\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor\n  Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023. Trustworthy llms: a survey and guideline\n   for evaluating large language models’ alignment. In Socially Responsible Language Modelling\n  Research.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n  Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining\n  approach. arXiv preprint arXiv:1907.11692.\n\nDo Long, Yiran Zhao, Hannah Brown, Yuxi Xie, James Zhao, Nancy Chen, Kenji Kawaguchi,\n  Michael Shieh, and Junxian He. 2024. Prompt optimization via adversarial in-context learning.\n  In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n  (Volume 1: Long Papers), pages 7308–7327, Bangkok, Thailand. Association for Computational\n   Linguistics.\n\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\n  Clark, and Ashwin Kalyan. 2023. Dynamic prompt learning via policy gradient for semi-structured\n  mathematical reasoning. In The Eleventh International Conference on Learning Representations.\n\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. 2022. The concrete distribution: A continuous\n   relaxation of discrete random variables. In International Conference on Learning Representations.\n\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto\n  Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models.\n  In Proceedings of the Ninth International Conference on Language Resources and Evaluation\n  (LREC’14), pages 216–223, Reykjavik, Iceland. European Language Resources Association\n  (ELRA).\n\nDeng Mingkai and Wang Jianyu. 2022. Rlprompt: Optimizing discrete text prompts with reinforce-\n  ment learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\n  Processing.\n\nJakob Mökander, Jonas Schuett, Hannah Rose Kirk, and Luciano Floridi. 2023. Auditing large\n  language models: a three-layered approach. AI and Ethics, pages 1–31.\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\n  Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to\n  follow instructions with human feedback. Advances in Neural Information Processing Systems,\n  35:27730–27744.\n\n\n                                       13\n\nKarl Pearson. 1900. X. on the criterion that a given system of deviations from the probable in the\n  case of a correlated system of variables is such that it can be reasonably supposed to have arisen\n  from random sampling. The London, Edinburgh, and Dublin Philosophical Magazine and Journal\n   of Science, 50(302):157–175.\n\nMartin Pincus. 1970. A monte carlo method for the approximate solution of certain types of\n  constrained optimization problems. Operations research, 18(6):1225–1228.\n\nReid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic\n  prompt optimization with “gradient descent” and beam search.  In Proceedings of the 2023\n  Conference on Empirical Methods in Natural Language Processing, pages 7957–7968.\n\nXiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.\n  2024. Fine-tuning aligned language models compromises safety, even when users do not intend to!\n  In The Twelfth International Conference on Learning Representations.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.\n  Language models are unsupervised multitask learners.\n\nJavier Rando and Florian Tramèr. 2023. Universal jailbreak backdoors from poisoned human feedback.\n  arXiv preprint arXiv:2311.14455.\n\nErfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2024. Jailbreak in pieces: Compositional\n   adversarial attacks on multi-modal language models. In The Twelfth International Conference on\n  Learning Representations.\n\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Auto-\n  prompt: Eliciting knowledge from language models with automatically generated prompts. In\n  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), pages 4222–4235.\n\nDavid So, Quoc Le, and Chen Liang. 2019. The evolved transformer. In International conference on\n  machine learning, pages 5877–5886. PMLR.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and\n  Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment\n  treebank.  In Proceedings of the 2013 conference on empirical methods in natural language\n  processing, pages 1631–1642.\n\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\n  Dario Amodei, and Paul F Christiano. 2020.  Learning to summarize with human feedback.\n  Advances in Neural Information Processing Systems, 33:3008–3021.\n\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\n  Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2023. Challenging big-bench tasks\n  and whether chain-of-thought can solve them. In Findings of the Association for Computational\n  Linguistics: ACL 2023, pages 13003–13051.\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\n  Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open\n  foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\n\nSam Toyer, Olivia Watkins, Ethan Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong,\n  Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, et al. 2023. Tensor trust: Interpretable prompt\n   injection attacks from an online game. In NeurIPS 2023 Workshop on Instruction Tuning and\n  Instruction Following.\n\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein.\n  2024. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and\n  discovery. Advances in Neural Information Processing Systems, 36.\n\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. Sheared llama: Accelerating\n  language model pre-training via structured pruning. In Workshop on Advancing Neural Network\n  Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS\n  2023).\n\n\n                                       14\n\nChunpu Xu, Steffi Chern, Ethan Chern, Ge Zhang, Zekun Wang, Ruibo Liu, Jing Li, Jie Fu, and\n  Pengfei Liu. 2023. Align on the fly: Adapting chatbot behavior to established norms. arXiv\n  preprint arXiv:2312.15907.\n\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang Yanggang, Haiyu Li, and Zhilin Yang. 2022.\n  Gps: Genetic prompt search for efficient few-shot learning. In Proceedings of the 2022 Conference\n  on Empirical Methods in Natural Language Processing, pages 8162–8171.\n\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and\n  Jason E Weston. 2024a. Self-rewarding language models. In Forty-first International Conference\n  on Machine Learning.\n\nYouliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and\n  Zhaopeng Tu. 2024b. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. In The\n  Twelfth International Conference on Learning Representations.\n\nJerrold H Zar. 2005. Spearman rank correlation. Encyclopedia of biostatistics, 7.\n\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. Tinyllama: An open-source small\n  language model. arXiv preprint arXiv:2401.02385.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\n  Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench\n  and chatbot arena. Advances in Neural Information Processing Systems, 36:46595–46623.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\n  Jimmy Ba. 2022. Large language models are human-level prompt engineers. In The Eleventh\n  International Conference on Learning Representations.\n\nAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.  Universal and transferable\n   adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043.\n\n\n\n\n\n                                       15\n\nA  Implementation\n\nThe following code shows the core implementation of probe sampling using PyTorch. As seen in the\ncode, the algorithm is relatively easy to use.\n\ndef draft_model_all(args):\n    draft_model.loss(control_cands)\n\n    queue.put('draft':loss_small)\n\ndef target_model_probe(args):\n    probe_index = random.sample(range(512), 512/16)\n    probe_control_cands = control_cands[probe_index]\n    target_model.loss(probe_control_cands)\n\n    queue.put('target':[loss_large_probe, probe_index])\n\n# Parallelly Calculate Loss on Batch and Probe Set\nargs=(control_cands, batch_size, queue)\nthreading.Thread(target=draft_model_all, args=args)\nthreading.Thread(target=target_model_probe, args=args)\n\n# Calculate Agreement Score\ncor = spearmanr(loss_small[probe_index], large_loss_probe)\n\n# Target Model Test on Filtered Set\nfiltered_size = int((1 - cor) * 512/8)\nindices = topk(loss_small, k=filtered_size, largest=False)\nfiltered_control_cands = control_cands[indices]\ntarget_model.loss(filtered_control_cands)\n\n# Return Lowest Loss Candidate\nreturn [large_loss_probe, filtered_control_cands].lowest()\n\n\nB  Converge Process\n\nIn Figure 5, we also show a few convergence processes with different values of R, where the pink\nline corresponds to R = 8. The pink line always achieves successful optimization while the other\nlines can lead to suboptimal results due to excessive randomness or insufficient randomness. In\nparticular, the blue and yellow lines can suffer from excessive randomness and the other lines might\nhave insufficient randomness.\n\n\n\n\n\n                 Figure 5: Converge progress with different sizes of filtered set.\n\n\nC  Software optimization\n\nIn other speedup works (He, 2023), using  Table 12: Results with torch.compile() enabled.\ntorch.compile() can lead to significant accelera-  torch.comple() does not lead to further speedup.\ntion. It compiles LLMs into an kernel and alle-\nviate the overhead of repeatedly launching the     Method  GCG   Probe sampling  PS (Compile)\nkernel. Table 12 shows that the time cost is sim-    ASR      66.0    85.0             85.0\nilar with or without this optimization enabled.     Time (s)   9.16    2.60 (3.5×)      2.54 (3.6×)\nThis is likely due to the fact that we use large\nbatch sizes and long input sequences, whose computation cost dominates the overhead caused by the\neager execution and launching the kernel repeatedly.\n\n\n                                       16",
"headers": [
"arXiv:2403.01251v3  [cs.CL]  8 Nov 2024",
"Accelerating Greedy Coordinate Gradient and",
"General Prompt Optimization via Probe Sampling",
"Abstract",
"1",
"Introduction",
"2",
"Proposed Method",
"3",
"Experiment",
"4",
"Related Work",
"5",
"Conclusion",
"Limitation and Impact Statements",
"Acknowledgements",
"References",
"A",
"Implementation",
"B",
"Converge Process",
"C",
"Software optimization"
],
"tables": [
"|Model Method|Harmful Strings<br>ASR Time (s) #FLOPs|Harmful Behaviors<br>Individual Multiple<br>Time (s) #FLOPs<br>ASR ASR (train) ASR (test)|Col4|Col5|\n|---|---|---|---|---|\n|Vicuna<br>(7b-v1.3)<br>GCG<br>GCG + Annealing<br>Probe sampling<br>PS + Annealing|88_._0<br>4_._1<br>97_._3 T<br>89_._0<br>1_._5 (2_._7_×_)<br>38_._5 T<br>91_._0<br>1_._7 (2_._4_×_)<br>42_._4 T<br>**93**_._**0**<br>**1**_._**1** (**3**_._**6**_×_)<br>**27**_._**8** T|99_._0<br>98_._0<br>**100**_._**0**<br>**100**_._**0**|**100**_._**0**<br>98_._0<br>92_._0<br>94_._0<br>96_._0<br>98_._0<br>96_._0<br>**99.0**|4_._8<br>106_._8 T<br>2_._1 (2_._3_×_)<br>46_._2 T<br>2_._3 (2_._1_×_)<br>53_._2 T<br>**1**_._**5** (**3**_._**2**_×_)<br>**24**_._**7** T|\n|Llama2<br>(7b-Chat)<br>GCG<br>GCG + Annealing<br>Probe sampling<br>PS + Annealing|57_._0<br>8_._9<br>198_._4 T<br>55_._0<br>2_._4 (3_._9_×_)<br>39_._7 T<br>**69**_._**0**<br>2_._2 (4_._1_×_)<br>43_._8 T<br>64_._0<br>**1**_._**4** (**6**_._**3**_×_)<br>**31**_._**2** T|69_._0<br>68_._0<br>**81**_._**0**<br>74_._0|88_._0<br>84_._0<br>92_._0<br>88_._0<br>92_._0<br>**93**_._**0**<br>**96**_._**0**<br>91_._0|9_._2<br>202_._3 T<br>2_._7 (3_._4_×_)<br>50_._6 T<br>2_._6 (3_._5_×_)<br>40_._7 T<br>**1**_._**6** (**5**_._**6**_×_)<br>**32**_._**3** T|",
"|Method|Direct<br>Llama2-7b|Transfer<br>Vicuna-7b Mistral-7b|\n|---|---|---|\n|GCG<br>PS (GPT-2)<br>PS (ShearedLlaMa)<br>PS (Flan-T5)|69_._0<br>85_._0<br>**91**_._**0**<br>57_._0|89_._0<br>**86**_._**0**<br>92_._0<br>83_._0<br>**93**_._**0**<br>85_._0<br>78_._0<br>69_._0|",
"|Method|Direct<br>Llama2-7b|Transfer<br>Vicuna-7b Mistral-7b|\n|---|---|---|\n|GCG<br>PS (_R_ = 64)<br>PS (_R_ = 8)<br>PS (_R_ = 1)|69_._0<br>60_._0<br>**85**_._**0**<br>79_._0|89_._0<br>**86**_._**0**<br>77_._0<br>74_._0<br>**92**_._**0**<br>83_._0<br>88_._0<br>84_._0|",
"|Method|ASR Time (s)|\n|---|---|\n|AutoDAN-GA<br>AutoDAN-GA + PS|**56**_._**2**<br>424_._2<br>55_._9<br>**182**_._**7** (**2**_._**3**_×_)|\n|AutoDAN-HGA<br>AutoDAN-HGA + PS|60_._8<br>237_._9<br>**62**_._**1**<br>**95**_._**3** (**2**_._**5**_×_)|",
"|Method|SST-2 SICK-E<br>Acc Time (s) Acc Time (s)|Col3|\n|---|---|---|\n|Original|85_._2<br>N / A|49_._4<br>N / A|\n|Autoprompt<br>Autoprompt + PS|**91**_._**4**<br>228_._4<br>90_._6<br>**127**_._**2** (**1**_._**8**_×_)|**69**_._**3**<br>42_._7<br>68_._9<br>**23**_._**6** (**1**_._**8**_×_)|",
"|Method|GSM8K<br>Acc Time (s)|MMLU<br>Acc Time (s)|BBH<br>Acc Time (s)|\n|---|---|---|---|\n|Vicuna<br>APE<br>APE+PS|20_._4<br>N/A<br>21_._3<br>431_._8<br>**22**_._**4**<br>**192**_._**3** (**2**_._**3**_×_)|45_._6<br>N / A<br>**48**_._**2**<br>187_._3<br>47_._3<br>**102**_._**5** (**1**_._**8**_×_)|38_._6<br>N / A<br>**40**_._**8**<br>265_._2<br>39_._9<br>**88**_._**7** (**3**_._**0**_×_)|",
"|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|66%|66%|66%|66%|66%|66%|66%|34%|34%|\n||||||||||\n|38%|38%|25%|25%|25%|25%|37%|37%|37%|\n||||||||||\n|43%|43%|43%|8%|49%|49%|49%|49%|49%|\n||||||||||",
"|Model|Vacant|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|||||||||\n|48%|48%|48%|48%|52%|52%|52%|52%|\n|||||||||\n|36%|36%|15%|15%|15%|49%|49%|49%|\n|||||||||\n|36%|36%|5%|59%<br>|59%<br>|59%<br>|59%<br>|59%<br>|\n|||||||||",
"|Reduction R|64|16|8|4|2|1|\n|---|---|---|---|---|---|---|\n|ASR|60_._0|70_._0|**85**_._**0**|81_._0|76_._0|79_._0|\n|Time (s)|**2**_._**01**|2_._31|2_._60|3_._02|3_._41|5_._19|",
"|Agreement α|0.9|0.6|0.3|0.0|Adaptive|\n|---|---|---|---|---|---|\n|ASR|70_._0|77_._0|75_._0|81_._0|**85**_._**0**|\n|Time (s)|**2**_._**17**|2_._41|2_._71|3_._01|2_._60|",
"|Cor|Spearman|Pearson|Kendall|Kruskal|\n|---|---|---|---|---|\n|ASR|**85**_._**0**|70_._0|74_._0|79_._0|\n|Time (s)|2_._60|2_._47|2_._53|**2**_._**43**|",
"|Probe|B/64|B/32|B/16|B/4|B/2|B|\n|---|---|---|---|---|---|---|\n|ASR|64_._0|72_._0|85_._0|86_._0|85_._0|**87**_._**0**|\n|Time (s)|**2**_._**10**|2_._57|2_._60|3_._41|5_._61|9_._58|",
"|Model|1 GPU<br>GPT-2 GPT-Neo Flan-T5 BART<br>(124M) (125M) (248M) (406M)|Col3|Col4|Col5|2 GPUs<br>TinyLlama Phi ShearedLlaMa<br>(1.1B) (1.3B) (1.3B)|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|**Model**|GPT-2<br>(124M)|GPT-2<br>(124M)|GPT-2<br>(124M)|GPT-2<br>(124M)|GPT-2<br>(124M)|GPT-2<br>(124M)|GPT-2<br>(124M)|\n|_α_|0_._45_ ±_ 0_._10|0_._51_ ±_ 0_._11|0_._61_ ±_ 0_._13|0_._46_ ±_ 0_._09|0_._52_ ±_ 0_._13|0_._52_ ±_ 0_._11|**0.35**_ ±_ 0_._12|\n|ASR|85_._0|81_._0|57_._0|76_._0|72_._0|82_._0|**91**_._**0**|\n|Time (s)|**2**_._**60**|2_._82|3_._89|2_._93|3_._38|4_._83|3_._93|",
"|Method|GCG|Probe sampling|PS (Compile)|\n|---|---|---|---|\n|ASR|66_._0|85_._0|85_._0|\n|Time (s)|9_._16|2_._60 (3_._5_×_)|2_._54 (3_._6_×_)|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2403.01251v3.pdf"
}