{
"text": "MAPO: Boosting Large Language Model Performance with\n                        Model-Adaptive Prompt Optimization\n\n                   Yuyan Chen1, Zhihao Wen3, Ge Fan4, Zhengyu Chen, Wei Wu5\n                        Dayiheng Liu, Zhixu Li1, Bang Liu, Yanghua Xiao1,2\n            1Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University,\n                          2Fudan-Aishu Cognitive Intelligence Joint Research Center,\n                            3Singapore Management University, 4Tencent, 5Ant Group\n          {chenyuyan21@m., zhixuli@, shawyh@}fudan.edu.cn, zhwen.2019@phdcs.smu.edu.sg,\n                ge.fan@outlook.com, chenzhengyu@zju.edu.cn, congyue.ww@antgroup.com,\n                      liudayiheng.ldyh@alibaba-inc.com, bang.liu@umontreal.ca\n\n\n                          Abstract                                  QA          Task                     GPT-J : The target is theModelpersonInferencewho is going to be doing the\n                                                                                                                                               Prompt: What other type of loop is            work.\n               Prompt engineering, as an efficient and effec-                        there other than an open loop?            BLOOM: 100% of the time, the loopis open.\n                     tive way to leverage Large Language Models                   GenerationGround Truth: closed loop                 LLaMA: Closed loop</s>\n                                                                                                                                                                                                      GPT-J:                                                                                                                                                                                                      bees                                                                                                                                                                                                                          \\n                                                                                                                                                                                   What                                                                                                                                                                                                                                             are                                                                                                                                                                                                                                      the topics in the sentence: beescollect\n                                                                                                                                                Prompt:                                                                                                                                          What                                                                                                                                                                                        are                                                                                                                                                                                   the                                                                                                                                                                                                      topics in                                                                                                                                                                                               the                                                                                                                                                                                                 from                                                                                                                                                                                                                          the                                                                                                                                                                                                  flowers               (LLM), has drawn a lot of attention from the                                                                                                                                                                        sentence:                                                                                                                                                              bees                                                                                                                                                                                                             collect                                                                                                                                                              nectar                                                                                                                                                                           from                                                                                                                                                                                                  the        nectar2024                                                                                                                                             flowers                                                                                                                                                           BLOOM:                                                                                                                                                                                                     bee,                                                                                                                                                                                                                                                                                collect,                                                                                                                                                                                                               flower</s>                   research community. The existing research pri-\n                                                                                                                                   Ground Truth: flower, bee, nectar            LLaMA: beescollect nectarfrom the flowers\\n What are the\n                  marily emphasizes the importance of adapting                       Classification                                      topics in the sentence: the beesJul                prompts to specific tasks, rather than specific                      Prompt:home againas it. Theturnssentimentout , you can go           GPT-J:same name.Yes. \\nTheTheoriginalmovie movieis a remakewas a of the 1979 movie of the\n                                                                                                                                                              expressed                                                                                                                                                                                                   for                                                                                                                                                                               the movie is               BLOOM:                                                                                                                                                                               No</s>4         LLMs. However, a good prompt is not solely\n                                                                                                                                                                            LLaMA:                                                                                                                                                                                                                                                          Yes.                                                                                                                                                                                                                           \\n                                                                                                                                                                                                    The                                                                                                                                                                                                       movie                                                                                                                                                                                                                                                                                                                                        is                                                                                                                                                                                                                      about a man who is a widower and                                                                                                                                   Ground                                                                                                                                                                                 Truth:                                                                                                                                                                                                       Positive\n                                                                                                                                                                                                     has a son.                                                                                                                                                                        He                                                                                                                                                                                                                                                                                                                       is                                                                                                                                                                                                                a very                                                                                                                                                                                         good                                                                                                                                                                                                                                                               father                   defined by its wording, but also binds to the na-\n                                                                                                                                                                                                            (a)                                                          (b)\n                   ture of the LLM in question. In this work, we\n                        first quantitatively demonstrate that different        Figure 1: Variance on answers from different LLMs (b) when\n                 prompts should be adapted to different LLMs to         they are given the same task-specific prompts (a).\n                 enhance their capabilities across various down-\n                                                               2023). While effective, such approaches are costly[cs.CL]           stream tasks in NLP. Then we novelly propose\n                                                       and unscalable. Hence, there has been a lot of                 a model-adaptive prompt optimizer (MAPO)\n                method that optimizes the original prompts for        effort to streamline the prompt optimization pro-\n                 each specific LLM in downstream tasks. Ex-       cess through automated or semi-automated ways,\n                   tensive experiments indicate that the proposed       including prompt retrieval (Ma et al., 2023; Zhou\n               method can effectively refine prompts for an         et al., 2022), prompt generation from scratch (Pang\n             LLM, leading to significant improvements over         et al., 2023), and prompt editing (Gao et al., 2020;\n                  various downstream tasks.                                                             Pryzant et al., 2023; Deng et al., 2022). For ex-\n        1  Introduction                           ample, in prompt retrieval, Ma et al. (2023) pro-\n                                                          pose a search strategy based on greedy search\n           Advancements in Large Language Models (LLMs)                                                                    to identify near-optimal prompts for in-context\n            have ushered in a transformative era in natural lan-                                                                    learning; in prompt generating from scratch, Pang\n            guage processing, showcasing their remarkable ca-                                                                         et al. (2023) introduce SharpT, which learns a\n               pabilities across a wide range of tasks (OpenAI,                                                              shared latent space and generates soft prompts us-arXiv:2407.04118v1\n            2023; Bubeck et al., 2023; Lyu et al., 2019). While                                                                 ing a lightweight prompt generator; in prompt edit-\n             these models possess human-like comprehension                                                                       ing, some approaches rely on reinforcement learn-\n            and response abilities, their performance is heavily                                                              ing or LLM-based feedback for prompt optimiza-\n             influenced by the quality of prompts. As can be                                                                      tion (Deng et al., 2022; Zhou et al., 2022).\n            observed in Fig. 1, answers from different LLMs\n             vary widely when they are provided with the same     However, the aforementioned research primarily\n              task-specific prompts. Therefore, it is necessary to   emphasizes the importance of adapting prompts\n             generate prompts that are most suitable for each    to specific tasks, rather than specific LLMs. The\n         LLM, thereby enhancing its performance on down-    latter, although very important, has not been stud-\n            stream tasks.                                       ied to date in NLP. The only relevant work so\n         A common practice towards prompt optimiza-    far has been done on multi-modal large models,\n              tion is to count on human expertise (White et al.,   which automatically optimizing prompts using re-\n            2023; Jiang et al., 2022; Zamfirescu-Pereira et al.,   inforcement learning to generate images based\n\n1.00\n                                                                          1.0                                          1.0\n                          0.75\n\n                                                                          0.5                                          0.5                                 BLOOM                                          Metric0.50                                                                                                                                        LLaMA\n                          0.25                                                                                                                                        GPT-J\n                                                                          0.0                                          0.0                          0.00\n\n                                            Generation                                      Classification                           QA\n\nFigure 2: The performance of different LLMs on task-specific prompts for three tasks: question-answering (a), classification\n(b), and generation (c). The results reveal significant variations across different LLMs’ performance.\non text (Hao et al., 2022). They underscore the  2  Empirical study\nconcept of “model-preferred prompts” or “model-\n                                                     In this section, we conduct empirical study on three\nspecific prompts”, emphasizing that there’s a need\n                                  LLMs (BLOOM-7B (Scao et al., 2022), GPT-J-\nfor a systematic method to automatically align user\n                                    6B (Wang and Komatsuzaki, 2021), and LLaMA-\nintentions with the specific prompt preferences of\n                                      7B (Scao et al., 2022)) to evaluate their separate per-\neach model (Lyu et al., 2024). Therefore, in this pa-\n                                               formance on question-answering (QA) (Chen et al.,\nper, we novelly propose a Model-Adaptive Prompt\n                                                2024b, 2023b), classification (Chen et al., 2023c,a),\nOptimization (i.e. MAPO) approach for LLMs in\n                                            and generation (Chen et al., 2022, 2024a, 2023d)\nNLP. Given the lack of effective training signals,\n                                                       tasks with same task-specific prompts. We use nine\nwe first establish a so-called warm-up dataset to\n                                                      dataset from P3 (Sanh et al., 2021) covering three\nobtain candidate prompts from an oracle LLM, and\n                                             downstream tasks with details in Appendix E. P3 is\nthen model the prompt optimization problem with\n                                                a widely-used prompt benchmark which contains\nreinforcement learning. Specifically, we first gen-\n                                                      original prompts and the corresponding ground-\nerate candidate prompts and search for the optimal\n                                                        truth answers. We adopt F1 score, accuracy and\nprompts to establish a warm-up dataset. After that,\n                                  ROUGE-L for QA, classification, and generation\nwe combine Supervised Fine-Tuning (SFT) and\n                                                        tasks, respectively. The visualization results are\nReinforcement Learning (RL) to optimize original\n                                         shown in the Fig. 2. From the violin plot, we ob-\nprompts for each specific LLM in various down-\n                                                   serve significant variations in distribution among\nstream tasks. Moreover, we make joint learning\n                                                        different LLMs in each task. For example, in the\nwith Proximal Policy Optimization (PPO) (Schul-\n                                                   generation task, the results of all three models are\nman et al., 2017) and RRMF (note that RRMF is\n                                                         distributed within the range of 0 to 0.5, but there are\ninspired by RRHF (Yuan et al., 2023)), to further\n                                                                         still differences in the specific distribution patterns.\nimprove the performance of RL. We conduct ex-\n                                                Moreover, the medians, means, and other statistical\ntensive experiments which validates the robustness\n                                               measures also differ greatly among three LLMs in\nand generalization of the proposed MAPO. To sum\n                                                each downstream task. Therefore, we consider that\nup, our main research question revolves around\n                                                     finding the optimal prompt for each specific LLM\nidentifying the optimal prompt that is suited for\n                                           on each task is meaningful, as it can help enhance\nvarious models. Our contributions are threefold:\n                                                    the LLMs’ performance on various downstream\n                                                          tasks.\n    • We are the first to quantitatively show that\n     different prompts should be adapted to differ-  3  Methods\n     ent Large Language Models (LLMs) in order\n                                           Based on the above empirical study, we present     to enhance their performance across various\n                                   MAPO, a model-adaptive prompt optimization ap-   NLP downstream tasks.\n                                                 proach for LLMs. It takes the original prompt as in-\n    • We introduce a novel approach called the    put and generate an optimized prompt which makes\n    Model-Adaptive Prompt Optimizer (MAPO),   an LLM give better outputs. The framework of\n     specifically designed to optimize the original  MAPO is shown in Fig. 3.\n    prompts for each particular LLM in down-                                               3.1  Warm-up Dataset Establishment\n     stream tasks.\n                                We first establish a warm-up dataset as training\n    • The experiments show that our proposed    dataset for prompt optimization.\n   MAPO model exhibits greater robustness and     Generating Candidate Prompts. The original\n     generalization and also achieves superior per-   prompts are from nine above-mentioned datasets\n    formance in a variety of downstream tasks.      in P3 (Sanh et al., 2021). We generate 1,000 can-\n\nWarm-up Dataset Establishment   Prompt Optimizer Construction\n\n                                                                                                   Actor Model\n                                                                                  LΦrθ (PPO)\n                                                                                      Lpg(PPO)\n\n                                                                                 Lv (PPO)\n                                                                     SFT\n                                                        Better Prompt                                 SFT Model    LΦSFT      RL Model              OptimizationPolicy\n                                              （po)                                  (Frozen Model)\n                             Original Prompt                                                                                      Lr  Lft   (PPO)\n                                                                                               (RRMF)\n                  （p）          Prompt Rank Sequence                      Reward Model\n                                               GPT-3.5         （p1, …, p, …, pk）                                                                                                                      Joint Learning\n                                                                                                                                         SFT\n                                                                                                                                      Approximating\n\n                               Candidate prompt 1                                                                                                                            Optimized Prompt\n                                                                                                                                                       Generalization\n                               Candidate prompt 2                                                                                      Pre-trainPre-train                        Maintaining\n        …                                                           General NLP Tasks        LΦpre(PPO)\n\n                               Candidate prompt k\n\n\n Figure 3: Framework of the proposed MAPO, including warm-up dataset establishment and prompt optimizer construction.\ndidate prompts per prompt using GPT-3.5 1. The                   Tasks   Dataset    Train (Pairs)   Val (Pairs)   Test (Pairs)\n                                                                                            AdverQA      10000        1000               -\ngenerated candidate prompts should maintain se-            QA    OpenQA       4957         500         500\n                                                                                                 CloseQA      11679        1000        1000\nmantic meaning similar to the original prompt but                       News         120000              -          7600\nmay have different expressions. To achieve this, we                    Class   MovieQASC         85308134         1066926         1066920\nuse the following instruction as input for GPT-3.5                Gen    TopicsSummary      1473267389        4018818         1497819\nto generate candidates: “Please rewrite the given                           Explan        9741         1221               -\ntext ‘original prompt’ while keeping the semantic\n                                                     Table 1: The amount of the warm-up dataset on variousmeaning unchanged.”. Some candidate prompts\n                                                      downstream tasks.\nare shown in Appendix A.\n                                             duce Po, which is more suitable for the LLM in  Searching for the Optimal Prompt. To deter-\n                                                 subsequent tasks.mine which candidate prompt is optimal for an orig-\ninal prompt, we compare the match degree, which                                                      3.2.1  Supervised Fine-tuning\nrefers to the similarity, between the outputs gener-\n                                We begin by employing the warm-up dataset toated using a candidate prompt and the ground truth\n                                                 conduct supervised fine-tuning (SFT) with an LLMoutput. The purpose is to identify the candidate\n                                                     across multiple downstream tasks. The objective ofprompt that produces an output most similar to the\n                                     SFT is to enhance the LLM’s capacity to generateground truth output. When a ground-truth output is\n                                                 responses that align with its preferences, utilizingnot available, the output of a stronger LLM, such\n                                                 annotated data. Prior research conducted by Ra-as GPT-3.5, is regarded as the ground-truth. Specif-\n                                           mamurthy et al. (2022) supports the notion that em-ically, first, we input the original prompt P and\n                                                 ploying SFT prior to reinforcement learning (RL)each candidate prompt into an LLM, respectively,\n                                                    leads to improved outcomes. Furthermore, to dif-for inference and obtain the corresponding outputs.\n                                                        ferentiate between specific tasks during training,Next, we compare the match degree with specified\n                                    we incorporate a brief instruction preceding theevaluation metrics. We adopt F1 score, accuracy,\n                                                       input, such as “This is a... (generative/question-and ROUGE-L (Lin, 2004) for QA, classification,\n                                                      answering/classification) task.”.and generation tasks, respectively. Based on these\nmetrics, we iterate the searching process and find                                                      3.2.2  Building Reward Model\nthe optimal prompt Po for an LLM in downstream\ntasks. The warm-up dataset consists of a collection    Next, we construct a reward model to learn the ef-\nof prompt pairs (referred to as {P, Po}), whose    fectiveness of prompts based on the preferences\ndistribution is shown in Table 1.                     of different LLMs. This approach is motivated by\n                                                      the fact that discriminative annotation through sort-\n3.2  Prompt Optimizer Construction                                                  ing incurs significantly lower costs compared to\n                                                  generating annotations for answers. Initially, weThe prompt optimizer seeks to refine the initial\n                                                   obtain a ranking sequence for an LLM in a spe-prompt (P) into an optimized prompt (Po) tailored\n                                                             cific downstream task. We sort the outputs gener-to a particular LLM. This refinement process en-\ntails altering the structure or wording of P to pro-   ated by candidate prompts {P1, P2, . . . , Pk−1, Pk}\n                                                  alongside  the  original prompt P,  using  the\n    1https://chat.openai.com/                       same evaluation metric as described in Sec. 3.1.\n\nThis sorting process yields a ranking sequence   model, which is LLM,ˆ   and a frozen model, which\n{P1, P2, . . . , P, Pk−1, Pk}. Prompts to the left of     is SFT model with its parameters frozen during the\nP exhibit poorer inference results, while prompts  RL training process. The frozen model serves as a\nto the right demonstrate better results. Next, we em-   benchmark to evaluate whether the updated actor\nploy the ranking sequence to train a reward model.   model has advantages over it. We then calculate\nWe utilize the same LLM utilized in the SFT pro-   the policy gradient loss (i.e., actor’s loss) based on\ncess (referred to as LLM)ˆ   and replace the softmax    the importance ratio and reward (r+γVnext −Vcur),\nlayer with a linear layer to construct the reward   and calculate the value loss (i.e., critic’s loss) by\nmodel. The reward model takes a prompt as input   comparing the predicted value Vpred with the true\nand produces a scalar score indicating the quality    value (r + Vnext) as follows:\nof the prompt. We form pairwise ranking pairs\n                                                                 Pπa(t)by   combining            prompts                   from the                              ranking                                    sequence\n                                                     Lpg = Pπf(t)(r + γVnext −Vcur),     (2)and    employ Pairwise                  Ranking                          Loss                                     for training,                                               as\nillustrated below:                                    Lv = ∥Vpred −(r + Vnext)∥.        (3)\n            Lθ = −1k E(x,yw,yl)∼D            Here, Pπa(t) represents the ratio of probabilities                                                                       Pπf(t)                          2                      (1)                                                                         (i.e., importance ratio) of generating the same token\n         [log(σ(rθ(x, yw) −rθ(x, yl))],            under the actor model and the frozen model. (r +\nwhere x represents the original prompt, yw and    γVnext −Vcur) represents the reward of the current\nyl denote the higher-scoring and lower-scoring    step. Vpred denotes the predicted value, and (r +\nprompts, respectively, in the corresponding ranking    Vnext) denotes the true value.\n                                                   Next, we maximize the mathematical expecta-pair. rθ represents the scalar output of the reward\nmodel, D is the set of ranking pairs, and K denotes    tion of the reward model, aiming to consistently\nthe number of candidate prompts. Through this pro-   generate prompts that LLMˆ   perceives as the best\ncess, based on the outputs generated by LLMˆ  with    in the RL-trained SFT model (referred to as RL\n                                                 model). We feed prompts x generated by the SFTthe given prompt, the reward model learns to assign\n                                            model based on the datasets shown in Table 3 (i.e.,higher scores (rewards) to better prompts and lower\n                                    D) into the RL model πRLϕ   to obtain an optimizedscores (rewards) to inferior prompts, thus imitating\n                                            prompt y. y changes every time the RL modelan LLM’s preferences.\n                                                                is updated. We then input (x, y) into the reward\n3.2.3  Reinforcement Learning\n                                            model rθ and calculate a score (i.e., reward), which\nSubsequently, we employ Reinforcement Learning    represents the real-time feedback from the reward\n(RL) to further fine-tune LLMs. RL is used to   model. The loss function is defined as follows:\nadjust the bias in the reward model’s scoring since\n                                                                                                        (4)the distribution of generated prompts might change                                                       Lrθϕ = E(x, y) ∼DπRLϕ  [rθ(x, y)].\nduring the SFT process. The primary objective of\n                                                            Finally, we combine the above loss functions tooptimization is to maximize the scores of prompts\ngenerated by LLMˆ   after SFT (referred to as the    optimize the RL policy from multiple perspectives.\n                                          The final loss function is defined as:SFT model), as evaluated by the reward model. To\nachieve this, we utilize a combination of Proximal                                               Lρ = α1Lpg + α2Lv + α3Lrθϕ ,       (5)\nPolicy Optimization (PPO) (Schulman et al., 2017)\nand RRMF algorithms (note that RRMF is inspred   where α1, α2, and α3 represent the optimal weights\nby RRHF (Yuan et al., 2023)) for joint learning.      of each function, which are determined through\n  Policy Optimization.  This step aims to opti-   experiments (the same applies below).\nmize the RL policy to improve the performance    SFT Approximating. This step aims to main-\nof the RL model. We first adopt the datasets    tain similarity between the RL model and the SFT\nshown in Table 3, which to construct environment-   model. When the RL model undergoes parame-\naction pairs. The environment refers to the original    ter updates, it leads to variations in the generated\nprompt, and the action represents the prompt gener-   prompt y based on the given prompt x. If there is\nated by LLMˆ  without instruct-tuning. We pass the   a significant discrepancy between the RL model\nenvironment-action pairs to the reward model to ob-   and the SFT model, it can result in inaccurate esti-\ntain rewards. In this process, we introduce an actor   mation of scores by the reward model. To address\n\nthis issue, we measure the distance between the   prompts are taken from Natural Instructions (Wang\nprompts generated by the RL model and the SFT    et al., 2022b). The loss function is as follows:\nmodel using Kullback-Leibler (KL) divergence.\n                                             LPre = γEx ∼Dpretrain[log(πRLϕ  (x))],  (11)The objective is to minimize the KL divergence\nand the loss function is defined as follows:                                             where Dpretrain represents the selected datasets for\n                                                         pre-training.\n  LSFTϕ  = −βlog(πRLϕ  (y|x)/πSFT (y|x)).   (6)                                                     Joint learning.  Finally, we make joint learning\n                                                  with the above-mentioned loss functions as follows:\nwhere πRLϕ  (y|x) and πSFT (y|x) represent prompts\ngenerated by RL model and the SFT model, respec-        Lϕ = γ1Lρ + γ2LSFT + γ3LPre.    (12)\ntively.\n  Next,  we  have  borrowed  the  idea  from  4  Experiments\nRRHF (Yuan et al., 2023) but adapt it to focus                                                     In this section, We conduct experiments with three\non “model feedback” instead of “human feedback”.                                                 popular LLMs as LLM,ˆ    respectively, including\nWe name it Ranking Responses from Model Feed-                              BLOOM (7B), GPT-J (6B), and LLaMA (7B), on\nback (RRMF). Specifically, we calculate the like-                                                          different downstream tasks to validate the effective-\nlihood probability of LLMˆ  during SFT and align                                                  ness of MAPO.\nthis probability with the score of the reward model.\n                                               4.1  Experimental SetupsTo optimize this objective, we employ supervised\nlearning with a rank loss, defined as follows:       The experiments are executed on 4 Nvidia A100\n                                   GPUs with 80GB each, using PyTorch in Python.\n       P log Pπ(yi|x, yi)                                2\n             pi =                             ,          (7)   DeepSpeed   is utilized in the training process. The\n                      ∥yi∥                 maximum sequence length for original prompts and\n         Lr = X max(0, pi −pj),        (8)   optimized prompts are both set to 512 tokens. The\n                  ri<rj                          number of epochs is set to 20 in the entire training\n                                                    process. We provide the detailed configuration of\nwhere pi is the conditional log probability which    the hyperparameters in Appendix B. The dataset\nrepresents the reward of each optimized prompt yi.   and metrics we utilize are the same as those de-\nri represents the reward model rθ(x, yi). We also    scribed in Sec. 2. All results are reported on the\nincorporate the cross-entropy loss introduced by                                                  corresponding test sets or 10% dev sets if a dataset\nRRHF (Yuan et al., 2023) to learn the generated                                                does not have a test set. Details of all used baselines\nprompts y′i with the highest reward r′i as follows:    and datasets are in Appendix D and E.\n          Lft = −ΣlogPπ(y′i|x, y′i).         (9)   4.2  Main Results\n                                          The main results are shown in Table 2. We observe\n   Finally, we combine the above loss functions for                                                         that the performance increase evidently among\nSFT approximating as follows:                                                                all LLMs during SFT. We then utilize MAPO to\n                                        make further optimization. We find the optimized\n    LSFT = β1LSFTϕ  + β2Lft + β3Lr.     (10)                                             prompts generated by MAPO are more adaptive\n                                                      in QA and generation task for BLOOM (increase  Generalization Maintaining.   This step ad-\n                                          by 20.5% for CloseQA and by 30.9% for Explandresses the issue of catastrophic forgetting by ensur-\n                                             compared with SFT (p<0.01)) and GPT-J (increaseing that an LLM performs well not only on specific\n                                          by 21.4% for CloseQA and by 20.6% for Explantasks but also on general NLP tasks. To achieve\n                                           compared with SFT (p<0.01)). And the promptsthis, we follow a similar approach as outlined in In-\n                                                    are more adaptive in classification task (increasestructGPT (Ouyang et al., 2022). We sample 10%\n                                          by 22.8% for News (p<0.01)) for LLaMA. Thesedata from general NLP tasks in GLUE (Wang et al.,\n                                                         results indicate that MAPO effectively enhances2018) and the SuperGLUE benchmark (Wang et al.,\n                                                      the performance of various LLMs and exhibits pref-2019), which are considered representative, as in-\n                                                  erences in different downstream tasks.dicated in Table 3, during the pre-training phase.\n                                           To validate the superiority of MAPO, we com-The objective of pre-training is to generate out-\n                                                 pare it with several SOTA prompt optimizationputs that are as good as or better than the original\none based on the original prompts. The original        2https://github.com/microsoft/DeepSpeed\n\nOriginal                            SFT-optimized                                    MAPO-optimized\n  Task    Dataset             BLOOM  GPT-J  LLaMA  BLOOM(↑(%))   GPT-J(↑(%))  LLaMA(↑(%))  ↑(%)  BLOOM (↑(%))   GPT-J(↑(%))  LLaMA(↑(%))  ↑(%)\n\n         AdverQA     13.5        3.0        3.2        18.3 (35.6)       9.4 (213.3)      23.2 (625.0)     291.3       19.5 (6.6)        11.0 (17.0)       25.1 (8.2)       10.6\n QA    OpenQA      25.9       17.0      13.3        26.7 (3.1)        20.3 (19.4)       15.4 (15.8)      12.8       27.2 (1.9)        21.0 (3.4)        16.1 (4.5)        3.3\n         CloseQA       6.4        6.9       10.8         7.8 (21.9)         8.4 (21.7)       13.9 (28.7)      24.1        9.4 (20.5)        10.2 (21.4)       14.8 (6.5)       16.1\n\n         News         92.8        0.0        1.1         95.5 (2.9)          5.5 (-)        10.1 (818.2)         -         98.7 (3.4)         6.3 (14.5)       12.4 (22.8)      13.6\n  CLS    Movie        90.9       51.1      78.7        92.6 (1.9)        52.7 (3.1)        81.3 (3.3)        2.8        93.3 (0.8)        53.9 (2.3)        82.5 (1.5)        1.5\n       QASC        99.4       54.0      61.6        99.9 (0.5)        56.3 (4.3)       70.2 (14.0)       6.2        99.9 (0.0)        56.8 (0.9)        72.8 (3.7)        1.5\n\n           Topics        29.5       17.5      14.3        34.8 (18.0)       21.6 (23.4)       18.6 (30.1)      23.8       36.2 (4.0)        23.4 (8.3)        19.5 (4.8)        5.7\n  GEN   Summary      46.1       13.1       6.6         48.8 (5.9)        16.7 (27.5)       10.7 (62.1)      31.8       50.2 (2.9)        17.8 (6.6)       12.2 (14.0)       7.8\n           Explan         5.7        8.5        6.9         6.8 (19.3)        10.7 (25.9)        8.2 (18.8)       21.3        8.9 (30.9)        12.9 (20.6)        9.1 (11.0)       20.8\n\n\nTable 2: Performance is evaluated for BLOOM, GPT-J, and LLaMA using original, SFT-optimized, and MAPO-optimized\nprompts with a frozen LLM during inference. The symbols ↑(%) and ↑(%) under SFT-optimized denote the relative increase from\noriginal prompts, while those under MAPO-optimized indicate improvement over SFT-optimized results. It’s emphasized that the\nterm “frozen LLM for inference” means the model hasn’t been trained directly on downstream tasks but only makes inferences.\nThus, there’s no training data with prompts as inputs and expected responses as outputs. “CLS” represents classification, and\n“GEN” stands for generation tasks.\n\n                 SST-2  YelpP. MR  CR  RTE  QNLI  SNLI  MNLI MRPC   the performance of almost all baselines, except\nF   Finetuning     80.6    88.7     67.4   73.3   58.6   60.2    54.6    47.8     77.4\nC   Softprompt     73.8    88.6     74.1   75.9   54.7   49.7    36.1    33.2     51.6   RLprompt, does not exceed that of Fine-tuning\n    Black-Box                    89.1                            93.2                                    86.6                                           87.4                                                 52.6                                                         48.8                                                                 46.6                                                                         42.9                                                                                 61.6    /SFT, and some even do not outperform the origi-    Autoprompt                 75                            79.8                                62                                           57.5                                                                 -                                                                          -          -                                                                                                -                                                                                                           -\nD  Manual        82.8    83      80.9   79.6   51.6   50.8    31.1    51.7     67.4    nal LLMs. This highlights the importance of SFT\n     In-Context                    85.9                            89.6                                    80.6                                           85.5                                                 60.4                                                         53.8                                                                 47.1                                                                         53.4\n                                                                                 45.8    in LLMs. When we add RL, as in the case of     Instructions                 89                            84.4                                    85.2                                           80.8                                                                 -                                                                          -          -                                                                                                -                                                                                                           -\n    GrIPS                    87.1                            88.2                                    86.1                                      80                                                                 -                                                                          -                                                                                     -                                                                                                -                                                                                                           -\n    RLprompt                    92.5                            95.1                                    87.1                                           89.5                                                                 -                                                                          -                                                                                     -                                                                                                -                                                                                                           -     RLprompt, the performance on downstream tasks\n                                           91.1   TEMPERA                    91.9                            92.6    88                                                 60.3                                                         57.4                                                                 56.4                                                                         45.2    74\n   AMA                    95.7                                     -                                                -        -                                                 75.1                                                                          -                                                                                     -                                                                                                -                                                                                                           -      surpasses that of SFT, indicating the significance of\n    SFT           94.9    92      88.5   87.6   74.3   62.5    58.8    54.6     78.5   RL for prompt optimization. Moreover, using our\n   MAPO-w/o g   96.0    93.3     90.1   88.7   75.2   63.0    59.8    55.7     79.0\nD  MAPO        96.1    93.5     90.2   88.9   75.3   63.1    60.0    55.7     79.3    proposed MAPO method to optimize the prompt\n                                                        further improves performance over RLprompt, ex-\nTable 3: The few-shot performance of SFT, MAPO                                                     cept in a very few cases, such as using BLOOM for\nwith SOTA prompt optimizing baselines in downstream\n                                           movie classification tasks. These experimental re-\ntasks. F: Finetuning, C: Continous prompt, D: Discrete\n                                                            sults demonstrate that the MAPO method proposedprompt.\n                                                       in this study makes a substantial contribution to\nbaselines in various popular tasks based on the                                               improving the performance and accuracy in down-\nsame setting Roberta-Large, as shown in Table 3.                                                 stream tasks.\nThe reported results represent the best-performing                                                 Moreover, we conduct experiments to evaluate\nLLM among the three LLMs, which indicates that                                                      the domain transfer performance of MAPO. The re-\nour method applies not only to LLMs but also to                                                            sults are presented in Table 5 and Table 6, while the\nsmaller LMs. We analyze the possible reasons as                                                           results of LLMs with original prompts are reported\nfollows: MAPO employs both SFT and RL to opti-                                           by Arora et al. (2022). Remarkably, we observe\nmize LLMs. In fact, the SFT process is not specific                                                         that each LLM, when using prompts optimized\nto LM. Fine-tuning smaller models is feasible and                                           by MAPO, displays improved performance across\ncommon, requiring fewer computational resources.                                                   various downstream tasks. Specifically, BLOOM\nRL is a widely-used algorithm across applications                                                      exhibits the highest increase in performance com-\nand model scales, and small models require less                                                pared with GPT-J and LLaMA. This experiment\ncomputational and storage resources, making RL                                                       clearly demonstrates the significant domain trans-\nmore feasible on them.                                                           fer capability of MAPO.\n\n  We also test the performance of MAPO with                                               4.3  Ablation Study\nthe above-mentioned SOTA prompt optimization\nbaselines. We use three LLMs, including BLOOM,  The effect of RL compared with SFT. From the\nGPT-J, and LLaMA, to replace the BART model    experiments (Table 3, Table 5 and Table 6), we can\nused in Table 3 for verifying the nine datasets in    observe that the performance improvements gained\nTable 2, as shown in Table 4. Due to SFT in LLMs    solely from using SFT are less than half of those\nequals fine-tuning pretrained language models, we    achieved by our proposed MAPO method, both on\ndirectly list the SFT results in the Fine-tuning row.   similar tasks and general NLP tasks. This clearly\nApart from Fine-tuning, we also freeze the LLMs    indicates the effectiveness of MAPO in optimizing\nand only modify the prompts for inference on down-   model-adaptive prompts.\nstream tasks. According to the experimental results,      In order to further demonstrate RL is necessary\n\nTask                       QA                                       CLS                                               GEN\n                      Ad  Op                                        Cl   Ne                                   Mo  QA   To                                                             Su   Ex      The effect of warm-up dataset. As shown in\n  (BLOOM)   Original         13.5   25.9   6.4    92.8   90.9   99.4   29.5   46.1   5.7      Fig. 5 and Table 11, our study examines the effects\n  F           Finetuning/SFT   18.3   26.7   7.8    95.5   92.6   99.9   34.8   48.8   6.8\n  C           Soft prompt      14.5   24.5   6.5    92.1   90.5   99.1   30.1   44.6   5.5      of different proportions of the warm-up dataset on\n              Black-Box       15.2   24.7   6.9    93.3   91.6   99.3   31.2   45.7   5.8\n              Autoprompt      15.7   25.0   7.1    93.6   91.9   99.4   31.6   46.0  6     MAPO’s performance. The results indicate that\n  D         Manual          13.7   24.6   6.8    91.8   90.9   99.0   31.5   45.1   5.7\n                In-Context       13.9   24.7   6.7    91.6   90.9   99.3   31.8   45.6   5.9      as the size of the warm-up dataset increases, per-\n                 Instructions      15.0   24.9   6.7    92.7   91.0   99.2   30.8   45.5   5.8\n             GrIPS           16.6   25.3   6.8    93.1   91.2   99.4   31.8   46.7   6.2     formance typically improves. BLOOM is particu-\n             RLprompt       19.2   26.9   8.9    97.5   94.1   99.9   35.9   49.1   8.2\n          TEMPERA      17.4   25.5   7.2    94.6   91.7   99.5   33.1   46.7   6.2       larly sensitive, showing a pronounced growth trend.\n         AMA            19.1   26.4   7.6    95.1   92.4   99.4   33.5   47.9   6.1\n  D      MAPO          19.5   27.2   9.4    98.7   93.3   99.9   36.2   50.2   8.9      Conversely, GPT-J shows a more gradual growth.\n   (GPT-J)                Original                                  3.0                                       17.0                                                6.9                                                      0.0                                                          51.1                                                         54                                                                       17.5   13.1\n                                                                                         8.5     LLaMA’s performance reveals an inflection around  F               Finetuning/SFT                                  9.4                                       20.3                                                8.4                                                      5.5                                                          52.7                                                                 56.3   21.6                                                                              16.7   10.7\n  C                Soft prompt                                  5.4                                       16.6   6.8                                                      2.1                                                          51.0                                                                 54.5                                                                       17.8                                                                              13.7\n                                                                                         8.9     60%, suggesting other factors also influence its per-              Black-Box                                  7.3                                       17.5                                                7.2    2.5                                                          51.4                                                                 54.7                                                                       18.2                                                                              14.3                                                                                         9.1\n              Autoprompt                                  7.8                                       17.9                                                7.3    1.9                                                          51.5                                                                 54.6                                                                       19.1                                                                              14.6                                                                                         9.3\n  D             Manual                                  5.7                                       15.9   6.5                                                      1.7                                                          50.9                                                                 54.9                                                                       18.7                                                                              13.9                                                                                         9.5     formance. Even with reduced dataset sizes, the\n                In-Context                                  5.6                                       16.5                                                6.7                                                      1.6                                                          50.7                                                                 54.6                                                                       19.2                                                                              14.0\n                                                                                         9.3     decrement in performance remains minimal, high-                 Instructions                                  7.1                                       17.1   7.2                                                      1.7                                                          51.6                                                                 54.0                                                                       19.1                                                                              14.3                                                                                         9.2\n             GrIPS                                  7.7                                       17.9                                                7.7                                                      4.3                                                          52.1                                                                 55.6                                                                       19.7                                                                              16.4\n                                                                                    10.3      lighting the method’s suitability for low-resource             RLprompt                                10.1   20.0                                                9.5                                                      5.7                                                          53.7                                                                 56.4                                                                       22.1                                                                              17.2                                                                                    11.8\n          TEMPERA                                  9.5                                       19.3                                                9.1                                                      4.8                                                          53.2                                                                 56.1                                                                       22.3   16.8\n                                                                                    11.4      tasks. We also conduct few-shot experiments on         AMA            9.6                                       19.1                                                8.8                                                      5.0                                                          52.9                                                                 56.3                                                                       22.4                                                                              16.5   11.6\n  D      MAPO          11.0   21.0   10.2   6.3    53.9   56.8   23.4   17.8   12.9     general NLP tasks with just 10% data and ob-\n  (LLaMA)   Original          3.2    13.3   10.8   1.1    78.7   61.6   14.3   6.6    6.9\n  F           Finetuning/SFT   23.2   15.4   13.9   10.1   81.3   70.2   18.6   10.7   8.2      serve promising improvements. This underlines\n  C           Soft prompt      7.7    12.6   10.2   4.4    77.4   62.7   15.6   8.2    7.3\n              Black-Box       9.2    13.3   10.7   5.6    78.1   63.1   16.2   8.4    7.5      our method’s adaptability and effectiveness in sce-\n              Autoprompt      10.3   13.5   11.0   7.4    78.4   65.2   16.7   9.0    7.6\n  D         Manual          12.3   12.7   11.1   7.5    77.5   64.8   16.2   8.3    6.7      narios of data scarcity.\n                In-Context       13.7   13.0   10.8   8.0    77.7   65.3   16.5   8.5    7.0\n                 Instructions      16.2   13.2   11.3   7.5    78.0   65.7   17.1   9.1    7.5\n             GrIPS           19.3   14.7   13.4   9.3    80.6   68.6   18.9   10.7   8.3      The effect of PPO and RRMF. To investigate\n             RLprompt       24.7   15.8   14.3   11.6   81.9   71.4   19.2   11.7   8.8\n          TEMPERA      22.6   15.4   13.8   8.9    81.5   69.6   18.7   9.6    8.7      the specific effects of PPO and RRMF during the\n         AMA            23.5   15.5   13.6   8.6    81.7   70.5   18.5   9.8    8.9\n  D      MAPO          25.1   16.1   14.8   12.4   82.5   72.8   19.5   12.2   9.1    RL process, we conduct separate experiments to\n                                                     evaluate the contributions of each component. The\nTable 4: The performance with a frozen LLM for infer-\n                                                 experimental results, depicted in Fig.6 (with de-\nence of MAPO with SOTA prompt optimizing baselines\n                                                                tails provided in Table 10 in Appendix F), clearlyin nine tasks from P3 benchmark using LLaMA. F: Fine-\ntuning/SFT, C: Continous prompt, D: Discrete prompt.    demonstrate the important roles played by PPO and\n                              RRMF in enhancing the performance of MAPO.\nand how it compares to simply extending SFT with\n                                We propose the following explanations for these\na larger warm-up dataset, we use various propor-\n                                                           results: PPO focuses on reducing the dissimilar-\ntions of the warm-up dataset to progressively in-\n                                                            ity between the RL model and the SFT model.\ncrease the SFT training data and then introduce RL\n                              RRMF aligns the scores from the reward model\nto it as shown in Table 17. Our findings consis-\n                                                with the likelihood probabilities of an LLM. Both\ntently show that RL adds value to the performance\n                                    PPO and RRMF aim to assign higher probabilities\nbeyond what is achieved by SFT alone across all\n                                                        to prompts that are more adaptable to the model.\nproportions of the dataset. This affirms the effec-\ntiveness of RL irrespective of the SFT dataset size.    The effect of the Randomness. We also incor-\nHowever, as the proportion of the warm-up dataset    porate randomness (e.g., temperature) during the\nincreases, the margin of improvement from adding    generation process of LLM. Given that our prompts\nRL begins to decline. While one could hypothe-   do not require high creativity, we have set a lower\nsize that adding RL to a very large SFT dataset    temperature range [0-0.5] for generation, within\nmight not result in as significant an improvement   which we aim to generate optimal prompts. To fur-\nas it would for a smaller dataset, this observation    ther investigate the impact of varying temperatures\nactually underscores our method’s suitability for   on the generated output, we conduct an additional\nlow-resource scenarios.                                 set of experiments to assess the performance of the\n                               MAPO method under different randomness settings\n  Moreover, we have tried different number of    (temperature=0,0.2,0.5,0.8) as shown in Table 14,\nepochs to see if extended training time consistently   Table 12, Table 15 and Table 16. Each experiment\nimproves SFT performance as shown in Table 18.   group runs 5 times. Our findings reveal that a high-\nExtending the training time does not consistently   temperature setting (t=0.8) tends to produce infe-\nlead to performance improvements for SFT. In    rior prompts that lead to less accurate outputs for a\nsome instances, the performance even declines. We    specific task. Lower temperature (t=0.2) or greedy\nalso find that some peak performance does not nec-    settings (t=0) are likely to produce more accurate\nessarily occur at the final epoch.                     outputs that are closer to our optimal results. This\n\nTask     Dataset   BLOOM  +SFT  (↑(%))  ↑(%)  +MAPO  (↑(%)  ↑(%)  GPT-J  +SFT  (↑(%))  ↑(%)  +MAPO  (↑(%))  ↑(%)\n\n                   Coref.  xwinograd     60.1       60.2    0.2      0.2     60.6       0.9      0.9      -           -          -           -        -             -           -\n\n            NLU   BoolQ         67.9       68.0    0.1      0.4     68.2       0.4      0.9     67.2     67.4    0.3      0.2    67.9       1.0      0.5\n                 CB            77.6       77.8    0.3              78.1       0.6             83.9     84.1    0.2             84.2       0.4\n                  COPA         74.0       74.3    0.4              75.0       1.4             84.0     84.2    0.2             84.2       0.2\n                       MultiRC       59.7       60.3    1.0              60.4       1.2             63.8     63.9    0.2             64.1       0.5\n                   ReCoRD      69.8       70.1    0.4              70.2       0.6             74.4     74.5    0.1             74.7       0.4\n                   WiC           61.4       61.6    0.3              62.0       1.0             61.0     61.1    0.2             61.3       0.5\n                WSC          64.4       64.7    0.5              65.1       1.1             77.9     78.0    0.1             78.1       0.3\n\n              NLI   ANLIR1       31.5       31.7    0.6      0.6     32.1       1.9      1.3     37.8     38.0    0.5      0.3    38.2       1.1      0.7\n                    ANLIR2       35.1       35.2    0.3              35.4       0.9             37.9     38.0    0.3             38.3       1.1\n                    ANLIR3       37.1       37.5    1.1              37.8       1.9             40.9     41.0    0.2             41.1       0.5\n                          StoryCloze    79.0       79.2    0.3              79.5       0.6             87.8     87.9    0.1             87.9       0.1\n\n             CLS   Amazon       65.2       66.4    1.8      1.4     67.7       3.8      3.3     68.2     68.7    0.7      0.6    69.4       1.8      1.6\n                       DBPedia      70.5       71.2    1.0              72.5       2.8             83.9     84.2    0.4             85.1       1.4\n\n           QA   DROP         67.9       68.2    0.4      1.9     69.9       2.9      5.6     51.6     51.9    0.6      1.3    52.8       2.3      3.2\n                NQ           15.1       15.4    2.0              16.1       6.6             19.6     20.1    2.6             20.8       6.1\n                      RealTimeQA   29.0       30.2    4.1              31.5       8.6             36.0     36.5    1.4             37.2       3.3\n                    WebQs        34.8       35.1    0.9              36.3       4.3             44.1     44.3    0.5             44.6       1.1\n\nTable 5: Zero-shot domain transfer performance based on BLOOM and GPT-J with original, SFT-optimized and\nMAPO-Optimized prompts. CLS: Classification, M: MAPO. The (↑(%) ) and ↑(%) represent the increase degree of\nMAPO-optimized prompts compared with original prompts in each dataset and task, respectively (The same below).\n\n Task     Dataset   LLaMA  +SFT  (↑(%))  ↑(%)  +MAPO  (↑(%))  ↑(%)    phrase “the term used” and substitutes “refer to”\nRS       BoolQ                      76.5                                76.6    0.1                                                   0.1                                                        76.7       0.3\n                                                                               0.5     with “denote”, while LLaMA adopts a more in-      PIQA                      79.8                                79.9                                          0.1                                                        80.0                                                                      0.3\n      SIQA                      48.9                                48.9                                          0.0                                                        49.0                                                                      0.2\n       HellaSwag                      76.1                                76.2                                          0.1                                                        76.5                                                                      0.5          formal style by mentioning the “commonly used\n       WinoGrande   70.1      70.2    0.1             70.5       0.6          term”. In the generation task, both BLOOM and\n      ARC-e        72.8      72.9    0.1             73.2       0.6\n      ARC-c        47.6      47.6    0.0             47.8       0.4         GPT-J present similar prompts that emphasize topic\n     OBQA        57.2      57.4    0.3             57.9       1.2           coverage. LLaMA maintains the original sentence\nQA   NQ           16.8      17.2    2.4      1.4    18.1       7.7      4.7\n     RACE        50.0      50.2    0.4             50.8       1.6           structure but modifies the subjects and replaces\n                                                  “decorate” with “adorn”. In the classification task,\nTable 6: Zero-shot domain transfer performance based\n                                                                all three LLMs rearrange the word order and of-\non LLaMA with original, SFT-optimized and MAPO-\n                                                         fer additional details about the topic. Therefore,Optimized prompts. RS: Commonsense Reasoning.\n                              MAPO demonstrates its prompt optimization capa-\nsuggests that in a task like prompt optimization,                                                                 bilities by adapting better prompts to specific tasks\nintroducing a stable (low temperature) but slight                                                        for different LLMs while preserving core informa-\ndegree of variability (non-zero temperature) yields                                                        tion and adjusting tone or structure as necessary.\nthe best results.                                               However, there are also some errors during\n4.4  Case Study and Error Analysis         prompt optimization, including prompts with in-\n                                                complete sentences, prompts with improper prepo-\nWe conduct a case study to visualize the prompts                                                         sitions or missing necessary parts, and prompts\noptimized by MAPO, as shown in Table 7. Addi-                                                 with ambiguous meanings, etc. Therefore, there is\ntional cases are included in Appendix G. We first                                           ample room for improvement in MAPO to better\nobserve that the majority of the original prompts                                                  adapt to different LLMs in downstream tasks.\nhave significant modifications after optimization\n                                               4.5  Exploratory Analysisthrough our MAPO method. Only about 10% of\nthe generated prompt pairs remain completely un-  We conduct an exploratory analysis to further inves-\nchanged. To further quantify these changes, we    tigate the patterns in optimized prompt as shown in\ncalculate a normalized edit distance. Given the    Fig. 7, Fig. 8 and Fig. 9. We extract the three most\nvarying lengths of different prompt pairs, we di-   frequent words from the original prompt and inves-\nvide the edit distance by the average length of the    tigate their distribution in the optimized prompt for\ntwo strings. This yields a value between 0 and 1,   each LLM, while either retaining high-frequency\nwhere 0 indicates identical strings and 1 indicates   words in instructions (including sentence, topics,\ncompletely different strings. The average normal-    subjects, present, statement, discussed, mentioned,\nized edit distance for all prompt pairs stands at    included, following) or removing them.\n0.67, demonstrating that most prompts do experi-     Taking the generation task (Fig. 7) as an exam-\nence substantial modifications.                          ple, when high-frequency words in instructions are\n  Next, we provide a detailed examination of these    retained, we observe that BLOOM retains a rel-\nmodifications. In the QA task, BLOOM transforms    atively higher proportion of the original prompts\nactive voice into passive voice, GPT-J utilizes the   compared to GPT-J and LLaMA, while LLaMA\n\nTask  Prompts\n\n          Original: How do people in Mexico refer to the largest Presbyterian church?\n      BLOOM: What is the way in which the biggest Presbyterian church is referred to by individuals in Mexico?\n QA\n        GPT-J: What is the term used by Mexicans to denote the biggest Presbyterian church?\n       LLaMA: What is the commonly used term for the biggest Presbyterian church in Mexico?\n\n          Original: What are the topics in the sentence: The furniture in the living room is decorated with flowers.\n      BLOOM: What topics does the sentence “The living room furniture is adorned with flowers” cover?\n  Gen\n        GPT-J: What topics are covered in the sentence “The living room furniture is adorned with flowers”?\n       LLaMA: Which subjects are present in the statement: Flowers adorn the furniture in the living room.\n\n          Original: Reincarnated Napster Goes Public Roxio bought the Napster name in a bankruptcy auction two years ago. Now, the company dumps its CD-burning software business\n           to concentrate on selling and delivering music over the web under the Napster name.What label best describes this news article?\n      BLOOM: Roxio, the buyer of Napster’s name in a bankrupt auction two years ago, has taken the reincarnated Napster public. The company has made the decision to abandon\n            their CD-burning software business and focus solely on the distribution and sale of music online via the Napster name. What genre of news article would this story fall under?\n  Class\n        GPT-J: Roxio, which acquired the Napster name through a bankruptcy auction two years ago, has taken Napster public again after rebranding it as a music delivery service. The\n        company has divested its CD-burning software business and now focuses solely on selling music online under the Napster name. What category would this news article fall under?\n       LLaMA: The Napster name, which was purchased by Roxio in a bankruptcy auction two years ago, has now been resurrected with a public launch. Roxio has shifted its focus\n           solely to the sale and distribution of music under the Napster name, leaving its CD-burning software business behind. What category would you assign to this news article?\n\n\n         Table 7: Original prompts and MAPO-optimized prompts for three LLMs in various downstream tasks.\n\nretains the fewest. When these words are removed,   LLMs. Moreover, some research focuses on incor-\nwe notice that BLOOM has a higher proportion of    porating additional knowledge to enhance prompt\nwords like “man”, “view” in its optimized prompts,    editing.  For example, Li et al. (2023) propose\nwhich are more relative with human. GPT-J has a   DSP to generate “directional stimulus” of each in-\nhigher proportion of words like “match”, “grass”,    put. Qin and Eisner (2021a) optimize a mixture\n“bathroom”, “white”, which suggests it focuses on    of prompts using gradient descent to generate rela-\nspecific scenes, objects, or themes. LLaMA has a    tional knowledge. Shin et al. (2020) develop Auto-\nhigher proportion of words like “room”, “close”,   prompt, a gradient-guided approach to find the best\n“playing”, indicating its preferences on place and    tokens in the prompt. Jiang et al. (2020) propose\nexperiences. The variations observed in word distri-   mining-based and paraphrasing-based methods to\nbution indicate that each LLM tends to emphasize    automatically generate diverse prompts. Further-\ndifferent aspects during the optimization process.   more, some research focus on continuous prompt\nAccurate conclusions need more experiments.       optimization instead of discrete prompt optimiza-\n                                                       tion mentioned before, such as research by Zheng\n5  Related Work                                 et al. (2023), Hambardzumyan et al. (2021), Zhong\n                                                           et al. (2021), etc. However, all above-mentionedLLMs’ prompt optimization process  involves\n                                             prompts optimization approaches aim to obtainprompt retrieval, prompt generation from scratch\n                                                       task-specific prompts instead of model-specificand prompt editing. For prompt retrieval, for ex-\n                                                   ones. Different from theirs, we dedicate at opti-ample, Ma et al. (2023) adopt greedy search to\n                                               mizing prompts for LLMs within the NLP domainidentify near-optimal prompts. Zhou et al. (2022)\n                                             and achieve impressive performance.introduce APE for automatic instruction selection,\netc. For prompt generation from scratch, Pang et al.                                 6  Conclusions\n(2023) introduce SharpT, which learns a shared\nlatent space and generates soft prompts. White   The remarkable capabilities of LLMs have revolu-\net al. (2023) describe a catalog of prompt engineer-   tionized NLP in various tasks. However, their per-\ning techniques. Zamfirescu-Pereira et al. (2023)   formance heavily relies on the quality of prompts.\ninvestigate end-user prompt engineering using a    In this work, we address the prompt optimization\nprototype LLM-based chatbot design tool. Wang    challenge by proposing a Model-Adaptive Prompt\net al. (2022a) present Self-Instruct for improving   Optimization (MAPO) approach. Through exten-\ninstruction-following capabilities of PLMs. For    sive experiments, we demonstrated that MAPO\nprompt editing, Gao et al. (2020) automatically se-   can adapt different LLMs with generating model-\nlect label words and generate templates. Pryzant    friendly prompts to enhance their capabilities\net al. (2023) introduce APO based on “gradients”   across various downstream tasks. In future work,\nto provide critical feedback on the current prompt.  we aim to construct more fine-grained model-\nDeng et al. (2022) propose RLprompt based on RL.   adaptive prompts that can adapt to the continuously\nZhang et al. (2023) propose TEMPERA, which    evolving data encountered in real-world production\nprovides interpretable prompts for different queries.   environments. Additionally, we intend to enhance\nPrasad et al. (2022) introduce GrIPS, a gradient-    its applicability across a broad spectrum of linguis-\nfree approach for improving task instructions for     tic contexts.\n\nLimitations                                          Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\n                                                            berg, et al. 2023. Sparks of artificial general intelli-\nIt is important to acknowledge certain limitations of      gence: Early experiments with gpt-4. arXiv preprint\nour approach. Firstly, the effectiveness of prompt      arXiv:2303.12712.\noptimization heavily relies on the availability and\n                                             Yuyan Chen, Qiang Fu, Ge Fan, Lun Du, Jian-Guang\nquality of the warm-up dataset. In cases where the\n                                                    Lou, Shi Han, Dongmei Zhang, Zhixu Li, and\ndataset is limited or does not sufficiently cover the     Yanghua Xiao. 2023a. Hadamard adapter: An ex-\nspecific task, the performance gains from prompt      treme parameter-efficient adapter tuning method for\noptimization may be constrained.  Additionally,       pre-trained language models. In Proceedings of the\n                                                  32nd ACM International Conference on InformationMAPO requires extensive SFT and RL, which can\n                                                 and Knowledge Management, pages 276–285.\nbe computationally expensive and time-consuming.\nThis could limit the scalability of MAPO, es-   Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen,\npecially when dealing with large-scale tasks or     Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li,\n                                                   and Yanghua Xiao. 2023b. Hallucination detection:datasets. Despite these limitations, our study pro-\n                                                      Robustly discerning reliable answers in large lan-\nvides valuable insights into model-adaptive prompt                                                   guage models.  In Proceedings of the 32nd ACM\noptimization for LLMs and contributes to the ongo-       International Conference on Information and Knowl-\ning efforts in improving the performance of these      edge Management, pages 245–255.\nLLMs in practical applications.\n                                             Yuyan Chen, Zhixu Li, Jiaqing Liang, Yanghua Xiao,\nAcknowledgement                          Bang Liu, and Yunwen Chen. 2023c. Can pre-trained\n                                                        language models understand chinese humor? In Pro-\nThis work  is supported by Shanghai Munic-      ceedings of the Sixteenth ACM International Confer-\nipal  Science and  Technology Major  Project      ence on Web Search and Data Mining, pages 465–\n                                                        480.\n(No.2021SHZDZX0103), Science and Technol-\nogy Commission of Shanghai Municipality Grant   Yuyan Chen, Yanghua Xiao, Zhixu Li, and Bang Liu.\n(No. 22511105902), the National Natural Science      2023d.  Xmqas:  Constructing complex-modified\nFoundation of China (No.62072323, U21A20488),      question-answering dataset for robust question un-\n                                                            derstanding. IEEE Transactions on Knowledge and\nShanghai Science and Technology Innovation Ac-\n                                                 Data Engineering.\ntion Plan (No.  22511104700), Key Projects of\nIndustrial Foresight and Key Core Technology Re-   Yuyan Chen, Yanghua Xiao, and Bang Liu. 2022. Grow-\nsearch and Development in Suzhou(SYC2022009).       and-clip: Informative-yet-concise evidence distilla-\n                                                              tion for answer explanation. In 2022 IEEE 38th In-\n                                                              ternational Conference on Data Engineering (ICDE),\nReferences                                        pages 741–754. IEEE.\n\nSimran Arora, Avanika Narayan, Mayee F Chen, Lau-   Yuyan Chen, Yichen Yuan, Panjun Liu, Dayiheng Liu,\n   rel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-     Qinghao Guan, Mengfei Guo, Haiming Peng, Bang\n   eric Sala, and Christopher Ré. 2022. Ask me any-       Liu, Zhixu Li, and Yanghua Xiao. 2024a. Talk funny!\n   thing: A simple strategy for prompting language mod-      a large-scale humor response dataset with chain-of-\n   els. arXiv preprint arXiv:2210.02441.                humor interpretation. In Proceedings of the AAAI\n                                                     Conference on Artificial Intelligence, volume 38,\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo      pages 17826–17834.\n  Giampiccolo. 2009.  The fifth pascal recognizing\n   textual entailment challenge. In TAC. Citeseer.       Yuyan Chen, Jin Zhao, Zhihao Wen, Zhixu Li, and\n                                                Yanghua Xiao. 2024b. Temporalmed: Advancing\nSamuel R Bowman, Gabor Angeli, Christopher Potts,      medical dialogues with time-aware responses in large\n  and Christopher D Manning. 2015. A large annotated      language models. In Proceedings of the 17th ACM\n   corpus for learning natural language inference. arXiv       International Conference on Web Search and Data\n   preprint arXiv:1508.05326.                            Mining, pages 116–124.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie    Ido Dagan, Oren Glickman, and Bernardo Magnini.\n   Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind      2005. The pascal recognising textual entailment chal-\n   Neelakantan, Pranav Shyam, Girish Sastry, Amanda       lenge.  In Machine learning challenges workshop,\n   Askell, et al. 2020. Language models are few-shot      pages 177–190. Springer.\n   learners. Advances in neural information processing\n   systems, 33:1877–1901.                              Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\n                                                     Transforming question answering datasets into nat-\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-       ural language inference datasets.  arXiv preprint\n   dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,      arXiv:1809.02922.\n\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan   Chin-Yew Lin. 2004. Rouge: A package for automatic\n  Wang, Han Guo, Tianmin Shu, Meng Song, Eric P       evaluation of summaries.  In Text summarization\n  Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing      branches out, pages 74–81.\n   discrete text prompts with reinforcement learning.\n  arXiv preprint arXiv:2205.12548.                   Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\n                                                         dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and     Luke Zettlemoyer, and Veselin Stoyanov. 2019.\n   Kristina Toutanova. 2018. Bert: Pre-training of deep      Roberta: A robustly optimized bert pretraining ap-\n   bidirectional transformers for language understand-      proach. arXiv preprint arXiv:1907.11692.\n   ing. arXiv preprint arXiv:1810.04805.\n                                            Weimin Lyu, Sheng Huang, Abdul Rafae Khan,\nBill Dolan and Chris Brockett. 2005.   Automati-                                                   Shengqiang Zhang, Weiwei Sun, and Jia Xu. 2019.\n   cally constructing a corpus of sentential paraphrases.                                                 Cuny-pku parser at semeval-2019 task 1:  Cross-\n   In Third International Workshop on Paraphrasing                                                             lingual semantic parsing with ucca. In Proceedings\n  (IWP2005).                                                                of the 13th international workshop on semantic eval-\n                                                              uation, pages 92–96.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\n  Making pre-trained language models better few-shot\n                                            Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang,\n   learners. arXiv preprint arXiv:2012.15723.\n                                                        Haibin Ling, Susmit Jha, and Chao Chen. 2024. Task-\n                                                          agnostic detector for insertion-based backdoor at-Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\n                                                                tacks. arXiv preprint arXiv:2403.17155.  William B Dolan. 2007. The third pascal recognizing\n   textual entailment challenge. In Proceedings of the\n                                           Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu,  ACL-PASCAL workshop on textual entailment and\n                                                          Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu,   paraphrasing, pages 1–9.\n                                                 Qinghua Hu, and Bingzhe Wu. 2023.   Fairness-\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo      guided few-shot prompting for large language mod-\n  Giampiccolo, Bernardo Magnini, and Idan Szpektor.        els. arXiv preprint arXiv:2303.13217.\n  2006. The second pascal recognising textual entail-\n  ment challenge. In Proceedings of the Second PAS-   Swaroop Mishra, Daniel Khashabi, Chitta Baral, and\n  CAL Challenges Workshop on Recognising Textual     Hannaneh Hajishirzi. 2021. Cross-task generaliza-\n   Entailment, volume 7.                                      tion via natural language crowdsourcing instructions.\n                                                        arXiv preprint arXiv:2104.08773.\nKaren Hambardzumyan,  Hrant  Khachatrian,  and\n  Jonathan May. 2021. Warp: Word-level adversarial   OpenAI. 2023. Gpt-4 technical report.\n  reprogramming. arXiv preprint arXiv:2101.00121.\n                                            Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022.       Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n  Optimizing prompts for text-to-image generation.      Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n  arXiv preprint arXiv:2212.09611.                      2022. Training language models to follow instruc-\n                                                              tions with human feedback.  Advances in Neural\nMinqing Hu and Bing Liu. 2004. Mining and summa-                                                         Information Processing Systems, 35:27730–27744.\n   rizing customer reviews. In Proceedings of the tenth\n ACM SIGKDD international conference on Knowl-                                         Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\n  edge discovery and data mining, pages 168–177.                                                               class relationships for sentiment categorization with\n                                                             respect to rating scales. arXiv preprint cs/0506075.\nEllen Jiang, Kristen Olson, Edwin Toh, Alejandra\n  Molina, Aaron Donsbach, Michael Terry, and Carrie J\n                                        Bo Pang, Semih Yavuz, Caiming Xiong, and Yingbo\n   Cai. 2022. Promptmaker: Prompt-based prototyping\n                                                   Zhou. 2023.  Sharpt: Shared latent space prompt\n  with large language models. In CHI Conference on\n                                                             tuning. In Findings of the Association for Computa-\n  Human Factors in Computing Systems Extended Ab-\n                                                                tional Linguistics: EACL 2023, pages 1214–1220.\n   stracts, pages 1–8.\n\n                                                     Archiki Prasad, Peter Hase, Xiang Zhou, and MohitZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\n                                                         Bansal. 2022. Grips: Gradient-free, edit-based in-  Neubig. 2020. How can we know what language\n                                                                 struction search for prompting large language models.  models know? Transactions of the Association for\n                                                        arXiv preprint arXiv:2203.07281.  Computational Linguistics, 8:423–438.\n\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:   Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\n  Optimizing continuous prompts for generation. arXiv      guang Zhu, and Michael Zeng. 2023.  Automatic\n   preprint arXiv:2101.00190.                          prompt optimization with\" gradient descent\" and\n                                              beam search. arXiv preprint arXiv:2305.03495.\nZekun Li, Baolin Peng, Pengcheng He, Michel Galley,\n   Jianfeng Gao, and Xifeng Yan. 2023. Guiding large   Guanghui Qin and Jason Eisner. 2021a. Learning how\n  language models via directional stimulus prompting.       to ask: Querying lms with mixtures of soft prompts.\n  arXiv preprint arXiv:2302.11520.                       arXiv preprint arXiv:2104.06599.\n\nGuanghui Qin and Jason Eisner. 2021b. Learning how   Alex Wang, Amanpreet Singh, Julian Michael, Felix\n   to ask: Querying LMs with mixtures of soft prompts.       Hill, Omer Levy, and Samuel R Bowman. 2018.\n   In Proceedings of the 2021 Conference of the North      Glue: A multi-task benchmark and analysis platform\n  American Chapter of the Association for Computa-       for natural language understanding. arXiv preprint\n   tional Linguistics: Human Language Technologies,      arXiv:1804.07461.\n  pages 5203–5212, Online. Association for Computa-\n                                           Ben Wang and Aran Komatsuzaki. 2021.   Gpt-   tional Linguistics.\n                                                                j-6b:  A  6  billion  parameter  autoregressive\nRajkumar  Ramamurthy,  Prithviraj  Ammanabrolu,      language model.    https://github.com/kingoflolz/\n  Kianté Brantley, Jack Hessel, Rafet Sifa, Christian       mesh-transformer-jax.\n  Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.\n                                                Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-  2022.  Is reinforcement learning (not) for natural\n                                                                 isa Liu, Noah A Smith, Daniel Khashabi, and Han-  language processing?: Benchmarks, baselines, and\n                                                    naneh Hajishirzi. 2022a. Self-instruct: Aligning lan-   building blocks for natural language policy optimiza-\n                                                    guage model with self generated instructions. arXiv   tion. arXiv preprint arXiv:2210.01241.\n                                                             preprint arXiv:2212.10560.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\n                                                Yizhong Wang, Swaroop Mishra, Pegah Alipoor-  Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\n                                                        molabashi,  Yeganeh  Kordi,  Amirreza  Mirzaei,   Chaffin, Arnaud Stiegler, Teven Le Scao, Arun\n                                                  Anjana Arunkumar, Arjun Ashok, Arut Selvan   Raja, et al. 2021. Multitask prompted training en-\n                                                     Dhanasekaran, Atharva Naik, David Stap,  et  al.   ables zero-shot task generalization. arXiv preprint\n                                                      2022b. Benchmarking generalization via in-context  arXiv:2110.08207.\n                                                                 instructions on 1,600+ language tasks. arXiv preprint\nTeven Le Scao, Angela Fan, Christopher Akiki, El-      arXiv:2204.07705.\n   lie Pavlick, Suzana  Ili´c, Daniel Hesslow, Roman\n                                                            Jules White, Quchen Fu, Sam Hays, Michael Sandborn,   Castagné, Alexandra Sasha Luccioni, François Yvon,\n                                                        Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse  Matthias Gallé,  et  al. 2022.  Bloom: A 176b-\n                                                       Spencer-Smith, and Douglas C Schmidt. 2023. A  parameter open-access multilingual language model.\n                                                   prompt pattern catalog to enhance prompt engineer-  arXiv preprint arXiv:2211.05100.\n                                                          ing with chatgpt. arXiv preprint arXiv:2302.11382.\nTimo Schick and Hinrich Schütze. 2020.   Exploit-\n                                               Adina Williams, Nikita Nangia, and Samuel R Bow-\n   ing cloze questions for few shot text classification\n                                                 man. 2017. A broad-coverage challenge corpus for\n  and natural language inference.   arXiv preprint\n                                                        sentence understanding through inference.  arXiv\n  arXiv:2001.07676.\n                                                             preprint arXiv:1704.05426.\nJohn Schulman,  Filip Wolski,  Prafulla Dhariwal,\n                                             Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\n  Alec Radford, and Oleg Klimov. 2017.   Proxi-\n                                                   Songfang Huang, and Fei Huang. 2023.   Rrhf:\n  mal policy optimization algorithms. arXiv preprint\n                                              Rank responses to align language models with\n  arXiv:1707.06347.\n                                             human feedback without tears.   arXiv preprint\n                                                        arXiv:2304.05302.Taylor Shin, Yasaman Razeghi, Robert L Logan IV,\n   Eric Wallace, and Sameer Singh. 2020. Autoprompt:                                          JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern\n   Eliciting knowledge from language models with                                                    Hartmann, and Qian Yang. 2023. Why johnny can’t\n   automatically generated prompts.  arXiv preprint                                                      prompt: how non-ai experts try (and fail) to design\n  arXiv:2010.15980.                                                      llm prompts. In Proceedings of the 2023 CHI Con-\n                                                          ference on Human Factors in Computing Systems,Richard Socher, Alex Perelygin, Jean Wu, Jason\n                                                      pages 1–21.  Chuang, Christopher D Manning, Andrew Y Ng, and\n   Christopher Potts. 2013. Recursive deep models for                                                      Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\n   semantic compositionality over a sentiment treebank.                                                     urmans, and Joseph E Gonzalez. 2023.  Tempera:\n   In Proceedings of the 2013 conference on empiri-                                                           Test-time prompt editing via reinforcement learning.\n   cal methods in natural language processing, pages                                                           In The Eleventh International Conference on Learn-\n  1631–1642.                                                          ing Representations.\n\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing                                               Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\n  Huang, and Xipeng Qiu. 2022. Black-box tuning for                                                              Character-level convolutional networks for text classi-\n   language-model-as-a-service. In International Con-                                                                    fication. Advances in neural information processing\n   ference on Machine Learning, pages 20841–20855.                                                            systems, 28.\n  PMLR.\n                                               Yuanhang Zheng, Zhixing Tan, Peng Li, and Yang Liu.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-                                                        2023. Black-box prompt tuning with subspace learn-\n   preet Singh, Julian Michael, Felix Hill, Omer Levy,                                                                 ing. arXiv preprint arXiv:2305.03518.\n  and Samuel Bowman. 2019.  Superglue: A stick-\n   ier benchmark for general-purpose language under-   Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\n   standing systems. Advances in neural information       Factual probing is [mask]: Learning vs. learning to\n   processing systems, 32.                                        recall. arXiv preprint arXiv:2104.05240.\n\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,     it’s O(N × M × log M) + O(T).\n   Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy      Inference Phase.  In the inference phase, an\n  Ba. 2022. Large language models are human-level\n                                                optimized prompt is generated from an original\n  prompt engineers. arXiv preprint arXiv:2211.01910.\n                                              prompt using the MAPO technique. The time com-\nA  Candidate Prompts                     plexity here is dominated by a single feed-forward\n                                                     operation, which is O(T). There is almost negli-\nFor the nine datasets selected in P3, we present    gible extra space required, making the space com-\none prompt and the corresponding three candidate    plexity effectively O(1) for this phase.\nprompts for each dataset, as shown in Table 8.      We also caclulate how long it roughly takes for\nB  Training Details                       a complete training run. For a LLaMA-7B model\n                                                  running on four A100 80GB GPUs, SFT on a high-\nWe provide the training details as shown in Table 9.    scale task (such as the News classification task with\nOther hyper-parameters are set to default.           120,000 training data) takes about 8 hours, RL takes\nC  Computational Cost                  about 12 hours, and the complete MAPO process\n                                                    takes roughly 20 hours in total; For a Bloom-7B\nWhile the training phase is computationally inten-   model under the same hardware conditions, SFT\nsive, the generation phase is relatively lightweight.   takes about 5 hours, RL takes about 9 hours, and\nSpecifically, once the prompt optimizing model    the total time for MAPO takes about 14 hours; For\nMAPO is trained, the prompt generation process   a GPT-J-6B model, SFT takes about 10 hours, RL\nsimply involves a feed forward propagation to gen-   takes about 16 hours, and the total time for MAPO\nerate the optimal prompt instead of further opti-   takes about 26 hours.\nmization through SFT and RL, thus significantly\nreducing the computational complexity. We list the  D  Baselines\ncomputational complexity during the training and  We compared MAPO with several State-Of-The-\ninference phase:                                  Art (SOTA) prompt optimization baselines, includ-\n  Training Phase.   During the training phase,   ing the following:\ninitially, a warm-up dataset is established. This in-\nvolves generating candidate prompts using a model        • Finetuning (Devlin et al., 2018): Finetuning\nlike GPT-3.5. For each original prompt, 1000 can-        (few-shot) involves finetuning the entire lan-\ndidate prompts are generated. This leads to a time       guage model with a classification head using\nand space complexity of O(N × M), where N        a few-shot dataset.\nis the number of original prompts, and M is the\n                                                                 • Soft Prompt (Qin and Eisner, 2021b; Linumber of candidates per prompt. Subsequently,\n                                                and Liang, 2021): Soft Prompt Tuning uti-an optimal prompt is searched for, which involves\n                                                               lizes continuous embeddings as a variant ofcomparisons among candidate prompts, yielding\n                                                          parameter-efficient transfer learning, replac-complexities of O(N × M) for both time and\n                                                       ing discrete prompts.space. Building the prompt optimizer is the next\nstage.  Supervised fine-tuning (SFT) has a time\n                                                                 • Black-Box (Sun et al., 2022): Black-Box Tun-\ncomplexity of O(E × B × T), with E being the\n                                                       ing combines discrete and soft prompts, with\nnumber of epochs, B the batch size, and T the\n                                                           the soft part trained using gradient descent and\nnumber of model parameters. Its space complex-\n                                                         the discrete part optimized using a gradient-\nity mainly arises from model parameters and gra-\n                                                             free tuner.\ndients, which is O(T). For building the reward\nmodel, both time and space complexities are mainly                                                                 • Autoprompt (Shin et al., 2020): Autoprompt\nO(N × M × log M). The reinforcement learning                                                        incorporates discrete trigger tokens and up-\n(RL) part requires O(E′ × B′ × T) time, where E′                                                        dates prompts  through  iterative  gradient\nis the number of epochs specific to RL, B′ is the                                                           search.\nbatch size in RL, and T remains the model param-\neters. The space complexity is O(T). Summing        • Manual (Brown et  al., 2020; Schick and\nthese up, the total time complexity for the training        Schütze, 2020; Sanh et al., 2021): Manual\nphase becomes O(N × M) + O(E × B × T) +       prompt achieves strong performance on vari-\nO(N ×M ×log M)+O(E′ ×B′ ×T). For space,       ous natural language understanding and natu-\n\nTask      Prompts\n\n              Original: Question: “Which happened earlier, the Chinese entered the war or President Truman dispatched the United States Seventh Fleet to the Taiwan Strait?”. Context:\n           “On 27 June 1950, ...”. Answer:\n            Candidate 1: Question: “Did President Truman dispatch the United States Seventh Fleet to the Taiwan Strait before or after the Chinese entered the war?” Context:“On 27\n             June 1950, ...”. Answer:\n  AdverQA\n            Candidate 2: Question: “Did the Chinese enter the war before President Truman dispatched the United States Seventh Fleet to the Taiwan Strait, or vice versa?” Context:“On 27\n             June 1950, ...”. Answer:\n            Candidate 3: Question: “Did the Chinese enter the war first or did President Truman send the United States Seventh Fleet to the Taiwan Strait earlier?”. Context:“On 27 June\n             1950, ...”. Answer:\n\n              Original: What’s above the muscles and needs direct sunlight Which is the correct answer? Options: ...\n            Candidate 1: What lies beyond the muscles and requires direct exposure to sunlight? Which is the correct answer? Options: ...\n OpenQA\n            Candidate 2:What is located above the muscles and requires direct sunlight? Which is the correct answer? -Options: ...\n            Candidate 3: Which body part requires direct sunlight and is located higher than the muscles? Which is the correct answer? Options: ...\n\n              Original: Q: What kind of relationship between glucagon and insulin is vital to managing fuel storage and consumption by body cells? A:\n            Candidate 1:Q: What is the essential connection between glucagon and insulin for regulating fuel storage and utilization in body cells? A:\n  CloseQA\n            Candidate 2:Q: In managing the storage and consumption of fuel by body cells, what is the crucial interrelation between insulin and glucagon? A:\n            Candidate 3: Q: What is the crucial connection between glucagon and insulin in regulating the storage and utilization of fuel by the cells in the body? A:\n\n              Original: Reincarnated Napster Goes Public Roxio bought the Napster name in a bankruptcy auction two years ago. Now, the company dumps its CD-burning software\n              business to concentrate on selling and delivering music over the web under the Napster name. What label best describes this news article?\n            Candidate 1: Roxio, the buyer of Napster’s name in a bankrupt auction two years ago, has taken the reincarnated Napster public. The company has made the decision to\n            abandon their CD-burning software business and focus solely on the distribution and sale of music online via the Napster name. What genre of news article would this story\n                  fall under?\n  News      Candidate 2: Roxio, which acquired the Napster name through a bankruptcy auction two years ago, has taken Napster public again after rebranding it as a music delivery\n               service. The company has divested its CD-burning software business and now focuses solely on selling music online under the Napster name. What category would this news\n                 article fall under?\n            Candidate 3:The Napster name, which was purchased by Roxio in a bankruptcy auction two years ago, has now been resurrected with a public launch. Roxio has shifted\n                   its focus solely to the sale and distribution of music under the Napster name, leaving its CD-burning software business behind. What category would you assign to this news\n                article?\n\n              Original: writer/director joe carnahan’s grimy crime drama is a manual of precinct cliches , but it moves fast enough to cover its clunky dialogue and lapses in logic . The\n              sentiment expressed for the movie is\n            Candidate 1: The gritty crime drama by writer and director Joe Carnahan may rely heavily on familiar tropes and cliches of the genre, but its quick pace manages to distract\n            from any awkward dialogue and illogical moments. The sentiment expressed for the movie is\n  Movie\n            Candidate 2: Joe Carnahan´s gritty crime drama relies heavily on standard police procedures, yet its rapid pace compensates for any cumbersome dialogues and unreasonable\n               plot holes. The sentiment expressed for the movie is\n            Candidate 3:Although writer/director Joe Carnahan´s gritty crime drama contains numerous stereotypes within the precinct environment, its swift pace effectively masks its\n           awkward dialogue and occasional lapses in logic. The sentiment expressed for the movie is\n\n              Original:If I tell you that Hydrogen bonds cause a tremendous force when a substance freezes, and ask you the question “hydrogen bonds cause a tremendous force when a\n              substance does what”, is the correct answer “strong”?\n            Candidate 1: If I were to inform you that when a substance freezes, Hydrogen bonds create a significant force and ask, “What term describes the force generated by Hydrogen\n            bonds when a\n QASC      substance freezes?”, would the appropriate response be “Powerful”?\n            Candidate 2: Suppose I inform you that the process of substance freezing is deeply influenced by Hydrogen bonds that generate an enormous force. Now, if I inquire, “What\n              occurs when the substance undergoes this process?”, would it be accurate to say that the force generated is “powerful”?\n            Candidate 3: Suppose I inform you that when a substance freezes, Hydrogen bonds result in a remarkable force, and inquire, “When a substance undergoes what, do hydrogen\n            bonds cause a remarkable force?” Would it be accurate to respond with “robust”?\n\n              Original: What are the topics in the sentence: A bathroom with the toilet missing and the room fairly torn up.\n            Candidate 1: What are the subjects of the sentence: A torn-up room without a toilet.\n  Topics\n            Candidate 2: Which subjects are covered in the phrase “A bathroom that lacks a toilet and has a considerably damaged room”?\n            Candidate 3:What are the subjects mentioned in the statement: A torn up room that lacks a toilet in the bathroom?\n\n              Original: Sum up the following dialogue: Gordon: Did you see my car, bro? Gordon: <file_photo> Gordon: It’s my first car ever! And I love it! :) Leo: Grats, bro! Leo: It\n              looks awesome, I have to see it with my own eyes! Gordon: Are you home? Leo: Yeah Gordon: Look out of the kitchen window :) Leo: No shit :D Leo: Wait, I’m coming!\n            Gordon: Waiting :D\n            Candidate 1: Sum up the following dialogue: Gordon asked, “Bro, have you seen my car?” and sent a file photo. He expressed his excitement saying it´s his first ever car and\n             he loves it. Leo congratulated him saying it looks awesome and expressed his wish to see it in person. Gordon asked if he was home and told him to look out of the kitchen\n            window. Leo was surprised and replied, “No shit :D” and said he was coming. Gordon eagerly waited for him.\n  Summary\n            Candidate 2: Sum up the following dialogue: Gordon inquires, “Hey bro, have you laid eyes on my car?” Gordon shares a photograph of his first vehicle and expresses his\n              adoration for it with a smiley face. Leo congratulates him and expresses interest in seeing it in person. Gordon asks if Leo is home and instructs him to look out of the kitchen\n            window. Leo is surprised and excited, responding with laughter and promising to come see it. Gordon waits patiently.\n            Candidate 3: Sum up the following dialogue: Gordon asked his brother if he had seen his car and sent a photo of it. He expressed his love for it as it was his first car ever. Leo\n              congratulated him and expressed his desire to see the car in person. Gordon asked if he was at home and told him to look out of the kitchen window. Leo was surprised and excited\n            and said he would be coming soon. Gordon waited for him to arrive.\n\n              Original: Question: What does a Christian do when they get what they needed? Options:... The answer is “thank god” because\n            Candidate 1: Question: How should a Christian proceed after they have received what they required? Options: ... The answer is “thank god” because\n  Explan\n            Candidate 2: Question: When a Christian receives what they needed, what actions do they take? Options:... The answer is “thank god” because\n            Candidate 3: Question: What should a Christian do upon receiving what they required? Options: ... The answer is “thank god” because\n\n\nTable 8: One sample prompt and the corresponding three candidate prompts generated by GPT-3.5 for each selected dataset in\nP3.\n\n      ral language generation tasks without relying        • GrIPS (Prasad et al., 2022): GrIPS performs\n    on training examples.                               phrase-level editing on the instructions and\n                                                              selects the best one.\n   • In-Context (Brown et al., 2020): In-Context\n    Demonstration randomly selects a training        • RLprompt (Deng et al., 2022): RLprompt gen-\n    example and concatenates it with the input         erates discrete prompts using a reinforcement\n     query.                                               learning (RL) framework.\n\n   • Instructions: Self-Instruction manually cre-       • TEMPERA (Zhang et al., 2023): TEMPERA\n     ates prompts for each task following Natural           is a test-time prompt editing method that uses\n     Instructions (Wang et al., 2022b), where the        reinforcement learning, efficiently leverag-\n    prompt is concatenated with the inputs.             ing prior knowledge and adapting to differ-\n\nValue     E  Datasets\n          Gradient Accumulation Steps    8         We  utilized nine representative datasets from\n         Weight Decay                   0.1                                        P3 (Sanh et al., 2021) to establish the warm-up\n         Learning Rate for Actor Model  2e-5\n                                                        dataset, covering question-answering, classifica-         Learning Rate for Critic Model  1e-5\n         Entropy Coefficient             0.005             tion, and generation tasks. The selected datasets\n         Value Loss Coefficient           0.5               for each task are as follows:\n         Mini Batch Size              32\n           Positive Lambda Coefficient     2.0                    • Question-Answering   Task:    AdverQA\n         Negative Lambda Coefficient    1.8\n                                                          (https://huggingface.co/datasets/bigscience/\n     GAMMA                      0.99\n                                                        P3/tree/main/data/adversarial_qa_       Adam Optimizer Epsilon        1e-5\n      GAE Lambda                  0.95                 dbidaf_question_context_answer),\n       Max Gradient Norm             0.5             OpenQA            (https://huggingface.co/\n       PPO Epochs                 20                   datasets/bigscience/P3/tree/main/data/\n          Clip Parameter                  0.2\n                                                    openbookqa_main_which_correct), CloseQA\n                                                          (https://huggingface.co/datasets/bigscience/\n Table 9: Hyperparameters used for MAPO in all the tasks.         P3/tree/main/data/sciq_Direct_Question_\n     ent queries, while providing an interpretable        Closed_Book_).\n    prompt for each query.\n                                                                 • Classification   Task:     News    (https:\n                                                         //huggingface.co/datasets/bigscience/P3/\n    • AMA (Arora et al., 2022): AMA recursively        tree/main/data/ag_news_classify),   Movie\n     reformats tasks and prompts using the LLM         (https://huggingface.co/datasets/bigscience/\n     to effectively aggregate predictions across        P3/tree/main/data/rotten_tomatoes_\n     prompts using weak supervision.                  Movie_Expressed_Sentiment),    QASC\n                                                          (https://huggingface.co/datasets/bigscience/\n                                                         P3/tree/main/data/qasc_is_correct_1)\nFor a fair assessment, we adopt the same exper-\nimental setup as in LM-BFF (Gao et al., 2020)        • Generation Task: Topics (https://huggingface.\nand RLPrompt (Deng et al., 2022). We take 16         co/datasets/bigscience/P3/tree/main/data/\ntraining samples from each class in our training        common_gen_topics_from_the_sentence),\ndataset for every task, making them our few-shot      Summary  (https://huggingface.co/datasets/\ndataset. So, if we consider all the classes (Y), we        bigscience/P3/tree/main/data/samsum_\nhave a total of 16 times the number of classes as        Sum_up_the_following_dialogue),  Explan\nour training samples. Similarly, we pick 16 sam-        (https://huggingface.co/datasets/bigscience/\nples from each class to form our validation dataset.        P3/tree/main/data/cos_e_v1.11_generate_\nBesides this usual setup, we also select n random         explanation_given_text).\nexamples from our training data. We call this our\n“in-context exemplar pool”. For consistency, we    We evaluate our proposed MAPO method, along\nrepeat our experiments four times using different   with other SOTA baselines, on the following\nrandom seeds. Afterward, we calculate the average    datasets for validation: SST-2 (Socher et al., 2013),\nresults and note down the usual variation we see be-   Yelp Polarity (Zhang et al., 2015), MR (Pang and\ntween the results. For our language model, we’ve   Lee, 2005), CR (Hu and Liu, 2004), RTE (Da-\nchosen to use RoBERTa large (Liu et al., 2019). We   gan et al., 2005; Haim et al., 2006; Giampiccolo\nbase our initial guidelines on the Natural Instruc-    et al., 2007; Bentivogli et al., 2009), QNLI (Dem-\ntions (Mishra et al., 2021). We also ensure that the   szky et al., 2018), SNLI (Bowman et al., 2015),\nfirst examples we give for context are randomly  MNLI (Williams et al., 2017), MRPC (Dolan and\npicked from a set of 16. This set is different from    Brockett, 2005). These datasets provide a compre-\nour few-shot dataset and is also randomly picked    hensive evaluation of MAPO’s performance com-\nfrom our main training data. By comparing MAPO   pared to other baselines across a range of tasks,\nwith these SOTA baselines, we gain insights into    including sentiment analysis, text classification,\nthe performance and effectiveness of MAPO in    natural language inference, and paraphrase identifi-\nvarious downstream tasks.                             cation, etc.\n\nSpecifically, for Table 3, the training data aligns    this part, but we happened to choose this subset.\nwith that used by TEMPERA (Zhang et al., 2023),   To mitigate the impact on the results, we also run\nthat is all experiments, including our own, use    another test with two steps: using the optimized\nRoberta-large as the backbone for validating the   prompts generated by MAPO and then using the\ndownstream tasks. Because the setup employs a    original Roberta-Large model to make inference.\n“few-shot” methodology that has elaborated before,  As shown in Table 3 (the row “MAPO-w/o g”),\nwe name Table 3 as “few-shot”. For Table 5 and 6,   the results do not show a significant decline, with\nthere is no training data involved; the LM performs   a t-test greater than 0.05. The use of data from\nzero-shot inference. That means all reported results   Table 3 for generalization is merely to ensure that\noccur without training on the datasets in Table 5    the prompt-optimized model retains its original\nand  6. The purpose is to demonstrate the gener-    capabilities for downstream tasks instead of data\nalization (domain transfer) ability of our MAPO    leakage.\nmethod. If one wishes to further enhance perfor-\n                     G  Additional Casesmance on these datasets, additional training with\nlabeled data on Table 5 and 6 becomes necessary.  We list more cases whose prompts have been opti-\n                                            mized by our proposed MAPO as shown in Table\nF  Additional Experiments              19. We make detailed analysis for the difference\n                                        among LLMs as follows:The performance of the reward model.  We\nplot the performance of the reward model during\n                                                                 • In SST-2, BLOOM and LLaMA both use\nthe training process of MAPO as shown in Fig. 4.\n                                                      phrases like “terrific flair” and “remarkable\nAs the training progresses, the reward model ex-\n                                                                skill” to describe Khouri’s ability, emphasiz-\nhibits consistent growth and improvement. The\n                                                        ing positive sentiment. GPT-J uses the phrase\nconsistent increase indicates that the reward model\n                                                   “tremendous artistry,” highlighting the artistic\nis gradually becoming more proficient in down-\n                                                             aspect, but does not explicitly convey the pos-\nstream tasks. It successfully adapts to its environ-\n                                                                   itive sentiment as strongly as BLOOM and\nment, leading to improved outcomes and higher\n                                        LLaMA.\ntask completion rates. Therefore, it can serve as\na discriminator of the goodness of an optimized\n                                                                 • In Yelp, BLOOM and LLaMA use phrases\nprompt.\n                                                                 like “quality of the food is commendable” and\n  The original capabilities maintaining ability                                                           “service provided is inconsistent” to provide a\nof MAPO. We further analyze the original capa-                                                     balanced assessment. GPT-J and the original\nbilities maintaining ability of MAPO. We use a                                                        version have the same wording, emphasizing\nlanguage model trained with MAPO, which has                                                          the hit-or-miss nature of the service.\nthe ability to optimize prompts but without losing\nits original capabilities, to modify prompts and ac-                                                                 • In MR, BLOOM and LLaMA use phrases like\ncomplish downstream tasks. We believe that the                                                     “admirable endeavor” and “praiseworthy pur-\nGLUE and SuperGLUE tasks are representative,                                                              suit” to highlight the positive qualities of the\nhence we use them as pre-training tasks. How-                                                         venture. GPT-J and the original version use\never, the improvements in Table 5 and 6 are not                                                             neutral language without explicitly conveying\nsignificant, possibly due to the limited scope of                                                             positive or negative sentiment.\nour pre-training tasks. Future work can explore\nusing a broader range of datasets for pre-training,                                                                 • In CR, BLOOM, GPT-J, and LLaMA all ex-\nwhich may lead to more significant improvements                                                          press confusion or potential confusion regard-\nin various downstream tasks.                            ing the positioning of the space key on a\n   Moreover, for Table 3, the training and validation        phone. The wording in BLOOM and LLaMA\ndata for SFT, RM, and RL are different from the        suggests that using a different key for text in-\ndata used for generalization, although they all come        put is more common in phones, implying a\nfrom Table 3. This is because we consider GLUE         deviation from the norm.\nand SuperGLUE tasks to be representative, hence\nwe use them as pre-training tasks. Theoretically, a        • In RTE, BLOOM and LLaMA emphasize the\nmore diverse NLP dataset should be selected for        impact of the situation by using phrases like\n\n0.9                                          0.9                                          0.9\n\n           0.8                                          0.8                                          0.8\n\n           0.7                                          0.7                                          0.7\n                                                                                                            BLOOM             Reward0.6                                          0.6                                          0.6                                              GPT-J\n                                                                                                                         LLaMA\n           0.5                                          0.5                                          0.5\n\n           0.4                                          0.4                                          0.4\n            0      5      10     15     20      0      5      10     15     20      0      5      10     15     20\n                     QA                                 Generation                                 Classification\n\n          Figure 4: The performance of the reward model in three LLMs during the training process of MAPO.\n\n                                                  60\n       100                                                                                 80                                         20\n                                                  50                                                                                  40        80\n                                                (%)40                                       60                                         60        60                                                                                                                            80\n                                                  30                                       40                                         100        40                                                                           Metric20\n        20                                       10                                       20\n         0                                        0                                        0\n                           Q-Adve   Q-Open   Q-Clos   C-News   C-Movi   C-QASC   G-Topi   G-Summ   G-Expl                  Q-Adve   Q-Open   Q-Clos   C-News   C-Movi   C-QASC   G-Topi   G-Summ   G-Expl                  Q-Adve   Q-Open   Q-Clos   C-News   C-Movi   C-QASC   G-Topi   G-Summ   G-Expl\n                    BLOOM                                        GPT-J                                LLaMA\n\nFigure 5: Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q: QA, C:\nclassification, G:generation. We only keep the first four letters of each dataset’s name in the figure.\n\n\n                              PPO                RRMF               MAPO\n           Tasks   Dataset\n                   BLOOM  GPT-J  LLaMA  BLOOM  GPT-J  LLaMA  BLOOM  GPT-J  LLaMA\n\n                 AdverQA     18.8        9.6       24.3       19.2        9.9       24.8       19.5       11.0      25.1\n       QA    OpenQA      26.9       20.7      16.0       27.0       20.9      15.8       27.2       21.0      16.1\n                  CloseQA       8.3        8.8       14.4        9.0        9.8       14.6        9.4       10.2      14.8\n\n                 News         96.9        6.1       12.1       98.3        6.2       12.2       98.7        6.3       12.4\n            Class   Movie        93.1       53.1      82.1       93.0       53.6      82.2       93.3       53.9      82.5\n              QASC        99.9       56.7      72.4       99.9       56.5      71.2       99.9       56.8      72.8\n\n                     Topics        35.9       23.1      18.9       35.2       23.2      19.1       36.2       23.4      19.5\n         Gen    Summary      50.1       17.2      11.7       49.4       17.7      12        50.2       17.8      12.2\n                    Explan         8.3       11.5       8.4         7.2       12.4       8.7         8.9       12.9       9.1\n\n\nTable 10: The separate effect of PPO and RRMF, which demonstrate the important roles played by both PPO and RRMF in\nenhancing the performance of MAPO.\n\n    “somber site” and “distressing sight” when de-         tain the same wording as the original sen-\n     scribing the washed-up marine animals. GPT-        tence 1. The variations in wording occur in\n     J and the original version provide more neutral        sentence 2, with BLOOM and GPT-J empha-\n     descriptions without explicitly conveying the         sizing the need for interest rates to increase,\n     emotional aspect.                                while LLaMA focuses on the importance of\n                                                       boosting savings.\n   • In QNLI, BLOOM, GPT-J, and LLaMA all\n     rephrase the sentence 2, maintaining the same        • In MRPC, BLOOM, GPT-J, and LLaMA all\n     overall meaning. The variations in wording        maintain the same wording as the original sen-\n     are mainly stylistic, with BLOOM, GPT-J,         tences. The variations in the rephrased sen-\n    and LLaMA using different synonyms to con-        tence 1 (BLOOM and LLaMA) emphasize\n    vey the same information.                           the 15 percent drop in revenue, while GPT-J\n                                                       maintains a more neutral tone.\n   • In SNLI, BLOOM, GPT-J, and LLaMA\n     rephrase the sentence 1 by adding additional  H  Additional Exploratory Analy-\n     details related to the slip and slide activity                                                    sis\n    and the celebratory context. The variations\n     in wording are mainly stylistic, enhancing the  We further analyze the distribution of the top 3\n     description of the baby’s experience and the   words from the original prompts in the optimized\n     wetness.                                  prompts of different LLMs in both the QA and\n                                                         classification tasks as shown in Fig. 8 and Fig.\n   • In MNLI, BLOOM, GPT-J, and LLaMA main-    9, respectively. In the QA task, we observe min-\n\n100                                       60                                       80\n                                                    50\n           80\n                                                    40                                       60       (%)                                                                                                                                   PPO           60\n               Metric 40                                       3020                                       40                                     RRMFMAPO\n           20                                       10                                       20\n            0                                        0                                        0\n                                       AdverQA   OpenQA   CloseQA  News  Movie  QASC   Topics   Summary   Explan                     AdverQA   OpenQA   CloseQA  News  Movie  QASC   Topics   Summary   Explan                     AdverQA   OpenQA   CloseQA  News  Movie  QASC   Topics   Summary   Explan\n                      BLOOM                                        GPT-J                                LLaMA\n\n           Figure 6: The separate effects of PPO and RRMF during the process of RL in constructing MAPO.\n\n\n                       BLOOM                               GPT-J                        LLaMA\n  Tasks    Dataset  20%   40%   60%   80%   100%  20%   40%   60%   80%   100%  20%   40%   60%   80%   100%\n\nQA     AdverQA   17.8     18.6     18.9     19.2     19.5      9.2       9.3       9.9       11.0     11.0     22.2     23.2     25.1     25.4     26.0\n        OpenQA   25.4     26.1     26.3     26.5     27.2     19.0     19.1     19.5     21.0     21.1     14.3     15.1     16.1     16.3     16.6\n         CloseQA   8.0       8.6       8.8       9.1       9.4       8.4       8.6       9.4       10.2     10.3     13.0     13.8     14.8     15.2     15.4\n Class    News      96.8     97.4     97.6     98.0     98.7      4.6       4.8       5.3       6.3       6.4       10.2     11.5     12.4     12.5     12.7\n         Movie      89.8     90.9     91.3     92.5     93.3     51.8     52.2     52.8     53.9     53.9     79.8     80.2     82.5     82.7     83.1\n       QASC     98.0     98.6     98.7     98.8     99.9     53.8     54.4     55.2     56.8     56.9     70.5     71.4     72.8     73.3     73.4\nGen      Topics     35.0     35.7     36       36.4     36.2     21.3     21.6     22.2     23.4     23.6     17.5     18.2     19.5     19.8     19.9\n        Summary   47.9     48.9     49.3     49.7     50.2     16.0     16.3     16.8     17.8     18.1     10.1     11.3     12.2     12.4     12.8\n          Explan     7.5       8.1       8.3       8.6       8.9       11.2     11.4     11.9     12.9     13.2      7.3       8.2       9.1       9.5       10.0\n\nAverage   -           47.4     48.1     48.4     48.8     49.3     21.7     22.0     22.6     23.7     23.8     27.2     28.1     29.4     29.7     30.0\n↓            -           1.9       1.2       0.9       0.5         -         2.1       1.9       1.3       0.1         -         2.8       1.9       0.6       0.3         -\n↓(%)       -           4.0       2.4       1.9       1.0         -         9.8       8.5       5.7       0.6         -         10.2      6.7       2.0      1           -\n\n Dataset    -         55822   111645  167468  223291  279113  55822   111645  167468  223291  279113  55822   111645  167468  223291  279113\nD-↓         -         223291  167468  111645  55822     -        223291  167468  111645  55822     -        223291  167468  111645  55822     -\nD-↓(%)    -          400.0    150.0    66.7     25.0       -        400.0    150.0    66.7     25.0       -        400.0    150.0    66.7     25.0       -\n\nTable 11: Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q:\nQA, C: classification, G:generation. ↓means the number of performance decline. ↓(%) means the percentage of\nperformance decline. D-↓means the number of data reduction. D-↓(%) means the percentage of data reduction.\n\nimal variations when considering whether to re-   process. Additional experiments will contribute to\nmove the instruction. After prompt optimization,   a more comprehensive understanding of the prompt\nBLOOM has a higher proportion of words like    optimization dynamics exhibited by each LLM.\n“contemporary”, “french”, “Methodist”, “places”,\n“education”, “power”, and “life” compared to the\nother two models. GPT-J has a higher proportion\nof words like “church”, “time”, “order”, “early”,\nand “year”, indicating a focus on temporal and se-\nquential aspects. And LLaMA has a higher propor-\ntion of words like “earlier”, “similar”, “number”,\n“song”, and “property” compared to the other two\nmodels. In the classification task, we also observe\nminimal variations when considering whether to re-\nmove the instruction. After optimization, BLOOM\nhas a higher proportion of the word “year”, “new”\ncompared to the other two models. GPT-J has a\nhigher proportion of words like “largest”, “music”,\n“national”, “school” and “poland”. LLaMA has a\nhigher proportion of words like “increase”, “gov-\nerment”, “executive”, “medical”, “warsaw”, and\n“parliament” compared to the other two LLMs.\n\n  These findings strongly suggest that each LLM\nexhibits unique preferences and patterns in prompt\noptimization across different tasks. The observed\nvariations in word distribution clearly indicate the\nspecific areas of focus and the semantic nuances\nthat each LLM emphasizes during the optimization\n\nSST-2  Yelp P. MR  CR  RTE  QNLI  SNLI  MNLI MRPC  Average\n\n            MAPO      96.1    93.5     90.2   88.9   75.3   63.1    60      55.7     79.3     78.0\n              MAPO-0    95.8    93.1     90.2   88.2   74.9   62.9    59.7    55.1     78.9     77.6\n                MAPO-0.2   95.9    93.3     90.3  88    75.2   63      59.8    54.8     78.3     77.6\n                MAPO-0.5   95.2    92.2     88.9   87.9   74.8   62.8    59.2   55      78.8     77.2\n                MAPO-0.8   95.3    92.3     88.7   87.9   74.5   62.7    59.2    54.9     78.7     77.1\n\nTable 12: The few-shot performance of MAPO with SOTA prompt optimizing baselines in downstream tasks.\nF: Finetuning, C: Continous prompt, D: Discrete prompt. MAPO means using MAPO with temperature [0,0.5].\nMAPO-0 means using MAPO with temperature 0.\n\n\n\n\n\n  Task    Natural Instructions\n\n           In this task, you are given sentences from movie reviews. The task is to classify the sentiment of the sentence. Your answer must be\n  SST-2\n            in the form of the letters “positive”, and “negative” respectively.\n\n           In this task, you are given sentences from Yelp reviews. The task is to classify the sentiment of the sentence. Your answer must be\n  Yelp\n            in the form of the letters “positive”, or “negative” respectively.\n\n           In this task, you are given sentences from movie reviews. The task is to classify the sentiment of the sentence. Your answer must be\n MR\n            in the form of the letters “positive”, or “negative” respectively.\n\n           In this task, you are given sentences from customer reviews. The task is to classify the sentiment of the sentence. Your answer must be\n CR\n            in the form of the letters “positive”, or “negative” respectively.’\n\n           In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree\n RTE\n            (entailment)/disagree (not entailment) with each other. Your answer must be in the form of the letters Yes, and No respectively.\n\n         You are given two sentences(Sentence1 and Sentence2). The task is to determine whether Sentence2 contains the answer to Sentence1.\n QNLI\n          Your answer must be in the form of the letters Yes, and No respectively.\n\n           In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree\n  SNLI    (entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the\n              letters “Yes”, “No”, and “Maybe” respectively.\n\n           In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree\n MNLI   (entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the\n              letters Yes, No, and Maybe respectively.\n\n           In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree\n MRPC\n            (entailment)/disagree (not entailment) with each other. Your answer must be in the form of the letters Yes, and No respectively.\n\n\n                           Table 13: Natural Instructions of various downstream tasks.\n\nTask   Dataset                     M\n                 BLOOM   (↑)   (↑(%))  GPT-J   (↑)  (↑(%))  LLaMA   (↑)   (↑(%))  ↑    ↑(%)\n\n      QA    AdverQA   19.5      6    44.4    11      8    266.7    25.1       21.9  684.4    12    331.8\n             OpenQA   27.2        1.3  5       21      4    23.5      16.1       2.8    21.1      2.7    16.5\n               CloseQA   9.4       3    46.9     10.2     3.3   47.8      14.8      4     37        3.4    43.9\n       CLS   News      98.7        5.9   6.4      6.3      6.3   -         12.4       11.3  1027.3   7.8    516.9\n               Movie      93.3        2.4   2.6      53.9     2.8   5.5       82.5       3.8    4.8      3      4.3\n            QASC     99.9        0.5   0.5      56.8     2.8   5.2       72.8       11.2   18.2      4.8   8\n      GEN    Topics     36.2        6.7   22.7     23.4     5.9   33.7      19.5       5.2    36.4      5.9    30.9\n              Summary   50.2        4.1   8.9      17.8     4.7   35.9      12.2       5.6    84.8      4.8    43.2\n                 Explan     8.9         3.2   56.1     12.9     4.4   51.8      9.1        2.2    31.9      3.3    46.6\n\n         Task   Dataset                                 M-0\n                 BLOOM   (↑)   (↑(%))  GPT-J   (↑)  (↑(%))  LLaMA   (↑)   (↑(%))  ↑    ↑(%)\n\n      QA    AdverQA   19.3        5.8  43       10.5     7.5  250      24.8       21.6  675      11.6  322.7\n             OpenQA   26.8        0.9   3.5      20.9     3.9   22.9      15.2       1.9    14.3      2.2    13.6\n               CloseQA   8.9         2.5   39.1     9.8      2.9  42      14         3.2    29.6      2.9    36.9\n       CLS   News      96.5        3.7  4        5.9      5.9   -         11.9       10.8  981.8     6.8    492.9\n               Movie      92.8        1.9   2.1      53.6     2.5   4.9      82         3.3    4.2       2.6    3.7\n            QASC     99.9        0.5   0.5      56.6     2.6   4.8       72.2       10.6   17.2      4.6    7.5\n      GEN    Topics     35.9        6.4   21.7    23       5.5   31.4     19         4.7    32.9      5.5    28.7\n              Summary   49.7        3.6   7.8     17       3.9   29.8      11.9       5.3    80.3      4.3    39.3\n                 Explan     7.8         2.1   36.8     12.3     3.8   44.7      8.8        1.9    27.5      2.6    36.3\n\n         Task   Dataset                                      M-0.2\n                 BLOOM   (↑)   (↑(%))  GPT-J   (↑)  (↑(%))  LLaMA   (↑)   (↑(%))  ↑    ↑(%)\n\n      QA    AdverQA  19         5.5   40.7     10.7     7.7  256.7    24.4       21.2  662.5    11.5  320\n             OpenQA   26.9      1    3.9      20.1     3.1   18.2      15.6       2.3    17.3      2.1    13.1\n               CloseQA   8.4       2    31.3    10       3.1   44.9      14.2       3.4    31.5      2.8    35.9\n       CLS   News      96.7        3.9   4.2      6.1      6.1   -         12.1      11    1000    7     502.1\n               Movie      92.9      2    2.2      53.2     2.1   4.1       81.5       2.8    3.6       2.3    3.3\n            QASC     99.9        0.5   0.5      56.6     2.6   4.8       72.3       10.7   17.4      4.6    7.6\n      GEN    Topics     36         6.5  22       23.3     5.8   33.1      19.3      5     35        5.8   30\n              Summary   50.2        4.1   8.9      16.6     3.5   26.7     12         5.4    81.8      4.3    39.1\n                 Explan     8.2         2.5   43.9     12.5    4    47.1     9          2.1    30.4      2.9    40.5\n\n         Task   Dataset                                      M-0.5\n                 BLOOM   (↑)   (↑(%))  GPT-J   (↑)  (↑(%))  LLaMA   (↑)   (↑(%))  ↑    ↑(%)\n\n      QA    AdverQA   18.8        5.3   39.3    10      7    233.3    23.6       20.4  637.5    10.9  303.4\n             OpenQA   27.1        1.2   4.6      20.4     3.4  20       15.8       2.5    18.8      2.4    14.5\n               CloseQA   8.3         1.9   29.7     8.5      1.6   23.2      14.2       3.4    31.5      2.3    28.1\n       CLS   News      96.2        3.4   3.7      5.7      5.7   -         10.3       9.2    836.4     6.1    420.1\n               Movie      93.2        2.3   2.5     53       1.9   3.7       81.6       2.9    3.7       2.4    3.3\n            QASC     99.9        0.5   0.5      56.5     2.5   4.6       70.6      9     14.6     4      6.6\n      GEN    Topics     35.5      6    20.3     21.9     4.4   25.1      18.9       4.6    32.2     5     25.9\n              Summary   49.2        3.1   6.7     17       3.9   29.8      11.1       4.5    68.2      3.8    34.9\n                 Explan     7.1         1.4   24.6     11.1     2.6   30.6      8.5        1.6    23.2      1.9    26.1\n\n         Task   Dataset                                      M-0.8\n                 BLOOM   (↑)   (↑(%))  GPT-J   (↑)  (↑(%))  LLaMA   (↑)   (↑(%))  ↑    ↑(%)\n\n      QA    AdverQA   18.3        4.8   35.6     9.8      6.8  226.7    23.3       20.1  628.1    10.6  296.8\n             OpenQA   26.7        0.8   3.1      20.6     3.6   21.2      15.6       2.3    17.3      2.2    13.9\n               CloseQA   7.9         1.5   23.4     8.6      1.7   24.6      14.1       3.3    30.6      2.2    26.2\n       CLS   News      95.9        3.1   3.3      5.8      5.8   -         10.2       9.1    827.3    6     415.3\n               Movie      92.8        1.9   2.1      52.9     1.8   3.5       81.5       2.8    3.6       2.2    3.1\n            QASC     99.9        0.5   0.5      56.6     2.6   4.8       70.4       8.8    14.3     4      6.5\n      GEN    Topics     35.1        5.6  19       21.9     4.4   25.1      18.7       4.4    30.8      4.8   25\n              Summary   50.1      4    8.7      16.8     3.7   28.2      10.9       4.3    65.2     4     34\n                 Explan     7.1         1.4   24.6     10.9     2.4   28.2      8.2        1.3    18.8      1.7    23.9\n\nTable 14: (↑) denotes the absolute performance increase achieved using MAPO-optimized prompts versus a frozen LLM,\nwhile (↑(%)) highlights the relative performance boost. Symbols ↑and ↑(%) represent the average absolute and relative\nenhancements across all three LLMs, respectively. These enhancements pertain to specific downstream tasks, with “CLS”\nsignifying classification and “GEN” indicating generation tasks. M, M-0.2, M-0.5, and M-0.8 correspond to using MAPO with\ntemperature settings of [0,0.5], 0, 0.2, 0.5, and 0.8, respectively.\n\ncity\n           topics\n                                                    home\n         sentence                                                      bathroom\n                                                                   place\n           city                                              view\n                                                            match\n        home                                              boat\n                                                                  white\n        bathroom                                                lake\n                                                       room          place\n                                                                         player\n         view                                                    grass\n       Word                                                                     close\n         match                                                       chair\n                                                                             playing\n         boat\n                                                                             flowers\n                                                              beach          white                                                    sky\n                                                           near          lake\n                                                                        bridge\n        room                                   man\n                                                                                  building\n          player                                                           final\n                                                                    Original                                                               Original\n                                             BLOOM                                          BLOOM                                                               GPT-J      wooden                                              GPT-J          grass                                     LLaMA                                                LLaMA                                                                          floor\n              0.00    0.02    0.04    0.06    0.08    0.10    0.12          0.000    0.002    0.004    0.006    0.008    0.010\n                                             (a)                                                                (b)\n\nFigure 7: The distribution of three most frequent words, which extracted from the original prompt, in the optimized prompt\namong different LLMs in the generation task. (a) retaining frequent words in the instruction, (b) removing frequent words in the\ninstruction.\n\nhappened                                                     happened\n      church                                                     church\n      following                                                      people\n      people                                    new\n                                         th     mentioned     new\n                                                         time\n     th\n                                                                                earlier      time\n                                                       museum        earlier\n    museum                                                                          contemporary                                                                              similar\n   contemporary                                                        french       similar\n       french                                                number\n                                                       songWord  number\n      song                                                 asia\n       asia                                                           methodist\n                                                                    places     methodist\n       places                                                                order\n       order\n                                                                          property      property                                                                           education                                                                city     education        city\n                                                                       century      century                                                                   early       early\n                                                                          students      students                                                           power      power                                                 Original                                                               Original\n                                                                                         BLOOM      year                                 BLOOM      year                                                           GPT-J                                                            GPT-J\n         life                                     LLaMA         life                                     LLaMA\n          0.000  0.002  0.004  0.006  0.008  0.010  0.012  0.014      0.000  0.002  0.004  0.006  0.008  0.010  0.012  0.014\n                                         (a)                                                                (b)\n\n Figure 8: The distribution of three most frequent words, which extracted from the original prompt, in the optimized prompt\n among different LLMs in the QA task. (a) retaining frequent words in the instruction, (b) removing frequent words in the\n instruction.\n\nuniversity                                                                   university\n                  year                                               year\n\n                      largest                                                              largest\n\n                  music                                                 music\n\n              new                                    new\n\n                      national                                                               national\n\n                know                                          know\n\n            uh                               uh\n                  Word\n                     increase                                                             increase\n                  government                                                        government\n                     executive                                                             executive\n                    medical                                                        medical\n                  warsaw                                                  warsaw\n\n                    school                                                        school\n\n                   poland                                                     poland                                                                               Original                                                               Original\n                                                     BLOOM                                          BLOOM\n                                                                         GPT-J                                                            GPT-J\n                                                           LLaMA                                                LLaMA                     parliament0.000   0.002   0.004   0.006   0.008   0.010   0.012 parliament0.000   0.002   0.004   0.006   0.008   0.010   0.012\n                                                        (a)                                                                (b)\n\nFigure 9: The distribution of three most frequent words, which extracted from the original prompt, in the optimized prompt\namong different LLMs in the classification task. (a) retaining frequent words in the instruction, (b) removing frequent words in\nthe instruction.\n\nTask   Dataset    BLOOM M     ↑    ↑   ↑(%)  ↑(%)  GPT-J  M     (↑)   ↑   (↑(%))  ↑(%)\n\nCoref.  xwinograd     60.1       60.6    0.5   0.5   0.9     0.9      -           -          -      -      -           -\n       BoolQ         67.9       68.2    0.3        0.4            67.2     67.9    0.7   0.3  1        0.5\n      CB            77.6       78.1    0.5        0.6            83.9     84.2    0.3        0.4\n      COPA         74.0       75.0    1.0        1.4           84       84.2    0.2        0.2\nNLU   MultiRC       59.7       60.4    0.7   0.6   1.2     0.9     63.8     64.1    0.3        0.5             Task    Dataset   LLaMA  M     (↑)   ↑   (↑(%))  ↑(%)\n      ReCoRD      69.8       70.2    0.4        0.6            74.4     74.7    0.3        0.4\n      WiC           61.4       62.0    0.6        1.0           61       61.3    0.3        0.5          RS     BoolQ        76.5       76.7    0.2   0.3   0.3      0.5\n     WSC          64.4       65.1    0.7        1.1            77.9     78.1    0.2        0.3\n      ANLI R1      31.5       32.1    0.6        1.9            37.8     38.2    0.4   0.3   1.1      0.7            PIQA         79.8      80      0.2        0.3\nNLI   ANLIANLI R2R3      35.137.1       35.437.8    0.30.7   0.5   0.91.9     1.3     37.940.9     38.341.1    0.40.2        1.10.5                SIQA         48.9      49      0.1        0.2\n        StoryCloze    79.0       79.5    0.5        0.6            87.8     87.9    0.1        0.1                    HellaSwag    76.1       76.5    0.4        0.5\nCLS   DBPediaAmazon       70.565.2       72.567.7    2.02.5   2.3   2.83.8     3.3     68.283.9     69.485.1    1.21.2   1.2   1.41.8      1.6             WinoGrande   70.1       70.5    0.4        0.6\n      DROP         67.9       69.9    2.0        2.9            51.6     52.8    1.2  1    2.3      3.2            ARC-e        72.8       73.2    0.4        0.5\nQA   NQRealTimeQA   15.129.0       16.131.5    1.02.5   1.8   6.68.6     5.6     19.636       20.837.2    1.21.2        6.13.3                 ARC-c        47.6       47.8    0.2        0.4\n      WebQs        34.8       36.3    1.5        4.3            44.1     44.6    0.5        1.1             OBQA        57.2       57.9    0.7        1.2\nTask   Dataset    BLOOM  M-0    ↑    ↑   ↑(%)  ↑(%)  GPT-J  M-0    (↑)   ↑   (↑(%))  ↑(%)    QA   NQ           16.8       18.1    1.3   1.1   7.7      4.7\nCoref.  xwinograd     60.1       60.5    0.4   0.4   0.7     0.7      -           -          -      -      -           -           RACE       50        50.8    0.8        1.6\n       BoolQ         67.9       68.0    0.1        0.1            67.2     67.8    0.6   0.3   0.9      0.4\n      CB            77.6       78.0    0.4        0.5            83.9     84.1    0.2        0.2             Task    Dataset   LLaMA  M-0    (↑)   ↑   (↑(%))  ↑(%)\n      COPA         74.0       74.8    0.8        1.1           84       84.2    0.2        0.2\nNLU   MultiRC       59.7       60.4    0.7   0.5   1.2     0.7     63.8     64.1    0.3        0.5          RS     BoolQ        76.5       76.7    0.2   0.3   0.3      0.4\n      ReCoRD                       69.8                                  70.2                                             0.4                                                        0.6                                                                    74.4                                                                             74.7                                                                                          0.3                                                                                                     0.4\n      WiC                       61.4                                  61.8                                             0.4                                                        0.7                                                            61                                                                             61.3                                                                                          0.3                                                                                                     0.5                PIQA         79.8      80      0.2        0.3\n     WSC          64.4       65.0    0.6        0.9            77.9     78.1    0.2        0.3                SIQA         48.9      49      0.1        0.2\n      ANLI           R1                       31.5       32.0                                             0.5                                                        1.6                                                                    37.8                                                                             38.2                                                                                          0.4                                                                                               0.2                                                                                                     1.1      0.6              HellaSwag                                                                                                       76.1                                                                                                                 76.4                                                                                                                               0.3                                                                                                                                           0.4           R2                       35.1                                  35.4                                             0.3                                                                    37.9                                                                             38.1                                                                                          0.2                                                                                                     0.5\n                                                                1.2NLI   ANLIANLI           R3                       37.1                                  37.8                                             0.7   0.5   0.91.9                                                                    40.9                                                                             41.1                                                                                          0.2                                                                                                     0.5                                                                           WinoGrande                                                                                                       70.1                                                                                                                 70.4                                                                                                                               0.3                                                                                                                                           0.4\n        StoryCloze                       79.0                                  79.4                                             0.4                                                        0.5                                                                    87.8                                                                             87.9                                                                                          0.1                                                                                                     0.1\n                                                                                                       72.8                                                                                                                 73.2                                                                                                                               0.4                                                                                                                                           0.5       Amazon                       65.2                                  67.3                                                                    69                                                                                          0.8                                                                                1                                                                                                     1.2      1.3            ARC-e\nCLS   DBPedia                       70.5                                  72.0                                             1.5                                             2.1   1.8   2.1                                                        3.2     2.7     68.283.9                                                                             85.1                                                                                          1.2                                                                                                     1.4                                                                    ARC-c                                                                                                       47.6                                                                                                                 47.8                                                                                                                               0.2                                                                                                                                           0.4\n      DROP                       67.9       69.5                                             1.6                                                        2.4                                                                    51.6                                                                             52.3                                                                                          0.7                                                                                               0.7                                                                                                     1.4      2.3\n                                                       OBQA                                                                                                       57.2                                                                                                                 57.8                                                                                                                               0.6                                                                                                                    1                       15.1                                  15.8                                             0.7                                                                    19.6                                                                             20.4                                                                                          0.8                                                                                                     4.1\n                                                                4.6QA   NQRealTimeQA                       29.0                                  31.2                                             2.2                                                   1.5   4.67.6                                                            36                                                                    37                                                                           1                                                                                                     2.8                                                QA                                                      NQ                                                                                                       16.8                                                                                                                 17.8    1                                                                                                                                     0.8                                                                                                                    6        3.6\n      WebQs        34.8       36.1    1.3        3.7            44.1     44.5    0.4        0.9               RACE       50        50.6    0.6        1.2\nTask   Dataset    BLOOM  M-0.2  ↑    ↑   ↑(%)  ↑(%)  GPT-J  M-0.2   (↑)   ↑   (↑(%))  ↑(%)\nCoref.  xwinograd     60.1       60.6    0.5   0.5   0.8     0.8      -           -          -      -      -           -         Task    Dataset   LLaMA  M-0.2   (↑)   ↑   (↑(%))  ↑(%)\n       BoolQ         67.9       68.1    0.2        0.3            67.2     67.9    0.7   0.3  1        0.4\n      CB            77.6       78.1    0.5        0.6            83.9     84.2    0.3        0.4          RS     BoolQ        76.5       76.7    0.2   0.3   0.3      0.4\n      COPA                       74.0                                  74.6                                             0.6                                                        0.8                                                            84                                                                             84.2                                                                                          0.2                                                                                                     0.2\nNLU       MultiRC                       59.7                                  60.3                                             0.6   0.5                                                        1.0     0.7                                                                    63.8                                                                    64                                                                                          0.2                                                                                                     0.3                PIQA         79.8      80      0.2        0.3\n      ReCoRD      69.8       70.2    0.4        0.6            74.4     74.6    0.2        0.3                SIQA         48.9      49      0.1        0.2\n      WiC                       61.4                                  61.7                                             0.3                                                        0.5                                                            61                                                                             61.3                                                                                          0.3                                                                                                     0.5\n     WSC                       64.4                                  65.1                                             0.7                                                        1.1                                                                    77.9                                                                             78.1                                                                                          0.2                                                                                                     0.3                    HellaSwag    76.1       76.4    0.3        0.4\n      ANLI R1      31.5       32.2    0.7        2.2            37.8     38.2    0.4   0.3   1.1      0.7             WinoGrande   70.1       70.4    0.3        0.4\nNLI   ANLIANLI R2R3      35.137.1       35.537.6    0.40.5   0.5   1.11.3     1.3     37.940.9     38.341.1    0.40.2        1.10.5                 ARC-e        72.8       73.1    0.3        0.4\n        StoryCloze    79.0       79.5    0.5        0.6            87.8     87.9    0.1        0.1                 ARC-c        47.6       47.8    0.2        0.4\nCLS   DBPediaAmazon       70.565.2       71.867.1    1.31.9   1.6   1.82.9     2.4     68.283.9     69.185      0.91.1  1    1.31.3      1.3          OBQA        57.2       57.7    0.5        0.9\n      DROP         67.9       69.3    1.4        2.1            51.6     52.6    1    0.7   1.9      2.1    QA   NQ           16.8      18      1.2  1    7.1      4.3\nQA   NQRealTimeQA   15.129.0       15.631.0    0.52.0   1.2   3.36.9     3.8     19.636       20.336.7    0.70.7        3.61.9               RACE       50        50.7    0.7        1.4\n      WebQs        34.8       35.8    1.0        2.9            44.1     44.5    0.4        0.9             Task    Dataset   LLaMA  M-0.5   (↑)   ↑   (↑(%))  ↑(%)\nTask   Dataset    BLOOM  M-0.5  ↑    ↑   ↑(%)  ↑(%)  GPT-J  M-0.5   (↑)   ↑   (↑(%))  ↑(%)\nCoref.  xwinograd     60.1       60.4    0.3   0.3   0.5     0.5      -           -          -      -      -           -      RS     BoolQ        76.5       76.7    0.2   0.2   0.3      0.3\n       BoolQ         67.9       68.0    0.1        0.1            67.2     67.6    0.4   0.2   0.6      0.2            PIQA         79.8       79.9    0.1        0.1\n      CB                       77.6                                  77.8                                             0.2                                                        0.3                                                                    83.9                                                                             84.1                                                                                          0.2                                                                                                     0.2\n      COPA                       74.0                                  74.4                                             0.4                                                        0.5                                                            84                                                                             84.2                                                                                          0.2                                                                                                     0.2                SIQA         48.9      49      0.1        0.2\nNLU   MultiRC       59.7       60.3    0.6   0.3   1.0     0.5     63.8     63.9    0.1        0.2                    HellaSwag    76.1       76.4    0.3        0.4\n      ReCoRD                       69.8                                  70.1                                             0.3                                                        0.4                                                                    74.4                                                                             74.5                                                                                          0.1                                                                                                     0.1\n      WiC                       61.4                                  61.6                                             0.2                                                        0.3                                                            61                                                                             61.2                                                                                          0.2                                                                                                     0.3                   WinoGrande   70.1       70.4    0.3        0.4\n     WSC          64.4       64.8    0.4        0.6            77.9    78      0.1        0.1                 ARC-e        72.8      73      0.2        0.3\n      ANLI           R1                       31.5       32.0                                             0.5                                                        1.6                                                                    37.8                                                                             38.2                                                                                          0.4                                                                                               0.2                                                                                                     1.1      0.5            ARC-c                                                                                                       47.6                                                                                                                 47.7                                                                                                                               0.1                                                                                                                                           0.2           R2                       35.1                                  35.3                                             0.2                                                                    37.9                                                                             38.1                                                                                          0.2                                                                                                     0.5\n                                                                0.9NLI   ANLIANLI           R3                       37.1                                  37.5                                             0.4   0.3   0.61.1                                                                    40.9                                                                    41                                                                                          0.1                                                                                                     0.2                                                       OBQA                                                                                                       57.2                                                                                                                 57.6                                                                                                                               0.4                                                                                                                                           0.7\n        StoryCloze                       79.0                                  79.3                                             0.3                                                        0.4                                                                    87.8                                                                             87.9                                                                                          0.1                                                                                                     0.1\n                                                      NQ                                                                                                       16.8                                                                                                                 17.7    0.9                                                                                                                                     0.7                                                                                                                                           5.4      3.2       Amazon                       65.2                                  66.7                                                                             68.9                                                                                          0.7                                                                                               0.5                                                                                    1        0.8    QA\nCLS   DBPedia                       70.5                                  71.4                                             0.9                                             1.5   1.2   1.3                                                        2.3     1.8     68.283.9                                                                             84.3                                                                                          0.4                                                                                                     0.5                                                           RACE                                                                                          50                                                                                                                 50.5                                                                                                                               0.5                                                                                                                    1\n      DROP         67.9       68.5    0.6        0.9            51.6     52.2    0.6   0.5   1.2      1.7\nQA   NQRealTimeQA   15.129.0       15.530.6    0.41.6   0.8   2.65.5     2.7     19.636       20.236.6    0.60.6        3.11.7             Task    Dataset   LLaMA  M-0.8   (↑)   ↑   (↑(%))  ↑(%)\n      WebQs        34.8       35.4    0.6        1.7            44.1     44.4    0.3        0.7          RS     BoolQ        76.5       76.6    0.1   0.2   0.1      0.2\nTask   Dataset    BLOOM  M-0.8  ↑    ↑   ↑(%)  ↑(%)  GPT-J  M-0.8   (↑)   ↑   (↑(%))  ↑(%)           PIQA         79.8       79.9    0.1        0.1\nCoref.        xwinograd                       60.1                                  60.2    0.1                                                   0.1   0.2                                                                0.2                                                                                          -                                                                                                      -          -                                                                                                                       -                                                                                                                              -                                                                                                                                          -             SIQA         48.9      49      0.1        0.2\n       BoolQ                       67.9                                  68.0                                             0.1                                                        0.1                                                                    67.2                                                                             67.5                                                                                          0.3                                                                                               0.2                                                                                                     0.4      0.2\n      CB            77.6       77.8    0.2        0.3            83.9     84.1    0.2        0.2                    HellaSwag    76.1       76.3    0.2        0.3\n      COPA                       74.0                                  74.3                                             0.3                                                        0.4                                                            84                                                                             84.2                                                                                          0.2                                                                                                     0.2\nNLU       MultiRC                       59.7                                  60.3                                             0.6   0.3                                                        1.0     0.4                                                                    63.8                                                                             63.9                                                                                          0.1                                                                                                     0.2                   WinoGrande   70.1       70.4    0.3        0.4\n      ReCoRD      69.8       70.1    0.3        0.4            74.4     74.5    0.1        0.1                 ARC-e        72.8      73      0.2        0.3\n      WiC                       61.4                                  61.6                                             0.2                                                        0.3                                                            61                                                                             61.2                                                                                          0.2                                                                                                     0.3\n     WSC                       64.4                                  64.7                                             0.3                                                        0.5                                                                    77.9                                                                    78                                                                                          0.1                                                                                                     0.1                 ARC-c        47.6       47.6    0        0\n      ANLI R1      31.5       31.8    0.3        1.0            37.8     38.1    0.3   0.2   0.8      0.4          OBQA        57.2       57.5    0.3        0.5\nNLI   ANLIANLI R2R3      35.137.1       35.337.5    0.20.4   0.3   0.61.1     0.8     37.940.9    3841      0.10.1        0.30.2        QA   NQ           16.8       17.6    0.8   0.5   4.8      2.7\n        StoryCloze    79.0       79.3    0.3        0.4            87.8     87.9    0.1        0.1               RACE       50        50.3    0.3        0.6\nCLS   DBPediaAmazon       70.565.2       71.366.5    0.81.3   1.1   1.12.0     1.6     68.283.9     68.884.2    0.60.3   0.4   0.40.9      0.7\n      DROP         67.9       68.3    0.4        0.6            51.6     52.1    0.5   0.4  1        1.5\n                                                     Table 16: Domain transfer performance with a frozen\nQA   NQRealTimeQA   15.129.0       15.530.5    0.41.5   0.7   2.65.2     2.5     19.636       20.136.6    0.50.6        2.61.7\n      WebQs        34.8       35.3    0.5        1.4            44.1     44.3    0.2        0.5     LLM for inference based on LLaMA with original and\n                                            MAPO-Optimized prompts. CLS: Classification, M:\nTable 15: Zero-shot domain transfer performance based   MAPO. The (↑) and ↑represent the absolute improve-\non BLOOM and GPT-J with original and MAPO-   ment scores of MAPO-optimized prompts compared\nOptimized prompts. CLS: Classification, M: MAPO.   with original prompts in each dataset and task, respec-\nThe (↑) and ↑represent the absolute improvement scores     tively. The (↑(%) ) and ↑(%) represent the relative im-\nof MAPO-optimized prompts compared with original    provement percentages of MAPO-optimized prompts\nprompts in each dataset and task, respectively. The    compared with original prompts in each dataset and task,\n(↑(%) ) and ↑(%) represent the relative improvement    respectively.\npercentages of MAPO-optimized prompts compared\nwith original prompts in each dataset and task, respec-\ntively.\n\nTasks    Dataset       20%         40%         60%         80%         100%\n\n          (BLOOM)          +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO\n         QA       AdverQA   14.6    17.8       16.9    18.6       17.5    18.9       17.7    19.2       18.3    19.5\n                    OpenQA   22.1    25.4       24.3    26.1       24.9    26.3       25.1    26.5       26.7    27.2\n                       CloseQA   6.6     8.0        7.7     8.6        7.3     8.8        7.8     9.1        7.8     9.4\n               Class      News      93.6    96.8       95.8    97.4       95.5    97.6       96.2    98.0       95.5    98.7\n                       Movie      87.1    89.8       88.3    90.9       88.9    91.3       90.7    92.5       92.6    93.3\n                  QASC     96.0    98.0       97.6    98.6       97.1    98.7       97.4    98.8       99.9    99.9\n           Gen         Topics     32.7    35.0       33.6    35.7       34.6    36.0       34.7    36.4       34.8    36.2\n                      Summary   45.8    47.9       47.1    48.9       47.3    49.3       48.5    49.7       48.8    50.2\n                         Explan     6.2     7.5        7.3     8.1        7.1     8.3        6.3     8.6        6.8     8.9\n              Average       -           45.0    47.4       46.5    48.1       46.7    48.4       47.2    48.8       47.9    49.3\n             ↑               -              -        2.4          -        1.6          -        1.7          -        1.6          -        1.4\n             ↑(%)           -              -        5.3          -        3.4          -        3.6          -        3.4          -        2.9\n\n                Tasks    Dataset       20%         40%         60%         80%         100%\n\n             (GPT-J)            +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO\n         QA       AdverQA   6.8     9.2        7.7     9.3        7.9     9.9        9.4     11.0       9.9     11.0\n                    OpenQA   17.3    19.0       17.3    19.1       18.2    19.5       20.3    21.0       19.8    21.1\n                       CloseQA   7.1     8.4        7.3     8.6        8.2     9.4        8.4     10.2       9.6     10.3\n               Class      News       2.4     4.6        2.8     4.8        3.9     5.3        5.5     6.3        5.5     6.4\n                       Movie      49.2    51.8       50.7    52.2       51.7    52.8       52.7    53.9       51.8    53.9\n                  QASC     50.3    53.8       52.8    54.4       53.6    55.2       56.3    56.8       55.2    56.9\n           Gen         Topics     19.5    21.3       20.0    21.6       20.8    22.2       21.6    23.4       21.8    23.6\n                      Summary   13.8    16.0       14.7    16.3       15.7    16.8       16.7    17.8       17.1    18.1\n                         Explan     8.5     11.2       9.9     11.4       10.1    11.9       10.7    12.9       11.8    13.2\n              Average       -           19.4    21.7       20.4    22.0       21.1    22.6       22.4    23.7       22.5    23.8\n             ↑               -              -        2.3          -        1.6          -        1.5          -        1.3          -        1.3\n             ↑(%)           -              -       11.9         -        7.8          -        7.1          -        5.8          -        5.8\n\n                Tasks    Dataset       20%         40%         60%         80%         100%\n\n           (LLaMA)           +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO  +SFT  +MAPO\n         QA       AdverQA   19.6    22.2       22.0    23.2       23.2    25.1       24.5    25.4       24.4    26.0\n                    OpenQA   12.8    14.3       13.5    15.1       15.4    16.1       14.9    16.3       15.9    16.6\n                       CloseQA   11.5    13.0       12.2    13.8       13.9    14.8       14.5    15.2       14.8    15.4\n               Class      News       8.8     10.2       9.8     11.5       10.1    12.4       11.0    12.5       10.8    12.7\n                       Movie      77.5    79.8       79.0    80.2       81.3    82.5       80.6    82.7       81.5    83.1\n                  QASC     67.3    70.5       68.3    71.4       70.2    72.8       71.2    73.3       71.6    73.4\n           Gen         Topics     16.1    17.5       17.2    18.2       18.6    19.5       18.3    19.8       18.0    19.9\n                      Summary   8.2     10.1       9.4     11.3       10.7    12.2       10.5    12.4       11.6    12.8\n                         Explan     5.6     7.3        7.6     8.2        8.2     9.1        8.8     9.5        8.8     10.0\n              Average       -           25.3    27.2       26.6    28.1       28.0    29.4       28.3    29.7       28.6    30.0\n             ↑               -              -        1.9          -        1.5          -        1.4          -        1.4          -        1.4\n             ↑(%)           -              -        7.5          -        5.6          -        5.0          -        4.9          -        4.9\n\nTable 17: Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q:\nQA, C: classification, G:generation. SFT means using SFT-optimized prompts without RL.\n\nTask   Dataset   Best Performance  Best Epoch              Epoch\n                         -          -              -                         -           1     5     10    15    20    50\n\n            QA    AdverQA   18.3              18           14.8   14.3   15.4   18.2   18.3   18.2\n                    OpenQA   26.7              14           26.0   26.0   26.1   26.7   26.7   26.6\n                       CloseQA   7.8               19            6.8    7.0    7.0    7.5    7.8    7.8\n              CLS   News      95.5              20           93.2   93.7   94.3   95.3   95.5   95.7\n                       Movie      92.6              15           91.4   91.7   91.3   92.6   92.4   92.5\n                  QASC     99.9              7            99.6   99.8   99.9   99.9   99.9   99.9\n            GEN    Topics     34.8              19           30.3   31.5   33.8   34.2   34.8   34.8\n                      Summary   48.8              18           46.1   46.4   47.5   48.4   48.8   48.9\n                          Explan     6.8               15            5.9    6.3    6.4    6.8    6.5    6.7\n\n                  Task   Dataset   Best Performance  Best Epoch              Epoch\n                         -          -              -                         -           1     5     10    15    20    50\n\n            QA    AdverQA   9.4               20            4.4    5.3    6.7    8.8    9.4    9.6\n                    OpenQA   20.3              15           17.5   18.1   18.8   20.3   20.2   20.0\n                       CloseQA   8.4               20            7.0    7.3    7.8    8.2    8.4    8.6\n              CLS   News       5.5               20            1.4    2.6    3.1    4.8    5.5    5.9\n                       Movie      52.7              15           51.1   51.6   52.6   52.7   52.4   52.7\n                  QASC     56.3              19           54.3   54.6   54.5   55.4   56.3   56.3\n            GEN    Topics     21.6              18           17.9   20.2   20.7   21.3   21.6   21.6\n                      Summary   16.7              20           13.2   14.5   15.2   16.3   16.7   17.1\n                          Explan     10.7              19            8.8    9.2    9.9    9.6    10.7   11.0\n\n                  Task   Dataset   Best Performance  Best Epoch              Epoch\n                         -          -              -                         -           1     5     10    15    20    50\n\n            QA    AdverQA   23.2              20            5.8    9.3    13.3   18.7   23.2   29.5\n                    OpenQA   15.4              15           13.6   14.1   14.6   15.4   15.3   16.8\n                       CloseQA   13.9              20           10.8   11.3   12.9   13.5   13.9   14.5\n              CLS   News      10.1              13            3.4    5.7    9.3    10.1   10.1   10.1\n                       Movie      81.3              19           79.2   79.6   80.5   81.1   81.3   81.2\n                  QASC     70.2              20           62.8   63.7   65.9   69.2   70.2   71.9\n            GEN    Topics     18.6              20           15.5   16.1   17.7   18.4   18.6   19.3\n                      Summary   10.7              20            6.6    7.1    8.6    9.4    10.7   11.5\n                          Explan     8.2               14            7.3    7.5    7.5    8.2    8.2    8.3\n\nTable 18: Performance of different number of epochs when training SFT. Best Performance means the best\nperformance within 20 epochs. Best Epoch means the epoch corresponding to the best performance. We list\nthe performance in the 1,5,10,15,20,50 epochs. We bold the performance metrics where a longer training epoch\n(epoch=50) results in a decline in performance.\n\nTask    Prompts\n\n           Original: The sentence is “khouri manages , with terrific flair, to keep the extremes of screwball farce and blood-curdling family intensity on\n          one continuum. ”.\n       BLOOM: The sentence is “khouri manages , with terrific flair, to keep the extremes of screwball farce and blood-curdling family intensity on\n          one continuum. ”.\n  SST-2\n         GPT-J: The sentence is “With tremendous artistry, Khouri expertly maintains a fluid continuum between the uproarious comedy of screwball\n            farce and the blood-curdling intensity of family dynamics.”.\n        LLaMA: The sentence is “With remarkable skill, Khouri successfully maintains a seamless connection between the wild hilarity of screwball\n            farce and the spine-chilling intensity of family dynamics.”.\n\n           Original: The sentence is “The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, the\n            waiters and waitresses are often very apologetic for the long waits and it’s pretty obvious that some of them avoid the tables after taking the initial\n           order to avoid hearing complaints.”.\n       BLOOM: The sentence is “The quality of the food is commendable. However, the service provided is inconsistent. The primary concern appears\n            to be with the kitchen, as the waitstaff frequently apologize for lengthy waits, and it’s apparent that some of them intentionally avoid tables after\n           taking initial orders to prevent complaints.”.\n  Yelp\n         GPT-J: The sentence is “The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, the waiters\n          and waitresses are often very apologetic for the long waits and it’s pretty obvious that some of them avoid the tables after taking the initial order to\n           avoid hearing complaints.”.\n        LLaMA: The sentence is “The quality of the food is commendable. However, the service provided is inconsistent. The primary concern appears\n            to be with the kitchen, as the waitstaff frequently apologize for lengthy waits, and it’s apparent that some of them intentionally avoid tables after\n           taking initial orders to prevent complaints.”.\n\n           Original: The sentence is “a respectable venture on its own terms , lacking the broader vision that has seen certain trek films . . . cross over to\n           a more mainstream audience .”.\n       BLOOM: The sentence is “An admirable endeavor in its own right, although it lacks the wider perspective that propelled certain Star Trek films\n            to cross over into a more mainstream audience.”.\n MR\n         GPT-J: The sentence is “a respectable venture on its own terms , lacking the broader vision that has seen certain trek films . . . cross over to a\n         more mainstream audience .”.\n        LLaMA: The sentence is “A praiseworthy pursuit on its own grounds, though it falls short of the broader vision that allowed certain Star Trek\n            films to captivate a more mainstream audience.”.\n\n           Original: The sentence is “the fact that the 0 key is the space key for text input is a bit confusing , as many phones use the key instead”.\n       BLOOM: The sentence is “It can be a little confusing that the space key for text input is the 0 key, considering that many phones utilize a different\n          key for that purpose.”.\n CR\n         GPT-J: The sentence is “the fact that the 0 key is the space key for text input is a bit confusing , as many phones use the key instead”.\n        LLaMA: The sentence is “It might cause confusion that the space key for text input is assigned to the 0 key, whereas many phones utilize a different\n          key for the same purpose.”.\n\n           Original: The sentence 1 is “More than 150 dolphins, marine turtles and beaked whales have been washed up dead on beaches in Africa.”. The\n           sentence 2 is “Dead dolphins, turtles and whales have been found on African beaches.”.\n       BLOOM: The sentence 1 is “African beaches have become a somber site with the washing up of over 150 deceased dolphins, marine turtles, and\n          beaked whales.”. The sentence 2 is “African beaches have become a distressing sight as dead dolphins, turtles, and whales have been uncovered.”.\n RTE\n         GPT-J: The sentence 1 is “The number of deceased dolphins, marine turtles, and beaked whales found on African beaches exceeds 150.”. The\n           sentence 2 is “There have been reports of deceased dolphins, turtles, and whales on African beaches.”.\n        LLaMA: The sentence 1 is “More than 150 dolphins, marine turtles and beaked whales have been washed up dead on beaches in Africa.”. The\n           sentence 2 is “Dead dolphins, turtles and whales have been found on African beaches.”.\n\n           Original: The sentence 1 is “What came into force after the new constitution was herald?”. The sentence 2 is “As of that day, the new constitution\n           heralding the Second Republic came into force.”.\n       BLOOM: The sentence 1 is “What started being in effect as a result of the heralding of the new constitution?”. The sentence 2 is “The new\n            constitution heralding the Second Republic started being in effect as of that day.”.\n QNLI\n         GPT-J: The sentence 1 is “What commenced after the new constitution was heralded?”. The sentence 2 is “The new constitution heralding the\n          Second Republic came into force as of that day.”.\n        LLaMA: The sentence 1 is “What became valid subsequent to the heralding of the new constitution?”. The sentence 2 is “ The new constitution\n           heralding the Second Republic started being in force on that day.”.\n\n           Original: The sentence 1 is “A baby at the end of a slip and slide at a party”. The sentence 2 is “The baby is wet”.\n       BLOOM: The sentence 1 is “At a party, a baby reaches the conclusion of their slip and slide adventure.”. The sentence 2 is “The little one is\n           dripping wet.”.\n  SNLI\n         GPT-J: The sentence 1 is “A baby sliding to the end of a slip and slide at a celebration.”. The sentence 2 is “The baby’s clothes are moistened.”.\n        LLaMA: The sentence 1 is “At the end of a festive occasion, a baby arrives at the final stretch of a slip and slide.”. The sentence 2 is “The baby\n              is entirely drenched.”.\n\n           Original: The sentence 1 is “Conversely, an increase in government saving adds to the supply of resources available for investment and may\n           put downward pressure on interest rates.”. The sentence 2 is “Interest rates should increase to increase saving.”.\n       BLOOM: The sentence 1 is “Conversely, an increase in government saving adds to the supply of resources available for investment and may\n           put downward pressure on interest rates.”. The sentence 2 is “Interest rates should increase to increase saving.”.\n MNLI\n         GPT-J: The sentence 1 is “Conversely, when the government saves more, it adds to the pool of resources for investment and may exert downward\n           pressure on interest rates.”. The sentence 2 is “The augmentation of interest rates is crucial for increasing saving.”.\n        LLaMA: The sentence 1 is “On the flip side, an increase in government saving enhances the resources accessible for investment and may\n            exert downward pressure on interest rates.”. The sentence 2 is “The escalation of interest rates is essential for boosting savings.”.\n\n           Original: The sentence 1 is “Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.”. The sentence 2 is\n          “With the scandal hanging over Stewart’s company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier.”.\n       BLOOM: The sentence 1 is “The first quarter of the year saw a 15 percent drop in revenue compared to the corresponding period a year earlier.”.\n         The sentence 2 is “The scandal’s impact on Stewart’s company led to a 15 percent drop in revenue during the first quarter, as compared to the same\n           period in the prior year.”\n MRPC  GPT-J: The sentence 1 is “There was a 15 percent decline in revenue during the first quarter of the year compared to the same period a year ago.”.\n         The sentence 2 is “Due to the scandal looming over Stewart’s company, revenue in the first quarter of the year witnessed a 15 percent decline\n          compared to the same period last year.”.\n        LLaMA: The sentence 1 is “There was a 15 percent reduction in revenue during the first quarter of the year compared to the same period in the\n            prior year.”. The sentence 2 is “With the scandal overshadowing Stewart’s company, revenue for the first quarter of the year dipped by 15 percent\n          compared to the corresponding period in the previous year.”.\n\n\nTable 19: Original prompts and MAPO-optimized prompts for different LLMs in more downstream tasks. Each prompt is start\nwith the corresponding instruction as shown in Table 13. We omit instructions in the following Table due to space limits.",
"headers": [
"arXiv:2407.04118v1  [cs.CL]  4 Jul 2024",
"Additional Exploratory Analy-",
"MAPO: Boosting Large Language Model Performance with",
"Model-Adaptive Prompt Optimization",
"1",
"Introduction",
"2",
"Empirical study",
"3",
"Methods",
"4",
"Experiments",
"5",
"Related Work",
"6",
"Conclusions",
"Limitations",
"Acknowledgement",
"References",
"A",
"Candidate Prompts",
"B",
"Training Details",
"C",
"Computational Cost",
"D",
"Baselines",
"E",
"Datasets",
"G",
"Additional Cases",
"F",
"Additional Experiments",
"H",
"sis",
"Yuyan Chen",
", Zhihao Wen",
", Ge Fan",
", Zhengyu Chen, Wei Wu",
"Dayiheng Liu",
",",
"Zhixu Li",
"Bang Liu",
"Yanghua Xiao",
"Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University,",
"Fudan-Aishu Cognitive Intelligence Joint Research Center,",
"Singapore Management University,",
"Tencent,",
"Ant Group",
"{chenyuyan21@m., zhixuli@, shawyh@}fudan.edu.cn",
"zhwen.2019@phdcs.smu.edu.sg",
"ge.fan@outlook.com",
"chenzhengyu@zju.edu.cn",
"congyue.ww@antgroup.com",
"liudayiheng.ldyh@alibaba-inc.com",
"bang.liu@umontreal.ca",
"Abstract",
"3.1",
"Warm-up Dataset Establishment",
"3.2",
"Prompt Optimizer Construction",
"4.1",
"Experimental Setups",
"4.2",
"Main Results",
"4.3",
"Ablation Study",
"4.4",
"Case Study and Error Analysis",
"4.5",
"Exploratory Analysis",
"and unscalable. Hence, there has been a lot of",
"effort to streamline the prompt optimization pro-",
"cess through automated or semi-automated ways,",
"Pryzant et al.",
"2023",
";",
"Deng et al.",
"2022",
"). For ex-",
"ample, in prompt retrieval,",
"Ma et al.",
"(",
") pro-",
"pose a search strategy based on greedy search",
"to identify near-optimal prompts for in-context",
"et al.",
") introduce SharpT, which learns a",
"pabilities across a wide range of tasks (",
"OpenAI",
"these models possess human-like comprehension",
"influenced by the quality of prompts. As can be",
"observed in Fig.",
", answers from different LLMs",
"generate prompts that are most suitable for each",
"A common practice towards prompt optimiza-",
"tion is to count on human expertise (",
"White et al.",
"emphasizes the importance of adapting prompts",
"to specific tasks, rather than specific LLMs. The",
"ied to date in NLP. The only relevant work so",
"far has been done on multi-modal large models,",
"inforcement learning to generate images based",
"on text (",
"Hao et al.",
"). They underscore the",
"NLP. Given the lack of effective training signals,",
"we first establish a so-called warm-up dataset to",
"we combine Supervised Fine-Tuning (SFT) and",
"prompts for each specific LLM in various down-",
"stream tasks. Moreover, we make joint learning",
"man et al.",
"2017",
") and RRMF (note that RRMF is",
"inspired by RRHF (",
"Yuan et al.",
")), to further",
"improve the performance of RL. We conduct ex-",
"up, our main research question revolves around",
"identifying the optimal prompt that is suited for",
"LLMs (BLOOM-7B (",
"Scao et al.",
"), GPT-J-",
"and generation (",
"Chen et al.",
"2024a",
"2023d",
")",
"original prompts and the corresponding ground-",
"truth answers. We adopt F1 score, accuracy and",
"ROUGE-L for QA, classification, and generation",
"tasks, respectively. The visualization results are",
"shown in the Fig.",
". From the violin plot, we ob-",
"serve significant variations in distribution among",
"different LLMs in each task. For example, in the",
"the LLMs’ performance on various downstream",
"We are the first to quantitatively show that",
"to enhance their performance across various",
"Based on the above empirical study, we present",
"an LLM give better outputs. The framework of",
"We introduce a novel approach called the",
"prompts for each particular LLM in down-",
"The experiments show that our proposed",
"We first establish a warm-up dataset as training",
"prompts are from nine above-mentioned datasets",
"in P3 (",
"Sanh et al.",
"2021",
"). We generate 1,000 can-",
"duce",
", which is more suitable for the LLM in",
"We begin by employing the warm-up dataset to",
"annotated data. Prior research conducted by",
"Ra-",
"leads to improved outcomes. Furthermore, to dif-",
"ferentiate between specific tasks during training,",
"we incorporate a brief instruction preceding the",
"input, such as “This is a... (generative/question-",
"didate prompts per prompt using GPT-3.5",
". The",
"generated candidate prompts should maintain se-",
"to generate candidates: “Please rewrite the given",
"meaning unchanged.”. Some candidate prompts",
"output. The purpose is to identify the candidate",
"not available, the output of a stronger LLM, such",
"ically, first, we input the original prompt",
"and",
"evaluation metrics. We adopt F1 score, accuracy,",
"of prompt pairs (referred to as",
"), whose",
"The prompt optimizer seeks to refine the initial",
"to a particular LLM. This refinement process en-",
"fectiveness of prompts based on the preferences",
"ing incurs significantly lower costs compared to",
"generating annotations for answers. Initially, we",
"obtain a ranking sequence for an LLM in a spe-",
"cific downstream task. We sort the outputs gener-",
"alongside the original prompt",
", using the",
"same evaluation metric as described in Sec.",
".",
"benchmark to evaluate whether the updated actor",
"model has advantages over it. We then calculate",
"and calculate the value loss (i.e., critic’s loss) by",
"comparing the predicted value",
"with the true",
"This sorting process yields a ranking sequence",
". Prompts to the left of",
"We utilize the same LLM utilized in the SFT pro-",
"layer with a linear layer to construct the reward",
"and produces a scalar score indicating the quality",
"of the prompt. We form pairwise ranking pairs",
"Here,",
"where",
"represents the original prompt,",
"denote the higher-scoring and lower-scoring",
"pair.",
"represents the scalar output of the reward",
"represents the ratio of probabilities",
"step.",
"denotes the predicted value, and",
"Next, we maximize the mathematical expecta-",
"tion of the reward model, aiming to consistently",
"generate prompts that",
"perceives as the best",
"in the RL-trained SFT model (referred to as RL",
"prompt",
"changes every time the RL model",
"is updated. We then input",
"into the reward",
"(RL) to further fine-tune LLMs. RL is used to",
"generated by",
"after SFT (referred to as the",
"Policy Optimization.",
"This step aims to opti-",
"mize the RL policy to improve the performance",
"of the RL model.",
"We first adopt the datasets",
"of each function, which are determined through",
"SFT Approximating.",
"This step aims to main-",
"model. When the RL model undergoes parame-",
"ter updates, it leads to variations in the generated",
"a significant discrepancy between the RL model",
"this issue, we measure the distance between the",
"prompts generated by the RL model and the SFT",
"model using Kullback-Leibler (KL) divergence.",
"The objective is to minimize the KL divergence",
"popular LLMs as",
", respectively, including",
"Next,",
"we have borrowed the idea from",
"RRHF (",
") but adapt it to focus",
"back (RRMF). Specifically, we calculate the like-",
"lihood probability of",
"during SFT and align",
"is the conditional log probability which",
"represents the reward model",
". We also",
"incorporate the cross-entropy loss introduced by",
") to learn the generated",
"The experiments are executed on 4 Nvidia A100",
"GPUs with 80GB each, using PyTorch in Python.",
"the hyperparameters in Appendix",
". The dataset",
"and metrics we utilize are the same as those de-",
"scribed in Sec.",
". All results are reported on the",
"that the performance increase evidently among",
"all LLMs during SFT. We then utilize MAPO to",
"prompts generated by MAPO are more adaptive",
"in QA and generation task for BLOOM (increase",
"by 20.5% for CloseQA and by 30.9% for Explan",
"by 21.4% for CloseQA and by 20.6% for Explan",
"compared with SFT (p<0.01)). And the prompts",
"are more adaptive in classification task (increase",
"by 22.8% for News (p<0.01)) for LLaMA. These",
"results indicate that MAPO effectively enhances",
"To validate the superiority of MAPO, we com-",
"pare it with several SOTA prompt optimization",
"Generalization Maintaining.",
"This step ad-",
"tasks but also on general NLP tasks. To achieve",
"dicated in Table",
", during the pre-training phase.",
"The objective of pre-training is to generate out-",
"one based on the original prompts. The original",
"baselines in various popular tasks based on the",
"same setting Roberta-Large, as shown in Table",
"our method applies not only to LLMs but also to",
"smaller LMs. We analyze the possible reasons as",
"and model scales, and small models require less",
"computational and storage resources, making RL",
"the performance of almost all baselines, except",
"RLprompt, does not exceed that of Fine-tuning",
"/SFT, and some even do not outperform the origi-",
"in LLMs. When we add RL, as in the case of",
"proposed MAPO method to optimize the prompt",
"in this study makes a substantial contribution to",
"Moreover, we conduct experiments to evaluate",
"by",
"Arora et al.",
"). Remarkably, we observe",
"that each LLM, when using prompts optimized",
"various downstream tasks. Specifically, BLOOM",
"pared with GPT-J and LLaMA. This experiment",
"We also test the performance of MAPO with",
"the above-mentioned SOTA prompt optimization",
"GPT-J, and LLaMA, to replace the BART model",
"used in Table",
"for verifying the nine datasets in",
"solely from using SFT are less than half of those",
"similar tasks and general NLP tasks. This clearly",
"The effect of warm-up dataset.",
"As shown in",
"MAPO’s performance. The results indicate that",
"as the size of the warm-up dataset increases, per-",
"Conversely, GPT-J shows a more gradual growth.",
"formance. Even with reduced dataset sizes, the",
"lighting the method’s suitability for low-resource",
"tasks. We also conduct few-shot experiments on",
"general NLP tasks with just 10% data and ob-",
"serve promising improvements. This underlines",
"the specific effects of PPO and RRMF during the",
"RL process, we conduct separate experiments to",
"experimental results, depicted in Fig.",
"(with de-",
"RRMF in enhancing the performance of MAPO.",
"We propose the following explanations for these",
"results: PPO focuses on reducing the dissimilar-",
"ity between the RL model and the SFT model.",
"RRMF aligns the scores from the reward model",
"a larger warm-up dataset, we use various propor-",
"tions of the warm-up dataset to progressively in-",
"to it as shown in Table",
"17",
". Our findings consis-",
"beyond what is achieved by SFT alone across all",
"proportions of the dataset. This affirms the effec-",
"RL begins to decline. While one could hypothe-",
"size that adding RL to a very large SFT dataset",
"might not result in as significant an improvement",
"as it would for a smaller dataset, this observation",
"actually underscores our method’s suitability for",
"Moreover, we have tried different number of",
"Extending the training time does not consistently",
"lead to performance improvements for SFT. In",
"porate randomness (e.g., temperature) during the",
"temperature range [0-0.5] for generation, within",
"temperature setting (t=0.8) tends to produce infe-",
"suggests that in a task like prompt optimization,",
"introducing a stable (low temperature) but slight",
"phrase “the term used” and substitutes “refer to”",
"with “denote”, while LLaMA adopts a more in-",
"formal style by mentioning the “commonly used",
"term”. In the generation task, both BLOOM and",
"structure but modifies the subjects and replaces",
"all three LLMs rearrange the word order and of-",
"fer additional details about the topic. Therefore,",
"However, there are also some errors during",
"prompt optimization, including prompts with in-",
"sitions or missing necessary parts, and prompts",
"ample room for improvement in MAPO to better",
"We conduct a case study to visualize the prompts",
"optimized by MAPO, as shown in Table",
"7",
". Addi-",
"tional cases are included in Appendix",
". We first",
"observe that the majority of the original prompts",
"have significant modifications after optimization",
"through our MAPO method. Only about 10% of",
"changed. To further quantify these changes, we",
"calculate a normalized edit distance. Given the",
"varying lengths of different prompt pairs, we di-",
"two strings. This yields a value between 0 and 1,",
"ized edit distance for all prompt pairs stands at",
"0.67, demonstrating that most prompts do experi-",
"each LLM, while either retaining high-frequency",
"words in instructions (including sentence, topics,",
"Taking the generation task (Fig.",
") as an exam-",
"retained, we observe that BLOOM retains a rel-",
"atively higher proportion of the original prompts",
"compared to GPT-J and LLaMA, while LLaMA",
"higher proportion of words like “room”, “close”,",
"“playing”, indicating its preferences on place and",
"different aspects during the optimization process.",
"editing. For example,",
"Li et al.",
") propose",
"put.",
"Qin and Eisner",
"2021a",
") optimize a mixture",
"tokens in the prompt.",
"Jiang et al.",
"2020",
"automatically generate diverse prompts. Further-",
"optimization instead of discrete prompt optimiza-",
"), etc. However, all above-mentioned",
"prompts optimization approaches aim to obtain",
"task-specific prompts instead of model-specific",
"ones. Different from theirs, we dedicate at opti-",
"In this work, we address the prompt optimization",
"Optimization (MAPO) approach. Through exten-",
"sive experiments, we demonstrated that MAPO",
"can adapt different LLMs with generating model-",
"friendly prompts to enhance their capabilities",
"across various downstream tasks. In future work,",
"we aim to construct more fine-grained model-",
"LLMs’ prompt optimization process involves",
"prompt retrieval, prompt generation from scratch",
"and prompt editing. For prompt retrieval, for ex-",
"ample,",
") adopt greedy search to",
") introduce SharpT, which learns a shared",
"latent space and generates soft prompts.",
"White",
"ing techniques.",
"Zamfirescu-Pereira et al.",
"investigate end-user prompt engineering using a",
"prototype LLM-based chatbot design tool.",
"Wang",
"2022a",
") present Self-Instruct for improving",
"instruction-following capabilities of PLMs. For",
"lect label words and generate templates.",
"Pryzant",
") introduce APO based on “gradients”",
"Zhang et al.",
") propose TEMPERA, which",
"Prasad et al.",
") introduce GrIPS, a gradient-",
"free approach for improving task instructions for",
"specific task, the performance gains from prompt",
"optimization may be constrained. Additionally,",
"This could limit the scalability of MAPO, es-",
"pecially when dealing with large-scale tasks or",
"datasets. Despite these limitations, our study pro-",
"This work is supported by Shanghai Munic-",
"ipal Science and Technology Major Project",
"(No.2021SHZDZX0103), Science and Technol-",
"tion Plan (No. 22511104700), Key Projects of",
"For the nine datasets selected in P3, we present",
"Inference Phase.",
"In the inference phase, an",
"optimized prompt is generated from an original",
"operation, which is",
". There is almost negli-",
"takes roughly 20 hours in total; For a Bloom-7B",
"model under the same hardware conditions, SFT",
"takes about 5 hours, RL takes about 9 hours, and",
"We compared MAPO with several State-Of-The-",
"Finetuning (",
"Devlin et al.",
"2018",
"): Finetuning",
"(few-shot) involves finetuning the entire lan-",
"Soft Prompt (",
"2021b",
"Li",
"and Liang",
"): Soft Prompt Tuning uti-",
"lizes continuous embeddings as a variant of",
"parameter-efficient transfer learning, replac-",
"the discrete part optimized using a gradient-",
"Specifically, once the prompt optimizing model",
"MAPO is trained, the prompt generation process",
"erate the optimal prompt instead of further opti-",
"mization through SFT and RL, thus significantly",
"Training Phase.",
"During the training phase,",
"and space complexity of",
", where",
"is the number of original prompts, and",
"is the",
"number of candidates per prompt. Subsequently,",
"comparisons among candidate prompts, yielding",
"complexities of",
"for both time and",
"space. Building the prompt optimizer is the next",
"stage. Supervised fine-tuning (SFT) has a time",
"complexity of",
", with",
"being the",
"number of epochs,",
"the batch size, and",
"the",
"number of model parameters. Its space complex-",
"ity mainly arises from model parameters and gra-",
"dients, which is",
". For building the reward",
"incorporates discrete trigger tokens and up-",
"dates prompts through iterative gradient",
"is the number of epochs specific to RL,",
"eters. The space complexity is",
". Summing",
"phase becomes",
"Manual (",
"Brown et al.",
"Schick and",
"Schütze",
"): Manual",
"phrase-level editing on the instructions and",
"In-Context (",
"): In-Context",
"Demonstration randomly selects a training",
"example and concatenates it with the input",
"Instructions: Self-Instruction manually cre-",
"Instructions (",
"Wang et al.",
"2022b",
"), where the",
"reinforcement learning, efficiently leverag-",
"ing prior knowledge and adapting to differ-",
"We utilized nine representative datasets from",
"P3 (",
") to establish the warm-up",
"dataset, covering question-answering, classifica-",
"tion, and generation tasks. The selected datasets",
"Question-Answering",
"Task:",
"AdverQA",
"https://huggingface.co/datasets/bigscience/",
"P3/tree/main/data/adversarial_qa_",
"dbidaf_question_context_answer",
"),",
"OpenQA",
"https://huggingface.co/",
"datasets/bigscience/P3/tree/main/data/",
"P3/tree/main/data/sciq_Direct_Question_",
"reformats tasks and prompts using the LLM",
"to effectively aggregate predictions across",
"Classification",
"News",
"https:",
"//huggingface.co/datasets/bigscience/P3/",
"tree/main/data/ag_news_classify",
"Movie",
"P3/tree/main/data/rotten_tomatoes_",
"Movie_Expressed_Sentiment",
"QASC",
"co/datasets/bigscience/P3/tree/main/data/",
"common_gen_topics_from_the_sentence",
"Summary",
"https://huggingface.co/datasets/",
"bigscience/P3/tree/main/data/samsum_",
"Sum_up_the_following_dialogue",
"Explan",
"P3/tree/main/data/cos_e_v1.11_generate_",
"For a fair assessment, we adopt the same exper-",
"imental setup as in LM-BFF (",
"Gao et al.",
"and RLPrompt (",
"). We take 16",
"training samples from each class in our training",
"dataset for every task, making them our few-shot",
"dataset. So, if we consider all the classes (Y), we",
"have a total of 16 times the number of classes as",
"our training samples. Similarly, we pick 16 sam-",
"examples from our training data. We call this our",
"“in-context exemplar pool”. For consistency, we",
"repeat our experiments four times using different",
"base our initial guidelines on the Natural Instruc-",
"first examples we give for context are randomly",
"our few-shot dataset and is also randomly picked",
"with these SOTA baselines, we gain insights into",
"the performance and effectiveness of MAPO in",
"with other SOTA baselines, on the following",
"Lee",
"2005",
"), CR (",
"Hu and Liu",
"2004",
"), RTE (",
"Da-",
"gan et al.",
"Haim et al.",
"2006",
"Giampiccolo",
"szky et al.",
"), SNLI (",
"Bowman et al.",
"2015",
"pared to other baselines across a range of tasks,",
"including sentiment analysis, text classification,",
"this part, but we happened to choose this subset.",
"To mitigate the impact on the results, we also run",
"another test with two steps: using the optimized",
"prompts generated by MAPO and then using the",
"original Roberta-Large model to make inference.",
"As shown in Table",
"(the row “MAPO-w/o g”),",
"a t-test greater than 0.05. The use of data from",
"the prompt-optimized model retains its original",
"capabilities for downstream tasks instead of data",
"that is all experiments, including our own, use",
"Roberta-large as the backbone for validating the",
"downstream tasks. Because the setup employs a",
"occur without training on the datasets in Table",
". The purpose is to demonstrate the gener-",
"alization (domain transfer) ability of our MAPO",
"method. If one wishes to further enhance perfor-",
"mance on these datasets, additional training with",
"19",
". We make detailed analysis for the difference",
"In SST-2, BLOOM and LLaMA both use",
"phrases like “terrific flair” and “remarkable",
"itive sentiment as strongly as BLOOM and",
"In Yelp, BLOOM and LLaMA use phrases",
"venture. GPT-J and the original version use",
"In CR, BLOOM, GPT-J, and LLaMA all ex-",
"ing the positioning of the space key on a",
"put is more common in phones, implying a",
"The performance of the reward model.",
"We",
"plot the performance of the reward model during",
"the training process of MAPO as shown in Fig.",
"As the training progresses, the reward model ex-",
"hibits consistent growth and improvement. The",
"is gradually becoming more proficient in down-",
"ment, leading to improved outcomes and higher",
"task completion rates. Therefore, it can serve as",
"a discriminator of the goodness of an optimized",
"of MAPO.",
"We further analyze the original capa-",
"bilities maintaining ability of MAPO. We use a",
"language model trained with MAPO, which has",
"complish downstream tasks. We believe that the",
"GLUE and SuperGLUE tasks are representative,",
"hence we use them as pre-training tasks. How-",
"ever, the improvements in Table",
"are not",
"significant, possibly due to the limited scope of",
"our pre-training tasks. Future work can explore",
"using a broader range of datasets for pre-training,",
"data for SFT, RM, and RL are different from the",
"more diverse NLP dataset should be selected for",
"impact of the situation by using phrases like",
"tain the same wording as the original sen-",
"tence 1. The variations in wording occur in",
"sizing the need for interest rates to increase,",
"while LLaMA focuses on the importance of",
"In QNLI, BLOOM, GPT-J, and LLaMA all",
"overall meaning. The variations in wording",
"are mainly stylistic, with BLOOM, GPT-J,",
"tences. The variations in the rephrased sen-",
"tence 1 (BLOOM and LLaMA) emphasize",
"the 15 percent drop in revenue, while GPT-J",
"In SNLI, BLOOM, GPT-J, and LLaMA",
"details related to the slip and slide activity",
"and the celebratory context. The variations",
"description of the baby’s experience and the",
"We further analyze the distribution of the top 3",
"prompts of different LLMs in both the QA and",
"classification tasks as shown in Fig.",
"8",
"and Fig.",
"9",
", respectively. In the QA task, we observe min-",
"imal variations when considering whether to re-",
"move the instruction. After prompt optimization,",
"BLOOM has a higher proportion of words like",
"“education”, “power”, and “life” compared to the",
"other two models. GPT-J has a higher proportion",
"of words like “church”, “time”, “order”, “early”,",
"tion of words like “earlier”, “similar”, “number”,",
"compared to the other two models. GPT-J has a",
"“national”, “school” and “poland”. LLaMA has a",
"higher proportion of words like “increase”, “gov-",
"erment”, “executive”, “medical”, “warsaw”, and",
"specific areas of focus and the semantic nuances",
"ing or LLM-based feedback for prompt optimiza-",
"metrics, we iterate the searching process and find",
"under the actor model and the frozen model.",
"based on the given prompt",
". If there is",
"about 12 hours, and the complete MAPO process",
"2019",
"), which are considered representative, as in-",
"improves SFT performance as shown in Table",
"18",
"settings (t=0) are likely to produce more accurate",
"higher proportion of words like “match”, “grass”,",
"GrIPS (",
"): GrIPS performs",
"ent queries, while providing an interpretable",
"Yelp Polarity (",
"), MR (",
"Pang and",
"skill” to describe Khouri’s ability, emphasiz-",
"stream tasks. It successfully adapts to its environ-",
"The original capabilities maintaining ability",
"In MRPC, BLOOM, GPT-J, and LLaMA all",
"optimization across different tasks. The observed",
"including prompt retrieval (",
"Zhou",
"reinforcement learning. Specifically, we first gen-",
"a widely-used prompt benchmark which contains",
"puts that are as good as or better than the original",
"more, some research focus on continuous prompt",
"identify near-optimal prompts.",
"Zhou et al.",
"our approach. Firstly, the effectiveness of prompt",
"ing efforts in improving the performance of these",
"a complete training run. For a LLaMA-7B model",
"picked from a set of 16. This set is different from",
"words from the original prompts in the optimized",
"6B (",
"Wang and Komatsuzaki",
"), and LLaMA-",
"tails altering the structure or wording of",
"to pro-",
"exhibit poorer inference results, while prompts",
"process. We provide the detailed configuration of",
"The effect of RL compared with SFT.",
"From the",
"tails provided in Table",
"10",
"in Appendix",
"), clearly",
"completely different strings. The average normal-",
"Besides this usual setup, we also select",
"random",
"tween the results. For our language model, we’ve",
"hensive evaluation of MAPO’s performance com-",
"the results do not show a significant decline, with",
"mized by our proposed MAPO as shown in Table",
"“contemporary”, “french”, “Methodist”, “places”,",
"active voice into passive voice, GPT-J utilizes the",
"which are more relative with human. GPT-J has a",
"latter, although very important, has not been stud-",
"ploying SFT prior to reinforcement learning (RL)",
"To optimize this objective, we employ supervised",
"exhibits the highest increase in performance com-",
"clearly demonstrates the significant domain trans-",
"formance typically improves. BLOOM is particu-",
"(temperature=0,0.2,0.5,0.8) as shown in Table",
"14",
"Autoprompt (",
"Shin et al.",
"): Autoprompt",
"AMA (",
"): AMA recursively",
"2007",
"Bentivogli et al.",
"2009",
"), QNLI (",
"Dem-",
"rephrase the sentence 1 by adding additional",
"O",
"ptimization (i.e. MAPO) approach for LLMs in",
"each candidate prompt into an LLM, respectively,",
"specific scenes, objects, or themes. LLaMA has a",
"optimization heavily relies on the availability and",
"prompt achieves strong performance on vari-",
"from Table",
". This is because we consider GLUE",
"during the SFT process. The primary objective of",
"Apart from Fine-tuning, we also freeze the LLMs",
"vide the edit distance by the average length of the",
"We also caclulate how long it roughly takes for",
"ates prompts for each task following Natural",
"MNLI (",
"Williams et al.",
"), MRPC (",
"Dolan and",
"suit” to highlight the positive qualities of the",
"concept of “model-preferred prompts” or “model-",
"responses that align with its preferences, utilizing",
"text ‘original prompt’ while keeping the semantic",
"and ROUGE-L (",
"Lin",
") for QA, classification,",
"model). We feed prompts",
"generated by the SFT",
"nal LLMs. This highlights the importance of SFT",
"RLprompt, the performance on downstream tasks",
"RL for prompt optimization. Moreover, using our",
"movie classification tasks. These experimental re-",
"formance heavily relies on the quality of prompts.",
"ogy Commission of Shanghai Municipality Grant",
"which automatically optimizing prompts using re-",
"intentions with the specific prompt preferences of",
"of different LLMs. This approach is motivated by",
"model, which is",
", and a frozen model, which",
"make further optimization. We find the optimized",
"the generated prompt pairs remain completely un-",
"where 0 indicates identical strings and 1 indicates",
"Table",
"for generalization is merely to ensure that",
"and SuperGLUE tasks to be representative, hence",
"These findings strongly suggest that each LLM",
"dataset from P3 (",
") covering three",
"BLOOM (7B), GPT-J (6B), and LLaMA (7B), on",
"structGPT (",
"Ouyang et al.",
"). We sample 10%",
"with the likelihood probabilities of an LLM. Both",
"mining-based and paraphrasing-based methods to",
"While the training phase is computationally inten-",
"computational complexity during the training and",
"balanced assessment. GPT-J and the original",
"the ability to optimize prompts but without losing",
"“song”, and “property” compared to the other two",
"shared latent space and generates soft prompts us-",
"generation task, the results of all three models are",
"model. The reward model takes a prompt as input",
"In order to further demonstrate RL is necessary",
"Shanghai Science and Technology Innovation Ac-",
"gible extra space required, making the space com-",
"sentence 2, with BLOOM and GPT-J empha-",
"has a higher proportion of the word “year”, “new”",
"use the following instruction as input for GPT-3.5",
"Next, we construct a reward model to learn the ef-",
"The effect of PPO and RRMF.",
"To investigate",
"do not require high creativity, we have set a lower",
"tion mentioned before, such as research by",
"Zheng",
"guage model with a classification head using",
"represents the real-time feedback from the reward",
"porating additional knowledge to enhance prompt",
"a GPT-J-6B model, SFT takes about 10 hours, RL",
"version have the same wording, emphasizing",
"finding the optimal prompt for each specific LLM",
"refers to the similarity, between the outputs gener-",
"mation of scores by the reward model. To address",
"outputs that are closer to our optimal results. This",
"“decorate” with “adorn”. In the classification task,",
"DSP to generate “directional stimulus” of each in-",
"“admirable endeavor” and “praiseworthy pur-",
"suggests that using a different key for text in-",
"and “year”, indicating a focus on temporal and se-",
"variations in word distribution clearly indicate the",
"on each task is meaningful, as it can help enhance",
"SFT is to enhance the LLM’s capacity to generate",
"by combining prompts from the ranking sequence",
"decrement in performance remains minimal, high-",
"on the generated output, we conduct an additional",
"The remarkable capabilities of LLMs have revolu-",
"tionized NLP in various tasks. However, their per-",
"environments. Additionally, we intend to enhance",
"(No. 22511105902), the National Natural Science",
"like GPT-3.5. For each original prompt, 1000 can-",
"an optimal prompt is searched for, which involves",
"learning; in prompt generating from scratch,",
"Pang",
"mantic meaning similar to the original prompt but",
"Searching for the Optimal Prompt.",
"To deter-",
"and generation tasks, respectively. Based on these",
"cess (referred to as",
") and replace the softmax",
"We list more cases whose prompts have been opti-",
"models. In the classification task, we also observe",
"ing, some approaches rely on reinforcement learn-",
"specifically designed to optimize the original",
") into the RL model",
"to obtain an optimized",
"further improves performance over RLprompt, ex-",
"to provide critical feedback on the current prompt.",
"ing combines discrete and soft prompts, with",
"didate prompts are generated. This leads to a time",
"descriptions without explicitly conveying the",
"and the SFT model, it can result in inaccurate esti-",
"to LM. Fine-tuning smaller models is feasible and",
"RL is a widely-used algorithm across applications",
"introduce APE for automatic instruction selection,",
"batch size in RL, and",
"remains the model param-",
"which may lead to more significant improvements",
"tion (",
").",
"stream tasks.",
"then model the prompt optimization problem with",
"with Proximal Policy Optimization (PPO) (",
"Schul-",
"various models. Our contributions are threefold:",
"tasks.",
"•",
"ent Large Language Models (LLMs) in order",
"NLP downstream tasks.",
"MAPO is shown in Fig.",
"Model-Adaptive Prompt Optimizer (MAPO),",
"formance in a variety of downstream tasks.",
"dataset for prompt optimization.",
"P",
"subsequent tasks.",
"3.2.1",
"Supervised Fine-tuning",
"answering/classification) task.”.",
"3.2.2",
"Building Reward Model",
"are shown in Appendix",
"inal prompt, we compare the match degree, which",
"{",
"P, P",
"}",
"distribution is shown in Table",
", P",
", . . . , P",
"ˆ",
"LLM",
"r",
"+",
"γV",
"−",
"V",
"value",
"as follows:",
"L",
"=",
"t",
"(2)",
"∥",
"(3)",
", . . . , P, P",
"illustrated below:",
"\u0000",
"\u0001",
"[",
"log",
"σ",
"x, y",
"))]",
"(1)",
"x",
"y",
"K",
"an LLM’s preferences.",
"3.2.3",
"Reinforcement Learning",
"denotes the true value.",
"π",
"model. The loss function is defined as follows:",
"∼",
")]",
"(4)",
"Finally, we combine the above loss functions to",
"The final loss function is defined as:",
"α",
"(5)",
"by RRHF (",
")) for joint learning.",
"experiments (the same applies below).",
"). The loss function is as follows:",
"γE",
"(11)",
"and the loss function is defined as follows:",
"βlog",
"|",
"/π",
"))",
"(6)",
"pre-training.",
"γ",
"(12)",
"ness of MAPO.",
"tively.",
"We name it Ranking Responses from Model Feed-",
"learning with a rank loss, defined as follows:",
"p",
"(7)",
"X",
"max",
"(0",
", p",
"(8)",
"prompts",
"with the highest reward",
"and datasets are in Appendix",
"Σ",
"logP",
"(9)",
"SFT approximating as follows:",
"β",
"(10)",
"erences in different downstream tasks.",
"more feasible on them.",
"fer capability of MAPO.",
"model-adaptive prompts.",
"narios of data scarcity.",
"to prompts that are more adaptable to the model.",
"low-resource scenarios.",
"essarily occur at the final epoch.",
"the best results.",
"tion and adjusting tone or structure as necessary.",
"adapt to different LLMs in downstream tasks.",
"ence substantial modifications.",
"included, following) or removing them.",
"bution indicate that each LLM tends to emphasize",
"Accurate conclusions need more experiments.",
"and achieve impressive performance.",
"tic contexts.",
"LLMs in practical applications.",
"prompts for each dataset, as shown in Table",
"Other hyper-parameters are set to default.",
"it’s",
"N",
"×",
"M",
") +",
"T",
"plexity effectively",
"for this phase.",
"takes about 26 hours.",
"ing the following:",
"a few-shot dataset.",
"ing discrete prompts.",
"free tuner.",
"inference phase:",
"initially, a warm-up dataset is established. This in-",
"search.",
")+",
"on training examples.",
"selects the best one.",
"query.",
"learning (RL) framework.",
"prompt is concatenated with the inputs.",
"for each task are as follows:",
"Closed_Book_",
"prompt for each query.",
"prompts using weak supervision.",
"P3/tree/main/data/qasc_is_correct_1",
"explanation_given_text",
"n",
"various downstream tasks.",
"cation, etc.",
"leakage.",
"labeled data on Table",
"becomes necessary.",
"among LLMs as follows:",
"LLaMA.",
"the hit-or-miss nature of the service.",
"positive or negative sentiment.",
"deviation from the norm.",
"prompt.",
"in various downstream tasks.",
"emotional aspect.",
"boosting savings.",
"vey the same information.",
"maintains a more neutral tone.",
"wetness.",
"optimization dynamics exhibited by each LLM.",
"“parliament” compared to the other two LLMs.",
"), and prompt editing (",
"represents the reward of the current",
"optimization is to maximize the scores of prompts",
"tain similarity between the RL model and the SFT",
"by MAPO, displays improved performance across",
"ous natural language understanding and natu-",
"ral language generation tasks without relying",
"TEMPERA (",
"): TEMPERA",
"Brockett",
"). These datasets provide a compre-",
"with that used by TEMPERA (",
"different prompts should be adapted to differ-",
"ated by",
"without instruct-tuning. We pass the",
"The reported results represent the best-performing",
"LLM among the three LLMs, which indicates that",
"improving the performance and accuracy in down-",
"directly list the SFT results in the Fine-tuning row.",
"of different proportions of the warm-up dataset on",
"our method’s adaptability and effectiveness in sce-",
"12",
", Table",
"15",
"and Table",
"16",
". Each experiment",
"with ambiguous meanings, etc. Therefore, there is",
"challenge by proposing a Model-Adaptive Prompt",
"one prompt and the corresponding three candidate",
"we use them as pre-training tasks. Theoretically, a",
"measures also differ greatly among three LLMs in",
"MAPO, a model-adaptive prompt optimization ap-",
"RL training process. The frozen model serves as a",
"achieve this, we utilize a combination of Proximal",
". The reinforcement learning",
"these up, the total time complexity for the training",
"its original capabilities, to modify prompts and ac-",
"specific prompts”, emphasizing that there’s a need",
"generalization and also achieves superior per-",
"ploy the ranking sequence to train a reward model.",
"coverage. LLaMA maintains the original sentence",
"for different LLMs while preserving core informa-",
"we notice that BLOOM has a higher proportion of",
"“bathroom”, “white”, which suggests it focuses on",
"LLMs. Moreover, some research focuses on incor-",
"sive, the generation phase is relatively lightweight.",
"process. Additional experiments will contribute to",
"the optimal prompt",
"for an LLM in downstream",
"tiveness of RL irrespective of the SFT dataset size.",
"The effect of the Randomness.",
"We also incor-",
"Industrial Foresight and Key Core Technology Re-",
"number of epochs is set to 20 in the entire training",
"specific task. Lower temperature (t=0.2) or greedy",
"subjects, present, statement, discussed, mentioned,",
"dataset is limited or does not sufficiently cover the",
"MAPO requires extensive SFT and RL, which can",
"have ushered in a transformative era in natural lan-",
"vary widely when they are provided with the same",
"Reinforcement Learning (RL) to optimize original",
"adjust the bias in the reward model’s scoring since",
"shown in Table",
", which to construct environment-",
"optimized prompts are both set to 512 tokens. The",
"PPO and RRMF aim to assign higher probabilities",
"of prompts using gradient descent to generate rela-",
"mizing prompts for LLMs within the NLP domain",
"prompt editing,",
") automatically se-",
"quality of the warm-up dataset. In cases where the",
"openbookqa_main_which_correct",
"), CloseQA",
"cess, based on the outputs generated by",
"with",
"model based on the datasets shown in Table",
"(i.e.,",
"equals fine-tuning pretrained language models, we",
"complete sentences, prompts with improper prepo-",
"). While effective, such approaches are costly",
"task-specific prompts. Therefore, it is necessary to",
"prompts to establish a warm-up dataset. After that,",
"the policy gradient loss (i.e., actor’s loss) based on",
"achieved by our proposed MAPO method, both on",
"tently show that RL adds value to the performance",
"plexity here is dominated by a single feed-forward",
"Generation Task: Topics (",
"https://huggingface.",
"Specifically, for Table",
", the training data aligns",
"guage processing, showcasing their remarkable ca-",
"Next, we compare the match degree with specified",
"and RRMF algorithms (note that RRMF is inspred",
"degree of variability (non-zero temperature) yields",
"Fig.",
", Fig.",
". We extract the three most",
"tional knowledge.",
") develop Auto-",
"ing positive sentiment. GPT-J uses the phrase",
"scribing the washed-up marine animals. GPT-",
"Generating Candidate Prompts.",
"The original",
"represents the reward of each optimized prompt",
"Foundation of China (No.62072323, U21A20488),",
"the total time for MAPO takes about 14 hours; For",
"Art (SOTA) prompt optimization baselines, includ-",
"erates discrete prompts using a reinforcement",
"ples from each class to form our validation dataset.",
"tions (",
"Mishra et al.",
"). We also ensure that the",
"“tremendous artistry,” highlighting the artistic",
"In RTE, BLOOM and LLaMA emphasize the",
"higher proportion of words like “largest”, “music”,",
"MAPO model exhibits greater robustness and",
"prompt that produces an output most similar to the",
"optimize the RL policy from multiple perspectives.",
"SFT model), as evaluated by the reward model. To",
"In this section, We conduct experiments with three",
"common, requiring fewer computational resources.",
"evaluate the contributions of each component. The",
"which we aim to generate optimal prompts. To fur-",
"retains the fewest. When these words are removed,",
"phone. The wording in BLOOM and LLaMA",
"erate candidate prompts and search for the optimal",
"and employ Pairwise Ranking Loss for training, as",
"increases, the margin of improvement from adding",
"aspect, but does not explicitly convey the pos-",
"press confusion or potential confusion regard-",
"and LLaMA using different synonyms to con-",
"exhibits unique preferences and patterns in prompt",
"Advancements in Large Language Models (LLMs)",
"Subsequently, we employ Reinforcement Learning",
"Joint learning.",
"Finally, we make joint learning",
"indicates the effectiveness of MAPO in optimizing",
"However, as the proportion of the warm-up dataset",
"takes about 16 hours, and the total time for MAPO",
"simply involves a feed forward propagation to gen-",
"move the instruction. After optimization, BLOOM",
"for inference and obtain the corresponding outputs.",
"prompt (",
") into an optimized prompt (",
") tailored",
"corresponding test sets or 10% dev sets if a dataset",
"MAPO demonstrates its prompt optimization capa-",
"is a test-time prompt editing method that uses",
"consistent increase indicates that the reward model",
"ing a lightweight prompt generator; in prompt edit-",
"and response abilities, their performance is heavily",
"tensive experiments which validates the robustness",
"still differences in the specific distribution patterns.",
"follows: MAPO employs both SFT and RL to opti-",
"ther investigate the impact of varying temperatures",
"rior prompts that lead to less accurate outputs for a",
"bilities by adapting better prompts to specific tasks",
"its applicability across a broad spectrum of linguis-",
"search and Development in Suzhou(SYC2022009).",
"quential aspects. And LLaMA has a higher propor-",
"LLM, thereby enhancing its performance on down-",
"However, the aforementioned research primarily",
"this probability with the score of the reward model.",
"compared with SFT (p<0.01)) and GPT-J (increase",
"ple, when high-frequency words in instructions are",
"neutral language without explicitly conveying",
"in wording are mainly stylistic, enhancing the",
"tain rewards. In this process, we introduce an actor",
", as shown in Table",
". Due to SFT in LLMs",
"frequent words from the original prompt and inves-",
") describe a catalog of prompt engineer-",
"prompt using the MAPO technique. The time com-",
"per, we novelly propose a",
"odel-",
"daptive",
"rompt",
"each downstream task. Therefore, we consider that",
"Policy Optimization (PPO) (",
"Schulman et al.",
"group runs 5 times. Our findings reveal that a high-",
"datasets for validation: SST-2 (",
"Socher et al.",
"2013",
"natural language inference, and paraphrase identifi-",
"“service provided is inconsistent” to provide a",
"), prompt generation from scratch (",
"Bubeck et al.",
"Lyu et al.",
"). While",
"for a systematic method to automatically align user",
"each model (",
"2024",
"). Therefore, in this pa-",
"obtain candidate prompts from an oracle LLM, and",
"and generalization of the proposed MAPO. To sum",
"In this section, we conduct empirical study on three",
"7B (",
")) to evaluate their separate per-",
"formance on question-answering (QA) (",
"2024b",
"2023b",
"), classification (",
"2023c",
"a",
"tasks with same task-specific prompts. We use nine",
"downstream tasks with details in Appendix",
". P3 is",
"distributed within the range of 0 to 0.5, but there are",
"Moreover, the medians, means, and other statistical",
"proach for LLMs. It takes the original prompt as in-",
"put and generate an optimized prompt which makes",
"conduct supervised fine-tuning (SFT) with an LLM",
"across multiple downstream tasks. The objective of",
"mamurthy et al.",
") supports the notion that em-",
"may have different expressions. To achieve this, we",
"mine which candidate prompt is optimal for an orig-",
"ated using a candidate prompt and the ground truth",
"ground truth output. When a ground-truth output is",
"as GPT-3.5, is regarded as the ground-truth. Specif-",
"tasks. The warm-up dataset consists of a collection",
"the fact that discriminative annotation through sort-",
"ated by candidate prompts",
"is SFT model with its parameters frozen during the",
"the importance ratio and reward",
"to the right demonstrate better results. Next, we em-",
"prompts, respectively, in the corresponding ranking",
"model,",
"is the set of ranking pairs, and",
"denotes",
"the number of candidate prompts. Through this pro-",
"the given prompt, the reward model learns to assign",
"higher scores (rewards) to better prompts and lower",
"scores (rewards) to inferior prompts, thus imitating",
"(i.e., importance ratio) of generating the same token",
"model",
"and calculate a score (i.e., reward), which",
"the distribution of generated prompts might change",
"action pairs. The environment refers to the original",
"prompt, and the action represents the prompt gener-",
"environment-action pairs to the reward model to ob-",
", and",
"represent the optimal weights",
"prompts are taken from Natural Instructions (",
"represents the selected datasets for",
"with the above-mentioned loss functions as follows:",
"different downstream tasks to validate the effective-",
"represent prompts",
"generated by RL model and the SFT model, respec-",
"on “model feedback” instead of “human feedback”.",
"DeepSpeed",
"is utilized in the training process. The",
"maximum sequence length for original prompts and",
"does not have a test set. Details of all used baselines",
"Finally, we combine the above loss functions for",
"The main results are shown in Table",
". We observe",
"the performance of various LLMs and exhibits pref-",
"dresses the issue of catastrophic forgetting by ensur-",
"ing that an LLM performs well not only on specific",
"this, we follow a similar approach as outlined in In-",
"data from general NLP tasks in GLUE (",
") and the SuperGLUE benchmark (",
"mize LLMs. In fact, the SFT process is not specific",
"surpasses that of SFT, indicating the significance of",
"cept in a very few cases, such as using BLOOM for",
"sults demonstrate that the MAPO method proposed",
"the domain transfer performance of MAPO. The re-",
"sults are presented in Table",
", while the",
"results of LLMs with original prompts are reported",
"baselines. We use three LLMs, including BLOOM,",
"and only modify the prompts for inference on down-",
"stream tasks. According to the experimental results,",
"experiments (Table",
"), we can",
"observe that the performance improvements gained",
"11",
", our study examines the effects",
"larly sensitive, showing a pronounced growth trend.",
"LLaMA’s performance reveals an inflection around",
"60%, suggesting other factors also influence its per-",
"demonstrate the important roles played by PPO and",
"and how it compares to simply extending SFT with",
"crease the SFT training data and then introduce RL",
"epochs to see if extended training time consistently",
"some instances, the performance even declines. We",
"also find that some peak performance does not nec-",
"generation process of LLM. Given that our prompts",
"set of experiments to assess the performance of the",
"MAPO method under different randomness settings",
"GPT-J present similar prompts that emphasize topic",
"Next, we provide a detailed examination of these",
"modifications. In the QA task, BLOOM transforms",
"We conduct an exploratory analysis to further inves-",
"tigate the patterns in optimized prompt as shown in",
"tigate their distribution in the optimized prompt for",
"words like “man”, “view” in its optimized prompts,",
"experiences. The variations observed in word distri-",
"prompt, a gradient-guided approach to find the best",
"Hambardzumyan et al.",
"Zhong",
"adaptive prompts that can adapt to the continuously",
"evolving data encountered in real-world production",
"etc. For prompt generation from scratch,",
"Pang et al.",
") propose RLprompt based on RL.",
"provides interpretable prompts for different queries.",
"It is important to acknowledge certain limitations of",
"be computationally expensive and time-consuming.",
"vides valuable insights into model-adaptive prompt",
"optimization for LLMs and contributes to the ongo-",
"We provide the training details as shown in Table",
"running on four A100 80GB GPUs, SFT on a high-",
"scale task (such as the News classification task with",
"120,000 training data) takes about 8 hours, RL takes",
"Black-Box (",
"Sun et al.",
"): Black-Box Tun-",
"the soft part trained using gradient descent and",
"reducing the computational complexity. We list the",
"volves generating candidate prompts using a model",
"model, both time and space complexities are mainly",
"(RL) part requires",
"time, where",
". For space,",
"RLprompt (",
"): RLprompt gen-",
"random seeds. Afterward, we calculate the average",
"results and note down the usual variation we see be-",
"chosen to use RoBERTa large (",
"Liu et al.",
"). We",
"from our main training data. By comparing MAPO",
"We evaluate our proposed MAPO method, along",
"“few-shot” methodology that has elaborated before,",
"we name Table",
"as “few-shot”. For Table",
"there is no training data involved; the LM performs",
"zero-shot inference. That means all reported results",
"like “quality of the food is commendable” and",
"In MR, BLOOM and LLaMA use phrases like",
"Moreover, for Table",
", the training and validation",
"data used for generalization, although they all come",
"“somber site” and “distressing sight” when de-",
"J and the original version provide more neutral",
"rephrase the sentence 2, maintaining the same",
"maintain the same wording as the original sen-",
"In MNLI, BLOOM, GPT-J, and LLaMA main-",
"a more comprehensive understanding of the prompt",
"minimal variations when considering whether to re-",
"that each LLM emphasizes during the optimization",
"Prompt engineering, as an efficient and effec-",
"tive way to leverage Large Language Models",
"(LLM), has drawn a lot of attention from the",
"prompts to specific tasks, rather than specific",
"LLMs. However, a good prompt is not solely",
"first quantitatively demonstrate that different",
"a model-adaptive prompt optimizer (MAPO)",
"each specific LLM in downstream tasks. Ex-",
"method can effectively refine prompts for an",
"Table 1:",
"Table 2:",
"Table 3: The few-shot performance of SFT, MAPO",
"on LLaMA with original, SFT-optimized and MAPO-",
"Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-",
"berg, et al. 2023. Sparks of artificial general intelli-",
"Yuyan Chen, Qiang Fu, Ge Fan, Lun Du, Jian-Guang",
"Lou, Shi Han, Dongmei Zhang, Zhixu Li, and",
"Yanghua Xiao. 2023a. Hadamard adapter: An ex-",
"Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen,",
"Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li,",
"Robustly discerning reliable answers in large lan-",
"guage models. In",
"Proceedings of the 32nd ACM",
"Yuyan Chen, Zhixu Li, Jiaqing Liang, Yanghua Xiao,",
"ence on Web Search and Data Mining",
", pages 465–",
"Yuyan Chen, Yanghua Xiao, Zhixu Li, and Bang Liu.",
"2023d.",
"Xmqas: Constructing complex-modified",
"question-answering dataset for robust question un-",
"and-clip: Informative-yet-concise evidence distilla-",
"tion for answer explanation. In",
"2022 IEEE 38th In-",
"Simran Arora, Avanika Narayan, Mayee F Chen, Lau-",
"eric Sala, and Christopher Ré. 2022. Ask me any-",
"a large-scale humor response dataset with chain-of-",
"humor interpretation. In",
"Proceedings of the AAAI",
"Conference on Artificial Intelligence",
", volume 38,",
"Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo",
"Giampiccolo. 2009. The fifth pascal recognizing",
"Yuyan Chen, Jin Zhao, Zhihao Wen, Zhixu Li, and",
"Yanghua Xiao. 2024b. Temporalmed: Advancing",
"language models. In",
"Proceedings of the 17th ACM",
"International Conference on Web Search and Data",
"Samuel R Bowman, Gabor Angeli, Christopher Potts,",
"Ido Dagan, Oren Glickman, and Bernardo Magnini.",
"lenge. In",
"Machine learning challenges workshop",
"Tom Brown, Benjamin Mann, Nick Ryder, Melanie",
"Askell, et al. 2020. Language models are few-shot",
"Transforming question answering datasets into nat-",
"ural language inference datasets.",
"arXiv preprint",
"Sébastien Bubeck, Varun Chandrasekaran, Ronen El-",
"dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,",
"evaluation of summaries.",
"In",
"Text summarization",
"Wang, Han Guo, Tianmin Shu, Meng Song, Eric P",
"discrete text prompts with reinforcement learning.",
"dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,",
"Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
"Roberta: A robustly optimized bert pretraining ap-",
"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and",
"Bill Dolan and Chris Brockett. 2005.",
"Automati-",
"Third International Workshop on Paraphrasing",
"Weimin Lyu, Sheng Huang, Abdul Rafae Khan,",
"Shengqiang Zhang, Weiwei Sun, and Jia Xu. 2019.",
"Cuny-pku parser at semeval-2019 task 1: Cross-",
"Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.",
"Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang,",
"agnostic detector for insertion-based backdoor at-",
"textual entailment challenge. In",
"Proceedings of the",
"ACL-PASCAL workshop on textual entailment and",
"Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu,",
"Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu,",
"Qinghua Hu, and Bingzhe Wu. 2023.",
"Fairness-",
"ment challenge. In",
"Proceedings of the Second PAS-",
"CAL Challenges Workshop on Recognising Textual",
"Swaroop Mishra, Daniel Khashabi, Chitta Baral, and",
"Hannaneh Hajishirzi. 2021. Cross-task generaliza-",
"Karen Hambardzumyan,",
"Hrant Khachatrian,",
"Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022.",
"Optimizing prompts for text-to-image generation.",
"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,",
"Sandhini Agarwal, Katarina Slama, Alex Ray, et al.",
"2022. Training language models to follow instruc-",
"tions with human feedback.",
"Advances in Neural",
"Minqing Hu and Bing Liu. 2004. Mining and summa-",
"ACM SIGKDD international conference on Knowl-",
"Bo Pang, Semih Yavuz, Caiming Xiong, and Yingbo",
"Zhou. 2023. Sharpt: Shared latent space prompt",
"Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra",
"Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit",
"Bansal. 2022. Grips: Gradient-free, edit-based in-",
"Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham",
"Neubig. 2020.",
"How can we know what language",
"models know?",
"Transactions of the Association for",
"Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:",
"Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-",
"guang Zhu, and Michael Zeng. 2023. Automatic",
"prompt optimization with\" gradient descent\" and",
"Alex Wang, Amanpreet Singh, Julian Michael, Felix",
"Hill, Omer Levy, and Samuel R Bowman. 2018.",
"for natural language understanding.",
"American Chapter of the Association for Computa-",
"tional Linguistics: Human Language Technologies",
"Ben Wang and Aran Komatsuzaki. 2021.",
"Gpt-",
"j-6b:",
"billion",
"parameter",
"autoregressive",
"language model",
"https://github.com/kingoflolz/",
"Rajkumar Ramamurthy,",
"Prithviraj Ammanabrolu,",
"Kianté Brantley, Jack Hessel, Rafet Sifa, Christian",
"Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.",
"2022. Is reinforcement learning (not) for natural",
"language processing?: Benchmarks, baselines, and",
"Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-",
"isa Liu, Noah A Smith, Daniel Khashabi, and Han-",
"Victor Sanh, Albert Webson, Colin Raffel, Stephen H",
"Bach, Lintang Sutawika, Zaid Alyafeai, Antoine",
"Chaffin, Arnaud Stiegler, Teven Le Scao, Arun",
"Raja, et al. 2021. Multitask prompted training en-",
"ables zero-shot task generalization.",
"Yizhong Wang, Swaroop Mishra, Pegah Alipoor-",
"molabashi,",
"Yeganeh Kordi,",
"Amirreza Mirzaei,",
"Anjana Arunkumar, Arjun Ashok, Arut Selvan",
"Dhanasekaran, Atharva Naik, David Stap, et al.",
"Teven Le Scao, Angela Fan, Christopher Akiki, El-",
"lie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman",
"Matthias Gallé, et al. 2022.",
"Bloom:",
"A 176b-",
"Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse",
"Spencer-Smith, and Douglas C Schmidt. 2023. A",
"Timo Schick and Hinrich Schütze. 2020.",
"Exploit-",
"ing cloze questions for few shot text classification",
"and natural language inference.",
"Adina Williams, Nikita Nangia, and Samuel R Bow-",
"man. 2017. A broad-coverage challenge corpus for",
"sentence understanding through inference.",
"arXiv",
"John Schulman, Filip Wolski, Prafulla Dhariwal,",
"Alec Radford, and Oleg Klimov. 2017.",
"Proxi-",
"mal policy optimization algorithms.",
"Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,",
"Songfang Huang, and Fei Huang. 2023.",
"Rrhf:",
"Rank responses to align language models with",
"human feedback without tears.",
"Taylor Shin, Yasaman Razeghi, Robert L Logan IV,",
"Eliciting knowledge from language models with",
"automatically generated prompts.",
"JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern",
"prompt: how non-ai experts try (and fail) to design",
"llm prompts. In",
"Proceedings of the 2023 CHI Con-",
"ference on Human Factors in Computing Systems",
"Richard Socher, Alex Perelygin, Jean Wu, Jason",
"Proceedings of the 2013 conference on empiri-",
"cal methods in natural language processing",
", pages",
"urmans, and Joseph E Gonzalez. 2023. Tempera:",
"Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.",
"Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing",
"ference on Machine Learning",
", pages 20841–20855.",
"and Samuel Bowman. 2019. Superglue: A stick-",
"ier benchmark for general-purpose language under-",
"standing systems.",
"Advances in neural information",
"Factual probing is [mask]: Learning vs. learning to",
"Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,",
"Ba. 2022. Large language models are human-level",
"Figure 5:",
"QA, C: classification, G:generation.",
"means the number of performance decline.",
"(%) means the percentage of",
"Table 12: The few-shot performance of MAPO with SOTA prompt optimizing baselines in downstream tasks.",
"F: Finetuning, C: Continous prompt, D: Discrete prompt. MAPO means using MAPO with temperature [0,0.5].",
"Table 14:",
"Table 16: Domain transfer performance with a frozen",
"MAPO-Optimized prompts. CLS: Classification, M:",
"MAPO. The (",
") and",
"represent the absolute improve-",
"ment scores of MAPO-optimized prompts compared",
"tively. The (",
"(%) ) and",
"(%) represent the relative im-",
"provement percentages of MAPO-optimized prompts",
"on BLOOM and GPT-J with original and MAPO-",
"Optimized prompts. CLS: Classification, M: MAPO.",
"of MAPO-optimized prompts compared with original",
"prompts in each dataset and task, respectively. The",
"(%) represent the relative improvement",
"percentages of MAPO-optimized prompts compared",
"Table 18: Performance of different number of epochs when training SFT. Best Performance means the best",
"performance within 20 epochs. Best Epoch means the epoch corresponding to the best performance. We list",
"the performance in the 1,5,10,15,20,50 epochs. We bold the performance metrics where a longer training epoch",
"ture of the LLM in question. In this work, we",
"bidirectional transformers for language understand-",
"2006. The second pascal recognising textual entail-",
"Jonathan May. 2021. Warp: Word-level adversarial",
"with large language models. In",
"CHI Conference on",
"Table 10:",
"Figure 7:",
"Figure 8:",
"Figure 9:",
"preet Singh, Julian Michael, Felix Hill, Omer Levy,",
"with original prompts in each dataset and task, respec-",
"Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy",
"and Yanghua Xiao. 2023b. Hallucination detection:",
"Table 5: Zero-shot domain transfer performance based on BLOOM and GPT-J with original, SFT-optimized and",
"pre-trained language models. In",
"Yuyan Chen, Yichen Yuan, Panjun Liu, Dayiheng Liu,",
"lingual semantic parsing with ucca. In",
"Proceedings",
"guided few-shot prompting for large language mod-",
"derstanding.",
"IEEE Transactions on Knowledge and",
"Qinghao Guan, Mengfei Guo, Haiming Peng, Bang",
"prompt pattern catalog to enhance prompt engineer-",
"Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.",
"Guanghui Qin and Jason Eisner. 2021a. Learning how",
"Zekun Li, Baolin Peng, Pengcheng He, Michel Galley,",
"to ask: Querying lms with mixtures of soft prompts.",
"language-model-as-a-service. In",
"International Con-",
"Figure 2:",
"Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing",
"Guanghui Qin and Jason Eisner. 2021b.",
"Learning how",
"Table 6: Zero-shot domain transfer performance based",
"tuning. In",
"Findings of the Association for Computa-",
"Jianfeng Gao, and Xifeng Yan. 2023. Guiding large",
"2022b. Benchmarking generalization via in-context",
"Proceedings of the 2021 Conference of the North",
"Christopher Potts. 2013. Recursive deep models for",
"The Eleventh International Conference on Learn-",
"Yuanhang Zheng, Zhixing Tan, Peng Li, and Yang Liu.",
"treme parameter-efficient adapter tuning method for",
"guage model with self generated instructions.",
"Hartmann, and Qian Yang. 2023. Why johnny can’t",
"Chin-Yew Lin. 2004. Rouge: A package for automatic",
"marily emphasizes the importance of adapting",
"tensive experiments indicate that the proposed",
"class relationships for sentiment categorization with",
"LLM for inference based on LLaMA with original and",
"Figure 3:",
"tuning/SFT, C: Continous prompt, D: Discrete prompt.",
"↑",
"Optimized prompts. RS: Commonsense Reasoning.",
"Table 7:",
"arXiv:2303.12712",
"and Knowledge Management",
", pages 276–285.",
"edge Management",
", pages 245–255.",
"480.",
"Data Engineering",
"pages 741–754. IEEE.",
"els.",
"arXiv preprint arXiv:2210.02441",
"pages 17826–17834.",
"TAC",
". Citeseer.",
"Mining",
", pages 116–124.",
"preprint arXiv:1508.05326",
"pages 177–190. Springer.",
"systems",
", 33:1877–1901.",
"arXiv:1809.02922",
"branches out",
", pages 74–81.",
"arXiv preprint arXiv:2205.12548",
"proach.",
"arXiv preprint arXiv:1907.11692",
"ing.",
"arXiv preprint arXiv:1810.04805",
"(IWP2005)",
"uation",
", pages 92–96.",
"learners.",
"arXiv preprint arXiv:2012.15723",
"tacks.",
"arXiv preprint arXiv:2403.17155",
"paraphrasing",
", pages 1–9.",
"arXiv preprint arXiv:2303.13217",
"Giampiccolo, Bernardo Magnini, and Idan Szpektor.",
"Entailment",
", volume 7.",
"arXiv preprint arXiv:2104.08773",
"OpenAI. 2023.",
"Gpt-4 technical report",
"reprogramming.",
"arXiv preprint arXiv:2101.00121",
"arXiv preprint arXiv:2212.09611",
"Information Processing Systems",
", 35:27730–27744.",
"edge discovery and data mining",
", pages 168–177.",
"respect to rating scales.",
"arXiv preprint cs/0506075",
"tional Linguistics: EACL 2023",
", pages 1214–1220.",
"Human Factors in Computing Systems Extended Ab-",
"stracts",
", pages 1–8.",
"arXiv preprint arXiv:2203.07281",
"Computational Linguistics",
", 8:423–438.",
"preprint arXiv:2101.00190",
"beam search.",
"arXiv preprint arXiv:2305.03495",
"arXiv preprint arXiv:2302.11520",
"arXiv preprint arXiv:2104.06599",
"arXiv:1804.07461",
"tional Linguistics.",
"mesh-transformer-jax",
"tion.",
"arXiv preprint arXiv:2210.01241",
"preprint arXiv:2212.10560",
"arXiv:2110.08207",
"arXiv:2204.07705",
"arXiv preprint arXiv:2211.05100",
"ing with chatgpt.",
"arXiv preprint arXiv:2302.11382",
"arXiv:2001.07676",
"preprint arXiv:1704.05426",
"arXiv:1707.06347",
"arXiv:2304.05302",
"arXiv:2010.15980",
"pages 1–21.",
"1631–1642.",
"ing Representations",
", 28.",
"PMLR.",
"arXiv preprint arXiv:2305.03518",
"processing systems",
", 32.",
"recall.",
"arXiv preprint arXiv:2104.05240",
"Table 9:",
"Figure 4:",
"Figure 6:",
"Table 11: Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q:",
"↓",
"performance decline. D-",
"means the number of data reduction. D-",
"(%) means the percentage of data reduction.",
"MAPO-0 means using MAPO with temperature 0.",
"Table 13:",
"respectively.",
"Table 17: Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q:",
"QA, C: classification, G:generation. SFT means using SFT-optimized prompts without RL.",
"(epoch=50) results in a decline in performance.",
"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-",
"Test-time prompt editing via reinforcement learning.",
"fication.",
"Advances in neural information processing",
"Table 4: The performance with a frozen LLM for infer-",
"Neelakantan, Pranav Shyam, Girish Sastry, Amanda",
"language models via directional stimulus prompting.",
"naneh Hajishirzi. 2022a. Self-instruct: Aligning lan-",
"Carroll Wainwright, Pamela Mishkin, Chong Zhang,",
"parameter open-access multilingual language model.",
"stream tasks in NLP. Then we novelly propose",
"tasks. F: Finetuning, C: Continous prompt, D: Discrete",
"32nd ACM International Conference on Information",
"Table 8:",
"gence: Early experiments with gpt-4.",
"prompt engineers.",
"arXiv preprint arXiv:2211.01910",
"ceedings of the Sixteenth ACM International Confer-",
"Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind",
"cally constructing a corpus of sentential paraphrases.",
"rizing customer reviews. In",
"Proceedings of the tenth",
"R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo",
"pages 5203–5212, Online. Association for Computa-",
"method that optimizes the original prompts for",
"Making pre-trained language models better few-shot",
"Cai. 2022. Promptmaker: Prompt-based prototyping",
"of the 13th international workshop on semantic eval-",
"enhance their capabilities across various down-",
"Eric Wallace, and Sameer Singh. 2020. Autoprompt:",
"Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-",
"2023. Black-box prompt tuning with subspace learn-",
"research community. The existing research pri-",
"Kristina Toutanova. 2018. Bert: Pre-training of deep",
"Huang, and Xipeng Qiu. 2022. Black-box tuning for",
"Table 19:",
"language models understand chinese humor? In",
"Pro-",
"tion via natural language crowdsourcing instructions.",
"Jules White, Quchen Fu, Sam Hays, Michael Sandborn,",
"to ask: Querying LMs with mixtures of soft prompts",
"Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan",
"Glue: A multi-task benchmark and analysis platform",
"with SOTA prompt optimizing baselines in downstream",
"MAPO-Optimized prompts. CLS: Classification, M: MAPO. The (",
"(%) represent the increase degree of",
"International Conference on Information and Knowl-",
"building blocks for natural language policy optimiza-",
"semantic compositionality over a sentiment treebank.",
"rel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-",
"Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018.",
"Table 15: Zero-shot domain transfer performance based",
"Figure 1:",
"defined by its wording, but also binds to the na-",
"prompts should be adapted to different LLMs to",
"LLM, leading to significant improvements over",
"ence of MAPO with SOTA prompt optimizing baselines",
"in nine tasks from P3 benchmark using LLaMA. F: Fine-",
"MAPO-optimized prompts compared with original prompts in each dataset and task, respectively (The same below).",
"Bang Liu, and Yunwen Chen. 2023c. Can pre-trained",
"Yuyan Chen, Yanghua Xiao, and Bang Liu. 2022. Grow-",
"ternational Conference on Data Engineering (ICDE)",
"thing: A simple strategy for prompting language mod-",
"Liu, Zhixu Li, and Yanghua Xiao. 2024a. Talk funny!",
"medical dialogues with time-aware responses in large",
"and Christopher D Manning. 2015. A large annotated",
"corpus for learning natural language inference.",
"2005. The pascal recognising textual entailment chal-",
"Haibin Ling, Susmit Jha, and Chao Chen. 2024. Task-",
"Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and",
"William B Dolan. 2007. The third pascal recognizing",
"Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting",
"Molina, Aaron Donsbach, Michael Terry, and Carrie J",
"struction search for prompting large language models.",
"Optimizing continuous prompts for generation.",
"instructions on 1,600+ language tasks.",
"Castagné, Alexandra Sasha Luccioni, François Yvon,",
"Chuang, Christopher D Manning, Andrew Y Ng, and",
"Character-level convolutional networks for text classi-",
"Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-",
"compared with original prompts in each dataset and task,",
"The (",
"represent the absolute improvement scores",
"Value",
"Gradient Accumulation Steps",
"Weight Decay",
"0.1",
"Learning Rate for Actor Model",
"2e-5",
"Learning Rate for Critic Model",
"1e-5",
"Entropy Coefficient",
"0.005",
"Value Loss Coefficient",
"0.5",
"Mini Batch Size",
"32",
"Positive Lambda Coefficient",
"2.0",
"Negative Lambda Coefficient",
"1.8",
"GAMMA",
"0.99",
"Adam Optimizer Epsilon",
"GAE Lambda",
"0.95",
"Max Gradient Norm",
"PPO Epochs",
"20",
"Clip Parameter",
"0.2",
"The amount of the warm-up dataset on various",
"Performance is evaluated for BLOOM, GPT-J, and LLaMA using original, SFT-optimized, and MAPO-optimized",
"Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q: QA, C:",
") denotes the absolute performance increase achieved using MAPO-optimized prompts versus a frozen LLM,",
"while (",
"(%)) highlights the relative performance boost. Symbols",
"(%) represent the average absolute and relative",
"enhancements across all three LLMs, respectively. These enhancements pertain to specific downstream tasks, with “CLS”",
"among different LLMs in the QA task. (a) retaining frequent words in the instruction, (b) removing frequent words in the",
"The separate effect of PPO and RRMF, which demonstrate the important roles played by both PPO and RRMF in",
"The distribution of three most frequent words, which extracted from the original prompt, in the optimized prompt",
"Thus, there’s no training data with prompts as inputs and expected responses as outputs. “CLS” represents classification, and",
"The performance of different LLMs on task-specific prompts for three tasks: question-answering (a), classification",
"they are given the same task-specific prompts (a).",
"(b), and generation (c). The results reveal significant variations across different LLMs’ performance.",
"Framework of the proposed MAPO, including warm-up dataset establishment and prompt optimizer construction.",
"downstream tasks.",
"https://chat.openai.com/",
"https://github.com/microsoft/DeepSpeed",
"“GEN” stands for generation tasks.",
"Original prompts and MAPO-optimized prompts for three LLMs in various downstream tasks.",
"P3.",
"Hyperparameters used for MAPO in all the tasks.",
"The performance of the reward model in three LLMs during the training process of MAPO.",
"classification, G:generation. We only keep the first four letters of each dataset’s name in the figure.",
"enhancing the performance of MAPO.",
"The separate effects of PPO and RRMF during the process of RL in constructing MAPO.",
"Natural Instructions of various downstream tasks.",
"temperature settings of [0,0.5], 0, 0.2, 0.5, and 0.8, respectively.",
"instruction.",
"the instruction.",
"with the corresponding instruction as shown in Table",
"13",
". We omit instructions in the following Table due to space limits.",
"signifying classification and “GEN” indicating generation tasks. M, M-0.2, M-0.5, and M-0.8 correspond to using MAPO with",
"among different LLMs in the classification task. (a) retaining frequent words in the instruction, (b) removing frequent words in",
"term “frozen LLM for inference” means the model hasn’t been trained directly on downstream tasks but only makes inferences.",
"One sample prompt and the corresponding three candidate prompts generated by GPT-3.5 for each selected dataset in",
"Original prompts and MAPO-optimized prompts for different LLMs in more downstream tasks. Each prompt is start",
"among different LLMs in the generation task. (a) retaining frequent words in the instruction, (b) removing frequent words in the",
"Variance on answers from different LLMs (b) when",
"prompts with a frozen LLM during inference. The symbols",
"(%) and",
"(%) under SFT-optimized denote the relative increase from",
"original prompts, while those under MAPO-optimized indicate improvement over SFT-optimized results. It’s emphasized that the"
],
"tables": [
"|QA<br>Prompt: What other type of loop is<br>there other than an open loop?<br>Ground Truth: closed loop<br>Generation<br>Prompt: What are the topics in the<br>sentence: bees collect nectar from the<br>flowers<br>Ground Truth: flower, bee, nectar<br>Classification<br>Prompt: as it turns out , you can go<br>home again . The sentiment<br>expressed for the movie is<br>Ground Truth: Positive|GPT-J : The target is the person who is going to be doing the<br>work.|Col3|\n|---|---|---|\n|**QA**<br>**Generation**<br>**Classification**<br>**Prompt:**What other type of loop is<br>there other than an open**loop**?<br>**Ground Truth:**closed loop<br>**Prompt:**What are the topics in the<br>sentence: bees collect**nectar** from the<br>**flowers**<br>**Ground Truth:**flower, bee, nectar<br>**Prompt:**as it turns out , you can go<br>home again . The sentiment<br>expressed for the movie is<br>**Ground Truth:**Positive|**BLOOM:**100% of the time, the**loop** is open.|**BLOOM:**100% of the time, the**loop** is open.|\n|**QA**<br>**Generation**<br>**Classification**<br>**Prompt:**What other type of loop is<br>there other than an open**loop**?<br>**Ground Truth:**closed loop<br>**Prompt:**What are the topics in the<br>sentence: bees collect**nectar** from the<br>**flowers**<br>**Ground Truth:**flower, bee, nectar<br>**Prompt:**as it turns out , you can go<br>home again . The sentiment<br>expressed for the movie is<br>**Ground Truth:**Positive|**LLaMA:**Closed**loop**</s>|**LLaMA:**Closed**loop**</s>|\n|**QA**<br>**Generation**<br>**Classification**<br>**Prompt:**What other type of loop is<br>there other than an open**loop**?<br>**Ground Truth:**closed loop<br>**Prompt:**What are the topics in the<br>sentence: bees collect**nectar** from the<br>**flowers**<br>**Ground Truth:**flower, bee, nectar<br>**Prompt:**as it turns out , you can go<br>home again . The sentiment<br>expressed for the movie is<br>**Ground Truth:**Positive|**GPT-J:**bees \\n What are the topics in the sentence:**bees** collect<br>**nectar** from the**flowers**||\n|**QA**<br>**Generation**<br>**Classification**<br>**Prompt:**What other type of loop is<br>there other than an open**loop**?<br>**Ground Truth:**closed loop<br>**Prompt:**What are the topics in the<br>sentence: bees collect**nectar** from the<br>**flowers**<br>**Ground Truth:**flower, bee, nectar<br>**Prompt:**as it turns out , you can go<br>home again . The sentiment<br>expressed for the movie is<br>**Ground Truth:**Positive|**BLOOM: bee**, collect,**flower**</s>|**BLOOM: bee**, collect,**flower**</s>|\n|**QA**<br>**Generation**<br>**Classification**<br>**Prompt:**What other type of loop is<br>there other than an open**loop**?<br>**Ground Truth:**closed loop<br>**Prompt:**What are the topics in the<br>sentence: bees collect**nectar** from the<br>**flowers**<br>**Ground Truth:**flower, bee, nectar<br>**Prompt:**as it turns out , you can go<br>home again . The sentiment<br>expressed for the movie is<br>**Ground Truth:**Positive|**LLaMA: bees** collect**nectar** from the**flowers** \\n What are the<br>topics in the sentence: the bees|**LLaMA: bees** collect**nectar** from the**flowers** \\n What are the<br>topics in the sentence: the bees|\n|**QA**<br>**Generation**<br>**Classification**<br>**Prompt:**What other type of loop is<br>there other than an open**loop**?<br>**Ground Truth:**closed loop<br>**Prompt:**What are the topics in the<br>sentence: bees collect**nectar** from the<br>**flowers**<br>**Ground Truth:**flower, bee, nectar<br>**Prompt:**as it turns out , you can go<br>home again . The sentiment<br>expressed for the movie is<br>**Ground Truth:**Positive|**GPT-J:**Yes. \\n The movie is a remake of the 1979 movie of the<br>same name. The original movie was a||\n|**QA**<br>**Generation**<br>**Classification**<br>**Prompt:**What other type of loop is<br>there other than an open**loop**?<br>**Ground Truth:**closed loop<br>**Prompt:**What are the topics in the<br>sentence: bees collect**nectar** from the<br>**flowers**<br>**Ground Truth:**flower, bee, nectar<br>**Prompt:**as it turns out , you can go<br>home again . The sentiment<br>expressed for the movie is<br>**Ground Truth:**Positive|**BLOOM:**No</s>|**BLOOM:**No</s>|\n|**Ground Truth:**Positive|**LLaMA:**Yes. \\n The movie is about a man who is a widower and<br>has a son. He is a very good father|**LLaMA:**Yes. \\n The movie is about a man who is a widower and<br>has a son. He is a very good father|",
"|1.00<br>1.0 1.0<br>0.75<br>Metric<br>0.50 0.5 0.5 BLO<br>LLa<br>0.25 GPT<br>0.00 0.0 0.0<br>Generation Classification QA|Col2|BLO|\n|---|---|---|\n|Generation<br>0.00<br>0.25<br>0.50<br>0.75<br>1.00<br>Metric<br>Classification<br>0.0<br>0.5<br>1.0<br>QA<br>0.0<br>0.5<br>1.0<br>BLO<br>LLa<br>GPT||LLa<br>GPT|",
"|Task|Prompts|\n|---|---|\n|QA|**Original**: How do people in Mexico refer to the largest Presbyterian church?<br>**BLOOM**: What is the way in which the biggest Presbyterian church is referred to by individuals in Mexico?<br>**GPT-J**: What is the term used by Mexicans to denote the biggest Presbyterian church?<br>**LLaMA**: What is the commonly used term for the biggest Presbyterian church in Mexico?|\n|Gen|**Original**: What are the topics in the sentence: The furniture in the living room is decorated with flowers.<br>**BLOOM**: What topics does the sentence “The living room furniture is adorned with flowers” cover?<br>**GPT-J**: What topics are covered in the sentence “The living room furniture is adorned with flowers”?<br>**LLaMA**: Which subjects are present in the statement: Flowers adorn the furniture in the living room.|\n|Class|**Original**: Reincarnated Napster Goes Public Roxio bought the Napster name in a bankruptcy auction two years ago. Now, the company dumps its CD-burning software business<br>to concentrate on selling and delivering music over the web under the Napster name.What label best describes this news article?<br>**BLOOM**: Roxio, the buyer of Napster’s name in a bankrupt auction two years ago, has taken the reincarnated Napster public. The company has made the decision to abandon<br>their CD-burning software business and focus solely on the distribution and sale of music online via the Napster name. What genre of news article would this story fall under?<br>**GPT-J**: Roxio, which acquired the Napster name through a bankruptcy auction two years ago, has taken Napster public again after rebranding it as a music delivery service. The<br>company has divested its CD-burning software business and now focuses solely on selling music online under the Napster name. What category would this news article fall under?<br>**LLaMA**: The Napster name, which was purchased by Roxio in a bankruptcy auction two years ago, has now been resurrected with a public launch. Roxio has shifted its focus<br>solely to the sale and distribution of music under the Napster name, leaving its CD-burning software business behind. What category would you assign to this news article?|",
"|Task|Prompts|\n|---|---|\n|AdverQA|**Original**: Question: “Which happened earlier, the Chinese entered the war or President Truman dispatched the United States Seventh Fleet to the Taiwan Strait?”. Context:<br>“On 27 June 1950, ...”. Answer:<br>**Candidate 1**: Question: “Did President Truman dispatch the United States Seventh Fleet to the Taiwan Strait before or after the Chinese entered the war?” Context:“On 27<br>June 1950, ...”. Answer:<br>**Candidate 2**: Question: “Did the Chinese enter the war before President Truman dispatched the United States Seventh Fleet to the Taiwan Strait, or vice versa?” Context:“On 27<br>June 1950, ...”. Answer:<br>**Candidate 3**: Question: “Did the Chinese enter the war first or did President Truman send the United States Seventh Fleet to the Taiwan Strait earlier?”. Context:“On 27 June<br>1950, ...”. Answer:|\n|OpenQA|**Original**: What’s above the muscles and needs direct sunlight Which is the correct answer? Options: ...<br>**Candidate 1**: What lies beyond the muscles and requires direct exposure to sunlight? Which is the correct answer? Options: ...<br>**Candidate 2**:What is located above the muscles and requires direct sunlight? Which is the correct answer? -Options: ...<br>**Candidate 3**: Which body part requires direct sunlight and is located higher than the muscles? Which is the correct answer? Options: ...|\n|CloseQA|**Original**: Q: What kind of relationship between glucagon and insulin is vital to managing fuel storage and consumption by body cells? A:<br>**Candidate 1**:Q: What is the essential connection between glucagon and insulin for regulating fuel storage and utilization in body cells? A:<br>**Candidate 2**:Q: In managing the storage and consumption of fuel by body cells, what is the crucial interrelation between insulin and glucagon? A:<br>**Candidate 3**: Q: What is the crucial connection between glucagon and insulin in regulating the storage and utilization of fuel by the cells in the body? A:|\n|News|**Original**: Reincarnated Napster Goes Public Roxio bought the Napster name in a bankruptcy auction two years ago. Now, the company dumps its CD-burning software<br>business to concentrate on selling and delivering music over the web under the Napster name. What label best describes this news article?<br>**Candidate 1**: Roxio, the buyer of Napster’s name in a bankrupt auction two years ago, has taken the reincarnated Napster public. The company has made the decision to<br>abandon their CD-burning software business and focus solely on the distribution and sale of music online via the Napster name. What genre of news article would this story<br>fall under?<br>**Candidate 2**: Roxio, which acquired the Napster name through a bankruptcy auction two years ago, has taken Napster public again after rebranding it as a music delivery<br>service. The company has divested its CD-burning software business and now focuses solely on selling music online under the Napster name. What category would this news<br>article fall under?<br>**Candidate 3**:The Napster name, which was purchased by Roxio in a bankruptcy auction two years ago, has now been resurrected with a public launch. Roxio has shifted<br>its focus solely to the sale and distribution of music under the Napster name, leaving its CD-burning software business behind. What category would you assign to this news<br>article?|\n|Movie|**Original**: writer/director joe carnahan’s grimy crime drama is a manual of precinct cliches , but it moves fast enough to cover its clunky dialogue and lapses in logic . The<br>sentiment expressed for the movie is<br>**Candidate 1**: The gritty crime drama by writer and director Joe Carnahan may rely heavily on familiar tropes and cliches of the genre, but its quick pace manages to distract<br>from any awkward dialogue and illogical moments. The sentiment expressed for the movie is<br>**Candidate 2**: Joe Carnahan´s gritty crime drama relies heavily on standard police procedures, yet its rapid pace compensates for any cumbersome dialogues and unreasonable<br>plot holes. The sentiment expressed for the movie is<br>**Candidate 3**:Although writer/director Joe Carnahan´s gritty crime drama contains numerous stereotypes within the precinct environment, its swift pace effectively masks its<br>awkward dialogue and occasional lapses in logic. The sentiment expressed for the movie is|\n|QASC|**Original**:If I tell you that Hydrogen bonds cause a tremendous force when a substance freezes, and ask you the question “hydrogen bonds cause a tremendous force when a<br>substance does what”, is the correct answer “strong”?<br>**Candidate 1**: If I were to inform you that when a substance freezes, Hydrogen bonds create a significant force and ask, “What term describes the force generated by Hydrogen<br>bonds when a<br>substance freezes?”, would the appropriate response be “Powerful”?<br>**Candidate 2**: Suppose I inform you that the process of substance freezing is deeply influenced by Hydrogen bonds that generate an enormous force. Now, if I inquire, “What<br>occurs when the substance undergoes this process?”, would it be accurate to say that the force generated is “powerful”?<br>**Candidate 3**: Suppose I inform you that when a substance freezes, Hydrogen bonds result in a remarkable force, and inquire, “When a substance undergoes what, do hydrogen<br>bonds cause a remarkable force?” Would it be accurate to respond with “robust”?|\n|Topics|**Original**: What are the topics in the sentence: A bathroom with the toilet missing and the room fairly torn up.<br>**Candidate 1**: What are the subjects of the sentence: A torn-up room without a toilet.<br>**Candidate 2**: Which subjects are covered in the phrase “A bathroom that lacks a toilet and has a considerably damaged room”?<br>**Candidate 3**:What are the subjects mentioned in the statement: A torn up room that lacks a toilet in the bathroom?|\n|Summary|**Original**: Sum up the following dialogue: Gordon: Did you see my car, bro? Gordon: <file_photo> Gordon: It’s my first car ever! And I love it! :) Leo: Grats, bro! Leo: It<br>looks awesome, I have to see it with my own eyes! Gordon: Are you home? Leo: Yeah Gordon: Look out of the kitchen window :) Leo: No shit :D Leo: Wait, I’m coming!<br>Gordon: Waiting :D<br>**Candidate 1**: Sum up the following dialogue: Gordon asked, “Bro, have you seen my car?” and sent a file photo. He expressed his excitement saying it´s his first ever car and<br>he loves it. Leo congratulated him saying it looks awesome and expressed his wish to see it in person. Gordon asked if he was home and told him to look out of the kitchen<br>window. Leo was surprised and replied, “No shit :D” and said he was coming. Gordon eagerly waited for him.<br>**Candidate 2**: Sum up the following dialogue: Gordon inquires, “Hey bro, have you laid eyes on my car?” Gordon shares a photograph of his first vehicle and expresses his<br>adoration for it with a smiley face. Leo congratulates him and expresses interest in seeing it in person. Gordon asks if Leo is home and instructs him to look out of the kitchen<br>window. Leo is surprised and excited, responding with laughter and promising to come see it. Gordon waits patiently.<br>**Candidate 3**: Sum up the following dialogue: Gordon asked his brother if he had seen his car and sent a photo of it. He expressed his love for it as it was his first car ever. Leo<br>congratulated him and expressed his desire to see the car in person. Gordon asked if he was at home and told him to look out of the kitchen window. Leo was surprised and excited<br>and said he would be coming soon. Gordon waited for him to arrive.|\n|Explan|**Original**: Question: What does a Christian do when they get what they needed? Options:... The answer is “thank god” because<br>**Candidate 1**: Question: How should a Christian proceed after they have received what they required? Options: ... The answer is “thank god” because<br>**Candidate 2**: Question: When a Christian receives what they needed, what actions do they take? Options:... The answer is “thank god” because<br>**Candidate 3**: Question: What should a Christian do upon receiving what they required? Options: ... The answer is “thank god” because|",
"|Task|Natural Instructions|\n|---|---|\n|SST-2|In this task, you are given sentences from movie reviews. The task is to classify the sentiment of the sentence. Your answer must be<br>in the form of the letters “positive”, and “negative” respectively.|\n|Yelp|In this task, you are given sentences from Yelp reviews. The task is to classify the sentiment of the sentence. Your answer must be<br>in the form of the letters “positive”, or “negative” respectively.|\n|MR|In this task, you are given sentences from movie reviews. The task is to classify the sentiment of the sentence. Your answer must be<br>in the form of the letters “positive”, or “negative” respectively.|\n|CR|In this task, you are given sentences from customer reviews. The task is to classify the sentiment of the sentence. Your answer must be<br>in the form of the letters “positive”, or “negative” respectively.’|\n|RTE|In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree<br>(entailment)/disagree (not entailment) with each other. Your answer must be in the form of the letters Yes, and No respectively.|\n|QNLI|You are given two sentences(Sentence1 and Sentence2). The task is to determine whether Sentence2 contains the answer to Sentence1.<br>Your answer must be in the form of the letters Yes, and No respectively.|\n|SNLI|In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree<br>(entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the<br>letters “Yes”, “No”, and “Maybe” respectively.|\n|MNLI|In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree<br>(entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the<br>letters Yes, No, and Maybe respectively.|\n|MRPC|In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree<br>(entailment)/disagree (not entailment) with each other. Your answer must be in the form of the letters Yes, and No respectively.|",
"|city<br>topics<br>hom e<br>placebathroom<br>sentence<br>city view<br>atch<br>m<br>hom e boat<br>hite<br>bathroom w<br>lake<br>room<br>place<br>player<br>view grass<br>Word<br>close<br>atch chair<br>m<br>ersplaying<br>boat<br>beachflow<br>hite<br>w sky<br>near<br>lake<br>bridge<br>room an<br>m<br>finalbuilding<br>player<br>Original Orig ooden<br>BLOOM BLO<br>GPT-J GPT- grass floorw<br>LLaMA LLaM<br>0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.000 0.002 0.004 0.006 0.008 0.010<br>(a) (b)|Orig<br>BLO<br>GPT-<br>LLaM|Col3|\n|---|---|---|\n|0.00<br>0.02<br>0.04<br>0.06<br>0.08<br>0.10<br>0.12<br>(a)<br>topics<br>sentence<br>city<br>home<br>bathroom<br>place<br>view<br>match<br>boat<br>white<br>lake<br>room<br>player<br>grass<br>Word<br>Original<br>BLOOM<br>GPT~~-~~J<br>LLaMA<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>(b)<br>city<br>home<br>bathroom<br>place<br>view<br>match<br>boat<br>white<br>lake<br>room<br>player<br>grass<br>close<br>chair<br>playing<br>flowers<br>beach<br>sky<br>near<br>bridge<br>man<br>building<br>final<br>wooden<br>floor<br>Orig<br>BLO<br>GPT~~-~~<br>LLaM|Orig<br>BLO<br>GPT~~-~~<br>LLaM|Orig<br>BLO<br>GPT~~-~~<br>LLaM|\n|0.00<br>0.02<br>0.04<br>0.06<br>0.08<br>0.10<br>0.12<br>(a)<br>topics<br>sentence<br>city<br>home<br>bathroom<br>place<br>view<br>match<br>boat<br>white<br>lake<br>room<br>player<br>grass<br>Word<br>Original<br>BLOOM<br>GPT~~-~~J<br>LLaMA<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>(b)<br>city<br>home<br>bathroom<br>place<br>view<br>match<br>boat<br>white<br>lake<br>room<br>player<br>grass<br>close<br>chair<br>playing<br>flowers<br>beach<br>sky<br>near<br>bridge<br>man<br>building<br>final<br>wooden<br>floor<br>Orig<br>BLO<br>GPT~~-~~<br>LLaM|||",
"|university university<br>year year<br>largest largest<br>usic usic<br>m m<br>new new<br>national national<br>know know<br>uh uh<br>Word<br>increase increase<br>ent ent<br>executivegovernm executivegovernm<br>edical edical<br>m m<br>warsaw warsaw<br>school school<br>poland poland<br>Original Origi<br>BLOOM BLO<br>GPT-J GPT- ent ent<br>LLaMA LLaM parliam parliam<br>0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.000 0.002 0.004 0.006 0.008 0.010<br>(a) (b)|Col2|\n|---|---|\n|0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br>(a)<br>university<br>year<br>largest<br>music<br>new<br>national<br>know<br>uh<br>increase<br>government<br>executive<br>medical<br>warsaw<br>school<br>poland<br>parliament<br>Word<br>Original<br>BLOOM<br>GPT~~-~~J<br>LLaMA<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br><br>(b)<br>university<br>year<br>largest<br>music<br>new<br>national<br>know<br>uh<br>increase<br>government<br>executive<br>medical<br>warsaw<br>school<br>poland<br>parliament<br>Origi<br>BLO<br>GPT~~-~~<br>LLaM||\n|0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br>(a)<br>university<br>year<br>largest<br>music<br>new<br>national<br>know<br>uh<br>increase<br>government<br>executive<br>medical<br>warsaw<br>school<br>poland<br>parliament<br>Word<br>Original<br>BLOOM<br>GPT~~-~~J<br>LLaMA<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br><br>(b)<br>university<br>year<br>largest<br>music<br>new<br>national<br>know<br>uh<br>increase<br>government<br>executive<br>medical<br>warsaw<br>school<br>poland<br>parliament<br>Origi<br>BLO<br>GPT~~-~~<br>LLaM||\n|0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br>(a)<br>university<br>year<br>largest<br>music<br>new<br>national<br>know<br>uh<br>increase<br>government<br>executive<br>medical<br>warsaw<br>school<br>poland<br>parliament<br>Word<br>Original<br>BLOOM<br>GPT~~-~~J<br>LLaMA<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br><br>(b)<br>university<br>year<br>largest<br>music<br>new<br>national<br>know<br>uh<br>increase<br>government<br>executive<br>medical<br>warsaw<br>school<br>poland<br>parliament<br>Origi<br>BLO<br>GPT~~-~~<br>LLaM||",
"|Task|Prompts|\n|---|---|\n|SST-2|**Original**: The sentence is “khouri manages , with terrific flair, to keep the extremes of screwball farce and blood-curdling family intensity on<br>one continuum. ”.<br>**BLOOM**: The sentence is “khouri manages , with terrific flair, to keep the extremes of screwball farce and blood-curdling family intensity on<br>one continuum. ”.<br>**GPT-J**: The sentence is “With tremendous artistry, Khouri expertly maintains a fluid continuum between the uproarious comedy of screwball<br>farce and the blood-curdling intensity of family dynamics.”.<br>**LLaMA**: The sentence is “With remarkable skill, Khouri successfully maintains a seamless connection between the wild hilarity of screwball<br>farce and the spine-chilling intensity of family dynamics.”.|\n|Yelp|**Original**: The sentence is “The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, the<br>waiters and waitresses are often very apologetic for the long waits and it’s pretty obvious that some of them avoid the tables after taking the initial<br>order to avoid hearing complaints.”.<br>**BLOOM**: The sentence is “The quality of the food is commendable. However, the service provided is inconsistent. The primary concern appears<br>to be with the kitchen, as the waitstaff frequently apologize for lengthy waits, and it’s apparent that some of them intentionally avoid tables after<br>taking initial orders to prevent complaints.”.<br>**GPT-J**: The sentence is “The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, the waiters<br>and waitresses are often very apologetic for the long waits and it’s pretty obvious that some of them avoid the tables after taking the initial order to<br>avoid hearing complaints.”.<br>**LLaMA**: The sentence is “The quality of the food is commendable. However, the service provided is inconsistent. The primary concern appears<br>to be with the kitchen, as the waitstaff frequently apologize for lengthy waits, and it’s apparent that some of them intentionally avoid tables after<br>taking initial orders to prevent complaints.”.|\n|MR|**Original**: The sentence is “a respectable venture on its own terms , lacking the broader vision that has seen certain trek films . . . cross over to<br>a more mainstream audience .”.<br>**BLOOM**: The sentence is “An admirable endeavor in its own right, although it lacks the wider perspective that propelled certain Star Trek films<br>to cross over into a more mainstream audience.”.<br>**GPT-J**: The sentence is “a respectable venture on its own terms , lacking the broader vision that has seen certain trek films . . . cross over to a<br>more mainstream audience .”.<br>**LLaMA**: The sentence is “A praiseworthy pursuit on its own grounds, though it falls short of the broader vision that allowed certain Star Trek<br>films to captivate a more mainstream audience.”.|\n|CR|**Original**: The sentence is “the fact that the 0 key is the space key for text input is a bit confusing , as many phones use the key instead”.<br>**BLOOM**: The sentence is “It can be a little confusing that the space key for text input is the 0 key, considering that many phones utilize a different<br>key for that purpose.”.<br>**GPT-J**: The sentence is “the fact that the 0 key is the space key for text input is a bit confusing , as many phones use the key instead”.<br>**LLaMA**: The sentence is “It might cause confusion that the space key for text input is assigned to the 0 key, whereas many phones utilize a different<br>key for the same purpose.”.|\n|RTE|**Original**: The sentence 1 is “More than 150 dolphins, marine turtles and beaked whales have been washed up dead on beaches in Africa.”. The<br>sentence 2 is “Dead dolphins, turtles and whales have been found on African beaches.”.<br>**BLOOM**: The sentence 1 is “African beaches have become a somber site with the washing up of over 150 deceased dolphins, marine turtles, and<br>beaked whales.”. The sentence 2 is “African beaches have become a distressing sight as dead dolphins, turtles, and whales have been uncovered.”.<br>**GPT-J**: The sentence 1 is “The number of deceased dolphins, marine turtles, and beaked whales found on African beaches exceeds 150.”. The<br>sentence 2 is “There have been reports of deceased dolphins, turtles, and whales on African beaches.”.<br>**LLaMA**: The sentence 1 is “More than 150 dolphins, marine turtles and beaked whales have been washed up dead on beaches in Africa.”. The<br>sentence 2 is “Dead dolphins, turtles and whales have been found on African beaches.”.|\n|QNLI|**Original**: The sentence 1 is “What came into force after the new constitution was herald?”. The sentence 2 is “As of that day, the new constitution<br>heralding the Second Republic came into force.”.<br>**BLOOM**: The sentence 1 is “What started being in effect as a result of the heralding of the new constitution?”. The sentence 2 is “The new<br>constitution heralding the Second Republic started being in effect as of that day.”.<br>**GPT-J**: The sentence 1 is “What commenced after the new constitution was heralded?”. The sentence 2 is “The new constitution heralding the<br>Second Republic came into force as of that day.”.<br>**LLaMA**: The sentence 1 is “What became valid subsequent to the heralding of the new constitution?”. The sentence 2 is “ The new constitution<br>heralding the Second Republic started being in force on that day.”.|\n|SNLI|**Original**: The sentence 1 is “A baby at the end of a slip and slide at a party”. The sentence 2 is “The baby is wet”.<br>**BLOOM**: The sentence 1 is “At a party, a baby reaches the conclusion of their slip and slide adventure.”. The sentence 2 is “The little one is<br>dripping wet.”.<br>**GPT-J**: The sentence 1 is “A baby sliding to the end of a slip and slide at a celebration.”. The sentence 2 is “The baby’s clothes are moistened.”.<br>**LLaMA**: The sentence 1 is “At the end of a festive occasion, a baby arrives at the final stretch of a slip and slide.”. The sentence 2 is “The baby<br>is entirely drenched.”.|\n|MNLI|**Original**: The sentence 1 is “Conversely, an increase in government saving adds to the supply of resources available for investment and may<br>put downward pressure on interest rates.”. The sentence 2 is “Interest rates should increase to increase saving.”.<br>**BLOOM**: The sentence 1 is “Conversely, an increase in government saving adds to the supply of resources available for investment and may<br>put downward pressure on interest rates.”. The sentence 2 is “Interest rates should increase to increase saving.”.<br>**GPT-J**: The sentence 1 is “Conversely, when the government saves more, it adds to the pool of resources for investment and may exert downward<br>pressure on interest rates.”. The sentence 2 is “The augmentation of interest rates is crucial for increasing saving.”.<br>**LLaMA**: The sentence 1 is “On the flip side, an increase in government saving enhances the resources accessible for investment and may<br>exert downward pressure on interest rates.”. The sentence 2 is “The escalation of interest rates is essential for boosting savings.”.|\n|MRPC|**Original**: The sentence 1 is “Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.”. The sentence 2 is<br>“With the scandal hanging over Stewart’s company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier.”.<br>**BLOOM**: The sentence 1 is “The first quarter of the year saw a 15 percent drop in revenue compared to the corresponding period a year earlier.”.<br>The sentence 2 is “The scandal’s impact on Stewart’s company led to a 15 percent drop in revenue during the first quarter, as compared to the same<br>period in the prior year.”<br>**GPT-J**: The sentence 1 is “There was a 15 percent decline in revenue during the first quarter of the year compared to the same period a year ago.”.<br>The sentence 2 is “Due to the scandal looming over Stewart’s company, revenue in the first quarter of the year witnessed a 15 percent decline<br>compared to the same period last year.”.<br>**LLaMA**: The sentence 1 is “There was a 15 percent reduction in revenue during the first quarter of the year compared to the same period in the<br>prior year.”. The sentence 2 is “With the scandal overshadowing Stewart’s company, revenue for the first quarter of the year dipped by 15 percent<br>compared to the corresponding period in the previous year.”.|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2407.04118v1.pdf"
}