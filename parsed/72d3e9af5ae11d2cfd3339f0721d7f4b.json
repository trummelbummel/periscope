{
"text": "Localized Zeroth-Order Prompt Optimization\n\n\n\n\n                         Wenyang Hu12, Yao Shu3, Zongmin Yu1, Zhaoxuan Wu2, Xiaoqiang Lin1,\n                                Zhongxiang Dai4, See-Kiong Ng12, & Bryan Kian Hsiang Low1\n                                  1Department of Computer Science, National University of Singapore\n                                               2Institute of Data Science, National University of Singapore\n                                      3Guangdong Lab of AI and Digital Economy (SZ)\n                                         4Laboratory for Information and Decision Systems, MIT\n                                   wenyang@comp.nus.edu.sg, shuyao@gml.ac.cn,\n                             {zongminy, wu.zhaoxuan, xiaoqiang.lin}@comp.nus.edu.sg,2024                       daizx@mit.edu, seekiong@nus.edu.sg, lowkh@comp.nus.edu.sg\nMar                                                 Abstract\n5\n                         The efficacy of large language models (LLMs) in understanding and generating\n                                  natural language has aroused a wide interest in developing prompt-based methods\n                                  to harness the power of black-box LLMs. Existing methodologies usually prior-\n                                      itize a global optimization for finding the global optimum, which however will\n                            perform poorly in certain tasks. This thus motivates us to re-think the necessity[cs.AI]                         of finding a global optimum in prompt optimization. To answer this, we conduct\n                              a thorough empirical study on prompt optimization and draw two major insights.\n                               Contrasting with the rarity of global optimum, local optima are usually prevalent\n                            and well-performed, which can be more worthwhile for efficient prompt optimiza-\n                                  tion (Insight I). The choice of the input domain, covering both the generation\n                           and the representation of prompts, affects the identification of well-performing\n                                   local optima (Insight II). Inspired by these insights, we propose a novel algorithm,\n                           namely localized zeroth-order prompt optimization (ZOPO), which incorporates a\n                              Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order\n                               optimization for an efficient search of well-performing local optima in prompt\n                                 optimization. Remarkably, ZOPO outperforms existing baselines in terms of both\n                                the optimization performance and the query efficiency, which we demonstrate\n                              through extensive experiments.\n\n\n                1  IntroductionarXiv:2403.02993v1\n                     Large language models (LLMs) have demonstrated remarkable capabilities for understanding and\n                       generating natural languages (Ouyang et al., 2022a; Touvron et al., 2023). Thanks to the instruction-\n                       following abilities of LLMs (Ouyang et al., 2022b), prompting—adding crafted, discrete prompts, or\n                    namely natural language text, to the input emerges as an effective and lightweight approach to direct\n                LLMs to generate specific, desired response (Mishra et al., 2021; Liu et al., 2023). Such an approach\n                              is of particular interest when users interact with state-of-the-art LLMs like ChatGPT (OpenAI, 2024a)\n                     and GPT-4 (OpenAI, 2023), which can only be accessed through black-box APIs (i.e., the interface of\n                      black-box LLMs only accepts discrete texts as input for querying). So, prompt optimization becomes\n                      a critical effort in pursuing the optimal performance of black-box LLMs on downstream tasks.\n\n                    Although human knowledge may subjectively guide prompt designs (Reynolds & McDonell, 2021;\n                     Mishra et al., 2021), this process is commonly time-intensive and its results are not always desirable\n                         in practice. To mitigate such human efforts and achieve better performance in optimizing crafted\n                      prompts, random sampling (Zhou et al., 2023), Bayesian optimization (Chen et al., 2023; Lin et al.,\n                      2023), and evolutionary algorithms (Guo et al., 2024) have been proposed to generate and select\n\n\n                            Preprint. Under review.\n\nAPE\n                                                                                               InstructZero\n                 0.75                                        INSTINCT\n                                                                             EvoPrompt\n                                                            ZOPO (ours)\n                 0.50                         ½(¿)\n\n                 0.25\n\n\n                    0        10        20\n                                  ¿\n\nFigure 1: The performance profile for different methods on instruction induction tasks, where τ\nindicates the distance from optimality, and ρ(τ) is the frequency for the method within τ distance to\noptimality.\n\n\n\n\nwell-performing prompts automatically. However, most of these existing strategies prioritize global\noptimization, dedicating substantial portions of the query budget to explore the entire search space\nfor the global optima and consequently making it query-inefficient in practice. Meanwhile, these\nstrategies typically implement their prompt optimization across various input domains, resulting\nin diverse performance outcomes in practice. These results consequently inspire us to re-think the\nquestions about the necessity of finding a global optimum and the essence of the input domain for\nefficient and effective prompt optimization.\n\nTo answer these questions, we provide a thorough empirical study on prompt optimization. Firstly,\nwe visualize the performance for a number of randomly sampled prompt candidates on various tasks\nto show that in contrast to the scarcity of global optima, local optima are commonly prevalent and per-\nform well, making them more valuable for query-efficient prompt optimization (Insight I in Sec. 3.1).\nSecondly, we visualize the estimated accuracy distributions for a number of prompt candidates as well\nas the corresponding function surfaces using various embeddings as their representation. The results\ndemonstrate that the selection of the input domain, including both the generation and representation\nof prompt candidates, will influence the identification of high-performing prompts, especially those\nlocal optimal ones (Insight II in Sec. 3.2). These insights consequently highlight the importance of\nlocal optima and input domain for efficient and effective prompt optimization.\n\nInspired by these insights, we novelly propose the Localized Zeroth-Order Prompt Optimization\n(ZOPO) algorithm for a considerably improved prompt optimization as evidenced by Fig. 1. Moti-\nvated by Insight II, we first propose a general domain transformation that utilizes LLMs for prompt\ngeneration and embedding models to transform these generated prompts into their corresponding\nhidden representations, which thereby enjoys not only the remarkable generation ability from any type\nof LLMs (white/black-box) (Zhou et al., 2023; Guo et al., 2024) but also the impressive representation\nability from many NLP embedding models (Chen et al., 2023; Lin et al., 2023) for our prompt opti-\nmization (Sec. 4.1). Inspired by Insight I, we then leverage a cutting-edge zeroth-order optimization\n(ZOO) method enhanced by a derived Gaussian process for efficient gradient estimation (Shu et al.,\n2023a) to underpin our localized prompt optimization, which goes one step further by incorporating\nthe Neural Tangent Kernel (NTK) (Jacot et al., 2018) to handle the complex and high-dimensional\nprompt optimization tasks (Sec. 4.2). Lastly, we present an uncertainty-informed local exploration\nmethod designed to improve the gradient estimation in our derived NTK-GP framework, thereby\naugmenting the practical performance of the ZOPO algorithm (Sec. 4.3). We also conduct extensive\nexperiments to demonstrate the efficacy of ZOPO (Sec. 5).\n\nTo summarize, the contributions of our work include:\n\n• To the best of our knowledge, we are the first to conduct a thorough empirical study in prompt\n  optimization to underscore the value of local optima and the essence of input domain for efficient\n  and effective prompt optimization.\n\n\n                                       2\n\ntaxonomy_animal   cause_and_effect  informal_to_formal     1.0\n\n\n                                                        0.5\n\n\n                                                        0.0\n\n\nFigure 2: The validation accuracy of 300 randomly sampled prompts with the last token representation\non various tasks.\n\n• Drawing on the insights gained from our empirical study, we introduce the ZOPO algorithm,\n  which outperforms existing baselines in terms of not only the optimization performance but also\n   the query efficiency.\n• We conduct extensive studies to confirm the efficacy of our algorithmic framework and elucidate\n   the underlying principles or insights of our ZOPO algorithm.\n\n2  Problem Setup\n\nGiven an NLP task that is characterized by a data distribution D and a black-box LLM f(·), e.g.,\nChatGPT (OpenAI, 2024a), discrete prompt optimization aims to generate a piece of human-readable\ntext, namely the prompt v, which will then be applied to the black-box LLM f(·) along with a test\ninput x such that the queried LLM output f([v; x]) is able to correctly predict the ground-truth label\ny for each (x, y) ∼D. This problem is then commonly framed as a black-box maximization problem\nover the discrete language space v ∈Ω(Chen et al., 2023; Lin et al., 2023):\n                  maxv∈ΩF(v) ≜E(x,y)∈DV [R (f([v; x]), y)]                           (1)\nwhere R (f([v; x]), y) is applied to measure the alignment between the LLM output f([v; x]) and\nthe groundtruth y, and DV is the validation set sampled from D. Note that the performance of the\noptimal instruction found on DV  (i.e., arg maxv F(v)) will be evaluated on a held-out test set DT .\n\n3  Empirical Study on Prompt Optimization\n\n3.1  Local Optima vs. Global Optimum\n\nIn prompt optimization, methods like (Chen et al., 2023; Lin et al., 2023) are generally more effective\nthan the others (Zhou et al., 2023; Guo et al., 2024), which is usually contributed to their usage of\nBayesian optimization, a popular global optimization strategy, that is able to find the global optimum\nin low-dimensional problems (Moriconi et al., 2020). However, these methods sometimes perform\npoorly in certain prompt optimization tasks, e.g., cause_and_effect and informal_to_formal,\nindicating that they will fail to find the global optimum in these tasks given a limited query budget.\nThis is likely because substantial portions of the budget are applied in these methods to explore\nthe entire search space for the global optimum, which hence leads to the critical question about the\nnecessity of finding the global optimum in query-efficient prompt optimization.\n\nTo answer this question, we have employed a 3-dimensional scatter plot to visualize the performance\n(differentiated by colors) for 300 randomly sampled prompt candidates on various tasks, whose\nprompt embeddings (i.e., the last token embedding as in Lin et al. (2023)) are reduced by t-distributed\nstochastic neighbor embedding (t-SNE) (see more details in our Appx. C.1.1). The results are in\nFig. 2 which shows that the global optimum (i.e., the points achieving an accuracy of ∼100%)\nis consistently rare for a range of prompt optimization tasks, making it extremely challenging to\nachieve this global optimum in practice. In contrast, prompt optimization often features a number\nof local optima (e.g., the points achieving accuracy higher than 50% in all the three tasks of Fig.2).\nImportantly, these local optima commonly enjoy impressive performance, suggesting that local\noptima shall be more worthwhile to obtain in prompt optimization, especially for the scenarios of\nlimited query budgets, as summarized below.\n\n\n                                       3\n\ntaxonomy_animal         cause_and_effect        informal_to_formal\n                  2.0                                                    8\n                              Vicuna-13B    4            Vicuna-13B              Vicuna-13B                  1.5                                                    6                                     Density              ChatGPT                  ChatGPT               ChatGPT\n                  1.0                                                    4\n                                           2\n                  0.5                                                    2                                                          Probability\n                  0.0                          0                          0\n                     0.0          0.5          1.0   0.0          0.5          1.0   0.0        0.2        0.4\n                                               Validation Accuracy\n\nFigure 3: The estimated accuracy distribution of prompts generated by Vicuna-13B or ChatGPT on\nvarious instruction induction tasks, where the vertical dotted line is the mean performance.\n\n\n\n Insight I\n\n Contrasting with the rarity of global optimum, local optima are usually prevalent and well-\n performed, which is more worthwhile for query-efficient prompt optimization.\n\n\n3.2  Essence of Input Domain\n\nBesides, existing works (Chen et al., 2023; Lin et al., 2023; Guo et al., 2024) typically apply their\nprompt optimization in differing input domains, leading to a wide range of performances in practice.\nThese results thus inspire us to ask: How essential is the input domain for finding well-performing\nprompts, particularly the local optimal ones? Thoroughly exploring this question is fundamental for\nthe design of a well-performing prompt optimization algorithm.\n\nTo answer this, we first visualize the accuracy distributions of 300 prompt candidates that are randomly\ngenerated by Vicuna-13B and ChatGPT for various tasks to study the essence of prompt generation in\nFig. 3 (more details in Appx. C.1.2). Fig. 3 reveals that the prompt candidates produced by ChatGPT\n(a black-box model) generally exhibit better performance than those produced by Vicuna-13B (a\nwhite-box model), which has been widely applied in (Chen et al., 2023; Lin et al., 2023) for prompt\noptimization. Importantly, ChatGPT demonstrates a greater likelihood of generating locally optimal\nprompt candidates (e.g., the ones of accuracy higher than 0.5 across all the three plots in Fig. 3).\nThese results indicate that the ability to generate well-performing local optima in prompt optimization\nusually varies for different NLP models. So, the selection of the prompt generation model is crucial\nfor finding well-performing optima.\n\nWe then investigate the function surface (i.e., accuracy landscape) using two different embeddings as\nthe representation for prompt candidates in Fig. 4 (more details in Appx. C.1.2) where the embeddings\nare mapped into a 2-dimensional domain using the t-SNE for better visualizations. Interestingly, Fig. 4\nunveils that different representations will convey a varying number of well-performing local optima\nin practice. Particularly, the last token embedding is usually able to produce a larger number of well-\nperforming local optima than the SBERT (i.e., a popular sentence embedding transformer Reimers &\nGurevych (2019)) embedding, making it easier to enjoy a good prompt optimization performance on\nthis domain, as validated in Tab. 5. This therefore implies that the choice of the prompt representation\nmodel is also essential for the finding of well-performing optima.\n\nIn all, we conclude our aforementioned insights as below.\n\n  Insight II\n\n The choice of the input domain, covering both the generation and the representation of prompt\n  candidates, affects the identification of well-performing local optima.\n\n\n4  The ZOPO Algorithm\n\nGiven the insights established in our Sec. 3, we then propose our Localized Zeroth-Order Prompt\nOptimization (ZOPO) algorithm (Algo. 1) for a better-performing as well as more query-efficient\n\n\n                                       4\n\ntaxonomy_animal   cause_and_effect  informal_to_formal\n\n                                                                                         1.0\n                                               Token                                                             0.8\n                                      Last                                                             0.6\n\n                                                                                         0.4\n\n                                                                                         0.2                                               SBERT\n                                                                                         0.0\n\n\nFigure 4: The function surfaces on various tasks using the last token embedding from Vicuna-13B or\nthe SBERT embedding as the representation for prompt candidates that are generated by Vicuna-13B.\n\n\nAlgorithm 1 The ZOPO Algorithm\n\n  1: Input: prompt generation model g(·), NLP embedding model h(·), size of prompt candidates m,\n     iteration number T, set V = ∅, set Z = ∅\n  2: repeat\n  3:   v ←g([Ddemo])\n  4:   z ←h(v)\n  5:     if v /∈V then V ←V S{v}, Z ←Z S{z}\n  6: until |V| = m\n  7: for t = 1 to T do\n  8:     if 1At(zt) = 1 then do local exploration in Sec. 4.3\n  9:   zt+1 = PZ(zt + ηtµt(zt))\n10:   vt+1 = h−1 (zt+1)\n11:   Query zt+1 to yield eF(zt+1)\n12: end for\n13: z∗←arg maxz1:T eF(z)\n14: Return h−1(z∗)\n\n\n\nprompt optimization. Specifically, following our Insight II, we first develop a more general transfor-\nmation for the input domain of prompt optimization (Sec. 4.1), which can enjoy both the remarkable\ngeneration ability from any type of LLMs (white/black-box) and the impressive representation ability\nfrom many NLP models. Subsequent to this transformation, inspired by our Insight I, we propose to\nuse zeroth-order optimization (ZOO) with a derived NTK Gaussian process inspired from (Shu et al.,\n2023a) to find well-performing local optima (Sec. 4.2). Lastly, we introduce an uncertainty-informed\nlocal exploration technique to refine the gradient estimation in our derived NTK Gaussian process,\naiming to enhance the performance of our ZOPO algorithm in practice (Sec. 4.3).\n\n\n4.1 A More General Input Domain Transformation\n\nAs introduced in our Sec. 3.2, the choice of input domain (including the generation and representation\nof candidates) significantly influences the ultimate performance in prompt optimization: Black-box\nLLMs (e.g., ChatGPT) typically enjoy an advanced generation ability and different embedding models\n(e.g., SBERT) have varying representative capacity for prompt optimization. This naturally inspires\nus to develop an improved domain transformation that can utilize not only the remarkable generation\nability from white/black-box LLMs but also the impressive representation ability from certain NLP\nmodels for our prompt optimization. To achieve this, we propose to make use of the prompt v ∈Ω\ngenerated from a LLM g(·) and subsequently transform it into a continuous hidden representation\nz ∈Z ⊂Rd by other sentence embedding model h(·) for the optimization, i.e., v = h−1(z), where\n(1) can then be re-framed as\n\n                max                    z∈Z eF(z) = E(x,y)∈D R f([h−1(z); x]), y     .                       (2)\n\n\n                                       5\n\nOf note, our input domain transformation and (2) enjoy a number of major advantages compared with\nprevious works: (a) Different from the direct optimization over the discrete and complex language\nspace v ∈Ωin Guo et al. (2024) where optimization algorithms in the numerical domain can hardly\nbe applied, our transformed input domain leads to a dense numerical space of lower dimension and\ntherefore allows the usage of query-efficient optimization algorithms for (2) (e.g., our Algo. 1). (b)\nDifferent from the potential many-to-one mapping in the previous works (Chen et al., 2023; Lin et al.,\n2023), i.e., the same discrete prompt v may be generated by various continuous soft prompts s, we\ndevelop a one-to-one mapping where one prompt generally has a unique hidden representation z,\nwhich thus can help eliminate the redundant queries during optimization and ultimately lead to more\nquery-efficient prompt optimization. (c) Our domain transformation with an independent generation\nand representation process is capable of enjoying the remarkable generation ability from any type of\nLLMs (white/black-box) and the impressive representation ability from many NLP models whereas\nprevious works are highly restricted to the LLMs, thus leading to a wider application.\n\nPractical Implementations.  Before the start of the optimization on (2), we usually generate\nnumerous prompt candidates V = {v} and their corresponding representations Z = {z} (line 2-6 of\nAlgo. 1). Two practical methods are considered here for prompt generation: (a) Feeding randomly\nsampled soft prompts s ∈Rd and a few demonstrations Ddemo into a white-box LLM g(·). (b)\nSampling the output distribution of a black-box LLM g(·) given a generation template filled with\nDdemo. The representations Z can be produced by an NLP model h(·). Specifically, if we consider\nthe generation method in (a), z can be chosen as the last token embedding from g(·) Lin et al. (2023)\nor the soft prompt s Chen et al. (2023) when generating v. Here h(·) then represents a mapping\nfunction from v to z.\n\n4.2  Local Optimization with Derived NTK-GP\n\nAs local optima are more prevalent than global optimum and can exhibit compelling performance for\nprompt optimization tasks (Sec. 3.1), we propose to apply zeroth-order optimization (ZOO), particu-\nlarly gradient descent using estimated gradients, for a well-performing local prompt optimization on\nour transformed input domain Z in Sec. 4.1. Unfortunately, existing ZOO algorithms are typically\nquery-inefficient as many additional queries are required for gradient estimation in every gradient\ndescent update (Flaxman et al., 2005; Nesterov & Spokoiny, 2017). In light of this, we resort to the\nmost recent ZoRD algorithm (Shu et al., 2023a) where a localized surrogate model will be applied\nfor query-efficient gradient estimations.\n\nSpecifically, according to (Shu et al., 2023a), given a well-specified kernel function k(·, ·) such\nthat the function eF is sampled from a Gaussian process eF ∼GP(0, k(·, ·)) or alternatively\nminG∼GP(0,k(·,·)) maxz∈Z |eF(z) −G(z)| = 0 and the observed value r of function eF follows\nthe Gaussian noise N(0, σ2), then conditioned on the history of function queries Dt ≜{(zτ, rτ)}tτ=1\nof size t, ∇eF follows a derived Gaussian Process GP(µ(·), Σ(·, ·)) , i.e.,\n                           ∇eF ∼GP  µt(·), Σ2t(·, ·)  ,                                  (3)\nin which the mean function µt(·) and the covariance function Σ2t(·, ·) are defined as\n\n                                           −1\n                       µt(z) ≜kt(z)⊤ Kt + σ2I    rt ,\n                                                                                                    (4)\n                Σ2t(z, z′) ≜k′′(z, z′) −kt(z)⊤ Kt + σ2I −1 kt(z′) .\nHere, kt(z)⊤≜[∂zk(z, zτ)]tτ=1 is a d × t-dimensional matrix, Kt ≜[k(zτ, k(zτ ′)]tτ,τ ′=1 is a t × t-\ndimensional matrix, r⊤t ≜[rτ]tτ=1 is a t-dimensional column vector, and k′′(z, z′) ≜∂z∂z′k(z, z′)\nis a d × d-dimensional matrix. As a result, µt(z) can be applied to estimate the gradient of the\nblack-box function eF at input z.\nOf note, the underlying black-box function eF here is highly related to deep neural networks (DNN),\nmore specifically transformers. It naturally inspires us to apply the Neural Tangent Kernel (NTK)\n(Jacot et al., 2018) theory for a better approach to the aforementioned assumption of a well-specified\nkernel function k(·, ·). This is because it has been widely proven that NTK is capable of well\ncharacterizing the predictions of neural networks (Arora et al., 2019; Lee et al., 2019; Shu et al.,\n2022a,b) and therefore should be a better-specified kernel in the setting of prompt optimization than\n\n\n                                       6\n\nthe simple kernel (i.e., Matérn kernel) applied in ZoRD (Shu et al., 2023a). Specifically, given a\nneural network ϕ(θ, z) parameterized by θ ∈Rp, we employ the following empirical NTK as the\nkernel in (3) and (4):\n                            k(z, z′) = ∇θϕ(θ, z)⊤∇θϕ(θ, z)                                  (5)\n                                                           θ=θ0\nwhere θ0 is the initialized parameter of neural network ϕ. By incorporating (5) into (4), we realize\nthe derived NTK-GP for the gradient estimation in our prompt optimization.\n\nBased on this derived NTK-GP, we finally apply standard first-order optimization (e.g., stochastic\ngradient descent) with projected gradients for our local prompt optimization. Specifically, in every\niteration t of our Algo. 1, the next promising prompt candidate will be selected via:\n                             vt+1 = h−1 (PZ(zt + ηtµt(zt)))                               (6)\nwhere PZ(z) ≜arg minz′∈Z ∥z −z′∥is the projection function that projects the updated z ∈Rd\ninto domain Z and ηt is learning rate.\n\nPractical Implementations.  Following the modeling principle of local optimization, only the\nneighbors of z in the query history Dt are used to calculate the gradient µt(z). As we do not know\nthe exact DNN for the underlying black-box function eF, we propose to approximate it using a small\nDNN, which is still able to work well thanks to the theoretically guaranteed universal approximation\nability of DNNs (Shen et al., 2022; Kratsios & Papon, 2022). Our experiments in Sec. 5.3 will further\nvalidate the effectiveness of this implementation.\n\n4.3  Uncertainty-Informed Local Exploration\n\nThough the derived NTK-GP allows us to estimate the gradient at any z ∈Z according to (Shu et al.,\n2023a), we introduce the following Prop. 1 to demonstrate that the error in gradient estimation at a\nspecific input z ∈Z implies considerable variability, which is strongly correlated with the number of\nhistorical queries that are effectively relevant for the gradient estimation at the specific input z ∈Z.\nThis insight, in turn, motivates the creation of our uncertainty-informed local exploration approach,\nas opposed to the adoption of the virtual update mechanism described in (Shu et al., 2023a) for our\nprompt optimization strategy.\nProposition 1. Assume k(z, z′) ≤α and ∥k′′(z, z)∥≤κ for any z, z′ ∈Z. Let δ ∈(0, 1) and\nNz,β ≜{z′ ∈{zτ}tτ=1  | ∥∂zk(z′, z)∥2 ≥β} for given input z ∈Z, the following holds with a\nprobability of at least 1 −δ,\n\n                                                ωβ/d\n                  ∥µt(z) −∇F(z)∥2 ≤ω Σ2t(z) ≤ωκ −\n                                        α + σ2/|Nz,β|\n         √\nwhere ω = d + 2(  d + 1) ln(1/δ) and Σ2t(z) ≜Σ2t(z, z).\n\nHere, Nz,β denotes a set of historical input queries that are effectively relevant for the gradient\nestimation at z where β can be regarded as a measure of effective relevance. Prop. 1 shows that\nthe gradient estimation error of (3) at a specific input z ∈Z is bounded by the norm of covariance\nmatrix Σ2t(z), which is related to the query set Nz,β of effective relevance. Specifically, the gradient\nestimation error at different z varies if the effective relevance β and the number of relevant queries\n|Nz,β| varies with z. When β or |Nz,β| becomes small during ZOO, the gradient estimation error is\nlikely increased, which will lead to poor performance in practice. This likely will happen in prompt\noptimization especially considering the sparsity of prompt candidates w.r.t. the continuous domain\nRd. That is, both the effective relevance β and the number of relevant queries |Nz,β| can be small\ndue to this sparsity. As a consequence, additional input queries should be conducted to increase both\nβ and |Nz,β| for a better-performing prompt optimization.\n\nTo this end, we propose an uncertainty-informed local exploration method that utilizes additional input\nqueries from local searches to reduce predictive uncertainty and hence the gradient estimation error\nin derived NTK-GP according to Prop. 1. Specifically, we propose the local exploration condition\ninformed by the local trajectory:\n\n                                        1   zt ∈At                                1At(zt) =                                        0   zt /∈At\n\n\n                                       7\n\nTable 1: Average test accuracy with standard error (3 runs) for the best prompt found by different\nmethods for 20 instruction induction tasks. ∆1 indicates the accuracy gap between ZOPO and the\nbest-performing baselines (among APE, InstructZero, INSTINCT, and EvoPrompt). ∆2 indicates the\naccuracy gap of ZOPOGPT. We bold the highest accuracy when comparing ZOPO with baselines,\nand use gray cell to highlight the highest accuracy when comparing ZOPOGPT with baselines.\n\n  Tasks             APE       InstructZero  INSTINCT  EvoPrompt  ZOPO    ZOPOGPT    ∆1    ∆2\n  antonyms                 63.7±14.2   82.7±0.7        84.7±0.3      84.0±0.0      85.2±3.2    84.0±1.4        0.5   −0.7\n  auto_categorization       25.0±0.9    25.7±1.2        25.0±3.3      31.0±1.0      32.7±1.9    27.0±5.0        1.7   −4.0\n  auto_debugging           29.2±3.4    37.5±0.0        29.2±3.4      33.0±7.2      41.7±15.6   29.2±5.9        4.2   −8.3\n  cause_and_effect         57.3±8.9    81.3±1.1        58.7±8.7       84.0±13.9     94.7±3.7    80.0±14.2      10.7   −4.0\n  common_concept         6.9±2.1     8.6±4.0         21.3±0.2      11.1±6.9      23.5±3.4    2.8±0.6         2.2  −18.5\n   diff                      67.3±26.7   69.3±22.2       100.0±0.0     27.3±42.2     100.0±0.0   100.0±0.0       0.0      0.0\n  informal_to_formal       57.4±0.3    53.1±0.2        55.3±0.0      51.6±0.9      61.3±2.7    61.9±2.9        3.9      4.5\n   letters_list                100.0±0.0   59.0±16.7       100.0±0.0     100.0±0.0     100.0±0.0   100.0±0.0       0.0      0.0\n  negation                  75.3±1.1    77.7±1.4        81.7±0.3       86.0±0.0      86.3±0.5    77.7±2.6        0.3   −8.3\n  object_counting           36.3±1.9    36.0±9.3        34.0±7.0      55.0±5.3      52.3±6.6    40.3±0.5     −2.7  −14.7\n  odd_one_out             63.3±1.4    61.3±8.7        70.0±1.6      10.0±0.0      32.0±11.3   68.7±2.5    −38.0   −1.3\n  orthography_starts_with   45.7±14.8   50.7±8.7        66.7±2.7      15.0±3.4      56.5±12.6   71.0±0.0    −10.2      4.3\n  rhymes                   15.7±6.4    100.0±0.0      100.0±0.0     59.7±3.1      100.0±0.0   61.0±2.8        0.0  −39.0\n  second_word_letter       74.7±20.3   43.3±18.7       10.0±4.1      24.7±0.6      25.7±4.7    96.7±2.4    −49.0    22.0\n  sentence_similarity        0.0±0.0     0.0±0.0         14.0±0.5      2.0±1.0        7.6±9.3     37.3±0.9     −6.4    23.3\n  sum                      67.3±26.7   100.0±0.0      100.0±0.0     100.0±0.0     100.0±0.0   100.0±0.0       0.0      0.0\n  synonyms                36.0±7.6    27.7±9.3        30.7±4.9      40.3±4.0      43.3±0.9    44.7±4.1        3.0      4.4\n  taxonomy_animal         34.7±23.4   71.7±8.4        85.7±6.0      83.0±4.6      90.0±7.1    92.3±0.5        4.3      6.6\n  word_sorting             33.0±3.7    31.0±11.4       51.3±0.3      48.0±21.3     60.0±4.2    60.3±3.1        8.7      9.0\n  word_unscrambling       44.0±13.9   55.0±1.7        63.3±0.7      51.3±4.5      59.3±2.8    58.3±1.9     −4.0   −5.0\n\n\nwhere At = {zt|σ(zt−i) ≥λ, ∀i ∈[0, ξ]} is the condition that incorporates uncertainties and λ, ξ\nare the thresholds. If this condition is met (i.e., 1At(zt) = 1), we will query the neighbors of zt in\nthe local region to update our derived NTK-GP, thus improving its gradient estimation.\n\nPractical Implementations.   If we define the set of the n nearest neighbors of zt as Nt ⊆\nZ s.t. |Nt| = n and ∀a ∈Z \\ Nt, ∥a −zt∥≥maxb∈Nt ∥b −zt∥, we propose to query each\nz ∈Nt, whenever 1At(zt) = 1.\n\n5  Experiments\n\nIn this section, we evaluate the performance of ZOPO against several baseline methods, including\nAPE Zhou et al. (2023), InstructZero Chen et al. (2023), INSTINCT Lin et al. (2023), and Evo-\nPrompt Guo et al. (2024), on instruction induction tasks Honovich et al. (2023), and on the arithmetic\nreasoning tasks with improved chain-of-thought prompts Zhou et al. (2023); Lin et al. (2023). We\nuse the performance profile Dolan & Moré (2002), defined in Appx. B.1, as the overall evaluation\nmetric that measures the frequency (i.e., ρ(τ)) of a method within some distance (i.e., τ) from the\nhighest accuracy achieved by any method. We defer more details on the experiments to Appx. B.\n\n\n5.1  Instruction Induction\n\nInstruction induction tasks are commonly used to investigate the prompt optimization performance\nby assessing LLM’s zero-shot in-context learning ability in previous works (Zhou et al., 2023;\nChen et al., 2023; Lin et al., 2023). Although our ZOPO is a general prompt optimization method\ngiven any prompt generation strategy, here we follow the same setting of prompt generation from\nINSTINCT and InstructZero, only for fair comparison. We also adopt the last token embedding\nfrom Vicuna-13B as the prompt representation (same as INSTINCT). Here Vicuna-13B is used to\ngenerate task-specific prompts by feeding random soft prompts, and ChatGPT, as the black-box LLM,\nis the objective function for prompt evaluation, with a fixed query budget of 165. Similarly, we also\nperform a grid search over soft prompt hyperparameters on the validation set. More experimental\ndetails are deferred to Appx. B.3.\n\nSuperior performance of ZOPO. For better distinguishability, we follow the experimental setting\nfrom Lin et al. (2023) to display the results on 20 challenging tasks reported in Tab. 1, where ZOPO\nsignificantly outperforms all baseline methods. Particularly, our ZOPO performs the best in 14 out\nof the 20 tasks presented, while achieving the best performance profile across different τ (see Fig. 1)\n\n\n                                       8\n\ntaxonomy_animal     cause_and_effect    informal_to_formal\n\n                                                                           0.6\n                                                0.8                      Acc. 0.8\n                      Test 0.6                        0.6                        0.4\n                     0.4\n                     40  80 120 160 200     40  80 120 160 200     40  80 120 160 200\n\n                                                                           0.6                                                0.8                   0.75                     Acc.\n                                                0.6                        0.5                     Val. 0.50\n\n                     40  80 120 160 200     40  80 120 160 200     40  80 120 160 200\n                                       # queries\n                       EvoPrompt        INSTINCT            InstructZero       ZOPO\n\nFigure 5: Comparison of the query efficiency between our ZOPO and other existing baselines on\ninstruction induction tasks. The first row shows the test accuracy and the second row shows the\nvalidation accuracy across different tasks.\n\n\ncompared with all baseline methods. For more results on all 30 tasks, refer to Tab. 3 in Appx. C.2,\nwhere the ZOPO consistently outperforms existing methods.\n\nZOPO has better query efficiency. To justify that our local optimization method is more query-\nefficient, we compare ZOPO against baselines at different query budget scales. The results shown in\nFig. 5 and Fig. 10 in Appx. C.2 illustrate that ZOPO generally achieves better performance with the\nsame number of queries compared with other baseline methods and yields superior performance upon\nconvergence. We notice that ZOPO achieves lower validation accuracy yet higher test accuracy on\nthe taxonomy_animal task than INSTINCT, which suggest ZOPO likely has better generalization\nability.\n\nConnecting ChatGPT with ZOPO. With our proposed domain transformation, we empirically\ndemonstrate that ZOPO is capable of performing numerical optimization on ChatGPT-generated\nprompts. Specifically, we use the same generation method as in APE (Zhou et al., 2023) to generate\ntask-specific prompts (i.e., V) from ChatGPT, and use a popular embedding model SBERT to\nprovide the corresponding sentence embeddings (i.e., Z) for V. Then we apply ZOPO to perform\noptimization over the given V and Z, which we name ZOPOGPT. The result of ZOPOGPT compared\nagainst other baselines is shown in Tab. 1, with the corresponding performance profile shown in\nFig. 9 in App. C.2. Fig. 9 demonstrates that ZOPOGPT significantly outperforms other baselines,\nachieving the best performance in 10 out of the 20 tasks as shown in Tab. 1. Specifically, ZOPOGPT\nachieves significantly higher accuracy on some challenging tasks such as second_word_letter and\nsentence_similarity (see the accuracy gap ∆2 = 22.0 and 23.3 in Tab. 1), which we attribute\nto the high-quality of prompt candidates generated by ChatGPT. This is also consistent with our\ndiscussion on the input domain in Sec. 3.2. Here we could not draw a direct comparison between\nZOPO and ZOPOGPT, as the Vicuna last token embedding is specifically associated with the prompt\ngeneration process in ZOPO and cannot be applied to ZOPOGPT. However, using either ZOPO\nor ZOPOGPT is sufficient to outperform baseline methods, which also provides the flexibility of\nprompt optimization in practice. Future research may consider employing better embeddings to\nfurther improve the performance of ZOPOGPT.\n\n5.2  Improving Chain-of-Thought prompt\n\nThe hand-crafted prompt “Let’s think step by step” Kojima et al. (2022) (denoted as hand-craft) has\nbeen shown effective in improving LLMs’ zero-shot multi-step reasoning performance. We show\nthat ZOPO can find a better chain-of-thought prompt across different arithmetic reasoning tasks, as\nevidenced in Table 2. Particularly, ZOPO produces a better prompt “Let’s find the solution by using\nthe given information.” on GSM8K compared to other baselines, improving the performance from\n71.8 (hand-craft) to 75.4. Refer to Appx. B.4 for more experimental details.\n\n\n                                       9\n\nTable 2: The performance of the best zero-shot CoT prompt found by different methods on three\nreasoning tasks.\n Method       Task          Best prompt                                         Score\n\n  hand-craft   AQUA-RAT   Let’s think step by step.                              52.362\n  InstructZero  AQUA-RAT   Let’s break down the problem.                        54.331\n INSTINCT   AQUA-RAT    I have a new solution.                                54.724\n EvoPrompt   AQUA-RAT   Let’s utilize the substitution method to find a solution,  52.756\n                              then try it out together.\n ZOPO     AQUA-RAT   Let’s find the solution by breaking down the problem.    54.724\n  hand-craft   SVAMP       Let’s think step by step.                               76.25\n  InstructZero  SVAMP       Let’s use the equation.                                 79.5\n INSTINCT  SVAMP       Let’s use our brains.                                   81.0\n EvoPrompt  SVAMP       Let’s break down the issue at hand using promptal meth-  79.5\n                            ods to gain a thorough analysis.\n ZOPO     SVAMP       Let’s use logic to solve the problem.                    81.0\n  hand-craft   GSM8K       Let’s think step by step.                              71.797\n  InstructZero  GSM8K       Let’s use the prompt to solve the problem.              74.299\n INSTINCT  GSM8K       Let’s think about it.                                  74.526\n EvoPrompt  GSM8K       Let’s attempt to analyze the situation and give it a shot.   74.526\n ZOPO     GSM8K       Let’s find the solution by using the given information.    75.360\n\n5.3  Ablation Study\n\nIn this subsection, we conduct quantitative analyses to better understand the main components of our\nmethod ZOPO.\n\nVerifying the Essence of Input Domain. To fairly validate the importance of input domain on\nprompt generation, we compare the optimization performances with different prompts generated\nby Vicuna-13B and ChatGPT respectively, using the same embedding model SBERT (i.e., h(·)).\nThe result is shown in Table. 4 in Appx. C.3, with the performance profile in Fig. 11 suggesting\nthat applying ZOPO on ChatGPT-generated prompts is better. We ascribe its better performance to\nChatGPT’s remarkable prompt generation ability. This confirms the importance of the input domain\non prompt generation in our Insight II.\n\nBesides, different embeddings (i.e., Z) of the same prompt candidates can potentially affect the\nfunction landscape as shown in Fig. 4. Thus, we need to study the performance of ZOPO using\ndifferent embedding representations given the same set of prompts. We consider four different\nembeddings here: the last token embedding from Vicuna-13B, the OpenAI embedding provided\nthrough an API (OpenAI, 2024b), the SBERT embedding, and a randomly projected embedding\nbaseline. We observe from Tab. 5 in Appx. C.3 that, although last token embedding is generally better,\nthere are certain tasks that OpenAI and SBERT embeddings perform equally well or better. Besides,\nrandom embedding shows a distinct lesser performance. This again highlights the importance of using\nmore structured embeddings for prompt optimization and indicates the optimal choice of embedding\ncan be task-dependent.\n\nStudy of NTK-GP and uncertainty-informed local exploration. Further experiments are conducted\nto validate the algorithmic design of NTK-GP (in Sec. 4.2) and uncertainty-informed local exploration\n(in Sec. 4.3) of ZOPO. We aim to precisely assess the individual contributions of these components\nby comparing two variations of the original ZOPO algorithm: (a) one with replacing the NTK\ncomponent with Matérn kernel (as in ZoRD), and (b) another with the uncertainty-informed local\nexploration feature removed. The two variations are compared against the original ZOPO on the\ninstruction induction tasks, with results shown in Tab. 6 in Appx. C.4. The superior performance of\nthe original ZOPO demonstrates clear insights into the significance of each component in the overall\nperformance of ZOPO.\n\nAdditional Results. We also perform an ablation study to examine the impact of a larger size of\nthe generated prompt candidates (i.e., |V|) on ZOPO and ZOPOGPT in Appx. C.5, which suggests a\nrelatively small set of strong prompt candidates (e.g., |V| = 500) is sufficient (compared with size\n1000 or 2000). Additionally, we provide more demonstrations of our empirical findings in Sec. 3 on\nother tasks in Appx. C.1, which is consistent with our findings.\n\n\n                                       10\n\n6  Related Work\n\nSoft Prompt Tuning. To control LLMs to perform specific downstream tasks (e.g., reasoning),\nsoft prompt tuning (Li & Liang, 2021) is usually used as a lightweight method to fine-tune the\nLLMs by only optimizing a continuous vector prepended to the input tokens using gradient descent.\nHowever, when the gradient information of the model is inaccessible, gradient-free prompt tuning\nmethods Sun et al. (2022b,a); Diao et al. (2023) are developed to alleviate human efforts in prompt\ndesign. However, those efforts to optimize the task-specific soft prompts have conventionally relied\non the white-box access to the embedding layers of LLMs, making it inapplicable to state-of-the-art\nLLMs like ChatGPT (OpenAI, 2024a) and GPT-4 (OpenAI, 2023) that can only be accessed through\nblack-box APIs (i.e., only accept natural language as input).\n\nDiscrete Prompt Optimization. We refer to the process of optimizing discrete prompts as “prompt\noptimization\", which is also a more practical setting as black-box LLMs only accept discrete inputs.\nReinforcement learning-based methods (Deng et al., 2022; Zhang et al., 2023) focus on discrete token\noptimization but rely on the output distribution of the LLMs, which is not accessible in black-box API\nLLMs (e.g., ChatGPT). Zhou et al. (2023) instead makes use of LLMs to produce promising candidate\nprompts through resampling without applying specific optimizations. The recent work of Guo et al.\n(2024) further extends this model-free approach to evolutionary algorithms and proposes EvoPrompt\nto optimize prompts through iterative mutation and crossover. However, these methods typically\nrequire a large number of iterations and queries to perform well. In this regard, InstructZero Chen\net al. (2023) leverages the induction ability from other white-box LLM g(·) for the generation of\ntask-specific prompts, that is v = g([s, Ddemo]) conditioned on a continuous soft prompt s ∈Rd\nand in-context demonstrations Ddemo. After that, the optimization on v can be transformed into\nan optimization on the soft prompt s, where BO algorithms are employed for a global black-box\noptimization. INSTINCT (Lin et al., 2023) further employs neural bandit algorithms and the last\ntoken embeddings from the white-box LLM to further improve the prompt optimization performance.\nHowever, these works prioritize a global optimization approach that emphasizes the exploration of the\nentire space. With an empirical understanding of the underlying target function (i.e., the black-box\nAPI LLMs), we propose a localized ZOO method that is in contrast to the global optimization\napproaches.\n\n7  Conclusion\n\nIn this work, we first provide a thorough empirical study to understand the characteristics of the\ntarget function, and then propose our ZOPO algorithm for prompt optimization. Specifically, ZOPO\nembraces a ZOO approach in pursuit of finding local optima efficiently. Extensive experiments on 30\ninstruction induction tasks and 3 reasoning tasks demonstrate the efficacy of ZOPO, and ablation\nstudies also validate the design principles of ZOPO. Besides, we propose a domain transformation\nthat connects powerful LLMs with remarkable embedding models, which provides the flexibility of\nchoices of input domains in prompt optimization. This may inspire future research in optimizing\nprompts with powerful embeddings.\n\n\n\n\n\n                                       11\n\nReferences\n\nArora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R. On exact computation with an\n   infinitely wide neural net. In NeurIPS, pp. 8139–8148, 2019.\n\nChen, L., Chen, J., Goldstein, T., Huang, H., and Zhou, T.  InstructZero: Efficient instruction\n  optimization for black-box large language models. arXiv preprint arXiv:2306.03082, 2023.\n\nDeng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E., and Hu, Z.\n  RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proc. EMNLP, pp.\n  3369–3391, 2022.\n\nDiao, S., Huang, Z., Xu, R., Li, X., Yong, L., Zhou, X., and Zhang, T. Black-box prompt learning for\n   pre-trained language models. Transactions on Machine Learning Research, 2023.\n\nDolan, E. D. and Moré, J. J.  Benchmarking optimization software with performance profiles.\n  Mathematical programming, 91:201–213, 2002.\n\nFlaxman, A., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting:\n  Gradient descent without a gradient. In Proc. SODA, 2005.\n\nGuo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. Connecting large\n  language models with evolutionary algorithms yields powerful prompt optimizers. In ICLR, 2024.\n\nHonovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples\n   to natural language task descriptions. In Proc. ACL, pp. 1935–1952, 2023.\n\nJacot, A., Hongler, C., and Gabriel, F. Neural Tangent Kernel: Convergence and generalization in\n  neural networks. In Proc. NeurIPS, pp. 8580–8589, 2018.\n\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot\n  reasoners. In Proc. NeurIPS, volume 35, pp. 22199–22213, 2022.\n\nKratsios, A. and Papon, L. Universal approximation theorems for differentiable geometric deep\n   learning. The Journal of Machine Learning Research, 23(1):8896–8968, 2022.\n\nLee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. Wide\n  neural networks of any depth evolve as linear models under gradient descent. In Proc. NeurIPS,\n  pp. 8572–8583, 2019.\n\nLi, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. Proc. ACL, pp.\n  4582–4597, 2021.\n\nLin, X., Wu, Z., Dai, Z., Hu, W., Shu, Y., Ng, S.-K., Jaillet, P., and Low, B. K. H. Use Your INSTINCT:\n  INSTruction optimization usIng Neural bandits Coupled with Transformers. In NeurIPS 2023\n  Workshop on Instruction Tuning and Instruction Following, 2023.\n\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A\n  systematic survey of prompting methods in natural language processing. ACM Computing Surveys,\n  55(9):1–35, 2023.\n\nMishra, S., Khashabi, D., Baral, C., Choi, Y., and Hajishirzi, H. Reframing instructional prompts to\n  gptk’s language. ACL Findings, pp. 589–612, 2021.\n\nMoriconi, R., Deisenroth, M. P., and Sesh Kumar, K. High-dimensional bayesian optimization using\n  low-dimensional feature spaces. Machine Learning, 109:1925–1943, 2020.\n\nNesterov, Y. E. and Spokoiny, V. G. Random gradient-free minimization of convex functions. Found.\n  Comput. Math., 17(2):527–566, 2017.\n\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\nOpenAI. ChatGPT. https://openai.com/blog/chatgpt, 2024a.\n\nOpenAI.                 Documentation      of     OpenAI’s       text      embeddings.\n  https://platform.openai.com/docs/guides/embeddings, 2024b.\n\n\n                                       12\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,\n  Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback.\n  Proc. NeurIPS, pp. 27730–27744, 2022a.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,\n   S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,\n  Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions\n  with human feedback. arXiv preprint arXiv:2203.02155, 2022b.\n\nReimers, N. and Gurevych, I. Sentence-BERT: Sentence embeddings using siamese bert-networks.\n  In Proc. EMNLP-IJCNLP, pp. 3982–3992, 2019.\n\nReynolds, L. and McDonell, K. Prompt programming for large language models: Beyond the\n  few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in\n  Computing Systems, pp. 1–7, 2021.\n\nShen, Z., Yang, H., and Zhang, S. Optimal approximation rate of relu networks in terms of width and\n  depth. Journal de Mathématiques Pures et Appliquées, 157:101–135, 2022.\n\nShu, Y., Cai, S., Dai, Z., Ooi, B. C., and Low, B. K. H. NASI: Label- and data-agnostic neural\n   architecture search at initialization. In Proc. ICLR, 2022a.\n\nShu, Y., Dai, Z., Wu, Z., and Low, B. K. H. Unifying and boosting gradient-based training-free neural\n   architecture search. In Proc. NeurIPS, pp. 33001–33015, 2022b.\n\nShu, Y., Dai, Z., Sng, W., Verma, A., Jaillet, P., and Low, B. K. H. Zeroth-order optimization with\n  trajectory-informed derivative estimation. In Proc. ICLR, 2023a.\n\nShu, Y., Lin, X., Dai, Z., and Low, B. K. H. Federated zeroth-order optimization using trajectory-\n  informed surrogate gradients. arXiv preprint arXiv:2308.04077, 2023b.\n\nSun, T., He, Z., Qian, H., Huang, X., and Qiu, X. Bbtv2: Pure black-box optimization can be\n  comparable to gradient descent for few-shot learning. arXiv preprint arXiv:2205.11200, 2022a.\n\nSun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Black-box tuning for language-model-as-a-service.\n  In International Conference on Machine Learning, pp. 20841–20855. PMLR, 2022b.\n\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,\n  Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D.,\n  Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,\n   S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\n  Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T.,\n  Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A.,\n   Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan,\n   J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A.,\n   Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models.\n  arXiv preprint arXiv:2307.09288, 2023.\n\nZhang, T., Wang, X., Zhou, D., Schuurmans, D., and Gonzalez, J. E. TEMPERA: Test-time prompt\n   editing via reinforcement learning. In Proc. ICLR, 2023.\n\nZhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models\n  are human-level prompt engineers. In ICLR, 2023.\n\n\n\n\n\n                                       13\n\nAppendix A  Proofs\n\nA.1  Proof of Prop. 1\n\nWe follow the ideas in (Shu et al., 2023a,b) to prove our Prop. 1. To begin with, we first introduce the\nfollowing lemmas adapted from (Shu et al., 2023a):\n                                     √\nLemma A.1 (Thm. 1 in (Shu et al., 2023a)). Let δ ∈(0, 1) and ω ≜d + 2(  d + 1) ln(1/δ). For\nany z ∈Z and any t ≥1, the following holds with probability of at least 1 −δ,\n\n                                                   2\n                            ∇eF(z) −µt(z)  ≤ω Σ2t(z)   .\nLemma A.2 (Lemma B.4 in (Shu et al., 2023a)). For any z ∈Z and any t ≥1, the following holds\n                            Σ2t(z) ≤ Σ2t−1(x)   .\n\nProof of Prop. 1. Recall that the covariance function (refer to (4)) of our derived NTK-GP conditioned\non the history of function queries Dt ≜{(zτ, rτ)}tτ=1 of size t will be\n                  Σ2t(z) = k′′(z, z) −kt(z)⊤ Kt + σ2I −1 kt(z) .                      (7)\n\nFor any c ∈R and z ∈Z, define Nz,β ≜{z′ ∈{zτ}tτ=1 | ∥∂zk(z, z′)∥2 ≥β} with |Nz,β| = N,\nthe following then holds on the set Nz,β:\n                                                     (a)≥1 tr kN(z)⊤kN(z)                      kN(z)⊤kN(z)\n                                       d\n                                                         (b) 1\n                        =   tr kN(z)kN(z)⊤\n                                      d\n                                 N                                            (8)\n                                                         (c) 1\n                        = X ∥∂zk(z, z′)∥2\n                                      d\n                                         n=1\n                                                      (d)≥Nβ\n                                       d\n\nwhere (a) comes from the fact the maximum eigenvalue of a matrix is always larger or equal to its\naveraged eigenvalues, (b) is based on tr(AB) = tr(BA), (c) is from the definition of kN(z), and\n(d) results from the definition of Nz,β.\n\nMeanwhile,\n                                  (a)                           −1\n                 Σ2t(z) ≼k′′(z, z) −kN(z)⊤ KN + σ2I   kN(z)\n                                     (b)                    −1\n                 ≼κI − λmax (KN) + σ2   kN(z)⊤kN(z)\n\n                                     (c)\n                 ≼κI −kN(z)⊤kN(z)                                            (9)\n                      Nα + σ2\n                                   (d)      Nβ/d\n                ≼  κ −            I\n                     Nα + σ2\nwhere (a) comes from Lemma A.2, (b) is based on the assumption of ∥k′′(z, z)∥≤κ and the defi-\nnition of maximum eigenvalue. In addition, (c) comes from λmax(KN) ≤N maxz,z′∈Nz,β k(z, z′)\n(i.e., the Gershgorin theorem) and the assumption that k(z, z′) ≤α for any z, z′ ∈Z, and (d) is\nbased on the results in (8).\n\nFinally, by introducing the results above into Lemma A.1, we conclude the proof.\n\n\n\n\n\n                                       14\n\nAppendix B  Details of Experimental Settings\n\nB.1  Evaluation Metrics\n\nFollowing previous works Zhou et al. (2023); Lin et al. (2023), we use the F1 score for tasks including\ncommon_concept and informal_to_formal; we use the exact set matching for orthography_starts_with\nand taxonomy_animal; we use the set containing for synonyms; we use the exact matching metric for\nthe rest of instruction induction tasks; and we use the accuracy metric for the arithmetic reasoning\ndatasets.\n\nAs the number of datasets is tremendous, we use the performance profile Dolan & Moré (2002) as\nthe evaluation metric that measures the frequency (i.e., ρ(τ)) of a method within some distance (i.e.,\nτ) from the optimality achieved by any method, defined below\n\n                                1\n                      ρm(τ) =    |{π ∈Π : r∗π −rπ,m ≤τ}|                        (10)                                    |Π|\n\nwhere Π is the set of all tasks, rπ,m is the accuracy of method m on task π, and r∗π = max{rπ,m :\n∀m ∈M} is the best performance achieved by any method in M on task π. Specifically, ρ(0)\nrepresents the number of tasks where a method achieves the best performance. Accordingly, we use\nboth ρ(0) and ρ(5) as the evaluation indicators in our tables to report the results.\n\nB.2  Hyperparameters\n\nFor all experiments using ZOPO in this work, we set the learning rate to 0.01, the uncertainty\nthresholds λ, ξ to 0.1 and 5 respectively, and the number n of nearest neighbors to query in local\nexploration (Section 4.3) to 10. A neural network with 2 fully connected layers of size 32 and ReLU\nactivation functions is used in NTK-GP as the kernel. We use 20 nearest neighbors to fit the NTK-GP.\n\nB.3  instruction induction\n\nIn this subsection, we describe the experimental details of the instruction induction tasks.\n\nB.3.1  Experimental Specifications\n\nThe same data partition and evaluation process as in previous works Zhou et al. (2023); Chen et al.\n(2023); Lin et al. (2023) is adopted in this work, where, for each task, we optimize the generated\nprompt on a training set D, and report the best-performing prompt’s inference accuracy on a held-out\ntest set DT . Specifically, 5 examples are sampled from the training set as the demonstrations (i.e.,\nDdemo) for instruction induction, and another sampled 20 examples from the training set are used as\nthe validation set DV to evaluate the objective function value as in Equation (1). The total query\nbudget for each instruction induction task is fixed at 165 for all methods.\n\nB.3.2  Implementation Details\n\nTo comprehensively compare with the baseline methods, we use GPT-3.5-turbo-0301 (supported by\nOpenAI API) as the black-box model for prompt evaluation and Vicuna-13B-v1.1 as the white-box\nLLM (i.e., g(·)) to generate the task-specific prompts by feeding g(·) with randomly sampled soft\nprompts and Ddemo, which is the same as InstructZero and INSTINCT. In the experiments, we only\ngenerate 500 prompt candidates for ZOPO (i.e., |V| = 500). Similarly, we also use 40 out of the 165\nqueries for random initialization of our optimization method, which could serve as the only global\nexploration of the function landscape at the beginning of local optimization.\n\nTo tackle the high dimensionality of soft prompt (i.e., 5120 for one token embedding as in Vicuna-13B)\nin optimization, InstructZero and INSTINCT use random projection to project the soft prompt into a\nmuch smaller intrinsic dimension (e.g., 100). This intrinsic dimension may empirically affect the\nquality of generated prompts, as shown in Lin et al. (2023). Therefore, tuning the intrinsic dimension\nand the soft token length could lead to better performance. Previous methods (i.e., InstructZero and\nINSTINCT) perform a grid search over the intrinsic dimension in {50, 100, 200} and the soft token\nlength {3, 5, 10} on the validation set and report the accuracy on a held-out test set using the best\nprompt found using the validation set. We also adopt this technique in ZOPO for fair comparison.\n\n\n                                       15\n\nThe soft prompt will be concatenated with the tokenized embedding of the prompt generation template\nto generate task-specific prompt from Vicuna-13B. The prompt generation template and the prompt\nevaluation template are shown below in the bounding boxes.\n\n\n\n       Prompt  Generation  Template  (Soft\n       Prompt)\n\n       Input: ⟨INPUT⟩\n       Output: ⟨OUTPUT⟩\n       Input: ⟨INPUT⟩                              Evaluation Template\n       Output: ⟨OUTPUT⟩\n                                              prompt: ⟨prompt (i.e., v)⟩\n       Input: ⟨INPUT⟩\n                                                    Input: ⟨TEST INPUT⟩\n       Output: ⟨OUTPUT⟩\n                                                Output:\n       Input: ⟨INPUT⟩\n       Output: ⟨OUTPUT⟩\n       Input: ⟨INPUT⟩\n       Output: ⟨OUTPUT⟩\n      The prompt was to?\n\n\n\n\nWe directly use the reported results of APE, IntructZero, and INSTINCT from Lin et al. (2023)\nfor comparison, and we report the results of EvoPrompt with our re-implementation. For a fair\ncomparison, we also use Vicuna-13B for generating the initial prompt population (of size 20) for\nEvoPrompt, and we use GPT-3.5 turbo to perform the genetic algorithm in EvoPrompt and generate\nits new prompts. Using GPT-3.5 turbo to generate new prompts will help improve EvoPrompt’s\nperformance, as compared with using the relatively smaller model Vicuna-13B.\n\n\nB.3.3  Experimental Details on Query Efficiency\n\nTo facilitate a more comprehensive comparison of different prompt optimization methods at different\nquery budget scales, we set the maximum query budget to 200, and report the test accuracy of the\nbest prompt found on the validation set with each incremental query budget, as shown in Fig. 5 in the\nmain text. We report the mean accuracy and standard error, using 3 runs with different random seeds.\nFor InstructZero, INSTINCT, and ZOPO, we directly fix the intrinsic dimension for generating the\nsoft prompt as 10 and the number of soft tokens as 5, without using the validation set to perform a\ngrid search over the intrinsic dimension and the number of soft tokens.\n\n\n\n                ChatGPT Prompt Generation Template\n\n                      I gave a friend an prompt. Based on the prompt they\n                 produced the following input-output pairs:\n\n                    Input: ⟨INPUT⟩\n                  Output: ⟨OUTPUT⟩\n                    Input: ⟨INPUT⟩\n                  Output: ⟨OUTPUT⟩\n                    Input: ⟨INPUT⟩\n                  Output: ⟨OUTPUT⟩\n                    Input: ⟨INPUT⟩\n                  Output: ⟨OUTPUT⟩\n                    Input: ⟨INPUT⟩\n                  Output: ⟨OUTPUT⟩\n\n                The prompt was to\n\n\n\n                                       16\n\nB.3.4  Experimental Details on ZOPOGPT\n\nFor our experiment on ZOPOGPT in the main text, we apply ZOPO on ChatGPT (i.e., GPT-3.5\nturbo) generated prompts. We follow the generation template from APE Zhou et al. (2023), as\nshown above, to generate task-specific prompts from ChatGPT. To generate various prompts using\nthe APE method, we need to sample different sets of demonstrations (i.e., Ddemo) from the training\nset, and, for each Ddemo, we also need to randomly sample from the ChatGPT’s response by setting\na high temperature (e.g., 0.95). To maintain the same size of prompt candidates as in the previous\nexperimental setting of ZOPO, we also generate 500 prompt candidates for each instruction induction\ntask. To harness the representation power of existing embedding models, we adopt the sentence\ntransformer model Reimers & Gurevych (2019) “all-mpnet-base-v2” from HuggingFace to generate\nthe high-dimensional sentence embedding for each generated prompt from ChatGPT.\n\nB.4  Improving Chain-of-Thought Prompt\n\nTo improve the zero-shot chain-of-thought prompt performance on arithmetic reasoning tasks, we\nmake use of the LLM’s induction ability and enable LLMs to generate different chain-of-thought\nprompt candidates by providing some example chain-of-thought prompts. We consider the evaluation\nof our method on three arithmetic reasoning datasets (i.e., GSM8K, AQUARAT, SVAMP). Similar as\nAPE Zhou et al. (2023), we use all data from the test set for GSM8K and AQUARAT, and we sample\n400 data points from AQUARAT’s test set to evaluate the corresponding test accuracy. For all these\nthree datasets, we sample 200 data points from their training dataset respectively as their individual\nvalidation dataset.\n\nWe follow the experimental setting of Lin et al. (2023): use the soft prompt to generate prompts from\nVicuna-13B with a fixed intrinsic dimension of 1000 and search the soft token length {3, 5, 10} on\nthe validation set. The corresponding prompt generation template is given below.\n\nSee Tab. 2 for details on the performance of ZOPO against other baselines on the three reasoning\ntasks.\n\n                  Prompt Generation Template for Chain-of-Thought\n\n                       I have some prompt examples for solving school math\n                   problems.\n\n                 prompt:\n                    Let’s figure it out!\n\n                 prompt:\n                    Let’s solve the problem.\n\n                 prompt:\n                    Let’s think step by step.\n\n                 Write your new prompt that  is  different from\n                    the examples to solve the school math problems.\n\n                 prompt:\n\n\n\n\n\n                                       17\n\nAppendix C  Additional Results\n\nC.1  Extended Empirical Study on Function Landscape\n\nIn Section 3, we have empirically studied the landscape of the target function and incorporated the\nfindings into the design of ZOPO. In the main text, we have demonstrated the results on three in-\nstruction induction datasets, including taxonomy_animal, cause_and_effect, and informal_to_formal.\nHere we use more datasets to validate our findings. Due to the large size of instruction induction\ntasks (i.e., 30 tasks in total) and the query budget limit (i.e., it incurs monetary costs when we query\nthe objective function ChatGPT to evaluate the prompt on the given task), we only experiment with\nfew more randomly chosen tasks here to further validate our findings.\n\nC.1.1  Local Optima vs. Global Optimum\n\nTo validate our local optimization design, we study the local optima in the function landscape, by\nusing a 3-dimensional (reduced by t-SNE) scatter plot to represent the prompt embeddings (last token\nembeddings from Vicuna-13B). Here we provide the empirical results on more instruction induction\ntasks, shown in Fig. 6. The heatmap color represents the validation accuracy of the corresponding\nprompt. This allows us to interpret the local optima visually, and we conclude that many local optima\ncan already exhibit compelling performance.\n\n\n            word_sorting            sentiment            synonyms          singular_to_plural     common_concept\n                                                                                                1.0\n\n\n                                                                                                0.5\n\n\n                                                                                                0.0\nFigure 6: The validation accuracy of 300 randomly sampled prompts with the last token representation\non various tasks.\n\n\nC.1.2  Essense of Input Domain\n\nPrompt Generation  To study the prompt quality of different prompt generation methods, we\ncompare the prompts generated from Vicuna-13B and those generated from ChatGPT (i.e., GPT\n3.5 turbo). For Vicuna-13B, we use the randomly sampled soft prompts with a fixed intrinsic\ndimension of 200 and a number token length of 10. For ChatGPT, we randomly sample prompts\nfrom the ChatGPT’s response by using the APE generation template filled with random example\ndemonstrations. For each generation method on each task, we generate 300 random prompts, and we\nquery the target function with all prompts. We show the validation accuracy distribution of prompts\ngenerated by the two methods on four more (due to budget constraints) tasks here in Fig. 7.  It\ndemonstrates that ChatGPT has a larger probability of generating prompts with higher accuracy, also\nwith a larger mean. The result shows that ChatGPT-generated prompts are generally better, further\nvalidating our finding of the importance of the input domain.\n\n                  auto_categorization      negation        singular_to_plural     synonyms\n\n                                                                             4                                     Density 10                   2                  10\n                                                        5\n                5                   1                                        2\n                                                          Probability  0                                        0                   0\n                     0.0          0.2        0.0        0.5          0               1   0.0           0.5\n                                                 Validation Accuracy\n\n                                                 Vicuna-13B         ChatGPT\nFigure 7: The estimated accuracy distribution of prompts generated by Vicuna-13B or ChatGPT on\nvarious instruction induction tasks, where the vertical dotted line indicates the mean performance.\n\n\nPrompt Embedding  The complexity of modeling the target function depends on its function\nlandscape defined by the embedding domain. To empirically analyze the black-box target function,\n\n\n                                       18\n\nwe show the accuracy landscape of different tasks, where we reduce the dimension of the prompt\nembedding (we use the last token embedding of Vicuna-13B here) to 2 by using t-SNE. The loss\nlandscape is visualized in the surface plot shown in Fig. 8. We observe that different optimization\nmethods achieve similar performances on tasks like sentiment and singular_to_plural, as they have\nmany good local optima. For other challenging tasks with complex function landscapes, the good\nlocal optima are less, but our methods can still achieve superior performance. This validates our\ninsight that there are many good local optima in the embedding space.\n\n\n         word_sorting         sentiment         synonyms       singular_to_plural   common_concept\n                                                                                                                1.0\n\n\n                                                                                                                0.5\n\n\n                                                                                                                0.0\n\n                                                          0.0                                                                                                                                             0.6                                                                                                                                               0.7                                                                                                                                                            0.045                                                           0.4                                                                                                      0.06                     0.05                                                                                                                         0.5                                                                           0.30.1                                                                                                                                                  0.8                                                             0.2                                                                  0.6                               0.150.10                  0.00                                                        0.5                                                                                                                                   0.18                                 0.05                                                                                               0.00                                                          0.7                                                                                                                        0.8                                                                                                                        0.5                                                                                                                  0.8                                                                                                                                                                      0.015                               0.00                                                                                                           0.9                                                                                                                                                                               0.015                                                                                                                                                                  0.015                                                                                                                                   0.7                                                                                                  0.24                                                                                                                                               0.8                                                                                                     0.12                                                                                      0.12                                                           0.0                                                                                                                                                     0.60.7                                                                                                                       0.3                                                                                                                                0.50.4                                                                              0.0                                                                                                                                 0.30.2                                                                                                                    0.60.20.8                                                                                                                           0.3                                                          0.1                                                    0.0                                                                                     0.18                                                                                                                               0.60.1                                                              0.1                                                                                                                                               0.50.70.60.8                                                                                                                   0.4                                                                                                                                 0.3                                                                                                                  0.7                               0.05                                                                                                                   0.1                                                       0.5                                              0.8                                                                                                                  0.3                                                                                                          0.00                                                                                                                       0.50.6                                                                                                                 0.9                                                                                                                                         0.2                                           0.3                                                                                                                                                                 0.5                                                                                                                          0.0                                                                                                                                      0.4                                                                                                  0.12                                                       0.1                                                                                                                                                         0.9                                                                                                                                                 0.50.60.4                                                             0.70.7                                                                                                                            0.8                                                                                                                      0.5                                                               0.50.7                                                                                                                              0.8                                                                                                                              0.7                                                                                                                                                                                                                                                                                                                               0.0150.030                                                               0.80.60.6                                                                   0.1                                                 0.70.20.6                                                                         0.50.40.6                                                          0.6                                                                                                                          0.4                                                                                                                     0.70.7                                                                                                                                   0.7                                                           0.7                                                                                                                                                                               0.6                                                                                            0.06                                                                                                                                  0.000.06                                                                                                                                                0.70.6                                                                                                                                      0.6                                                                                                                                                       0.70.5                                                          0.5                                                                                                                            0.4                                                                                                                         0.70.30.6                                                                                    0.0                                                                                                                         0.70.8                                                   0.1                                                                                                                                                    0.030                                                                                                                         0.50.40.50.6                                                                                         0.06                                                                                                                                                                                                                0.030                                                               0.4                                                                                                        0.00.20.1                                                                                                                               0.7                                                                                                                                                                 0.015                                                                                              0.060.12                                                        0.0                                                                 0.2                                                                    0.4                                                 0.5                                                      0.5                                                 0.1                                                                0.05                                                                 0.8                                                                                                                                                                                                                                                                                                                                                                                                       0.015                                              0.0                                                                                                                      0.4                               0.00                                                                                                                                                  0.2                                                                  0.7                                                          0.3                                                         0.40.50.2                                                                0.40.30.3                                                           0.30.4                                                       0.6                                                                                                                                        0.8                                                                                                                                                                                                                                                                                                                                   0.015                                                            0.0                                                                                                                             0.8                                                                                                                            0.5                                                          0.50.20.60.10.3                                                            0.8                                                                                                                   0.10.6                                                                         0.3                          0.00                                                 0.2                                                 0.2                                                                                                               0.3                                                                                                               0.5                                                                                                                  0.70.7                                                0.70.3                                                                                                                          0.20.3                                                                                                                  0.60.6                                                                                                               0.20.4                                                                                                              0.60.5                                            0.4                                                                                                0.00                                                                                                               0.06                                                        0.0                                                                                                              0.3                                                0.60.7                                                                     0.1                          0.05                                                           0.2                                                                                                                                                                                               0.015                                                                                                                  0.80.5                                              0.50.4                                                                                                                          0.50.4                                                                    0.1                                               0.6                                               0.5                                                                                         0.06                                                                                                           0.50.20.4                                                                                                                                                                                                 0.000                                                                                                                                                                                                                                                                                                               0.015                                                                                                                                    0.1                                                                                                   0.12                                                                                                                                                                                                 0.030                                                        0.2                                                                                                            0.3                                                                                                                                 0.12                                                 0.3                                                               0.40.2                                                               0.0                               0.05                                                                                                  0.18                                0.100.15                                                                                                                                            0.4                                                            0.1                      0.100.15                                                                                                                    0.6                                                                                                                                      0.7                           0.10                                                                                                                                                                      0.000                                                 0.3                   0.050.05                                                                       0.5                                                                                                                 0.6                                                                       0.2                                                                0.3                                                                                                                   0.8\nFigure 8: The function surfaces on various tasks using the last token embedding from Vicuna-13B as\nthe representation for prompt candidates that are generated by Vicuna-13B, with contour plots shown\nbelow.\n\n\n\n\n\n                                       19\n\nC.2  Comparison on Instruction Induction Tasks\n\nIn Section 5.1 of the main text, we compared our methods with other baselines on 20 challenging\ninstruction induction tasks. Here we provide the full results on 30 instruction induction tasks in\nSection 5.1.\n\n\nTable 3: Average test accuracy with standard error (3 runs) for the best prompt found by different\nmethods for all 30 instruction induction tasks.\n\n  Tasks              APE        InstructZero  INSTINCT  EvoPrompt  ZOPO    ZOPOGPT\n  active_to_passive          100.0±0.0   99.7±0.3      97.0±2.5     100.0±0.0    100.0±0.0   100.0±0.0\n  antonyms                 63.7±14.2   82.7±0.7      84.7±0.3     84.0±0.0     85.2±3.2    84.0±1.4\n  auto_categorization        25.0±0.9    25.7±1.2      25.0±3.3     31.0±1.0     32.7±1.9    27.0±5.0\n  auto_debugging           29.2±3.4    37.5±0.0      29.2±3.4     33.0±7.2     41.7±15.6   29.2±5.9\n  cause_and_effect          57.3±8.9    81.33±1.1     58.7±8.7     84.0±13.9    94.7±3.7    80.0±14.2\n  common_concept          6.9±2.1     8.6±4.0       21.3±0.2     11.1±6.9     23.5±3.4    2.8±0.6\n  diff                       67.3±26.7   69.3±22.2     100.0±0.0    27.3±42.2    100.0±0.0   100.0±0.0\n  first_word_letter          100.0±0.0   100.0±0.0     93.0±5.3     100.0±0.0    100.0±0.0   100.0±0.0\n  informal_to_formal        57.4±0.3    53.1±0.2      55.3±0.0     51.6±0.9     61.3±2.7    61.9±2.9\n  larger_animal             89.7±0.5    90.0±4.1      93.7±0.3     87.3±3.1     92.3±2.9    92.7±1.2\n   letters_list                100.0±0.0   59.0±16.7     100.0±0.0    100.0±0.0    100.0±0.0   100.0±0.0\n  negation                  75.3±1.1    77.7±1.4      81.7±0.3     86.0±0.0     86.3±0.5    77.7±2.6\n  num_to_verbal            99.7±0.3    100.0±0.0     100.0±0.0    100.0±0.0    100.0±0.0   100.0±0.0\n  object_counting           36.3±1.9    36.0±9.3      34.0±7.0     55.0±5.3     52.3±6.6    40.3±0.5\n  odd_one_out              63.3±1.4    61.3±8.7      70.0±1.6     10.0±0.0     32.0±11.3   68.7±2.5\n  orthography_starts_with   45.7±14.8   50.7±8.7      66.7±2.7     15.0±3.4     56.5±12.6   71.0±0.0\n  periodic_elements         92.7±2.2    86.7±6.1      92.7±2.7     98.0±1.2     100.0±0.0   94.7±3.1\n  rhymes                   15.7±6.4    100.0±0.0     100.0±0.0    59.7±3.1     100.0±0.0   61.0±2.8\n  second_word_letter        74.7±20.3   43.3±18.7     10.0±4.1     24.7±0.6     25.7±4.7    96.7±2.4\n  sentence_similarity        0.0±0.0     0.0±0.0       14.0±0.5      2.0±1.0       7.6±9.3     37.3±0.9\n  sentiment                 91.3±1.4    87.7±2.4      89.7±1.4     93.0±0.0     93.5±0.5    89.3±2.1\n  singular_to_plural         100.0±0.0   98.7±1.1      100.0±0.0    100.0±0.0    100.0±0.0   100.0±0.0\n  sum                      67.3±26.7   100.0±0.0     100.0±0.0    100.0±0.0    100.0±0.0   100.0±0.0\n  synonyms                 36.0±7.6    27.7±9.3      30.7±4.9     40.3±4.0     43.3±0.9    44.7±4.1\n  taxonomy_animal         34.7±23.4   71.7±8.4      85.7±6.0     83.0±4.6     90.0±7.1    92.3±0.5\n  translation_en-de          84.0±0.5    82.3±0.1      84.0±0.5     85.0±0.0     85.3±0.5    84.7±0.6\n  translation_en-es          87.0±0.0    87.3±0.1      88.0±0.0     82.3±7.4     85.3±2.1    86.3±2.5\n  translation_en-fr          88.7±0.3    87.7±0.0      83.0±2.1     80.7±4.5     91.0±0.0    86.7±2.1\n  word_sorting              33.0±3.7    31.0±11.4     51.3±0.3     48.0±21.3    60.0±4.2    60.3±3.1\n  word_unscrambling       44.0±13.9   59.0±5.3      63.3±0.7     51.3±4.5     59.3±2.8    58.3±1.9\n  # best-performing tasks   4         4           10          7          18        15\n  performance profile ρ(5)   0.37        0.43          0.57         0.47         0.87        0.73\n\n\n\n\n\n                                       20\n\nThe performance profile of ZOPOGPT compared against other baseline methods is shown in Fig. 9.\nThis corresponds to the result shown in Tab. 1.\n\n\n\n                                                                  APE\n                                                                                                     InstructZero\n                     0.75                                        INSTINCT\n                                                                                   EvoPrompt\n                                                                             ZOPOGPT(ours)\n                     0.50                                ½(¿)\n\n                     0.25\n\n\n                        0        10        20\n                                      ¿\n\nFigure 9: The performance profile of ZOPOGPT compared against other baseline methods on 20\ninstruction induction tasks.\n\n\nWe also provide additional results on other instruction induction tasks to compare ZOPO against\nbaseline methods in terms of query efficiency. The result is shown in Fig. 10.\n\n\n           word_sorting       auto_debugging       synonyms      word_unscrambling   common_concept\n                                                                                    0.6\n                                                          0.4                                                 0.2     0.50 Acc.                            0.4\n                                                                                    0.4\n Test 0.25                                                 0.2                                                 0.1\n     0.00                       0.2                                                 0.2\n         40  80  120 160 200   40  80  120 160 200   40  80  120 160 200   40  80  120 160 200   40  80  120 160 200\n                                0.6\n                                                          0.6                                                 0.3\n Acc. 0.50                                                                           0.6\n                                                          0.4                                                 0.2\n Val. 0.25                       0.4                                                 0.5\n                                                          0.2                                                 0.1\n         40  80  120 160 200   40  80  120 160 200   40  80  120 160 200   40  80  120 160 200   40  80  120 160 200\n                                            # queries\n                            EvoPrompt        INSTINCT            InstructZero       ZOPO\n\nFigure 10: Comparison of the query efficiency between ZOPO and other existing baselines on\nvarious instruction induction tasks.\n\n\n\n\n\n                                       21\n\nC.3  Verifying the Essence of Input Domain\n\n\n\nPrompt Generation  To fairly compare the effect of prompts generated by Vicuna-13B and Chat-\nGPT in terms of the optimization performance by using ZOPO, we adopt the same embedding\nrepresentations here, that is we use the SBERT embedding model for both prompts generated by\nVicuna-13B and ChatGPT. For the prompt generation process, we fix the number of prompt candidates\nfor both methods to 500. The result of the comparison on 20 instruction induction tasks is shown\nin Table. 4, where the corresponding performance profile shown in Fig. 11 suggests that applying\nZOPO on ChatGPT-generated prompts is better than applying it on Vicuna-generated prompts. This\nagain confirms the importance of the choice of the input domain (i.e., the prompt generation).\n\n\n\n\n\n                                          Table 4: Fair comparison of the optimization perfor-\n                                   mance of ZOPO with different generated prompts\n                                            but the same embedding model (i.e., SBERT).\n                                             Tasks                 Vicuna-13B  ChatGPT\n       0.9                                               Vicuna-13B    antonyms                 78.3±4.5       84.0±1.4\n                                                       ChatGPT       auto_categorization       29.7±2.9       27.0±5.0\n                                               auto_debugging           41.7±15.6      29.2±5.9       0.8\n                                                  cause_and_effect          86.7±7.5       80.0±14.2\n      ½(¿) 0.7                                   common_concept         24.9±0.0       2.8±0.6                                                             diff                       8.0±7.1        100.0±0.0\n                                                 informal_to_formal       62.0±3.3       61.9±2.9\n       0.6                                                   letters_list                100.0±0.0     100.0±0.0\n                                                   negation                  82.0±2.9       77.7±2.6\n          0        10        20                object_counting           45.3±10.3      40.3±0.5\n                      ¿                         odd_one_out             20.0±3.3       68.7±2.5\n                                                   orthography_starts_with   51.0±6.1       71.0±0.0\n Figure 11: The corresponding performance                                            rhymes                  100.0±0.0     61.0±2.8\n profile for results shown in Tab. 4.             second_word_letter       24.3±6.0       96.7±2.4\n                                                     sentence_similarity       10.3±14.6      37.3±0.9\n                                       sum                     100.0±0.0     100.0±0.0\n                                           synonyms                40.3±1.7       44.7±4.1\n                                             taxonomy_animal         91.7±2.1       92.3±0.5\n                                                 word_sorting             62.7±0.5       60.3±3.1\n                                              word_unscrambling       53.0±0.0       58.3±1.9\n\n\n\n\n\nPrompt Embedding  Here we analyze how different embeddings affect the optimization of ZOPO.\nWe first generate a fixed set of prompts of size 500 from Vicuna-13B as those in Tab. 1. For the same\nset of prompts, we consider four different embeddings here: (a) the Last Token embedding from\nVicuna-13B (b) the OpenAI embedding obtained through its embedding model “text-embedding-ada-\n002\" API. (OpenAI, 2024b), (c) the SBERT embedding obtained through the sentence transformer\n(“all-mpnet-base-v2” from HuggingFace), and (d) the Random embedding obtained by randomly\nprojecting the Vicuna embedding into the same dimension. The dimensions of the four embeddings\n(from (a) to (d)) are 1536, 756, and 5120 respectively. We compare the optimization performance of\nthe four embeddings using ZOPO and the results are shown in Tab. 5. We observe although last token\nembedding is generally better, there are certain tasks that OpenAI and SBERT embeddings perform\nequally well or better, which indicates the optimal choice of embedding can be task-dependent.\nIntuitively, random embedding is not representative. Its lesser performance shown in Tab. 5 again\nconfirms our Insight II in Sec. 3.2, which says the choice of embedding/input domain is important in\nprompt optimization.\n\n\n                                       22\n\nTable 5: Average test accuracy with standard error (3 runs) for the best prompt found by ZOPO with\nfour different embeddings on 20 instruction induction tasks.\n\n  Tasks                    Last Token (5120)  OpenAI (1536)  SBERT (756)  Random (5120)\n\n  antonyms                     85.2±3.2           76.7±0.4        78.3±4.5        79.3±3.4\n  auto_categorization            32.7±1.9           31.0±2.9        29.7±2.9        32.3±1.7\n  auto_debugging               41.7±15.6          29.2±5.9        41.7±15.6        37.5±17.7\n  cause_and_effect              94.7±3.7           82.7±6.8        86.7±7.5        68.0±8.6\n  common_concept             23.5±3.4          24.4±1.5        24.9±0.0         22.4±1.8\n  diff                          100.0±0.0          94.7±3.1         8.0±7.1         15.7±7.4\n  informal_to_formal            61.3±2.7          59.4±2.4        62.0±3.3         58.5±3.7\n   letters_list                    100.0±0.0         100.0±0.0       100.0±0.0       100.0±0.0\n  negation                      86.3±0.5           82.3±1.9        82.0±2.9        84.0±2.2\n  object_counting               52.3±6.6           51.7±6.1        45.3±10.3        51.7±6.2\n  odd_one_out                  32.0±11.3          24.0±8.6        20.0±3.3        20.0±12.3\n  orthography_starts_with       56.5±12.6          56.0±4.3        51.0±6.1        46.7±4.7\n  rhymes                      100.0±0.0          68.7±21.5       100.0±0.0        96.3±2.4\n  second_word_letter            25.7±4.7           24.3±5.2        24.3±6.0        24.3±4.5\n  sentence_similarity             7.6±9.3           10.3±14.6        10.3±14.6         6.3±6.4\n  sum                         100.0±0.0         100.0±0.0       100.0±0.0       100.0±0.0\n  synonyms                     43.3±0.9           40.0±0.0        40.3±1.7        42.3±3.1\n  taxonomy_animal             90.0±7.1           91.7±2.6        91.7±2.1         89.3±6.2\n  word_sorting                  60.0±4.2           63.0±1.4        62.7±0.5        59.7±3.8\n  word_unscrambling            59.3±2.8           56.3±1.7        53.0±0.0        47.3±4.2\n  # best-performing tasks         15              5             8             2\n\n\n\n\n\n                                       23\n\nC.4  Study of NTK-GP and Uncertainty-Informed Local Exploration\n\nTo validate the effectiveness of the components, namely NTK-GP (in Sec. 4.2) and uncertainty-\ninformed local exploration (in Sec. 4.3) of ZOPO, we perform controlled experiments to replace\nthese components. Specifically, we (a) replace the NTK component with Matérn kernel (as in the\nrecent ZOO method ZoRD), and (b) remove the uncertainty-informed local exploration feature. We\nevaluate the two settings on 20 instruction induction tasks. The result shown in Table 6 illustrates these\ntwo settings are both significantly worse than the original ZOPO, which validates the effectiveness\nof NTK-GP and uncertainty-informed local exploration.\n\n\nTable 6: Ablation study of the design components in ZOPO showing the average test accuracy\nreported with standard error (3 runs) on 20 instruction induction tasks.\n\n   Tasks            ZOPO   ZOPO w/o NTK  ZOPO w/o Local Exploration\n\n   antonyms                 85.2±3.2    79.7±9.0           78.7±3.1\n   auto_categorization        32.7±1.9    34.7±3.7           28.3±4.9\n   auto_debugging           41.7±15.6   29.2±5.9           25.0±0.0\n   cause_and_effect          94.7±3.7    93.3±1.9           85.3±6.8\n  common_concept         23.5±3.4    9.2±4.1             22.0±5.6\n    diff                      100.0±0.0   13.7±6.1           13.7±6.1\n   informal_to_formal        61.3±2.7    63.4±0.0           63.4±0.0\n    letters_list                100.0±0.0   100.0±0.0          100.0±0.0\n   negation                  86.3±0.5    85.7±0.5           84.7±3.3\n   object_counting           52.3±6.6    39.0±7.1           51.7±6.2\n   odd_one_out              32.0±11.3   14.7±5.0           32.0±8.6\n   orthography_starts_with   56.5±12.6   49.3±8.2           46.3±9.7\n   rhymes                   100.0±0.0   90.7±0.5           93.3±6.6\n   second_word_letter        25.7±4.7    25.7±6.8           19.7±6.8\n   sentence_similarity        7.6±9.3     0.0±0.0             0.0±0.0\n  sum                      100.0±0.0   93.7±9.0           100.0±0.0\n  synonyms                 43.3±0.9    38.3±0.9           39.7±2.5\n   taxonomy_animal         90.0±7.1    74.7±15.1          91.3±4.1\n   word_sorting              60.0±4.2    29.3±12.7          56.3±0.9\n   word_unscrambling       59.3±2.8    47.3±0.9           50.0±4.2\n  # best-performing tasks   17        4                5\n   performance profile ρ(5)   1.0         0.35                0.5\n\n\n\n\n\n                                       24\n\nC.5  Study of ZOPO with More Prompt Candidates\n\nIntuitively, generating more prompt candidates offers a closer approximation to the true function\nlandscape. As our optimization method ZOPO is operated under a given set of prompt candidates,\nwe here conduct an ablation study to examine the impact of a larger size of the generated prompt\ncandidates (i.e., |V|) on the optimization performance. For ZOPO, we use random soft prompts to\nfeed Vicuna-13B and generate prompts until V = 500 or V = 2000. We compare the optimization\nresults of ZOPO using the two different sizes of prompts, and the results are shown in Table 7.\nWe also follow the APE generation template to prompt ChatGPT to generate different sizes of\nprompt candidates and use SBERT to produce their embeddings. For ChatGPT-generated prompts\nin ZOPOGPT, we also consider two settings, V = 500 or V = 1000 (due to budget constraint). The\ncorresponding result is shown in Table 8. We observe from the two tables that a larger set of prompt\ncandidates may not necessarily lead to strictly better performance, and generating a relatively small\nset of strong prompt candidates (e.g., of size 500) is already good enough when we aim to find the\noptimal prompt.\n\n\nTable 7: Ablation study of different sizes of   Table 8: Ablation study of different sizes of\nprompt candidates in ZOPO.                  prompt candidates in ZOPOGPT.\n  Tasks                     |V| = 500   |V| = 2000      Tasks                     |V| = 500   |V| = 1000\n\n  antonyms                 85.2±3.2    86.3±0.9         antonyms                 84.0±1.4    80.3±1.2\n  auto_categorization        32.7±1.9    37.3±1.2          auto_categorization        27.0±5.0    28.3±2.4\n  auto_debugging           41.7±15.6   33.3±11.8        auto_debugging           29.2±5.9    37.5±10.2\n  cause_and_effect          94.7±3.7    94.7±1.9          cause_and_effect          80.0±14.2   78.7±3.8\n  common_concept         23.5±3.4    17.0±6.1        common_concept          2.8±0.6      11.7±6.8\n   diff                      100.0±0.0   100.0±0.0          diff                      100.0±0.0   100.0±0.0\n  informal_to_formal        61.3±2.7    56.6±4.1          informal_to_formal        61.9±2.9    57.2±8.9\n   letters_list                100.0±0.0   100.0±0.0           letters_list                100.0±0.0   99.3±0.5\n  negation                  86.3±0.5    86.3±0.5          negation                  77.7±2.6    75.0±1.6\n  object_counting           52.3±6.6    53.0±6.5          object_counting           40.3±0.5    41.3±1.2\n  odd_one_out              32.0±11.3   20.7±6.6         odd_one_out              68.7±2.5    72.0±0.0\n  orthography_starts_with   56.5±12.6   46.0±6.9          orthography_starts_with   71.0±0.0    71.3±0.9\n  rhymes                   100.0±0.0   100.0±0.0        rhymes                   61.0±2.8    100.0±0.0\n  second_word_letter        25.7±4.7    35.3±27.5         second_word_letter        96.7±2.4    99.7±0.5\n  sentence_similarity        7.6±9.3      24.7±6.1          sentence_similarity        37.3±0.9    0.0±0.0\n  sum                      100.0±0.0   100.0±0.0       sum                      100.0±0.0   100.0±0.0\n  synonyms                 43.3±0.9    40.0±3.3        synonyms                 44.7±4.1    45.3±1.7\n  taxonomy_animal         90.0±7.1    91.3±7.6         taxonomy_animal         92.3±0.5    89.3±1.9\n  word_sorting              60.0±4.2    59.0±6.4         word_sorting              60.3±3.1    54.3±7.0\n  word_unscrambling       59.3±2.8    54.7±3.3         word_unscrambling       58.3±1.9    60.3±2.5\n  # best-performing tasks   14         12             # best-performing tasks   10         12\n  performance profile ρ(5)   0.9          0.8              performance profile ρ(5)   0.85        0.9\n\n\n\n\n\n                                       25\n\nC.6  Best Prompts Found\n\nWe list the best prompts discovered by our method ZOPO for every instruction induction task here in\nTable 9, which corresponds to the results in Table 3.\n\n\n\nTable 9: The best prompts discovered by our method ZOPO for every instruction induction task,\nwhere “*” indicates the best prompt is found by ZOPOGPT for that task.\n\n  Task                 Best prompt\n  active_to_passive     The prompt was to convert the given sentence into passive voice.\n  antonyms            The prompt was to rewrite the given words into their opposite meaning. So, “humor-\n                            less\" becomes “humorous\", “depressing\" becomes “cheerful\", “unwrap\" becomes\n                        “wrap\", “consumptive\" becomes “generative\", “uncoil\" becomes “coil\".\n  auto_categorization    The prompt was to input the given names and output the corresponding apparel. For\n                       example, the input “Nature Nanotechnology, Annual Review of Biochemistry, and\n                     The Lancet Neurology\" would output as “top journals\".\n  auto_debugging      The prompt was to write a program that would take the given input and output\n                          the expected output. For example, the first input was a simple calculation, and the\n                         expected output was “2550\". The second input was a class definition with a method,\n                      and the expected output was “5\".\n  cause_and_effect      The prompt was to identify the sentence that is the cause and the sentence that is the\n                             effect in each pair of sentences. The input sentences are given, and the output is the\n                        cause sentence.\n  common_concept     The prompt was to create a series of pairs of inputs and outputs, where the outputs are\n                            related to the inputs in some way. For example, the inputs “guitars\" and “pendulums\"\n                          are related to the output of “involve oscillations.\n   diff                The prompt was to subtract the second number from the first number. For example,\n                           the first input would be 41 and the second input would be 13, so the output would be\n                     28 (41 - 13). The same process would be applied for the other inputs and outputs.\n  first_word_letter      The prompt was to create a program that takes a single input (a word representing\n                        a legal concept or term) and outputs a corresponding letter of the alphabet that\n                          represents that concept or term.\n                       For example, if the input is “year\", the program should output “y\".\n  informal_to_formal*   The prompt was to rephrase each input sentence using a more formal or polite\n                         language.\n  larger_animal        The prompt was to create a program that takes two input animals and outputs the\n                        animal that is bigger. The program uses the “>=\" operator to compare the size of the\n                                   first animal to the size of the second animal. If the first animal is bigger, the program\n                          outputs the first animal.\n   letters_list           The prompt was to create a program that takes a single word input (e.g. “year\") and\n                          outputs a concatenated string of letters and spaces that approximates the pronuncia-\n                            tion of that word (e.g. “y e a r\").\n  negation            The prompt was to flip the truth value of the input statements. For example, if the\n                          input statement is “Cany Ash and Robert Sakula are both Architects,\" the output\n                        should be “Cany Ash and Robert Sakula are not Architects.\n  num_to_verbal       The prompt was to write a program that takes a number as input and outputs the\n                     number in words, using the appropriate number formatting. The examples provided\n                            in the input show the expected output for each number.\n  object_counting      The prompts were to provide the output of a given input, where the input is a list of\n                        items and the output is a number representing the total count of those items. The\n                       examples given in the prompt show how the prompts should be used to generate the\n                          desired output.\n  odd_one_out*        The prompt was to identify the word that is most different from the others in the\n                         group.\n\n\n\n\n\n                                       26\n\northography_starts_with*   The prompt was to identify the first word that begins with a specific letter in each\n                            sentence.\nperiodic_elements        The prompts were to write a program that takes an input value and outputs the\n                          corresponding element name based on that value.\n                         For example, if the input is 24, the program would output “chromium.\nrhymes                The prompts were to create a program that takes in a word as input and outputs a\n                             related word based on a specific set of rules. The rules are as follows: If the input\n                       word starts with “tri\", the output should be “slip\".\nsecond_word_letter*      The prompt was to “Identify and return the second letter of the input word\".\nsentence_similarity*       The prompt was to create two different sentences that have similar meanings but are\n                           not identical. The output of each input-output pair indicates how closely the two\n                           sentences match in terms of meaning.\n\n                           Explanation of outputs:\n                                   - 5 - perfectly: The two sentences are very similar in meaning and can be considered\n                            as equivalent.\n                                  - 3 - probably: The two sentences have some similarities in meaning but there are\n                             also some differences, making it less certain that they are equivalent.\n                                  - 2 - possibly: The two sentences have some similarities but also significant\n                              differences, making it unlikely that they are equivalent.\n                                   - 1 - probably not: The two sentences have very different meanings and are unlikely\n                              to be considered as equivalent.\n                                  - 0 - definitely not: The two sentences have no similarity in meaning and cannot be\n                           considered as equivalent.\nsentiment               The prompt was to classify the given reviews as positive or negative based on the\n                          given input and output. The output is positive when the review is positive, and\n                            negative when the review is negative.\nsingular_to_plural        The prompt was to convert the input words to their plural form by adding “s\" to the\n                        end of the word. This was done by using the “replace\" function in Excel, which\n                           allows you to replace a specific text string with another text string.\nsum                    The prompt was to write a program that takes two numbers as input and outputs their\n                     sum as the result. The program uses the ‘scanf‘ function to read the input numbers\n                        from the user, and the ‘printf‘ function to display the result.\nsynonyms*              The prompt was to create a list of words that are synonyms or closely related to the\n                           given word.\ntaxonomy_animal*       The prompt was to select all the animals in the input and output them in the order\n                           they appear.\ntranslation_en-de         The prompts were to input various words and have the model generate the corre-\n                         sponding output in German. It appears that the model was successful in generating\n                            the desired output for each of the input words provided. If there are any additional\n                         prompts or clarification needed, please let me know.\ntranslation_en-es         The prompts were to translate a set of words from Spanish to English using the\n                          provided translation table.\ntranslation_en-fr          The prompt was to input a word and then output the corresponding word in French.\n                                     It appears that the input and output words are being matched correctly, with the\n                            exception of the word “initiative,\" which should have the output “initiative\" in French,\n                           not “enterprise.\nword_sorting*           The prompt was to alphabetize the input list in ascending order and provide the\n                              resulting output as a list.\nword_unscrambling       The prompt was to create a program that takes an input word and outputs the\n                           corresponding word with the letters rearranged in order. For example, given the input\n                            “eccpat\", the program should output “accept\".\n\n\n\n\n\n                                     27",
"headers": [
"arXiv:2403.02993v1  [cs.AI]  5 Mar 2024",
"Localized Zeroth-Order Prompt Optimization",
"1.0",
"0.5",
"0.0",
"0.75",
"0.50",
"½",
"(",
"¿",
")",
"0.25",
"0",
"10",
"20",
"Abstract",
"1",
"Introduction",
"2",
"Problem Setup",
"3",
"Empirical Study on Prompt Optimization",
"4",
"The",
"ZOPO",
"Algorithm",
"5",
"Experiments",
"6",
"Related Work",
"7",
"Conclusion",
"References",
"Appendix A",
"Proofs",
"Appendix B",
"Details of Experimental Settings",
"Appendix C",
"Additional Results",
"The efficacy of large language models (LLMs) in understanding and generating",
"to harness the power of black-box LLMs. Existing methodologies usually prior-",
"itize a global optimization for finding the global optimum, which however will",
"perform poorly in certain tasks. This thus motivates us to re-think the necessity",
"tion (",
"Insight I",
"). The choice of the input domain, covering both the generation",
"and the representation of prompts, affects the identification of well-performing",
"optimization for an efficient search of well-performing local optima in prompt",
"the optimization performance and the query efficiency, which we demonstrate",
"in practice. To mitigate such human efforts and achieve better performance in optimizing crafted",
"2023), and evolutionary algorithms (Guo et al., 2024) have been proposed to generate and select",
"Figure 1: The performance profile for different methods on instruction induction tasks, where",
"for the global optima and consequently making it query-inefficient in practice. Meanwhile, these",
"strategies typically implement their prompt optimization across various input domains, resulting",
"Inspired by these insights, we novelly propose the",
"Localized",
"eroth",
"-",
"rder",
"rompt",
"ptimization",
"generation and embedding models to transform these generated prompts into their corresponding",
"method designed to improve the gradient estimation in our derived NTK-GP framework, thereby",
"To the best of our knowledge, we are the first to conduct a thorough empirical study in prompt",
"Drawing on the insights gained from our empirical study, we introduce the",
"algorithm,",
"Given an NLP task that is characterized by a data distribution",
"and a black-box LLM",
", e.g.,",
"where",
"is applied to measure the alignment between the LLM output",
"and",
"the groundtruth",
", and",
"is the validation set sampled from",
". Note that the performance of the",
"than the others (Zhou et al., 2023; Guo et al., 2024), which is usually contributed to their usage of",
"This is likely because substantial portions of the budget are applied in these methods to explore",
"(differentiated by colors) for 300 randomly sampled prompt candidates on various tasks, whose",
"stochastic neighbor embedding (t-SNE) (see more details in our Appx. C.1.1). The results are in",
"Fig. 2 which shows that the global optimum (i.e., the points achieving an accuracy of",
"is consistently rare for a range of prompt optimization tasks, making it extremely challenging to",
"achieve this global optimum in practice. In contrast, prompt optimization often features a number",
"Importantly, these local optima commonly enjoy impressive performance, suggesting that local",
"optima shall be more worthwhile to obtain in prompt optimization, especially for the scenarios of",
"Contrasting with the rarity of global optimum, local optima are usually prevalent and well-",
"Besides, existing works (Chen et al., 2023; Lin et al., 2023; Guo et al., 2024) typically apply their",
"(a black-box model) generally exhibit better performance than those produced by Vicuna-13B (a",
"prompt candidates (e.g., the ones of accuracy higher than 0.5 across all the three plots in Fig. 3).",
"The choice of the input domain, covering both the generation and the representation of prompt",
"Given the insights established in our Sec. 3, we then propose our",
") algorithm (Algo. 1) for a better-performing as well as more query-efficient",
"develop a one-to-one mapping where one prompt generally has a unique hidden representation",
",",
"Practical Implementations.",
"Before the start of the optimization on",
", we usually generate",
"sampled soft prompts",
"and a few demonstrations",
"into a white-box LLM",
".",
"(b)",
"Sampling the output distribution of a black-box LLM",
"given a generation template filled with",
"or the soft prompt",
"Chen et al. (2023) when generating",
". Here",
"then represents a mapping",
"query-inefficient as many additional queries are required for gradient estimation in every gradient",
"Specifically, according to (Shu et al., 2023a), given a well-specified kernel function",
"such",
"that the function",
"is sampled from a Gaussian process",
"or alternatively",
"and the observed value",
"of function",
"follows",
"is a",
"-dimensional matrix. As a result,",
"can be applied to estimate the gradient of the",
"more specifically transformers. It naturally inspires us to apply the Neural Tangent Kernel (NTK)",
"kernel function",
". This is because it has been widely proven that NTK is capable of well",
"characterizing the predictions of neural networks (Arora et al., 2019; Lee et al., 2019; Shu et al.,",
"the simple kernel (i.e., Matérn kernel) applied in",
"(Shu et al., 2023a). Specifically, given a",
"neural network",
"parameterized by",
", we employ the following empirical NTK as the",
"Based on this derived NTK-GP, we finally apply standard first-order optimization (e.g., stochastic",
"is the projection function that projects the updated",
"Following the modeling principle of local optimization, only the",
"Proposition 1.",
"Assume",
"for any",
". Let",
"for given input",
", the following holds with a",
"Here,",
"denotes a set of historical input queries that are effectively relevant for the gradient",
"estimation at",
"can be regarded as a measure of effective relevance. Prop. 1 shows that",
". That is, both the effective relevance",
"and the number of relevant queries",
"can be small",
"in derived NTK-GP according to Prop. 1. Specifically, we propose the local exploration condition",
"Table 1: Average test accuracy with standard error (3 runs) for the best prompt found by different",
"is the condition that incorporates uncertainties and",
"are the thresholds. If this condition is met (i.e.,",
"), we will query the neighbors of",
"in",
"If we define the set of the",
"nearest neighbors of",
"as",
", we propose to query each",
"APE Zhou et al. (2023), InstructZero Chen et al. (2023), INSTINCT Lin et al. (2023), and Evo-",
"use the performance profile Dolan & Moré (2002), defined in Appx. B.1, as the overall evaluation",
"metric that measures the frequency (i.e.,",
") of a method within some distance (i.e.,",
") from the",
"by assessing LLM’s zero-shot in-context learning ability in previous works (Zhou et al., 2023;",
"given any prompt generation strategy, here we follow the same setting of prompt generation from",
"INSTINCT and InstructZero, only for",
"fair comparison",
". We also adopt the last token embedding",
"from Vicuna-13B as the prompt representation (same as INSTINCT). Here Vicuna-13B is used to",
"perform a grid search over soft prompt hyperparameters on the validation set. More experimental",
"Figure 5: Comparison of the query efficiency between our",
"and other existing baselines on",
"instruction induction tasks. The first row shows the test accuracy and the second row shows the",
"has better query efficiency.",
"To justify that our local optimization method is more",
"query-",
"Connecting ChatGPT with",
"With our proposed domain transformation, we empirically",
"demonstrate that",
"is capable of performing numerical optimization on ChatGPT-generated",
"task-specific prompts (i.e.,",
") from ChatGPT, and use a popular embedding model SBERT to",
"against other baselines is shown in Tab. 1, with the corresponding performance profile shown in",
"Fig. 9 in App. C.2. Fig. 9 demonstrates that",
"significantly outperforms other baselines,",
"(see the accuracy gap",
"in Tab. 1), which we attribute",
"to the high-quality of prompt candidates generated by ChatGPT. This is also consistent with our",
"discussion on the input domain in Sec. 3.2. Here we could not draw a direct comparison between",
"generation process in",
"and cannot be applied to",
". However, using either",
"or",
"is sufficient to outperform baseline methods, which also provides the flexibility of",
"prompt optimization in practice. Future research may consider employing better embeddings to",
"been shown effective in improving LLMs’ zero-shot multi-step reasoning performance. We show",
"Table 2: The performance of the best zero-shot CoT prompt found by different methods on three",
"Let’s utilize the substitution method to find a solution,",
"Verifying the Essence of Input Domain.",
"To fairly validate the importance of input domain on",
"prompt generation, we compare the optimization performances with different prompts generated",
"by Vicuna-13B and ChatGPT respectively, using the same embedding model SBERT (i.e.,",
").",
"The result is shown in Table. 4 in Appx. C.3, with the performance profile in Fig. 11 suggesting",
"Besides, different embeddings (i.e.,",
") of the same prompt candidates can potentially affect the",
"function landscape as shown in Fig. 4. Thus, we need to study the performance of",
"using",
"different embedding representations given the same set of prompts. We consider four different",
"embeddings here: the last token embedding from Vicuna-13B, the OpenAI embedding provided",
"through an API (OpenAI, 2024b), the SBERT embedding, and a randomly projected embedding",
"by comparing two variations of the original",
"algorithm: (a) one with replacing the NTK",
"component with Matérn kernel (as in",
"), and (b) another with the uncertainty-informed local",
"exploration feature removed. The two variations are compared against the original",
"on the",
"Additional Results.",
"We also perform an ablation study to examine the impact of a larger size of",
"relatively small set of strong prompt candidates (e.g.,",
") is sufficient (compared with size",
"Soft Prompt Tuning.",
"To control LLMs to perform specific downstream tasks (e.g., reasoning),",
"soft prompt tuning (Li & Liang, 2021) is usually used as a lightweight method to fine-tune the",
"to optimize prompts through iterative mutation and crossover. However, these methods typically",
"et al. (2023) leverages the induction ability from other white-box LLM",
"for the generation of",
"task-specific prompts, that is",
"conditioned on a continuous soft prompt",
"and in-context demonstrations",
". After that, the optimization on",
"can be transformed into",
"an optimization on the soft prompt",
", where BO algorithms are employed for a global black-box",
"optimization. INSTINCT (Lin et al., 2023) further employs neural bandit algorithms and the last",
"API LLMs), we propose a localized ZOO method that is in contrast to the global optimization",
"In this work, we first provide a thorough empirical study to understand the characteristics of the",
"instruction induction tasks and 3 reasoning tasks demonstrate the efficacy of",
", and ablation",
"choices of input domains in prompt optimization. This may inspire future research in optimizing",
"Chen, L., Chen, J., Goldstein, T., Huang, H., and Zhou, T. InstructZero: Efficient instruction",
"Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E., and Hu, Z.",
"Dolan, E. D. and Moré, J. J. Benchmarking optimization software with performance profiles.",
"Flaxman, A., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting:",
"Jacot, A., Hongler, C., and Gabriel, F. Neural Tangent Kernel: Convergence and generalization in",
"Kratsios, A. and Papon, L. Universal approximation theorems for differentiable geometric deep",
"INSTruction optimization usIng Neural bandits Coupled with Transformers. In",
"NeurIPS 2023",
"Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A",
"OpenAI.",
"Documentation",
"of",
"OpenAI’s",
"text",
"embeddings.",
"Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,",
"Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,",
"S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,",
"Reynolds, L. and McDonell, K. Prompt programming for large language models: Beyond the",
"few-shot paradigm. In",
"Extended Abstracts of the 2021 CHI Conference on Human Factors in",
"Shu, Y., Cai, S., Dai, Z., Ooi, B. C., and Low, B. K. H. NASI: Label- and data-agnostic neural",
"Shu, Y., Lin, X., Dai, Z., and Low, B. K. H. Federated zeroth-order optimization using trajectory-",
"Sun, T., He, Z., Qian, H., Huang, X., and Qiu, X. Bbtv2: Pure black-box optimization can be",
"Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,",
"Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T.,",
"Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan,",
"J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A.,",
"Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models.",
"Lemma A.1",
"(Thm. 1 in (Shu et al., 2023a))",
"Let",
". For",
"For any",
", define",
"with",
"averaged eigenvalues,",
"is based on",
"is from the definition of",
"(i.e., the Gershgorin theorem) and the assumption that",
"is",
"is the set of all tasks,",
"is the accuracy of method",
"on task",
"is the best performance achieved by any method in",
". Specifically,",
"For all experiments using",
"in this work, we set the learning rate to 0.01, the uncertainty",
"thresholds",
"to 0.1 and 5 respectively, and the number",
"of nearest neighbors to query in local",
"(2023); Lin et al. (2023) is adopted in this work, where, for each task, we optimize the generated",
"test set",
". Specifically, 5 examples are sampled from the training set as the demonstrations (i.e.,",
"the",
"validation set",
"to evaluate the objective function value as in Equation (1). The total query",
"LLM (i.e.,",
") to generate the task-specific prompts by feeding",
"with randomly sampled soft",
"much smaller intrinsic dimension (e.g., 100). This intrinsic dimension may empirically affect the",
"length {3, 5, 10} on the validation set and report the accuracy on a held-out test set using the best",
"Prompt",
"Generation",
"Template",
"(Soft",
"We directly use the reported results of APE, IntructZero, and INSTINCT from Lin et al. (2023)",
"for comparison, and we report the results of EvoPrompt with our re-implementation. For a fair",
"comparison, we also use Vicuna-13B for generating the initial prompt population (of size 20) for",
"its new prompts. Using GPT-3.5 turbo to generate new prompts will help improve EvoPrompt’s",
"query budget scales, we set the maximum query budget to 200, and report the test accuracy of the",
"For our experiment on",
"in the main text, we apply",
"on ChatGPT (i.e., GPT-3.5",
"turbo) generated prompts. We follow the generation template from APE Zhou et al. (2023), as",
"a high temperature (e.g., 0.95). To maintain the same size of prompt candidates as in the previous",
"task. To harness the representation power of existing embedding models, we adopt the sentence",
"To improve the zero-shot chain-of-thought prompt performance on arithmetic reasoning tasks, we",
"make use of the LLM’s induction ability and enable LLMs to generate different chain-of-thought",
"See Tab. 2 for details on the performance of",
"against other baselines on the three reasoning",
"Write your new prompt that is different from",
"findings into the design of",
". In the main text, we have demonstrated the results on three in-",
"Here we use more datasets to validate our findings. Due to the large size of instruction induction",
"To validate our local optimization design, we study the local optima in the function landscape, by",
"Prompt Generation",
"To study the prompt quality of different prompt generation methods, we",
"compare the prompts generated from Vicuna-13B and those generated from ChatGPT (i.e., GPT",
"3.5 turbo). For Vicuna-13B, we use the randomly sampled soft prompts with a fixed intrinsic",
"dimension of 200 and a number token length of 10. For ChatGPT, we randomly sample prompts",
"from the ChatGPT’s response by using the APE generation template filled with random example",
"generated by the two methods on four more (due to budget constraints) tasks here in Fig. 7. It",
"Prompt Embedding",
"The complexity of modeling the target function depends on its function",
"we show the accuracy landscape of different tasks, where we reduce the dimension of the prompt",
"embedding (we use the last token embedding of Vicuna-13B here) to 2 by using t-SNE. The loss",
"landscape is visualized in the surface plot shown in Fig. 8. We observe that different optimization",
"many good local optima. For other challenging tasks with complex function landscapes, the good",
"local optima are less, but our methods can still achieve superior performance. This validates our",
"In Section 5.1 of the main text, we compared our methods with other baselines on 20 challenging",
"instruction induction tasks. Here we provide the full results on 30 instruction induction tasks in",
"Table 3: Average test accuracy with standard error (3 runs) for the best prompt found by different",
"Figure 9: The performance profile of",
"compared against other baseline methods on 20",
"We also provide additional results on other instruction induction tasks to compare",
"against",
"Figure 10: Comparison of the query efficiency between",
"GPT in terms of the optimization performance by using",
", we adopt the same embedding",
"representations here, that is we use the SBERT embedding model for both prompts generated by",
"for both methods to 500. The result of the comparison on 20 instruction induction tasks is shown",
"in Table. 4, where the corresponding performance profile shown in Fig. 11 suggests that applying",
"mance of",
"with different generated prompts",
"Figure 11: The corresponding performance",
"set of prompts, we consider four different embeddings here: (a) the",
"Last Token",
"embedding from",
"equally well or better, which indicates the optimal choice of embedding can be",
"task-dependent",
"Intuitively, random embedding is not representative. Its lesser performance shown in Tab. 5 again",
"To validate the effectiveness of the components, namely NTK-GP (in Sec. 4.2) and uncertainty-",
"informed local exploration (in Sec. 4.3) of",
", we perform controlled experiments to replace",
"these components. Specifically, we (a) replace the NTK component with Matérn kernel (as in the",
"Table 6: Ablation study of the design components in",
"showing the average test accuracy",
"Intuitively, generating more prompt candidates offers a closer approximation to the true function",
"we here conduct an ablation study to examine the impact of a larger size of the generated prompt",
"results of",
"using the two different sizes of prompts, and the results are shown in Table 7.",
"We also follow the APE generation template to prompt ChatGPT to generate different sizes of",
"Table 7: Ablation study of different sizes of",
"Table 8: Ablation study of different sizes of",
"Table 9: The best prompts discovered by our method",
"for every instruction induction task,",
"Large language models (LLMs) have demonstrated remarkable capabilities for understanding and",
"in diverse performance outcomes in practice. These results consequently inspire us to re-think the",
"questions about the necessity of finding a global optimum and the essence of the input domain for",
"generated from a LLM",
"and subsequently transform it into a continuous hidden representation",
"reasoning tasks with improved chain-of-thought prompts Zhou et al. (2023); Lin et al. (2023). We",
"provide the corresponding sentence embeddings (i.e.,",
") for",
". Then we apply",
"to perform",
"require a large number of iterations and queries to perform well. In this regard, InstructZero Chen",
"prompt found using the validation set. We also adopt this technique in",
"for fair comparison.",
"tasks, shown in Fig. 6. The heatmap color represents the validation accuracy of the corresponding",
"(“all-mpnet-base-v2” from HuggingFace), and (d) the",
"Random",
"embedding obtained by randomly",
"compared with all baseline methods. For more results on all 30 tasks, refer to Tab. 3 in Appx. C.2,",
"However, when the gradient information of the model is inaccessible, gradient-free prompt tuning",
"Shu, Y., Dai, Z., Sng, W., Verma, A., Jaillet, P., and Low, B. K. H. Zeroth-order optimization with",
"feed Vicuna-13B and generate prompts until",
". We compare the optimization",
". The representations",
"can be produced by an NLP model",
". Specifically, if we consider",
"comes from Lemma A.2,",
"is based on the assumption of",
"and the defi-",
"is the initialized parameter of neural network",
". By incorporating",
"into",
", we realize",
"the gradient estimation error of",
"at a specific input",
"is bounded by the norm of covariance",
"accuracy gap of",
". We",
"bold",
"the highest accuracy when comparing",
"with baselines,",
"neural networks of any depth evolve as linear models under gradient descent. In",
"Proc. NeurIPS",
"the rest of instruction induction tasks; and we use the accuracy metric for the arithmetic reasoning",
"soft prompt as 10 and the number of soft tokens as 5, without using the validation set to perform a",
"text, namely the prompt",
", which will then be applied to the black-box LLM",
"along with a test",
"These results thus inspire us to ask:",
"How essential is the input domain for finding well-performing",
"Chen et al., 2023; Lin et al., 2023). Although our",
"is a general prompt optimization method",
"As the number of datasets is tremendous, we use the performance profile Dolan & Moré (2002) as",
"queries for random initialization of our optimization method, which could serve as the only global",
"set of strong prompt candidates (e.g., of size 500) is already good enough when we aim to find the",
"of finding a global optimum in prompt optimization. To answer this, we conduct",
"a thorough empirical study on prompt optimization and draw two major insights.",
"the Neural Tangent Kernel (NTK) (Jacot et al., 2018) to handle the complex and high-dimensional",
"local exploration technique to refine the gradient estimation in our derived NTK Gaussian process,",
"Algo. 1). Two practical methods are considered here for prompt generation:",
"(a)",
"Feeding randomly",
"our transformed input domain",
"in Sec. 4.1. Unfortunately, existing ZOO algorithms are typically",
"methods for 20 instruction induction tasks.",
"indicates the accuracy gap between",
"and the",
"landscape. As our optimization method",
"is operated under a given set of prompt candidates,",
"prompt candidates and use SBERT to produce their embeddings. For ChatGPT-generated prompts",
"optimization",
", dedicating substantial portions of the query budget to explore the entire search space",
"To answer these questions, we provide a thorough empirical study on prompt optimization. Firstly,",
"prompt optimization tasks (Sec. 4.2). Lastly, we present an uncertainty-informed local exploration",
"In this section, we evaluate the performance of",
"against several baseline methods, including",
"the given information.” on GSM8K compared to other baselines, improving the performance from",
"comes from the fact the maximum eigenvalue of a matrix is always larger or equal to its",
"The same data partition and evaluation process as in previous works Zhou et al. (2023); Chen et al.",
"with a larger mean. The result shows that ChatGPT-generated prompts are generally better, further",
"candidates (i.e.,",
") on the optimization performance. For",
", we use random soft prompts to",
"of local optima (e.g., the points achieving accuracy higher than 50% in all the three tasks of Fig.2).",
"previous works:",
"Different from the direct optimization over the discrete and complex language",
"gradient descent) with projected gradients for our local prompt optimization. Specifically, in every",
"neighbors of",
"in the query history",
"are used to calculate the gradient",
". As we do not know",
"Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot",
"shown above, to generate task-specific prompts from ChatGPT. To generate various prompts using",
"In Section 3, we have empirically studied the landscape of the target function and incorporated the",
"in low-dimensional problems (Moriconi et al., 2020). However, these methods sometimes perform",
"indicating that they will fail to find the global optimum in these tasks given a limited query budget.",
"models for our prompt optimization. To achieve this, we propose to make use of the prompt",
"2023), i.e., the same discrete prompt",
"may be generated by various continuous soft prompts",
", we",
"RLPrompt: Optimizing discrete text prompts with reinforcement learning. In",
"Proc. EMNLP",
", pp.",
", we also consider two settings,",
"(due to budget constraint). The",
"therefore allows the usage of query-efficient optimization algorithms for",
"(e.g., our Algo. 1).",
"dimensional matrix,",
"-dimensional column vector, and",
"estimation error at different",
"varies if the effective relevance",
"optimization especially considering the sparsity of prompt candidates w.r.t. the continuous domain",
"entire space. With an empirical understanding of the underlying target function (i.e., the black-box",
"the APE method, we need to sample different sets of demonstrations (i.e.,",
") from the training",
"The performance profile of",
"compared against other baseline methods is shown in Fig. 9.",
"Contrasting with the rarity of global optimum, local optima are usually prevalent",
") algorithm for a considerably improved prompt optimization as evidenced by Fig. 1. Moti-",
"most recent",
"algorithm (Shu et al., 2023a) where a localized surrogate model will be applied",
"2023a), we introduce the following Prop. 1 to demonstrate that the error in gradient estimation at a",
"Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback.",
"Vicuna-13B with a fixed intrinsic dimension of 1000 and search the soft token length {3, 5, 10} on",
"which outperforms existing baselines in terms of not only the optimization performance but also",
"the entire search space for the global optimum, which hence leads to the critical question about",
"Instruction induction tasks are commonly used to investigate the prompt optimization performance",
"landscape defined by the embedding domain. To empirically analyze the black-box target function,",
"as opposed to the adoption of the virtual update mechanism described in (Shu et al., 2023a) for our",
"convergence. We notice that",
"achieves lower validation accuracy yet higher test accuracy on",
"methods Sun et al. (2022b,a); Diao et al. (2023) are developed to alleviate human efforts in prompt",
"studies also validate the design principles of",
". Besides, we propose a domain transformation",
"Reimers, N. and Gurevych, I. Sentence-BERT: Sentence embeddings using siamese bert-networks.",
"prompts and",
", which is the same as InstructZero and INSTINCT. In the experiments, we only",
"historical queries that are",
"effectively",
"relevant for the gradient estimation at the specific input",
"set, and, for each",
", we also need to randomly sample from the ChatGPT’s response by setting",
"To fairly compare the effect of prompts generated by Vicuna-13B and Chat-",
"002\" API. (OpenAI, 2024b), (c) the",
"SBERT",
"embedding obtained through the sentence transformer",
"Although human knowledge may subjectively guide prompt designs (Reynolds & McDonell, 2021;",
"prompts, random sampling (Zhou et al., 2023), Bayesian optimization (Chen et al., 2023; Lin et al.,",
"be applied, our transformed input domain leads to a dense numerical space of lower dimension and",
"This insight, in turn, motivates the creation of our uncertainty-informed local exploration approach,",
"significantly outperforms all baseline methods. Particularly, our",
"performs the best in 14 out",
"prompts through resampling without applying specific optimizations. The recent work of Guo et al.",
"OpenAI API) as the black-box model for prompt evaluation and Vicuna-13B-v1.1 as the white-box",
"ability from many NLP embedding models (Chen et al., 2023; Lin et al., 2023) for our prompt opti-",
"(ZOO) method enhanced by a derived Gaussian process for efficient gradient estimation (Shu et al.,",
"We conduct extensive studies to confirm the efficacy of our algorithmic framework and elucidate",
"poorly in certain prompt optimization tasks, e.g.,",
"methods achieve similar performances on tasks like",
"sentiment",
"singular_to_plural",
", as they have",
"two settings are both significantly worse than the original",
", which validates the effectiveness"
],
"tables": [
"|Col1|Vicuna-13B<br>ChatGPT|\n|---|---|",
"|Col1|Vicuna-13B<br>ChatGPT|\n|---|---|",
"|accuracy|y gap of ZO|OPOGPT. We bold the highest accuracy when comparing ZOPO with baselines|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|and use|gray cell|to highlight the highest accuracy when comparing ZOPOGPT with baselines.|to highlight the highest accuracy when comparing ZOPOGPT with baselines.|to highlight the highest accuracy when comparing ZOPOGPT with baselines.|to highlight the highest accuracy when comparing ZOPOGPT with baselines.|to highlight the highest accuracy when comparing ZOPOGPT with baselines.|\n|**Tasks**<br>**APE**<br>**InstructZero**<br>**INSTINCT**<br>**EvoPrompt**<br>**ZOPO**<br>**ZOPO**GPT<br>∆1<br>∆2|**Tasks**<br>**APE**<br>**InstructZero**<br>**INSTINCT**<br>**EvoPrompt**<br>**ZOPO**<br>**ZOPO**GPT<br>∆1<br>∆2|**Tasks**<br>**APE**<br>**InstructZero**<br>**INSTINCT**<br>**EvoPrompt**<br>**ZOPO**<br>**ZOPO**GPT<br>∆1<br>∆2|**Tasks**<br>**APE**<br>**InstructZero**<br>**INSTINCT**<br>**EvoPrompt**<br>**ZOPO**<br>**ZOPO**GPT<br>∆1<br>∆2|**Tasks**<br>**APE**<br>**InstructZero**<br>**INSTINCT**<br>**EvoPrompt**<br>**ZOPO**<br>**ZOPO**GPT<br>∆1<br>∆2|**Tasks**<br>**APE**<br>**InstructZero**<br>**INSTINCT**<br>**EvoPrompt**<br>**ZOPO**<br>**ZOPO**GPT<br>∆1<br>∆2|**Tasks**<br>**APE**<br>**InstructZero**<br>**INSTINCT**<br>**EvoPrompt**<br>**ZOPO**<br>**ZOPO**GPT<br>∆1<br>∆2|\n|antonyms<br>63_._7_±_14_._2<br>82_._7_±_0_._7<br>84_._7_±_0_._3<br>84_._0_±_0_._0<br>auto_categorization<br>25_._0_±_0_._9<br>25_._7_±_1_._2<br>25_._0_±_3_._3<br>31_._0_±_1_._0<br>auto_debugging<br>29_._2_±_3_._4<br>37_._5_±_0_._0<br>29_._2_±_3_._4<br>33_._0_±_7_._2<br>cause_and_effect<br>57_._3_±_8_._9<br>81_._3_±_1_._1<br>58_._7_±_8_._7<br>84_._0_±_13_._9<br>common_concept<br>6_._9_±_2_._1<br>8_._6_±_4_._0<br>21_._3_±_0_._2<br>11_._1_±_6_._9<br>diff<br>67_._3_±_26_._7<br>69_._3_±_22_._2<br>**100.0**_±_0_._0<br>27_._3_±_42_._2<br>informal_to_formal<br>57_._4_±_0_._3<br>53_._1_±_0_._2<br>55_._3_±_0_._0<br>51_._6_±_0_._9<br>letters_list<br>**100.0**_±_0_._0<br>59_._0_±_16_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>negation<br>75_._3_±_1_._1<br>77_._7_±_1_._4<br>81_._7_±_0_._3<br>86_._0_±_0_._0<br>object_counting<br>36_._3_±_1_._9<br>36_._0_±_9_._3<br>34_._0_±_7_._0<br>**55.0**_±_5_._3<br>odd_one_out<br>63_._3_±_1_._4<br>61_._3_±_8_._7<br>**70.0**_±_1_._6<br>10_._0_±_0_._0<br>orthography_starts_with<br>45_._7_±_14_._8<br>50_._7_±_8_._7<br>**66.7**_±_2_._7<br>15_._0_±_3_._4<br>rhymes<br>15_._7_±_6_._4<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>59_._7_±_3_._1<br>second_word_letter<br>**74.7**_±_20_._3<br>43_._3_±_18_._7<br>10_._0_±_4_._1<br>24_._7_±_0_._6<br>sentence_similarity<br>0_._0_±_0_._0<br>0_._0_±_0_._0<br>**14.0**_±_0_._5<br>2_._0_±_1_._0<br>sum<br>67_._3_±_26_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>synonyms<br>36_._0_±_7_._6<br>27_._7_±_9_._3<br>30_._7_±_4_._9<br>40_._3_±_4_._0<br>taxonomy_animal<br>34_._7_±_23_._4<br>71_._7_±_8_._4<br>85_._7_±_6_._0<br>83_._0_±_4_._6<br>word_sorting<br>33_._0_±_3_._7<br>31_._0_±_11_._4<br>51_._3_±_0_._3<br>48_._0_±_21_._3<br>word_unscrambling<br>44_._0_±_13_._9<br>55_._0_±_1_._7<br>**63.3**_±_0_._7<br>51_._3_±_4_._5|antonyms<br>63_._7_±_14_._2<br>82_._7_±_0_._7<br>84_._7_±_0_._3<br>84_._0_±_0_._0<br>auto_categorization<br>25_._0_±_0_._9<br>25_._7_±_1_._2<br>25_._0_±_3_._3<br>31_._0_±_1_._0<br>auto_debugging<br>29_._2_±_3_._4<br>37_._5_±_0_._0<br>29_._2_±_3_._4<br>33_._0_±_7_._2<br>cause_and_effect<br>57_._3_±_8_._9<br>81_._3_±_1_._1<br>58_._7_±_8_._7<br>84_._0_±_13_._9<br>common_concept<br>6_._9_±_2_._1<br>8_._6_±_4_._0<br>21_._3_±_0_._2<br>11_._1_±_6_._9<br>diff<br>67_._3_±_26_._7<br>69_._3_±_22_._2<br>**100.0**_±_0_._0<br>27_._3_±_42_._2<br>informal_to_formal<br>57_._4_±_0_._3<br>53_._1_±_0_._2<br>55_._3_±_0_._0<br>51_._6_±_0_._9<br>letters_list<br>**100.0**_±_0_._0<br>59_._0_±_16_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>negation<br>75_._3_±_1_._1<br>77_._7_±_1_._4<br>81_._7_±_0_._3<br>86_._0_±_0_._0<br>object_counting<br>36_._3_±_1_._9<br>36_._0_±_9_._3<br>34_._0_±_7_._0<br>**55.0**_±_5_._3<br>odd_one_out<br>63_._3_±_1_._4<br>61_._3_±_8_._7<br>**70.0**_±_1_._6<br>10_._0_±_0_._0<br>orthography_starts_with<br>45_._7_±_14_._8<br>50_._7_±_8_._7<br>**66.7**_±_2_._7<br>15_._0_±_3_._4<br>rhymes<br>15_._7_±_6_._4<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>59_._7_±_3_._1<br>second_word_letter<br>**74.7**_±_20_._3<br>43_._3_±_18_._7<br>10_._0_±_4_._1<br>24_._7_±_0_._6<br>sentence_similarity<br>0_._0_±_0_._0<br>0_._0_±_0_._0<br>**14.0**_±_0_._5<br>2_._0_±_1_._0<br>sum<br>67_._3_±_26_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>synonyms<br>36_._0_±_7_._6<br>27_._7_±_9_._3<br>30_._7_±_4_._9<br>40_._3_±_4_._0<br>taxonomy_animal<br>34_._7_±_23_._4<br>71_._7_±_8_._4<br>85_._7_±_6_._0<br>83_._0_±_4_._6<br>word_sorting<br>33_._0_±_3_._7<br>31_._0_±_11_._4<br>51_._3_±_0_._3<br>48_._0_±_21_._3<br>word_unscrambling<br>44_._0_±_13_._9<br>55_._0_±_1_._7<br>**63.3**_±_0_._7<br>51_._3_±_4_._5|antonyms<br>63_._7_±_14_._2<br>82_._7_±_0_._7<br>84_._7_±_0_._3<br>84_._0_±_0_._0<br>auto_categorization<br>25_._0_±_0_._9<br>25_._7_±_1_._2<br>25_._0_±_3_._3<br>31_._0_±_1_._0<br>auto_debugging<br>29_._2_±_3_._4<br>37_._5_±_0_._0<br>29_._2_±_3_._4<br>33_._0_±_7_._2<br>cause_and_effect<br>57_._3_±_8_._9<br>81_._3_±_1_._1<br>58_._7_±_8_._7<br>84_._0_±_13_._9<br>common_concept<br>6_._9_±_2_._1<br>8_._6_±_4_._0<br>21_._3_±_0_._2<br>11_._1_±_6_._9<br>diff<br>67_._3_±_26_._7<br>69_._3_±_22_._2<br>**100.0**_±_0_._0<br>27_._3_±_42_._2<br>informal_to_formal<br>57_._4_±_0_._3<br>53_._1_±_0_._2<br>55_._3_±_0_._0<br>51_._6_±_0_._9<br>letters_list<br>**100.0**_±_0_._0<br>59_._0_±_16_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>negation<br>75_._3_±_1_._1<br>77_._7_±_1_._4<br>81_._7_±_0_._3<br>86_._0_±_0_._0<br>object_counting<br>36_._3_±_1_._9<br>36_._0_±_9_._3<br>34_._0_±_7_._0<br>**55.0**_±_5_._3<br>odd_one_out<br>63_._3_±_1_._4<br>61_._3_±_8_._7<br>**70.0**_±_1_._6<br>10_._0_±_0_._0<br>orthography_starts_with<br>45_._7_±_14_._8<br>50_._7_±_8_._7<br>**66.7**_±_2_._7<br>15_._0_±_3_._4<br>rhymes<br>15_._7_±_6_._4<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>59_._7_±_3_._1<br>second_word_letter<br>**74.7**_±_20_._3<br>43_._3_±_18_._7<br>10_._0_±_4_._1<br>24_._7_±_0_._6<br>sentence_similarity<br>0_._0_±_0_._0<br>0_._0_±_0_._0<br>**14.0**_±_0_._5<br>2_._0_±_1_._0<br>sum<br>67_._3_±_26_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>synonyms<br>36_._0_±_7_._6<br>27_._7_±_9_._3<br>30_._7_±_4_._9<br>40_._3_±_4_._0<br>taxonomy_animal<br>34_._7_±_23_._4<br>71_._7_±_8_._4<br>85_._7_±_6_._0<br>83_._0_±_4_._6<br>word_sorting<br>33_._0_±_3_._7<br>31_._0_±_11_._4<br>51_._3_±_0_._3<br>48_._0_±_21_._3<br>word_unscrambling<br>44_._0_±_13_._9<br>55_._0_±_1_._7<br>**63.3**_±_0_._7<br>51_._3_±_4_._5|84_._7_±_0_._3<br>|84_._0_±_0_._0<br>|**85.2**_±_3_._2<br>84_._0_±_1_._4<br>**32.7**_±_1_._9<br>27_._0_±_5_._0<br>**41.7**_±_15_._6<br>29_._2_±_5_._9<br>**94.7**_±_3_._7<br>80_._0_±_14_._2<br>**23.5**_±_3_._4<br>2_._8_±_0_._6<br>**100.0**_±_0_._0<br>100_._0_±_0_._0<br>**61.3**_±_2_._7<br>61_._9_±_2_._9<br>**100.0**_±_0_._0<br>100_._0_±_0_._0<br>**86.3**_±_0_._5<br>77_._7_±_2_._6<br>52_._3_±_6_._6<br>40_._3_±_0_._5<br>32_._0_±_11_._3<br>68_._7_±_2_._5<br>56_._5_±_12_._6<br>71_._0_±_0_._0<br>**100.0**_±_0_._0<br>61_._0_±_2_._8<br>25_._7_±_4_._7<br>96_._7_±_2_._4<br>7_._6_±_9_._3<br>37_._3_±_0_._9<br>**100.0**_±_0_._0<br>100_._0_±_0_._0<br>**43.3**_±_0_._9<br>44_._7_±_4_._1<br>**90.0**_±_7_._1<br>92_._3_±_0_._5<br>**60.0**_±_4_._2<br>60_._3_±_3_._1<br>59_._3_±_2_._8<br>58_._3_±_1_._9|0_._5<br>_−_0_._7<br>1_._7<br>_−_4_._0<br>4_._2<br>_−_8_._3<br>10_._7<br>_−_4_._0<br>2_._2<br>_−_18_._5<br>0_._0<br>0_._0<br>3_._9<br>4_._5<br>0_._0<br>0_._0<br>0_._3<br>_−_8_._3<br>_−_2_._7<br>_−_14_._7<br>_−_38_._0<br>_−_1_._3<br>_−_10_._2<br>4_._3<br>0_._0<br>_−_39_._0<br>_−_49_._0<br>22_._0<br>_−_6_._4<br>23_._3<br>0_._0<br>0_._0<br>3_._0<br>4_._4<br>4_._3<br>6_._6<br>8_._7<br>9_._0<br>_−_4_._0<br>_−_5_._0|\n|antonyms<br>63_._7_±_14_._2<br>82_._7_±_0_._7<br>84_._7_±_0_._3<br>84_._0_±_0_._0<br>auto_categorization<br>25_._0_±_0_._9<br>25_._7_±_1_._2<br>25_._0_±_3_._3<br>31_._0_±_1_._0<br>auto_debugging<br>29_._2_±_3_._4<br>37_._5_±_0_._0<br>29_._2_±_3_._4<br>33_._0_±_7_._2<br>cause_and_effect<br>57_._3_±_8_._9<br>81_._3_±_1_._1<br>58_._7_±_8_._7<br>84_._0_±_13_._9<br>common_concept<br>6_._9_±_2_._1<br>8_._6_±_4_._0<br>21_._3_±_0_._2<br>11_._1_±_6_._9<br>diff<br>67_._3_±_26_._7<br>69_._3_±_22_._2<br>**100.0**_±_0_._0<br>27_._3_±_42_._2<br>informal_to_formal<br>57_._4_±_0_._3<br>53_._1_±_0_._2<br>55_._3_±_0_._0<br>51_._6_±_0_._9<br>letters_list<br>**100.0**_±_0_._0<br>59_._0_±_16_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>negation<br>75_._3_±_1_._1<br>77_._7_±_1_._4<br>81_._7_±_0_._3<br>86_._0_±_0_._0<br>object_counting<br>36_._3_±_1_._9<br>36_._0_±_9_._3<br>34_._0_±_7_._0<br>**55.0**_±_5_._3<br>odd_one_out<br>63_._3_±_1_._4<br>61_._3_±_8_._7<br>**70.0**_±_1_._6<br>10_._0_±_0_._0<br>orthography_starts_with<br>45_._7_±_14_._8<br>50_._7_±_8_._7<br>**66.7**_±_2_._7<br>15_._0_±_3_._4<br>rhymes<br>15_._7_±_6_._4<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>59_._7_±_3_._1<br>second_word_letter<br>**74.7**_±_20_._3<br>43_._3_±_18_._7<br>10_._0_±_4_._1<br>24_._7_±_0_._6<br>sentence_similarity<br>0_._0_±_0_._0<br>0_._0_±_0_._0<br>**14.0**_±_0_._5<br>2_._0_±_1_._0<br>sum<br>67_._3_±_26_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>synonyms<br>36_._0_±_7_._6<br>27_._7_±_9_._3<br>30_._7_±_4_._9<br>40_._3_±_4_._0<br>taxonomy_animal<br>34_._7_±_23_._4<br>71_._7_±_8_._4<br>85_._7_±_6_._0<br>83_._0_±_4_._6<br>word_sorting<br>33_._0_±_3_._7<br>31_._0_±_11_._4<br>51_._3_±_0_._3<br>48_._0_±_21_._3<br>word_unscrambling<br>44_._0_±_13_._9<br>55_._0_±_1_._7<br>**63.3**_±_0_._7<br>51_._3_±_4_._5|antonyms<br>63_._7_±_14_._2<br>82_._7_±_0_._7<br>84_._7_±_0_._3<br>84_._0_±_0_._0<br>auto_categorization<br>25_._0_±_0_._9<br>25_._7_±_1_._2<br>25_._0_±_3_._3<br>31_._0_±_1_._0<br>auto_debugging<br>29_._2_±_3_._4<br>37_._5_±_0_._0<br>29_._2_±_3_._4<br>33_._0_±_7_._2<br>cause_and_effect<br>57_._3_±_8_._9<br>81_._3_±_1_._1<br>58_._7_±_8_._7<br>84_._0_±_13_._9<br>common_concept<br>6_._9_±_2_._1<br>8_._6_±_4_._0<br>21_._3_±_0_._2<br>11_._1_±_6_._9<br>diff<br>67_._3_±_26_._7<br>69_._3_±_22_._2<br>**100.0**_±_0_._0<br>27_._3_±_42_._2<br>informal_to_formal<br>57_._4_±_0_._3<br>53_._1_±_0_._2<br>55_._3_±_0_._0<br>51_._6_±_0_._9<br>letters_list<br>**100.0**_±_0_._0<br>59_._0_±_16_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>negation<br>75_._3_±_1_._1<br>77_._7_±_1_._4<br>81_._7_±_0_._3<br>86_._0_±_0_._0<br>object_counting<br>36_._3_±_1_._9<br>36_._0_±_9_._3<br>34_._0_±_7_._0<br>**55.0**_±_5_._3<br>odd_one_out<br>63_._3_±_1_._4<br>61_._3_±_8_._7<br>**70.0**_±_1_._6<br>10_._0_±_0_._0<br>orthography_starts_with<br>45_._7_±_14_._8<br>50_._7_±_8_._7<br>**66.7**_±_2_._7<br>15_._0_±_3_._4<br>rhymes<br>15_._7_±_6_._4<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>59_._7_±_3_._1<br>second_word_letter<br>**74.7**_±_20_._3<br>43_._3_±_18_._7<br>10_._0_±_4_._1<br>24_._7_±_0_._6<br>sentence_similarity<br>0_._0_±_0_._0<br>0_._0_±_0_._0<br>**14.0**_±_0_._5<br>2_._0_±_1_._0<br>sum<br>67_._3_±_26_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>synonyms<br>36_._0_±_7_._6<br>27_._7_±_9_._3<br>30_._7_±_4_._9<br>40_._3_±_4_._0<br>taxonomy_animal<br>34_._7_±_23_._4<br>71_._7_±_8_._4<br>85_._7_±_6_._0<br>83_._0_±_4_._6<br>word_sorting<br>33_._0_±_3_._7<br>31_._0_±_11_._4<br>51_._3_±_0_._3<br>48_._0_±_21_._3<br>word_unscrambling<br>44_._0_±_13_._9<br>55_._0_±_1_._7<br>**63.3**_±_0_._7<br>51_._3_±_4_._5|antonyms<br>63_._7_±_14_._2<br>82_._7_±_0_._7<br>84_._7_±_0_._3<br>84_._0_±_0_._0<br>auto_categorization<br>25_._0_±_0_._9<br>25_._7_±_1_._2<br>25_._0_±_3_._3<br>31_._0_±_1_._0<br>auto_debugging<br>29_._2_±_3_._4<br>37_._5_±_0_._0<br>29_._2_±_3_._4<br>33_._0_±_7_._2<br>cause_and_effect<br>57_._3_±_8_._9<br>81_._3_±_1_._1<br>58_._7_±_8_._7<br>84_._0_±_13_._9<br>common_concept<br>6_._9_±_2_._1<br>8_._6_±_4_._0<br>21_._3_±_0_._2<br>11_._1_±_6_._9<br>diff<br>67_._3_±_26_._7<br>69_._3_±_22_._2<br>**100.0**_±_0_._0<br>27_._3_±_42_._2<br>informal_to_formal<br>57_._4_±_0_._3<br>53_._1_±_0_._2<br>55_._3_±_0_._0<br>51_._6_±_0_._9<br>letters_list<br>**100.0**_±_0_._0<br>59_._0_±_16_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>negation<br>75_._3_±_1_._1<br>77_._7_±_1_._4<br>81_._7_±_0_._3<br>86_._0_±_0_._0<br>object_counting<br>36_._3_±_1_._9<br>36_._0_±_9_._3<br>34_._0_±_7_._0<br>**55.0**_±_5_._3<br>odd_one_out<br>63_._3_±_1_._4<br>61_._3_±_8_._7<br>**70.0**_±_1_._6<br>10_._0_±_0_._0<br>orthography_starts_with<br>45_._7_±_14_._8<br>50_._7_±_8_._7<br>**66.7**_±_2_._7<br>15_._0_±_3_._4<br>rhymes<br>15_._7_±_6_._4<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>59_._7_±_3_._1<br>second_word_letter<br>**74.7**_±_20_._3<br>43_._3_±_18_._7<br>10_._0_±_4_._1<br>24_._7_±_0_._6<br>sentence_similarity<br>0_._0_±_0_._0<br>0_._0_±_0_._0<br>**14.0**_±_0_._5<br>2_._0_±_1_._0<br>sum<br>67_._3_±_26_._7<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>**100.0**_±_0_._0<br>synonyms<br>36_._0_±_7_._6<br>27_._7_±_9_._3<br>30_._7_±_4_._9<br>40_._3_±_4_._0<br>taxonomy_animal<br>34_._7_±_23_._4<br>71_._7_±_8_._4<br>85_._7_±_6_._0<br>83_._0_±_4_._6<br>word_sorting<br>33_._0_±_3_._7<br>31_._0_±_11_._4<br>51_._3_±_0_._3<br>48_._0_±_21_._3<br>word_unscrambling<br>44_._0_±_13_._9<br>55_._0_±_1_._7<br>**63.3**_±_0_._7<br>51_._3_±_4_._5|84_._7_±_0_._3<br>|31_._0_±_1_._0<br>|31_._0_±_1_._0<br>|31_._0_±_1_._0<br>|",
"|Col1|84.0±13.9|\n|---|---|\n|21_._3_±_0_._2<br>|21_._3_±_0_._2<br>|\n|**100.0**_±_0_._0<br>|**100.0**_±_0_._0<br>|",
"|100.0±0.0|100.0±0.0|\n|---|---|\n|**100.0**_±_0_._0<br>|86_._0_±_0_._0<br>|\n|**100.0**_±_0_._0<br>|**55.0**_±_5_._3<br>|\n|**70.0**_±_1_._6<br>|**70.0**_±_1_._6<br>|",
"|100.0±0.0|100.0±0.0|\n|---|---|",
"|100.0±0.0|100.0±0.0|100.0±0.0|\n|---|---|---|",
"|Col1|Col2|\n|---|---|",
"|Col1|1|\n|---|---|",
"|Col1|Col2|\n|---|---|",
"|Task|Best prompt|\n|---|---|\n|active_to_passive|Theprompt was to convert thegiven sentence intopassive voice.|\n|antonyms|The prompt was to rewrite the given words into their opposite meaning. So, “humor-<br>less\" becomes “humorous\", “depressing\" becomes “cheerful\", “unwrap\" becomes<br>“wrap\", “consumptive\" becomes “generative\", “uncoil\" becomes “coil\".|\n|auto_categorization|The prompt was to input the given names and output the corresponding apparel. For<br>example, the input “Nature Nanotechnology, Annual Review of Biochemistry, and<br>The Lancet Neurology\" would output as “top journals\".|\n|auto_debugging|The prompt was to write a program that would take the given input and output<br>the expected output. For example, the first input was a simple calculation, and the<br>expected output was “2550\". The second input was a class definition with a method,<br>and the expected output was “5\".|\n|cause_and_effect|The prompt was to identify the sentence that is the cause and the sentence that is the<br>effect in each pair of sentences. The input sentences are given, and the output is the<br>cause sentence.|\n|common_concept|The prompt was to create a series of pairs of inputs and outputs, where the outputs are<br>related to the inputs in some way. For example, the inputs “guitars\" and “pendulums\"<br>are related to the output of “involve oscillations.|\n|diff|The prompt was to subtract the second number from the first number. For example,<br>the first input would be 41 and the second input would be 13, so the output would be<br>28(41 - 13). The sameprocess would be applied for the other inputs and outputs.|\n|first_word_letter|The prompt was to create a program that takes a single input (a word representing<br>a legal concept or term) and outputs a corresponding letter of the alphabet that<br>represents that concept or term.<br>For example, if the input is “year\", theprogram should output “y\".|\n|informal_to_formal*|The prompt was to rephrase each input sentence using a more formal or polite<br>language.|\n|larger_animal|The prompt was to create a program that takes two input animals and outputs the<br>animal that is bigger. The program uses the “>=\" operator to compare the size of the<br>first animal to the size of the second animal. If the first animal is bigger, the program<br>outputs the first animal.|\n|letters_list|The prompt was to create a program that takes a single word input (e.g. “year\") and<br>outputs a concatenated string of letters and spaces that approximates the pronuncia-<br>tion of that word(e.g. “y e a r\").|\n|negation|The prompt was to flip the truth value of the input statements. For example, if the<br>input statement is “Cany Ash and Robert Sakula are both Architects,\" the output<br>should be “Cany Ash and Robert Sakula are not Architects.|\n|num_to_verbal|The prompt was to write a program that takes a number as input and outputs the<br>number in words, using the appropriate number formatting. The examples provided<br>in the input show the expected output for each number.|\n|object_counting|The prompts were to provide the output of a given input, where the input is a list of<br>items and the output is a number representing the total count of those items. The<br>examples given in the prompt show how the prompts should be used to generate the<br>desired output.|\n|odd_one_out*|The prompt was to identify the word that is most different from the others in the<br>group.|",
"|orthography starts with*<br>_ _|The prompt was to identify the first word that begins with a specific letter in each<br>sentence.|\n|---|---|\n|periodic_elements|The prompts were to write a program that takes an input value and outputs the<br>corresponding element name based on that value.<br>For example, if the input is 24, theprogram would output “chromium.|\n|rhymes|The prompts were to create a program that takes in a word as input and outputs a<br>related word based on a specific set of rules. The rules are as follows: If the input<br>word starts with “tri\", the output should be “slip\".|\n|second_word_letter*|Theprompt was to “Identify and return the second letter of the input word\".|\n|sentence_similarity*|The prompt was to create two different sentences that have similar meanings but are<br>not identical. The output of each input-output pair indicates how closely the two<br>sentences match in terms of meaning.<br>Explanation of outputs:<br>- 5 - perfectly: The two sentences are very similar in meaning and can be considered<br>as equivalent.<br>- 3 - probably: The two sentences have some similarities in meaning but there are<br>also some differences, making it less certain that they are equivalent.<br>- 2 - possibly: The two sentences have some similarities but also significant<br>differences, making it unlikely that they are equivalent.<br>- 1 - probably not: The two sentences have very different meanings and are unlikely<br>to be considered as equivalent.<br>- 0 - definitely not: The two sentences have no similarity in meaning and cannot be<br>considered as equivalent.|\n|sentiment|The prompt was to classify the given reviews as positive or negative based on the<br>given input and output. The output is positive when the review is positive, and<br>negative when the review is negative.|\n|singular_to_plural|The prompt was to convert the input words to their plural form by adding “s\" to the<br>end of the word. This was done by using the “replace\" function in Excel, which<br>allowsyou to replace a specific text string with another text string.|\n|sum|The prompt was to write a program that takes two numbers as input and outputs their<br>sum as the result. The program uses the ‘scanf‘ function to read the input numbers<br>from the user, and the ‘printf‘ function to display the result.|\n|synonyms*|The prompt was to create a list of words that are synonyms or closely related to the<br>given word.|\n|taxonomy_animal*|The prompt was to select all the animals in the input and output them in the order<br>they appear.|\n|translation_en-de|The prompts were to input various words and have the model generate the corre-<br>sponding output in German. It appears that the model was successful in generating<br>the desired output for each of the input words provided. If there are any additional<br>prompts or clarification needed, please let me know.|\n|translation_en-es|The prompts were to translate a set of words from Spanish to English using the<br>provided translation table.|\n|translation_en-fr|The prompt was to input a word and then output the corresponding word in French.<br>It appears that the input and output words are being matched correctly, with the<br>exception of the word “initiative,\" which should have the output “initiative\" in French,<br>not “enterprise.|\n|word_sorting*|The prompt was to alphabetize the input list in ascending order and provide the<br>resulting output as a list.|\n|word_unscrambling|The prompt was to create a program that takes an input word and outputs the<br>corresponding word with the letters rearranged in order. For example, given the input<br>“eccpat\", theprogram should output “accept\".|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/test/2403.02993v1.pdf"
}