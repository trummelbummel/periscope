{
"text": "TRPrompt: Bootstrapping Query-Aware Prompt\n                    Optimization from Textual Rewards\n\n\n\n\n                                   Andreea Nica                       Ivan Zakazov∗\n                                  EPFL                         EPFL\n                              andreea.nica@epfl.ch             ivan.zakazov@epfl.ch\n\n                            Nicolas Mario Baldwin∗           Saibo Geng              Robert West\n                            EPFL                  EFPL           EPFL, Microsoft Research2025                nicolas.baldwin@epfl.ch    saibo.geng@epfl.ch     robert.west@epfl.ch\nJul\n                                                 Abstract\n24\n                           Prompt optimization improves the reasoning abilities of large language models\n                         (LLMs) without requiring parameter updates to the target model.  Following\n                                heuristic-based \"Think step by step\" approaches, the field has evolved in two\n                          main directions: while one group of methods uses textual feedback to elicit im-\n                             proved prompts from general-purpose LLMs in a training-free way, a concurrent\n                                    line of research relies on numerical rewards to train a special prompt model, tailored[cs.CL]                                  for providing optimal prompts to the target model. In this paper, we introduce\n                                the Textual Reward Prompt framework (TRPrompt), which unifies these ap-\n                             proaches by directly incorporating textual feedback into training of the prompt\n                              model. Our framework does not require prior dataset collection and is being itera-\n                                    tively improved with the feedback on the generated prompts. When coupled with\n                                the capacity of an LLM to internalize the notion of what a \"good\" prompt is, the\n                                 high-resolution signal provided by the textual rewards allows us to train a prompt\n                           model yielding state-of-the-art query-specific prompts for the problems from the\n                               challenging math datasets GSMHard and MATH.\n\n\n                1  Introduction\n\n                   The success of recent state-of-the-art large language models (LLMs) should not only be associated\n                     with massive scaling of parameters but also with aligning these models with human feedback [13].\n                      Reinforcement Learning from Human Feedback (RLHF) proved to be an extremely effective methodarXiv:2507.18618v1                         to inject human knowledge into LLMs, enabling them to achieve human-level performance on a\n                         variety of benchmarks [12]. RLHF employs a reward model that generates numerical rewards used\n                         to guide the optimisation of the language model. The numerical rewards act as proxy for human\n                        preference and play a crucial role in training. However, depending on the downstream task, numerical\n                     rewards can be sparse, uninformative and very difficult to define. Textual feedback, on the other\n                      hand, leverages the richness of language and can offer a more informative and nuanced learning\n                          signal. One such task where numerical rewards are hard to model is the query-dependent prompt\n                      optimisation problem, where the goal is to generate a prompt tailored to each input query, helping\n                      guide the language model toward high-quality outputs.\n\n                      Despite their remarkable power, LLMs seem to be challenged when it comes to mathematical and\n                         logical reasoning tasks [8]. Prompt engineering (concatenating the input query with an instruction)\n                    emerged as a lightweight solution to enhance the reasoning abilities of LLMs [17, 6] since it eliminates\n\n                          ∗IZ and NMB co-supervised the project and contributed equally.\n\n\n                      Under review at NeurIPS 2025.\n\nthe need for parameter tuning. Research efforts have explored different methods of finding optimal\nprompts. One direction focuses on using LLMs as black-box optimizers to iteratively update the\noptimal prompt using textual feedback [22, 11, 14, 19]. This train-free methods solely rely on the\ninteractions with an off-the-shelf LLM. In contrast, other methods investigate training a policy model\nthrough reinforcement learning that is able to generate the optimal prompt [16, 3, 10, 7]. Although\nmost of these methods focus on finding an optimal task-level prompt [3, 22] that achieves optimal\naverage performance throughout the entire dataset, query-level prompt optimization techniques\n[7, 10, 16] can yield better results, since the prompt is specifically adapted to fit a particular query.\nGiven the binary aspect of the query-dependent problem (the prompt guided the LLM to arrive at\na correct/incorrect answer), designing a numerical reward to successfully guide the prompt-space\nexploration for a given query is extremely challenging due to the difficulty in encoding nuanced\npreferences within sparse signals. Figure 1 shows a motivating example.\n\nA textual reward is an evaluation expressed\nin the natural language of a prompt, serving\nas a qualitative measure of its effectiveness\nin achieving a specific objective. Some suc-\ncessful applications of textual rewards focus\non improving the quality of the reward models\n[18], by concatenating synthetic critiques with\nthe inputs. The textual critiques improve the\nperformance and data efficiency of the reward\nmodel. However, research on employing tex-\ntual rewards for prompt optimization problems\nremains critically scarce.\n\nSolution: TRPrompt. To address the afore-  Figure 1: Motivating Example: Textual reward is\nmentioned limitations in the existing literature,  more expressive than numerical reward. When us-\nin this paper, we propose TRPrompt, a Tex-  ing both prompts, the target model reaches a wrong\ntual Reward Prompt optimization method that  answer. While the numerical reward is the same\ninvestigates the potential benefits of using tex-  (0), the textual reward distinctively captures each\ntual rewards instead of numerical ones during  prompt fails to guide the target language model in\nthe training of the prompt model. We believe  answering correctly.\nthat the richness of the textual reward is able to\nencapsulate more information about the perfor-\nmance of a given prompt compared to a numerical value. The textual reward should offer stronger\nsignals and more robust features for fine-tuning the prompt model used to generate optimal prompts.\n\nWe develop a proof-of-concept framework that fine-tunes a relatively small model (Llama-3-8B-\nInstruct [1]) to generate query-level optimal prompts. Since there is little work in this space, we have\ndeveloped the entire methodology of applying textual rewards during training for prompt optimization\nproblems, adapted for the query-dependent objective, where numerical rewards are harder to model.\nWe train the prompt model through supervised fine-tuning (SFT) using a synthetically generated\ndatabase of pairs of prompts and their corresponding textual rewards. The pipeline is fully automated,\nwith textual rewards being generated by another language model that acts as a black-box reward\nmodel. In our experiments, we use a model within the same family (Llama-3-8B-Instruct) to generate\ntextual rewards, facilitating the process of self-improvement. Consequently, we establish an iterative\nprocess where the fine-tuned prompt model efficiently generates improved, better prompts, which\nare then used to further fine-tune the prompt model, creating a cyclical feedback loop. In this way,\nat each iteration, the prompt model receives signals through the textual rewards about the actual\nperformance of the prompts that are considered optimal, facilitating improvement.\n\n\n\nTable 1: Overview of prompt optimization methods categorized by reward type and training setup.\nTRPrompt is the only method that combines textual rewards with a trainable approach.\n\n                                      Train                      Train-free\n\n      Textual Rewards           TRPrompt (ours)          Textgrad [19], APO[14]\n      Numerical Rewards     Prompt-OIRL [16], QPO[7]           APE[22]\n\n\n\n                                       2\n\nOur framework can be applied to a wide range of tasks for evaluating the mathematical reasoning\nskills of LLMs. TRPrompt can be initialized on any off-the-shelf LLM and does not require prior\ndataset collection, which makes it instantly adaptable to any dataset. We perform experiments on\nmultiple datasets and analyze the effectiveness of our approach in producing optimal prompts for the\ndesired objective. Our contributions can be summarized as follows:\n\n        • We advance the concept of textual rewards by being the first to incorporate them as a\n         supervision signal directly used during training — moving beyond prior train-free approaches\n        such as TextGrad[19].\n\n        • We propose and develop a novel methodology TRPrompt that is able to exploit the benefits of\n          textual rewards for the query-dependent prompt optimisation objective to improve prompting\n        performance. Table 1 compares prior methods with ours, showing how TRPrompt is the\n            first to embed textual rewards directly into the training loop.\n\n        • We validate the efficacy of our method in various data sets, providing a comprehensive\n         analysis of its performance.\n\n        • We achieve state-of-the-art performance in prompt optimization on the more challenging\n          datasets, GSMHard and MATH, demonstrating the effectiveness of our method.\n\n\n\n\n\nFigure 2: Overview of our query-dependent prompt optimization iterative pipeline, containing 3 steps:\n(1) synthetic training dataset creation, (2) fine-tuning of the prompt model and (3) optimal reward\nsearch.\n\n\n2  The query-dependent prompting problem\n\nThe query-dependent prompting problem centers on guiding a target model to correctly solve a\ngiven question. In this work, we focus specifically on mathematical questions. To achieve this, we\nprepend the question with a query-dependent prompt – an instruction specifically tailored to the given\nquestion to improve the model’s reasoning. This prompt is generated by a separate prompt model,\nwhich is trained to produce effective query-dependent instructions. The prompt model conditions its\ngenerations on textual rewards — natural language feedback that indicates the quality of the desired\nprompt. To generate prompts that reliably lead the target model to correct answers, we condition\nthe prompt model on the optimal textual reward, i.e., the best feedback that a prompt can receive\nfor a given question. This process is illustrated in Figure 2, Step 1. A concrete example is shown\nin Figure 3. Thus, training the prompt model involves two interconnected goals: (1) fine-tuning the\nprompt model and (2) identifying the optimal textual reward. This process constitutes what we term\nthe query-dependent prompting problem, which we now formalize.\nQuestions and answers. We consider the task of answering mathematical questions q ∈Q = V∞\nwhere each question is expressed in natural language using vocabulary V. Each question q is expected\nto have a mathematically correct answer y∗∈Ygt ⊂Y which will be considered the ground-truth.\n\nPrompting. The performance of a target LLM in answering questions can be enhanced by using\nappropriate prompting. A prompt p ∈P =∈V∞is a combination of words in the vocabulary V, an\ninstruction in natural language that explains how to solve a task. In this work, prompts are appended\n\n\n                                       3\n\nat the end of a question. A query-dependent prompt is a prompt generated specifically to guide the\ntarget model to the correct result for a specific question.\n\nTarget LLM. The mathematical questions are answered using a target LLM Mtarget : Q →Y to\nobtain answers. The generated answers can be mathematically described as ˆyi = Mtarget(qi, pi) where\nqi is the i-th question and pi is the query-dependent prompt corresponding to question qi .\n\nThe quality of these generations can be evaluated using the following\nmetric r(y∗i , ˆyi) = 1{ˆyi = y∗i }, which checks for equality between the\ngenerated answer and the ground-truth.\n\nTextual reward. In order to assess the quality of a query-dependent\nprompt, we will use a textual reward t ∈T = V∞. The textual\nreward represents feedback expressed in natural language form that will\nsynthesize the performance of the query-dependent prompt in guiding\nthe Target LLM to the correct answer.\nOptimal textual reward. We define t∗∈T as the optimal textual\nreward, corresponding to the best achievable feedback that a query-\ndependent prompt can obtain, in line with the objective scope.\n\nTextual reward model. A textual reward model  Rtextual where\nRtextual : P × Q × Ygen × Ygt →T  is a language model that can\nproduce textual reward t ∈T for a query-dependent prompt pi ∈P\nbased on the answer generated by the Target LLM ˆyi ∈Ygen ⊂Y\nand the ground-truth for the question y∗i ∈Ygt ⊂Y. We have\nt = Rtextual(qi, pi, ˆyi, y∗i ).\nPrompt model. A prompt model Πquery : Q × T →P is a language\nmodel that is used to generate query-dependent prompts. For some\nquery qi ∈Q and a textual feedback t ∈T , we have Πquery(qi, t) =\npi ∈P. The role of the prompt model is to produce a query-dependent\nprompt, instruction that will enhance the performance of the target\nmodel in the way specified by the feedback t for the specific question                                                                 Figure 3: Step 1: Prompt\nqi.                                                                 generation and textual re-\nDual objective. To maximize the performance of the Target LLM  ward calculation\non reasoning tasks, the query-dependent prompting problem can be\nformulated as a dual-objective optimization problem.\n\n      1. Prompt model objective. The first objective is to optimize the prompt model Πquery to\n        generate query-dependent prompts corresponding to the optimal textual reward t∗for the\n        given questions. These prompt should guide the Target LLM towards producing the correct\n        answers. The goal is to maximize the target model’s performance by ensuring the prompts\n          satisfies the highest-quality feedback signal.\n                Π∗query = arg max r(y∗i , Mtarget(qi, Πquery(qi, t∗)))i∈[N],N=|Q|            (1)\n                                                   Πquery\n\n      2. Optimal textual reward objective. The second objective focuses on identifying the optimal\n         textual reward t∗for a given prompt model Πquery.  This reward represents the most\n         informative and effective natural language feedback that, when used to condition the prompt\n        model, results in query-dependent prompts that maximizes the target model’s accuracy.\n                 t∗= arg max r(y∗i , Mtarget(qi, Πquery(qi, t)))i∈[N],N=|Q|               (2)\n                                 t∈T\n\n3  Prompt optimization with textual reward\n\nExisting query-dependent methods rely exclusively on numerical rewards to guide training [16, 7].\nIn contrast, we argue that textual rewards provide a richer and more expressive supervisory signal,\ncapable of more effectively steering the learning process. We introduce TRPrompt, a framework for\nprompt optimization that leverages textual rewards as the primary training signal. Our iterative, multi-\nloop design enables continuous improvement, with textual feedback facilitating effective knowledge\ntransfer to the prompt model throughout the learning process.\n\n\n                                       4\n\nFigure 2 displays our iterative approach on query-dependent optimization problem. The pipeline\ncontains three important steps: Step 1. synthetic training dataset creation, Step 2. prompt model\nfine-tuning and Step 3. optimal reward update.\n\nStep 1. Query-dependent prompt generation & textual reward calculation. The synthetic training\ndataset contains query-dependent prompts and their textual rewards. To generate the question-specific\nprompt, an optimal feedback signal t∗∈T  is provided together with the question q ∈Q to the\nprompt model Πquery:\n                                 pi = Πquery(qi, t∗) ∈P,   ∀qi ∈Q\nThe query-specific generated prompt is then concatenated with the question to produce the answer\nfrom the Target LLM.\n                         yi = Mtarget(qi, pi),   ∀qi ∈Q, pi = Πquery(qi, t∗)\n\nAnalysing the generated answer and the ground-truth, the textual reward model Rtextual : P × Q ×\nYgen × Ygt →T produces a detailed textual feedback regarding the performance of the query-\ndependent prompt for the specific task.\n                                         Rtextual(pi, (qi, yi, y∗i )) = ti\n\nThe textual rewards corresponding to a prompt for a specific task are collected for fine-tuning\nDtrain = {(pi, qi, ti)}. A concrete example of the entire flow of STEP 1 can be seen in Figure 3\n\nStep 2. Prompt model fine-tuning. For the query-specific task, we fine-tune the prompt model\nΠquery on the prompts p ∈P conditioned on the textual reward t ∈T and the question q ∈Q.\nSpecifically, we maximize the log-likelihood:\n                    LSF T = −E(pi,qi,ti)∼Dtrain log P(pi|qi, ti)                           (3)\n\nStep 3. Optimal textual reward update. Since the optimal reward is a static value that has not been\nseen during training, finding the best possible optimal reward that will leverage all the knowledge\naccumulated throughout fine-tuning is an optimization problem on its own. Specifically, we want\nto find a textual reward inline with the objective from Equation 2. To search for the best optimal\ntextual reward, we employ a train-free optimization strategy, Textgrad. [19]. Using Textgrad, we are\nable to explore the reward space. This step is completed after each round of fine-tuning, to get the\ncorresponding optimal reward for each model. At the end of fine-tuning the prompt model Πquery we\nhave:\n                        t∗= Textgrad(Πquery)\n\nThe iterative procedure for the query-dependent objective, repeating Steps 1, 2, and 3 over K\niterations, is detailed in Algorithm 1.\n\n4  Related work\n\nPrompt optimization has followed two main paths: training-based methods using numerical rewards\n[16, 7], and train-free approaches leveraging natural language to refine prompt outputs without weight\nupdates [19, 14]. TRPrompt bridges these paradigms by enabling gradient-based training directly\nfrom textual rewards.\n\nPrompt optimisation. Discrete prompts have one important feature: interpretability. Research\non zero-shot prompting has shown that LLMs are good zero-shot reasoners simply by using the\nprompt ’Let’s think step by step’ (CoT) [6]. Although most prompt-engineering research efforts have\nfocused on finding the optimal prompts at the task-level (a prompt efficient for the entire dataset),\nthis approach might yield suboptimal results at the per-instance level. A generally effective prompt,\nwhile performing well on average, may steer the LLM towards incorrect results [16]. This is why\nincreasing attention is now being placed on query-dependent methods [16, 21, 10, 7]. Prompt-OIRL\n[16] uses inverse RL to train a reward model which selects the best one of the n-candidate prompts\nfor a query. QPO [7] uses reinforcement learning guided by a custom numerical reward to fine-tune a\nsmall pre-trained language model to generate query-dependent optimal prompts tailored to the input\nqueries. All of the aforementioned methods uniquely rely on numerical rewards to guide the training\nprocess.\n\n\n                                       5\n\nAlgorithm 1 Query-Dependent Prompt Optimization\nRequire: Question dataset Q, number of iterations K, a language model as textual reward model\nRtextual, a Target LLM Mtarget, SFT(·) denotes the supervised fine-tuning process, Textgrad(·) denotes\napplying Textgrad for optimal reward search, an initial optimal textual reward t∗0, the number of\nquestions to analyze the textual reward N, a Prompt Mode Πquery that generates query-dependent\nprompts.\n  1: for k = 1 to K do\n  2:      Dtrain ←∅\n  3:     for qi in Q do\n  4:      Prompt Generation: pi ←Πk−1query(qi, t∗k−1)\n  5:       Question Answering: yi = Mtarget(qi, pi)\n  6:        Textual Reward ti ←Rtextual(pi, (qi, yi, y∗i )\n  7:          Dtrain ←Dtrain ∪{(pi, qi, ti)}\n  8:    end for\n  9:    Fine-tuning: Πkquery ←SFT(Πk−1query, Dtrain)\n10:    Optimal Reward Update: t∗k = Textgrad(Πkquery, t∗k−1)\n11: end for\n12: return Prompt Model ΠKquery and optimal reward t∗K which can generate optimal prompts for a\n     specific task.\n\n\n\nFinetuning conditioned on reward.  Conditioning generation on reward signals during super-\nvised learning has been explored in prior work [20, 15], showing success in domains such as code\noptimization[15]. However, these methods rely solely on numerical reward labels and do not leverage\nthe richer supervision offered by textual rewards.\n\nOptimization with natural language feedback. Recent lines of work investigate the use of natural\nlanguage feedback to improve the LLM’s overall performance. One line of research utilizes textual\nfeedback for self-improvement by refining the model outputs into better ones. These applications\ndeploy LLMs as black-box optimizers in a train-free method where the textual feedback is used to\nobtain iterative improvement in model’s output [14, 19]. Textgrad [19] takes inspiration from gradient\ndescent, providing a customizable framework that can apply a \"textual gradient\" (textual feedback)\nto improve the generation of an LLM on a variety of tasks, including prompt optimization. TPO\n[9] translates a numerical reward signal generated by the reward model into textual critiques and\nuses them as textual rewards to iteratively refine the final output of the model. Textual rewards are\nused to help the policy model come with suggestions guiding the generations for the next iteration.\nAnother line of research explores using critiques for training reward models [18], thus converting the\nricher signals of the synthetic critiques into a numerical representation. However, no method to our\nknowledge uses textual rewards as direct signals during training.\n\nTable 2: TRPrompt differentiates from existing research by (1) not requiring a set of manually\ndefined prompts, (2) being completely independent from the starting prompts since our method\nconstructs the entire training dataset synthetically, avoiding the need for initial prompts, (3) offering\na query-dependent methodology for the prompt optimisation problem, (4) relying on textual rewards\nthat are used as the main signal during training (5).\n\n\nMethod               (1) Avoids     (2) Indepen-    (3) Query     (4) Textual     (5) Textual\n                   Static/Manual   dent of     Dependent    Rewards     Rewards\n                  Prompts         Initial      Prompt                     For\n                               Prompts                                  Training\nCoT[6]              ✗          ✗          ✗          ✗          ✗\nPrompt-OIRL [16]      ✗          ✗        ✓          ✗          ✗\nQPO [7]          ✓          ✗        ✓          ✗          ✗\nTextgrad [19]       ✓          ✗          ✗        ✓          ✗\nTPO [9]              ✗          ✗          ✗        ✓          ✗\nTRPrompt (ours)     ✓        ✓        ✓        ✓        ✓\n\n\n\n\n                                       6\n\n5  Experiments\n\nIn this section, we present empirical evidence illustrating the efficacy of our method using textual\nrewards in guiding the training of the optimal query-dependent prompt model. We start by outlining\nthe general experimental setup, followed by a in-depth analysis of results.\n\nTasks. We perform experiments on 3 math-reasoning datasets of various complexities: GSM8K [2],\nGSMHard [4] which is a harder version of GSM8K and MATH [5]. These arithmetic reasoning tasks\nare widely studied in the zero-shot prompting domain.\n\nModels. Our framework consists of three distinct models: 1. Target LLM for which the optimal\nprompts are being searched. 2. Textual reward model that generates the textual rewards. 3. Prompt\nmodel that will generate the optimal prompts for the Target LLM, the model being fine-tuned. We\nset all these models to be Meta-Llama-3-8B-Instruct [1]. Our method is model-agnostic so different\nmodel instances can be changed for different components. However, we choose to explore a same-\nmodel approach since we want to investigate the ability of the model for self-improvement. For\nTextgrad[19], we used GPT-4o-mini to do the optimal textual reward updates. Technical details\nabout the experiments can be found in the Technical Appendix.\n\nTable 3: Comparison between state-of-the-art methods and TRPrompt on GSM8K, GSMHard, and\nMATH datasets.\n\n Name                    GSM8K (Acc.)  GSMHard (Acc.)  MATH (Acc.)\n\n CoT [6]                              85.59%          27.98%         39.35%\n Prompt-OIRL (6 prompts) [16]            84.53%          28.61%         21.31%\n QPO (1 starting prompt) [7]              84.76%          27.98%         28.37%\n QPO (500 starting prompts) [7]           86.05%         30.80%         37.31%\n\n TRPrompt Query-Dependent (Ours)     84.53%         31.76%        41.37%\n\n\nTable 3 presents a comprehensive comparison between the query-dependent optimized prompts\ngenerated by TRPrompt against several SOTA methods: CoT \"Let’s think step by step\" prompt,\nquery-dependent prompts produced by QPO under two distinct initialization configurations (initial\ndataset containing 1 expert prompt vs 500 expert prompts), Prompt-OIRL with reward learned on 6\nexpert prompts and the task-level prompt obtained by applying Textgrad.\n\nFrom the results, we can conclude: (1). On challenging mathematical benchmarks such as GSMHard\nand MATH, where the target model demonstrates difficulty in generating correct answers, TRPrompt\noutperforms all other state-of-the-art methods by +1% on GSMHard and by +2% on MATH. In\nthe case of GSM8K, where the model answers most questions correctly by default, textual rewards\nappear to provide a weaker signal compared to their effectiveness on more challenging datasets\nlike GSMHard and MATH. (2). While QPO and Prompt-OIRL exhibit a strong dependence on\ninitial expert-provided prompts, TRPrompt avoids this limitation entirely by eliminating the need\nfor handcrafted starting prompts, thereby avoiding initialization bias. Prompt-OIRL selects one of\nthe predefined prompts, whereas QPO generates new prompts that are heavily influenced by the\nstructure and content of the initial prompt set. The accuracy improves as the number of initial\nprompts is increased. TRPrompt explores the prompt space more freely, resulting in higher-quality\nquery-dependent prompts.\n\n\n5.1  Analysis\n\nIteratively better prompts. The iterative nature of our pipeline is essential for ensuring that the\nprompt model learns from its own limitations. For each iteration, a new synthetic training dataset is\nconstructed using the prompts generated by the most recent version of the prompt model, conditioned\non the current optimal textual reward. This setup guarantees that the textual reward is addressing\nthe model’s latest behavior, allowing it to receive precise critiques. As a result, the prompt model\nprogressively refines its generations by learning from past errors, enabling targeted improvements.\nFigure 4 shows the increasing improvement in prompt quality across iterations, with the fine-tuned\nprompt model achieving better performance ( + 7.5%) than the base model over the course of training,\nespecially for the harder datasets GSMHard and MATH. Each iteration exploits newly learned patterns\n\n\n                                       7\n\nFigure 4: Accuracy gain (in per-                                 Figure  5:   Iterative improvement  of query-dependent\ncentage points difference) com-                            prompts on a GSMHard question. Prompts generated by\npared to the base model across iter-                                    the prompt model become increasingly structured and general\nations. Each iteration improves the                                    across iterations, reducing reliance on partial solutions and irrel-\nability of the prompt model to gen-                                    evant details. By the final iteration, the improved prompt guides\nerate prompts that guide the target                                    the target model to the correct answer.\nmodel to the correct answer.\n\n\n\nthrough the textual rewards, gradually improving the quality of the generated prompts. Figure 5\nexemplifies the progress of a prompt across iterations for a GSMHard question. The prompt model\ntransitions from including explicit computations, to producing generic and occasionally irrelevant\nsuggestions, before converging on effective, task-aligned instructions.\n\nFor the harder datasets (MATH and GSMHard), the target model struggles to generate correct answers,\nallowing textual rewards to provide highly targeted and constructive feedback. As a result, the prompt\nmodel shows substantial improvement from one iteration to the next, effectively learning from its\nerrors. In contrast, on simpler datasets like GSM8K —where the target model already performs well\nwithout any prompt — the corrective power of textual rewards is diminished, leading to more modest\ngains. We hypothesize that the limited performance gains on GSM8K are partly due to an unbalanced\ntraining set dominated by positive feedback. When most prompts already lead to correct answers,\nthe textual rewards overwhelmingly reinforce success, providing little contrast between effective\nand ineffective prompts. The lack of negative supervision undermines the quality of training, as the\nprompt model receives insufficient signal to refine its behavior.\n\nAblation insights: why SFT + optimal reward search matter. We investigate the impact of each\nstep on the efficiency of our method. We compute the accuracy after each intermediate step in the\npipeline and report the results for each dataset. Figure 6 illustrates the impact of each step in the\nconsistent increase in accuracy across iterations as the prompt model is fine-tuned using Supervised\nFine-Tuning (SFT) and then applying Optimal Reward Search. The performance gains result from the\ncombined effect of both steps, jointly enabling effective exploration of the prompt space. In contrast,\nFigure 7 presents an ablation where the Optimal Reward Search step is removed from our framework\nand only SFT is applied. In this setting, performance gradually declines with each iteration across all\ndatasets, suggesting that without updating the optimal textual reward, the prompt model begins to\noverfit to suboptimal feedback. These results highlight the critical role of Optimal Reward Search\nin guiding the prompt model towards meaningful exploration of the prompt space and sustaining\nperformance improvements over time.\n\nEvaluating cross-dataset transfer: accuracy of prompt models on unseen tasks. We evaluate the\ngeneralization ability of TRPrompt and query-dependent baselines by testing whether a prompt model\ntrained on one benchmark can generate effective prompts for unseen datasets. For this experiment,\nthe prompt model is trained on a single dataset and evaluated on the remaining datasets. As shown in\nTable 4, all methods show a drop in performance when applied out-of-distribution, confirming that\nprompt optimization is sensitive to the training distribution. However, TRPrompt achieves the highest\naccuracy on the MATH dataset when the prompt model is trained on either GSM8K or GSMHard,\nconsistently outperforming Prompt-OIRL and QPO. Although TRPrompt does not exceed baseline\n\n\n                                       8\n\nFigure 6: Intermediate performance obtained by\nthe prompt model after each step in our pipeline.   Figure 7: Impact of eliminating the Optimal Re-\nBoth SFT and the Optimal Reward Search step   ward Search step from our pipeline: accuracy is\ncontribute to enhancing the prompt model ability    consistently decreasing across iterations.\nto generate efficient prompts.\n\n\nperformance on GSM8K when trained on GSMHard, this observation aligns with prior findings\nsuggesting that performance gains are typically less pronounced on simpler datasets. Collectively,\nthese results highlight the superior generalization capabilities of TRPrompt, particularly in more\nchallenging domains.\n\nTable 4:  Cross-dataset generalization: accuracy when models trained on one dataset are evaluated on\nanother datasets.\n\n   Trained on ↓  Method            Tested on →  GSM8K  GSMHard  MATH\n                                                  Accuracy   Accuracy   Accuracy\n\n  GSM8K      Prompt-OIRL                                   -       29.55%    22.14%\n            QPO                                              -       28.61%    36.36%\n              TRPrompt (Ours)                            -       27.67%    37.20%\n\n   GSMHard     Prompt-OIRL                    84.15%           -       26.53%\n            QPO                           82.94%           -       26.35%\n              TRPrompt (Ours)                83.09%           -       39.41%\n\n\n6  Limitations & future work\n\nReduced gains on easier datasets. TRPrompt shows relatively low performance gains on GSM8K,\nboth in-distribution and in out-of-distribution settings. The dominance of correct answer and positive\ntextual rewards weakens the learning signal. Addressing this issue may require rebalancing the\ntraining data or introducing more nuanced textual feedback to better reflect subtle variations in\nprompt quality.\n\nHigh computational cost. The Optimal Reward Search step involving Textgrad is computationally\nexpensive and difficult to parallelize, making it the main bottleneck in the pipeline. This limits\nscalability and slows down training, especially on larger datasets. Future work could focus on more\nefficient optimal reward search methods.\n\nFurther leveraging textual rewards. Our framework can be extended to tasks where numerical\nrewards are difficult or unnatural to define, such as creative writing or poetry. In these domains,\nquantitative metrics often fail to capture quality, coherence, or style, whereas textual feedback can\nprovide richer, more interpretable signals. TRPrompt offers a promising foundation for future research\nin this area.\n\n\n                                       9\n\n7  Conclusion\n\nWe introduce TRPrompt, a novel query-dependent prompt optimization framework that leverages\nthe expressiveness of textual rewards to directly guide the fine-tuning of a prompt model. Unlike\nprior work, TRPrompt removes strong dependence on initial expert-provided prompts, enabling\nprompt learning from scratch through an iterative process guided by textual feedback. Experiments\nacross arithmetic reasoning tasks show that TRPrompt excels on challenging datasets like MATH\nand GSMHard, where rich textual feedback offers meaningful guidance. These results highlight\nTRPrompt’s potential for aligning LLMs in settings where numerical rewards are limited or ill-defined,\nand position textual supervision as a powerful tool for prompt optimization.\n\n\nAcknowledgments and Disclosure of Funding\n\nReferences\n\n [1] AI@Meta. Llama 3 model card. 2024.\n\n [2] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n     Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\n     solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n\n [3] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng\n     Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforce-\n    ment learning. arXiv preprint arXiv:2205.12548, 2022.\n\n [4] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\n     and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435,\n     2022.\n\n [5] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\n     Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\n     NeurIPS, 2021.\n\n [6] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n     language models are zero-shot reasoners. Advances in neural information processing systems,\n     35:22199–22213, 2022.\n\n [7] Yilun Kong, Hangyu Mao, Qi Zhao, Bin Zhang, Jingqing Ruan, Li Shen, Yongzhe Chang,\n    Xueqian Wang, Rui Zhao, and Dacheng Tao. Qpo: Query-dependent prompt optimization via\n     multi-loop offline reinforcement learning. arXiv preprint arXiv:2408.10504, 2024.\n\n [8] Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: A comprehen-\n      sive benchmark for evaluating the robustness of llms as mathematical problem solvers. arXiv\n     preprint arXiv:2402.19255, 2024.\n\n [9] Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, and Yu Cheng. Test-time preference optimization:\n     On-the-fly alignment via iterative textual feedback. arXiv preprint arXiv:2501.12895, 2025.\n\n[10] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. Guiding\n     large language models via directional stimulus prompting. Advances in Neural Information\n     Processing Systems, 36, 2024.\n\n[11] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language\n     models as evolutionary optimizers. In 2024 IEEE Congress on Evolutionary Computation\n     (CEC), pages 1–8. IEEE, 2024.\n\n[12] Josh OpenAI, Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\n      cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\n     Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\n\n                                       10\n\n[13] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\n    Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\n     follow instructions with human feedback. Advances in neural information processing systems,\n     35:27730–27744, 2022.\n\n[14] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic\n     prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495,\n     2023.\n\n[15] Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi,\n    Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, and Amir Yazdanbakhsh. Learning\n     performance-improving code edits. arXiv preprint arXiv:2302.07867, 2023.\n\n[16] Hao Sun, Alihan Hüyük, and Mihaela van der Schaar. Query-dependent prompt evaluation\n     and optimization with offline inverse rl. In The Twelfth International Conference on Learning\n     Representations, 2023.\n\n[17] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\n    Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\n    Advances in neural information processing systems, 35:24824–24837, 2022.\n\n[18] Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos,\n    and Matthias Gallé.  Improving reward models with synthetic critiques.  arXiv preprint\n     arXiv:2405.20850, 2024.\n\n[19] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and\n    James Zou. Textgrad: Automatic\" differentiation\" via text. arXiv preprint arXiv:2406.07496,\n     2024.\n\n[20] Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph E. Gonzalez. The\n    wisdom of hindsight makes language models better instruction followers. In Proceedings of the\n     40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023.\n\n[21] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera:\n     Test-time prompting via reinforcement learning. arXiv preprint arXiv:2211.11890, 2022.\n\n[22] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\n    and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint\n     arXiv:2211.01910, 2022.\n\n\nA  Technical Appendices and Supplementary Material\n\nA.1  Synthetic Dataset Creation\n\nA central component of our pipeline is the construction of synthetic training datasets. At each\niteration, we use the current versions of the Prompt Model and Reward Model to generate a new\ndataset, which is then used to fine-tune the next version of the Prompt Model. This process allows the\nmodel to iteratively improve by learning from its own outputs and the corresponding feedback.\n\nSynthetic dataset creation consists of two key steps: (1) Prompt Generation, where the Prompt\nModel generates a query-specific prompt for a given question, and (2) Textual Reward Generation,\nwhere the Reward Model produces textual rewards for the generated prompt.\n\nGiven that both models rely on LLMs, clear and precise instruction is critical to ensure the correct\nbehavior of each model during this process. To this end, meta-instructions were carefully designed to\nalign each model with its respective objective.\n\nPrompt Generation  To generate prompts conditioned on the optimal textual reward, we instruct\nthe Prompt Model using a structured input format. The meta-instructions specify both the task and\nthe corresponding Optimal Reward, which serves as the guiding feedback. As illustrated in Figure\n8, the Prompt Model is prompted with a clear definition of its input and output expectations and\nis explicitly instructed to generate a query-dependent prompt that aligns with the provided textual\nreward.\n\n\n                                       11\n\nFigure 8: Meta-Instruction template and example used to generate query-dependent prompts condi-\ntioned on the Optimal Reward.\n\n\nTextual Reward Generation  To generate a textual reward for a specific prompt, we provide the\nReward Model with the generated prompt, the Target Model’s output for the associated question, and\nthe corresponding ground-truth answer. The Reward Model is guided by a meta-instruction designed\nto produce a natural language critique that assesses the quality of the prompt based on the accuracy\nand relevance of the generated answer. This critique is the textual reward for the query-dependent\nprompt. An example of this meta-instruction template, along with a concrete instantiation, is shown\nin Figure 9.\n\nTraining Samples  Each synthetic training sample consists of a query-dependent prompt paired with\nits corresponding textual reward. To incorporate this supervision, we perform Supervised Fine-Tuning\n(SFT), training the Prompt Model to generate the original prompt conditioned on the textual reward.\nThe model is optimized by minimizing the loss between the generated prompt and the reference\nprompt that was originally used to compute the textual reward. An example training pair, consisting\nof a textual reward and its associated prompt, is shown in Figure 10.\n\n\nA.2  Training Details\n\nDatasets.  For each dataset, we compute test scores on the dedicated test sets, which consist of\n1,319 samples for GSM8K, 319 for GSMHard, and 3,369 for MATH. The training sets include\n7,473 questions for GSM8K, 1,000 for GSMHard, and 5,136 for MATH. We apply a 90%–10%\ntrain-validation split on each training set.\n\nSynthetic Dataset.  At each iteration, a synthetic dataset of 800 lengths is created, containing\nprompts and their corresponding textual rewards. In order to create this dataset, we randomly sample\n800 questions from the train set on each iteration. All the final results we report in the paper are on the\ndedicated test set for each dataset. We use the validation set in order to choose the best checkpoint.\n\nGenerations.  When assessing a prompt’s performance, we configure the Target LLM’s inference\ntemperature to be 0.001 when generating a response to a question concatenated with the query-\ndependent prompt we wish to evaluate. We also set the temperature to be 0.001 for textual reward\ngenerations. We set the default temperature to be 0.9 when generating new query-dependent optimal\nprompts with the Prompt Model to encourage diverse and creative generation of new prompts for\ntraining. We use a batch size of 64 for generations.\n\nTraining Details.  We train the Prompt Model for 4 iterations.\n\n\n                                       12\n\nFigure 9: Instruction template and example used to guide the Reward Model in generating a textual\nreward. The input includes the task, the query-dependent prompt, the ground-truth answer, and the\nTarget Model’s answer to the question using the query-dependent prompt.\n\n\nSFT. We train the Prompt Model on the synthetic dataset containing pairs of prompts and their\nrewards via supervised fine-tuning (SFT). Due to memory and cost restrictions, we choose to use\nLow-Rank Adaptation (LoRA), a method designed to accelerate fine-tuning while keeping memory\nconsumption low. LoRA decomposes the weights-update-matrix into two lower-dimension matrices\nthat are easier to compute and store in memory. Due to its benefits, LoRa is a suitable choice for\noptimally fine-tuning the LLM while maintaining computational efficiency and preserving model\nperformance. For LoRA parameters, we utilize r = 256 and α = 256. The Prompt Model is trained\nat each iteration for 2 epoch with a 2 × 10−5 learning rate. We use Adam optimizer, with a linear\nlearning rate decay.\n\nTextgrad  For Textgrad, we use GPT-4o-mini to make the changes to the prompt. Textgrad is run\nfor 10 iterations and uses the validation set in order to compare the quality of the Optimal Reward.\n\nHardware Requirements.  The training was performed on NVIDIA A100 GPUs with 80 GB of\nRAM. The training is done using GPU memory. Training time is between 48h - 72h depending on the\ndataset. The bottleneck of the training was the Textgrad step, which is not parallelizable. Identifying\nthe Optimal Reward at each iteration accounted for around 70% of the total training time, primarily\ndue to the non-parallelized evaluation step in Textgrad, which significantly slowed down the training\nprocess.\n\n\n\n\n\n                                       13\n\nFigure 10: Example of a training pair used in Supervised Fine-Tuning. The Prompt Model is trained\nto reproduce the query-dependent prompt based on the provided textual reward.\n\n\n\n\n\n                                       14",
"headers": [
"arXiv:2507.18618v1  [cs.CL]  24 Jul 2025",
"TRPrompt: Bootstrapping Query-Aware Prompt",
"Optimization from Textual Rewards",
"Abstract",
"1",
"Introduction",
"2",
"The query-dependent prompting problem",
"3",
"Prompt optimization with textual reward",
"4",
"Related work",
"5",
"Experiments",
"6",
"Limitations & future work",
"7",
"Conclusion",
"Acknowledgments and Disclosure of Funding",
"References",
"A",
"Technical Appendices and Supplementary Material"
],
"tables": [
"|Col1|Train Train-free|\n|---|---|",
"|Textual Rewards<br>Numerical Rewards|TRPrompt (ours) Textgrad [19], APO[14]<br>Prompt-OIRL [16], QPO[7] APE[22]|\n|---|---|",
"|Method|(1) Avoids (2) Indepen- (3) Query (4) Textual (5) Textual<br>Static/Manual dent of Dependent Rewards Rewards<br>Prompts Initial Prompt For<br>Prompts Training|\n|---|---|",
"|CoT[6]<br>Prompt-OIRL [16]<br>QPO [7]<br>Textgrad [19]<br>TPO [9]|✗ ✗ ✗ ✗ ✗<br>✗ ✗ ✓ ✗ ✗<br>✓ ✗ ✓ ✗ ✗<br>✓ ✗ ✗ ✓ ✗<br>✗ ✗ ✗ ✓ ✗|\n|---|---|",
"|TRPrompt (ours)|✓ ✓ ✓ ✓ ✓|\n|---|---|"
],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2507.18618v1.pdf"
}