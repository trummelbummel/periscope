{
"text": "Meta-prompting Optimized Retrieval-augmented\n                                  Generation\n\n\n                                           João Rodrigues and António Branco\n\n                                                              University of Lisbon\n                              NLX—Natural Language and Speech Group, Dept of Informatics\n                                Faculdade de Ciências (FCUL), Campo Grande, 1749-016 Lisboa, Portugal\n                                             {jarodrigues,antonio.branco}@fc.ul.pt2024\nJul                          Abstract. Retrieval-augmented generation resorts to content retrieved\n                                 from external sources in order to leverage the performance of large lan-4\n                                 guage models in downstream tasks. The excessive volume of retrieved\n                                      content, the possible dispersion of its parts, or their out of focus range\n                           may happen nevertheless to eventually have a detrimental rather than an\n                                    incremental eﬀect. To mitigate this issue and improve retrieval-augmented\n                                      generation, we propose a method to reﬁne the retrieved content before it\n                                                 is included in the prompt by resorting to meta-prompting optimization.[cs.CL]                         Put to empirical test with the demanding multi-hop question answer-\n                                     ing task from the StrategyQA dataset, the evaluation results indicate\n                                    that this method outperforms a similar retrieval-augmented system but\n                                  without this method by over 30 %.\n\n                            Keywords: RAG  · Retrieval-Augmented Generation  · Prompt Opti-\n                                   mization · Large Language Models · Meta-prompting · Multi-hop QA\n\n\n                  1  Introduction\n\n\n                           Pre-trained Large Language Models (LLMs) [32,22] are known for their hallu-\n                             cinations [12] and for their further limitations regarding truthfulness [17]. To\n                             tackle these issues, remediation techniques have been explored such as, for in-\n                             stance, ﬁne-tuning [10], prompt-engineering [18] or Retrieval-Augmented Gen-\n                            eration [15], initiated by Houlsby et al. [9] among several others.arXiv:2407.03955v1\n\n\n                         1.1  Retrieval-augmented generation\n\n                          Focusing on Retrieval-Augmented Generation (RAG), this approach seeks to\n                         enhance truthfulness and curb hallucinations by expanding the initial prompt,\n                         which contains the initial query, with additional content retrieved from sources\n                           that are external to the LLM. Such additional content is obtained with the\n                           help of an auxiliary Retrieval Model where the retrieval model may be a simple\n                         Jacquard model or a vector database that extracts relevant content from external\n                            sources and pass it on to a Large Language Model that generates an appropriate\n                           response given the original query and the extracted content. If this external\n\n2      João Rodrigues and António Branco\n\n\ncontent is unstructured text, it may be of diﬀerent lengths, such as sentences,\nparagraphs or full documents, among others.\n  By feeding LLM’s knowledge, and curbing its whim, with further knowledge\nfrom external sources, more accurate answers are likely to be provided.\n   Compared with other techniques, such as ﬁne-tuning or prompt-engineering,\nRAG key advantage is the ease with which newer, up-to-date content is taken\nadvantage of, as this does not require the costly compute of re-training neural\nnetworks (as in ﬁne-tuning) or the costly human labour for the creation of further\nmanually designed prompts (as in prompt-engineering). To be sure, all these\ntechniques can nevertheless be mixed and function together.\n\n\n1.2  Prompt optimization\n\nUsually, the pieces of content retrieved may be from heterogeneous sources and\nthey tend to lack a connecting thread. They may also be redundant or may be of\nvery high volume. These, among other aspects, may end up having a detrimental\neﬀect and eventually jeopardizing the generation task, rather than enhancing it.\n   To mitigate this problem, we present a method that consists of adding an\nintermediate step between the retrieval of the external content and the entering\nof the expanded prompt into the LLM to ﬁnally obtain the response to the\ninitial query. Aiming at improving the performance of this generation-LLM, this\nintermediate step seeks to obtain a reﬁned version of the external knowledge.\n   This reﬁnement is accomplished by means of an auxiliary transformation-\nLLM that is entered with a prompt containing the pieces of retrieved contents,\npreceded by an instruction with the request for the sought reﬁnement.\n   For example, if several pages of Wikipedia are retrieved as possible relevant\ncontent, the transformation-LLM processes this content and may generate a\nsummary or remove unnecessary information from that original content.\n   Turning to this reﬁnement instruction, this is obtained by an automatic pro-\ncedure that is preliminary to running the RAG system made of transformation-\nand generation-LLMs, and it is undertaken by yet a third LLM.\n   Inspired in Yang et al. [35], in this procedure a meta-prompt is used as\ninput to this third, optimizer-LLM for this to iteratively generate new tentative\ninstructions, score them, and retain, in the meta-prompt itself, a list with the\ntop k ones that induce better performance for the RAG system. By the end\nof this optimization process, the best scoring instruction in this list is the one\nretained to be used in the reﬁnement step of the retrieved contents with the\ntransformation-LLM.\n   This meta-prompt contains a meta-instruction and a list of tentative instruc-\ntions that is aimed at being updated during this process with new instructions\nthat induce better RAG performance. After a new tentative instruction is gen-\nerated, its contribution to approximate the gold output to the initial query is\nscored, and the list of tentative instructions in the meta-prompt is possibly up-\ndated so that it retains the top-performing ones so far. This is iterated, and an\noptimization trajectory is hence accomplished to eventually ﬁnd the new reﬁne-\nment instruction that maximizes the success of the RAG system.\n\nMeta-prompting Optimized Retrieval-augmented Generation      3\n\n\n                                 ***\n\n   In this paper, we propose a method for RAG to be enhanced with the re-\nﬁnement of the retrieved content, a reﬁnement that is optimized by resorting to\niterative meta-prompting. This is a novel method that can be combined with\nprevious approaches aimed at enhancing RAG.\n  We report on the experiments performed to put this method to the test. This\napproach is extrinsically evaluated by being embedded in a demanding question-\nanswering downstream task. Its performance demonstrates that it is an eﬀective\nmethod to enhance RAG by improving by 30% the performance of a baseline\nRAG without this method, and that it can be combined with other previous\nstate of the art methods for RAG enhancement proposed in the literature.\n   The remainder of this paper is structured as follows: Section 2 discusses\nrelated work; Section 3 describes the method proposed in this study; Section 4\nreports on the models and dataset resorted to; Section 5 presents the experiments\nundertaken and their evaluation, and discusses the results obtained; Section 6\naddresses future research paths; and ﬁnally, Section 7 closes this paper with\nconcluding remarks.\n\n\n2  Related work\n\n\nPrompt optimization has gained traction as an eﬀective mechanism for enhancing\nLLMs in several downstream tasks [1][14].\n   The earliest approaches in prompt optimization sought to directly optimize\nthe prompt embedding space, such as preﬁx-tuning [16] or OptiPrompt [37].\nThese aimed at optimizing a sequence of continuous task-speciﬁc vectors applied\nto the prompt to leverage downstream tasks.\n   More recent studies have introduced further techniques to enhance prompts,\nsuch as chain-of-thought [34] and tree-of-thoughts [36]. The former involves ex-\ntending prompts with a few manually written chain of thought demonstrations\nas examples, which results in improved performance across various tasks, includ-\ning arithmetic, commonsense and symbolic reasoning. The latter builds upon the\nchain-of-thought by considering multiple reasoning paths, self-evaluating choices,\nand by making global decisions by looking ahead or backtracking when necessary.\n   Other methods for optimizing prompts include searching through a pool of\nprompt candidates generated by an LLM, employing principled planning algo-\nrithms based on Monte Carlo tree search [33], or applying iterative local edit\noperations at a syntactic phrase-level split within the prompts [21].\n   Further proposals encompass EvoPrompt [7], which uses evolutionary opera-\ntors over a prompt population for optimization, while Sabbatella et al. employs\nBayesian Optimization within a prompt search space [26], reinforcement learning\nto rewrite prompts [13] or a prompt optimization that integrates human-design\nfeedback rules to suggest improvements automatically [5].\n   Recently, Yang et al. [35] introduced OPRO, leveraging LLMs as optimiz-\ners through meta-prompts, which are natural language descriptions that guide\n\n4      João Rodrigues and António Branco\n\n\nprompt optimization. It was applied to optimize prompts by retrieving and re-\nranking top-K relevant instructions with respect to an initial instruction, and by\nappending them to the global task description.\n   In contrast, to enhance RAG, we propose a method to optimize the prompt\nthat diﬀers from the previous proposals in the literature.\n  A prompt for RAG includes a query and the content retrieved from external\nsources on the basis of that query. It may contain also an instruction about\nhow to handle the query or how the retrieved content should be used by the\ngeneration-LLM to answer it. Related work for RAG enhanced with prompt\noptimization has concentrated on optimizing the instruction and/or the query.\nDiﬀerently from previous approaches, our method focuses instead on optimizing\nthe version of the retrieved content that is included in the prompt entered into the\ngeneration-LLM. Hence, rather than being an approach alternative to previous\nones, it is a new one that is complementary to them and may be combined.\n\n3  Method\n\nThe objective of our method is to enhance the RAG performance of a generation-\nLLM by means of the improvement of its input prompt, which is made of a query\nintroduced by the user and of pieces of content retrieved from external sources on\nthe basis of that query. Before it is entered into the generation-LLM, this prompt\nis improved by means of a reﬁnement of the retrieved content, performed by a\ntransformation-LLM.\n\n\n\n\n      I have some prompts along with their corresponding scores. The prompts are arranged\n      in ascending order based on their scores, where higher scores indicate better quality.\n     Together with relevant information extracted from a database, these prompts are given as\n     input to a large language model in order to optimize the provided relevant information.\n     Several techniques may help the optimization, such as re-ranking paragraphs, cleaning,\n     ﬁltering and summarization. Write your new prompt taking into account the previous\n     ones and aiming to achieve a higher score.\n\n     prompt:\n    Summarize the main idea of the previous text.\n      score:\n      3.0\n\n     prompt:\n    Summarize the main points in 30 words or less.\n      score:\n      3.0\n\n\nTable 1. Meta-prompt - An example of a meta-prompt: in black, the top paragraph\nwith the meta-instruction actually used in the experiments; below, in green, the list of\ntop performing instructions so far, and the respective scores.\n\n\n\n\n  And before a ﬁrst query is accepted to put the RAG system to use, the prompt\nto be used with the transformation-LLM for reﬁnement purposes is optimized.\n\nMeta-prompting Optimized Retrieval-augmented Generation      5\n\n\nThis prompt includes a reﬁnement instruction and the pieces of retrieved content\nto be reﬁned. It is optimized by means of the optimization of this instruction\nthrough iterative meta-prompting.\n\n   This meta-prompting optimization is undertaken by an optimizer-LLM that\nis entered with a (meta-)prompt that includes a (meta-)instruction and a list\nof tentative reﬁnement instructions and respective performance scores. These\nscores are obtained by running the RAG with the tentative reﬁnement instruction\nthrough a sample of training examples and evaluating the output against the\nrespective gold responses.\n\n   Focusing on the optimization phase, a meta-prompt is used that contains\nboth the description of the optimization problem and the history with previous\nbest solutions for the instruction. Such meta-prompt is iteratively entered into\nthe optimizer-LLM, and at each iteration that history is possibly updated with\ngenerated instructions if these support better performance for the task at stake\nin the generation phase. The instruction selected out of this optimization process\nis the best scoring one in the history obtained as this iteration is over.\n\n  An example of a meta-prompt is in Table 1, and a detailed description of\nthis optimization via iterative meta-prompting is presented in Algorithm 1.\n\n\nAlgorithm 1 Optimization with meta-prompting\n 1: Input: Dataset D with n examples, each containing a query q, retrieved contents\n    c and the answer a; meta-prompt metaP with the description of the optimization\n    task and with a list of instructions and respective scores\n 2: Output: List of scored instructions and the best scoring instruction\n 3: while optimizing prompt do\n 4:    Enter meta-prompt to optimizer-LLM\n 5:    Generate new instructions I\n 6:     Select a random subset E of examples e from D\n 7:    for each instruction Ij do\n 8:        for each example ek in E do\n 9:          Assemble prompt T ransP from Ij and contents ck\n10:          Enter T ransP to transformation-LLM\n11:          Generate transformed contents tc\n12:          Assemble prompt T askP from query qk and tc\n13:          Input T askP to generation-LLM\n14:          Generate answer and evaluate it against gold ak\n15:      end for\n16:      Compute Ijscore\n17:   end for\n18:    Update metaP by replacing its worst scoring instruction by Ij and Ijscore if\n    this is better scored\n19: end while\n\n6      João Rodrigues and António Branco\n\n4  Dataset and models\n\n\nTo empirically assess the performance gains of the proposed method, it was\nintegrated into an RAG for question-answering whose performance provides for\nits extrinsic evaluation.\n\n\n4.1  Task and dataset\n\nMulti-hop question answering requires taking into account disparate pieces of\ncontent to get at the answer for a query, which constitutes a most demanding\nscenario for the task of question answering.\n  We resorted to a most complex benchmark for multi-hop question-answering\navailable in the literature, the StrategyQA dataset [6,19,11,8], which contains\n2,780 queries, each associated with related content made of paragraphs and the\nrespective yes or no answer. Based on Wikipedia content, this dataset covers\na range of diverse topics and the task consists in, given a query, to provide an\naccurate answer to it together with the passages retrieved from Wikipedia with\nthe most relevant content to get at that answer — Table 2 displays an example.\n\n\n\nQuery               Could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?\nContent #1              It set out to tell the news through people, and for many decades through\n                        the late 1960s, the magazine’s cover depicted a single person. [...] Raymond\n                         Fielding also noted that Larsen was \"originally circulation manager and\n                       then general manager of Time, later publisher of Life, for many years\n                         president of Time Inc., and in the long history of the corporation the\n                     most inﬂuential and important ﬁgure after Luce\"\nContent #2           Total eclipses are rare because the timing of the new moon within the\n                           eclipse season needs to be more exact for an alignment between the ob-\n                          server (on Earth) and the centers of the Sun and Moon. [...] because to-\n                             tality exists only along a narrow path on the Earth’s surface traced by the\n                     Moon’s full shadow or umbra.\nContent #3         At least two lunar eclipses and as many as ﬁve occur every year, although\n                           total lunar eclipses are signiﬁcantly less common. If the date and time of\n                     an eclipse is known, the occurrences of upcoming eclipses are predictable\n                        using an eclipse cycle, like the saros.\nAnswer               Yes\n\nTable 2. StrategyQA - An example from the StrategyQA, with a query, three of the\nmost relevant pieces of content, and the respective answer.\n\n   To provide for the evaluation of the proposed method, and isolate the ac-\ncrued performance induced by it, thus disregarding possible ﬂuctuation or loss\nof performance due to the retrieval process, only the gold pieces of content from\na test set should be taken into account. Since the answers are not provided in the\noriginal test set of StrategyQA, a new test set for the present evaluation exercise\nhad to be built. Accordingly, we divided the original training set into two parts:\na new test set with 490 of the original training examples, which matches the size\nof the original test set, and a new training set containing a subset with 1800 such\nexamples. The resulting train and test sets have an average query length of 9.6\nwords, and 2.33 contents (paragraphs) per query and are almost balanced. The\ntraining set contains 834 yes answers and 966 no answers (46.32 % / 53.68 %).\nThe test set contains 237 yes answers and 253 no answers (48.40 % / 51.60 %).\n\nMeta-prompting Optimized Retrieval-augmented Generation      7\n\n\n4.2  Models\n\nTwo Transformer-based language models with 70 Billion parameters were used,\na pre-trained Llama-2-70b and an instruct model Llama-2-70b-chat ﬁne-tuned\nfor dialogue use [31]. These models were trained and ﬁne-tuned with a context\nlength of 4k tokens over 2 Trillion tokens on a mix of publicly available data.\n   In general, the default Llama2 model hyper-parameters were applied and\nno hyper-parameters search bound was performed. All language models use a\ntemperature value of 1.0, a maximum of 64 generation tokens for the new in-\nstructions, a maximum of 128 generation tokens for the reﬁned content, and a\nmaximum of 64 generation tokens for the response to the task. The optimization\nrun was performed for two days on two NVIDIA A100 40GB GPUs.\n   All software and versioning along with hyper-parameters are fully described\nin the source code of these experiments.1\n\n\n4.3  Evaluation procedure and metrics\n\nBased on empirical experimentation, we arrived at a meta-prompt, presented\nin Table 1, that indicates the aim of the optimization problem and includes a\nstarting example instruction.2\n   The instruction optimization was iterated over 100 steps. At each step, 3\ninstructions were generated, each such instruction was evaluated on a random\nsample of 6 training examples, and the meta-prompt was eventually updated to\ncontaining the 8 top scoring queries so far. When this iteration was concluded,\nthe best scoring instruction was retained as the optimized instruction.\n  We compare against the same generation-LLM using test queries and associ-\nated pieces of content, that is without the later being reﬁned by the transformation-\nLLM under the instruction that was optimized by the optimizer-LLM.\n   For the StrategyQA task, a Boolean answer is expected. Accuracy is thus the\nmetric used for evaluating the match between the answer output by the system\nand the gold answer in the data set. Accuracy score is given by the proportion\nof correct answers, and a generated response was counted as correct if the gold\nanswer was found in the exact beginning of it. The response underwent minimal\nnormalization, with just lowercasing.\n   As for the instrumental process of instruction optimization, it is worth recall-\ning that the evaluation is performed over sample examples from the training set.\nFor a tentatively generated instruction, a correct answer to it counted 1 point; an\nincorrect answer, in turn, counted 0.5 points if it was nevertheless in a Boolean\nformat, or counted 0 points otherwise. The maximum possible score was thus 6\npoints, given each tentative instruction was evaluated against 6 sampled queries\nas indicated above.\n\n 1 For   the   sake   of   reproducibility,   data  and   code   are   available   at\n  https://github.com/nlx-group/rag-meta-prompt\n 2 The starting instruction is \"Clean and organize the previous text.\"\n\n8      João Rodrigues and António Branco\n\n5  Results and discussion\n\n\nIn this section, we report on the evaluation exercise undertaken to assess the\nproposed method and discuss its results, summarized in Table 3\n\n\n5.1  Experiments\n\nAll in all, six experiments were undertaken, two resorting to the model Llama-2-\n70b, developed with a pre-training regime only, and four resorting to the model\nLlama-2-70b-chat, which resulted from further ﬁne-tuning it with dialogue data.\n\n\n\n        Model              Method          Accuracy\n    Llama-2-70b        query                           17 (3.46 %)\n    Llama-2-70b        query+contents                  33 (6.73 %)\n    Llama-2-70b-chat    query                           81 (16.53 %)\n    Llama-2-70b-chat    query+contents (plain RAG)      128 (26.12 %)\n    Llama-2-70b-chat    reﬁned query+contents (ours)     170 (34.69 %)\n    Llama-2-70b-chat     ref. query+contents no iteration   127 (25.92 %)\n\nTable 3. Evaluation - From the total 490 test set examples, the number of correct\nanswers is presented and the respective accuracy.\n\n   Both these models were used in two evaluation scenarios. In one of these\nscenarios—noted as query in Table 3—, the response to the query entered was\nprovided by the LLM alone, with no further content from external sources being\nentered. In the other scenario, in turn,—noted as query+contents—, further\ncontent from external sources was included in the prompt as well. The perfor-\nmance scores for these two scenarios with the two models are displayed in the\ntop four rows 3.\n   External, non-parametric content improves generation As expected,\nand in line with results in the literature, the retrieval-augmented generation\n(26.12%) outperforms the plain generation based solely on the query (16.53%).\n   Fine-tuning improves generation Also as expected, and by a very large\nmargin, better performance scores were obtained with Llama-2-70b-chat, which\nhad been ﬁne-tuned on dialogue tasks, namely 16.53% against 3.46%, with the\nquery only, and 26.12% against 6.73%, with the query and external content.\n   Retrieved content reﬁnement via meta-prompting optimization im-\nproves RAG — the proposed method is eﬀective The model Llama-2-70b-\nchat was thus retained and two further evaluation scenarios were considered.\n  A scenario with the application of the proposed method—noted as reﬁned\nquery+contents—, where the external content was reﬁned with the help of an\ninstruction optimized with meta-prompting.\n   The performance scores indicate that, with 34.69% accuracy, our proposed\nmethod of enhanced RAG outperforms plain RAG, with 26.12%, thus contribut-\ning for a large improvement of over 8.5 percentage points, that represents here\nan improvement rate of almost 33%.\n\nMeta-prompting Optimized Retrieval-augmented Generation      9\n\n\n   Retrieved content reﬁnement via \"brute force\" optimization does\nnot improve RAG A sixth scenario—noted as reﬁned query+contents no\niter—was also considered. Here the external content was reﬁned as in the pro-\nposed method, but the instruction was reﬁned under an alternative way that\ndispensed with iterative meta-prompting.\n   All in all, 300 tentative instructions are generated during all optimization\nsteps —recall that we had 100 iteration steps with 3 tentative instructions gen-\nerated per step with meta-prompting optimization. To dispense with this iter-\native meta-prompting, the same number of 300 new tentative instructions were\ngenerated at once, in a \"brute force\" fashion. By the end of this process, all ten-\ntative instructions were scored with the same scoring function as in the proposed\nmethod, and the top instruction was evaluated on the test set.\n   This \"brute force\" optimization approach, scoring 25.92%, is outperformed\nnot only by the proposed method of meta-prompting optimization, with 34.69%,\nbut even also by the baseline, plain RAG, with 26.12%.\n   Statistical signiﬁcance To assess the statistical signiﬁcance of the improve-\nments by our method, we employed the unpaired t-test.3 We evaluated the base-\nline system with three seeds and did the same for the meta-prompting optimized\nsystem. Both samples are independent and one may assume the samples are nor-\nmally distributed. Applying the unpaired t-test, a two-tailed P value equal to\n0.0004 was obtained, which is considered statistically signiﬁcant.\n\n\n5.2  Examining the tentative instructions\n\nTable 4 presents the top generated prompts. The best scoring prompt (last row),\nwith 5.5 (out of a maximum of 6), was obtained at iteration step 46 (out of 300\nsteps in total), and a good prompt (ﬁrst row) can be obtained with only 28 steps.\n  When reading the best prompt (last row), one realizes that it aims to improve\nthe task through the summarization of the retrieved contents, considering their\nbroader context, and identifying the main theme or message. It appears thus\nlike a reasonable prompt a human might have thoughtfully arrived at if aiming\nat improving the performance of the task.\n    It is reasonable to assume that the meta-prompt iteration in subsequent steps\nused this query and its score in its search for further tentative instructions, with\nthe generated instructions in three subsequent steps (58, 65 and 72) being some\nderivation of it (second, fourth and ﬁfth rows).\n  When taking a look at the entire set of generated queries, a high ﬂuctuation\nof the evaluation scores can be observed along the iteration steps. This is likely\ndue to some interim, generated instructions happening to perform poorly.\n\n\n5.3  Examining the responses\n\nTo gain insight about where our method outperformed the plain RAG baseline,\nwe examined the ﬁrst 10 instances where our method correctly provided the an-\n 3 The unpaired t-test evaluates  if there exists a statistically signiﬁcant distinction\n  between the means of two independent samples by comparing them.\n\n10     João Rodrigues and António Branco\n                    Generated instruction                   Score  Iter. step\n\n   Summarize the previous text in 2-3 sentences, while also considering\n   the broader context, the author’s intent, the potential implications of\n   the information, and also identify the main theme or message and its    5       65\n   signiﬁcance, and also analyze the impact of the information on the\n   reader.\n\n   Summarize the previous text in 1 sentence, while also considering the\n   broader context, the author’s intent, the potential implications of the\n   information, and also identify the main theme or message and its sig-    5       72\n   niﬁcance, and also analyze the impact of the information on the reader,\n   and also provide recommendations for further\n\n   Summarize the previous text in 2-3 sentences, while also considering\n   the broader context, the author’s intent and the potential implications   5.5      46\n    of the information, and also identify the main theme or message.\n\nTable 4. Top meta-prompting optimized instructions scoring 5 or higher, with\nrespective scores and iteration steps at which they were obtained, ordered top to bot-\ntom, with the top-scoring, retained instruction in the last row.\n\nswer while the baseline failed. Among these, in six cases, the baseline provided\na verbose response and might have failed the exact-match evaluation criterion\nused.4 In the remaining four cases, the baseline either answered incorrectly, re-\nsponded with a query, or failed to provide an answer.\n   Conversely, we reviewed the ﬁrst 10 instances where our method failed to\nprovide the correct answer while the baseline succeeded. We observed that our\nmethod exhibited a verbose response behavior in ﬁve cases that eventually ar-\nrived at the correct answer but failed the exact-match evaluation criterion. In\ntwo other cases, our method gave a verbose response without providing an an-\nswer, while in two remainder cases, it provided an incorrect response. Finally, in\none instance, our method did not provide any response.\n   Both methods seem thus to be similarly penalized by the evaluation criterion\nfor not providing straight answers when the correct answer may happen to be\nincluded in the verbose response.\n\n\n6  Future work\n\n\nWhile providing a method that eﬀectively enhances RAG, our proposal paves\nthe way for future research, such as the exploration of optimal hyper-parameters,\nreﬁning content retrieved without gold content, scaling up with larger models,\nexploring further evaluation functions, or tackling other downstream tasks. A\nsigniﬁcant number of hyper-parameters remain unexplored, which is an oppor-\ntunity to further enhance the eﬃcacy of this method.\n    It is worth noting that the evaluation with exact matching is a binary task,\nand achieving an exact match with a task demanding a more complex string\nmatch still needs to be studied, questioning the need for additional training, a\ndiﬀerent meta-prompt, or a diﬀerent approach.\n\n 4 An example of a verbose response: [query] Was Superhero ﬁction invented in the digi-\n   tal format? [response] The answer is no; superhero ﬁction did not originate in digital\n  format. Superheroes have their roots in pulp magazines, comic strips, and comic\n  books, which were all print media formats before the advent of digital technology.\n\nMeta-prompting Optimized Retrieval-augmented Generation     11\n\n\n    It will be interesting also to study the interaction of our proposed method\nwith the Portuguese language [20,2] with the existing family of LLMs [24,30,29]\nand multi-modal LLMs [27] as also with other tasks such as argument mining\n[25], exploring data spuriousness and others [4,28,3,23].\n\n\n7  Conclusion\n\n\nThis paper introduces a novel method that enhances RAG and that can be\ncombined with previous approaches for RAG enhancement. It consists in reﬁning\nthe retrieved content included in the prompt entered into the generation model\nwith the help of a reﬁnement instruction that was obtained by means of meta-\nprompting optimization.\n    It reports also on the empirical assessment of this proposal by means of it\nbeing embedded in a most demanding multi-hop question answering task. The\nevaluation results indicate that it is highly eﬀective in as much as it outperforms\nRAG without this method by over 30%.\n\n\nAcknowledgments. This research was partially supported by: PORTULAN CLARIN\n— Research Infrastructure for the Science and Technology of Language, funded by Lis-\nboa 2020, Alentejo 2020 and FCT (PINFRA/22117/2016); ACCELERAT.AI - Multi-\nlingual Intelligent Contact Centers, funded by IAPMEI (C625734525-00462629);\n\n\nReferences\n\n\n 1. Aarohi Srivastava, A.R., et al.: Quantifying and extrapolating the capabilities of\n    language models. Transactions on Machine Learning Research (2023)\n 2. Branco, A., Mendes, A., Quaresma, P., Gomes, L., Silva, J., Teixeira, A.: Infras-\n    tructure for the science and technology of language PORTULAN CLARIN. In:\n   Rehm, G., Bontcheva, K., Choukri, K., Hajič, J., Piperidis, S., Vasil,jevs, A. (eds.)\n    Proceedings of the 1st International Workshop on Language Technology Platforms.\n    pp. 1–7. European Language Resources Association, Marseille, France (May 2020),\n    https://aclanthology.org/2020.iwltp-1.1\n 3. Branco, A., Rodrigues, J., Salawa, M., Branco, R., Saedi, C.: Comparative probing\n     of lexical semantics theories for cognitive plausibility and technological usefulness.\n     In: Proceedings of the 28th International Conference on Computational Linguistics.\n    pp. 4004–4019 (2020)\n 4. Branco, R., Branco, A., António Rodrigues, J., Silva, J.R.: Shortcutted common-\n    sense: Data spuriousness in deep learning of commonsense reasoning. In: Moens,\n    M.F., Huang, X., Specia, L., Yih, S.W.t. (eds.) Proceedings of the 2021 Con-\n    ference on Empirical Methods in Natural Language Processing. pp. 1504–1521.\n    Association  for Computational Linguistics, Online and Punta Cana, Domini-\n    can Republic (Nov 2021). https://doi.org/10.18653/v1/2021.emnlp-main.113,\n    https://aclanthology.org/2021.emnlp-main.113\n 5. Chen, Y., Arkin, J., Hao, Y., Zhang, Y., Roy, N., Fan, C.: Prompt optimization in\n    multi-step tasks (promst): Integrating human feedback and preference alignment.\n    arXiv preprint arXiv:2402.08702 (2024)\n\n12     João Rodrigues and António Branco\n\n\n 6. Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., Berant, J.: Did aristotle\n    use a laptop? a question answering benchmark with implicit reasoning strategies.\n    Transactions of the Association for Computational Linguistics 9, 346–361 (2021)\n 7. Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., Yang, Y.:\n    Connecting large language models with evolutionary algorithms yields powerful\n   prompt optimizers. arXiv preprint arXiv:2309.08532 (2023)\n 8. Ho, X., Duong Nguyen, A.K., Sugawara, S., Aizawa, A.: Constructing a multi-hop\n  QA dataset for comprehensive evaluation of reasoning steps. In: Proceedings of\n    the 28th International Conference on Computational Linguistics. pp. 6609–6625.\n    International Committee on Computational Linguistics (Dec 2020)\n 9. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-\n   mundo, A., Attariyan, M., Gelly, S.: Parameter-eﬃcient transfer learning for nlp.\n     In: International conference on machine learning. pp. 2790–2799. PMLR (2019)\n10. Howard, J., Ruder, S.: Universal language model ﬁne-tuning for text classiﬁcation.\n     In: Proceedings of the 56th Annual Meeting of the Association for Computational\n    Linguistics (Volume 1: Long Papers). pp. 328–339 (2018)\n11. Inoue, N., Stenetorp, P., Inui, K.: R4C: A benchmark for evaluating RC systems\n    to get the right answer for the right reason. In: Proceedings of the 58th Annual\n    Meeting of the Association for Computational Linguistics. pp. 6740–6750 (Jul 2020)\n12.  Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A.,\n    Fung, P.: Survey of hallucination in natural language generation. ACM Computing\n    Surveys 55(12), 1–38 (2023)\n13. Kong, W., Amba Hombaiah, S., Zhang, M., Mei, Q., Bendersky, M.: Prewrite:\n   Prompt rewriting with reinforcement learning. arXiv e-prints pp. arXiv–2401\n    (2024)\n14. Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-eﬃcient\n   prompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in\n    Natural Language Processing. pp. 3045–3059 (2021)\n15. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H.,\n    Lewis, M., Yih, W.t., Rocktäschel, T., et al.: Retrieval-augmented generation for\n    knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems\n    33, 9459–9474 (2020)\n16. Li, X.L., Liang, P.: Preﬁx-tuning: Optimizing continuous prompts for generation.\n     In: Proceedings of the 59th Annual Meeting of the Association for Computational\n    Linguistics. pp. 4582–4597 (2021)\n17. Lin, S., Hilton, J., Evans, O.: Truthfulqa: Measuring how models mimic human\n    falsehoods. In: Proceedings of the 60th Annual Meeting of the Association for\n    Computational Linguistics. pp. 3214–3252 (2022)\n18. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and\n    predict: A systematic survey of prompting methods in natural language processing.\n  ACM Computing Surveys 55(9), 1–35 (2023)\n19. Min, S., Wallace, E., Singh, S., Gardner, M., Hajishirzi, H., Zettlemoyer, L.: Com-\n    positional questions do not necessitate multi-hop reasoning. In: Proceedings of the\n    57th Annual Meeting of the Association for Computational Linguistics (Jul 2019)\n20. Osório, T.F., Leite, B., Lopes Cardoso, H., Gomes, L., Rodrigues, J., Santos, R.,\n    Branco, A.: PORTULAN ExtraGLUE datasets and models: Kick-starting a bench-\n   mark for the neural processing of Portuguese. In: Zweigenbaum, P., Rapp, R.,\n    Sharoﬀ, S. (eds.) Proceedings of the 17th Workshop on Building and Using Com-\n    parable Corpora (BUCC) @ LREC-COLING 2024. pp. 24–34. ELRA and ICCL,\n    Torino, Italia (May 2024), https://aclanthology.org/2024.bucc-1.3\n\nMeta-prompting Optimized Retrieval-augmented Generation     13\n\n\n21. Prasad, A., Hase, P., Zhou, X., Bansal, M.: Grips: Gradient-free, edit-based in-\n    struction search for prompting large language models. In: Proceedings of the 17th\n    Conference of the EACL. pp. 3845–3864 (2023)\n22. Raﬀel, C., Shazeer: Exploring the limits of transfer learning with a uniﬁed text-to-\n    text transformer. Machine Learning Research 21, 5485–5551 (2020)\n23. Rodrigues, J., Branco, R., Silva, J., Saedi, C., Branco, A.: Predicting brain activa-\n    tion with wordnet embeddings. In: Proceedings of the Eight Workshop on Cognitive\n    Aspects of Computational Language Learning and Processing. pp. 1–5 (2018)\n24. Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., Cardoso, H.L., Osório,\n    T.: Advancing neural encoding of portuguese with transformer albertina pt. In:\n   EPIA Conference on Artiﬁcial Intelligence. pp. 441–453. Springer (2023)\n25. Rodrigues, J.A., Branco, A.: Transferring conﬂuent knowledge to argument min-\n     ing. In: Calzolari, N., Huang, C.R., Kim, H., Pustejovsky, J., Wanner, L., Choi,\n    K.S., Ryu, P.M., Chen, H.H., Donatelli, L., Ji, H., Kurohashi, S., Paggio, P., Xue,\n    N., Kim, S., Hahm, Y., He, Z., Lee, T.K., Santus, E., Bond, F., Na, S.H. (eds.)\n    Proceedings of the 29th International Conference on Computational Linguistics.\n    pp. 6859–6874. International Committee on Computational Linguistics, Gyeongju,\n    Republic of Korea (Oct 2022), https://aclanthology.org/2022.coling-1.597\n26. Sabbatella, A., Ponti, A., Giordani, I., Candelieri, A., Archetti, F.: Prompt opti-\n    mization in large language models. Mathematics 12(6) (2024)\n27. Santos, R., Branco, A., Silva, J.R.: Cost-eﬀective language driven image editing\n    with LX-DRIM. In: Proceedings of the First Workshop on Performance and In-\n    terpretability Evaluations of Multimodal, Multipurpose, Massive-Scale Models. pp.\n    31–43. International Conference on Computational Linguistics, Virtual (Oct 2022),\n    https://aclanthology.org/2022.mmmpie-1.5\n28. Santos, R., Rodrigues, J., Branco, A., Vaz, R.: Neural text categorization with\n    transformers for learning portuguese as a second language. In: Progress in Artiﬁcial\n     Intelligence: 20th EPIA Conference on Artiﬁcial Intelligence, EPIA 2021, Virtual\n    Event, September 7–9, 2021, Proceedings 20. pp. 715–726. Springer (2021)\n29. Santos, R., Rodrigues, J., Gomes, L., Silva, J.R., Branco, A., Lopes Cardoso,\n    H., Osório, T.F., Leite, B.: Fostering the ecosystem of open neural encoders for\n    Portuguese with albertina PT* family. In: Melero, M., Sakti, S., Soria, C. (eds.)\n    Proceedings of the 3rd Annual Meeting of the Special Interest Group on Under-\n    resourced Languages @ LREC-COLING 2024. pp. 105–114. ELRA and ICCL,\n    Torino, Italia (May 2024), https://aclanthology.org/2024.sigul-1.14\n30. Santos, R., Silva, J.R., Gomes, L., Rodrigues, J., Branco, A.: Advancing generative\n   AI for Portuguese with open decoder gervásio PT*. In: Melero, M., Sakti, S., Soria,\n    C. (eds.) Proceedings of the 3rd Annual Meeting of the Special Interest Group on\n    Under-resourced Languages @ LREC-COLING 2024. pp. 16–26. ELRA and ICCL,\n    Torino, Italia (May 2024), https://aclanthology.org/2024.sigul-1.3\n31. Touvron, H., Martin, L., Stone, K., et al.: Llama 2: Open foundation and ﬁne-tuned\n    chat models. arXiv preprint arXiv:2307.09288 (2023)\n32. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n     Ł., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\n    cessing systems 30 (2017)\n33. Wang, X., Li, C., Wang, Z., Bai, F., Luo, H., Zhang, J., Jojic, N., Xing, E.P., Hu,\n     Z.: Promptagent: Strategic planning with language models enables expert-level\n   prompt optimization. arXiv preprint arXiv:2310.16427 (2023)\n34. Wei, J., Wang, X., Schuurmans, D., et al.: Chain-of-thought prompting elicits rea-\n    soning in large language models. Advances in NIPS 35 (2022)\n\n14     João Rodrigues and António Branco\n\n\n35. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.V., Zhou, D., Chen, X.: Large language\n    models as optimizers. arXiv preprint arXiv:2309.03409 (2023)\n36. Yao, S., Yu, D., Zhao, J., et al.: Tree of thoughts: Deliberate problem solving with\n    large language models. Advances in NIPS 36 (2024)\n37. Zhong, Z., Friedman, D., Chen, D.: Factual probing is [mask]: Learning vs. learning\n    to recall. In: Proceedings of the 2021 Conference of the NACL (2021)",
"headers": [
"arXiv:2407.03955v1  [cs.CL]  4 Jul 2024",
"Meta-prompting Optimized Retrieval-augmented",
"Generation",
"1",
"Introduction",
"2",
"Related work",
"3",
"Method",
"4",
"Dataset and models",
"5",
"Results and discussion",
"6",
"Future work",
"7",
"Conclusion",
"References"
],
"tables": [],
"file_path": "/Users/theresa/projects/miro_case/periscope/data/2407.03955v1.pdf"
}